{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the full aggregation pipeline for this ChangeStream and return a CommandCursor.", "response": "def _run_aggregation_cmd(self, session, explicit_session):\n        \"\"\"Run the full aggregation pipeline for this ChangeStream and return\n        the corresponding CommandCursor.\n        \"\"\"\n        read_preference = self._target._read_preference_for(session)\n        client = self._database.client\n\n        def _cmd(session, server, sock_info, slave_ok):\n            pipeline = self._full_pipeline()\n            cmd = SON([(\"aggregate\", self._aggregation_target),\n                       (\"pipeline\", pipeline),\n                       (\"cursor\", {})])\n\n            result = sock_info.command(\n                self._database.name,\n                cmd,\n                slave_ok,\n                read_preference,\n                self._target.codec_options,\n                parse_write_concern_error=True,\n                read_concern=self._target.read_concern,\n                collation=self._collation,\n                session=session,\n                client=self._database.client)\n\n            cursor = result[\"cursor\"]\n\n            if (self._start_at_operation_time is None and\n                self._resume_token is None and\n                cursor.get(\"_id\") is None and\n                sock_info.max_wire_version >= 7):\n                self._start_at_operation_time = result[\"operationTime\"]\n\n            ns = cursor[\"ns\"]\n            _, collname = ns.split(\".\", 1)\n            aggregation_collection = self._database.get_collection(\n                collname, codec_options=self._target.codec_options,\n                read_preference=read_preference,\n                write_concern=self._target.write_concern,\n                read_concern=self._target.read_concern\n            )\n\n            return CommandCursor(\n                aggregation_collection, cursor, sock_info.address,\n                batch_size=self._batch_size or 0,\n                max_await_time_ms=self._max_await_time_ms,\n                session=session, explicit_session=explicit_session)\n\n        return client._retryable_read(_cmd, read_preference, session)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreestablishing this change stream after a resumable error.", "response": "def _resume(self):\n        \"\"\"Reestablish this change stream after a resumable error.\"\"\"\n        try:\n            self._cursor.close()\n        except PyMongoError:\n            pass\n        self._cursor = self._create_cursor()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef next(self):\n        while self.alive:\n            doc = self.try_next()\n            if doc is not None:\n                return doc\n\n        raise StopIteration", "response": "Advance the cursor.\n\n        This method blocks until the next change document is returned or an\n        unrecoverable error is raised. This method is used when iterating over\n        all changes in the cursor. For example::\n\n            try:\n                with db.collection.watch(\n                        [{'$match': {'operationType': 'insert'}}]) as stream:\n                    for insert_change in stream:\n                        print(insert_change)\n            except pymongo.errors.PyMongoError:\n                # The ChangeStream encountered an unrecoverable error or the\n                # resume attempt failed to recreate the cursor.\n                logging.error('...')\n\n        Raises :exc:`StopIteration` if this ChangeStream is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadvances the cursor without blocking indefinitely for the next change.", "response": "def try_next(self):\n        \"\"\"Advance the cursor without blocking indefinitely.\n\n        This method returns the next change document without waiting\n        indefinitely for the next change. For example::\n\n            with db.collection.watch() as stream:\n                while stream.alive:\n                    change = stream.try_next()\n                    if change is not None:\n                        print(change)\n                    elif stream.alive:\n                        # We end up here when there are no recent changes.\n                        # Sleep for a while to avoid flooding the server with\n                        # getMore requests when no changes are available.\n                        time.sleep(10)\n\n        If no change document is cached locally then this method runs a single\n        getMore command. If the getMore yields any documents, the next\n        document is returned, otherwise, if the getMore returns no documents\n        (because there have been no changes) then ``None`` is returned.\n\n        :Returns:\n          The next change document or ``None`` when no document is available\n          after running a single getMore or when the cursor is closed.\n\n        .. versionadded:: 3.8\n        \"\"\"\n        # Attempt to get the next change with at most one getMore and at most\n        # one resume attempt.\n        try:\n            change = self._cursor._try_next(True)\n        except ConnectionFailure:\n            self._resume()\n            change = self._cursor._try_next(False)\n        except OperationFailure as exc:\n            if exc.code in _NON_RESUMABLE_GETMORE_ERRORS:\n                raise\n            self._resume()\n            change = self._cursor._try_next(False)\n\n        # No changes are available.\n        if change is None:\n            return None\n\n        try:\n            resume_token = change['_id']\n        except KeyError:\n            self.close()\n            raise InvalidOperation(\n                \"Cannot provide resume functionality when the resume \"\n                \"token is missing.\")\n        self._resume_token = copy.copy(resume_token)\n        self._start_at_operation_time = None\n\n        if self._decode_custom:\n            return _bson_to_dict(change.raw, self._orig_codec_options)\n        return change"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_integer(option, value):\n    if isinstance(value, integer_types):\n        return value\n    elif isinstance(value, string_type):\n        try:\n            return int(value)\n        except ValueError:\n            raise ValueError(\"The value of %s must be \"\n                             \"an integer\" % (option,))\n    raise TypeError(\"Wrong type for %s, value must be an integer\" % (option,))", "response": "Validates that value is an integer or basestring representation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that value is an integer or a string.", "response": "def validate_non_negative_int_or_basestring(option, value):\n    \"\"\"Validates that 'value' is an integer or string.\n    \"\"\"\n    if isinstance(value, integer_types):\n        return value\n    elif isinstance(value, string_type):\n        try:\n            val = int(value)\n        except ValueError:\n            return value\n        return validate_non_negative_integer(option, val)\n    raise TypeError(\"Wrong type for %s, value must be an \"\n                    \"non negative integer or a string\" % (option,))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_document_class(option, value):\n    if not issubclass(value, (abc.MutableMapping, RawBSONDocument)):\n        raise TypeError(\"%s must be dict, bson.son.SON, \"\n                        \"bson.raw_bson.RawBSONDocument, or a \"\n                        \"sublass of collections.MutableMapping\" % (option,))\n    return value", "response": "Validate the document_class option."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_type_registry(option, value):\n    if value is not None and not isinstance(value, TypeRegistry):\n        raise TypeError(\"%s must be an instance of %s\" % (\n            option, TypeRegistry))\n    return value", "response": "Validate the type_registry option."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_is_document_type(option, value):\n    if not isinstance(value, (abc.MutableMapping, RawBSONDocument)):\n        raise TypeError(\"%s must be an instance of dict, bson.son.SON, \"\n                        \"bson.raw_bson.RawBSONDocument, or \"\n                        \"a type that inherits from \"\n                        \"collections.MutableMapping\" % (option,))", "response": "Validate the type of method arguments that expect a MongoDB document."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating the driver keyword arg.", "response": "def validate_driver_or_none(option, value):\n    \"\"\"Validate the driver keyword arg.\"\"\"\n    if value is None:\n        return value\n    if not isinstance(value, DriverInfo):\n        raise TypeError(\"%s must be an instance of DriverInfo\" % (option,))\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef validate_is_callable_or_none(option, value):\n    if value is None:\n        return value\n    if not callable(value):\n        raise ValueError(\"%s must be a callable\" % (option,))\n    return value", "response": "Validates that value is a callable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the read preference for this instance or session.", "response": "def _read_preference_for(self, session):\n        \"\"\"Read only access to the read preference of this instance or session.\n        \"\"\"\n        # Override this operation's read preference with the transaction's.\n        if session:\n            return session._txn_read_preference() or self.__read_preference\n        return self.__read_preference"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets data files from top directory.", "response": "def get_data_files(top):\n    \"\"\"Get data files\"\"\"\n\n    data_files = []\n    ntrim = len(here + os.path.sep)\n\n    for (d, _, filenames) in os.walk(top):\n        data_files.append((\n            d[ntrim:],\n            [os.path.join(d, f) for f in filenames]\n        ))\n    return data_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_cmdclass(develop_wrappers=None, distribute_wrappers=None, data_dirs=None):\n    develop_wrappers = develop_wrappers or []\n    distribute_wrappers = distribute_wrappers or []\n    data_dirs = data_dirs or []\n    develop_wrapper = functools.partial(wrap_command, develop_wrappers, data_dirs)\n    distribute_wrapper = functools.partial(wrap_command, distribute_wrappers, data_dirs)\n    cmdclass = dict(\n        develop=develop_wrapper(develop, strict=True),\n        sdist=distribute_wrapper(sdist, strict=True),\n        bdist_egg=bdist_egg if 'bdist_egg' in sys.argv else bdist_egg_disabled\n    )\n    if bdist_wheel:\n        cmdclass['bdist_wheel'] = bdist_wheel\n    return cmdclass", "response": "Create a command class with the given optional wrappers."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nechoes a command before running it. Defaults to repo as cwd", "response": "def run(cmd, *args, **kwargs):\n    \"\"\"Echo a command before running it.  Defaults to repo as cwd\"\"\"\n    log.info('> ' + list2cmdline(cmd))\n    kwargs.setdefault('cwd', here)\n    kwargs.setdefault('shell', sys.platform == 'win32')\n    if not isinstance(cmd, list):\n        cmd = cmd.split()\n    return check_call(cmd, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_stale(target, source):\n    if not os.path.exists(target):\n        return True\n    target_mtime = recursive_mtime(target) or 0\n    return compare_recursive_mtime(source, cutoff=target_mtime)", "response": "Test whether the target file is stale based on the source file or directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a Command that combines several commands.", "response": "def combine_commands(*commands):\n    \"\"\"Return a Command that combines several commands.\"\"\"\n\n    class CombinedCommand(Command):\n\n        def initialize_options(self):\n            self.commands = []\n            for C in commands:\n                self.commands.append(C(self.distribution))\n            for c in self.commands:\n                c.initialize_options()\n\n        def finalize_options(self):\n            for c in self.commands:\n                c.finalize_options()\n\n        def run(self):\n            for c in self.commands:\n                c.run()\n\n    return CombinedCommand"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compare_recursive_mtime(path, cutoff, newest=True):\n    if os.path.isfile(path):\n        mt = mtime(path)\n        if newest:\n            if mt > cutoff:\n                return True\n        elif mt < cutoff:\n            return True\n    for dirname, _, filenames in os.walk(path, topdown=False):\n        for filename in filenames:\n            mt = mtime(os.path.join(dirname, filename))\n            if newest:  # Put outside of loop?\n                if mt > cutoff:\n                    return True\n            elif mt < cutoff:\n                return True\n    return False", "response": "Compare the newest mtime for all files in a directory and all files in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the newest or oldest mtime for all files in a directory.", "response": "def recursive_mtime(path, newest=True):\n    \"\"\"Gets the newest/oldest mtime for all files in a directory.\"\"\"\n    if os.path.isfile(path):\n        return mtime(path)\n    current_extreme = None\n    for dirname, _, filenames in os.walk(path, topdown=False):\n        for filename in filenames:\n            mt = mtime(os.path.join(dirname, filename))\n            if newest:  # Put outside of loop?\n                if mt >= (current_extreme or mt):\n                    current_extreme = mt\n            elif mt <= (current_extreme or mt):\n                current_extreme = mt\n    return current_extreme"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Command that installs node_modules.", "response": "def install_node_modules(path=None, build_dir=None, source_dir=None, build_cmd='build', force=False):\n    \"\"\"Return a Command for managing an node_modules installation.\n    Note: The command is skipped if the `--skip-yarn` flag is used.\n    \n    Parameters\n    ----------\n    path: str, optional\n        The base path of the node package.  Defaults to the repo root.\n    build_dir: str, optional\n        The target build directory.  If this and source_dir are given,\n        the JavaScript will only be build if necessary.\n    source_dir: str, optional\n        The source code directory.\n    build_cmd: str, optional\n        The yarn command to build assets to the build_dir.\n    \"\"\"\n\n    class Yarn(BaseCommand):\n        description = 'install package.json dependencies using yarn'\n\n        def run(self):\n            if skip_yarn:\n                log.info('Skipping yarn-installation')\n                return\n            node_package = path or here\n            node_modules = os.path.join(node_package, 'node_modules')\n\n            if not which(\"yarn\"):\n                log.error(\"`yarn` unavailable.  If you're running this command \"\n                          \"using sudo, make sure `yarn` is availble to sudo\")\n                return\n            if force or is_stale(node_modules, os.path.join(node_package, 'package.json')):\n                log.info('Installing build dependencies with yarn.  This may '\n                         'take a while...')\n                run(['yarn', 'install'], cwd=node_package)\n            if build_dir and source_dir and not force:\n                should_build = is_stale(build_dir, source_dir)\n            else:\n                should_build = True\n            if should_build:\n                run(['yarn', 'run', build_cmd], cwd=node_package)\n\n    return Yarn"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_gradle(path=kernel_path, cmd='build', skip_tests=False):\n\n    class Gradle(BaseCommand):\n        description = 'Run gradle script'\n\n        def skip_test_option(self, skip):\n            if skip:\n                return '-Dskip.tests=True'\n            else:\n                return '-Dskip.tests=False'\n\n        def run(self):\n            run([('' if sys.platform == 'win32' else './') + 'gradlew', '--no-daemon', cmd,\n                 self.skip_test_option(skip_tests)], cwd=path)\n\n    return Gradle", "response": "Returns a Command for running gradle scripts."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwrapping a command in a setup command that runs the commands and returns the result.", "response": "def wrap_command(cmds, data_dirs, cls, strict=True):\n    \"\"\"Wrap a setup command\n    Parameters\n    ----------\n    cmds: list(str)\n        The names of the other commands to run prior to the command.\n    strict: boolean, optional\n        Wether to raise errors when a pre-command fails.\n    \"\"\"\n\n    class WrappedCommand(cls):\n\n        def run(self):\n            if not getattr(self, 'uninstall', None):\n                try:\n                    [self.run_command(cmd) for cmd in cmds]\n                except Exception:\n                    if strict:\n                        raise\n                    else:\n                        pass\n\n            result = cls.run(self)\n            data_files = []\n            for dname in data_dirs:\n                data_files.extend(get_data_files(dname))\n            # update data-files in case this created new files\n            self.distribution.data_files = data_files\n            # also update package data\n            update_package_data(self.distribution)\n            return result\n\n    return WrappedCommand"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the initialization status of Vault.", "response": "def read_init_status(self):\n        \"\"\"Read the initialization status of Vault.\n\n        Supported methods:\n            GET: /sys/init. Produces: 200 application/json\n\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/sys/init'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a new version of the object.", "response": "def initialize(self, secret_shares=5, secret_threshold=3, pgp_keys=None, root_token_pgp_key=None,\n                   stored_shares=None, recovery_shares=None, recovery_threshold=None, recovery_pgp_keys=None):\n        \"\"\"Initialize a new Vault.\n\n        The Vault must not have been previously initialized. The recovery options, as well as the stored shares option,\n        are only available when using Vault HSM.\n\n        Supported methods:\n            PUT: /sys/init. Produces: 200 application/json\n\n        :param secret_shares: The number of shares to split the master key into.\n        :type secret_shares: int\n        :param secret_threshold: Specifies the number of shares required to reconstruct the master key. This must be\n            less than or equal secret_shares. If using Vault HSM with auto-unsealing, this value must be the same as\n            secret_shares.\n        :type secret_threshold: int\n        :param pgp_keys: List of PGP public keys used to encrypt the output unseal keys.\n            Ordering is preserved. The keys must be base64-encoded from their original binary representation.\n            The size of this array must be the same as secret_shares.\n        :type pgp_keys: list\n        :param root_token_pgp_key: Specifies a PGP public key used to encrypt the initial root token. The\n            key must be base64-encoded from its original binary representation.\n        :type root_token_pgp_key: str | unicode\n        :param stored_shares: <enterprise only> Specifies the number of shares that should be encrypted by the HSM and\n            stored for auto-unsealing. Currently must be the same as secret_shares.\n        :type stored_shares: int\n        :param recovery_shares: <enterprise only> Specifies the number of shares to split the recovery key into.\n        :type recovery_shares: int\n        :param recovery_threshold: <enterprise only> Specifies the number of shares required to reconstruct the recovery\n            key. This must be less than or equal to recovery_shares.\n        :type recovery_threshold: int\n        :param recovery_pgp_keys: <enterprise only> Specifies an array of PGP public keys used to encrypt the output\n            recovery keys. Ordering is preserved. The keys must be base64-encoded from their original binary\n            representation. The size of this array must be the same as recovery_shares.\n        :type recovery_pgp_keys: list\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        params = {\n            'secret_shares': secret_shares,\n            'secret_threshold': secret_threshold,\n            'root_token_pgp_key': root_token_pgp_key,\n        }\n\n        if pgp_keys is not None:\n            if len(pgp_keys) != secret_shares:\n                raise ParamValidationError('length of pgp_keys list argument must equal secret_shares value')\n            params['pgp_keys'] = pgp_keys\n\n        if stored_shares is not None:\n            if stored_shares != secret_shares:\n                raise ParamValidationError('value for stored_shares argument must equal secret_shares argument')\n            params['stored_shares'] = stored_shares\n\n        if recovery_shares is not None:\n            params['recovery_shares'] = recovery_shares\n\n        if recovery_threshold is not None:\n            if recovery_threshold > recovery_shares:\n                error_msg = 'value for recovery_threshold argument be less than or equal to recovery_shares argument'\n                raise ParamValidationError(error_msg)\n            params['recovery_threshold'] = recovery_threshold\n\n        if recovery_pgp_keys is not None:\n            if len(recovery_pgp_keys) != recovery_shares:\n                raise ParamValidationError('length of recovery_pgp_keys list argument must equal recovery_shares value')\n            params['recovery_pgp_keys'] = recovery_pgp_keys\n\n        api_path = '/v1/sys/init'\n        response = self._adapter.put(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef list_secrets(self, path, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/{path}'.format(mount_point=mount_point, path=path)\n        response = self._adapter.list(\n            url=api_path,\n        )\n        return response.json()", "response": "List the key names at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate or update a secret at the specified location.", "response": "def create_or_update_secret(self, path, secret, method=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Store a secret at the specified location.\n\n        If the value does not yet exist, the calling token must have an ACL policy granting the create capability.\n        If the value already exists, the calling token must have an ACL policy granting the update capability.\n\n        Supported methods:\n            POST: /{mount_point}/{path}. Produces: 204 (empty body)\n            PUT: /{mount_point}/{path}. Produces: 204 (empty body)\n\n        :param path: Specifies the path of the secrets to create/update. This is specified as part of the URL.\n        :type path: str | unicode\n        :param secret: Specifies keys, paired with associated values, to be held at the given location. Multiple\n            key/value pairs can be specified, and all will be returned on a read operation. A key called ttl will\n            trigger some special behavior. See the Vault KV secrets engine documentation for details.\n        :type secret: dict\n        :param method: Optional parameter to explicitly request a POST (create) or PUT (update) request to the selected\n            kv secret engine. If no argument is provided for this parameter, hvac attempts to intelligently determine\n            which method is appropriate.\n        :type method: str | unicode\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the create_or_update_secret request.\n        :rtype: requests.Response\n        \"\"\"\n        if method is None:\n            # If no method was selected by the caller, use the result of a `read_secret()` call to determine if we need\n            # to perform an update (PUT) or creation (POST) request.\n            try:\n                self.read_secret(\n                    path=path,\n                    mount_point=mount_point,\n                )\n                method = 'PUT'\n            except exceptions.InvalidPath:\n                method = 'POST'\n\n        if method == 'POST':\n            api_path = '/v1/{mount_point}/{path}'.format(mount_point=mount_point, path=path)\n            return self._adapter.post(\n                url=api_path,\n                json=secret,\n            )\n\n        elif method == 'PUT':\n            api_path = '/v1/{mount_point}/{path}'.format(mount_point=mount_point, path=path)\n            return self._adapter.post(\n                url=api_path,\n                json=secret,\n            )\n\n        else:\n            error_message = '\"method\" parameter provided invalid value; POST or PUT allowed, \"{method}\" provided'.format(method=method)\n            raise exceptions.ParamValidationError(error_message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete the secret at the specified location.", "response": "def delete_secret(self, path, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Delete the secret at the specified location.\n\n        Supported methods:\n            DELETE: /{mount_point}/{path}. Produces: 204 (empty body)\n\n\n        :param path: Specifies the path of the secret to delete.\n            This is specified as part of the URL.\n        :type path: str | unicode\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the delete_secret request.\n        :rtype: requests.Response\n        \"\"\"\n        api_path = '/v1/{mount_point}/{path}'.format(mount_point=mount_point, path=path)\n        return self._adapter.delete(\n            url=api_path,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configure(self, host, secret, port=1812, unregistered_user_policies=None, dial_timeout=10, nas_port=10,\n                  mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"\n        Configure the RADIUS auth method.\n\n        Supported methods:\n            POST: /auth/{mount_point}/config. Produces: 204 (empty body)\n\n        :param host: The RADIUS server to connect to. Examples: radius.myorg.com, 127.0.0.1\n        :type host: str | unicode\n        :param secret: The RADIUS shared secret.\n        :type secret: str | unicode\n        :param port: The UDP port where the RADIUS server is listening on. Defaults is 1812.\n        :type port: int\n        :param unregistered_user_policies: A comma-separated list of policies to be granted to unregistered users.\n        :type unregistered_user_policies: list\n        :param dial_timeout: Number of second to wait for a backend connection before timing out. Default is 10.\n        :type dial_timeout: int\n        :param nas_port: The NAS-Port attribute of the RADIUS request. Defaults is 10.\n        :type nas_port: int\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the configure request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'host': host,\n            'secret': secret,\n            'port': port,\n            'dial_timeout': dial_timeout,\n            'nas_port': nas_port,\n        }\n        # Fill out params dictionary with any optional parameters provided\n        if unregistered_user_policies is not None:\n            if not isinstance(unregistered_user_policies, list):\n                error_msg = (\n                    '\"unregistered_user_policies\" argument must be an instance of list or None, '\n                    '\"{unregistered_user_policies}\" provided.'\n                ).format(unregistered_user_policies=type(unregistered_user_policies))\n                raise exceptions.ParamValidationError(error_msg)\n\n            params['unregistered_user_policies'] = ','.join(unregistered_user_policies)\n\n        api_path = '/v1/auth/{mount_point}/config'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Configure the RADIUS API method."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef configure_root_iam_credentials(self, access_key, secret_key, region=None, iam_endpoint=None, sts_endpoint=None,\n                                       max_retries=-1, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Configure the root IAM credentials to communicate with AWS.\n\n        There are multiple ways to pass root IAM credentials to the Vault server, specified below with the highest\n        precedence first. If credentials already exist, this will overwrite them.\n\n        The official AWS SDK is used for sourcing credentials from env vars, shared files, or IAM/ECS instances.\n\n            * Static credentials provided to the API as a payload\n            * Credentials in the AWS_ACCESS_KEY, AWS_SECRET_KEY, and AWS_REGION environment variables on the server\n            * Shared credentials files\n            * Assigned IAM role or ECS task role credentials\n\n        At present, this endpoint does not confirm that the provided AWS credentials are valid AWS credentials with\n        proper permissions.\n\n        Supported methods:\n            POST: /{mount_point}/config/root. Produces: 204 (empty body)\n\n        :param access_key: Specifies the AWS access key ID.\n        :type access_key: str | unicode\n        :param secret_key: Specifies the AWS secret access key.\n        :type secret_key: str | unicode\n        :param region: Specifies the AWS region. If not set it will use the AWS_REGION env var, AWS_DEFAULT_REGION env\n            var, or us-east-1 in that order.\n        :type region: str | unicode\n        :param iam_endpoint: Specifies a custom HTTP IAM endpoint to use.\n        :type iam_endpoint: str | unicode\n        :param sts_endpoint: Specifies a custom HTTP STS endpoint to use.\n        :type sts_endpoint: str | unicode\n        :param max_retries: Number of max retries the client should use for recoverable errors. The default (-1) falls\n            back to the AWS SDK's default behavior.\n        :type max_retries: int\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'access_key': access_key,\n            'secret_key': secret_key,\n            'region': region,\n            'iam_endpoint': iam_endpoint,\n            'sts_endpoint': sts_endpoint,\n            'max_retries': max_retries,\n        }\n        api_path = '/v1/{mount_point}/config/root'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Configure the root IAM credentials for the Vault server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rotate_root_iam_credentials(self, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/config/rotate-root'.format(mount_point=mount_point)\n        response = self._adapter.post(\n            url=api_path,\n        )\n        return response.json()", "response": "Rotate static root IAM credentials."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef configure_lease(self, lease, lease_max, mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'lease': lease,\n            'lease_max': lease_max,\n        }\n        api_path = '/v1/{mount_point}/config/lease'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Configure the lease settings for the AWS secrets engine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read_lease_config(self, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/config/lease'.format(mount_point=mount_point)\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "Read the current lease settings for the AWS secrets engine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_or_update_role(self, name, credential_type, policy_document=None, default_sts_ttl=None, max_sts_ttl=None,\n                              role_arns=None, policy_arns=None, legacy_params=False, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Create or update the role with the given name.\n\n        If a role with the name does not exist, it will be created. If the role exists, it will be updated with the new\n        attributes.\n\n        Supported methods:\n            POST: /{mount_point}/roles/{name}. Produces: 204 (empty body)\n\n        :param name: Specifies the name of the role to create. This is part of the request URL.\n        :type name: str | unicode\n        :param credential_type: Specifies the type of credential to be used when retrieving credentials from the role.\n            Must be one of iam_user, assumed_role, or federation_token.\n        :type credential_type: str | unicode\n        :param policy_document: The IAM policy document for the role. The behavior depends on the credential type. With\n            iam_user, the policy document will be attached to the IAM user generated and augment the permissions the IAM\n            user has. With assumed_role and federation_token, the policy document will act as a filter on what the\n            credentials can do.\n        :type policy_document: dict | str | unicode\n        :param default_sts_ttl: The default TTL for STS credentials. When a TTL is not specified when STS credentials\n            are requested, and a default TTL is specified on the role, then this default TTL will be used. Valid only\n            when credential_type is one of assumed_role or federation_token.\n        :type default_sts_ttl: str | unicode\n        :param max_sts_ttl: The max allowed TTL for STS credentials (credentials TTL are capped to max_sts_ttl). Valid\n            only when credential_type is one of assumed_role or federation_token.\n        :type max_sts_ttl: str | unicode\n        :param role_arns: Specifies the ARNs of the AWS roles this Vault role is allowed to assume. Required when\n            credential_type is assumed_role and prohibited otherwise. This is a comma-separated string or JSON array.\n            String types supported for Vault legacy parameters.\n        :type role_arns: list | str | unicode\n        :param policy_arns: Specifies the ARNs of the AWS managed policies to be attached to IAM users when they are\n            requested. Valid only when credential_type is iam_user. When credential_type is iam_user, at least one of\n            policy_arns or policy_document must be specified. This is a comma-separated string or JSON array.\n        :type policy_arns: list\n        :param legacy_params: Flag to send legacy (Vault versions < 0.11.0) parameters in the request. When this is set\n            to True, policy_document and policy_arns are the only parameters used from this method.\n        :type legacy_params: bool\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if credential_type not in ALLOWED_CREDS_TYPES:\n            error_msg = 'invalid credential_type argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=credential_type,\n                allowed_types=', '.join(ALLOWED_CREDS_TYPES),\n            ))\n        if isinstance(policy_document, dict):\n            policy_document = json.dumps(policy_document, indent=4, sort_keys=True)\n\n        if legacy_params:\n            # Support for Vault <0.11.0\n            params = {\n                'policy': policy_document,\n                'arn': policy_arns[0] if isinstance(policy_arns, list) else policy_arns,\n            }\n        else:\n            params = {\n                'credential_type': credential_type,\n                'policy_document': policy_document,\n                'default_sts_ttl': default_sts_ttl,\n                'max_sts_ttl': max_sts_ttl,\n                'role_arns': role_arns,\n                'policy_arns': policy_arns,\n            }\n        api_path = '/v1/{mount_point}/roles/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Create or update a Vault role."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_role(self, name, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/roles/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.delete(\n            url=api_path,\n        )", "response": "Delete an existing role by the given name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngenerates credentials for a named role.", "response": "def generate_credentials(self, name, role_arn=None, ttl=\"3600s\", endpoint='creds', mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Generates credential based on the named role.\n\n        This role must be created before queried.\n\n        The /aws/creds and /aws/sts endpoints are almost identical. The exception is when retrieving credentials for a\n        role that was specified with the legacy arn or policy parameter. In this case, credentials retrieved through\n        /aws/sts must be of either the assumed_role or federation_token types, and credentials retrieved through\n        /aws/creds must be of the iam_user type.\n\n        :param name: Specifies the name of the role to generate credentials against. This is part of the request URL.\n        :type name: str | unicode\n        :param role_arn: The ARN of the role to assume if credential_type on the Vault role is assumed_role. Must match\n            one of the allowed role ARNs in the Vault role. Optional if the Vault role only allows a single AWS role\n            ARN; required otherwise.\n        :type role_arn: str | unicode\n        :param ttl: Specifies the TTL for the use of the STS token. This is specified as a string with a duration\n            suffix. Valid only when credential_type is assumed_role or federation_token. When not specified, the default\n            sts_ttl set for the role will be used. If that is also not set, then the default value of 3600s will be\n            used. AWS places limits on the maximum TTL allowed. See the AWS documentation on the DurationSeconds\n            parameter for AssumeRole (for assumed_role credential types) and GetFederationToken (for federation_token\n            credential types) for more details.\n        :type ttl: str | unicode\n        :param endpoint: Supported endpoints:\n            GET: /{mount_point}/creds/{name}. Produces: 200 application/json\n            GET: /{mount_point}/sts/{name}. Produces: 200 application/json\n        :type endpoint: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        if endpoint not in ALLOWED_CREDS_ENDPOINTS:\n            error_msg = 'invalid endpoint argument provided \"{arg}\", supported types: \"{allowed_endpoints}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=endpoint,\n                allowed_endpoints=', '.join(ALLOWED_CREDS_ENDPOINTS),\n            ))\n        params = {\n            'name': name,\n            'role_arn': role_arn,\n            'ttl': ttl,\n        }\n        api_path = '/v1/{mount_point}/{endpoint}/{name}'.format(\n            mount_point=mount_point,\n            endpoint=endpoint,\n            name=name,\n        )\n\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef configure(self, subscription_id, tenant_id, client_id=\"\", client_secret=\"\", environment='AzurePublicCloud',\n                  mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Configure the credentials required for the plugin to perform API calls to Azure.\n\n        These credentials will be used to query roles and create/delete service principals. Environment variables will\n        override any parameters set in the config.\n\n        Supported methods:\n            POST: /{mount_point}/config. Produces: 204 (empty body)\n\n\n        :param subscription_id: The subscription id for the Azure Active Directory\n        :type subscription_id: str | unicode\n        :param tenant_id: The tenant id for the Azure Active Directory.\n        :type tenant_id: str | unicode\n        :param client_id: The OAuth2 client id to connect to Azure.\n        :type client_id: str | unicode\n        :param client_secret: The OAuth2 client secret to connect to Azure.\n        :type client_secret: str | unicode\n        :param environment: The Azure environment. If not specified, Vault will use Azure Public Cloud.\n        :type environment: str | unicode\n        :param mount_point: The OAuth2 client secret to connect to Azure.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if environment not in VALID_ENVIRONMENTS:\n            error_msg = 'invalid environment argument provided \"{arg}\", supported environments: \"{environments}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=environment,\n                environments=','.join(VALID_ENVIRONMENTS),\n            ))\n        params = {\n            'subscription_id': subscription_id,\n            'tenant_id': tenant_id,\n            'client_id': client_id,\n            'client_secret': client_secret,\n            'environment': environment,\n        }\n        api_path = '/v1/{mount_point}/config'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Configure the credentials required for the plugin to perform API calls to Azure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_config(self, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/config'.format(mount_point=mount_point)\n        return self._adapter.delete(\n            url=api_path,\n        )", "response": "Delete the stored Azure configuration and credentials."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_or_update_role(self, name, azure_roles, ttl=\"\", max_ttl=\"\", mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'azure_roles': json.dumps(azure_roles),\n            'ttl': ttl,\n            'max_ttl': max_ttl,\n        }\n        api_path = '/v1/{mount_point}/roles/{name}'.format(\n            mount_point=mount_point,\n            name=name\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Create or update a Vault role."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring the LDAP auth method.", "response": "def configure(self, user_dn, group_dn, url='ldap://127.0.0.1', case_sensitive_names=False, starttls=False,\n                  tls_min_version='tls12', tls_max_version='tls12', insecure_tls=False, certificate=None, bind_dn=None,\n                  bind_pass=None, user_attr='cn', discover_dn=False, deny_null_bind=True, upn_domain=None,\n                  group_filter=DEFAULT_GROUP_FILTER, group_attr='cn', mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"\n        Configure the LDAP auth method.\n\n        Supported methods:\n            POST: /auth/{mount_point}/config. Produces: 204 (empty body)\n\n        :param user_dn: Base DN under which to perform user search. Example: ou=Users,dc=example,dc=com\n        :type user_dn: str | unicode\n        :param group_dn: LDAP search base to use for group membership search. This can be the root containing either\n            groups or users. Example: ou=Groups,dc=example,dc=com\n        :type group_dn: str | unicode\n        :param url: The LDAP server to connect to. Examples: ldap://ldap.myorg.com, ldaps://ldap.myorg.com:636.\n            Multiple URLs can be specified with commas, e.g. ldap://ldap.myorg.com,ldap://ldap2.myorg.com; these will be\n            tried in-order.\n        :type url: str | unicode\n        :param case_sensitive_names: If set, user and group names assigned to policies within the backend will be case\n            sensitive. Otherwise, names will be normalized to lower case. Case will still be preserved when sending the\n            username to the LDAP server at login time; this is only for matching local user/group definitions.\n        :type case_sensitive_names: bool\n        :param starttls: If true, issues a StartTLS command after establishing an unencrypted connection.\n        :type starttls: bool\n        :param tls_min_version: Minimum TLS version to use. Accepted values are tls10, tls11 or tls12.\n        :type tls_min_version: str | unicode\n        :param tls_max_version: Maximum TLS version to use. Accepted values are tls10, tls11 or tls12.\n        :type tls_max_version: str | unicode\n        :param insecure_tls: If true, skips LDAP server SSL certificate verification - insecure, use with caution!\n        :type insecure_tls: bool\n        :param certificate: CA certificate to use when verifying LDAP server certificate, must be x509 PEM encoded.\n        :type certificate: str | unicode\n        :param bind_dn: Distinguished name of object to bind when performing user search. Example:\n            cn=vault,ou=Users,dc=example,dc=com\n        :type bind_dn: str | unicode\n        :param bind_pass:  Password to use along with binddn when performing user search.\n        :type bind_pass: str | unicode\n        :param user_attr: Attribute on user attribute object matching the username passed when authenticating. Examples:\n            sAMAccountName, cn, uid\n        :type user_attr: str | unicode\n        :param discover_dn: Use anonymous bind to discover the bind DN of a user.\n        :type discover_dn: bool\n        :param deny_null_bind: This option prevents users from bypassing authentication when providing an empty password.\n        :type deny_null_bind: bool\n        :param upn_domain: The userPrincipalDomain used to construct the UPN string for the authenticating user. The\n            constructed UPN will appear as [username]@UPNDomain. Example: example.com, which will cause vault to bind as\n            username@example.com.\n        :type upn_domain: str | unicode\n        :param group_filter: Go template used when constructing the group membership query. The template can access the\n            following context variables: [UserDN, Username]. The default is\n            `(|(memberUid={{.Username}})(member={{.UserDN}})(uniqueMember={{.UserDN}}))`, which is compatible with several\n            common directory schemas. To support nested group resolution for Active Directory, instead use the following\n            query: (&(objectClass=group)(member:1.2.840.113556.1.4.1941:={{.UserDN}})).\n        :type group_filter: str | unicode\n        :param group_attr: LDAP attribute to follow on objects returned by groupfilter in order to enumerate user group\n            membership. Examples: for groupfilter queries returning group objects, use: cn. For queries returning user\n            objects, use: memberOf. The default is cn.\n        :type group_attr: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the configure request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'userdn': user_dn,\n            'groupdn': group_dn,\n            'url': url,\n            'case_sensitive_names': case_sensitive_names,\n            'starttls': starttls,\n            'tls_min_version': tls_min_version,\n            'tls_max_version': tls_max_version,\n            'insecure_tls': insecure_tls,\n            'certificate': certificate,\n            'userattr': user_attr,\n            'discoverdn': discover_dn,\n            'deny_null_bind': deny_null_bind,\n            'groupfilter': group_filter,\n            'groupattr': group_attr,\n        }\n        # Fill out params dictionary with any optional parameters provided\n        if upn_domain is not None:\n            params['upndomain'] = upn_domain\n        if bind_dn is not None:\n            params['binddn'] = bind_dn\n        if bind_pass is not None:\n            params['bindpass'] = bind_pass\n        if certificate is not None:\n            params['certificate'] = certificate\n\n        api_path = '/v1/auth/{mount_point}/config'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates or updates an LDAP group.", "response": "def create_or_update_group(self, name, policies=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"\n        Create or update LDAP group policies.\n\n        Supported methods:\n            POST: /auth/{mount_point}/groups/{name}. Produces: 204 (empty body)\n\n\n        :param name: The name of the LDAP group\n        :type name: str | unicode\n        :param policies: List of policies associated with the group. This parameter is transformed to a comma-delimited\n            string before being passed to Vault.\n        :type policies: list\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the create_or_update_group request.\n        :rtype: requests.Response\n        \"\"\"\n        if policies is None:\n            policies = []\n        if not isinstance(policies, list):\n            error_msg = '\"policies\" argument must be an instance of list or None, \"{policies_type}\" provided.'.format(\n                policies_type=type(policies),\n            )\n            raise exceptions.ParamValidationError(error_msg)\n\n        params = {\n            'policies': ','.join(policies),\n        }\n        api_path = '/v1/auth/{mount_point}/groups/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate or updates the LDAP user with the given policies and group associations.", "response": "def create_or_update_user(self, username, policies=None, groups=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"\n        Create or update LDAP users policies and group associations.\n\n        Supported methods:\n            POST: /auth/{mount_point}/users/{username}. Produces: 204 (empty body)\n\n\n        :param username: The username of the LDAP user\n        :type username: str | unicode\n        :param policies: List of policies associated with the user. This parameter is transformed to a comma-delimited\n            string before being passed to Vault.\n        :type policies: str | unicode\n        :param groups: List of groups associated with the user. This parameter is transformed to a comma-delimited\n            string before being passed to Vault.\n        :type groups: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the create_or_update_user request.\n        :rtype: requests.Response\n        \"\"\"\n        if policies is None:\n            policies = []\n        if groups is None:\n            groups = []\n        list_required_params = {\n            'policies': policies,\n            'groups': groups,\n        }\n        for param_name, param_arg in list_required_params.items():\n            if not isinstance(param_arg, list):\n                error_msg = '\"{param_name}\" argument must be an instance of list or None, \"{param_type}\" provided.'.format(\n                    param_name=param_name,\n                    param_type=type(param_arg),\n                )\n                raise exceptions.ParamValidationError(error_msg)\n\n        params = {\n            'policies': ','.join(policies),\n            'groups': ','.join(groups),\n        }\n        api_path = '/v1/auth/{mount_point}/users/{username}'.format(\n            mount_point=mount_point,\n            username=username,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nunwrap the original response inside the given wrapping token.", "response": "def unwrap(self, token=None):\n        \"\"\"Return the original response inside the given wrapping token.\n\n        Unlike simply reading cubbyhole/response (which is deprecated), this endpoint provides additional validation\n        checks on the token, returns the original value on the wire rather than a JSON string representation of it, and\n        ensures that the response is properly audit-logged.\n\n        Supported methods:\n            POST: /sys/wrapping/unwrap. Produces: 200 application/json\n\n        :param token: Specifies the wrapping token ID. This is required if the client token is not the wrapping token.\n            Do not use the wrapping token in both locations.\n        :type token: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        params = {}\n        if token is not None:\n            params['token'] = token\n\n        api_path = '/v1/sys/wrapping/unwrap'\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconfiguring the backend level settings that are applied to every key in the key - value store.", "response": "def configure(self, max_versions=10, cas_required=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Configure backend level settings that are applied to every key in the key-value store.\n\n        Supported methods:\n            POST: /{mount_point}/config. Produces: 204 (empty body)\n\n\n        :param max_versions: The number of versions to keep per key. This value applies to all keys, but a key's\n            metadata setting can overwrite this value. Once a key has more than the configured allowed versions the\n            oldest version will be permanently deleted. Defaults to 10.\n        :type max_versions: int\n        :param cas_required: If true all keys will require the cas parameter to be set on all write requests.\n        :type cas_required: bool\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'max_versions': max_versions,\n        }\n        if cas_required is not None:\n            params['cas_required'] = cas_required\n        api_path = '/v1/{mount_point}/config'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the secret at the specified location.", "response": "def read_secret_version(self, path, version=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Retrieve the secret at the specified location.\n\n        Supported methods:\n            GET: /{mount_point}/data/{path}. Produces: 200 application/json\n\n\n        :param path: Specifies the path of the secret to read. This is specified as part of the URL.\n        :type path: str | unicode\n        :param version: Specifies the version to return. If not set the latest version is returned.\n        :type version: int\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        params = {}\n        if version is not None:\n            params['version'] = version\n        api_path = '/v1/{mount_point}/data/{path}'.format(mount_point=mount_point, path=path)\n        response = self._adapter.get(\n            url=api_path,\n            params=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new version of a secret at the specified location.", "response": "def create_or_update_secret(self, path, secret, cas=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Create a new version of a secret at the specified location.\n\n        If the value does not yet exist, the calling token must have an ACL policy granting the create capability. If\n        the value already exists, the calling token must have an ACL policy granting the update capability.\n\n        Supported methods:\n            POST: /{mount_point}/data/{path}. Produces: 200 application/json\n\n        :param path: Path\n        :type path: str | unicode\n        :param cas: Set the \"cas\" value to use a Check-And-Set operation. If not set the write will be allowed. If set\n            to 0 a write will only be allowed if the key doesn't exist. If the index is non-zero the write will only be\n            allowed if the key's current version matches the version specified in the cas parameter.\n        :type cas: int\n        :param secret: The contents of the \"secret\" dict will be stored and returned on read.\n        :type secret: dict\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        params = {\n            'options': {},\n            'data': secret,\n        }\n\n        if cas is not None:\n            params['options']['cas'] = cas\n\n        api_path = '/v1/{mount_point}/data/{path}'.format(mount_point=mount_point, path=path)\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef patch(self, path, secret, mount_point=DEFAULT_MOUNT_POINT):\n        # First, do a read.\n        try:\n            current_secret_version = self.read_secret_version(\n                path=path,\n                mount_point=mount_point,\n            )\n        except exceptions.InvalidPath:\n            raise exceptions.InvalidPath('No value found at \"{path}\"; patch only works on existing data.'.format(path=path))\n\n        # Update existing secret dict.\n        patched_secret = current_secret_version['data']['data']\n        patched_secret.update(secret)\n\n        # Write back updated secret.\n        return self.create_or_update_secret(\n            path=path,\n            cas=current_secret_version['data']['metadata']['version'],\n            secret=patched_secret,\n            mount_point=mount_point,\n        )", "response": "Updates the secret dict at the specified path with the given secret."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_secret_versions(self, path, versions, mount_point=DEFAULT_MOUNT_POINT):\n        if not isinstance(versions, list) or len(versions) == 0:\n            error_msg = 'argument to \"versions\" must be a list containing one or more integers, \"{versions}\" provided.'.format(\n                versions=versions\n            )\n            raise exceptions.ParamValidationError(error_msg)\n        params = {\n            'versions': versions,\n        }\n        api_path = '/v1/{mount_point}/delete/{path}'.format(mount_point=mount_point, path=path)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Issue a soft delete of the specified versions of the secret at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_secret_metadata(self, path, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/metadata/{path}'.format(mount_point=mount_point, path=path)\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "Read the metadata and versions for the secret at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the max_versions of the cas_required setting on an existing path.", "response": "def update_metadata(self, path, max_versions=None, cas_required=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Updates the max_versions of cas_required setting on an existing path.\n\n        Supported methods:\n            POST: /{mount_point}/metadata/{path}. Produces: 204 (empty body)\n\n\n        :param path: Path\n        :type path: str | unicode\n        :param max_versions: The number of versions to keep per key. If not set, the backend's configured max version is\n            used. Once a key has more than the configured allowed versions the oldest version will be permanently\n            deleted.\n        :type max_versions: int\n        :param cas_required: If true the key will require the cas parameter to be set on all write requests. If false,\n            the backend's configuration will be used.\n        :type cas_required: bool\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {}\n        if max_versions is not None:\n            params['max_versions'] = max_versions\n        if cas_required is not None:\n            if not isinstance(cas_required, bool):\n                error_msg = 'bool expected for cas_required param, {type} received'.format(type=type(cas_required))\n                raise exceptions.ParamValidationError(error_msg)\n            params['cas_required'] = cas_required\n        api_path = '/v1/{mount_point}/metadata/{path}'.format(mount_point=mount_point, path=path)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconfiguring the credentials required for the GCP auth method to perform API calls to Google Cloud.", "response": "def configure(self, credentials=\"\", google_certs_endpoint=GCP_CERTS_ENDPOINT, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Configure the credentials required for the GCP auth method to perform API calls to Google Cloud.\n\n        These credentials will be used to query the status of IAM entities and get service account or other Google\n        public certificates to confirm signed JWTs passed in during login.\n\n        Supported methods:\n            POST: /auth/{mount_point}/config. Produces: 204 (empty body)\n\n\n        :param credentials: A JSON string containing the contents of a GCP credentials file. The credentials file must\n            have the following permissions: `iam.serviceAccounts.get`, `iam.serviceAccountKeys.get`.\n            If this value is empty, Vault will try to use Application Default Credentials from the machine on which the\n            Vault server is running. The project must have the iam.googleapis.com API enabled.\n        :type credentials: str | unicode\n        :param google_certs_endpoint: The Google OAuth2 endpoint from which to obtain public certificates. This is used\n            for testing and should generally not be set by end users.\n        :type google_certs_endpoint: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'credentials': credentials,\n            'google_certs_endpoint': google_certs_endpoint,\n        }\n        api_path = '/v1/auth/{mount_point}/config'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a new role in the GCP auth method.", "response": "def create_role(self, name, role_type, project_id, ttl=\"\", max_ttl=\"\", period=\"\", policies=None,\n                    bound_service_accounts=None, max_jwt_exp='15m', allow_gce_inference=True, bound_zones=None,\n                    bound_regions=None, bound_instance_groups=None, bound_labels=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Register a role in the GCP auth method.\n\n        Role types have specific entities that can perform login operations against this endpoint. Constraints specific\n            to the role type must be set on the role. These are applied to the authenticated entities attempting to\n            login.\n\n        Supported methods:\n            POST: /auth/{mount_point}/role/{name}. Produces: 204 (empty body)\n\n\n        :param name: The name of the role.\n        :type name: str | unicode\n        :param role_type: The type of this role. Certain fields correspond to specific roles and will be rejected\n            otherwise.\n        :type role_type: str | unicode\n        :param project_id: The GCP project ID. Only entities belonging to this project can authenticate with this role.\n        :type project_id: str | unicode\n        :param ttl: The TTL period of tokens issued using this role. This can be specified as an integer number of\n            seconds or as a duration value like \"5m\".\n        :type ttl: str | unicode\n        :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role. This can be specified\n            as an integer number of seconds or as a duration value like \"5m\".\n        :type max_ttl: str | unicode\n        :param period: If set, indicates that the token generated using this role should never expire. The token should\n            be renewed within the duration specified by this value. At each renewal, the token's TTL will be set to the\n            value of this parameter. This can be specified as an integer number of seconds or as a duration value like\n            \"5m\".\n        :type period: str | unicode\n        :param policies: The list of policies to be set on tokens issued using this role.\n        :type policies: list\n        :param bound_service_accounts: <required for iam> A list of service account emails or IDs that login is\n            restricted  to. If set to `*`, all service accounts are allowed (role will still be bound by project). Will be\n            inferred from service account used to issue metadata token for GCE instances.\n        :type bound_service_accounts: list\n        :param max_jwt_exp: <iam only> The number of seconds past the time of authentication that the login param JWT\n            must expire within. For example, if a user attempts to login with a token that expires within an hour and\n            this is set to 15 minutes, Vault will return an error prompting the user to create a new signed JWT with a\n            shorter exp. The GCE metadata tokens currently do not allow the exp claim to be customized.\n        :type max_jwt_exp: str | unicode\n        :param allow_gce_inference: <iam only> A flag to determine if this role should allow GCE instances to\n            authenticate by inferring service accounts from the GCE identity metadata token.\n        :type allow_gce_inference: bool\n        :param bound_zones: <gce only> The list of zones that a GCE instance must belong to in order to be\n            authenticated. If bound_instance_groups is provided, it is assumed to be a zonal group and the group must\n            belong to this zone.\n        :type bound_zones: list\n        :param bound_regions: <gce only> The list of regions that a GCE instance must belong to in order to be\n            authenticated. If bound_instance_groups is provided, it is assumed to be a regional group and the group\n            must belong to this region. If bound_zones are provided, this attribute is ignored.\n        :type bound_regions: list\n        :param bound_instance_groups: <gce only> The instance groups that an authorized instance must belong to in\n            order to be authenticated. If specified, either bound_zones or bound_regions must be set too.\n        :type bound_instance_groups: list\n        :param bound_labels: <gce only> A list of GCP labels formatted as \"key:value\" strings that must be set on\n            authorized GCE instances. Because GCP labels are not currently ACL'd, we recommend that this be used in\n            conjunction with other restrictions.\n        :type bound_labels: list\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The data key from the JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        type_specific_params = {\n            'iam': {\n                'max_jwt_exp': '15m',\n                'allow_gce_inference': True,\n            },\n            'gce': {\n                'bound_zones': None,\n                'bound_regions': None,\n                'bound_instance_groups': None,\n                'bound_labels': None,\n            },\n        }\n\n        list_of_strings_params = {\n            'policies': policies,\n            'bound_service_accounts': bound_service_accounts,\n            'bound_zones': bound_zones,\n            'bound_regions': bound_regions,\n            'bound_instance_groups': bound_instance_groups,\n            'bound_labels': bound_labels,\n\n        }\n        for param_name, param_argument in list_of_strings_params.items():\n            validate_list_of_strings_param(\n                param_name=param_name,\n                param_argument=param_argument,\n            )\n\n        if role_type not in ALLOWED_ROLE_TYPES:\n            error_msg = 'unsupported role_type argument provided \"{arg}\", supported types: \"{role_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=type,\n                role_types=','.join(ALLOWED_ROLE_TYPES),\n            ))\n\n        params = {\n            'type': role_type,\n            'project_id': project_id,\n            'ttl': ttl,\n            'max_ttl': max_ttl,\n            'period': period,\n            'policies': list_to_comma_delimited(policies),\n            'bound_service_accounts': list_to_comma_delimited(bound_service_accounts),\n        }\n        if role_type == 'iam':\n            params['max_jwt_exp'] = max_jwt_exp\n            params['allow_gce_inference'] = allow_gce_inference\n            for param, default_arg in type_specific_params['gce'].items():\n                if locals().get(param) != default_arg:\n                    warning_msg = 'Argument for parameter \"{param}\" ignored for role type iam'.format(\n                        param=param\n                    )\n                    logger.warning(warning_msg)\n        elif role_type == 'gce':\n            params['bound_zones'] = list_to_comma_delimited(bound_zones)\n            params['bound_regions'] = list_to_comma_delimited(bound_regions)\n            params['bound_instance_groups'] = list_to_comma_delimited(bound_instance_groups)\n            params['bound_labels'] = list_to_comma_delimited(bound_labels)\n            for param, default_arg in type_specific_params['iam'].items():\n                if locals().get(param) != default_arg:\n                    warning_msg = 'Argument for parameter \"{param}\" ignored for role type gce'.format(\n                        param=param\n                    )\n                    logger.warning(warning_msg)\n\n        api_path = '/v1/auth/{mount_point}/role/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef edit_labels_on_gce_role(self, name, add=None, remove=None, mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'add': add,\n            'remove': remove,\n        }\n        api_path = '/v1/auth/{mount_point}/role/{name}/labels'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Edit labels for an existing GCE role."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_role(self, role, mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'role': role,\n        }\n        api_path = '/v1/auth/{mount_point}/role/{role}'.format(\n            mount_point=mount_point,\n            role=role,\n        )\n        return self._adapter.delete(\n            url=api_path,\n            json=params,\n        )", "response": "Delete the previously registered role."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef login(self, role, jwt, use_token=True, mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'role': role,\n            'jwt': jwt,\n        }\n        api_path = '/v1/auth/{mount_point}/login'.format(mount_point=mount_point)\n        response = self._adapter.login(\n            url=api_path,\n            use_token=use_token,\n            json=params,\n        )\n        return response", "response": "Login to retrieve a Vault token via the GCP auth method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconfigure the connection parameters for the user group membership of a user group.", "response": "def configure(self, org_name, api_token=None, base_url='okta.com', ttl=None, max_ttl=None, bypass_okta_mfa=False,\n                  mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Configure the connection parameters for Okta.\n\n        This path honors the distinction between the create and update capabilities inside ACL policies.\n\n        Supported methods:\n            POST: /auth/{mount_point}/config. Produces: 204 (empty body)\n\n\n        :param org_name: Name of the organization to be used in the Okta API.\n        :type org_name: str | unicode\n        :param api_token: Okta API token. This is required to query Okta for user group membership. If this is not\n            supplied only locally configured groups will be enabled.\n        :type api_token: str | unicode\n        :param base_url:  If set, will be used as the base domain for API requests.  Examples are okta.com,\n            oktapreview.com, and okta-emea.com.\n        :type base_url: str | unicode\n        :param ttl: Duration after which authentication will be expired.\n        :type ttl: str | unicode\n        :param max_ttl: Maximum duration after which authentication will be expired.\n        :type max_ttl: str | unicode\n        :param bypass_okta_mfa: Whether to bypass an Okta MFA request. Useful if using one of Vault's built-in MFA\n            mechanisms, but this will also cause certain other statuses to be ignored, such as PASSWORD_EXPIRED.\n        :type bypass_okta_mfa: bool\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'org_name': org_name,\n            'api_token': api_token,\n            'base_url': base_url,\n            'ttl': ttl,\n            'max_ttl': max_ttl,\n            'bypass_okta_mfa': bypass_okta_mfa,\n        }\n        api_path = '/v1/auth/{mount_point}/config'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a new user and maps a set of policies to it.", "response": "def register_user(self, username, groups=None, policies=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Register a new user and maps a set of policies to it.\n\n        Supported methods:\n            POST: /auth/{mount_point}/users/{username}. Produces: 204 (empty body)\n\n        :param username: Name of the user.\n        :type username: str | unicode\n        :param groups: List or comma-separated string of groups associated with the user.\n        :type groups: list\n        :param policies: List or comma-separated string of policies associated with the user.\n        :type policies: list\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'username': username,\n            'groups': groups,\n            'policies': policies,\n        }\n        api_path = '/v1/auth/{mount_point}/users/{username}'.format(\n            mount_point=mount_point,\n            username=username,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the properties of an existing username.", "response": "def read_user(self, username, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Read the properties of an existing username.\n\n        Supported methods:\n            GET: /auth/{mount_point}/users/{username}. Produces: 200 application/json\n\n        :param username: Username for this user.\n        :type username: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        params = {\n            'username': username,\n        }\n        api_path = '/v1/auth/{mount_point}/users/{username}'.format(\n            mount_point=mount_point,\n            username=username,\n        )\n        response = self._adapter.get(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes an existing user from the method.", "response": "def delete_user(self, username, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Delete an existing username from the method.\n\n        Supported methods:\n            DELETE: /auth/{mount_point}/users/{username}. Produces: 204 (empty body)\n\n        :param username: Username for this user.\n        :type username: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'username': username,\n        }\n        api_path = '/v1/auth/{mount_point}/users/{username}'.format(\n            mount_point=mount_point,\n            username=username,\n        )\n        return self._adapter.delete(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister a new group and maps a set of policies to it.", "response": "def register_group(self, name, policies=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Register a new group and maps a set of policies to it.\n\n        Supported methods:\n            POST: /auth/{mount_point}/groups/{name}. Produces: 204 (empty body)\n\n        :param name: The name of the group.\n        :type name: str | unicode\n        :param policies: The list or comma-separated string of policies associated with the group.\n        :type policies: list\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'policies': policies,\n        }\n        api_path = '/v1/auth/{mount_point}/groups/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adapter(self, adapter):\n        self._adapter = adapter\n        for implemented_class in self.implemented_classes:\n            class_name = implemented_class.__name__.lower()\n            getattr(self, self.get_private_attr_name(class_name)).adapter = adapter", "response": "Sets the _adapter property in use by this class and all implemented classes under this category."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef configure(self, organization, base_url='', ttl='', max_ttl='', mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'organization': organization,\n            'base_url': base_url,\n            'ttl': ttl,\n            'max_ttl': max_ttl,\n        }\n        api_path = '/v1/auth/{mount_point}/config'.format(\n            mount_point=mount_point\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Configure the connection parameters for GitHub."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap a list of policies to a GitHub team.", "response": "def map_team(self, team_name, policies=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Map a list of policies to a team that exists in the configured GitHub organization.\n\n        Supported methods:\n            POST: /auth/{mount_point}/map/teams/{team_name}. Produces: 204 (empty body)\n\n\n        :param team_name: GitHub team name in \"slugified\" format\n        :type team_name: str | unicode\n        :param policies: Comma separated list of policies to assign\n        :type policies: List[str]\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the map_github_teams request.\n        :rtype: requests.Response\n        \"\"\"\n        # First, perform parameter validation.\n        if policies is None:\n            policies = []\n        if not isinstance(policies, list) or not all([isinstance(p, str) for p in policies]):\n            error_msg = 'unsupported policies argument provided \"{arg}\" ({arg_type}), required type: List[str]\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=policies,\n                arg_type=type(policies),\n            ))\n        # Then, perform request.\n        params = {\n            'value': ','.join(policies),\n        }\n        api_path = '/v1/auth/{mount_point}/map/teams/{team_name}'.format(\n            mount_point=mount_point,\n            team_name=team_name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreads the GitHub team policy mapping.", "response": "def read_team_mapping(self, team_name, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Read the GitHub team policy mapping.\n\n        Supported methods:\n            GET: /auth/{mount_point}/map/teams/{team_name}. Produces: 200 application/json\n\n\n        :param team_name: GitHub team name\n        :type team_name: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the read_team_mapping request.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/auth/{mount_point}/map/teams/{team_name}'.format(\n            mount_point=mount_point,\n            team_name=team_name,\n        )\n        response = self._adapter.get(url=api_path)\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_user_mapping(self, user_name, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/auth/{mount_point}/map/users/{user_name}'.format(\n            mount_point=mount_point,\n            user_name=user_name,\n        )\n        response = self._adapter.get(url=api_path)\n        return response.json()", "response": "Read the GitHub user policy mapping."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlogin using GitHub access token.", "response": "def login(self, token, use_token=True, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Login using GitHub access token.\n\n        Supported methods:\n            POST: /auth/{mount_point}/login. Produces: 200 application/json\n\n\n        :param token: GitHub personal API token.\n        :type token: str | unicode\n        :param use_token: if True, uses the token in the response received from the auth request to set the \"token\"\n            attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.\n        :type use_token: bool\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the login request.\n        :rtype: dict\n        \"\"\"\n        params = {\n            'token': token,\n        }\n        api_path = '/v1/auth/{mount_point}/login'.format(mount_point=mount_point)\n        return self._adapter.login(\n            url=api_path,\n            use_token=use_token,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_sigv4_auth_request(header_value=None):\n    request = requests.Request(\n        method='POST',\n        url='https://sts.amazonaws.com/',\n        headers={'Content-Type': 'application/x-www-form-urlencoded; charset=utf-8', 'Host': 'sts.amazonaws.com'},\n        data='Action=GetCallerIdentity&Version=2011-06-15',\n    )\n\n    if header_value:\n        request.headers['X-Vault-AWS-IAM-Server-ID'] = header_value\n\n    prepared_request = request.prepare()\n    return prepared_request", "response": "This function generates a request to subsequently generate a Signature Version 4 header."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the lease metadata.", "response": "def read_lease(self, lease_id):\n        \"\"\"Retrieve lease metadata.\n\n        Supported methods:\n            PUT: /sys/leases/lookup. Produces: 200 application/json\n\n        :param lease_id: the ID of the lease to lookup.\n        :type lease_id: str | unicode\n        :return: Parsed JSON response from the leases PUT request\n        :rtype: dict.\n        \"\"\"\n        params = {\n            'lease_id': lease_id\n        }\n        api_path = '/v1/sys/leases/lookup'\n        response = self._adapter.put(\n            url=api_path,\n            json=params\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef list_leases(self, prefix):\n        api_path = '/v1/sys/leases/lookup/{prefix}'.format(prefix=prefix)\n        response = self._adapter.list(\n            url=api_path,\n        )\n        return response.json()", "response": "Returns a list of lease ids."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nrenews a lease, requesting to extend the lease. Supported methods: PUT: /sys/leases/renew. Produces: 200 application/json :param lease_id: The ID of the lease to extend. :type lease_id: str | unicode :param increment: The requested amount of time (in seconds) to extend the lease. :type increment: int :return: The JSON response of the request :rtype: dict", "response": "def renew_lease(self, lease_id, increment=None):\n        \"\"\"Renew a lease, requesting to extend the lease.\n\n        Supported methods:\n            PUT: /sys/leases/renew. Produces: 200 application/json\n\n        :param lease_id: The ID of the lease to extend.\n        :type lease_id: str | unicode\n        :param increment: The requested amount of time (in seconds) to extend the lease.\n        :type increment: int\n        :return: The JSON response of the request\n        :rtype: dict\n        \"\"\"\n        params = {\n            'lease_id': lease_id,\n            'increment': increment,\n        }\n        api_path = '/v1/sys/leases/renew'\n        response = self._adapter.put(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef revoke_lease(self, lease_id):\n        params = {\n            'lease_id': lease_id,\n        }\n        api_path = '/v1/sys/leases/revoke'\n        return self._adapter.put(\n            url=api_path,\n            json=params,\n        )", "response": "Revoke a lease immediately."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrevoke all secrets generated under a given lease ID prefix immediately.", "response": "def revoke_prefix(self, prefix):\n        \"\"\"Revoke all secrets (via a lease ID prefix) or tokens (via the tokens' path property) generated under a given\n        prefix immediately.\n\n        This requires sudo capability and access to it should be tightly controlled as it can be used to revoke very\n        large numbers of secrets/tokens at once.\n\n        Supported methods:\n            PUT: /sys/leases/revoke-prefix/{prefix}. Produces: 204 (empty body)\n\n\n        :param prefix: The prefix to revoke.\n        :type prefix: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'prefix': prefix,\n        }\n        api_path = '/v1/sys/leases/revoke-prefix/{prefix}'.format(prefix=prefix)\n        return self._adapter.put(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate or updates an entity.", "response": "def create_or_update_entity(self, name, entity_id=None, metadata=None, policies=None, disabled=False,\n                                mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Create or update an Entity.\n\n        Supported methods:\n            POST: /{mount_point}/entity. Produces: 200 application/json\n\n        :param entity_id: ID of the entity. If set, updates the corresponding existing entity.\n        :type entity_id: str | unicode\n        :param name: Name of the entity.\n        :type name: str | unicode\n        :param metadata: Metadata to be associated with the entity.\n        :type metadata: dict\n        :param policies: Policies to be tied to the entity.\n        :type policies: str | unicode\n        :param disabled: Whether the entity is disabled. Disabled entities' associated tokens cannot be used, but are\n            not revoked.\n        :type disabled: bool\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response for creates, the generic response object for updates, of the request.\n        :rtype: dict | requests.Response\n        \"\"\"\n        if metadata is None:\n            metadata = {}\n        if not isinstance(metadata, dict):\n            error_msg = 'unsupported metadata argument provided \"{arg}\" ({arg_type}), required type: dict\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=metadata,\n                arg_type=type(metadata),\n            ))\n        params = {\n            'name': name,\n            'metadata': metadata,\n            'policies': policies,\n            'disabled': disabled,\n        }\n        if entity_id is not None:\n            params['id'] = entity_id\n        api_path = '/v1/{mount_point}/entity'.format(mount_point=mount_point)\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        if response.status_code == 204:\n            return response\n        else:\n            return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_entity(self, entity_id, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/entity/id/{id}'.format(\n            mount_point=mount_point,\n            id=entity_id,\n        )\n        response = self._adapter.get(url=api_path)\n        return response.json()", "response": "Query an entity by its identifier."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndeleting an entity and all its associated aliases.", "response": "def delete_entity(self, entity_id, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Delete an entity and all its associated aliases.\n\n        Supported methods:\n            DELETE: /{mount_point}/entity/id/:id. Produces: 204 (empty body)\n\n        :param entity_id: Identifier of the entity.\n        :type entity_id: str\n        :param mount_point: The \"path\" the secret engine was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        api_path = '/v1/{mount_point}/entity/id/{id}'.format(\n            mount_point=mount_point,\n            id=entity_id,\n        )\n        return self._adapter.delete(\n            url=api_path,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef merge_entities(self, from_entity_ids, to_entity_id, force=False, mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'from_entity_ids': from_entity_ids,\n            'to_entity_id': to_entity_id,\n            'force': force,\n        }\n        api_path = '/v1/{mount_point}/entity/merge'.format(mount_point=mount_point)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Merge many entities into one entity."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying the entity alias by its identifier.", "response": "def read_entity_alias(self, alias_id, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Query the entity alias by its identifier.\n\n        Supported methods:\n            GET: /{mount_point}/entity-alias/id/{id}. Produces: 200 application/json\n\n        :param alias_id: Identifier of entity alias.\n        :type alias_id: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/{mount_point}/entity-alias/id/{id}'.format(\n            mount_point=mount_point,\n            id=alias_id,\n        )\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate an existing entity alias.", "response": "def update_entity_alias(self, alias_id, name, canonical_id, mount_accessor, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Update an existing entity alias.\n\n        Supported methods:\n            POST: /{mount_point}/entity-alias/id/{id}. Produces: 200 application/json\n\n        :param alias_id: Identifier of the entity alias.\n        :type alias_id: str | unicode\n        :param name: Name of the alias. Name should be the identifier of the client in the authentication source. For\n            example, if the alias belongs to userpass backend, the name should be a valid username within userpass\n            backend. If alias belongs to GitHub, it should be the GitHub username.\n        :type name: str | unicode\n        :param canonical_id: Entity ID to which this alias belongs to.\n        :type canonical_id: str | unicode\n        :param mount_accessor: Accessor of the mount to which the alias should belong to.\n        :type mount_accessor: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response where available, otherwise the generic response object, of the request.\n        :rtype: dict | requests.Response\n        \"\"\"\n        params = {\n            'name': name,\n            'canonical_id': canonical_id,\n            'mount_accessor': mount_accessor,\n        }\n        api_path = '/v1/{mount_point}/entity-alias/id/{id}'.format(\n            mount_point=mount_point,\n            id=alias_id,\n        )\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        if response.status_code == 204:\n            return response\n        else:\n            return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting available entity aliases.", "response": "def list_entity_aliases(self, method='LIST', mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"List available entity aliases by their identifiers.\n\n        :param method: Supported methods:\n            LIST: /{mount_point}/entity-alias/id. Produces: 200 application/json\n            GET: /{mount_point}/entity-alias/id?list=true. Produces: 200 application/json\n        :type method: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The the JSON response of the request.\n        :rtype: dict\n        \"\"\"\n\n        if method == 'LIST':\n            api_path = '/v1/{mount_point}/entity-alias/id'.format(mount_point=mount_point)\n            response = self._adapter.list(\n                url=api_path,\n            )\n\n        elif method == 'GET':\n            api_path = '/v1/{mount_point}/entity-alias/id?list=true'.format(mount_point=mount_point)\n            response = self._adapter.get(\n                url=api_path,\n            )\n        else:\n            error_message = '\"method\" parameter provided invalid value; LIST or GET allowed, \"{method}\" provided'.format(method=method)\n            raise exceptions.ParamValidationError(error_message)\n\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_entity_alias(self, alias_id, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/entity-alias/id/{id}'.format(\n            mount_point=mount_point,\n            id=alias_id,\n        )\n        return self._adapter.delete(\n            url=api_path,\n        )", "response": "Delete an entity alias."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_member_id_params_for_group_type(group_type, params, member_group_ids, member_entity_ids):\n        if group_type == 'external':\n            if member_entity_ids is not None:\n                logger.warning(\"InvalidRequest: member entities can't be set manually for external groupsl ignoring member_entity_ids argument.\")\n        else:\n            params['member_entity_ids'] = member_entity_ids\n\n        if group_type == 'external':\n            if member_group_ids is not None:\n                logger.warning(\"InvalidRequest: member groups can't be set for external groups; ignoring member_group_ids argument.\")\n        else:\n            params['member_group_ids'] = member_group_ids\n\n        return params", "response": "Validate the parameters for a group create or update request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read_group(self, group_id, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/group/id/{id}'.format(\n            mount_point=mount_point,\n            id=group_id,\n        )\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "Query the group by its identifier."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a group. Supported methods: DELETE: /{mount_point}/group/id/{id}. Produces: 204 (empty body) :param group_id: Identifier of the entity. :type group_id: str | unicode :param mount_point: The \"path\" the method/backend was mounted on. :type mount_point: str | unicode :return: The response of the request. :rtype: requests.Response", "response": "def delete_group(self, group_id, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Delete a group.\n\n        Supported methods:\n            DELETE: /{mount_point}/group/id/{id}. Produces: 204 (empty body)\n\n        :param group_id: Identifier of the entity.\n        :type group_id: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        api_path = '/v1/{mount_point}/group/id/{id}'.format(\n            mount_point=mount_point,\n            id=group_id,\n        )\n        return self._adapter.delete(\n            url=api_path,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_or_update_group_by_name(self, name, group_type=\"internal\", metadata=None, policies=None, member_group_ids=None,\n                                       member_entity_ids=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Create or update a group by its name.\n\n        Supported methods:\n            POST: /{mount_point}/group/name/{name}. Produces: 200 application/json\n\n        :param name: Name of the group.\n        :type name: str | unicode\n        :param group_type: Type of the group, internal or external. Defaults to internal.\n        :type group_type: str | unicode\n        :param metadata: Metadata to be associated with the group.\n        :type metadata: dict\n        :param policies: Policies to be tied to the group.\n        :type policies: str | unicode\n        :param member_group_ids: Group IDs to be assigned as group members.\n        :type member_group_ids: str | unicode\n        :param member_entity_ids: Entity IDs to be assigned as group members.\n        :type member_entity_ids: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n\n        if metadata is None:\n            metadata = {}\n        if not isinstance(metadata, dict):\n            error_msg = 'unsupported metadata argument provided \"{arg}\" ({arg_type}), required type: dict\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=metadata,\n                arg_type=type(metadata),\n            ))\n        if group_type not in ALLOWED_GROUP_TYPES:\n            error_msg = 'unsupported group_type argument provided \"{arg}\", allowed values: ({allowed_values})'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=group_type,\n                allowed_values=ALLOWED_GROUP_TYPES,\n            ))\n        params = {\n            'type': group_type,\n            'metadata': metadata,\n            'policies': policies,\n            'member_group_ids': member_group_ids,\n            'member_entity_ids': member_entity_ids,\n        }\n        api_path = '/v1/{mount_point}/group/name/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response", "response": "Creates or updates a group by its name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating or updates a group alias.", "response": "def create_or_update_group_alias(self, name, alias_id=None, mount_accessor=None, canonical_id=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Creates or update a group alias.\n\n        Supported methods:\n            POST: /{mount_point}/group-alias. Produces: 200 application/json\n\n        :param alias_id: ID of the group alias. If set, updates the corresponding existing group alias.\n        :type alias_id: str | unicode\n        :param name: Name of the group alias.\n        :type name: str | unicode\n        :param mount_accessor: Mount accessor to which this alias belongs to\n        :type mount_accessor: str | unicode\n        :param canonical_id: ID of the group to which this is an alias.\n        :type canonical_id: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'name': name,\n            'mount_accessor': mount_accessor,\n            'canonical_id': canonical_id,\n        }\n        if alias_id is not None:\n            params['id'] = alias_id\n        api_path = '/v1/{mount_point}/group-alias'.format(mount_point=mount_point)\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates an existing group alias.", "response": "def update_group_alias(self, entity_id, name, mount_accessor=\"\", canonical_id=\"\", mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Update an existing group alias.\n\n        Supported methods:\n            POST: /{mount_point}/group-alias/id/{id}. Produces: 200 application/json\n\n        :param entity_id: ID of the group alias.\n        :type entity_id: str | unicode\n        :param name: Name of the group alias.\n        :type name: str | unicode\n        :param mount_accessor: Mount accessor to which this alias belongs\n            toMount accessor to which this alias belongs to.\n        :type mount_accessor: str | unicode\n        :param canonical_id: ID of the group to which this is an alias.\n        :type canonical_id: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'name': name,\n            'mount_accessor': mount_accessor,\n            'canonical_id': canonical_id,\n        }\n        api_path = '/v1/{mount_point}/group-alias/id/{id}'.format(\n            mount_point=mount_point,\n            id=entity_id,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nqueries an entity based on the given criteria.", "response": "def lookup_entity(self, name=None, entity_id=None, alias_id=None, alias_name=None, alias_mount_accessor=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Query an entity based on the given criteria.\n\n        The criteria can be name, id, alias_id, or a combination of alias_name and alias_mount_accessor.\n\n        Supported methods:\n            POST: /{mount_point}/lookup/entity. Produces: 200 application/json\n\n        :param name: Name of the entity.\n        :type name: str | unicode\n        :param entity_id: ID of the entity.\n        :type entity_id: str | unicode\n        :param alias_id: ID of the alias.\n        :type alias_id: str | unicode\n        :param alias_name: Name of the alias. This should be supplied in conjunction with alias_mount_accessor.\n        :type alias_name: str | unicode\n        :param alias_mount_accessor: Accessor of the mount to which the alias belongs to. This should be supplied in conjunction with alias_name.\n        :type alias_mount_accessor: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request if a entity / entity alias is found in the lookup, None otherwise.\n        :rtype: dict | None\n        \"\"\"\n        params = {}\n        if name is not None:\n            params['name'] = name\n        elif entity_id is not None:\n            params['id'] = entity_id\n        elif alias_id is not None:\n            params['alias_id'] = alias_id\n        elif alias_name is not None and alias_mount_accessor is not None:\n            params['alias_name'] = alias_name\n            params['alias_mount_accessor'] = alias_mount_accessor\n        api_path = '/v1/{mount_point}/lookup/entity'.format(mount_point=mount_point)\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        if response.status_code == 204:\n            logger.debug('Identity.lookup_entity: no entities found with params: {params}'.format(params=params))\n            return None\n        else:\n            return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_key(self, name, convergent_encryption=False, derived=False, exportable=False, allow_plaintext_backup=False,\n                   key_type=\"aes256-gcm96\", mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Create a new named encryption key of the specified type.\n\n        The values set here cannot be changed after key creation.\n\n        Supported methods:\n            POST: /{mount_point}/keys/{name}. Produces: 204 (empty body)\n\n        :param name: Specifies the name of the encryption key to create. This is specified as part of the URL.\n        :type name: str | unicode\n        :param convergent_encryption: If enabled, the key will support convergent encryption, where the same plaintext\n            creates the same ciphertext. This requires derived to be set to true. When enabled, each\n            encryption(/decryption/rewrap/datakey) operation will derive a nonce value rather than randomly generate it.\n        :type convergent_encryption: bool\n        :param derived: Specifies if key derivation is to be used. If enabled, all encrypt/decrypt requests to this\n            named key must provide a context which is used for key derivation.\n        :type derived: bool\n        :param exportable: Enables keys to be exportable. This allows for all the valid keys in the key ring to be\n            exported. Once set, this cannot be disabled.\n        :type exportable: bool\n        :param allow_plaintext_backup: If set, enables taking backup of named key in the plaintext format. Once set,\n            this cannot be disabled.\n        :type allow_plaintext_backup: bool\n        :param key_type: Specifies the type of key to create. The currently-supported types are:\n\n            * **aes256-gcm96**: AES-256 wrapped with GCM using a 96-bit nonce size AEAD\n            * **chacha20-poly1305**: ChaCha20-Poly1305 AEAD (symmetric, supports derivation and convergent encryption)\n            * **ed25519**: ED25519 (asymmetric, supports derivation).\n            * **ecdsa-p256**: ECDSA using the P-256 elliptic curve (asymmetric)\n            * **rsa-2048**: RSA with bit size of 2048 (asymmetric)\n            * **rsa-4096**: RSA with bit size of 4096 (asymmetric)\n        :type key_type: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if convergent_encryption and not derived:\n            raise exceptions.ParamValidationError('derived must be set to True when convergent_encryption is True')\n        if key_type not in transit_constants.ALLOWED_KEY_TYPES:\n            error_msg = 'invalid key_type argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=key_type,\n                allowed_types=', '.join(transit_constants.ALLOWED_KEY_TYPES),\n            ))\n        params = {\n            'convergent_encryption': convergent_encryption,\n            'derived': derived,\n            'exportable': exportable,\n            'allow_plaintext_backup': allow_plaintext_backup,\n            'type': key_type,\n        }\n        api_path = '/v1/{mount_point}/keys/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Create a new named encryption key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating the configuration values for a given key.", "response": "def update_key_configuration(self, name, min_decryption_version=0, min_encryption_version=0, deletion_allowed=False,\n                                 exportable=False, allow_plaintext_backup=False, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Tune configuration values for a given key.\n\n        These values are returned during a read operation on the named key.\n\n        Supported methods:\n            POST: /{mount_point}/keys/{name}/config. Produces: 204 (empty body)\n\n        :param name: Specifies the name of the encryption key to update configuration for.\n        :type name: str | unicode\n        :param min_decryption_version: Specifies the minimum version of ciphertext allowed to be decrypted. Adjusting\n            this as part of a key rotation policy can prevent old copies of ciphertext from being decrypted, should they\n            fall into the wrong hands. For signatures, this value controls the minimum version of signature that can be\n            verified against. For HMACs, this controls the minimum version of a key allowed to be used as the key for\n            verification.\n        :type min_decryption_version: int\n        :param min_encryption_version: Specifies the minimum version of the key that can be used to encrypt plaintext,\n            sign payloads, or generate HMACs. Must be 0 (which will use the latest version) or a value greater or equal\n            to min_decryption_version.\n        :type min_encryption_version: int\n        :param deletion_allowed: Specifies if the key is allowed to be deleted.\n        :type deletion_allowed: bool\n        :param exportable: Enables keys to be exportable. This allows for all the valid keys in the key ring to be\n            exported. Once set, this cannot be disabled.\n        :type exportable: bool\n        :param allow_plaintext_backup: If set, enables taking backup of named key in the plaintext format. Once set,\n            this cannot be disabled.\n        :type allow_plaintext_backup: bool\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if min_encryption_version != 0 and min_encryption_version <= min_decryption_version:\n            raise exceptions.ParamValidationError('min_encryption_version must be 0 or > min_decryption_version')\n        params = {\n            'min_decryption_version': min_decryption_version,\n            'min_encryption_version': min_encryption_version,\n            'deletion_allowed': deletion_allowed,\n            'exportable': exportable,\n            'allow_plaintext_backup': allow_plaintext_backup,\n        }\n        api_path = '/v1/{mount_point}/keys/{name}/config'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrotating the version of the named key.", "response": "def rotate_key(self, name, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Rotate the version of the named key.\n\n        After rotation, new plaintext requests will be encrypted with the new version of the key. To upgrade ciphertext\n        to be encrypted with the latest version of the key, use the rewrap endpoint. This is only supported with keys\n        that support encryption and decryption operations.\n\n        Supported methods:\n            POST: /{mount_point}/keys/{name}/rotate. Produces: 204 (empty body)\n\n        :param name: Specifies the name of the key to read information about. This is specified as part of the URL.\n        :type name: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        api_path = '/v1/{mount_point}/keys/{name}/rotate'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the named key. The keys object shows the value of the key for each version. If version is specified, the specific version will be returned. If latest is provided as the version, the current key will be provided. Depending on the type of key, different information may be returned. The key must be exportable to support this operation and the version must still be valid. Supported methods: GET: /{mount_point}/export/{key_type}/{name}(/{version}). Produces: 200 application/json :param name: Specifies the name of the key to read information about. This is specified as part of the URL. :type name: str | unicode :param key_type: Specifies the type of the key to export. This is specified as part of the URL. Valid values are: encryption-key signing-key hmac-key :type key_type: str | unicode :param version: Specifies the version of the key to read. If omitted, all versions of the key will be returned. If the version is set to latest, the current key will be returned. :type version: str | unicode :param mount_point: The \"path\" the method/backend was mounted on. :type mount_point: str | unicode :return: The JSON response of the request. :rtype: requests.Response", "response": "def export_key(self, name, key_type, version=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Return the named key.\n\n        The keys object shows the value of the key for each version. If version is specified, the specific version will\n        be returned. If latest is provided as the version, the current key will be provided. Depending on the type of\n        key, different information may be returned. The key must be exportable to support this operation and the version\n        must still be valid.\n\n        Supported methods:\n            GET: /{mount_point}/export/{key_type}/{name}(/{version}). Produces: 200 application/json\n\n        :param name: Specifies the name of the key to read information about. This is specified as part of the URL.\n        :type name: str | unicode\n        :param key_type: Specifies the type of the key to export. This is specified as part of the URL. Valid values are:\n            encryption-key\n            signing-key\n            hmac-key\n        :type key_type: str | unicode\n        :param version: Specifies the version of the key to read. If omitted, all versions of the key will be returned.\n            If the version is set to latest, the current key will be returned.\n        :type version: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if key_type not in transit_constants.ALLOWED_EXPORT_KEY_TYPES:\n            error_msg = 'invalid key_type argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=key_type,\n                allowed_types=', '.join(transit_constants.ALLOWED_EXPORT_KEY_TYPES),\n            ))\n        api_path = '/v1/{mount_point}/export/{key_type}/{name}'.format(\n            mount_point=mount_point,\n            key_type=key_type,\n            name=name,\n        )\n        if version is not None:\n            api_path = self._adapter.urljoin(api_path, version)\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nencrypts the provided plaintext using the named key.", "response": "def encrypt_data(self, name, plaintext, context=\"\", key_version=0, nonce=None, batch_input=None, type=\"aes256-gcm96\",\n                     convergent_encryption=\"\", mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Encrypt the provided plaintext using the named key.\n\n        This path supports the create and update policy capabilities as follows: if the user has the create capability\n        for this endpoint in their policies, and the key does not exist, it will be upserted with default values\n        (whether the key requires derivation depends on whether the context parameter is empty or not). If the user only\n        has update capability and the key does not exist, an error will be returned.\n\n        Supported methods:\n            POST: /{mount_point}/encrypt/{name}. Produces: 200 application/json\n\n        :param name: Specifies the name of the encryption key to encrypt against. This is specified as part of the URL.\n        :type name: str | unicode\n        :param plaintext: Specifies base64 encoded plaintext to be encoded.\n        :type plaintext: str | unicode\n        :param context: Specifies the base64 encoded context for key derivation. This is required if key derivation is\n            enabled for this key.\n        :type context: str | unicode\n        :param key_version: Specifies the version of the key to use for encryption. If not set, uses the latest version.\n            Must be greater than or equal to the key's min_encryption_version, if set.\n        :type key_version: int\n        :param nonce: Specifies the base64 encoded nonce value. This must be provided if convergent encryption is\n            enabled for this key and the key was generated with Vault 0.6.1. Not required for keys created in 0.6.2+.\n            The value must be exactly 96 bits (12 bytes) long and the user must ensure that for any given context (and\n            thus, any given encryption key) this nonce value is never reused.\n        :type nonce: str | unicode\n        :param batch_input: Specifies a list of items to be encrypted in a single batch. When this parameter is set, if\n            the parameters 'plaintext', 'context' and 'nonce' are also set, they will be ignored. The format for the\n            input is: [dict(context=\"b64_context\", plaintext=\"b64_plaintext\"), ...]\n        :type batch_input: List[dict]\n        :param type: This parameter is required when encryption key is expected to be created. When performing an\n            upsert operation, the type of key to create.\n        :type type: str | unicode\n        :param convergent_encryption: This parameter will only be used when a key is expected to be created. Whether to\n            support convergent encryption. This is only supported when using a key with key derivation enabled and will\n            require all requests to carry both a context and 96-bit (12-byte) nonce. The given nonce will be used in\n            place of a randomly generated nonce. As a result, when the same context and nonce are supplied, the same\n            ciphertext is generated. It is very important when using this mode that you ensure that all nonces are\n            unique for a given context. Failing to do so will severely impact the ciphertext's security.\n        :type convergent_encryption: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'plaintext': plaintext,\n            'context': context,\n            'key_version': key_version,\n            'nonce': nonce,\n            'batch_input': batch_input,\n            'type': type,\n            'convergent_encryption': convergent_encryption,\n        }\n        api_path = '/v1/{mount_point}/encrypt/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decrypt_data(self, name, ciphertext, context=\"\", nonce=\"\", batch_input=None, mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'ciphertext': ciphertext,\n            'context': context,\n            'nonce': nonce,\n            'batch_input': batch_input,\n        }\n        api_path = '/v1/{mount_point}/decrypt/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()", "response": "Decrypt the provided ciphertext using the named key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a new high - entropy key and the value encrypted with the named key.", "response": "def generate_data_key(self, name, key_type, context=\"\", nonce=\"\", bits=256, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Generates a new high-entropy key and the value encrypted with the named key.\n\n        Optionally return the plaintext of the key as well. Whether plaintext is returned depends on the path; as a\n        result, you can use Vault ACL policies to control whether a user is allowed to retrieve the plaintext value of a\n        key. This is useful if you want an untrusted user or operation to generate keys that are then made available to\n        trusted users.\n\n        Supported methods:\n            POST: /{mount_point}/datakey/{key_type}/{name}. Produces: 200 application/json\n\n        :param name: Specifies the name of the encryption key to use to encrypt the datakey. This is specified as part\n            of the URL.\n        :type name: str | unicode\n        :param key_type: Specifies the type of key to generate. If plaintext, the plaintext key will be returned along\n            with the ciphertext. If wrapped, only the ciphertext value will be returned. This is specified as part of\n            the URL.\n        :type key_type: str | unicode\n        :param context: Specifies the key derivation context, provided as a base64-encoded string. This must be provided\n            if derivation is enabled.\n        :type context: str | unicode\n        :param nonce: Specifies a nonce value, provided as base64 encoded. Must be provided if convergent encryption is\n            enabled for this key and the key was generated with Vault 0.6.1. Not required for keys created in 0.6.2+.\n            The value must be exactly 96 bits (12 bytes) long and the user must ensure that for any given context (and\n            thus, any given encryption key) this nonce value is never reused.\n        :type nonce: str | unicode\n        :param bits: Specifies the number of bits in the desired key. Can be 128, 256, or 512.\n        :type bits: int\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if key_type not in transit_constants.ALLOWED_DATA_KEY_TYPES:\n            error_msg = 'invalid key_type argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=key_type,\n                allowed_types=', '.join(transit_constants.ALLOWED_DATA_KEY_TYPES),\n            ))\n        if bits not in transit_constants.ALLOWED_DATA_KEY_BITS:\n            error_msg = 'invalid bits argument provided \"{arg}\", supported values: \"{allowed_values}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=bits,\n                allowed_values=', '.join([str(b) for b in transit_constants.ALLOWED_DATA_KEY_BITS]),\n            ))\n        params = {\n            'context': context,\n            'nonce': nonce,\n            'bits': bits,\n        }\n        api_path = '/v1/{mount_point}/datakey/{key_type}/{name}'.format(\n            mount_point=mount_point,\n            key_type=key_type,\n            name=name,\n        )\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate high - quality random bytes of the specified length.", "response": "def generate_random_bytes(self, n_bytes=32, output_format=\"base64\", mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Return high-quality random bytes of the specified length.\n\n        Supported methods:\n            POST: /{mount_point}/random(/{bytes}). Produces: 200 application/json\n\n        :param n_bytes: Specifies the number of bytes to return. This value can be specified either in the request body,\n            or as a part of the URL.\n        :type n_bytes: int\n        :param output_format: Specifies the output encoding. Valid options are hex or base64.\n        :type output_format: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'bytes': n_bytes,\n            'format': output_format,\n        }\n        api_path = '/v1/{mount_point}/random'.format(mount_point=mount_point)\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the cryptographic hash of given data using the specified algorithm. Supported methods: POST: /{mount_point}/hash(/{algorithm}). Produces: 200 application/json :param hash_input: Specifies the base64 encoded input data. :type hash_input: str | unicode :param algorithm: Specifies the hash algorithm to use. This can also be specified as part of the URL. Currently-supported algorithms are: sha2-224, sha2-256, sha2-384, sha2-512 :type algorithm: str | unicode :param output_format: Specifies the output encoding. This can be either hex or base64. :type output_format: str | unicode :param mount_point: The \"path\" the method/backend was mounted on. :type mount_point: str | unicode :return: The JSON response of the request. :rtype: requests.Response", "response": "def hash_data(self, hash_input, algorithm=\"sha2-256\", output_format=\"hex\", mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Return the cryptographic hash of given data using the specified algorithm.\n\n        Supported methods:\n            POST: /{mount_point}/hash(/{algorithm}). Produces: 200 application/json\n\n        :param hash_input: Specifies the base64 encoded input data.\n        :type hash_input: str | unicode\n        :param algorithm: Specifies the hash algorithm to use. This can also be specified as part of the URL.\n            Currently-supported algorithms are: sha2-224, sha2-256, sha2-384, sha2-512\n        :type algorithm: str | unicode\n        :param output_format: Specifies the output encoding. This can be either hex or base64.\n        :type output_format: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if algorithm not in transit_constants.ALLOWED_HASH_DATA_ALGORITHMS:\n            error_msg = 'invalid algorithm argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=algorithm,\n                allowed_types=', '.join(transit_constants.ALLOWED_HASH_DATA_ALGORITHMS),\n            ))\n        if output_format not in transit_constants.ALLOWED_HASH_DATA_FORMATS:\n            error_msg = 'invalid output_format argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=output_format,\n                allowed_types=', '.join(transit_constants.ALLOWED_HASH_DATA_FORMATS),\n            ))\n        params = {\n            'input': hash_input,\n            'algorithm': algorithm,\n            'format': output_format,\n        }\n        api_path = '/v1/{mount_point}/hash'.format(mount_point=mount_point)\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_hmac(self, name, hash_input, key_version=None, algorithm=\"sha2-256\", mount_point=DEFAULT_MOUNT_POINT):\n        if algorithm not in transit_constants.ALLOWED_HASH_DATA_ALGORITHMS:\n            error_msg = 'invalid algorithm argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=algorithm,\n                allowed_types=', '.join(transit_constants.ALLOWED_HASH_DATA_ALGORITHMS),\n            ))\n        params = {\n            'input': hash_input,\n            'key_version': key_version,\n            'algorithm': algorithm,\n        }\n        api_path = '/v1/{mount_point}/hmac/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        resposne = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return resposne.json()", "response": "Generate a HMAC of given data using the specified key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsigning the given data using the named key and returns the cryptographic signature of the input data.", "response": "def sign_data(self, name, hash_input, key_version=None, hash_algorithm=\"sha2-256\", context=\"\", prehashed=False,\n                  signature_algorithm=\"pss\", mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Return the cryptographic signature of the given data using the named key and the specified hash algorithm.\n\n        The key must be of a type that supports signing.\n\n        Supported methods:\n            POST: /{mount_point}/sign/{name}(/{hash_algorithm}). Produces: 200 application/json\n\n        :param name: Specifies the name of the encryption key to use for signing. This is specified as part of the URL.\n        :type name: str | unicode\n        :param hash_input: Specifies the base64 encoded input data.\n        :type hash_input: str | unicode\n        :param key_version: Specifies the version of the key to use for signing. If not set, uses the latest version.\n            Must be greater than or equal to the key's min_encryption_version, if set.\n        :type key_version: int\n        :param hash_algorithm: Specifies the hash algorithm to use for supporting key types (notably, not including\n            ed25519 which specifies its own hash algorithm). This can also be specified as part of the URL.\n            Currently-supported algorithms are: sha2-224, sha2-256, sha2-384, sha2-512\n        :type hash_algorithm: str | unicode\n        :param context: Base64 encoded context for key derivation. Required if key derivation is enabled; currently only\n            available with ed25519 keys.\n        :type context: str | unicode\n        :param prehashed: Set to true when the input is already hashed. If the key type is rsa-2048 or rsa-4096, then\n            the algorithm used to hash the input should be indicated by the hash_algorithm parameter. Just as the value\n            to sign should be the base64-encoded representation of the exact binary data you want signed, when set, input\n            is expected to be base64-encoded binary hashed data, not hex-formatted. (As an example, on the command line,\n            you could generate a suitable input via openssl dgst -sha256 -binary | base64.)\n        :type prehashed: bool\n        :param signature_algorithm: When using a RSA key, specifies the RSA signature algorithm to use for signing.\n            Supported signature types are: pss, pkcs1v15\n        :type signature_algorithm: str | unicode\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if hash_algorithm not in transit_constants.ALLOWED_HASH_DATA_ALGORITHMS:\n            error_msg = 'invalid hash_algorithm argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=hash_algorithm,\n                allowed_types=', '.join(transit_constants.ALLOWED_HASH_DATA_ALGORITHMS),\n            ))\n        if signature_algorithm not in transit_constants.ALLOWED_SIGNATURE_ALGORITHMS:\n            error_msg = 'invalid signature_algorithm argument provided \"{arg}\", supported types: \"{allowed_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=signature_algorithm,\n                allowed_types=', '.join(transit_constants.ALLOWED_SIGNATURE_ALGORITHMS),\n            ))\n        params = {\n            'input': hash_input,\n            'key_version': key_version,\n            'hash_algorithm': hash_algorithm,\n            'context': context,\n            'prehashed': prehashed,\n            'signature_algorithm': signature_algorithm,\n        }\n        api_path = '/v1/{mount_point}/sign/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        response = self._adapter.post(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef backup_key(self, name, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/{mount_point}/backup/{name}'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "Return a plaintext backup of a named key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrestore the backup as a named key.", "response": "def restore_key(self, backup, name=None, force=False, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Restore the backup as a named key.\n\n        This will restore the key configurations and all the versions of the named key along with HMAC keys. The input\n        to this endpoint should be the output of /backup endpoint. For safety, by default the backend will refuse to\n        restore to an existing key. If you want to reuse a key name, it is recommended you delete the key before\n        restoring. It is a good idea to attempt restoring to a different key name first to verify that the operation\n        successfully completes.\n\n        Supported methods:\n            POST: /{mount_point}/restore(/name). Produces: 204 (empty body)\n\n        :param backup: Backed up key data to be restored. This should be the output from the /backup endpoint.\n        :type backup: str | unicode\n        :param name: If set, this will be the name of the restored key.\n        :type name: str | unicode\n        :param force: If set, force the restore to proceed even if a key by this name already exists.\n        :type force: bool\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'backup': backup,\n            'force': force,\n        }\n        api_path = '/v1/{mount_point}/restore'.format(mount_point=mount_point)\n        if name is not None:\n            api_path = self._adapter.urljoin(api_path, name)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef trim_key(self, name, min_version, mount_point=DEFAULT_MOUNT_POINT):\n        params = {\n            'min_available_version': min_version,\n        }\n        api_path = '/v1/{mount_point}/keys/{name}/trim'.format(\n            mount_point=mount_point,\n            name=name,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Trims older key versions setting a minimum version for the keyring."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef raise_for_error(status_code, message=None, errors=None):\n    if status_code == 400:\n        raise exceptions.InvalidRequest(message, errors=errors)\n    elif status_code == 401:\n        raise exceptions.Unauthorized(message, errors=errors)\n    elif status_code == 403:\n        raise exceptions.Forbidden(message, errors=errors)\n    elif status_code == 404:\n        raise exceptions.InvalidPath(message, errors=errors)\n    elif status_code == 429:\n        raise exceptions.RateLimitExceeded(message, errors=errors)\n    elif status_code == 500:\n        raise exceptions.InternalServerError(message, errors=errors)\n    elif status_code == 501:\n        raise exceptions.VaultNotInitialized(message, errors=errors)\n    elif status_code == 503:\n        raise exceptions.VaultDown(message, errors=errors)\n    else:\n        raise exceptions.UnexpectedError(message)", "response": "Helper method to raise exceptions based on the status code received from Vault."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_method_deprecation_message(to_be_removed_in_version, old_method_name, method_name=None, module_name=None):\n    message = \"Call to deprecated function '{old_method_name}'. This method will be removed in version '{version}'\".format(\n        old_method_name=old_method_name,\n        version=to_be_removed_in_version,\n    )\n    if method_name is not None and module_name is not None:\n        message += \" Please use the '{method_name}' method on the '{module_name}' class moving forward.\".format(\n            method_name=method_name,\n            module_name=module_name,\n        )\n    return message", "response": "Generate a deprecation warning message for the indicated method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_property_deprecation_message(to_be_removed_in_version, old_name, new_name, new_attribute,\n                                          module_name='Client'):\n    \"\"\"Generate a message to be used when warning about the use of deprecated properties.\n\n    :param to_be_removed_in_version: Version of this module the deprecated property will be removed in.\n    :type to_be_removed_in_version: str\n    :param old_name: Deprecated property name.\n    :type old_name: str\n    :param new_name: Name of the new property name to use.\n    :type new_name: str\n    :param new_attribute: The new attribute where the new property can be found.\n    :type new_attribute: str\n    :param module_name: Name of the module containing the new method to use.\n    :type module_name: str\n    :return: Full deprecation warning message for the indicated property.\n    :rtype: str\n    \"\"\"\n    message = \"Call to deprecated property '{name}'. This property will be removed in version '{version}'\".format(\n        name=old_name,\n        version=to_be_removed_in_version,\n    )\n    message += \" Please use the '{new_name}' property on the '{module_name}.{new_attribute}' attribute moving forward.\".format(\n        new_name=new_name,\n        module_name=module_name,\n        new_attribute=new_attribute,\n    )\n    return message", "response": "Generate a deprecation warning message for a property."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getattr_with_deprecated_properties(obj, item, deprecated_properties):\n    if item in deprecated_properties:\n        deprecation_message = generate_property_deprecation_message(\n            to_be_removed_in_version=deprecated_properties[item]['to_be_removed_in_version'],\n            old_name=item,\n            new_name=deprecated_properties[item].get('new_property', item),\n            new_attribute=deprecated_properties[item]['client_property'],\n        )\n        warnings.simplefilter('always', DeprecationWarning)\n        warnings.warn(\n            message=deprecation_message,\n            category=DeprecationWarning,\n            stacklevel=2,\n        )\n        warnings.simplefilter('default', DeprecationWarning)\n        client_property = getattr(obj, deprecated_properties[item]['client_property'])\n        return getattr(client_property, deprecated_properties[item].get('new_property', item))\n\n    raise AttributeError(\"'{class_name}' has no attribute '{item}'\".format(\n        class_name=obj.__class__.__name__,\n        item=item,\n    ))", "response": "Helper method to use in the getattr method of a class with deprecated properties."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_list_of_strings_param(param_name, param_argument):\n    if param_argument is None:\n        param_argument = []\n    if isinstance(param_argument, str):\n        param_argument = param_argument.split(',')\n    if not isinstance(param_argument, list) or not all([isinstance(p, str) for p in param_argument]):\n        error_msg = 'unsupported {param} argument provided \"{arg}\" ({arg_type}), required type: List[str]'\n        raise exceptions.ParamValidationError(error_msg.format(\n            param=param_name,\n            arg=param_argument,\n            arg_type=type(param_argument),\n        ))", "response": "Validate that an argument is a list of strings."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntries to get the token from env var VAULT_TOKEN if not set attempt to get the token from ~. vault - token", "response": "def get_token_from_env():\n    \"\"\"Get the token from env var, VAULT_TOKEN. If not set, attempt to get the token from, ~/.vault-token\n\n    :return: The vault token if set, else None\n    :rtype: str | None\n    \"\"\"\n    token = os.getenv('VAULT_TOKEN')\n    if not token:\n        token_file_path = os.path.expanduser('~/.vault-token')\n        if os.path.exists(token_file_path):\n            with open(token_file_path, 'r') as f_in:\n                token = f_in.read().strip()\n\n    if not token:\n        return None\n\n    return token"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef comma_delimited_to_list(list_param):\n    if isinstance(list_param, list):\n        return list_param\n    if isinstance(list_param, str):\n        return list_param.split(',')\n    else:\n        return []", "response": "Convert comma - delimited list into a list of strings\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nvalidating that an argument is a PEM - formatted public key or certificate.", "response": "def validate_pem_format(param_name, param_argument):\n    \"\"\"Validate that an argument is a PEM-formatted public key or certificate\n\n    :param param_name: The name of the parameter being validate. Used in any resulting exception messages.\n    :type param_name: str | unicode\n    :param param_argument: The argument to validate\n    :type param_argument: str | unicode\n    :return: True if the argument is validate False otherwise\n    :rtype: bool\n    \"\"\"\n\n    def _check_pem(arg):\n        arg = arg.strip()\n        if not arg.startswith('-----BEGIN CERTIFICATE-----') \\\n                or not arg.endswith('-----END CERTIFICATE-----'):\n            return False\n        return True\n\n    if isinstance(param_argument, str):\n        param_argument = [param_argument]\n\n    if not isinstance(param_argument, list) or not all(_check_pem(p) for p in param_argument):\n        error_msg = 'unsupported {param} public key / certificate format, required type: PEM'\n        raise exceptions.ParamValidationError(error_msg.format(param=param_name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_auth_methods(self):\n        api_path = '/v1/sys/auth'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "List all enabled auth methods."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef enable_auth_method(self, method_type, description=None, config=None, plugin_name=None, local=False, path=None):\n        if path is None:\n            path = method_type\n\n        params = {\n            'type': method_type,\n            'description': description,\n            'config': config,\n            'plugin_name': plugin_name,\n            'local': local,\n        }\n        api_path = '/v1/sys/auth/{path}'.format(path=path)\n        return self._adapter.post(\n            url=api_path,\n            json=params\n        )", "response": "Enable a new auth method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef disable_auth_method(self, path):\n        api_path = '/v1/sys/auth/{path}'.format(path=path)\n        return self._adapter.delete(\n            url=api_path,\n        )", "response": "Disable the auth method at the given auth path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_auth_method_tuning(self, path):\n        api_path = '/v1/sys/auth/{path}/tune'.format(\n            path=path,\n        )\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "Read the given auth path s configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntuning configuration parameters for a given auth path.", "response": "def tune_auth_method(self, path, default_lease_ttl=None, max_lease_ttl=None, description=None,\n                         audit_non_hmac_request_keys=None, audit_non_hmac_response_keys=None, listing_visibility='',\n                         passthrough_request_headers=None):\n        \"\"\"Tune configuration parameters for a given auth path.\n\n        This endpoint requires sudo capability on the final path, but the same functionality can be achieved without\n        sudo via sys/mounts/auth/[auth-path]/tune.\n\n        Supported methods:\n            POST: /sys/auth/{path}/tune. Produces: 204 (empty body)\n\n        :param path: The path the method was mounted on. If not provided, defaults to the value of the \"method_type\"\n            argument.\n        :type path: str | unicode\n        :param default_lease_ttl: Specifies the default time-to-live. If set on a specific auth path, this overrides the\n            global default.\n        :type default_lease_ttl: int\n        :param max_lease_ttl: The maximum time-to-live. If set on a specific auth path, this overrides the global\n            default.\n        :type max_lease_ttl: int\n        :param description: Specifies the description of the mount. This overrides the current stored value, if any.\n        :type description: str | unicode\n        :param audit_non_hmac_request_keys: Specifies the list of keys that will not be HMAC'd by audit devices in the\n            request data object.\n        :type audit_non_hmac_request_keys: array\n        :param audit_non_hmac_response_keys: Specifies the list of keys that will not be HMAC'd by audit devices in the\n            response data object.\n        :type audit_non_hmac_response_keys: list\n        :param listing_visibility: Specifies whether to show this mount in the UI-specific listing endpoint. Valid\n            values are \"unauth\" or \"\".\n        :type listing_visibility: list\n        :param passthrough_request_headers: List of headers to whitelist and pass from the request to the backend.\n        :type passthrough_request_headers: list\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n\n        if listing_visibility not in ['unauth', '']:\n            error_msg = 'invalid listing_visibility argument provided: \"{arg}\"; valid values: \"unauth\" or \"\"'.format(\n                arg=listing_visibility,\n            )\n            raise exceptions.ParamValidationError(error_msg)\n\n        # All parameters are optional for this method. Until/unless we include input validation, we simply loop over the\n        # parameters and add which parameters are set.\n        optional_parameters = {\n            'default_lease_ttl': dict(),\n            'max_lease_ttl': dict(),\n            'description': dict(),\n            'audit_non_hmac_request_keys': dict(comma_delimited_list=True),\n            'audit_non_hmac_response_keys': dict(comma_delimited_list=True),\n            'listing_visibility': dict(),\n            'passthrough_request_headers': dict(comma_delimited_list=True),\n        }\n        params = {}\n        for optional_parameter, parameter_specification in optional_parameters.items():\n            if locals().get(optional_parameter) is not None:\n                if parameter_specification.get('comma_delimited_list'):\n                    argument = locals().get(optional_parameter)\n                    validate_list_of_strings_param(optional_parameter, argument)\n                    params[optional_parameter] = list_to_comma_delimited(argument)\n                else:\n                    params[optional_parameter] = locals().get(optional_parameter)\n\n        api_path = '/v1/sys/auth/{path}/tune'.format(path=path)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads the seal status of the Vault.", "response": "def read_seal_status(self):\n        \"\"\"Read the seal status of the Vault.\n\n        This is an unauthenticated endpoint.\n\n        Supported methods:\n            GET: /sys/seal-status. Produces: 200 application/json\n\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/sys/seal-status'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef submit_unseal_key(self, key=None, reset=False, migrate=False):\n\n        params = {\n            'migrate': migrate,\n        }\n        if not reset and key is not None:\n            params['key'] = key\n        elif reset:\n            params['reset'] = reset\n\n        api_path = '/v1/sys/unseal'\n        response = self._adapter.put(\n            url=api_path,\n            json=params,\n        )\n        return response.json()", "response": "Submit a single master key share to progress the unsealing of the Vault."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubmitting multiple master key shares to the Vault unsealing the master key share.", "response": "def submit_unseal_keys(self, keys, migrate=False):\n        \"\"\"Enter multiple master key share to progress the unsealing of the Vault.\n\n        :param keys: List of master key shares.\n        :type keys: List[str]\n        :param migrate: Available in 1.0 Beta - Used to migrate the seal from shamir to autoseal or autoseal to shamir.\n            Must be provided on all unseal key calls.\n        :type: migrate: bool\n        :return: The JSON response of the last unseal request.\n        :rtype: dict\n        \"\"\"\n        result = None\n\n        for key in keys:\n            result = self.submit_unseal_key(\n                key=key,\n                migrate=migrate,\n            )\n            if not result['sealed']:\n                break\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a login request.", "response": "def login(self, url, use_token=True, **kwargs):\n        \"\"\"Perform a login request.\n\n        Associated request is typically to a path prefixed with \"/v1/auth\") and optionally stores the client token sent\n            in the resulting Vault response for use by the :py:meth:`hvac.adapters.Adapter` instance under the _adapater\n            Client attribute.\n\n        :param url: Path to send the authentication request to.\n        :type url: str | unicode\n        :param use_token: if True, uses the token in the response received from the auth request to set the \"token\"\n            attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.\n        :type use_token: bool\n        :param kwargs: Additional keyword arguments to include in the params sent with the request.\n        :type kwargs: dict\n        :return: The response of the auth request.\n        :rtype: requests.Response\n        \"\"\"\n        response = self.post(url, **kwargs).json()\n\n        if use_token:\n            self.token = response['auth']['client_token']\n\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading the configuration and process of the current root generation attempt.", "response": "def read_root_generation_progress(self):\n        \"\"\"Read the configuration and process of the current root generation attempt.\n\n        Supported methods:\n            GET: /sys/generate-root/attempt. Produces: 200 application/json\n\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/sys/generate-root/attempt'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_root_token_generation(self, otp=None, pgp_key=None):\n        params = {}\n        if otp is not None and pgp_key is not None:\n            raise ParamValidationError('one (and only one) of otp or pgp_key arguments are required')\n        if otp is not None:\n            params['otp'] = otp\n        if pgp_key is not None:\n            params['pgp_key'] = pgp_key\n\n        api_path = '/v1/sys/generate-root/attempt'\n        response = self._adapter.put(\n            url=api_path,\n            json=params\n        )\n        return response.json()", "response": "Initialize a new root token generation attempt."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate_root(self, key, nonce):\n        params = {\n            'key': key,\n            'nonce': nonce,\n        }\n        api_path = '/v1/sys/generate-root/update'\n        response = self._adapter.put(\n            url=api_path,\n            json=params,\n        )\n        return response.json()", "response": "Enter a single master key share to progress the root generation attempt."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cancel_root_generation(self):\n        api_path = '/v1/sys/generate-root/attempt'\n        response = self._adapter.delete(\n            url=api_path,\n        )\n        return response", "response": "Cancel any in - progress root generation attempt."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread information about the current encryption key used by Vault.", "response": "def get_encryption_key_status(self):\n        \"\"\"Read information about the current encryption key used by Vault.\n\n        Supported methods:\n            GET: /sys/key-status. Produces: 200 application/json\n\n        :return: JSON response with information regarding the current encryption key used by Vault.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/sys/key-status'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef rotate_encryption_key(self):\n        api_path = '/v1/sys/rotate'\n        response = self._adapter.put(\n            url=api_path,\n        )\n        return response", "response": "Trigger a rotation of the backend encryption key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninitializing a new recovery key attempt.", "response": "def start_rekey(self, secret_shares=5, secret_threshold=3, pgp_keys=None, backup=False, require_verification=False, recovery_key=False):\n        \"\"\"Initializes a new rekey attempt.\n\n        Only a single recovery key rekeyattempt can take place at a time, and changing the parameters of a rekey\n        requires canceling and starting a new rekey, which will also provide a new nonce.\n\n        Supported methods:\n            PUT: /sys/rekey/init. Produces: 204 (empty body)\n            PUT: /sys/rekey-recovery-key/init. Produces: 204 (empty body)\n\n        :param secret_shares: Specifies the number of shares to split the master key into.\n        :type secret_shares: int\n        :param secret_threshold: Specifies the number of shares required to reconstruct the master key. This must be\n            less than or equal to secret_shares.\n        :type secret_threshold: int\n        :param pgp_keys: Specifies an array of PGP public keys used to encrypt the output unseal keys. Ordering is\n            preserved. The keys must be base64-encoded from their original binary representation. The size of this array\n            must be the same as secret_shares.\n        :type pgp_keys: list\n        :param backup: Specifies if using PGP-encrypted keys, whether Vault should also store a plaintext backup of the\n            PGP-encrypted keys at core/unseal-keys-backup in the physical storage backend. These can then be retrieved\n            and removed via the sys/rekey/backup endpoint.\n        :type backup: bool\n        :param require_verification: This turns on verification functionality. When verification is turned on, after\n            successful authorization with the current unseal keys, the new unseal keys are returned but the master key\n            is not actually rotated. The new keys must be provided to authorize the actual rotation of the master key.\n            This ensures that the new keys have been successfully saved and protects against a risk of the keys being\n            lost after rotation but before they can be persisted. This can be used with without pgp_keys, and when used\n            with it, it allows ensuring that the returned keys can be successfully decrypted before committing to the\n            new shares, which the backup functionality does not provide.\n        :param recovery_key: If true, send requests to \"rekey-recovery-key\" instead of \"rekey\" api path.\n        :type recovery_key: bool\n        :type require_verification: bool\n        :return: The JSON dict of the response.\n        :rtype: dict | request.Response\n        \"\"\"\n        params = {\n            'secret_shares': secret_shares,\n            'secret_threshold': secret_threshold,\n            'require_verification': require_verification,\n        }\n\n        if pgp_keys:\n            if len(pgp_keys) != secret_shares:\n                raise ParamValidationError('length of pgp_keys argument must equal secret shares value')\n\n            params['pgp_keys'] = pgp_keys\n            params['backup'] = backup\n\n        api_path = '/v1/sys/rekey/init'\n        if recovery_key:\n            api_path = '/v1/sys/rekey-recovery-key/init'\n        response = self._adapter.put(\n            url=api_path,\n            json=params,\n        )\n\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncancel any in - progress rekey.", "response": "def cancel_rekey(self, recovery_key=False):\n        \"\"\"Cancel any in-progress rekey.\n\n        This clears the rekey settings as well as any progress made. This must be called to change the parameters of the\n        rekey.\n\n        Note: Verification is still a part of a rekey. If rekeying is canceled during the verification flow, the current\n        unseal keys remain valid.\n\n        Supported methods:\n            DELETE: /sys/rekey/init. Produces: 204 (empty body)\n            DELETE: /sys/rekey-recovery-key/init. Produces: 204 (empty body)\n\n        :param recovery_key: If true, send requests to \"rekey-recovery-key\" instead of \"rekey\" api path.\n        :type recovery_key: bool\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        api_path = '/v1/sys/rekey/init'\n        if recovery_key:\n            api_path = '/v1/sys/rekey-recovery-key/init'\n        response = self._adapter.delete(\n            url=api_path,\n        )\n        return response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenter a single recovery key share to progress the rekey of the Vault.", "response": "def rekey(self, key, nonce=None, recovery_key=False):\n        \"\"\"Enter a single recovery key share to progress the rekey of the Vault.\n\n        If the threshold number of recovery key shares is reached, Vault will complete the rekey. Otherwise, this API\n        must be called multiple times until that threshold is met. The rekey nonce operation must be provided with each\n        call.\n\n        Supported methods:\n            PUT: /sys/rekey/update. Produces: 200 application/json\n            PUT: /sys/rekey-recovery-key/update. Produces: 200 application/json\n\n        :param key: Specifies a single recovery share key.\n        :type key: str | unicode\n        :param nonce: Specifies the nonce of the rekey operation.\n        :type nonce: str | unicode\n        :param recovery_key: If true, send requests to \"rekey-recovery-key\" instead of \"rekey\" api path.\n        :type recovery_key: bool\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        params = {\n            'key': key,\n        }\n\n        if nonce is not None:\n            params['nonce'] = nonce\n\n        api_path = '/v1/sys/rekey/update'\n        if recovery_key:\n            api_path = '/v1/sys/rekey-recovery-key/update'\n        response = self._adapter.put(\n            url=api_path,\n            json=params,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rekey_multi(self, keys, nonce=None, recovery_key=False):\n        result = None\n\n        for key in keys:\n            result = self.rekey(\n                key=key,\n                nonce=nonce,\n                recovery_key=recovery_key,\n            )\n            if result.get('complete'):\n                break\n\n        return result", "response": "Enter multiple recovery key shares to progress the rekey of the Vault."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_backup_keys(self, recovery_key=False):\n        api_path = '/v1/sys/rekey/backup'\n        if recovery_key:\n            api_path = '/v1/sys/rekey-recovery-key/backup'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "Retrieve the backup copy of PGP - encrypted unseal keys."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nenable a new secrets engine at the given path.", "response": "def enable_secrets_engine(self, backend_type, path=None, description=None, config=None, plugin_name=None,\n                              options=None, local=False, seal_wrap=False):\n        \"\"\"Enable a new secrets engine at the given path.\n\n        Supported methods:\n            POST: /sys/mounts/{path}. Produces: 204 (empty body)\n\n        :param backend_type: The name of the backend type, such as \"github\" or \"token\".\n        :type backend_type: str | unicode\n        :param path: The path to mount the method on. If not provided, defaults to the value of the \"method_type\"\n            argument.\n        :type path: str | unicode\n        :param description: A human-friendly description of the mount.\n        :type description: str | unicode\n        :param config: Configuration options for this mount. These are the possible values:\n\n            * **default_lease_ttl**: The default lease duration, specified as a string duration like \"5s\" or \"30m\".\n            * **max_lease_ttl**: The maximum lease duration, specified as a string duration like \"5s\" or \"30m\".\n            * **force_no_cache**: Disable caching.\n            * **plugin_name**: The name of the plugin in the plugin catalog to use.\n            * **audit_non_hmac_request_keys**: Comma-separated list of keys that will not be HMAC'd by audit devices in\n              the request data object.\n            * **audit_non_hmac_response_keys**: Comma-separated list of keys that will not be HMAC'd by audit devices in\n              the response data object.\n            * **listing_visibility**: Specifies whether to show this mount in the UI-specific listing endpoint. (\"unauth\" or \"hidden\")\n            * **passthrough_request_headers**: Comma-separated list of headers to whitelist and pass from the request to\n              the backend.\n        :type config: dict\n        :param options: Specifies mount type specific options that are passed to the backend.\n\n            * **version**: <KV> The version of the KV to mount. Set to \"2\" for mount KV v2.\n        :type options: dict\n        :param plugin_name: Specifies the name of the plugin to use based from the name in the plugin catalog. Applies only to plugin backends.\n        :type plugin_name: str | unicode\n        :param local: <Vault enterprise only> Specifies if the auth method is a local only. Local auth methods are not\n            replicated nor (if a secondary) removed by replication.\n        :type local: bool\n        :param seal_wrap: <Vault enterprise only> Enable seal wrapping for the mount.\n        :type seal_wrap: bool\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if path is None:\n            path = backend_type\n\n        params = {\n            'type': backend_type,\n            'description': description,\n            'config': config,\n            'options': options,\n            'plugin_name': plugin_name,\n            'local': local,\n            'seal_wrap': seal_wrap,\n        }\n\n        api_path = '/v1/sys/mounts/{path}'.format(path=path)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef disable_secrets_engine(self, path):\n        api_path = '/v1/sys/mounts/{path}'.format(path=path)\n        return self._adapter.delete(\n            url=api_path,\n        )", "response": "Disable the secrets engine at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntune the configuration parameters for a given mount point.", "response": "def tune_mount_configuration(self, path, default_lease_ttl=None, max_lease_ttl=None, description=None,\n                                 audit_non_hmac_request_keys=None, audit_non_hmac_response_keys=None,\n                                 listing_visibility=None, passthrough_request_headers=None, options=None):\n        \"\"\"Tune configuration parameters for a given mount point.\n\n        Supported methods:\n            POST: /sys/mounts/{path}/tune. Produces: 204 (empty body)\n\n        :param path: Specifies the path where the secrets engine will be mounted. This is specified as part of the URL.\n        :type path: str | unicode\n        :param mount_point: The path the associated secret backend is mounted\n        :type mount_point: str\n        :param description: Specifies the description of the mount. This overrides the current stored value, if any.\n        :type description: str\n        :param default_lease_ttl: Default time-to-live. This overrides the global default. A value of 0 is equivalent to\n            the system default TTL\n        :type default_lease_ttl: int\n        :param max_lease_ttl: Maximum time-to-live. This overrides the global default. A value of 0 are equivalent and\n            set to the system max TTL.\n        :type max_lease_ttl: int\n        :param audit_non_hmac_request_keys: Specifies the comma-separated list of keys that will not be HMAC'd by audit\n            devices in the request data object.\n        :type audit_non_hmac_request_keys: list\n        :param audit_non_hmac_response_keys: Specifies the comma-separated list of keys that will not be HMAC'd by audit\n            devices in the response data object.\n        :type audit_non_hmac_response_keys: list\n        :param listing_visibility: Speficies whether to show this mount in the UI-specific listing endpoint. Valid\n            values are \"unauth\" or \"\".\n        :type listing_visibility: str\n        :param passthrough_request_headers: Comma-separated list of headers to whitelist and pass from the request\n            to the backend.\n        :type passthrough_request_headers: str\n        :param options: Specifies mount type specific options that are passed to the backend.\n\n            * **version**: <KV> The version of the KV to mount. Set to \"2\" for mount KV v2.\n        :type options: dict\n        :return: The response from the request.\n        :rtype: request.Response\n        \"\"\"\n        # All parameters are optional for this method. Until/unless we include input validation, we simply loop over the\n        # parameters and add which parameters are set.\n        optional_parameters = [\n            'default_lease_ttl',\n            'max_lease_ttl',\n            'description',\n            'audit_non_hmac_request_keys',\n            'audit_non_hmac_response_keys',\n            'listing_visibility',\n            'passthrough_request_headers',\n            'options',\n        ]\n        params = {}\n        for optional_parameter in optional_parameters:\n            if locals().get(optional_parameter) is not None:\n                params[optional_parameter] = locals().get(optional_parameter)\n\n        api_path = '/v1/sys/mounts/{path}/tune'.format(path=path)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmoves an already - mounted backend to a new mount point.", "response": "def move_backend(self, from_path, to_path):\n        \"\"\"Move an already-mounted backend to a new mount point.\n\n        Supported methods:\n            POST: /sys/remount. Produces: 204 (empty body)\n\n        :param from_path: Specifies the previous mount point.\n        :type from_path: str | unicode\n        :param to_path: Specifies the new destination mount point.\n        :type to_path: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'from': from_path,\n            'to': to_path,\n        }\n        api_path = '/v1/sys/remount'\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconfigures the connection parameters for the given Kubernetes service account.", "response": "def configure(self, kubernetes_host, kubernetes_ca_cert='', token_reviewer_jwt='', pem_keys=None,\n                  mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Configure the connection parameters for Kubernetes.\n\n        This path honors the distinction between the create and update capabilities inside ACL policies.\n\n        Supported methods:\n            POST: /auth/{mount_point}/config. Produces: 204 (empty body)\n\n        :param kubernetes_host: Host must be a host string, a host:port pair, or a URL to the base of the\n            Kubernetes API server. Example: https://k8s.example.com:443\n        :type kubernetes_host: str | unicode\n        :param kubernetes_ca_cert: PEM encoded CA cert for use by the TLS client used to talk with the Kubernetes API.\n            NOTE: Every line must end with a newline: \\n\n        :type kubernetes_ca_cert: str | unicode\n        :param token_reviewer_jwt: A service account JWT used to access the TokenReview API to validate other\n            JWTs during login. If not set the JWT used for login will be used to access the API.\n        :type token_reviewer_jwt: str | unicode\n        :param pem_keys: Optional list of PEM-formatted public keys or certificates used to verify the signatures of\n            Kubernetes service account JWTs. If a certificate is given, its public key will be extracted. Not every\n            installation of Kubernetes exposes these keys.\n        :type pem_keys: list\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the configure_method request.\n        :rtype: requests.Response\n        \"\"\"\n        if pem_keys is None:\n            pem_keys = []\n\n        list_of_pem_params = {\n            'kubernetes_ca_cert': kubernetes_ca_cert,\n            'pem_keys': pem_keys\n        }\n        for param_name, param_argument in list_of_pem_params.items():\n            validate_pem_format(\n                param_name=param_name,\n                param_argument=param_argument,\n            )\n\n        params = {\n            'kubernetes_host': kubernetes_host,\n            'kubernetes_ca_cert': kubernetes_ca_cert,\n            'token_reviewer_jwt': token_reviewer_jwt,\n            'pem_keys': pem_keys,\n        }\n        api_path = '/v1/auth/{mount_point}/config'.format(\n            mount_point=mount_point\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_role(self, name, bound_service_account_names, bound_service_account_namespaces, ttl=\"\", max_ttl=\"\",\n                    period=\"\", policies=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Create a role in the method.\n\n        Registers a role in the auth method. Role types have specific entities that can perform login operations\n        against this endpoint. Constraints specific to the role type must be set on the role. These are applied to\n        the authenticated entities attempting to login.\n\n        Supported methods:\n            POST: /auth/{mount_point}/role/{name}. Produces: 204 (empty body)\n\n        :param name: Name of the role.\n        :type name: str | unicode\n        :param bound_service_account_names: List of service account names able to access this role. If set to \"*\"\n            all names are allowed, both this and bound_service_account_namespaces can not be \"*\".\n        :type bound_service_account_names: list | str | unicode\n        :param bound_service_account_namespaces: List of namespaces allowed to access this role. If set to \"*\" all\n            namespaces are allowed, both this and bound_service_account_names can not be set to \"*\".\n        :type bound_service_account_namespaces: list | str | unicode\n        :param ttl: The TTL period of tokens issued using this role in seconds.\n        :type ttl: str | unicode\n        :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role.\n        :type max_ttl: str | unicode\n        :param period: If set, indicates that the token generated using this role should never expire. The token should\n            be renewed within the duration specified by this value. At each renewal, the token's TTL will be set to the\n            value of this parameter.\n        :type period: str | unicode\n        :param policies: Policies to be set on tokens issued using this role.\n        :type policies: list | str | unicode\n        :param mount_point: The \"path\" the azure auth method was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        list_of_strings_params = {\n            'bound_service_account_names': bound_service_account_names,\n            'bound_service_account_namespaces': bound_service_account_namespaces,\n            'policies': policies\n        }\n        for param_name, param_argument in list_of_strings_params.items():\n            validate_list_of_strings_param(\n                param_name=param_name,\n                param_argument=param_argument,\n            )\n\n        if bound_service_account_names in (\"*\", [\"*\"]) and bound_service_account_namespaces in (\"*\", [\"*\"]):\n            error_msg = 'unsupported combination of `bind_service_account_names` and ' \\\n                        '`bound_service_account_namespaces` arguments. Both of them can not be set to `*`'\n            raise exceptions.ParamValidationError(error_msg)\n\n        params = {\n            'bound_service_account_names': comma_delimited_to_list(bound_service_account_names),\n            'bound_service_account_namespaces': comma_delimited_to_list(bound_service_account_namespaces),\n            'ttl': ttl,\n            'max_ttl': max_ttl,\n            'period': period,\n            'policies': comma_delimited_to_list(policies),\n        }\n\n        api_path = '/v1/auth/{mount_point}/role/{name}'.format(mount_point=mount_point, name=name)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Creates a new role in the auth method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_role(self, name, policies=None, ttl=None, max_ttl=None, period=None, bound_service_principal_ids=None,\n                    bound_group_ids=None, bound_location=None, bound_subscription_ids=None,\n                    bound_resource_group_names=None, bound_scale_sets=None, mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Create a role in the method.\n\n        Role types have specific entities that can perform login operations against this endpoint. Constraints specific\n        to the role type must be set on the role. These are applied to the authenticated entities attempting to login.\n\n        Supported methods:\n            POST: /auth/{mount_point}/role/{name}. Produces: 204 (empty body)\n\n\n        :param name: Name of the role.\n        :type name: str | unicode\n        :param policies: Policies to be set on tokens issued using this role.\n        :type policies: list\n        :param ttl: The TTL period of tokens issued using this role in seconds.\n        :type ttl: str | unicode\n        :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role.\n        :type max_ttl: str | unicode\n        :param period: If set, indicates that the token generated using this role should never expire. The token should\n            be renewed within the duration specified by this value. At each renewal, the token's TTL will be set to the\n            value of this parameter.\n        :type period: str | unicode\n        :param bound_service_principal_ids: The list of Service Principal IDs that login is restricted to.\n        :type bound_service_principal_ids: list\n        :param bound_group_ids: The list of group ids that login is restricted to.\n        :type bound_group_ids: list\n        :param bound_location: The list of locations that login is restricted to.\n        :type bound_location: list\n        :param bound_subscription_ids: The list of subscription IDs that login is restricted to.\n        :type bound_subscription_ids: list\n        :param bound_resource_group_names: The list of resource groups that login is restricted to.\n        :type bound_resource_group_names: list\n        :param bound_scale_sets: The list of scale set names that the login is restricted to.\n        :type bound_scale_sets: list\n        :param mount_point: The \"path\" the azure auth method was mounted on.\n        :type mount_point: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if policies is None:\n            policies = []\n        if not isinstance(policies, list) or not all([isinstance(p, str) for p in policies]):\n            error_msg = 'unsupported policies argument provided \"{arg}\" ({arg_type}), required type: List[str]\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                arg=policies,\n                arg_type=type(policies),\n            ))\n        params = {\n            'policies': policies,\n            'ttl': ttl,\n            'max_ttl': max_ttl,\n            'period': period,\n            'bound_service_principal_ids': bound_service_principal_ids,\n            'bound_group_ids': bound_group_ids,\n            'bound_location': bound_location,\n            'bound_subscription_ids': bound_subscription_ids,\n            'bound_resource_group_names': bound_resource_group_names,\n            'bound_scale_sets': bound_scale_sets,\n        }\n\n        api_path = '/v1/auth/{mount_point}/role/{name}'.format(mount_point=mount_point, name=name)\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Create a new role in the endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list_roles(self, mount_point=DEFAULT_MOUNT_POINT):\n        api_path = '/v1/auth/{mount_point}/roles'.format(mount_point=mount_point)\n        response = self._adapter.list(\n            url=api_path\n        )\n        return response.json().get('data')", "response": "List all the roles that are registered with the plugin."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef login(self, role, jwt, subscription_id=None, resource_group_name=None, vm_name=None, vmss_name=None, use_token=True,\n              mount_point=DEFAULT_MOUNT_POINT):\n        \"\"\"Fetch a token.\n\n        This endpoint takes a signed JSON Web Token (JWT) and a role name for some entity. It verifies the JWT signature\n        to authenticate that entity and then authorizes the entity for the given role.\n\n        Supported methods:\n            POST: /auth/{mount_point}/login. Produces: 200 application/json\n\n\n        :param role: Name of the role against which the login is being attempted.\n        :type role: str | unicode\n        :param jwt: Signed JSON Web Token (JWT) from Azure MSI.\n        :type jwt: str | unicode\n        :param subscription_id: The subscription ID for the machine that generated the MSI token. This information can\n            be obtained through instance metadata.\n        :type subscription_id: str | unicode\n        :param resource_group_name: The resource group for the machine that generated the MSI token. This information\n            can be obtained through instance metadata.\n        :type resource_group_name: str | unicode\n        :param vm_name: The virtual machine name for the machine that generated the MSI token. This information can be\n            obtained through instance metadata.  If vmss_name is provided, this value is ignored.\n        :type vm_name: str | unicode\n        :param vmss_name: The virtual machine scale set name for the machine that generated the MSI token. This\n            information can be obtained through instance metadata.\n        :type vmss_name: str | unicode\n        :param use_token: if True, uses the token in the response received from the auth request to set the \"token\"\n            attribute on the the :py:meth:`hvac.adapters.Adapter` instance under the _adapater Client attribute.\n        :type use_token: bool\n        :param mount_point: The \"path\" the azure auth method was mounted on.\n        :type mount_point: str | unicode\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        params = {\n            'role': role,\n            'jwt': jwt,\n        }\n        if subscription_id is not None:\n            params['subscription_id'] = subscription_id\n        if resource_group_name is not None:\n            params['resource_group_name'] = resource_group_name\n        if vm_name is not None:\n            params['vm_name'] = vm_name\n        if vmss_name is not None:\n            params['vmss_name'] = vmss_name\n        api_path = '/v1/auth/{mount_point}/login'.format(mount_point=mount_point)\n        response = self._adapter.login(\n            url=api_path,\n            use_token=use_token,\n            json=params,\n        )\n        return response", "response": "This endpoint authenticates a resource group and a resource group and a MSI token and authorizes the entity for the given role."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef enable_audit_device(self, device_type, description=None, options=None, path=None):\n\n        if path is None:\n            path = device_type\n\n        params = {\n            'type': device_type,\n            'description': description,\n            'options': options,\n        }\n\n        api_path = '/v1/sys/audit/{path}'.format(path=path)\n        return self._adapter.post(\n            url=api_path,\n            json=params\n        )", "response": "Enable a new audit device at the supplied path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disable_audit_device(self, path):\n        api_path = '/v1/sys/audit/{path}'.format(path=path)\n        return self._adapter.delete(\n            url=api_path,\n        )", "response": "Disable the audit device at the given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef calculate_hash(self, path, input_to_hash):\n        params = {\n            'input': input_to_hash,\n        }\n\n        api_path = '/v1/sys/audit-hash/{path}'.format(path=path)\n        response = self._adapter.post(\n            url=api_path,\n            json=params\n        )\n        return response.json()", "response": "This endpoint generates hashes for the given input data with the specified audit device s hash function and salt."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist all configured policies.", "response": "def list_policies(self):\n        \"\"\"List all configured policies.\n\n        Supported methods:\n            GET: /sys/policy. Produces: 200 application/json\n\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/sys/policy'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_policy(self, name):\n        api_path = '/v1/sys/policy/{name}'.format(name=name)\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()", "response": "Retrieve the body for the named policy."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds a new or update an existing policy.", "response": "def create_or_update_policy(self, name, policy, pretty_print=True):\n        \"\"\"Add a new or update an existing policy.\n\n        Once a policy is updated, it takes effect immediately to all associated users.\n\n        Supported methods:\n            PUT: /sys/policy/{name}. Produces: 204 (empty body)\n\n        :param name: Specifies the name of the policy to create.\n        :type name: str | unicode\n        :param policy: Specifies the policy document.\n        :type policy: str | unicode | dict\n        :param pretty_print: If True, and provided a dict for the policy argument, send the policy JSON to Vault with\n            \"pretty\" formatting.\n        :type pretty_print: bool\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        if isinstance(policy, dict):\n            if pretty_print:\n                policy = json.dumps(policy, indent=4, sort_keys=True)\n            else:\n                policy = json.dumps(policy)\n        params = {\n            'policy': policy,\n        }\n        api_path = '/v1/sys/policy/{name}'.format(name=name)\n        return self._adapter.put(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete the policy with the given name.", "response": "def delete_policy(self, name):\n        \"\"\"Delete the policy with the given name.\n\n        This will immediately affect all users associated with this policy.\n\n        Supported methods:\n            DELETE: /sys/policy/{name}. Produces: 204 (empty body)\n\n        :param name: Specifies the name of the policy to delete.\n        :type name: str | unicode\n        :return: The response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        api_path = '/v1/sys/policy/{name}'.format(name=name)\n        return self._adapter.delete(\n            url=api_path,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets / <path >", "response": "def read(self, path, wrap_ttl=None):\n        \"\"\"GET /<path>\n\n        :param path:\n        :type path:\n        :param wrap_ttl:\n        :type wrap_ttl:\n        :return:\n        :rtype:\n        \"\"\"\n        try:\n            return self._adapter.get('/v1/{0}'.format(path), wrap_ttl=wrap_ttl).json()\n        except exceptions.InvalidPath:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef list(self, path):\n        try:\n            payload = {\n                'list': True\n            }\n            return self._adapter.get('/v1/{0}'.format(path), params=payload).json()\n        except exceptions.InvalidPath:\n            return None", "response": "GET / <path >"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write(self, path, wrap_ttl=None, **kwargs):\n        response = self._adapter.post('/v1/{0}'.format(path), json=kwargs, wrap_ttl=wrap_ttl)\n\n        if response.status_code == 200:\n            return response.json()", "response": "POST a new entry in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nretrieves the body for the named policy.", "response": "def get_policy(self, name, parse=False):\n        \"\"\"Retrieve the policy body for the named policy.\n\n        :param name: The name of the policy to retrieve.\n        :type name: str | unicode\n        :param parse: Specifies whether to parse the policy body using pyhcl or not.\n        :type parse: bool\n        :return: The (optionally parsed) policy body for the specified policy.\n        :rtype: str | dict\n        \"\"\"\n        try:\n            policy = self.sys.read_policy(name=name)['data']['rules']\n        except exceptions.InvalidPath:\n            return None\n\n        if parse:\n            if not has_hcl_parser:\n                raise ImportError('pyhcl is required for policy parsing')\n            policy = hcl.loads(policy)\n\n        return policy"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_token(self, role=None, token_id=None, policies=None, meta=None,\n                     no_parent=False, lease=None, display_name=None,\n                     num_uses=None, no_default_policy=False,\n                     ttl=None, orphan=False, wrap_ttl=None, renewable=None,\n                     explicit_max_ttl=None, period=None, token_type=None):\n        \"\"\"POST /auth/token/create\n\n        POST /auth/token/create/<role>\n\n        POST /auth/token/create-orphan\n\n        :param role:\n        :type role:\n        :param token_id:\n        :type token_id:\n        :param policies:\n        :type policies:\n        :param meta:\n        :type meta:\n        :param no_parent:\n        :type no_parent:\n        :param lease:\n        :type lease:\n        :param display_name:\n        :type display_name:\n        :param num_uses:\n        :type num_uses:\n        :param no_default_policy:\n        :type no_default_policy:\n        :param ttl:\n        :type ttl:\n        :param orphan:\n        :type orphan:\n        :param wrap_ttl:\n        :type wrap_ttl:\n        :param renewable:\n        :type renewable:\n        :param explicit_max_ttl:\n        :type explicit_max_ttl:\n        :param period:\n        :type period:\n        :param token_type:\n        :type token_type:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'id': token_id,\n            'policies': policies,\n            'meta': meta,\n            'no_parent': no_parent,\n            'display_name': display_name,\n            'num_uses': num_uses,\n            'no_default_policy': no_default_policy,\n            'renewable': renewable\n        }\n\n        if lease:\n            params['lease'] = lease\n        else:\n            params['ttl'] = ttl\n            params['explicit_max_ttl'] = explicit_max_ttl\n\n        if explicit_max_ttl:\n            params['explicit_max_ttl'] = explicit_max_ttl\n\n        if period:\n            params['period'] = period\n        if token_type:\n            params['type'] = token_type\n\n        if orphan:\n            return self._adapter.post('/v1/auth/token/create-orphan', json=params, wrap_ttl=wrap_ttl).json()\n        elif role:\n            return self._adapter.post('/v1/auth/token/create/{0}'.format(role), json=params, wrap_ttl=wrap_ttl).json()\n        else:\n            return self._adapter.post('/v1/auth/token/create', json=params, wrap_ttl=wrap_ttl).json()", "response": "Creates a new token in the given role."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lookup_token(self, token=None, accessor=False, wrap_ttl=None):\n        token_param = {\n            'token': token,\n        }\n        accessor_param = {\n            'accessor': token,\n        }\n        if token:\n            if accessor:\n                path = '/v1/auth/token/lookup-accessor'\n                return self._adapter.post(path, json=accessor_param, wrap_ttl=wrap_ttl).json()\n            else:\n                path = '/v1/auth/token/lookup'\n                return self._adapter.post(path, json=token_param).json()\n        else:\n            path = '/v1/auth/token/lookup-self'\n            return self._adapter.get(path, wrap_ttl=wrap_ttl).json()", "response": "Get the token and associated accessory information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef revoke_token(self, token, orphan=False, accessor=False):\n        if accessor and orphan:\n            msg = \"revoke_token does not support 'orphan' and 'accessor' flags together\"\n            raise exceptions.InvalidRequest(msg)\n        elif accessor:\n            params = {'accessor': token}\n            self._adapter.post('/v1/auth/token/revoke-accessor', json=params)\n        elif orphan:\n            params = {'token': token}\n            self._adapter.post('/v1/auth/token/revoke-orphan', json=params)\n        else:\n            params = {'token': token}\n            self._adapter.post('/v1/auth/token/revoke', json=params)", "response": "Revoke the specified token from the current token store."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef renew_token(self, token=None, increment=None, wrap_ttl=None):\n        params = {\n            'increment': increment,\n        }\n\n        if token is not None:\n            params['token'] = token\n            return self._adapter.post('/v1/auth/token/renew', json=params, wrap_ttl=wrap_ttl).json()\n        else:\n            return self._adapter.post('/v1/auth/token/renew-self', json=params, wrap_ttl=wrap_ttl).json()", "response": "Renew the current token."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_token_role(self, role,\n                          allowed_policies=None, disallowed_policies=None,\n                          orphan=None, period=None, renewable=None,\n                          path_suffix=None, explicit_max_ttl=None):\n        \"\"\"POST /auth/token/roles/<role>\n\n        :param role:\n        :type role:\n        :param allowed_policies:\n        :type allowed_policies:\n        :param disallowed_policies:\n        :type disallowed_policies:\n        :param orphan:\n        :type orphan:\n        :param period:\n        :type period:\n        :param renewable:\n        :type renewable:\n        :param path_suffix:\n        :type path_suffix:\n        :param explicit_max_ttl:\n        :type explicit_max_ttl:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'allowed_policies': allowed_policies,\n            'disallowed_policies': disallowed_policies,\n            'orphan': orphan,\n            'period': period,\n            'renewable': renewable,\n            'path_suffix': path_suffix,\n            'explicit_max_ttl': explicit_max_ttl\n        }\n        return self._adapter.post('/v1/auth/token/roles/{0}'.format(role), json=params)", "response": "Creates a new token role."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_authenticated(self):\n        if not self.token:\n            return False\n\n        try:\n            self.lookup_token()\n            return True\n        except exceptions.Forbidden:\n            return False\n        except exceptions.InvalidPath:\n            return False\n        except exceptions.InvalidRequest:\n            return False", "response": "Helper method which returns the authentication status of the client"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef auth_app_id(self, app_id, user_id, mount_point='app-id', use_token=True):\n        params = {\n            'app_id': app_id,\n            'user_id': user_id,\n        }\n\n        return self.login('/v1/auth/{0}/login'.format(mount_point), json=params, use_token=use_token)", "response": "Login to the specified app_id and user_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef auth_tls(self, mount_point='cert', use_token=True):\n        return self.login('/v1/auth/{0}/login'.format(mount_point), use_token=use_token)", "response": "POST / auth / tls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nposting / auth / userpass", "response": "def auth_userpass(self, username, password, mount_point='userpass', use_token=True, **kwargs):\n        \"\"\"POST /auth/<mount point>/login/<username>\n\n        :param username:\n        :type username:\n        :param password:\n        :type password:\n        :param mount_point:\n        :type mount_point:\n        :param use_token:\n        :type use_token:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'password': password,\n        }\n\n        params.update(kwargs)\n\n        return self.login('/v1/auth/{0}/login/{1}'.format(mount_point, username), json=params, use_token=use_token)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nposts /auth/<mount point>/login :param access_key: AWS IAM access key ID :type access_key: str :param secret_key: AWS IAM secret access key :type secret_key: str :param session_token: Optional AWS IAM session token retrieved via a GetSessionToken AWS API request. see: https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html :type session_token: str :param header_value: Vault allows you to require an additional header, X-Vault-AWS-IAM-Server-ID, to be present to mitigate against different types of replay attacks. Depending on the configuration of the AWS auth backend, providing a argument to this optional parameter may be required. :type header_value: str :param mount_point: The \"path\" the AWS auth backend was mounted on. Vault currently defaults to \"aws\". \"aws-ec2\" is the default argument for backwards comparability within this module. :type mount_point: str :param role: Name of the role against which the login is being attempted. If role is not specified, then the login endpoint looks for a role bearing the name of the AMI ID of the EC2 instance that is trying to login if using the ec2 auth method, or the \"friendly name\" (i.e., role name or username) of the IAM principal authenticated. If a matching role is not found, login fails. :type role: str :param use_token: If True, uses the token in the response received from the auth request to set the \"token\" attribute on the current Client class instance. :type use_token: bool. :return: The response from the AWS IAM login request attempt. :rtype: requests.Response", "response": "def auth_aws_iam(self, access_key, secret_key, session_token=None, header_value=None, mount_point='aws', role='', use_token=True, region='us-east-1'):\n        \"\"\"POST /auth/<mount point>/login\n\n        :param access_key: AWS IAM access key ID\n        :type access_key: str\n        :param secret_key: AWS IAM secret access key\n        :type secret_key: str\n        :param session_token: Optional AWS IAM session token retrieved via a GetSessionToken AWS API request.\n            see: https://docs.aws.amazon.com/STS/latest/APIReference/API_GetSessionToken.html\n        :type session_token: str\n        :param header_value: Vault allows you to require an additional header, X-Vault-AWS-IAM-Server-ID, to be present\n            to mitigate against different types of replay attacks. Depending on the configuration of the AWS auth\n            backend, providing a argument to this optional parameter may be required.\n        :type header_value: str\n        :param mount_point: The \"path\" the AWS auth backend was mounted on. Vault currently defaults to \"aws\". \"aws-ec2\"\n            is the default argument for backwards comparability within this module.\n        :type mount_point: str\n        :param role: Name of the role against which the login is being attempted. If role is not specified, then the\n            login endpoint looks for a role bearing the name of the AMI ID of the EC2 instance that is trying to login\n            if using the ec2 auth method, or the \"friendly name\" (i.e., role name or username) of the IAM principal\n            authenticated. If a matching role is not found, login fails.\n        :type role: str\n        :param use_token: If True, uses the token in the response received from the auth request to set the \"token\"\n            attribute on the current Client class instance.\n        :type use_token: bool.\n        :return: The response from the AWS IAM login request attempt.\n        :rtype: requests.Response\n        \"\"\"\n        request = aws_utils.generate_sigv4_auth_request(header_value=header_value)\n\n        auth = aws_utils.SigV4Auth(access_key, secret_key, session_token, region)\n        auth.add_auth(request)\n\n        # https://github.com/hashicorp/vault/blob/master/builtin/credential/aws/cli.go\n        headers = json.dumps({k: [request.headers[k]] for k in request.headers})\n        params = {\n            'iam_http_request_method': request.method,\n            'iam_request_url': b64encode(request.url.encode('utf-8')).decode('utf-8'),\n            'iam_request_headers': b64encode(headers.encode('utf-8')).decode('utf-8'),\n            'iam_request_body': b64encode(request.body.encode('utf-8')).decode('utf-8'),\n            'role': role,\n        }\n\n        return self.login('/v1/auth/{0}/login'.format(mount_point), json=params, use_token=use_token)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlogging-in to Vault for AWS EC2", "response": "def auth_ec2(self, pkcs7, nonce=None, role=None, use_token=True, mount_point='aws-ec2'):\n        \"\"\"POST /auth/<mount point>/login\n\n        :param pkcs7: PKCS#7 version of an AWS Instance Identity Document from the EC2 Metadata Service.\n        :type pkcs7: str.\n        :param nonce: Optional nonce returned as part of the original authentication request. Not required if the backend\n            has \"allow_instance_migration\" or \"disallow_reauthentication\" options turned on.\n        :type nonce: str.\n        :param role: Identifier for the AWS auth backend role being requested.\n        :type role: str.\n        :param use_token: If True, uses the token in the response received from the auth request to set the \"token\"\n            attribute on the current Client class instance.\n        :type use_token: bool.\n        :param mount_point: The \"path\" the AWS auth backend was mounted on. Vault currently defaults to \"aws\". \"aws-ec2\"\n            is the default argument for backwards comparability within this module.\n        :type mount_point: str.\n        :return: parsed JSON response from the auth POST request\n        :rtype: dict.\n\n        \"\"\"\n        params = {'pkcs7': pkcs7}\n        if nonce:\n            params['nonce'] = nonce\n        if role:\n            params['role'] = role\n\n        return self.login('/v1/auth/{0}/login'.format(mount_point), json=params, use_token=use_token)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a userpass for the specified user.", "response": "def create_userpass(self, username, password, policies, mount_point='userpass', **kwargs):\n        \"\"\"POST /auth/<mount point>/users/<username>\n\n        :param username:\n        :type username:\n        :param password:\n        :type password:\n        :param policies:\n        :type policies:\n        :param mount_point:\n        :type mount_point:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n\n        # Users can have more than 1 policy. It is easier for the user to pass in the\n        # policies as a list so if they do, we need to convert to a , delimited string.\n        if isinstance(policies, (list, set, tuple)):\n            policies = ','.join(policies)\n\n        params = {\n            'password': password,\n            'policies': policies\n        }\n        params.update(kwargs)\n\n        return self._adapter.post('/v1/auth/{}/users/{}'.format(mount_point, username), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget /auth/<mount point>/users?list=true :param mount_point: :type mount_point: :return: :rtype:", "response": "def list_userpass(self, mount_point='userpass'):\n        \"\"\"GET /auth/<mount point>/users?list=true\n\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        try:\n            return self._adapter.get('/v1/auth/{}/users'.format(mount_point), params={'list': True}).json()\n        except exceptions.InvalidPath:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_userpass(self, username, mount_point='userpass'):\n        return self._adapter.get('/v1/auth/{}/users/{}'.format(mount_point, username)).json()", "response": "Read user pass information."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the user s policies.", "response": "def update_userpass_policies(self, username, policies, mount_point='userpass'):\n        \"\"\"POST /auth/<mount point>/users/<username>/policies\n\n        :param username:\n        :type username:\n        :param policies:\n        :type policies:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        # userpass can have more than 1 policy. It is easier for the user to pass in the\n        # policies as a list so if they do, we need to convert to a , delimited string.\n        if isinstance(policies, (list, set, tuple)):\n            policies = ','.join(policies)\n\n        params = {\n            'policies': policies\n        }\n\n        return self._adapter.post('/v1/auth/{}/users/{}/policies'.format(mount_point, username), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the user s password.", "response": "def update_userpass_password(self, username, password, mount_point='userpass'):\n        \"\"\"POST /auth/<mount point>/users/<username>/password\n\n        :param username:\n        :type username:\n        :param password:\n        :type password:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'password': password\n        }\n        return self._adapter.post('/v1/auth/{}/users/{}/password'.format(mount_point, username), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete /auth/<mount point>/users/<username> :param username: :type username: :param mount_point: :type mount_point: :return: :rtype:", "response": "def delete_userpass(self, username, mount_point='userpass'):\n        \"\"\"DELETE /auth/<mount point>/users/<username>\n\n        :param username:\n        :type username:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        return self._adapter.delete('/v1/auth/{}/users/{}'.format(mount_point, username))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an app - id for the specified app - id and policies.", "response": "def create_app_id(self, app_id, policies, display_name=None, mount_point='app-id', **kwargs):\n        \"\"\"POST /auth/<mount point>/map/app-id/<app_id>\n\n        :param app_id:\n        :type app_id:\n        :param policies:\n        :type policies:\n        :param display_name:\n        :type display_name:\n        :param mount_point:\n        :type mount_point:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n\n        # app-id can have more than 1 policy. It is easier for the user to pass in the\n        # policies as a list so if they do, we need to convert to a , delimited string.\n        if isinstance(policies, (list, set, tuple)):\n            policies = ','.join(policies)\n\n        params = {\n            'value': policies\n        }\n\n        # Only use the display_name if it has a value. Made it a named param for user\n        # convienence instead of leaving it as part of the kwargs\n        if display_name:\n            params['display_name'] = display_name\n\n        params.update(kwargs)\n\n        return self._adapter.post('/v1/auth/{}/map/app-id/{}'.format(mount_point, app_id), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the app - id for the specified app.", "response": "def get_app_id(self, app_id, mount_point='app-id', wrap_ttl=None):\n        \"\"\"GET /auth/<mount_point>/map/app-id/<app_id>\n\n        :param app_id:\n        :type app_id:\n        :param mount_point:\n        :type mount_point:\n        :param wrap_ttl:\n        :type wrap_ttl:\n        :return:\n        :rtype:\n        \"\"\"\n        path = '/v1/auth/{0}/map/app-id/{1}'.format(mount_point, app_id)\n        return self._adapter.get(path, wrap_ttl=wrap_ttl).json()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef delete_app_id(self, app_id, mount_point='app-id'):\n        return self._adapter.delete('/v1/auth/{0}/map/app-id/{1}'.format(mount_point, app_id))", "response": "Delete the app - id entry for the specified app_id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a user - id associated with an app - id.", "response": "def create_user_id(self, user_id, app_id, cidr_block=None, mount_point='app-id', **kwargs):\n        \"\"\"POST /auth/<mount point>/map/user-id/<user_id>\n\n        :param user_id:\n        :type user_id:\n        :param app_id:\n        :type app_id:\n        :param cidr_block:\n        :type cidr_block:\n        :param mount_point:\n        :type mount_point:\n        :param kwargs:\n        :type kwargs:\n        :return:\n        :rtype:\n        \"\"\"\n\n        # user-id can be associated to more than 1 app-id (aka policy). It is easier for the user to\n        # pass in the policies as a list so if they do, we need to convert to a , delimited string.\n        if isinstance(app_id, (list, set, tuple)):\n            app_id = ','.join(app_id)\n\n        params = {\n            'value': app_id\n        }\n\n        # Only use the cidr_block if it has a value. Made it a named param for user\n        # convienence instead of leaving it as part of the kwargs\n        if cidr_block:\n            params['cidr_block'] = cidr_block\n\n        params.update(kwargs)\n\n        return self._adapter.post('/v1/auth/{}/map/user-id/{}'.format(mount_point, user_id), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_user_id(self, user_id, mount_point='app-id', wrap_ttl=None):\n        path = '/v1/auth/{0}/map/user-id/{1}'.format(mount_point, user_id)\n        return self._adapter.get(path, wrap_ttl=wrap_ttl).json()", "response": "Get the user s metadata."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_user_id(self, user_id, mount_point='app-id'):\n        return self._adapter.delete('/v1/auth/{0}/map/user-id/{1}'.format(mount_point, user_id))", "response": "Delete a user - id entry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_vault_ec2_client_configuration(self, access_key, secret_key, endpoint=None, mount_point='aws-ec2'):\n        params = {\n            'access_key': access_key,\n            'secret_key': secret_key\n        }\n        if endpoint is not None:\n            params['endpoint'] = endpoint\n\n        return self._adapter.post('/v1/auth/{0}/config/client'.format(mount_point), json=params)", "response": "Create the client configuration for AWS Vault."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates Vault EC2 certificate configuration.", "response": "def create_vault_ec2_certificate_configuration(self, cert_name, aws_public_cert, mount_point='aws-ec2'):\n        \"\"\"POST /auth/<mount_point>/config/certificate/<cert_name>\n\n        :param cert_name:\n        :type cert_name:\n        :param aws_public_cert:\n        :type aws_public_cert:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'cert_name': cert_name,\n            'aws_public_cert': aws_public_cert\n        }\n        return self._adapter.post('/v1/auth/{0}/config/certificate/{1}'.format(mount_point, cert_name), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets Vault EC2 certificate configuration.", "response": "def get_vault_ec2_certificate_configuration(self, cert_name, mount_point='aws-ec2'):\n        \"\"\"GET /auth/<mount_point>/config/certificate/<cert_name>\n\n        :param cert_name:\n        :type cert_name:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        return self._adapter.get('/v1/auth/{0}/config/certificate/{1}'.format(mount_point, cert_name)).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists the EC2 certificate configurations for the specified vault.", "response": "def list_vault_ec2_certificate_configurations(self, mount_point='aws-ec2'):\n        \"\"\"GET /auth/<mount_point>/config/certificates?list=true\n\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {'list': True}\n        return self._adapter.get('/v1/auth/{0}/config/certificates'.format(mount_point), params=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate an EC2 role", "response": "def create_ec2_role(self, role, bound_ami_id=None, bound_account_id=None, bound_iam_role_arn=None,\n                        bound_iam_instance_profile_arn=None, bound_ec2_instance_id=None, bound_region=None,\n                        bound_vpc_id=None, bound_subnet_id=None, role_tag=None,  ttl=None, max_ttl=None, period=None,\n                        policies=None, allow_instance_migration=False, disallow_reauthentication=False,\n                        resolve_aws_unique_ids=None, mount_point='aws-ec2'):\n        \"\"\"POST /auth/<mount_point>/role/<role>\n\n        :param role:\n        :type role:\n        :param bound_ami_id:\n        :type bound_ami_id:\n        :param bound_account_id:\n        :type bound_account_id:\n        :param bound_iam_role_arn:\n        :type bound_iam_role_arn:\n        :param bound_iam_instance_profile_arn:\n        :type bound_iam_instance_profile_arn:\n        :param bound_ec2_instance_id:\n        :type bound_ec2_instance_id:\n        :param bound_region:\n        :type bound_region:\n        :param bound_vpc_id:\n        :type bound_vpc_id:\n        :param bound_subnet_id:\n        :type bound_subnet_id:\n        :param role_tag:\n        :type role_tag:\n        :param ttl:\n        :type ttl:\n        :param max_ttl:\n        :type max_ttl:\n        :param period:\n        :type period:\n        :param policies:\n        :type policies:\n        :param allow_instance_migration:\n        :type allow_instance_migration:\n        :param disallow_reauthentication:\n        :type disallow_reauthentication:\n        :param resolve_aws_unique_ids:\n        :type resolve_aws_unique_ids:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'role': role,\n            'auth_type': 'ec2',\n            'disallow_reauthentication': disallow_reauthentication,\n            'allow_instance_migration': allow_instance_migration\n        }\n\n        if bound_ami_id is not None:\n            params['bound_ami_id'] = bound_ami_id\n        if bound_account_id is not None:\n            params['bound_account_id'] = bound_account_id\n        if bound_iam_role_arn is not None:\n            params['bound_iam_role_arn'] = bound_iam_role_arn\n        if bound_ec2_instance_id is not None:\n            params['bound_iam_instance_profile_arn'] = bound_ec2_instance_id\n        if bound_iam_instance_profile_arn is not None:\n            params['bound_iam_instance_profile_arn'] = bound_iam_instance_profile_arn\n        if bound_region is not None:\n            params['bound_region'] = bound_region\n        if bound_vpc_id is not None:\n            params['bound_vpc_id'] = bound_vpc_id\n        if bound_subnet_id is not None:\n            params['bound_subnet_id'] = bound_subnet_id\n        if role_tag is not None:\n            params['role_tag'] = role_tag\n        if ttl is not None:\n            params['ttl'] = ttl\n        else:\n            params['ttl'] = 0\n        if max_ttl is not None:\n            params['max_ttl'] = max_ttl\n        else:\n            params['max_ttl'] = 0\n        if period is not None:\n            params['period'] = period\n        else:\n            params['period'] = 0\n        if policies is not None:\n            params['policies'] = policies\n        if resolve_aws_unique_ids is not None:\n            params['resolve_aws_unique_ids'] = resolve_aws_unique_ids\n\n        return self._adapter.post('/v1/auth/{0}/role/{1}'.format(mount_point, role), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the set of keys associated with an EC2 role.", "response": "def get_ec2_role(self, role, mount_point='aws-ec2'):\n        \"\"\"GET /auth/<mount_point>/role/<role>\n\n        :param role:\n        :type role:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        return self._adapter.get('/v1/auth/{0}/role/{1}'.format(mount_point, role)).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_ec2_role(self, role, mount_point='aws-ec2'):\n        return self._adapter.delete('/v1/auth/{0}/role/{1}'.format(mount_point, role))", "response": "Delete an EC2 role."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate an EC2 role tag.", "response": "def create_ec2_role_tag(self, role, policies=None, max_ttl=None, instance_id=None,\n                            disallow_reauthentication=False, allow_instance_migration=False, mount_point='aws-ec2'):\n        \"\"\"POST /auth/<mount_point>/role/<role>/tag\n\n        :param role:\n        :type role:\n        :param policies:\n        :type policies:\n        :param max_ttl:\n        :type max_ttl:\n        :param instance_id:\n        :type instance_id:\n        :param disallow_reauthentication:\n        :type disallow_reauthentication:\n        :param allow_instance_migration:\n        :type allow_instance_migration:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'role': role,\n            'disallow_reauthentication': disallow_reauthentication,\n            'allow_instance_migration': allow_instance_migration\n        }\n\n        if max_ttl is not None:\n            params['max_ttl'] = max_ttl\n        if policies is not None:\n            params['policies'] = policies\n        if instance_id is not None:\n            params['instance_id'] = instance_id\n        return self._adapter.post('/v1/auth/{0}/role/{1}/tag'.format(mount_point, role), json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef login(self, url, use_token=True, **kwargs):\n        return self._adapter.login(\n            url=url,\n            use_token=use_token,\n            **kwargs\n        )", "response": "Perform a login request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_role(self, role_name, mount_point='approle', **kwargs):\n\n        return self._adapter.post('/v1/auth/{0}/role/{1}'.format(mount_point, role_name), json=kwargs)", "response": "Create a new role on the keystone."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_role(self, role_name, mount_point='approle'):\n\n        return self._adapter.delete('/v1/auth/{0}/role/{1}'.format(mount_point, role_name))", "response": "Delete a role from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the ID of the role with the given name.", "response": "def get_role_id(self, role_name, mount_point='approle'):\n        \"\"\"GET /auth/<mount_point>/role/<role name>/role-id\n\n        :param role_name:\n        :type role_name:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n\n        url = '/v1/auth/{0}/role/{1}/role-id'.format(mount_point, role_name)\n        return self._adapter.get(url).json()['data']['role_id']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the role ID for a user.", "response": "def set_role_id(self, role_name, role_id, mount_point='approle'):\n        \"\"\"POST /auth/<mount_point>/role/<role name>/role-id\n\n        :param role_name:\n        :type role_name:\n        :param role_id:\n        :type role_id:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n\n        url = '/v1/auth/{0}/role/{1}/role-id'.format(mount_point, role_name)\n        params = {\n            'role_id': role_id\n        }\n        return self._adapter.post(url, json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_role(self, role_name, mount_point='approle'):\n        return self._adapter.get('/v1/auth/{0}/role/{1}'.format(mount_point, role_name)).json()", "response": "Get a specific role from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a secret ID for a role.", "response": "def create_role_secret_id(self, role_name, meta=None, cidr_list=None, wrap_ttl=None, mount_point='approle'):\n        \"\"\"POST /auth/<mount_point>/role/<role name>/secret-id\n\n        :param role_name:\n        :type role_name:\n        :param meta:\n        :type meta:\n        :param cidr_list:\n        :type cidr_list:\n        :param wrap_ttl:\n        :type wrap_ttl:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n\n        url = '/v1/auth/{0}/role/{1}/secret-id'.format(mount_point, role_name)\n        params = {}\n        if meta is not None:\n            params['metadata'] = json.dumps(meta)\n        if cidr_list is not None:\n            params['cidr_list'] = cidr_list\n        return self._adapter.post(url, json=params, wrap_ttl=wrap_ttl).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the secret id for a role.", "response": "def get_role_secret_id(self, role_name, secret_id, mount_point='approle'):\n        \"\"\"POST /auth/<mount_point>/role/<role name>/secret-id/lookup\n\n        :param role_name:\n        :type role_name:\n        :param secret_id:\n        :type secret_id:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/auth/{0}/role/{1}/secret-id/lookup'.format(mount_point, role_name)\n        params = {\n            'secret_id': secret_id\n        }\n        return self._adapter.post(url, json=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_role_secrets(self, role_name, mount_point='approle'):\n        url = '/v1/auth/{mount_point}/role/{name}/secret-id'.format(\n            mount_point=mount_point,\n            name=role_name\n        )\n        return self._adapter.list(url).json()", "response": "List the secrets for the AppRole."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the secret ID for a role.", "response": "def get_role_secret_id_accessor(self, role_name, secret_id_accessor, mount_point='approle'):\n        \"\"\"POST /auth/<mount_point>/role/<role name>/secret-id-accessor/lookup\n\n        :param role_name:\n        :type role_name:\n        :param secret_id_accessor:\n        :type secret_id_accessor:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/auth/{0}/role/{1}/secret-id-accessor/lookup'.format(mount_point, role_name)\n        params = {'secret_id_accessor': secret_id_accessor}\n        return self._adapter.post(url, json=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_role_custom_secret_id(self, role_name, secret_id, meta=None, mount_point='approle'):\n        url = '/v1/auth/{0}/role/{1}/custom-secret-id'.format(mount_point, role_name)\n        params = {\n            'secret_id': secret_id\n        }\n        if meta is not None:\n            params['meta'] = meta\n        return self._adapter.post(url, json=params).json()", "response": "Create a custom secret id for a role."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npost / auth/<mount_point > approle", "response": "def auth_approle(self, role_id, secret_id=None, mount_point='approle', use_token=True):\n        \"\"\"POST /auth/<mount_point>/login\n\n        :param role_id:\n        :type role_id:\n        :param secret_id:\n        :type secret_id:\n        :param mount_point:\n        :type mount_point:\n        :param use_token:\n        :type use_token:\n        :return:\n        :rtype:\n        \"\"\"\n        params = {\n            'role_id': role_id\n        }\n        if secret_id is not None:\n            params['secret_id'] = secret_id\n\n        return self.login('/v1/auth/{0}/login'.format(mount_point), json=params, use_token=use_token)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new configuration for the other K8s service account.", "response": "def create_kubernetes_configuration(self, kubernetes_host, kubernetes_ca_cert=None, token_reviewer_jwt=None, pem_keys=None, mount_point='kubernetes'):\n        \"\"\"POST /auth/<mount_point>/config\n\n        :param kubernetes_host: A host:port pair, or a URL to the base of the Kubernetes API server.\n        :type kubernetes_host: str.\n        :param kubernetes_ca_cert: PEM encoded CA cert for use by the TLS client used to talk with the Kubernetes API.\n        :type kubernetes_ca_cert: str.\n        :param token_reviewer_jwt: A service account JWT used to access the TokenReview API to validate other\n            JWTs during login. If not set the JWT used for login will be used to access the API.\n        :type token_reviewer_jwt: str.\n        :param pem_keys: Optional list of PEM-formated public keys or certificates used to verify the signatures of\n            Kubernetes service account JWTs. If a certificate is given, its public key will be extracted. Not every\n            installation of Kubernetes exposes these keys.\n        :type pem_keys: list.\n        :param mount_point: The \"path\" the k8s auth backend was mounted on. Vault currently defaults to \"kubernetes\".\n        :type mount_point: str.\n        :return: Will be an empty body with a 204 status code upon success\n        :rtype: requests.Response.\n        \"\"\"\n        params = {\n            'kubernetes_host': kubernetes_host,\n            'kubernetes_ca_cert': kubernetes_ca_cert,\n        }\n\n        if token_reviewer_jwt is not None:\n            params['token_reviewer_jwt'] = token_reviewer_jwt\n        if pem_keys is not None:\n            params['pem_keys'] = pem_keys\n\n        url = 'v1/auth/{0}/config'.format(mount_point)\n        return self._adapter.post(url, json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the configuration of the k8s auth backend.", "response": "def get_kubernetes_configuration(self, mount_point='kubernetes'):\n        \"\"\"GET /auth/<mount_point>/config\n\n        :param mount_point: The \"path\" the k8s auth backend was mounted on. Vault currently defaults to \"kubernetes\".\n        :type mount_point: str.\n        :return: Parsed JSON response from the config GET request\n        :rtype: dict.\n        \"\"\"\n\n        url = '/v1/auth/{0}/config'.format(mount_point)\n        return self._adapter.get(url).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_kubernetes_role(self, name, bound_service_account_names, bound_service_account_namespaces, ttl=\"\",\n                               max_ttl=\"\", period=\"\", policies=None, mount_point='kubernetes'):\n        \"\"\"POST /auth/<mount_point>/role/:name\n\n        :param name: Name of the role.\n        :type name: str.\n        :param bound_service_account_names: List of service account names able to access this role. If set to \"*\" all\n            names are allowed, both this and bound_service_account_namespaces can not be \"*\".\n        :type bound_service_account_names: list.\n        :param bound_service_account_namespaces: List of namespaces allowed to access this role. If set to \"*\" all\n            namespaces are allowed, both this and bound_service_account_names can not be set to \"*\".\n        :type bound_service_account_namespaces: list.\n        :param ttl: The TTL period of tokens issued using this role in seconds.\n        :type ttl: str.\n        :param max_ttl: The maximum allowed lifetime of tokens issued in seconds using this role.\n        :type max_ttl: str.\n        :param period: If set, indicates that the token generated using this role should never expire.\n            The token should be renewed within the duration specified by this value. At each renewal, the token's TTL will\n            be set to the value of this parameter.\n        :type period: str.\n        :param policies: Policies to be set on tokens issued using this role\n        :type policies: list.\n        :param mount_point: The \"path\" the k8s auth backend was mounted on. Vault currently defaults to \"kubernetes\".\n        :type mount_point: str.\n        :return: Will be an empty body with a 204 status code upon success\n        :rtype: requests.Response.\n        \"\"\"\n        if bound_service_account_names == '*' and bound_service_account_namespaces == '*':\n            error_message = 'bound_service_account_names and bound_service_account_namespaces can not both be set to \"*\"'\n            raise exceptions.ParamValidationError(error_message)\n\n        params = {\n            'bound_service_account_names': bound_service_account_names,\n            'bound_service_account_namespaces': bound_service_account_namespaces,\n            'ttl': ttl,\n            'max_ttl': max_ttl,\n            'period': period,\n            'policies': policies,\n        }\n        url = 'v1/auth/{0}/role/{1}'.format(mount_point, name)\n        return self._adapter.post(url, json=params)", "response": "Creates a Kubernetes role."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the Kubernetes roles for the specified mount point.", "response": "def list_kubernetes_roles(self, mount_point='kubernetes'):\n        \"\"\"GET /auth/<mount_point>/role?list=true\n\n        :param mount_point: The \"path\" the k8s auth backend was mounted on. Vault currently defaults to \"kubernetes\".\n        :type mount_point: str.\n        :return: Parsed JSON response from the list roles GET request.\n        :rtype: dict.\n        \"\"\"\n\n        url = 'v1/auth/{0}/role?list=true'.format(mount_point)\n        return self._adapter.get(url).json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a Kubernetes role.", "response": "def delete_kubernetes_role(self, role, mount_point='kubernetes'):\n        \"\"\"DELETE /auth/<mount_point>/role/:role\n\n        :type role: Name of the role.\n        :param role: str.\n        :param mount_point: The \"path\" the k8s auth backend was mounted on. Vault currently defaults to \"kubernetes\".\n        :type mount_point: str.\n        :return: Will be an empty body with a 204 status code upon success.\n        :rtype: requests.Response.\n        \"\"\"\n\n        url = 'v1/auth/{0}/role/{1}'.format(mount_point, role)\n        return self._adapter.delete(url)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nauthenticate a Kubernetes service account.", "response": "def auth_kubernetes(self, role, jwt, use_token=True, mount_point='kubernetes'):\n        \"\"\"POST /auth/<mount_point>/login\n\n        :param role: Name of the role against which the login is being attempted.\n        :type role: str.\n        :param jwt: Signed JSON Web Token (JWT) for authenticating a service account.\n        :type jwt: str.\n        :param use_token: if True, uses the token in the response received from the auth request to set the \"token\"\n            attribute on the current Client class instance.\n        :type use_token: bool.\n        :param mount_point: The \"path\" the k8s auth backend was mounted on. Vault currently defaults to \"kubernetes\".\n        :type mount_point: str.\n        :return: Parsed JSON response from the config POST request.\n        :rtype: dict.\n        \"\"\"\n        params = {\n            'role': role,\n            'jwt': jwt\n        }\n        url = 'v1/auth/{0}/login'.format(mount_point)\n        return self.login(url, json=params, use_token=use_token)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transit_create_key(self, name, convergent_encryption=None, derived=None, exportable=None,\n                           key_type=None, mount_point='transit'):\n        \"\"\"POST /<mount_point>/keys/<name>\n\n        :param name:\n        :type name:\n        :param convergent_encryption:\n        :type convergent_encryption:\n        :param derived:\n        :type derived:\n        :param exportable:\n        :type exportable:\n        :param key_type:\n        :type key_type:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/{0}/keys/{1}'.format(mount_point, name)\n        params = {}\n        if convergent_encryption is not None:\n            params['convergent_encryption'] = convergent_encryption\n        if derived is not None:\n            params['derived'] = derived\n        if exportable is not None:\n            params['exportable'] = exportable\n        if key_type is not None:\n            params['type'] = key_type\n\n        return self._adapter.post(url, json=params)", "response": "Create a new key in the transit store."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transit_read_key(self, name, mount_point='transit'):\n        url = '/v1/{0}/keys/{1}'.format(mount_point, name)\n        return self._adapter.get(url).json()", "response": "Get a key from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef transit_list_keys(self, mount_point='transit'):\n        url = '/v1/{0}/keys?list=true'.format(mount_point)\n        return self._adapter.get(url).json()", "response": "List the keys in the current store."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef transit_delete_key(self, name, mount_point='transit'):\n        url = '/v1/{0}/keys/{1}'.format(mount_point, name)\n        return self._adapter.delete(url)", "response": "Delete a key from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the key s configuration.", "response": "def transit_update_key(self, name, min_decryption_version=None, min_encryption_version=None, deletion_allowed=None,\n                           mount_point='transit'):\n        \"\"\"POST /<mount_point>/keys/<name>/config\n\n        :param name:\n        :type name:\n        :param min_decryption_version:\n        :type min_decryption_version:\n        :param min_encryption_version:\n        :type min_encryption_version:\n        :param deletion_allowed:\n        :type deletion_allowed:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/{0}/keys/{1}/config'.format(mount_point, name)\n        params = {}\n        if min_decryption_version is not None:\n            params['min_decryption_version'] = min_decryption_version\n        if min_encryption_version is not None:\n            params['min_encryption_version'] = min_encryption_version\n        if deletion_allowed is not None:\n            params['deletion_allowed'] = deletion_allowed\n\n        return self._adapter.post(url, json=params)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrotate a key from the current state to the current state.", "response": "def transit_rotate_key(self, name, mount_point='transit'):\n        \"\"\"POST /<mount_point>/keys/<name>/rotate\n\n        :param name:\n        :type name:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/{0}/keys/{1}/rotate'.format(mount_point, name)\n        return self._adapter.post(url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a specific key from the specified key type and version.", "response": "def transit_export_key(self, name, key_type, version=None, mount_point='transit'):\n        \"\"\"GET /<mount_point>/export/<key_type>/<name>(/<version>)\n\n        :param name:\n        :type name:\n        :param key_type:\n        :type key_type:\n        :param version:\n        :type version:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        if version is not None:\n            url = '/v1/{0}/export/{1}/{2}/{3}'.format(mount_point, key_type, name, version)\n        else:\n            url = '/v1/{0}/export/{1}/{2}'.format(mount_point, key_type, name)\n        return self._adapter.get(url).json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntransit encrypt data for a user - specified key.", "response": "def transit_encrypt_data(self, name, plaintext, context=None, key_version=None, nonce=None, batch_input=None,\n                             key_type=None, convergent_encryption=None, mount_point='transit'):\n        \"\"\"POST /<mount_point>/encrypt/<name>\n\n        :param name:\n        :type name:\n        :param plaintext:\n        :type plaintext:\n        :param context:\n        :type context:\n        :param key_version:\n        :type key_version:\n        :param nonce:\n        :type nonce:\n        :param batch_input:\n        :type batch_input:\n        :param key_type:\n        :type key_type:\n        :param convergent_encryption:\n        :type convergent_encryption:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/{0}/encrypt/{1}'.format(mount_point, name)\n        params = {\n            'plaintext': plaintext\n        }\n        if context is not None:\n            params['context'] = context\n        if key_version is not None:\n            params['key_version'] = key_version\n        if nonce is not None:\n            params['nonce'] = nonce\n        if batch_input is not None:\n            params['batch_input'] = batch_input\n        if key_type is not None:\n            params['type'] = key_type\n        if convergent_encryption is not None:\n            params['convergent_encryption'] = convergent_encryption\n\n        return self._adapter.post(url, json=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npost /<mount_point>/decrypt/<name> :param name: :type name: :param ciphertext: :type ciphertext: :param context: :type context: :param nonce: :type nonce: :param batch_input: :type batch_input: :param mount_point: :type mount_point: :return: :rtype:", "response": "def transit_decrypt_data(self, name, ciphertext, context=None, nonce=None, batch_input=None, mount_point='transit'):\n        \"\"\"POST /<mount_point>/decrypt/<name>\n\n        :param name:\n        :type name:\n        :param ciphertext:\n        :type ciphertext:\n        :param context:\n        :type context:\n        :param nonce:\n        :type nonce:\n        :param batch_input:\n        :type batch_input:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/{0}/decrypt/{1}'.format(mount_point, name)\n        params = {\n            'ciphertext': ciphertext\n        }\n        if context is not None:\n            params['context'] = context\n        if nonce is not None:\n            params['nonce'] = nonce\n        if batch_input is not None:\n            params['batch_input'] = batch_input\n\n        return self._adapter.post(url, json=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a new data key for a given key.", "response": "def transit_generate_data_key(self, name, key_type, context=None, nonce=None, bits=None, mount_point='transit'):\n        \"\"\"POST /<mount_point>/datakey/<type>/<name>\n\n        :param name:\n        :type name:\n        :param key_type:\n        :type key_type:\n        :param context:\n        :type context:\n        :param nonce:\n        :type nonce:\n        :param bits:\n        :type bits:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        url = '/v1/{0}/datakey/{1}/{2}'.format(mount_point, key_type, name)\n        params = {}\n        if context is not None:\n            params['context'] = context\n        if nonce is not None:\n            params['nonce'] = nonce\n        if bits is not None:\n            params['bits'] = bits\n\n        return self._adapter.post(url, json=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transit_generate_rand_bytes(self, data_bytes=None, output_format=None, mount_point='transit'):\n        if data_bytes is not None:\n            url = '/v1/{0}/random/{1}'.format(mount_point, data_bytes)\n        else:\n            url = '/v1/{0}/random'.format(mount_point)\n\n        params = {}\n        if output_format is not None:\n            params[\"format\"] = output_format\n\n        return self._adapter.post(url, json=params).json()", "response": "Generate random bytes for a set of data and output_format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transit_hash_data(self, hash_input, algorithm=None, output_format=None, mount_point='transit'):\n        if algorithm is not None:\n            url = '/v1/{0}/hash/{1}'.format(mount_point, algorithm)\n        else:\n            url = '/v1/{0}/hash'.format(mount_point)\n\n        params = {\n            'input': hash_input\n        }\n        if output_format is not None:\n            params['format'] = output_format\n\n        return self._adapter.post(url, json=params).json()", "response": "Send a transit hash data to the specified hash."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a new HMAC for a transit user.", "response": "def transit_generate_hmac(self, name, hmac_input, key_version=None, algorithm=None, mount_point='transit'):\n        \"\"\"POST /<mount_point>/hmac/<name>(/<algorithm>)\n\n        :param name:\n        :type name:\n        :param hmac_input:\n        :type hmac_input:\n        :param key_version:\n        :type key_version:\n        :param algorithm:\n        :type algorithm:\n        :param mount_point:\n        :type mount_point:\n        :return:\n        :rtype:\n        \"\"\"\n        if algorithm is not None:\n            url = '/v1/{0}/hmac/{1}/{2}'.format(mount_point, name, algorithm)\n        else:\n            url = '/v1/{0}/hmac/{1}'.format(mount_point, name)\n        params = {\n            'input': hmac_input\n        }\n        if key_version is not None:\n            params['key_version'] = key_version\n\n        return self._adapter.post(url, json=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transit_sign_data(self, name, input_data, key_version=None, algorithm=None, context=None, prehashed=None,\n                          mount_point='transit', signature_algorithm='pss'):\n        \"\"\"POST /<mount_point>/sign/<name>(/<algorithm>)\n\n        :param name:\n        :type name:\n        :param input_data:\n        :type input_data:\n        :param key_version:\n        :type key_version:\n        :param algorithm:\n        :type algorithm:\n        :param context:\n        :type context:\n        :param prehashed:\n        :type prehashed:\n        :param mount_point:\n        :type mount_point:\n        :param signature_algorithm:\n        :type signature_algorithm:\n        :return:\n        :rtype:\n        \"\"\"\n        if algorithm is not None:\n            url = '/v1/{0}/sign/{1}/{2}'.format(mount_point, name, algorithm)\n        else:\n            url = '/v1/{0}/sign/{1}'.format(mount_point, name)\n\n        params = {\n            'input': input_data\n        }\n        if key_version is not None:\n            params['key_version'] = key_version\n        if context is not None:\n            params['context'] = context\n        if prehashed is not None:\n            params['prehashed'] = prehashed\n        params['signature_algorithm'] = signature_algorithm\n\n        return self._adapter.post(url, json=params).json()", "response": "Signs the data for a transit user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npost /<mount_point>/verify/<name>(/<algorithm>) :param name: :type name: :param input_data: :type input_data: :param algorithm: :type algorithm: :param signature: :type signature: :param hmac: :type hmac: :param context: :type context: :param prehashed: :type prehashed: :param mount_point: :type mount_point: :param signature_algorithm: :type signature_algorithm: :return: :rtype:", "response": "def transit_verify_signed_data(self, name, input_data, algorithm=None, signature=None, hmac=None, context=None,\n                                   prehashed=None, mount_point='transit', signature_algorithm='pss'):\n        \"\"\"POST /<mount_point>/verify/<name>(/<algorithm>)\n\n        :param name:\n        :type name:\n        :param input_data:\n        :type input_data:\n        :param algorithm:\n        :type algorithm:\n        :param signature:\n        :type signature:\n        :param hmac:\n        :type hmac:\n        :param context:\n        :type context:\n        :param prehashed:\n        :type prehashed:\n        :param mount_point:\n        :type mount_point:\n        :param signature_algorithm:\n        :type signature_algorithm:\n        :return:\n        :rtype:\n        \"\"\"\n        if algorithm is not None:\n            url = '/v1/{0}/verify/{1}/{2}'.format(mount_point, name, algorithm)\n        else:\n            url = '/v1/{0}/verify/{1}'.format(mount_point, name)\n\n        params = {\n            'input': input_data\n        }\n        if signature is not None:\n            params['signature'] = signature\n        if hmac is not None:\n            params['hmac'] = hmac\n        if context is not None:\n            params['context'] = context\n        if prehashed is not None:\n            params['prehashed'] = prehashed\n        params['signature_algorithm'] = signature_algorithm\n\n        return self._adapter.post(url, json=params).json()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_policy(self, name, rules):\n        if isinstance(rules, dict):\n            rules = json.dumps(rules)\n        params = {\n            'rules': rules,\n        }\n        api_path = '/v1/sys/policy/{name}'.format(\n            name=name,\n        )\n        self._adapter.put(api_path, json=params)", "response": "Add a new or update an existing policy."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_secret_backend_tuning(self, backend_type, mount_point=None):\n        if not mount_point:\n            mount_point = backend_type\n        return self.sys.read_mount_configuration(\n            path=mount_point,\n        )", "response": "Get the configuration of a secret backend."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading the health status of a specific node in Vault.", "response": "def read_health_status(self, standby_ok=False, active_code=200, standby_code=429, dr_secondary_code=472,\n                           performance_standby_code=473, sealed_code=503, uninit_code=501, method='HEAD'):\n        \"\"\"Read the health status of Vault.\n\n        This matches the semantics of a Consul HTTP health check and provides a simple way to monitor the health of a\n        Vault instance.\n\n\n        :param standby_ok: Specifies if being a standby should still return the active status code instead of the\n            standby status code. This is useful when Vault is behind a non-configurable load balance that just wants a\n            200-level response.\n        :type standby_ok: bool\n        :param active_code: The status code that should be returned for an active node.\n        :type active_code: int\n        :param standby_code: Specifies the status code that should be returned for a standby node.\n        :type standby_code: int\n        :param dr_secondary_code: Specifies the status code that should be returned for a DR secondary node.\n        :type dr_secondary_code: int\n        :param performance_standby_code: Specifies the status code that should be returned for a performance standby\n            node.\n        :type performance_standby_code: int\n        :param sealed_code: Specifies the status code that should be returned for a sealed node.\n        :type sealed_code: int\n        :param uninit_code: Specifies the status code that should be returned for a uninitialized node.\n        :type uninit_code: int\n        :param method: Supported methods:\n            HEAD: /sys/health. Produces: 000 (empty body)\n            GET: /sys/health. Produces: 000 application/json\n        :type method: str | unicode\n        :return: The JSON response of the request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'standby_ok': standby_ok,\n            'active_code': active_code,\n            'standby_code': standby_code,\n            'dr_secondary_code': dr_secondary_code,\n            'performance_standby_code': performance_standby_code,\n            'sealed_code': sealed_code,\n            'uninit_code': uninit_code,\n        }\n\n        if method == 'HEAD':\n            api_path = '/v1/sys/health'.format()\n            response = self._adapter.head(\n                url=api_path,\n                raise_exception=False,\n            )\n            return response\n        elif method == 'GET':\n            api_path = '/v1/sys/health'.format()\n            response = self._adapter.get(\n                url=api_path,\n                json=params,\n                raise_exception=False,\n            )\n            return response.json()\n        else:\n            error_message = '\"method\" parameter provided invalid value; HEAD or GET allowed, \"{method}\" provided'.format(method=method)\n            raise exceptions.ParamValidationError(error_message)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the high availability status and current leader instance of Vault.", "response": "def read_leader_status(self):\n        \"\"\"Read the high availability status and current leader instance of Vault.\n\n        Supported methods:\n            GET: /sys/leader. Produces: 200 application/json\n\n        :return: The JSON response of the request.\n        :rtype: dict\n        \"\"\"\n        api_path = '/v1/sys/leader'\n        response = self._adapter.get(\n            url=api_path,\n        )\n        return response.json()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring multi - factor authentication with a given backend.", "response": "def configure(self, mount_point, mfa_type='duo', force=False):\n        \"\"\"Configure MFA for a supported method.\n\n        This endpoint allows you to turn on multi-factor authentication with a given backend.\n        Currently only Duo is supported.\n\n        Supported methods:\n            POST: /auth/{mount_point}/mfa_config. Produces: 204 (empty body)\n\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :param mfa_type: Enables MFA with given backend (available: duo)\n        :type mfa_type: str | unicode\n        :param force: If True, make the \"mfa_config\" request regardless of circumstance. If False (the default), verify\n            the provided mount_point is available and one of the types of methods supported by this feature.\n        :type force: bool\n        :return: The response of the configure MFA request.\n        :rtype: requests.Response\n        \"\"\"\n        if mfa_type != 'duo' and not force:\n            # The situation described via this exception is not likely to change in the future.\n            # However we provided that flexibility here just in case.\n            error_msg = 'Unsupported mfa_type argument provided \"{arg}\", supported types: \"{mfa_types}\"'\n            raise exceptions.ParamValidationError(error_msg.format(\n                mfa_types=','.join(SUPPORTED_MFA_TYPES),\n                arg=mfa_type,\n            ))\n        params = {\n            'type': mfa_type,\n        }\n\n        api_path = '/v1/auth/{mount_point}/mfa_config'.format(\n            mount_point=mount_point\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef configure_duo_access(self, mount_point, host, integration_key, secret_key):\n        params = {\n            'host': host,\n            'ikey': integration_key,\n            'skey': secret_key,\n        }\n        api_path = '/v1/auth/{mount_point}/duo/access'.format(\n            mount_point=mount_point,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )", "response": "Configure the access keys and host for Duo API connections."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconfigure Duo second factor behavior.", "response": "def configure_duo_behavior(self, mount_point, push_info=None, user_agent=None, username_format='%s'):\n        \"\"\"Configure Duo second factor behavior.\n\n        This endpoint allows you to configure how the original auth method username maps to the Duo username by\n        providing a template format string.\n\n        Supported methods:\n            POST: /auth/{mount_point}/duo/config. Produces: 204 (empty body)\n\n\n        :param mount_point: The \"path\" the method/backend was mounted on.\n        :type mount_point: str | unicode\n        :param push_info: A string of URL-encoded key/value pairs that provides additional context about the\n            authentication attempt in the Duo Mobile app\n        :type push_info: str | unicode\n        :param user_agent: User agent to connect to Duo (default \"\")\n        :type user_agent: str | unicode\n        :param username_format: Format string given auth method username as argument to create Duo username\n            (default '%s')\n        :type username_format: str | unicode\n        :return: The response of the configure_duo_behavior request.\n        :rtype: requests.Response\n        \"\"\"\n        params = {\n            'username_format': username_format,\n        }\n        if push_info is not None:\n            params['push_info'] = push_info\n        if user_agent is not None:\n            params['user_agent'] = user_agent\n        api_path = '/v1/auth/{mount_point}/duo/config'.format(\n            mount_point=mount_point,\n        )\n        return self._adapter.post(\n            url=api_path,\n            json=params,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconnect to the kik servers.", "response": "def _connect(self):\n        \"\"\"\n        Runs the kik connection thread, which creates an encrypted (SSL based) TCP connection\n        to the kik servers.\n        \"\"\"\n        self.kik_connection_thread = Thread(target=self._kik_connection_thread_function, name=\"Kik Connection\")\n        self.kik_connection_thread.start()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _on_connection_made(self):\n        if self.username is not None and self.password is not None and self.kik_node is not None:\n            # we have all required credentials, we can authenticate\n            log.info(\"[+] Establishing authenticated connection using kik node '{}'...\".format(self.kik_node))\n\n            message = login.EstablishAuthenticatedSessionRequest(self.kik_node, self.username, self.password, self.device_id_override)\n            self.initial_connection_payload = message.serialize()\n        else:\n            self.initial_connection_payload = '<k anon=\"\">'.encode()\n\n        self.connection.send_raw_data(self.initial_connection_payload)", "response": "Called when the TCP connection to kik s servers is established."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _establish_authenticated_session(self, kik_node):\n        self.kik_node = kik_node\n        log.info(\"[+] Closing current connection and creating a new authenticated one.\")\n\n        self.disconnect()\n        self._connect()", "response": "Establishes a new authenticated session to the kik servers."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef login(self, username, password, captcha_result=None):\n        self.username = username\n        self.password = password\n        login_request = login.LoginRequest(username, password, captcha_result, self.device_id_override, self.android_id_override)\n        log.info(\"[+] Logging in with username '{}' and a given password...\"\n                 .format(username, '*' * len(password)))\n        return self._send_xmpp_element(login_request)", "response": "Sends a login request to the server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register(self, email, username, password, first_name, last_name, birthday=\"1974-11-20\", captcha_result=None):\n        self.username = username\n        self.password = password\n        register_message = sign_up.RegisterRequest(email, username, password, first_name, last_name, birthday, captcha_result,\n                                                   self.device_id_override, self.android_id_override)\n        log.info(\"[+] Sending sign up request (name: {} {}, email: {})...\".format(first_name, last_name, email))\n        return self._send_xmpp_element(register_message)", "response": "Send a register request to kik with the given details."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a chat message to another person or group.", "response": "def send_chat_message(self, peer_jid: str, message: str, bot_mention_jid=None):\n        \"\"\"\n        Sends a text chat message to another person or a group with the given JID/username.\n\n        :param peer_jid: The Jabber ID for which to send the message (looks like username_ejs@talk.kik.com)\n                         If you don't know the JID of someone, you can also specify a kik username here.\n        :param message: The actual message body\n        :param bot_mention_jid: If an official bot is referenced, their jid must be embedded as mention for them\n        to respond.\n        \"\"\"\n        if self.is_group_jid(peer_jid):\n            log.info(\"[+] Sending chat message '{}' to group '{}'...\".format(message, peer_jid))\n            return self._send_xmpp_element(chatting.OutgoingGroupChatMessage(peer_jid, message, bot_mention_jid))\n        else:\n            log.info(\"[+] Sending chat message '{}' to user '{}'...\".format(message, peer_jid))\n            return self._send_xmpp_element(chatting.OutgoingChatMessage(peer_jid, message, False, bot_mention_jid))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_read_receipt(self, peer_jid: str, receipt_message_id: str, group_jid=None):\n        log.info(\"[+] Sending read receipt to JID {} for message ID {}\".format(peer_jid, receipt_message_id))\n        return self._send_xmpp_element(chatting.OutgoingReadReceipt(peer_jid, receipt_message_id, group_jid))", "response": "Sends a read receipt for a previously sent message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a receipt indicating that a specific message was received to another person.", "response": "def send_delivered_receipt(self, peer_jid: str, receipt_message_id: str):\n        \"\"\"\n        Sends a receipt indicating that a specific message was received, to another person.\n\n        :param peer_jid: The other peer's JID to send to receipt to\n        :param receipt_message_id: The message ID for which to generate the receipt\n        \"\"\"\n        log.info(\"[+] Sending delivered receipt to JID {} for message ID {}\".format(peer_jid, receipt_message_id))\n        return self._send_xmpp_element(chatting.OutgoingDeliveredReceipt(peer_jid, receipt_message_id))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_is_typing(self, peer_jid: str, is_typing: bool):\n        if self.is_group_jid(peer_jid):\n            return self._send_xmpp_element(chatting.OutgoingGroupIsTypingEvent(peer_jid, is_typing))\n        else:\n            return self._send_xmpp_element(chatting.OutgoingIsTypingEvent(peer_jid, is_typing))", "response": "Updates the is typing status of the bot during a conversation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrequest basic information of some peer JIDs.", "response": "def request_info_of_jids(self, peer_jids: Union[str, List[str]]):\n        \"\"\"\n        Requests basic information (username, display name, picture) of some peer JIDs.\n        When the information arrives, the callback on_peer_info_received() will fire.\n\n        :param peer_jids: The JID(s) for which to request the information. If you want to request information for\n                          more than one JID, supply a list of strings. Otherwise, supply a string\n        \"\"\"\n        return self._send_xmpp_element(roster.BatchPeerInfoRequest(peer_jids))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall the new format xiphias message to request user data such as profile creation date and background picture URL.", "response": "def xiphias_get_users(self, peer_jids: Union[str, List[str]]):\n        \"\"\"\n        Calls the new format xiphias message to request user data such as profile creation date\n        and background picture URL.\n\n        :param peer_jids: one jid, or a list of jids\n        \"\"\"\n        return self._send_xmpp_element(xiphias.UsersRequest(peer_jids))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nliking xiphias_get_users but for aliases instead of jids.", "response": "def xiphias_get_users_by_alias(self, alias_jids: Union[str, List[str]]):\n        \"\"\"\n        Like xiphias_get_users, but for aliases instead of jids.\n\n        :param alias_jids: one jid, or a list of jids\n        \"\"\"\n        return self._send_xmpp_element(xiphias.UsersByAliasRequest(alias_jids))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef change_group_name(self, group_jid: str, new_name: str):\n        log.info(\"[+] Requesting a group name change for JID {} to '{}'\".format(group_jid, new_name))\n        return self._send_xmpp_element(group_adminship.ChangeGroupNameRequest(group_jid, new_name))", "response": "Changes the group s name to something new"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding someone to a group", "response": "def add_peer_to_group(self, group_jid, peer_jid):\n        \"\"\"\n        Adds someone to a group\n\n        :param group_jid: The JID of the group into which to add a user\n        :param peer_jid: The JID of the user to add\n        \"\"\"\n        log.info(\"[+] Requesting to add user {} into the group {}\".format(peer_jid, group_jid))\n        return self._send_xmpp_element(group_adminship.AddToGroupRequest(group_jid, peer_jid))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the user from a group", "response": "def remove_peer_from_group(self, group_jid, peer_jid):\n        \"\"\"\n        Kicks someone out of a group\n\n        :param group_jid: The group JID from which to remove the user\n        :param peer_jid: The JID of the user to remove\n        \"\"\"\n        log.info(\"[+] Requesting removal of user {} from group {}\".format(peer_jid, group_jid))\n        return self._send_xmpp_element(group_adminship.RemoveFromGroupRequest(group_jid, peer_jid))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ban_member_from_group(self, group_jid, peer_jid):\n        log.info(\"[+] Requesting ban of user {} from group {}\".format(peer_jid, group_jid))\n        return self._send_xmpp_element(group_adminship.BanMemberRequest(group_jid, peer_jid))", "response": "Bans a member from a group"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unban_member_from_group(self, group_jid, peer_jid):\n        log.info(\"[+] Requesting un-banning of user {} from the group {}\".format(peer_jid, group_jid))\n        return self._send_xmpp_element(group_adminship.UnbanRequest(group_jid, peer_jid))", "response": "Unban a member from a group"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to join a specific group with a cryptographic token that was received earlier from a search Requested", "response": "def join_group_with_token(self, group_hashtag, group_jid, join_token):\n        \"\"\"\n        Tries to join into a specific group, using a cryptographic token that was received earlier from a search\n\n        :param group_hashtag: The public hashtag of the group into which to join (like '#Music')\n        :param group_jid: The JID of the same group\n        :param join_token: a token that can be extracted in the callback on_group_search_response, after calling\n                           search_group()\n        \"\"\"\n        log.info(\"[+] Trying to join the group '{}' with JID {}\".format(group_hashtag, group_jid))\n        return self._send_xmpp_element(roster.GroupJoinRequest(group_hashtag, join_token, group_jid))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef leave_group(self, group_jid):\n        log.info(\"[+] Leaving group {}\".format(group_jid))\n        return self._send_xmpp_element(group_adminship.LeaveGroupRequest(group_jid))", "response": "Leaves a specific group"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npromote some group member into an admin.", "response": "def promote_to_admin(self, group_jid, peer_jid):\n        \"\"\"\n        Turns some group member into an admin\n\n        :param group_jid: The group JID for which the member will become an admin\n        :param peer_jid: The JID of user to turn into an admin\n        \"\"\"\n        log.info(\"[+] Promoting user {} to admin in group {}\".format(peer_jid, group_jid))\n        return self._send_xmpp_element(group_adminship.PromoteToAdminRequest(group_jid, peer_jid))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef demote_admin(self, group_jid, peer_jid):\n        log.info(\"[+] Demoting user {} to a regular member in group {}\".format(peer_jid, group_jid))\n        return self._send_xmpp_element(group_adminship.DemoteAdminRequest(group_jid, peer_jid))", "response": "Demotes an admin of a group into a regular member."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_members(self, group_jid, peer_jids: Union[str, List[str]]):\n        log.info(\"[+] Adding some members to the group {}\".format(group_jid))\n        return self._send_xmpp_element(group_adminship.AddMembersRequest(group_jid, peer_jids))", "response": "Adds multiple users to a specific group at once"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef search_group(self, search_query):\n        log.info(\"[+] Initiating a search for groups using the query '{}'\".format(search_query))\n        return self._send_xmpp_element(roster.GroupSearchRequest(search_query))", "response": "Search for public groups using a query"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if the given username is available for registration.", "response": "def check_username_uniqueness(self, username):\n        \"\"\"\n        Checks if the given username is available for registration.\n        Results are returned in the on_username_uniqueness_received() callback\n\n        :param username: The username to check for its existence\n        \"\"\"\n        log.info(\"[+] Checking for Uniqueness of username '{}'\".format(username))\n        return self._send_xmpp_element(sign_up.CheckUsernameUniquenessRequest(username))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the profile picture to the given filename", "response": "def set_profile_picture(self, filename):\n        \"\"\"\n        Sets the profile picture\n\n        :param filename: The filename on disk of the image to set\n        \"\"\"\n        log.info(\"[+] Setting the profile picture to file '{}'\".format(filename))\n        profile_pictures.set_profile_picture(filename, self.kik_node + '@talk.kik.com', self.username, self.password)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_captcha_result(self, stc_id, captcha_result):\n        log.info(\"[+] Trying to solve a captcha with result: '{}'\".format(captcha_result))\n        return self._send_xmpp_element(login.CaptchaSolveRequest(stc_id, captcha_result))", "response": "Send a CaptchaSolveRequest to the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef change_display_name(self, first_name, last_name):\n        log.info(\"[+] Changing the display name to '{} {}'\".format(first_name, last_name))\n        return self._send_xmpp_element(account.ChangeNameRequest(first_name, last_name))", "response": "Changes the display name of the resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef change_password(self, new_password, email):\n        log.info(\"[+] Changing the password of the account\")\n        return self._send_xmpp_element(account.ChangePasswordRequest(self.password, new_password, email, self.username))", "response": "Changes the login password of the account"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the email of the current account", "response": "def change_email(self, old_email, new_email):\n        \"\"\"\n        Changes the email of the current account\n\n        :param old_email: The current email\n        :param new_email: The new email to set\n        \"\"\"\n        log.info(\"[+] Changing account email to '{}'\".format(new_email))\n        return self._send_xmpp_element(account.ChangeEmailRequest(self.password, old_email, new_email))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nserializing and sends the given XMPP element to kik servers and returns the UUID of the element that was sent.", "response": "def _send_xmpp_element(self, xmpp_element: XMPPElement):\n        \"\"\"\n        Serializes and sends the given XMPP element to kik servers\n\n        :param xmpp_element: The XMPP element to send\n        :return: The UUID of the element that was sent\n        \"\"\"\n        while not self.connected:\n            log.debug(\"[!] Waiting for connection.\")\n            time.sleep(0.1)\n\n        self.loop.call_soon_threadsafe(self.connection.send_raw_data, (xmpp_element.serialize()))\n        return xmpp_element.message_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when a new data is received from the kik server.", "response": "def _on_new_data_received(self, data: bytes):\n        \"\"\"\n        Gets called whenever we get a whole new XML element from kik's servers.\n        :param data: The data received (bytes)\n        \"\"\"\n        if data == b' ':\n            # Happens every half hour. Disconnect after 10th time. Some kind of keep-alive? Let's send it back.\n            self.loop.call_soon_threadsafe(self.connection.send_raw_data, b' ')\n            return\n\n        xml_element = BeautifulSoup(data.decode(), features='xml')\n        xml_element = next(iter(xml_element)) if len(xml_element) > 0 else xml_element\n\n        # choose the handler based on the XML tag name\n\n        if xml_element.name == \"k\":\n            self._handle_received_k_element(xml_element)\n        if xml_element.name == \"iq\":\n            self._handle_received_iq_element(xml_element)\n        elif xml_element.name == \"message\":\n            self._handle_xmpp_message(xml_element)\n        elif xml_element.name == 'stc':\n            self.callback.on_captcha_received(login.CaptchaElement(xml_element))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _handle_received_k_element(self, k_element: BeautifulSoup):\n        if k_element['ok'] == \"1\":\n            self.connected = True\n\n            if 'ts' in k_element.attrs:\n                # authenticated!\n                log.info(\"[+] Authenticated successfully.\")\n                self.authenticated = True\n                self.callback.on_authenticated()\n            elif self.should_login_on_connection:\n                self.login(self.username, self.password)\n                self.should_login_on_connection = False\n        else:\n            self.callback.on_connection_failed(login.ConnectionFailedResponse(k_element))", "response": "Handle the k element received from the kik server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_xmpp_message(self, xmpp_message: BeautifulSoup):\n        if 'xmlns' in xmpp_message.attrs:\n            self._handle_xmlns(xmpp_message['xmlns'], xmpp_message)\n        elif xmpp_message['type'] == 'receipt':\n            if xmpp_message.g:\n                self.callback.on_group_receipts_received(chatting.IncomingGroupReceiptsEvent(xmpp_message))\n            else:\n                self.xml_namespace_handlers['jabber:client'].handle(xmpp_message)\n        else:\n            # iPads send messages without xmlns, try to handle it as jabber:client\n            self.xml_namespace_handlers['jabber:client'].handle(xmpp_message)", "response": "Handles a message from the XMPP server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle_cancellation(session: CommandSession):\n\n    def control(value):\n        if _is_cancellation(value) is True:\n            session.finish(render_expression(\n                session.bot.config.SESSION_CANCEL_EXPRESSION))\n        return value\n\n    return control", "response": "If the input is a string of cancellation word, finish the command session."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses a command string into a list of Command objects and current arg string.", "response": "def parse_command(bot: NoneBot,\n                  cmd_string: str) -> Tuple[Optional[Command], Optional[str]]:\n    \"\"\"\n    Parse a command string (typically from a message).\n\n    :param bot: NoneBot instance\n    :param cmd_string: command string\n    :return: (Command object, current arg string)\n    \"\"\"\n    logger.debug(f'Parsing command: {cmd_string}')\n\n    matched_start = None\n    for start in bot.config.COMMAND_START:\n        # loop through COMMAND_START to find the longest matched start\n        curr_matched_start = None\n        if isinstance(start, type(re.compile(''))):\n            m = start.search(cmd_string)\n            if m and m.start(0) == 0:\n                curr_matched_start = m.group(0)\n        elif isinstance(start, str):\n            if cmd_string.startswith(start):\n                curr_matched_start = start\n\n        if curr_matched_start is not None and \\\n                (matched_start is None or\n                 len(curr_matched_start) > len(matched_start)):\n            # a longer start, use it\n            matched_start = curr_matched_start\n\n    if matched_start is None:\n        # it's not a command\n        logger.debug('It\\'s not a command')\n        return None, None\n\n    logger.debug(f'Matched command start: '\n                 f'{matched_start}{\"(empty)\" if not matched_start else \"\"}')\n    full_command = cmd_string[len(matched_start):].lstrip()\n\n    if not full_command:\n        # command is empty\n        return None, None\n\n    cmd_name_text, *cmd_remained = full_command.split(maxsplit=1)\n    cmd_name = _aliases.get(cmd_name_text)\n\n    if not cmd_name:\n        for sep in bot.config.COMMAND_SEP:\n            # loop through COMMAND_SEP to find the most optimized split\n            curr_cmd_name = None\n            if isinstance(sep, type(re.compile(''))):\n                curr_cmd_name = tuple(sep.split(cmd_name_text))\n            elif isinstance(sep, str):\n                curr_cmd_name = tuple(cmd_name_text.split(sep))\n\n            if curr_cmd_name is not None and \\\n                    (not cmd_name or len(curr_cmd_name) > len(cmd_name)):\n                # a more optimized split, use it\n                cmd_name = curr_cmd_name\n\n        if not cmd_name:\n            cmd_name = (cmd_name_text,)\n\n    logger.debug(f'Split command name: {cmd_name}')\n    cmd = _find_command(cmd_name)\n    if not cmd:\n        logger.debug(f'Command {cmd_name} not found')\n        return None, None\n\n    logger.debug(f'Command {cmd.name} found, function: {cmd.func}')\n    return cmd, ''.join(cmd_remained)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle a message as a command.", "response": "async def handle_command(bot: NoneBot, ctx: Context_T) -> bool:\n    \"\"\"\n    Handle a message as a command.\n\n    This function is typically called by \"handle_message\".\n\n    :param bot: NoneBot instance\n    :param ctx: message context\n    :return: the message is handled as a command\n    \"\"\"\n    cmd, current_arg = parse_command(bot, str(ctx['message']).lstrip())\n    is_privileged_cmd = cmd and cmd.privileged\n    if is_privileged_cmd and cmd.only_to_me and not ctx['to_me']:\n        is_privileged_cmd = False\n    disable_interaction = is_privileged_cmd\n\n    if is_privileged_cmd:\n        logger.debug(f'Command {cmd.name} is a privileged command')\n\n    ctx_id = context_id(ctx)\n\n    if not is_privileged_cmd:\n        # wait for 1.5 seconds (at most) if the current session is running\n        retry = 5\n        while retry > 0 and \\\n                _sessions.get(ctx_id) and _sessions[ctx_id].running:\n            retry -= 1\n            await asyncio.sleep(0.3)\n\n    check_perm = True\n    session = _sessions.get(ctx_id) if not is_privileged_cmd else None\n    if session:\n        if session.running:\n            logger.warning(f'There is a session of command '\n                           f'{session.cmd.name} running, notify the user')\n            asyncio.ensure_future(send(\n                bot, ctx,\n                render_expression(bot.config.SESSION_RUNNING_EXPRESSION)\n            ))\n            # pretend we are successful, so that NLP won't handle it\n            return True\n\n        if session.is_valid:\n            logger.debug(f'Session of command {session.cmd.name} exists')\n            # since it's in a session, the user must be talking to me\n            ctx['to_me'] = True\n            session.refresh(ctx, current_arg=str(ctx['message']))\n            # there is no need to check permission for existing session\n            check_perm = False\n        else:\n            # the session is expired, remove it\n            logger.debug(f'Session of command {session.cmd.name} is expired')\n            if ctx_id in _sessions:\n                del _sessions[ctx_id]\n            session = None\n\n    if not session:\n        if not cmd:\n            logger.debug('Not a known command, ignored')\n            return False\n        if cmd.only_to_me and not ctx['to_me']:\n            logger.debug('Not to me, ignored')\n            return False\n        session = CommandSession(bot, ctx, cmd, current_arg=current_arg)\n        logger.debug(f'New session of command {session.cmd.name} created')\n\n    return await _real_run_command(session, ctx_id, check_perm=check_perm,\n                                   disable_interaction=disable_interaction)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def call_command(bot: NoneBot, ctx: Context_T,\n                       name: Union[str, CommandName_T], *,\n                       current_arg: str = '',\n                       args: Optional[CommandArgs_T] = None,\n                       check_perm: bool = True,\n                       disable_interaction: bool = False) -> bool:\n    \"\"\"\n    Call a command internally.\n\n    This function is typically called by some other commands\n    or \"handle_natural_language\" when handling NLPResult object.\n\n    Note: If disable_interaction is not True, after calling this function,\n    any previous command session will be overridden, even if the command\n    being called here does not need further interaction (a.k.a asking\n    the user for more info).\n\n    :param bot: NoneBot instance\n    :param ctx: message context\n    :param name: command name\n    :param current_arg: command current argument string\n    :param args: command args\n    :param check_perm: should check permission before running command\n    :param disable_interaction: disable the command's further interaction\n    :return: the command is successfully called\n    \"\"\"\n    cmd = _find_command(name)\n    if not cmd:\n        return False\n    session = CommandSession(bot, ctx, cmd, current_arg=current_arg, args=args)\n    return await _real_run_command(session, context_id(session.ctx),\n                                   check_perm=check_perm,\n                                   disable_interaction=disable_interaction)", "response": "Call a command internally."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nkill the current session of the given context.", "response": "def kill_current_session(ctx: Context_T) -> None:\n    \"\"\"\n    Force kill current session of the given context,\n    despite whether it is running or not.\n\n    :param ctx: message context\n    \"\"\"\n    ctx_id = context_id(ctx)\n    if ctx_id in _sessions:\n        del _sessions[ctx_id]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def run(self, session, *,\n                  check_perm: bool = True,\n                  dry: bool = False) -> bool:\n        \"\"\"\n        Run the command in a given session.\n\n        :param session: CommandSession object\n        :param check_perm: should check permission before running\n        :param dry: just check any prerequisite, without actually running\n        :return: the command is finished (or can be run, given dry == True)\n        \"\"\"\n        has_perm = await self._check_perm(session) if check_perm else True\n        if self.func and has_perm:\n            if dry:\n                return True\n\n            if session.current_arg_filters is not None and \\\n                    session.current_key is not None:\n                # argument-level filters are given, use them\n                arg = session.current_arg\n                config = session.bot.config\n                for f in session.current_arg_filters:\n                    try:\n                        res = f(arg)\n                        if isinstance(res, Awaitable):\n                            res = await res\n                        arg = res\n                    except ValidateError as e:\n                        # validation failed\n                        if config.MAX_VALIDATION_FAILURES > 0:\n                            # should check number of validation failures\n                            session.state['__validation_failure_num'] = \\\n                                session.state.get(\n                                    '__validation_failure_num', 0) + 1\n\n                            if session.state['__validation_failure_num'] >= \\\n                                    config.MAX_VALIDATION_FAILURES:\n                                # noinspection PyProtectedMember\n                                session.finish(render_expression(\n                                    config.TOO_MANY_VALIDATION_FAILURES_EXPRESSION\n                                ), **session._current_send_kwargs)\n\n                        failure_message = e.message\n                        if failure_message is None:\n                            failure_message = render_expression(\n                                config.DEFAULT_VALIDATION_FAILURE_EXPRESSION\n                            )\n                        # noinspection PyProtectedMember\n                        session.pause(failure_message,\n                                      **session._current_send_kwargs)\n\n                # passed all filters\n                session.state[session.current_key] = arg\n            else:\n                # fallback to command-level args_parser_func\n                if self.args_parser_func:\n                    await self.args_parser_func(session)\n                if session.current_key is not None and \\\n                        session.current_key not in session.state:\n                    # args_parser_func didn't set state, here we set it\n                    session.state[session.current_key] = session.current_arg\n\n            await self.func(session)\n            return True\n        return False", "response": "Run the command in a given command session."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the session has sufficient permission to read the object s cache.", "response": "async def _check_perm(self, session) -> bool:\n        \"\"\"\n        Check if the session has sufficient permission to\n        call the command.\n\n        :param session: CommandSession object\n        :return: the session has the permission\n        \"\"\"\n        return await perm.check_permission(session.bot, session.ctx,\n                                           self.permission)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_valid(self) -> bool:\n        if self.bot.config.SESSION_EXPIRE_TIMEOUT and \\\n                self._last_interaction and \\\n                datetime.now() - self._last_interaction > \\\n                self.bot.config.SESSION_EXPIRE_TIMEOUT:\n            return False\n        return True", "response": "Check if the session is expired or not."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef current_arg_images(self) -> List[str]:\n        if self._current_arg_images is None:\n            self._current_arg_images = [\n                s.data['url'] for s in Message(self.current_arg)\n                if s.type == 'image' and 'url' in s.data\n            ]\n        return self._current_arg_images", "response": "Returns the list of images in the current argument."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef refresh(self, ctx: Context_T, *, current_arg: str = '') -> None:\n        self.ctx = ctx\n        self.current_arg = current_arg\n        self._current_arg_text = None\n        self._current_arg_images = None", "response": "Refresh the session with a new message context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting an argument with a given key.", "response": "def get(self, key: str, *,\n            prompt: Optional[Message_T] = None,\n            arg_filters: Optional[List[Filter_T]] = None,\n            **kwargs) -> Any:\n        \"\"\"\n        Get an argument with a given key.\n\n        If the argument does not exist in the current session,\n        a pause exception will be raised, and the caller of\n        the command will know it should keep the session for\n        further interaction with the user.\n\n        :param key: argument key\n        :param prompt: prompt to ask the user\n        :param arg_filters: argument filters for the next user input\n        :return: the argument value\n        \"\"\"\n        if key in self.state:\n            return self.state[key]\n\n        self.current_key = key\n        self.current_arg_filters = arg_filters\n        self._current_send_kwargs = kwargs\n        self.pause(prompt, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget an optional argument with given key.", "response": "def get_optional(self, key: str,\n                     default: Optional[Any] = None) -> Optional[Any]:\n        \"\"\"\n        Simply get a argument with given key.\n\n        Deprecated. Use `session.state.get()` instead.\n        \"\"\"\n        return self.state.get(key, default)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pause(self, message: Optional[Message_T] = None, **kwargs) -> None:\n        if message:\n            asyncio.ensure_future(self.send(message, **kwargs))\n        raise _PauseException", "response": "Pause the session for further interaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef switch(self, new_ctx_message: Message_T) -> None:\n        if self.is_first_run:\n            # if calling this method during first run,\n            # we think the command is not handled\n            raise _FinishException(result=False)\n\n        if not isinstance(new_ctx_message, Message):\n            new_ctx_message = Message(new_ctx_message)\n        raise SwitchException(new_ctx_message)", "response": "Switches the current session to a new message context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def approve(self, remark: str = '') -> None:\n        try:\n            await self.bot.call_action(\n                action='.handle_quick_operation_async',\n                self_id=self.ctx.get('self_id'),\n                context=self.ctx,\n                operation={'approve': True, 'remark': remark}\n            )\n        except CQHttpError:\n            pass", "response": "Approve the request.\n\n        :param remark: remark of friend (only works in friend request)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def reject(self, reason: str = '') -> None:\n        try:\n            await self.bot.call_action(\n                action='.handle_quick_operation_async',\n                self_id=self.ctx.get('self_id'),\n                context=self.ctx,\n                operation={'approve': False, 'reason': reason}\n            )\n        except CQHttpError:\n            pass", "response": "Reject the request.\n\n        :param reason: reason to reject (only works in group request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef init(config_object: Optional[Any] = None) -> None:\n    global _bot\n    _bot = NoneBot(config_object)\n\n    if _bot.config.DEBUG:\n        logger.setLevel(logging.DEBUG)\n    else:\n        logger.setLevel(logging.INFO)\n\n    _bot.server_app.before_serving(_start_scheduler)", "response": "Initialize NoneBot instance.\n\n    This function must be called at the very beginning of code,\n    otherwise the get_bot() function will return None and nothing\n    is gonna work properly.\n\n    :param config_object: configuration object"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef run(host: Optional[str] = None, port: Optional[int] = None,\n        *args, **kwargs) -> None:\n    \"\"\"Run the NoneBot instance.\"\"\"\n    get_bot().run(host=host, port=port, *args, **kwargs)", "response": "Run the NoneBot instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef context_id(ctx: Context_T, *,\n               mode: str = 'default', use_hash: bool = False) -> str:\n    \"\"\"\n    Calculate a unique id representing the current context.\n\n    mode:\n      default: one id for one context\n      group: one id for one group or discuss\n      user: one id for one user\n\n    :param ctx: the context dict\n    :param mode: unique id mode: \"default\", \"group\", or \"user\"\n    :param use_hash: use md5 to hash the id or not\n    \"\"\"\n    ctx_id = ''\n    if mode == 'default':\n        if ctx.get('group_id'):\n            ctx_id = f'/group/{ctx[\"group_id\"]}'\n        elif ctx.get('discuss_id'):\n            ctx_id = f'/discuss/{ctx[\"discuss_id\"]}'\n        if ctx.get('user_id'):\n            ctx_id += f'/user/{ctx[\"user_id\"]}'\n    elif mode == 'group':\n        if ctx.get('group_id'):\n            ctx_id = f'/group/{ctx[\"group_id\"]}'\n        elif ctx.get('discuss_id'):\n            ctx_id = f'/discuss/{ctx[\"discuss_id\"]}'\n        elif ctx.get('user_id'):\n            ctx_id = f'/user/{ctx[\"user_id\"]}'\n    elif mode == 'user':\n        if ctx.get('user_id'):\n            ctx_id = f'/user/{ctx[\"user_id\"]}'\n\n    if ctx_id and use_hash:\n        ctx_id = hashlib.md5(ctx_id.encode('ascii')).hexdigest()\n    return ctx_id", "response": "Calculate a unique id representing the current context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend a message to a node.", "response": "async def send(bot: NoneBot, ctx: Context_T,\n               message: Message_T, *,\n               ensure_private: bool = False,\n               ignore_failure: bool = True,\n               **kwargs) -> Any:\n    \"\"\"Send a message ignoring failure by default.\"\"\"\n    try:\n        if ensure_private:\n            ctx = ctx.copy()\n            ctx['message_type'] = 'private'\n        return await bot.send(ctx, message, **kwargs)\n    except CQHttpError:\n        if not ignore_failure:\n            raise\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender an expression to message string.", "response": "def render_expression(expr: Expression_T, *args,\n                      escape_args: bool = True, **kwargs) -> str:\n    \"\"\"\n    Render an expression to message string.\n\n    :param expr: expression to render\n    :param escape_args: should escape arguments or not\n    :param args: positional arguments used in str.format()\n    :param kwargs: keyword arguments used in str.format()\n    :return: the rendered message\n    \"\"\"\n    if isinstance(expr, Callable):\n        expr = expr(*args, **kwargs)\n    elif isinstance(expr, Sequence) and not isinstance(expr, str):\n        expr = random.choice(expr)\n    if escape_args:\n        for k, v in kwargs.items():\n            if isinstance(v, str):\n                kwargs[k] = escape(v)\n    return expr.format(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a module as a plugin.", "response": "def load_plugin(module_name: str) -> bool:\n    \"\"\"\n    Load a module as a plugin.\n\n    :param module_name: name of module to import\n    :return: successful or not\n    \"\"\"\n    try:\n        module = importlib.import_module(module_name)\n        name = getattr(module, '__plugin_name__', None)\n        usage = getattr(module, '__plugin_usage__', None)\n        _plugins.add(Plugin(module, name, usage))\n        logger.info(f'Succeeded to import \"{module_name}\"')\n        return True\n    except Exception as e:\n        logger.error(f'Failed to import \"{module_name}\", error: {e}')\n        logger.exception(e)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_plugins(plugin_dir: str, module_prefix: str) -> int:\n    count = 0\n    for name in os.listdir(plugin_dir):\n        path = os.path.join(plugin_dir, name)\n        if os.path.isfile(path) and \\\n                (name.startswith('_') or not name.endswith('.py')):\n            continue\n        if os.path.isdir(path) and \\\n                (name.startswith('_') or not os.path.exists(\n                    os.path.join(path, '__init__.py'))):\n            continue\n\n        m = re.match(r'([_A-Z0-9a-z]+)(.py)?', name)\n        if not m:\n            continue\n\n        if load_plugin(f'{module_prefix}.{m.group(1)}'):\n            count += 1\n    return count", "response": "Find all non - hidden modules or packages in a given directory and import them with the given module prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_builtin_plugins() -> int:\n    plugin_dir = os.path.join(os.path.dirname(__file__), 'plugins')\n    return load_plugins(plugin_dir, 'nonebot.plugins')", "response": "Load built - in plugins distributed along with nonebot package."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts all image urls from a message - like object.", "response": "def _extract_image_urls(arg: Message_T) -> List[str]:\n    \"\"\"Extract all image urls from a message-like object.\"\"\"\n    arg_as_msg = Message(arg)\n    return [s.data['url'] for s in arg_as_msg\n            if s.type == 'image' and 'url' in s.data]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nextracts all numbers from a message - like object.", "response": "def _extract_numbers(arg: Message_T) -> List[float]:\n    \"\"\"Extract all numbers (integers and floats) from a message-like object.\"\"\"\n    s = str(arg)\n    return list(map(float, re.findall(r'[+-]?(\\d*\\.?\\d+|\\d+\\.?\\d*)', s)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef on_natural_language(keywords: Union[Optional[Iterable], Callable] = None,\n                        *, permission: int = perm.EVERYBODY,\n                        only_to_me: bool = True,\n                        only_short_message: bool = True,\n                        allow_empty_message: bool = False) -> Callable:\n    \"\"\"\n    Decorator to register a function as a natural language processor.\n\n    :param keywords: keywords to respond to, if None, respond to all messages\n    :param permission: permission required by the processor\n    :param only_to_me: only handle messages to me\n    :param only_short_message: only handle short messages\n    :param allow_empty_message: handle empty messages\n    \"\"\"\n\n    def deco(func: Callable) -> Callable:\n        nl_processor = NLProcessor(func=func, keywords=keywords,\n                                   permission=permission,\n                                   only_to_me=only_to_me,\n                                   only_short_message=only_short_message,\n                                   allow_empty_message=allow_empty_message)\n        _nl_processors.add(nl_processor)\n        return func\n\n    if isinstance(keywords, Callable):\n        # here \"keywords\" is the function to be decorated\n        return on_natural_language()(keywords)\n    else:\n        return deco", "response": "Decorator to register a function as a natural language processor."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle a message as natural language.", "response": "async def handle_natural_language(bot: NoneBot, ctx: Context_T) -> bool:\n    \"\"\"\n    Handle a message as natural language.\n\n    This function is typically called by \"handle_message\".\n\n    :param bot: NoneBot instance\n    :param ctx: message context\n    :return: the message is handled as natural language\n    \"\"\"\n    session = NLPSession(bot, ctx, str(ctx['message']))\n\n    # use msg_text here because CQ code \"share\" may be very long,\n    # at the same time some plugins may want to handle it\n    msg_text_length = len(session.msg_text)\n\n    futures = []\n    for p in _nl_processors:\n        if not p.allow_empty_message and not session.msg:\n            # don't allow empty msg, but it is one, so skip to next\n            continue\n\n        if p.only_short_message and \\\n                msg_text_length > bot.config.SHORT_MESSAGE_MAX_LENGTH:\n            continue\n\n        if p.only_to_me and not ctx['to_me']:\n            continue\n\n        should_run = await perm.check_permission(bot, ctx, p.permission)\n        if should_run and p.keywords:\n            for kw in p.keywords:\n                if kw in session.msg_text:\n                    break\n            else:\n                # no keyword matches\n                should_run = False\n\n        if should_run:\n            futures.append(asyncio.ensure_future(p.func(session)))\n\n    if futures:\n        # wait for intent commands, and sort them by confidence\n        intent_commands = []\n        for fut in futures:\n            try:\n                res = await fut\n                if isinstance(res, NLPResult):\n                    intent_commands.append(res.to_intent_command())\n                elif isinstance(res, IntentCommand):\n                    intent_commands.append(res)\n            except Exception as e:\n                logger.error('An exception occurred while running '\n                             'some natural language processor:')\n                logger.exception(e)\n\n        intent_commands.sort(key=lambda ic: ic.confidence, reverse=True)\n        logger.debug(f'Intent commands: {intent_commands}')\n\n        if intent_commands and intent_commands[0].confidence >= 60.0:\n            # choose the intent command with highest confidence\n            chosen_cmd = intent_commands[0]\n            logger.debug(\n                f'Intent command with highest confidence: {chosen_cmd}')\n            return await call_command(\n                bot, ctx, chosen_cmd.name,\n                args=chosen_cmd.args,\n                current_arg=chosen_cmd.current_arg,\n                check_perm=False\n            )\n        else:\n            logger.debug('No intent command has enough confidence')\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a message to the current context.", "response": "async def send(self, message: Message_T, *,\n                   at_sender: bool = False,\n                   ensure_private: bool = False,\n                   ignore_failure: bool = True,\n                   **kwargs) -> None:\n        \"\"\"\n        Send a message ignoring failure by default.\n\n        :param message: message to send\n        :param at_sender: @ the sender if in group or discuss chat\n        :param ensure_private: ensure the message is sent to private chat\n        :param ignore_failure: if any CQHttpError raised, ignore it\n        :return: the result returned by CQHTTP\n        \"\"\"\n        return await send(self.bot, self.ctx, message,\n                          at_sender=at_sender,\n                          ensure_private=ensure_private,\n                          ignore_failure=ignore_failure, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _simple_chinese_to_bool(text: str) -> Optional[bool]:\n    text = text.strip().lower().replace(' ', '') \\\n        .rstrip(',.!?~\uff0c\u3002\uff01\uff1f\uff5e\u4e86\u7684\u5462\u5427\u5440\u554a\u5457\u5566')\n    if text in {'\u8981', '\u7528', '\u662f', '\u597d', '\u5bf9', '\u55ef', '\u884c',\n                'ok', 'okay', 'yeah', 'yep',\n                '\u5f53\u771f', '\u5f53\u7136', '\u5fc5\u987b', '\u53ef\u4ee5', '\u80af\u5b9a', '\u6ca1\u9519', '\u786e\u5b9a', '\u786e\u8ba4'}:\n        return True\n    if text in {'\u4e0d', '\u4e0d\u8981', '\u4e0d\u7528', '\u4e0d\u662f', '\u5426', '\u4e0d\u597d', '\u4e0d\u5bf9', '\u4e0d\u884c', '\u522b',\n                'no', 'nono', 'nonono', 'nope', '\u4e0dok', '\u4e0d\u53ef\u4ee5', '\u4e0d\u80fd',\n                '\u4e0d\u53ef\u4ee5'}:\n        return False\n    return None", "response": "Convert a simple chinese text to boolean."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def check_permission(bot: NoneBot, ctx: Context_T,\n                           permission_required: int) -> bool:\n    \"\"\"\n    Check if the context has the permission required.\n\n    :param bot: NoneBot instance\n    :param ctx: message context\n    :param permission_required: permission required\n    :return: the context has the permission\n    \"\"\"\n    min_ctx_kwargs = {}\n    for field in _min_context_fields:\n        if field in ctx:\n            min_ctx_kwargs[field] = ctx[field]\n        else:\n            min_ctx_kwargs[field] = None\n    min_ctx = _MinContext(**min_ctx_kwargs)\n    return await _check(bot, min_ctx, permission_required)", "response": "Check if the context has the permission required."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef not_empty(message=None) -> Filter_T:\n\n    def validate(value):\n        if value is None:\n            _raise_failure(message)\n        if hasattr(value, '__len__') and value.__len__() == 0:\n            _raise_failure(message)\n        return value\n\n    return validate", "response": "Validate any object to ensure it s not empty."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fit_size(min_length: int = 0, max_length: int = None,\n             message=None) -> Filter_T:\n    \"\"\"\n    Validate any sized object to ensure the size/length\n    is in a given range [min_length, max_length].\n    \"\"\"\n\n    def validate(value):\n        length = len(value) if value is not None else 0\n        if length < min_length or \\\n                (max_length is not None and length > max_length):\n            _raise_failure(message)\n        return value\n\n    return validate", "response": "Validate any sized object to ensure the size is in a given range [ min_length max_length )."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating any string object to ensure it matches a given pattern.", "response": "def match_regex(pattern: str, message=None, *, flags=0,\n                fullmatch: bool = False) -> Filter_T:\n    \"\"\"\n    Validate any string object to ensure it matches a given pattern.\n    \"\"\"\n\n    pattern = re.compile(pattern, flags)\n\n    def validate(value):\n        if fullmatch:\n            if not re.fullmatch(pattern, value):\n                _raise_failure(message)\n        else:\n            if not re.match(pattern, value):\n                _raise_failure(message)\n        return value\n\n    return validate"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates any object to ensure that the result of applying a boolean function to it is True.", "response": "def ensure_true(bool_func: Callable[[Any], bool],\n                message=None) -> Filter_T:\n    \"\"\"\n    Validate any object to ensure the result of applying\n    a boolean function to it is True.\n    \"\"\"\n\n    def validate(value):\n        if bool_func(value) is not True:\n            _raise_failure(message)\n        return value\n\n    return validate"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates any comparable object to ensure it s between start and end inclusively.", "response": "def between_inclusive(start=None, end=None, message=None) -> Filter_T:\n    \"\"\"\n    Validate any comparable object to ensure it's between\n    `start` and `end` inclusively.\n    \"\"\"\n\n    def validate(value):\n        if start is not None and value < start:\n            _raise_failure(message)\n        if end is not None and end < value:\n            _raise_failure(message)\n        return value\n\n    return validate"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the source has what looks like a docstring that is not at the beginning of the source. Returns nonzero if the source has what looks like a docstring that is not at the beginning of the source.", "response": "def check_docstring_first(src, filename='<unknown>'):\n    # type: (bytes, str) -> int\n    \"\"\"Returns nonzero if the source has what looks like a docstring that is\n    not at the beginning of the source.\n\n    A string will be considered a docstring if it is a STRING token with a\n    col offset of 0.\n    \"\"\"\n    found_docstring_line = None\n    found_code_line = None\n\n    tok_gen = tokenize_tokenize(io.BytesIO(src).readline)\n    for tok_type, _, (sline, scol), _, _ in tok_gen:\n        # Looks like a docstring!\n        if tok_type == tokenize.STRING and scol == 0:\n            if found_docstring_line is not None:\n                print(\n                    '{}:{} Multiple module docstrings '\n                    '(first docstring on line {}).'.format(\n                        filename, sline, found_docstring_line,\n                    ),\n                )\n                return 1\n            elif found_code_line is not None:\n                print(\n                    '{}:{} Module docstring appears after code '\n                    '(code seen on line {}).'.format(\n                        filename, sline, found_code_line,\n                    ),\n                )\n                return 1\n            else:\n                found_docstring_line = sline\n        elif tok_type not in NON_CODE_TOKENS and found_code_line is None:\n            found_code_line = sline\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_aws_secrets_from_env():  # type: () -> Set[str]\n    keys = set()\n    for env_var in (\n        'AWS_SECRET_ACCESS_KEY', 'AWS_SECURITY_TOKEN', 'AWS_SESSION_TOKEN',\n    ):\n        if env_var in os.environ:\n            keys.add(os.environ[env_var])\n    return keys", "response": "Extract AWS secrets from environment variables."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts AWS secrets from a configuration file.", "response": "def get_aws_secrets_from_file(credentials_file):  # type: (str) -> Set[str]\n    \"\"\"Extract AWS secrets from configuration files.\n\n    Read an ini-style configuration file and return a set with all found AWS\n    secret access keys.\n    \"\"\"\n    aws_credentials_file_path = os.path.expanduser(credentials_file)\n    if not os.path.exists(aws_credentials_file_path):\n        return set()\n\n    parser = configparser.ConfigParser()\n    try:\n        parser.read(aws_credentials_file_path)\n    except configparser.MissingSectionHeaderError:\n        return set()\n\n    keys = set()\n    for section in parser.sections():\n        for var in (\n            'aws_secret_access_key', 'aws_security_token',\n            'aws_session_token',\n        ):\n            try:\n                key = parser.get(section, var).strip()\n                if key:\n                    keys.add(key)\n            except configparser.NoOptionError:\n                pass\n    return keys"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if files contain AWS secrets and keys.", "response": "def check_file_for_aws_keys(filenames, keys):\n    # type: (Sequence[str], Set[str]) -> List[Dict[str, str]]\n    \"\"\"Check if files contain AWS secrets.\n\n    Return a list of all files containing AWS secrets and keys found, with all\n    but the first four characters obfuscated to ease debugging.\n    \"\"\"\n    bad_files = []\n\n    for filename in filenames:\n        with open(filename, 'r') as content:\n            text_body = content.read()\n            for key in keys:\n                # naively match the entire file, low chance of incorrect\n                # collision\n                if key in text_body:\n                    bad_files.append({\n                        'filename': filename, 'key': key[:4] + '*' * 28,\n                    })\n    return bad_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds breakpoint to debug list", "response": "def visit_Call(self, node):  # type: (ast.Call) -> None\n        \"\"\"python3.7+ breakpoint()\"\"\"\n        if isinstance(node.func, ast.Name) and node.func.id == 'breakpoint':\n            st = Debug(node.lineno, node.col_offset, node.func.id, 'called')\n            self.breakpoints.append(st)\n        self.generic_visit(node)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sort(lines):  # type: (List[str]) -> List[str]\n    # make a copy of lines since we will clobber it\n    lines = list(lines)\n    new_lines = parse_block(lines, header=True)\n\n    for block in sorted(parse_blocks(lines), key=first_key):\n        if new_lines:\n            new_lines.append('')\n        new_lines.extend(block)\n\n    return new_lines", "response": "Sort a YAML file in alphabetical order keeping blocks together."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_block(lines, header=False):  # type: (List[str], bool) -> List[str]\n    block_lines = []\n    while lines and lines[0] and (not header or lines[0].startswith('#')):\n        block_lines.append(lines.pop(0))\n    return block_lines", "response": "Parse and return a single block."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_blocks(lines):  # type: (List[str]) -> List[List[str]]\n    blocks = []\n\n    while lines:\n        if lines[0] == '':\n            lines.pop(0)\n        else:\n            blocks.append(parse_block(lines))\n\n    return blocks", "response": "Parse and return all possible blocks in a list of lines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the first YAML key in a list of lines.", "response": "def first_key(lines):  # type: (List[str]) -> str\n    \"\"\"Returns a string representing the sort key of a block.\n\n    The sort key is the first YAML key we encounter, ignoring comments, and\n    stripping leading quotes.\n\n    >>> print(test)\n    # some comment\n    'foo': true\n    >>> first_key(test)\n    'foo'\n    \"\"\"\n    for line in lines:\n        if line.startswith('#'):\n            continue\n        if any(line.startswith(quote) for quote in QUOTES):\n            return line[1:]\n        return line\n    else:\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Redis Sentinel client.", "response": "async def create_sentinel(sentinels, *, db=None, password=None,\n                          encoding=None, minsize=1, maxsize=10,\n                          ssl=None, timeout=0.2, loop=None):\n    \"\"\"Creates Redis Sentinel client.\n\n    `sentinels` is a list of sentinel nodes.\n    \"\"\"\n\n    if loop is None:\n        loop = asyncio.get_event_loop()\n\n    pool = await create_sentinel_pool(sentinels,\n                                      db=db,\n                                      password=password,\n                                      encoding=encoding,\n                                      minsize=minsize,\n                                      maxsize=maxsize,\n                                      ssl=ssl,\n                                      timeout=timeout,\n                                      loop=loop)\n    return RedisSentinel(pool)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef execute(self, command, *args, **kwargs):\n        return self._pool.execute(\n            b'SENTINEL', command, *args, **kwargs)", "response": "Execute Sentinel command.\n\n        It will be prefixed with SENTINEL automatically."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef master(self, name):\n        fut = self.execute(b'MASTER', name, encoding='utf-8')\n        return wait_convert(fut, parse_sentinel_master)", "response": "Returns a dictionary containing the specified masters state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple containing the host and port of the master host that is given by the given name.", "response": "def master_address(self, name):\n        \"\"\"Returns a (host, port) pair for the given ``name``.\"\"\"\n        fut = self.execute(b'get-master-addr-by-name', name, encoding='utf-8')\n        return wait_convert(fut, parse_address)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef masters(self):\n        fut = self.execute(b'MASTERS', encoding='utf-8')\n        # TODO: process masters: we can adjust internal state\n        return wait_convert(fut, parse_sentinel_masters)", "response": "Returns a list of dictionaries containing each master s state."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef slaves(self, name):\n        fut = self.execute(b'SLAVES', name, encoding='utf-8')\n        return wait_convert(fut, parse_sentinel_slaves_and_sentinels)", "response": "Returns a list of slaves for name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sentinels(self, name):\n        fut = self.execute(b'SENTINELS', name, encoding='utf-8')\n        return wait_convert(fut, parse_sentinel_slaves_and_sentinels)", "response": "Returns a list of sentinels for name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef monitor(self, name, ip, port, quorum):\n        fut = self.execute(b'MONITOR', name, ip, port, quorum)\n        return wait_ok(fut)", "response": "Add a new master to Sentinel to be monitored."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, name):\n        fut = self.execute(b'REMOVE', name)\n        return wait_ok(fut)", "response": "Remove a master from Sentinel s monitoring."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset Sentinel monitoring parameters for a given master.", "response": "def set(self, name, option, value):\n        \"\"\"Set Sentinel monitoring parameters for a given master.\"\"\"\n        fut = self.execute(b\"SET\", name, option, value)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef failover(self, name):\n        fut = self.execute(b'FAILOVER', name)\n        return wait_ok(fut)", "response": "Force a failover of a named master."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef blpop(self, key, *keys, timeout=0, encoding=_NOTSET):\n        if not isinstance(timeout, int):\n            raise TypeError(\"timeout argument must be int\")\n        if timeout < 0:\n            raise ValueError(\"timeout must be greater equal 0\")\n        args = keys + (timeout,)\n        return self.execute(b'BLPOP', key, *args, encoding=encoding)", "response": "Remove and get the first element in a list or block until one is available."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves and get the last element in a list or block until one is available.", "response": "def brpop(self, key, *keys, timeout=0, encoding=_NOTSET):\n        \"\"\"Remove and get the last element in a list, or block until one\n        is available.\n\n        :raises TypeError: if timeout is not int\n        :raises ValueError: if timeout is less than 0\n        \"\"\"\n        if not isinstance(timeout, int):\n            raise TypeError(\"timeout argument must be int\")\n        if timeout < 0:\n            raise ValueError(\"timeout must be greater equal 0\")\n        args = keys + (timeout,)\n        return self.execute(b'BRPOP', key, *args, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef brpoplpush(self, sourcekey, destkey, timeout=0, encoding=_NOTSET):\n        if not isinstance(timeout, int):\n            raise TypeError(\"timeout argument must be int\")\n        if timeout < 0:\n            raise ValueError(\"timeout must be greater equal 0\")\n        return self.execute(b'BRPOPLPUSH', sourcekey, destkey, timeout,\n                            encoding=encoding)", "response": "Remove and push an item from a list to a new key."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets an element from a list by its index.", "response": "def lindex(self, key, index, *, encoding=_NOTSET):\n        \"\"\"Get an element from a list by its index.\n\n        :raises TypeError: if index is not int\n        \"\"\"\n        if not isinstance(index, int):\n            raise TypeError(\"index argument must be int\")\n        return self.execute(b'LINDEX', key, index, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninsert value into the list stored at key pivot.", "response": "def linsert(self, key, pivot, value, before=False):\n        \"\"\"Inserts value in the list stored at key either before or\n        after the reference value pivot.\n        \"\"\"\n        where = b'AFTER' if not before else b'BEFORE'\n        return self.execute(b'LINSERT', key, where, pivot, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove and returns the first element of the list stored at key.", "response": "def lpop(self, key, *, encoding=_NOTSET):\n        \"\"\"Removes and returns the first element of the list stored at key.\"\"\"\n        return self.execute(b'LPOP', key, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ninserting all the specified values at the head of the list stored at key. Returns the number of elements inserted.", "response": "def lpush(self, key, value, *values):\n        \"\"\"Insert all the specified values at the head of the list\n        stored at key.\n        \"\"\"\n        return self.execute(b'LPUSH', key, value, *values)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef lrange(self, key, start, stop, *, encoding=_NOTSET):\n        if not isinstance(start, int):\n            raise TypeError(\"start argument must be int\")\n        if not isinstance(stop, int):\n            raise TypeError(\"stop argument must be int\")\n        return self.execute(b'LRANGE', key, start, stop, encoding=encoding)", "response": "Returns the specified elements of the list stored at key."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef lrem(self, key, count, value):\n        if not isinstance(count, int):\n            raise TypeError(\"count argument must be int\")\n        return self.execute(b'LREM', key, count, value)", "response": "Removes the first count occurrences of elements equal to value from the list stored at key. Returns the number of elements removed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the list element at index to value.", "response": "def lset(self, key, index, value):\n        \"\"\"Sets the list element at index to value.\n\n        :raises TypeError: if index is not int\n        \"\"\"\n        if not isinstance(index, int):\n            raise TypeError(\"index argument must be int\")\n        return self.execute(b'LSET', key, index, value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ltrim(self, key, start, stop):\n        if not isinstance(start, int):\n            raise TypeError(\"start argument must be int\")\n        if not isinstance(stop, int):\n            raise TypeError(\"stop argument must be int\")\n        fut = self.execute(b'LTRIM', key, start, stop)\n        return wait_ok(fut)", "response": "Trim an existing list so that it will contain only the specified start and stop elements."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef rpop(self, key, *, encoding=_NOTSET):\n        return self.execute(b'RPOP', key, encoding=encoding)", "response": "Removes and returns the last element of the list stored at key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting all the specified values at the tail of the list stored at key.", "response": "def rpush(self, key, value, *values):\n        \"\"\"Insert all the specified values at the tail of the list\n        stored at key.\n        \"\"\"\n        return self.execute(b'RPUSH', key, value, *values)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwatch the given keys to determine execution of the MULTI or EXEC block.", "response": "def watch(self, key, *keys):\n        \"\"\"Watch the given keys to determine execution of the MULTI/EXEC block.\n        \"\"\"\n        # FIXME: we can send watch through one connection and then issue\n        #   'multi/exec' command through other.\n        # Possible fix:\n        #   \"Remember\" a connection that was used for 'watch' command\n        #   and then send 'multi / exec / discard' through it.\n        fut = self._pool_or_conn.execute(b'WATCH', key, *keys)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef multi_exec(self):\n        return MultiExec(self._pool_or_conn, self.__class__,\n                         loop=self._pool_or_conn._loop)", "response": "Returns a new MultiExec pipeline that returns the result of executing the current redis command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a pipeline that will execute the commands in a single thread.", "response": "def pipeline(self):\n        \"\"\"Returns :class:`Pipeline` object to execute bulk of commands.\n\n        It is provided for convenience.\n        Commands can be pipelined without it.\n\n        Example:\n\n        >>> pipe = redis.pipeline()\n        >>> fut1 = pipe.incr('foo') # NO `await` as it will block forever!\n        >>> fut2 = pipe.incr('bar')\n        >>> result = await pipe.execute()\n        >>> result\n        [1, 1]\n        >>> await asyncio.gather(fut1, fut2)\n        [1, 1]\n        >>> #\n        >>> # The same can be done without pipeline:\n        >>> #\n        >>> fut1 = redis.incr('foo')    # the 'INCRY foo' command already sent\n        >>> fut2 = redis.incr('bar')\n        >>> await asyncio.gather(fut1, fut2)\n        [2, 2]\n        \"\"\"\n        return Pipeline(self._pool_or_conn, self.__class__,\n                        loop=self._pool_or_conn._loop)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting all buffered commands.", "response": "async def execute(self, *, return_exceptions=False):\n        \"\"\"Execute all buffered commands.\n\n        Any exception that is raised by any command is caught and\n        raised later when processing results.\n\n        Exceptions can also be returned in result if\n        `return_exceptions` flag is set to True.\n        \"\"\"\n        assert not self._done, \"Pipeline already executed. Create new one.\"\n        self._done = True\n\n        if self._pipeline:\n            if isinstance(self._pool_or_conn, AbcPool):\n                async with self._pool_or_conn.get() as conn:\n                    return await self._do_execute(\n                        conn, return_exceptions=return_exceptions)\n            else:\n                return await self._do_execute(\n                    self._pool_or_conn,\n                    return_exceptions=return_exceptions)\n        else:\n            return await self._gather_result(return_exceptions)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreading a Redis object from the parser.", "response": "async def readobj(self):\n        \"\"\"\n        Return a parsed Redis object or an exception\n        when something wrong happened.\n        \"\"\"\n        assert self._parser is not None, \"set_parser must be called\"\n        while True:\n            obj = self._parser.gets()\n\n            if obj is not False:\n                # TODO: implement resume the read\n\n                # Return any valid object and the Nil->None\n                # case. When its False there is nothing there\n                # to be parsed and we have to wait for more data.\n                return obj\n\n            if self._exception:\n                raise self._exception\n\n            if self._eof:\n                break\n\n            await self._wait_for_data('readobj')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef client_list(self):\n        fut = self.execute(b'CLIENT', b'LIST', encoding='utf-8')\n        return wait_convert(fut, to_tuples)", "response": "Get the list of client connections."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef client_getname(self, encoding=_NOTSET):\n        return self.execute(b'CLIENT', b'GETNAME', encoding=encoding)", "response": "Get the current connection name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstopping processing commands from clients for * timeout * milliseconds.", "response": "def client_pause(self, timeout):\n        \"\"\"Stop processing commands from clients for *timeout* milliseconds.\n\n        :raises TypeError: if timeout is not int\n        :raises ValueError: if timeout is less than 0\n        \"\"\"\n        if not isinstance(timeout, int):\n            raise TypeError(\"timeout argument must be int\")\n        if timeout < 0:\n            raise ValueError(\"timeout must be greater equal 0\")\n        fut = self.execute(b'CLIENT', b'PAUSE', timeout)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the current connection name.", "response": "def client_setname(self, name):\n        \"\"\"Set the current connection name.\"\"\"\n        fut = self.execute(b'CLIENT', b'SETNAME', name)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef command_getkeys(self, command, *args, encoding='utf-8'):\n        return self.execute(b'COMMAND', b'GETKEYS', command, *args,\n                            encoding=encoding)", "response": "Extract keys given a full Redis command."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets array of specific Redis command details.", "response": "def command_info(self, command, *commands):\n        \"\"\"Get array of specific Redis command details.\"\"\"\n        return self.execute(b'COMMAND', b'INFO', command, *commands,\n                            encoding='utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef config_get(self, parameter='*'):\n        if not isinstance(parameter, str):\n            raise TypeError(\"parameter must be str\")\n        fut = self.execute(b'CONFIG', b'GET', parameter, encoding='utf-8')\n        return wait_make_dict(fut)", "response": "Get the value of a configuration parameter."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config_set(self, parameter, value):\n        if not isinstance(parameter, str):\n            raise TypeError(\"parameter must be str\")\n        fut = self.execute(b'CONFIG', b'SET', parameter, value)\n        return wait_ok(fut)", "response": "Set a configuration parameter to the given value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsuspending connection for timeout seconds.", "response": "def debug_sleep(self, timeout):\n        \"\"\"Suspend connection for timeout seconds.\"\"\"\n        fut = self.execute(b'DEBUG', b'SLEEP', timeout)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all keys from all databases.", "response": "def flushall(self, async_op=False):\n        \"\"\"\n        Remove all keys from all databases.\n\n        :param async_op: lets the entire dataset to be freed asynchronously. \\\n        Defaults to False\n        \"\"\"\n        if async_op:\n            fut = self.execute(b'FLUSHALL', b'ASYNC')\n        else:\n            fut = self.execute(b'FLUSHALL')\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove all keys from the current database.", "response": "def flushdb(self, async_op=False):\n        \"\"\"\n        Remove all keys from the current database.\n\n        :param async_op: lets a single database to be freed asynchronously. \\\n        Defaults to False\n        \"\"\"\n        if async_op:\n            fut = self.execute(b'FLUSHDB', b'ASYNC')\n        else:\n            fut = self.execute(b'FLUSHDB')\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting information about the server.", "response": "def info(self, section='default'):\n        \"\"\"Get information and statistics about the server.\n\n        If called without argument will return default set of sections.\n        For available sections, see http://redis.io/commands/INFO\n\n        :raises ValueError: if section is invalid\n\n        \"\"\"\n        if not section:\n            raise ValueError(\"invalid section\")\n        fut = self.execute(b'INFO', section, encoding='utf-8')\n        return wait_convert(fut, parse_info)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef role(self):\n        fut = self.execute(b'ROLE', encoding='utf-8')\n        return wait_convert(fut, parse_role)", "response": "Return the role of the server instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef shutdown(self, save=None):\n        if save is self.SHUTDOWN_SAVE:\n            return self.execute(b'SHUTDOWN', b'SAVE')\n        elif save is self.SHUTDOWN_NOSAVE:\n            return self.execute(b'SHUTDOWN', b'NOSAVE')\n        else:\n            return self.execute(b'SHUTDOWN')", "response": "Synchronously save the dataset to disk and then shut down the server."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef slaveof(self, host, port=None):\n        if host is None and port is None:\n            return self.execute(b'SLAVEOF', b'NO', b'ONE')\n        return self.execute(b'SLAVEOF', host, port)", "response": "Make the server a slave of another instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef slowlog_get(self, length=None):\n        if length is not None:\n            if not isinstance(length, int):\n                raise TypeError(\"length must be int or None\")\n            return self.execute(b'SLOWLOG', b'GET', length)\n        else:\n            return self.execute(b'SLOWLOG', b'GET')", "response": "Returns the Redis slow queries log."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef publish_json(self, channel, obj):\n        return self.publish(channel, json.dumps(obj))", "response": "Post a JSON - encoded message to channel."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef subscribe(self, channel, *channels):\n        conn = self._pool_or_conn\n        return wait_return_channels(\n            conn.execute_pubsub(b'SUBSCRIBE', channel, *channels),\n            conn, 'pubsub_channels')", "response": "Switch connection to Pub / Sub mode and subscribe to specified channels."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unsubscribe(self, channel, *channels):\n        conn = self._pool_or_conn\n        return conn.execute_pubsub(b'UNSUBSCRIBE', channel, *channels)", "response": "Unsubscribe from specific channels."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nswitch connection to Pub / Sub mode and subscribe to specified patterns.", "response": "def psubscribe(self, pattern, *patterns):\n        \"\"\"Switch connection to Pub/Sub mode and\n        subscribe to specified patterns.\n\n        Arguments can be instances of :class:`~aioredis.Channel`.\n\n        Returns :func:`asyncio.gather()` coroutine which when done will return\n        a list of subscribed :class:`~aioredis.Channel` objects with\n        ``is_pattern`` property set to ``True``.\n        \"\"\"\n        conn = self._pool_or_conn\n        return wait_return_channels(\n            conn.execute_pubsub(b'PSUBSCRIBE', pattern, *patterns),\n            conn, 'pubsub_patterns')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef punsubscribe(self, pattern, *patterns):\n        conn = self._pool_or_conn\n        return conn.execute_pubsub(b'PUNSUBSCRIBE', pattern, *patterns)", "response": "Unsubscribe from specific patterns."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the currently active channels.", "response": "def pubsub_channels(self, pattern=None):\n        \"\"\"Lists the currently active channels.\"\"\"\n        args = [b'PUBSUB', b'CHANNELS']\n        if pattern is not None:\n            args.append(pattern)\n        return self.execute(*args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def get(self, *, encoding=None, decoder=None):\n        assert decoder is None or callable(decoder), decoder\n        if self._queue.exhausted:\n            raise ChannelClosedError()\n        msg = await self._queue.get()\n        if msg is EndOfStream:\n            # TODO: maybe we need an explicit marker for \"end of stream\"\n            #       currently, returning None may overlap with\n            #       possible return value from `decoder`\n            #       so the user would have to check `ch.is_active`\n            #       to determine if its EoS or payload\n            return\n        if self._is_pattern:\n            dest_channel, msg = msg\n        if encoding is not None:\n            msg = msg.decode(encoding)\n        if decoder is not None:\n            msg = decoder(msg)\n        if self._is_pattern:\n            return dest_channel, msg\n        return msg", "response": "Coroutine that waits for and returns a message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef iter(self, *, encoding=None, decoder=None):\n        return _IterHelper(self,\n                           is_active=lambda ch: ch.is_active,\n                           encoding=encoding,\n                           decoder=decoder)", "response": "Same as get method but its native coroutine."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a channel. Returns ``_Sender`` object implementing :class:`~aioredis.abc.AbcChannel`.", "response": "def channel(self, name):\n        \"\"\"Create a channel.\n\n        Returns ``_Sender`` object implementing\n        :class:`~aioredis.abc.AbcChannel`.\n        \"\"\"\n        enc_name = _converters[type(name)](name)\n        if (enc_name, False) not in self._refs:\n            ch = _Sender(self, enc_name,\n                         is_pattern=False)\n            self._refs[(enc_name, False)] = ch\n            return ch\n        return self._refs[(enc_name, False)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pattern(self, pattern):\n        enc_pattern = _converters[type(pattern)](pattern)\n        if (enc_pattern, True) not in self._refs:\n            ch = _Sender(self, enc_pattern,\n                         is_pattern=True)\n            self._refs[(enc_pattern, True)] = ch\n        return self._refs[(enc_pattern, True)]", "response": "Create a pattern channel."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef channels(self):\n        return types.MappingProxyType({\n            ch.name: ch for ch in self._refs.values()\n            if not ch.is_pattern})", "response": "Read - only channels dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwaits for and return a message from one of the channels.", "response": "async def get(self, *, encoding=None, decoder=None):\n        \"\"\"Wait for and return pub/sub message from one of channels.\n\n        Return value is either:\n\n        * tuple of two elements: channel & message;\n\n        * tuple of three elements: pattern channel, (target channel & message);\n\n        * or None in case Receiver is not active or has just been stopped.\n\n        :raises aioredis.ChannelClosedError: If listener is stopped\n            and all messages have been received.\n        \"\"\"\n        # TODO: add note about raised exception and end marker.\n        #   Flow before ClosableQueue:\n        #   - ch.get() -> message\n        #   - ch.close() -> ch.put(None)\n        #   - ch.get() -> None\n        #   - ch.get() -> ChannelClosedError\n        #   Current flow:\n        #   - ch.get() -> message\n        #   - ch.close() -> ch._closed = True\n        #   - ch.get() -> ChannelClosedError\n        assert decoder is None or callable(decoder), decoder\n        if self._queue.exhausted:\n            raise ChannelClosedError()\n        obj = await self._queue.get()\n        if obj is EndOfStream:\n            return\n        ch, msg = obj\n        if ch.is_pattern:\n            dest_ch, msg = msg\n        if encoding is not None:\n            msg = msg.decode(encoding)\n        if decoder is not None:\n            msg = decoder(msg)\n        if ch.is_pattern:\n            return ch, (dest_ch, msg)\n        return ch, msg"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nblock until a new message appear.", "response": "async def wait_message(self):\n        \"\"\"Blocks until new message appear.\"\"\"\n        if not self._queue.empty():\n            return True\n        if self._queue.closed:\n            return False\n        await self._queue.wait()\n        return self.is_active"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_active(self):\n        if self._queue.exhausted:\n            return False\n        return any(ch.is_active for ch in self._refs.values())", "response": "Returns True if any active subscription is available."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns an iterator over the current set of items in the queue.", "response": "def iter(self, *, encoding=None, decoder=None):\n        \"\"\"Returns async iterator.\n\n        Usage example:\n\n        >>> async for ch, msg in mpsc.iter():\n        ...     print(ch, msg)\n        \"\"\"\n        return _IterHelper(self,\n                           is_active=lambda r: not r._queue.exhausted,\n                           encoding=encoding,\n                           decoder=decoder)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new Redis Pool instance.", "response": "async def create_pool(address, *, db=None, password=None, ssl=None,\n                      encoding=None, minsize=1, maxsize=10,\n                      parser=None, loop=None, create_connection_timeout=None,\n                      pool_cls=None, connection_cls=None):\n    # FIXME: rewrite docstring\n    \"\"\"Creates Redis Pool.\n\n    By default it creates pool of Redis instances, but it is\n    also possible to create pool of plain connections by passing\n    ``lambda conn: conn`` as commands_factory.\n\n    *commands_factory* parameter is deprecated since v0.2.9\n\n    All arguments are the same as for create_connection.\n\n    Returns RedisPool instance or a pool_cls if it is given.\n    \"\"\"\n    if pool_cls:\n        assert issubclass(pool_cls, AbcPool),\\\n                \"pool_class does not meet the AbcPool contract\"\n        cls = pool_cls\n    else:\n        cls = ConnectionsPool\n    if isinstance(address, str):\n        address, options = parse_url(address)\n        db = options.setdefault('db', db)\n        password = options.setdefault('password', password)\n        encoding = options.setdefault('encoding', encoding)\n        create_connection_timeout = options.setdefault(\n            'timeout', create_connection_timeout)\n        if 'ssl' in options:\n            assert options['ssl'] or (not options['ssl'] and not ssl), (\n                \"Conflicting ssl options are set\", options['ssl'], ssl)\n            ssl = ssl or options['ssl']\n        # TODO: minsize/maxsize\n\n    pool = cls(address, db, password, encoding,\n               minsize=minsize, maxsize=maxsize,\n               ssl=ssl, parser=parser,\n               create_connection_timeout=create_connection_timeout,\n               connection_cls=connection_cls,\n               loop=loop)\n    try:\n        await pool._fill_free(override_min=False)\n    except Exception:\n        pool.close()\n        await pool.wait_closed()\n        await pool.wait_closed()\n        raise\n    return pool"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a redis command and returns a future that returns the result.", "response": "def execute(self, command, *args, **kw):\n        \"\"\"Executes redis command in a free connection and returns\n        future waiting for result.\n\n        Picks connection from free pool and send command through\n        that connection.\n        If no connection is found, returns coroutine waiting for\n        free connection to execute command.\n        \"\"\"\n        conn, address = self.get_connection(command, args)\n        if conn is not None:\n            fut = conn.execute(command, *args, **kw)\n            return self._check_result(fut, command, args, kw)\n        else:\n            coro = self._wait_execute(address, command, args, kw)\n            return self._check_result(coro, command, args, kw)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute a command on a set of channels.", "response": "def execute_pubsub(self, command, *channels):\n        \"\"\"Executes Redis (p)subscribe/(p)unsubscribe commands.\n\n        ConnectionsPool picks separate connection for pub/sub\n        and uses it until explicitly closed or disconnected\n        (unsubscribing from all channels/patterns will leave connection\n         locked for pub/sub use).\n\n        There is no auto-reconnect for this PUB/SUB connection.\n\n        Returns asyncio.gather coroutine waiting for all channels/patterns\n        to receive answers.\n        \"\"\"\n        conn, address = self.get_connection(command)\n        if conn is not None:\n            return conn.execute_pubsub(command, *channels)\n        else:\n            return self._wait_execute_pubsub(address, command, channels, {})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_connection(self, command, args=()):\n        # TODO: find a better way to determine if connection is free\n        #       and not havily used.\n        command = command.upper().strip()\n        is_pubsub = command in _PUBSUB_COMMANDS\n        if is_pubsub and self._pubsub_conn:\n            if not self._pubsub_conn.closed:\n                return self._pubsub_conn, self._pubsub_conn.address\n            self._pubsub_conn = None\n        for i in range(self.freesize):\n            conn = self._pool[0]\n            self._pool.rotate(1)\n            if conn.closed:  # or conn._waiters: (eg: busy connection)\n                continue\n            if conn.in_pubsub:\n                continue\n            if is_pubsub:\n                self._pubsub_conn = conn\n                self._pool.remove(conn)\n                self._used.add(conn)\n            return conn, conn.address\n        return None, self._address", "response": "Get free connection from pool."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def _wait_execute(self, address, command, args, kw):\n        conn = await self.acquire(command, args)\n        try:\n            return (await conn.execute(command, *args, **kw))\n        finally:\n            self.release(conn)", "response": "Wait for a command to be executed and return the result."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def select(self, db):\n        res = True\n        async with self._cond:\n            for i in range(self.freesize):\n                res = res and (await self._pool[i].select(db))\n            else:\n                self._db = db\n        return res", "response": "Changes the index of all free connections."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nacquires a connection from free pool. Returns a connection if needed.", "response": "async def acquire(self, command=None, args=()):\n        \"\"\"Acquires a connection from free pool.\n\n        Creates new connection if needed.\n        \"\"\"\n        if self.closed:\n            raise PoolClosedError(\"Pool is closed\")\n        async with self._cond:\n            if self.closed:\n                raise PoolClosedError(\"Pool is closed\")\n            while True:\n                await self._fill_free(override_min=True)\n                if self.freesize:\n                    conn = self._pool.popleft()\n                    assert not conn.closed, conn\n                    assert conn not in self._used, (conn, self._used)\n                    self._used.add(conn)\n                    return conn\n                else:\n                    await self._cond.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn used connection back into pool.", "response": "def release(self, conn):\n        \"\"\"Returns used connection back into pool.\n\n        When returned connection has db index that differs from one in pool\n        the connection will be closed and dropped.\n        When queue of free connections is full the connection will be dropped.\n        \"\"\"\n        assert conn in self._used, (\n            \"Invalid connection, maybe from other pool\", conn)\n        self._used.remove(conn)\n        if not conn.closed:\n            if conn.in_transaction:\n                logger.warning(\n                    \"Connection %r is in transaction, closing it.\", conn)\n                conn.close()\n            elif conn.in_pubsub:\n                logger.warning(\n                    \"Connection %r is in subscribe mode, closing it.\", conn)\n                conn.close()\n            elif conn._waiters:\n                logger.warning(\n                    \"Connection %r has pending commands, closing it.\", conn)\n                conn.close()\n            elif conn.db == self.db:\n                if self.maxsize and self.freesize < self.maxsize:\n                    self._pool.append(conn)\n                else:\n                    # consider this connection as old and close it.\n                    conn.close()\n            else:\n                conn.close()\n        # FIXME: check event loop is not closed\n        asyncio.ensure_future(self._wakeup(), loop=self._loop)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef feed(self, data, o: int = 0, l: int = -1):\n        if l == -1:\n            l = len(data) - o\n        if o < 0 or l < 0:\n            raise ValueError(\"negative input\")\n        if o + l > len(data):\n            raise ValueError(\"input is larger than buffer size\")\n        self._parser.buf.extend(data[o:o+l])", "response": "Feed data to parser."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef encode_command(*args, buf=None):\n    if buf is None:\n        buf = bytearray()\n    buf.extend(b'*%d\\r\\n' % len(args))\n\n    try:\n        for arg in args:\n            barg = _converters[type(arg)](arg)\n            buf.extend(b'$%d\\r\\n%s\\r\\n' % (len(barg), barg))\n    except KeyError:\n        raise TypeError(\"Argument {!r} expected to be of bytearray, bytes,\"\n                        \" float, int, or str type\".format(arg))\n    return buf", "response": "Encodes arguments into redis bulk - strings array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_url(url):\n    r = urlparse(url)\n\n    assert r.scheme in ('', 'redis', 'rediss', 'unix'), (\n        \"Unsupported URI scheme\", r.scheme)\n    if r.scheme == '':\n        return url, {}\n    query = {}\n    for p, v in parse_qsl(r.query, keep_blank_values=True):\n        assert p not in query, (\"Multiple parameters are not allowed\", p, v)\n        assert v, (\"Empty parameters are not allowed\", p, v)\n        query[p] = v\n\n    if r.scheme == 'unix':\n        assert r.path, (\"Empty path is not allowed\", url)\n        assert not r.netloc, (\n            \"Netlocation is not allowed for unix scheme\", r.netloc)\n        return r.path, _parse_uri_options(query, '', r.password)\n\n    address = (r.hostname or 'localhost', int(r.port or 6379))\n    path = r.path\n    if path.startswith('/'):\n        path = r.path[1:]\n    options = _parse_uri_options(query, path, r.password)\n    if r.scheme == 'rediss':\n        options['ssl'] = True\n    return address, options", "response": "Parse Redis connection URI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def create_connection(address, *, db=None, password=None, ssl=None,\n                            encoding=None, parser=None, loop=None,\n                            timeout=None, connection_cls=None):\n    \"\"\"Creates redis connection.\n\n    Opens connection to Redis server specified by address argument.\n    Address argument can be one of the following:\n    * A tuple representing (host, port) pair for TCP connections;\n    * A string representing either Redis URI or unix domain socket path.\n\n    SSL argument is passed through to asyncio.create_connection.\n    By default SSL/TLS is not used.\n\n    By default any timeout is applied at the connection stage, however\n    you can set a limitted time used trying to open a connection via\n    the `timeout` Kw.\n\n    Encoding argument can be used to decode byte-replies to strings.\n    By default no decoding is done.\n\n    Parser parameter can be used to pass custom Redis protocol parser class.\n    By default hiredis.Reader is used (unless it is missing or platform\n    is not CPython).\n\n    Return value is RedisConnection instance or a connection_cls if it is\n    given.\n\n    This function is a coroutine.\n    \"\"\"\n    assert isinstance(address, (tuple, list, str)), \"tuple or str expected\"\n    if isinstance(address, str):\n        logger.debug(\"Parsing Redis URI %r\", address)\n        address, options = parse_url(address)\n        db = options.setdefault('db', db)\n        password = options.setdefault('password', password)\n        encoding = options.setdefault('encoding', encoding)\n        timeout = options.setdefault('timeout', timeout)\n        if 'ssl' in options:\n            assert options['ssl'] or (not options['ssl'] and not ssl), (\n                \"Conflicting ssl options are set\", options['ssl'], ssl)\n            ssl = ssl or options['ssl']\n\n    if timeout is not None and timeout <= 0:\n        raise ValueError(\"Timeout has to be None or a number greater than 0\")\n\n    if connection_cls:\n        assert issubclass(connection_cls, AbcConnection),\\\n                \"connection_class does not meet the AbcConnection contract\"\n        cls = connection_cls\n    else:\n        cls = RedisConnection\n\n    if loop is None:\n        loop = asyncio.get_event_loop()\n\n    if isinstance(address, (list, tuple)):\n        host, port = address\n        logger.debug(\"Creating tcp connection to %r\", address)\n        reader, writer = await asyncio.wait_for(open_connection(\n            host, port, limit=MAX_CHUNK_SIZE, ssl=ssl, loop=loop),\n            timeout, loop=loop)\n        sock = writer.transport.get_extra_info('socket')\n        if sock is not None:\n            sock.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n            address = sock.getpeername()\n        address = tuple(address[:2])\n    else:\n        logger.debug(\"Creating unix connection to %r\", address)\n        reader, writer = await asyncio.wait_for(open_unix_connection(\n            address, ssl=ssl, limit=MAX_CHUNK_SIZE, loop=loop),\n            timeout, loop=loop)\n        sock = writer.transport.get_extra_info('socket')\n        if sock is not None:\n            address = sock.getpeername()\n\n    conn = cls(reader, writer, encoding=encoding,\n               address=address, parser=parser,\n               loop=loop)\n\n    try:\n        if password is not None:\n            await conn.auth(password)\n        if db is not None:\n            await conn.select(db)\n    except Exception:\n        conn.close()\n        await conn.wait_closed()\n        raise\n    return conn", "response": "Creates a new Redis connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a command and returns a Future that will be returned when the command is completed or None if the command is not supported.", "response": "def execute(self, command, *args, encoding=_NOTSET):\n        \"\"\"Executes redis command and returns Future waiting for the answer.\n\n        Raises:\n        * TypeError if any of args can not be encoded as bytes.\n        * ReplyError on redis '-ERR' responses.\n        * ProtocolError when response can not be decoded meaning connection\n          is broken.\n        * ConnectionClosedError when either client or server has closed the\n          connection.\n        \"\"\"\n        if self._reader is None or self._reader.at_eof():\n            msg = self._close_msg or \"Connection closed or corrupted\"\n            raise ConnectionClosedError(msg)\n        if command is None:\n            raise TypeError(\"command must not be None\")\n        if None in args:\n            raise TypeError(\"args must not contain None\")\n        command = command.upper().strip()\n        is_pubsub = command in _PUBSUB_COMMANDS\n        is_ping = command in ('PING', b'PING')\n        if self._in_pubsub and not (is_pubsub or is_ping):\n            raise RedisError(\"Connection in SUBSCRIBE mode\")\n        elif is_pubsub:\n            logger.warning(\"Deprecated. Use `execute_pubsub` method directly\")\n            return self.execute_pubsub(command, *args)\n\n        if command in ('SELECT', b'SELECT'):\n            cb = partial(self._set_db, args=args)\n        elif command in ('MULTI', b'MULTI'):\n            cb = self._start_transaction\n        elif command in ('EXEC', b'EXEC'):\n            cb = partial(self._end_transaction, discard=False)\n        elif command in ('DISCARD', b'DISCARD'):\n            cb = partial(self._end_transaction, discard=True)\n        else:\n            cb = None\n        if encoding is _NOTSET:\n            encoding = self._encoding\n        fut = self._loop.create_future()\n        if self._pipeline_buffer is None:\n            self._writer.write(encode_command(command, *args))\n        else:\n            encode_command(command, *args, buf=self._pipeline_buffer)\n        self._waiters.append((fut, encoding, cb))\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute a Pub / Sub command.", "response": "def execute_pubsub(self, command, *channels):\n        \"\"\"Executes redis (p)subscribe/(p)unsubscribe commands.\n\n        Returns asyncio.gather coroutine waiting for all channels/patterns\n        to receive answers.\n        \"\"\"\n        command = command.upper().strip()\n        assert command in _PUBSUB_COMMANDS, (\n            \"Pub/Sub command expected\", command)\n        if self._reader is None or self._reader.at_eof():\n            raise ConnectionClosedError(\"Connection closed or corrupted\")\n        if None in set(channels):\n            raise TypeError(\"args must not contain None\")\n        if not len(channels):\n            raise TypeError(\"No channels/patterns supplied\")\n        is_pattern = len(command) in (10, 12)\n        mkchannel = partial(Channel, is_pattern=is_pattern, loop=self._loop)\n        channels = [ch if isinstance(ch, AbcChannel) else mkchannel(ch)\n                    for ch in channels]\n        if not all(ch.is_pattern == is_pattern for ch in channels):\n            raise ValueError(\"Not all channels {} match command {}\"\n                             .format(channels, command))\n        cmd = encode_command(command, *(ch.name for ch in channels))\n        res = []\n        for ch in channels:\n            fut = self._loop.create_future()\n            res.append(fut)\n            cb = partial(self._update_pubsub, ch=ch)\n            self._waiters.append((fut, None, cb))\n        if self._pipeline_buffer is None:\n            self._writer.write(cmd)\n        else:\n            self._pipeline_buffer.extend(cmd)\n        return asyncio.gather(*res, loop=self._loop)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef closed(self):\n        closed = self._closing or self._closed\n        if not closed and self._reader and self._reader.at_eof():\n            self._closing = closed = True\n            self._loop.call_soon(self._do_close, None)\n        return closed", "response": "True if connection is closed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef select(self, db):\n        if not isinstance(db, int):\n            raise TypeError(\"DB must be of int type, not {!r}\".format(db))\n        if db < 0:\n            raise ValueError(\"DB must be greater or equal 0, got {!r}\"\n                             .format(db))\n        fut = self.execute('SELECT', db)\n        return wait_ok(fut)", "response": "Change the selected database for the current connection."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geohash(self, key, member, *members, **kwargs):\n        return self.execute(\n            b'GEOHASH', key, member, *members, **kwargs\n        )", "response": "Returns members of a geospatial index as standard geohash strings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the longitude and latitude of members of a geospatial index.", "response": "def geopos(self, key, member, *members, **kwargs):\n        \"\"\"Returns longitude and latitude of members of a geospatial index.\n\n        :rtype: list[GeoPoint or None]\n        \"\"\"\n        fut = self.execute(b'GEOPOS', key, member, *members, **kwargs)\n        return wait_convert(fut, make_geopos)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the distance between two members of a geospatial index.", "response": "def geodist(self, key, member1, member2, unit='m'):\n        \"\"\"Returns the distance between two members of a geospatial index.\n\n        :rtype: list[float or None]\n        \"\"\"\n        fut = self.execute(b'GEODIST', key, member1, member2, unit)\n        return wait_convert(fut, make_geodist)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nquery a sorted set representing a geospatial index to fetch members of a given maximum distance from a point.", "response": "def georadius(self, key, longitude, latitude, radius, unit='m', *,\n                  with_dist=False, with_hash=False, with_coord=False,\n                  count=None, sort=None, encoding=_NOTSET):\n        \"\"\"Query a sorted set representing a geospatial index to fetch members\n        matching a given maximum distance from a point.\n\n        Return value follows Redis convention:\n\n        * if none of ``WITH*`` flags are set -- list of strings returned:\n\n            >>> await redis.georadius('Sicily', 15, 37, 200, 'km')\n            [b\"Palermo\", b\"Catania\"]\n\n        * if any flag (or all) is set -- list of named tuples returned:\n\n            >>> await redis.georadius('Sicily', 15, 37, 200, 'km',\n            ...                       with_dist=True)\n            [GeoMember(name=b\"Palermo\", dist=190.4424, hash=None, coord=None),\n             GeoMember(name=b\"Catania\", dist=56.4413, hash=None, coord=None)]\n\n        :raises TypeError: radius is not float or int\n        :raises TypeError: count is not int\n        :raises ValueError: if unit not equal ``m``, ``km``, ``mi`` or ``ft``\n        :raises ValueError: if sort not equal ``ASC`` or ``DESC``\n\n        :rtype: list[str] or list[GeoMember]\n        \"\"\"\n        args = validate_georadius_options(\n            radius, unit, with_dist, with_hash, with_coord, count, sort\n        )\n\n        fut = self.execute(\n            b'GEORADIUS', key, longitude, latitude, radius,\n            unit, *args, encoding=encoding\n        )\n        if with_dist or with_hash or with_coord:\n            return wait_convert(fut, make_geomember,\n                                with_dist=with_dist,\n                                with_hash=with_hash,\n                                with_coord=with_coord)\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nquerying a sorted set representing a geospatial index to fetch members of a given maximum distance from a member.", "response": "def georadiusbymember(self, key, member, radius, unit='m', *,\n                          with_dist=False, with_hash=False, with_coord=False,\n                          count=None, sort=None, encoding=_NOTSET):\n        \"\"\"Query a sorted set representing a geospatial index to fetch members\n        matching a given maximum distance from a member.\n\n        Return value follows Redis convention:\n\n        * if none of ``WITH*`` flags are set -- list of strings returned:\n\n            >>> await redis.georadiusbymember('Sicily', 'Palermo', 200, 'km')\n            [b\"Palermo\", b\"Catania\"]\n\n        * if any flag (or all) is set -- list of named tuples returned:\n\n            >>> await redis.georadiusbymember('Sicily', 'Palermo', 200, 'km',\n            ...                               with_dist=True)\n            [GeoMember(name=b\"Palermo\", dist=190.4424, hash=None, coord=None),\n             GeoMember(name=b\"Catania\", dist=56.4413, hash=None, coord=None)]\n\n        :raises TypeError: radius is not float or int\n        :raises TypeError: count is not int\n        :raises ValueError: if unit not equal ``m``, ``km``, ``mi`` or ``ft``\n        :raises ValueError: if sort not equal ``ASC`` or ``DESC``\n\n        :rtype: list[str] or list[GeoMember]\n        \"\"\"\n        args = validate_georadius_options(\n            radius, unit, with_dist, with_hash, with_coord, count, sort\n        )\n\n        fut = self.execute(\n            b'GEORADIUSBYMEMBER', key, member, radius,\n            unit, *args, encoding=encoding)\n        if with_dist or with_hash or with_coord:\n            return wait_convert(fut, make_geomember,\n                                with_dist=with_dist,\n                                with_hash=with_hash,\n                                with_coord=with_coord)\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef eval(self, script, keys=[], args=[]):\n        return self.execute(b'EVAL', script, len(keys), *(keys + args))", "response": "Execute a Lua script server side."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexecute a Lua script server side by its SHA1 digest.", "response": "def evalsha(self, digest, keys=[], args=[]):\n        \"\"\"Execute a Lua script server side by its SHA1 digest.\"\"\"\n        return self.execute(b'EVALSHA', digest, len(keys), *(keys + args))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef script_exists(self, digest, *digests):\n        return self.execute(b'SCRIPT', b'EXISTS', digest, *digests)", "response": "Check existence of scripts in the script cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bitcount(self, key, start=None, end=None):\n        if start is None and end is not None:\n            raise TypeError(\"both start and stop must be specified\")\n        elif start is not None and end is None:\n            raise TypeError(\"both start and stop must be specified\")\n        elif start is not None and end is not None:\n            args = (start, end)\n        else:\n            args = ()\n        return self.execute(b'BITCOUNT', key, *args)", "response": "Returns the number of bits set in a string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming bitwise AND operations between strings.", "response": "def bitop_and(self, dest, key, *keys):\n        \"\"\"Perform bitwise AND operations between strings.\"\"\"\n        return self.execute(b'BITOP', b'AND', dest, key, *keys)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nperform bitwise OR operations between strings.", "response": "def bitop_or(self, dest, key, *keys):\n        \"\"\"Perform bitwise OR operations between strings.\"\"\"\n        return self.execute(b'BITOP', b'OR', dest, key, *keys)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bitop_xor(self, dest, key, *keys):\n        return self.execute(b'BITOP', b'XOR', dest, key, *keys)", "response": "Perform bitwise XOR operations between strings."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nperforms bitwise NOT operations between strings.", "response": "def bitop_not(self, dest, key):\n        \"\"\"Perform bitwise NOT operations between strings.\"\"\"\n        return self.execute(b'BITOP', b'NOT', dest, key)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the first set or clear in a string.", "response": "def bitpos(self, key, bit, start=None, end=None):\n        \"\"\"Find first bit set or clear in a string.\n\n        :raises ValueError: if bit is not 0 or 1\n        \"\"\"\n        if bit not in (1, 0):\n            raise ValueError(\"bit argument must be either 1 or 0\")\n        bytes_range = []\n        if start is not None:\n            bytes_range.append(start)\n        if end is not None:\n            if start is None:\n                bytes_range = [0, end]\n            else:\n                bytes_range.append(end)\n        return self.execute(b'BITPOS', key, bit, *bytes_range)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decrby(self, key, decrement):\n        if not isinstance(decrement, int):\n            raise TypeError(\"decrement must be of type int\")\n        return self.execute(b'DECRBY', key, decrement)", "response": "Decrement the integer value of a key by the given number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the value of a key.", "response": "def get(self, key, *, encoding=_NOTSET):\n        \"\"\"Get the value of a key.\"\"\"\n        return self.execute(b'GET', key, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getbit(self, key, offset):\n        if not isinstance(offset, int):\n            raise TypeError(\"offset argument must be int\")\n        if offset < 0:\n            raise ValueError(\"offset must be greater equal 0\")\n        return self.execute(b'GETBIT', key, offset)", "response": "Returns the bit value at offset in the string value stored at key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a substring of the string stored at a key.", "response": "def getrange(self, key, start, end, *, encoding=_NOTSET):\n        \"\"\"Get a substring of the string stored at a key.\n\n        :raises TypeError: if start or end is not int\n        \"\"\"\n        if not isinstance(start, int):\n            raise TypeError(\"start argument must be int\")\n        if not isinstance(end, int):\n            raise TypeError(\"end argument must be int\")\n        return self.execute(b'GETRANGE', key, start, end, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef getset(self, key, value, *, encoding=_NOTSET):\n        return self.execute(b'GETSET', key, value, encoding=encoding)", "response": "Set the string value of a key and return its old value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef incrby(self, key, increment):\n        if not isinstance(increment, int):\n            raise TypeError(\"increment must be of type int\")\n        return self.execute(b'INCRBY', key, increment)", "response": "Increment the integer value of a key by the given amount."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef incrbyfloat(self, key, increment):\n        if not isinstance(increment, float):\n            raise TypeError(\"increment must be of type int\")\n        fut = self.execute(b'INCRBYFLOAT', key, increment)\n        return wait_convert(fut, float)", "response": "Increment the float value of a key by the given amount."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the values of all the given keys.", "response": "def mget(self, key, *keys, encoding=_NOTSET):\n        \"\"\"Get the values of all the given keys.\"\"\"\n        return self.execute(b'MGET', key, *keys, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mset(self, *args):\n        data = args\n        if len(args) == 1:\n            if not isinstance(args[0], dict):\n                raise TypeError(\"if one arg it should be a dict\")\n            data = chain.from_iterable(args[0].items())\n        elif len(args) % 2 != 0:\n            raise TypeError(\"length of pairs must be even number\")\n        fut = self.execute(b'MSET', *data)\n        return wait_ok(fut)", "response": "Set multiple keys to multiple values or unpack dict to keys & values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef msetnx(self, key, value, *pairs):\n        if len(pairs) % 2 != 0:\n            raise TypeError(\"length of pairs must be even number\")\n        return self.execute(b'MSETNX', key, value, *pairs)", "response": "Set multiple keys to multiple values only if none of the keys exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef psetex(self, key, milliseconds, value):\n        if not isinstance(milliseconds, int):\n            raise TypeError(\"milliseconds argument must be int\")\n        fut = self.execute(b'PSETEX', key, milliseconds, value)\n        return wait_ok(fut)", "response": "Set the value and expiration in milliseconds of a key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set(self, key, value, *, expire=0, pexpire=0, exist=None):\n        if expire and not isinstance(expire, int):\n            raise TypeError(\"expire argument must be int\")\n        if pexpire and not isinstance(pexpire, int):\n            raise TypeError(\"pexpire argument must be int\")\n\n        args = []\n        if expire:\n            args[:] = [b'EX', expire]\n        if pexpire:\n            args[:] = [b'PX', pexpire]\n\n        if exist is self.SET_IF_EXIST:\n            args.append(b'XX')\n        elif exist is self.SET_IF_NOT_EXIST:\n            args.append(b'NX')\n        fut = self.execute(b'SET', key, value, *args)\n        return wait_ok(fut)", "response": "Set the string value of a key."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting or clears the bit at offset in the string value stored at key.", "response": "def setbit(self, key, offset, value):\n        \"\"\"Sets or clears the bit at offset in the string value stored at key.\n\n        :raises TypeError: if offset is not int\n        :raises ValueError: if offset is less than 0 or value is not 0 or 1\n        \"\"\"\n        if not isinstance(offset, int):\n            raise TypeError(\"offset argument must be int\")\n        if offset < 0:\n            raise ValueError(\"offset must be greater equal 0\")\n        if value not in (0, 1):\n            raise ValueError(\"value argument must be either 1 or 0\")\n        return self.execute(b'SETBIT', key, offset, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value and expiration of a key.", "response": "def setex(self, key, seconds, value):\n        \"\"\"Set the value and expiration of a key.\n\n        If seconds is float it will be multiplied by 1000\n        coerced to int and passed to `psetex` method.\n\n        :raises TypeError: if seconds is neither int nor float\n        \"\"\"\n        if isinstance(seconds, float):\n            return self.psetex(key, int(seconds * 1000), value)\n        if not isinstance(seconds, int):\n            raise TypeError(\"milliseconds argument must be int\")\n        fut = self.execute(b'SETEX', key, seconds, value)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets the value of a key only if the key does not exist.", "response": "def setnx(self, key, value):\n        \"\"\"Set the value of a key, only if the key does not exist.\"\"\"\n        fut = self.execute(b'SETNX', key, value)\n        return wait_convert(fut, bool)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noverwrite part of a string at key starting at the specified offset.", "response": "def setrange(self, key, offset, value):\n        \"\"\"Overwrite part of a string at key starting at the specified offset.\n\n        :raises TypeError: if offset is not int\n        :raises ValueError: if offset less than 0\n        \"\"\"\n        if not isinstance(offset, int):\n            raise TypeError(\"offset argument must be int\")\n        if offset < 0:\n            raise ValueError(\"offset must be greater equal 0\")\n        return self.execute(b'SETRANGE', key, offset, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting a timeout on key.", "response": "def expire(self, key, timeout):\n        \"\"\"Set a timeout on key.\n\n        if timeout is float it will be multiplied by 1000\n        coerced to int and passed to `pexpire` method.\n\n        Otherwise raises TypeError if timeout argument is not int.\n        \"\"\"\n        if isinstance(timeout, float):\n            return self.pexpire(key, int(timeout * 1000))\n        if not isinstance(timeout, int):\n            raise TypeError(\n                \"timeout argument must be int, not {!r}\".format(timeout))\n        fut = self.execute(b'EXPIRE', key, timeout)\n        return wait_convert(fut, bool)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef expireat(self, key, timestamp):\n        if isinstance(timestamp, float):\n            return self.pexpireat(key, int(timestamp * 1000))\n        if not isinstance(timestamp, int):\n            raise TypeError(\"timestamp argument must be int, not {!r}\"\n                            .format(timestamp))\n        fut = self.execute(b'EXPIREAT', key, timestamp)\n        return wait_convert(fut, bool)", "response": "Set the expire timestamp on a key."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn all keys matching pattern.", "response": "def keys(self, pattern, *, encoding=_NOTSET):\n        \"\"\"Returns all keys matching pattern.\"\"\"\n        return self.execute(b'KEYS', pattern, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef migrate(self, host, port, key, dest_db, timeout, *,\n                copy=False, replace=False):\n        \"\"\"Atomically transfer a key from a Redis instance to another one.\"\"\"\n        if not isinstance(host, str):\n            raise TypeError(\"host argument must be str\")\n        if not isinstance(timeout, int):\n            raise TypeError(\"timeout argument must be int\")\n        if not isinstance(dest_db, int):\n            raise TypeError(\"dest_db argument must be int\")\n        if not host:\n            raise ValueError(\"Got empty host\")\n        if dest_db < 0:\n            raise ValueError(\"dest_db must be greater equal 0\")\n        if timeout < 0:\n            raise ValueError(\"timeout must be greater equal 0\")\n\n        flags = []\n        if copy:\n            flags.append(b'COPY')\n        if replace:\n            flags.append(b'REPLACE')\n        fut = self.execute(b'MIGRATE', host, port,\n                           key, dest_db, timeout, *flags)\n        return wait_ok(fut)", "response": "Atomically transfer a key from a Redis instance to another Redis instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move(self, key, db):\n        if not isinstance(db, int):\n            raise TypeError(\"db argument must be int, not {!r}\".format(db))\n        if db < 0:\n            raise ValueError(\"db argument must be not less than 0, {!r}\"\n                             .format(db))\n        fut = self.execute(b'MOVE', key, db)\n        return wait_convert(fut, bool)", "response": "Move a key from currently selected database to specified destination."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef persist(self, key):\n        fut = self.execute(b'PERSIST', key)\n        return wait_convert(fut, bool)", "response": "Remove the existing timeout on key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pexpire(self, key, timeout):\n        if not isinstance(timeout, int):\n            raise TypeError(\"timeout argument must be int, not {!r}\"\n                            .format(timeout))\n        fut = self.execute(b'PEXPIRE', key, timeout)\n        return wait_convert(fut, bool)", "response": "Set a milliseconds timeout on a key."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pexpireat(self, key, timestamp):\n        if not isinstance(timestamp, int):\n            raise TypeError(\"timestamp argument must be int, not {!r}\"\n                            .format(timestamp))\n        fut = self.execute(b'PEXPIREAT', key, timestamp)\n        return wait_convert(fut, bool)", "response": "Set expire timestamp on key timestamp in milliseconds."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrenaming the entry in the specified key to the specified newkey.", "response": "def rename(self, key, newkey):\n        \"\"\"Renames key to newkey.\n\n        :raises ValueError: if key == newkey\n        \"\"\"\n        if key == newkey:\n            raise ValueError(\"key and newkey are the same\")\n        fut = self.execute(b'RENAME', key, newkey)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef renamenx(self, key, newkey):\n        if key == newkey:\n            raise ValueError(\"key and newkey are the same\")\n        fut = self.execute(b'RENAMENX', key, newkey)\n        return wait_convert(fut, bool)", "response": "Renames the key to newkey only if newkey does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a key associated with a value that is obtained via DUMP.", "response": "def restore(self, key, ttl, value):\n        \"\"\"Creates a key associated with a value that is obtained via DUMP.\"\"\"\n        return self.execute(b'RESTORE', key, ttl, value)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsort the elements in a list set or sorted set.", "response": "def sort(self, key, *get_patterns,\n             by=None, offset=None, count=None,\n             asc=None, alpha=False, store=None):\n        \"\"\"Sort the elements in a list, set or sorted set.\"\"\"\n        args = []\n        if by is not None:\n            args += [b'BY', by]\n        if offset is not None and count is not None:\n            args += [b'LIMIT', offset, count]\n        if get_patterns:\n            args += sum(([b'GET', pattern] for pattern in get_patterns), [])\n        if asc is not None:\n            args += [asc is True and b'ASC' or b'DESC']\n        if alpha:\n            args += [b'ALPHA']\n        if store is not None:\n            args += [b'STORE', store]\n        return self.execute(b'SORT', key, *args)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a key asynchronously in another thread.", "response": "def unlink(self, key, *keys):\n        \"\"\"Delete a key asynchronously in another thread.\"\"\"\n        return wait_convert(self.execute(b'UNLINK', key, *keys), int)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting one or more hash fields.", "response": "def hdel(self, key, field, *fields):\n        \"\"\"Delete one or more hash fields.\"\"\"\n        return self.execute(b'HDEL', key, field, *fields)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining if hash field exists.", "response": "def hexists(self, key, field):\n        \"\"\"Determine if hash field exists.\"\"\"\n        fut = self.execute(b'HEXISTS', key, field)\n        return wait_convert(fut, bool)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the value of a hash field.", "response": "def hget(self, key, field, *, encoding=_NOTSET):\n        \"\"\"Get the value of a hash field.\"\"\"\n        return self.execute(b'HGET', key, field, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hgetall(self, key, *, encoding=_NOTSET):\n        fut = self.execute(b'HGETALL', key, encoding=encoding)\n        return wait_make_dict(fut)", "response": "Get all the fields and values in a hash."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nincrements the integer value of a hash field by the given number.", "response": "def hincrby(self, key, field, increment=1):\n        \"\"\"Increment the integer value of a hash field by the given number.\"\"\"\n        return self.execute(b'HINCRBY', key, field, increment)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hincrbyfloat(self, key, field, increment=1.0):\n        fut = self.execute(b'HINCRBYFLOAT', key, field, increment)\n        return wait_convert(fut, float)", "response": "Increment the float value of a hash field by the given number."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets all the fields in a hash.", "response": "def hkeys(self, key, *, encoding=_NOTSET):\n        \"\"\"Get all the fields in a hash.\"\"\"\n        return self.execute(b'HKEYS', key, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the values of all the given fields.", "response": "def hmget(self, key, field, *fields, encoding=_NOTSET):\n        \"\"\"Get the values of all the given fields.\"\"\"\n        return self.execute(b'HMGET', key, field, *fields, encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset multiple hash fields to multiple values.", "response": "def hmset(self, key, field, value, *pairs):\n        \"\"\"Set multiple hash fields to multiple values.\"\"\"\n        if len(pairs) % 2 != 0:\n            raise TypeError(\"length of pairs must be even number\")\n        return wait_ok(self.execute(b'HMSET', key, field, value, *pairs))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hmset_dict(self, key, *args, **kwargs):\n        if not args and not kwargs:\n            raise TypeError(\"args or kwargs must be specified\")\n        pairs = ()\n        if len(args) > 1:\n            raise TypeError(\"single positional argument allowed\")\n        elif len(args) == 1:\n            if not isinstance(args[0], dict):\n                raise TypeError(\"args[0] must be dict\")\n            elif not args[0] and not kwargs:\n                raise ValueError(\"args[0] is empty dict\")\n            pairs = chain.from_iterable(args[0].items())\n        kwargs_pairs = chain.from_iterable(kwargs.items())\n        return wait_ok(self.execute(\n            b'HMSET', key, *chain(pairs, kwargs_pairs)))", "response": "Set multiple hash fields to multiple values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the string value of a hash field.", "response": "def hset(self, key, field, value):\n        \"\"\"Set the string value of a hash field.\"\"\"\n        return self.execute(b'HSET', key, field, value)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hsetnx(self, key, field, value):\n        return self.execute(b'HSETNX', key, field, value)", "response": "Set the value of a hash field only if the field does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hvals(self, key, *, encoding=_NOTSET):\n        return self.execute(b'HVALS', key, encoding=encoding)", "response": "Get all the values in a hash."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef hscan(self, key, cursor=0, match=None, count=None):\n        args = [key, cursor]\n        match is not None and args.extend([b'MATCH', match])\n        count is not None and args.extend([b'COUNT', count])\n        fut = self.execute(b'HSCAN', *args)\n        return wait_convert(fut, _make_pairs)", "response": "Incrementally iterate hash fields and associated values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ihscan(self, key, *, match=None, count=None):\n        return _ScanIter(lambda cur: self.hscan(key, cur,\n                                                match=match,\n                                                count=count))", "response": "Incrementally iterate sorted set items using async for. ihscan"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding one or more members to a sorted set or update its score.", "response": "def zadd(self, key, score, member, *pairs, exist=None):\n        \"\"\"Add one or more members to a sorted set or update its score.\n\n        :raises TypeError: score not int or float\n        :raises TypeError: length of pairs is not even number\n        \"\"\"\n        if not isinstance(score, (int, float)):\n            raise TypeError(\"score argument must be int or float\")\n        if len(pairs) % 2 != 0:\n            raise TypeError(\"length of pairs must be even number\")\n\n        scores = (item for i, item in enumerate(pairs) if i % 2 == 0)\n        if any(not isinstance(s, (int, float)) for s in scores):\n            raise TypeError(\"all scores must be int or float\")\n\n        args = []\n        if exist is self.ZSET_IF_EXIST:\n            args.append(b'XX')\n        elif exist is self.ZSET_IF_NOT_EXIST:\n            args.append(b'NX')\n\n        args.extend([score, member])\n        if pairs:\n            args.extend(pairs)\n        return self.execute(b'ZADD', key, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zcount(self, key, min=float('-inf'), max=float('inf'),\n               *, exclude=None):\n        \"\"\"Count the members in a sorted set with scores\n        within the given values.\n\n        :raises TypeError: min or max is not float or int\n        :raises ValueError: if min greater than max\n        \"\"\"\n        if not isinstance(min, (int, float)):\n            raise TypeError(\"min argument must be int or float\")\n        if not isinstance(max, (int, float)):\n            raise TypeError(\"max argument must be int or float\")\n        if min > max:\n            raise ValueError(\"min could not be greater than max\")\n        return self.execute(b'ZCOUNT', key,\n                            *_encode_min_max(exclude, min, max))", "response": "Return the number of members in a sorted set with scores\n        within the given values."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zincrby(self, key, increment, member):\n        if not isinstance(increment, (int, float)):\n            raise TypeError(\"increment argument must be int or float\")\n        fut = self.execute(b'ZINCRBY', key, increment, member)\n        return wait_convert(fut, int_or_float)", "response": "Increment the score of a member in a sorted set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zlexcount(self, key, min=b'-', max=b'+', include_min=True,\n                  include_max=True):\n        \"\"\"Count the number of members in a sorted set between a given\n        lexicographical range.\n\n        :raises TypeError: if min is not bytes\n        :raises TypeError: if max is not bytes\n        \"\"\"\n        if not isinstance(min, bytes):  # FIXME\n            raise TypeError(\"min argument must be bytes\")\n        if not isinstance(max, bytes):  # FIXME     Why only bytes?\n            raise TypeError(\"max argument must be bytes\")\n        if not min == b'-':\n            min = (b'[' if include_min else b'(') + min\n        if not max == b'+':\n            max = (b'[' if include_max else b'(') + max\n        return self.execute(b'ZLEXCOUNT', key, min, max)", "response": "Return the number of members in a sorted set between a given lexicographical range."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a range of members in a sorted set by lexicographical range.", "response": "def zrangebylex(self, key, min=b'-', max=b'+', include_min=True,\n                    include_max=True, offset=None, count=None,\n                    encoding=_NOTSET):\n        \"\"\"Return a range of members in a sorted set, by lexicographical range.\n\n        :raises TypeError: if min is not bytes\n        :raises TypeError: if max is not bytes\n        :raises TypeError: if both offset and count are not specified\n        :raises TypeError: if offset is not bytes\n        :raises TypeError: if count is not bytes\n        \"\"\"\n        if not isinstance(min, bytes):  # FIXME\n            raise TypeError(\"min argument must be bytes\")\n        if not isinstance(max, bytes):  # FIXME\n            raise TypeError(\"max argument must be bytes\")\n        if not min == b'-':\n            min = (b'[' if include_min else b'(') + min\n        if not max == b'+':\n            max = (b'[' if include_max else b'(') + max\n\n        if (offset is not None and count is None) or \\\n                (count is not None and offset is None):\n            raise TypeError(\"offset and count must both be specified\")\n        if offset is not None and not isinstance(offset, int):\n            raise TypeError(\"offset argument must be int\")\n        if count is not None and not isinstance(count, int):\n            raise TypeError(\"count argument must be int\")\n\n        args = []\n        if offset is not None and count is not None:\n            args.extend([b'LIMIT', offset, count])\n\n        return self.execute(b'ZRANGEBYLEX', key, min, max, *args,\n                            encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves one or more members from a sorted set.", "response": "def zrem(self, key, member, *members):\n        \"\"\"Remove one or more members from a sorted set.\"\"\"\n        return self.execute(b'ZREM', key, member, *members)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves all members in a sorted set between the given lexicographical range.", "response": "def zremrangebylex(self, key, min=b'-', max=b'+',\n                       include_min=True, include_max=True):\n        \"\"\"Remove all members in a sorted set between the given\n        lexicographical range.\n\n        :raises TypeError: if min is not bytes\n        :raises TypeError: if max is not bytes\n        \"\"\"\n        if not isinstance(min, bytes):  # FIXME\n            raise TypeError(\"min argument must be bytes\")\n        if not isinstance(max, bytes):  # FIXME\n            raise TypeError(\"max argument must be bytes\")\n        if not min == b'-':\n            min = (b'[' if include_min else b'(') + min\n        if not max == b'+':\n            max = (b'[' if include_max else b'(') + max\n        return self.execute(b'ZREMRANGEBYLEX', key, min, max)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef zremrangebyrank(self, key, start, stop):\n        if not isinstance(start, int):\n            raise TypeError(\"start argument must be int\")\n        if not isinstance(stop, int):\n            raise TypeError(\"stop argument must be int\")\n        return self.execute(b'ZREMRANGEBYRANK', key, start, stop)", "response": "Remove all members in a sorted set within the given indexes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef zremrangebyscore(self, key, min=float('-inf'), max=float('inf'),\n                         *, exclude=None):\n        \"\"\"Remove all members in a sorted set within the given scores.\n\n        :raises TypeError: if min or max is not int or float\n        \"\"\"\n        if not isinstance(min, (int, float)):\n            raise TypeError(\"min argument must be int or float\")\n        if not isinstance(max, (int, float)):\n            raise TypeError(\"max argument must be int or float\")\n\n        min, max = _encode_min_max(exclude, min, max)\n        return self.execute(b'ZREMRANGEBYSCORE', key, min, max)", "response": "Remove all members in a sorted set within the given scores."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef zrevrange(self, key, start, stop, withscores=False, encoding=_NOTSET):\n        if not isinstance(start, int):\n            raise TypeError(\"start argument must be int\")\n        if not isinstance(stop, int):\n            raise TypeError(\"stop argument must be int\")\n        if withscores:\n            args = [b'WITHSCORES']\n        else:\n            args = []\n        fut = self.execute(b'ZREVRANGE', key, start, stop, *args,\n                           encoding=encoding)\n        if withscores:\n            return wait_convert(fut, pairs_int_or_float)\n        return fut", "response": "Return a range of members in a sorted set by index."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a range of members in a sorted set by score.", "response": "def zrevrangebyscore(self, key, max=float('inf'), min=float('-inf'),\n                         *, exclude=None, withscores=False,\n                         offset=None, count=None, encoding=_NOTSET):\n        \"\"\"Return a range of members in a sorted set, by score,\n        with scores ordered from high to low.\n\n        :raises TypeError: if min or max is not float or int\n        :raises TypeError: if both offset and count are not specified\n        :raises TypeError: if offset is not int\n        :raises TypeError: if count is not int\n        \"\"\"\n        if not isinstance(min, (int, float)):\n            raise TypeError(\"min argument must be int or float\")\n        if not isinstance(max, (int, float)):\n            raise TypeError(\"max argument must be int or float\")\n\n        if (offset is not None and count is None) or \\\n                (count is not None and offset is None):\n            raise TypeError(\"offset and count must both be specified\")\n        if offset is not None and not isinstance(offset, int):\n            raise TypeError(\"offset argument must be int\")\n        if count is not None and not isinstance(count, int):\n            raise TypeError(\"count argument must be int\")\n\n        min, max = _encode_min_max(exclude, min, max)\n\n        args = []\n        if withscores:\n            args = [b'WITHSCORES']\n        if offset is not None and count is not None:\n            args.extend([b'LIMIT', offset, count])\n        fut = self.execute(b'ZREVRANGEBYSCORE', key, max, min, *args,\n                           encoding=encoding)\n        if withscores:\n            return wait_convert(fut, pairs_int_or_float)\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef zscore(self, key, member):\n        fut = self.execute(b'ZSCORE', key, member)\n        return wait_convert(fut, optional_int_or_float)", "response": "Get the score associated with the given member in a sorted set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd multiple sorted sets and store result in a new key.", "response": "def zunionstore(self, destkey, key, *keys,\n                    with_weights=False, aggregate=None):\n        \"\"\"Add multiple sorted sets and store result in a new key.\"\"\"\n        keys = (key,) + keys\n        numkeys = len(keys)\n        args = []\n        if with_weights:\n            assert all(isinstance(val, (list, tuple)) for val in keys), (\n                \"All key arguments must be (key, weight) tuples\")\n            weights = ['WEIGHTS']\n            for key, weight in keys:\n                args.append(key)\n                weights.append(weight)\n            args.extend(weights)\n        else:\n            args.extend(keys)\n\n        if aggregate is self.ZSET_AGGREGATE_SUM:\n            args.extend(('AGGREGATE', 'SUM'))\n        elif aggregate is self.ZSET_AGGREGATE_MAX:\n            args.extend(('AGGREGATE', 'MAX'))\n        elif aggregate is self.ZSET_AGGREGATE_MIN:\n            args.extend(('AGGREGATE', 'MIN'))\n        fut = self.execute(b'ZUNIONSTORE', destkey, numkeys, *args)\n        return fut"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef zscan(self, key, cursor=0, match=None, count=None):\n        args = []\n        if match is not None:\n            args += [b'MATCH', match]\n        if count is not None:\n            args += [b'COUNT', count]\n        fut = self.execute(b'ZSCAN', key, cursor, *args)\n\n        def _converter(obj):\n            return (int(obj[0]), pairs_int_or_float(obj[1]))\n\n        return wait_convert(fut, _converter)", "response": "Incrementally iterate sorted sets elements and associated scores."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef izscan(self, key, *, match=None, count=None):\n        return _ScanIter(lambda cur: self.zscan(key, cur,\n                                                match=match,\n                                                count=count))", "response": "Incrementally iterate sorted set items using async for. izscan"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zpopmin(self, key, count=None, *, encoding=_NOTSET):\n        if count is not None and not isinstance(count, int):\n            raise TypeError(\"count argument must be int\")\n\n        args = []\n        if count is not None:\n            args.extend([count])\n\n        fut = self.execute(b'ZPOPMIN', key, *args, encoding=encoding)\n        return fut", "response": "Removes and returns up to count members with the lowest scores\n        in the sorted set stored at key. Returns up to count members with the lowest scores\n        in the sorted set stored at key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef zpopmax(self, key, count=None, *, encoding=_NOTSET):\n        if count is not None and not isinstance(count, int):\n            raise TypeError(\"count argument must be int\")\n\n        args = []\n        if count is not None:\n            args.extend([count])\n\n        fut = self.execute(b'ZPOPMAX', key, *args, encoding=encoding)\n        return fut", "response": "Removes and returns up to count members with the highest scores\n        in the sorted set stored at key. Returns up to count members with the highest scores\n        in the sorted set stored at key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert a flat list of key - value pairs into an OrderedDict", "response": "def fields_to_dict(fields, type_=OrderedDict):\n    \"\"\"Convert a flat list of key/values into an OrderedDict\"\"\"\n    fields_iterator = iter(fields)\n    return type_(zip(fields_iterator, fields_iterator))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_messages_by_stream(messages_by_stream):\n    if messages_by_stream is None:\n        return []\n\n    parsed = []\n    for stream, messages in messages_by_stream:\n        for message_id, fields in parse_messages(messages):\n            parsed.append((stream, message_id, fields))\n    return parsed", "response": "Parses the messages returned by a stream into a list of tuples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a message to a stream.", "response": "def xadd(self, stream, fields, message_id=b'*', max_len=None,\n             exact_len=False):\n        \"\"\"Add a message to a stream.\"\"\"\n        args = []\n        if max_len is not None:\n            if exact_len:\n                args.extend((b'MAXLEN', max_len))\n            else:\n                args.extend((b'MAXLEN', b'~', max_len))\n\n        args.append(message_id)\n\n        for k, v in fields.items():\n            args.extend([k, v])\n        return self.execute(b'XADD', stream, *args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xrange(self, stream, start='-', stop='+', count=None):\n        if count is not None:\n            extra = ['COUNT', count]\n        else:\n            extra = []\n        fut = self.execute(b'XRANGE', stream, start, stop, *extra)\n        return wait_convert(fut, parse_messages)", "response": "Retrieve messages from a stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xrevrange(self, stream, start='+', stop='-', count=None):\n        if count is not None:\n            extra = ['COUNT', count]\n        else:\n            extra = []\n        fut = self.execute(b'XREVRANGE', stream, start, stop, *extra)\n        return wait_convert(fut, parse_messages)", "response": "Retrieve messages from a stream in reverse order."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xread(self, streams, timeout=0, count=None, latest_ids=None):\n        args = self._xread(streams, timeout, count, latest_ids)\n        fut = self.execute(b'XREAD', *args)\n        return wait_convert(fut, parse_messages_by_stream)", "response": "Perform a blocking read on the given streams."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a blocking read on the given consumer group", "response": "def xread_group(self, group_name, consumer_name, streams, timeout=0,\n                    count=None, latest_ids=None):\n        \"\"\"Perform a blocking read on the given stream as part of a consumer group\n\n        :raises ValueError: if the length of streams and latest_ids do\n                            not match\n        \"\"\"\n        args = self._xread(streams, timeout, count, latest_ids)\n        fut = self.execute(\n            b'XREADGROUP', b'GROUP', group_name, consumer_name, *args\n        )\n        return wait_convert(fut, parse_messages_by_stream)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef xgroup_create(self, stream, group_name, latest_id='$', mkstream=False):\n        args = [b'CREATE', stream, group_name, latest_id]\n        if mkstream:\n            args.append(b'MKSTREAM')\n        fut = self.execute(b'XGROUP', *args)\n        return wait_ok(fut)", "response": "Create a consumer group"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the latest ID for a consumer group", "response": "def xgroup_setid(self, stream, group_name, latest_id='$'):\n        \"\"\"Set the latest ID for a consumer group\"\"\"\n        fut = self.execute(b'XGROUP', b'SETID', stream, group_name, latest_id)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a consumer group", "response": "def xgroup_destroy(self, stream, group_name):\n        \"\"\"Delete a consumer group\"\"\"\n        fut = self.execute(b'XGROUP', b'DESTROY', stream, group_name)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef xgroup_delconsumer(self, stream, group_name, consumer_name):\n        fut = self.execute(\n            b'XGROUP', b'DELCONSUMER', stream, group_name, consumer_name\n        )\n        return wait_convert(fut, int)", "response": "Delete a specific consumer from a group"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef xpending(self, stream, group_name, start=None, stop=None, count=None,\n                 consumer=None):\n        \"\"\"Get information on pending messages for a stream\n\n        Returned data will vary depending on the presence (or not)\n        of the start/stop/count parameters. For more details see:\n        https://redis.io/commands/xpending\n\n        :raises ValueError: if the start/stop/count parameters are only\n                            partially specified\n        \"\"\"\n        # Returns: total pel messages, min id, max id, count\n        ssc = [start, stop, count]\n        ssc_count = len([v for v in ssc if v is not None])\n        if ssc_count != 3 and ssc_count != 0:\n            raise ValueError(\n                'Either specify non or all of the start/stop/count arguments'\n            )\n        if not any(ssc):\n            ssc = []\n\n        args = [stream, group_name] + ssc\n        if consumer:\n            args.append(consumer)\n        return self.execute(b'XPENDING', *args)", "response": "Get information on pending messages for a given stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclaiming a message for a given consumer", "response": "def xclaim(self, stream, group_name, consumer_name, min_idle_time,\n               id, *ids):\n        \"\"\"Claim a message for a given consumer\"\"\"\n        fut = self.execute(\n            b'XCLAIM', stream, group_name, consumer_name, min_idle_time,\n            id, *ids\n        )\n        return wait_convert(fut, parse_messages)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nacknowledges a message for a given consumer group", "response": "def xack(self, stream, group_name, id, *ids):\n        \"\"\"Acknowledge a message for a given consumer group\"\"\"\n        return self.execute(b'XACK', stream, group_name, id, *ids)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves consumers of a consumer group", "response": "def xinfo_consumers(self, stream, group_name):\n        \"\"\"Retrieve consumers of a consumer group\"\"\"\n        fut = self.execute(b'XINFO', b'CONSUMERS', stream, group_name)\n\n        return wait_convert(fut, parse_lists_to_dicts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving the consumer groups for a stream", "response": "def xinfo_groups(self, stream):\n        \"\"\"Retrieve the consumer groups for a stream\"\"\"\n        fut = self.execute(b'XINFO', b'GROUPS', stream)\n        return wait_convert(fut, parse_lists_to_dicts)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef xinfo_stream(self, stream):\n        fut = self.execute(b'XINFO', b'STREAM', stream)\n        return wait_make_dict(fut)", "response": "Retrieve information about the given stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve help regarding the XINFO sub - commands", "response": "def xinfo_help(self):\n        \"\"\"Retrieve help regarding the ``XINFO`` sub-commands\"\"\"\n        fut = self.execute(b'XINFO', b'HELP')\n        return wait_convert(fut, lambda l: b'\\n'.join(l))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwrapping up common functionality between xread and xread_group.", "response": "def _xread(self, streams, timeout=0, count=None, latest_ids=None):\n        \"\"\"Wraps up common functionality between ``xread()``\n        and ``xread_group()``\n\n        You should probably be using ``xread()`` or ``xread_group()`` directly.\n        \"\"\"\n        if latest_ids is None:\n            latest_ids = ['$'] * len(streams)\n        if len(streams) != len(latest_ids):\n            raise ValueError(\n                'The streams and latest_ids parameters must be of the '\n                'same length'\n            )\n\n        count_args = [b'COUNT', count] if count else []\n        if timeout is None:\n            block_args = []\n        elif not isinstance(timeout, int):\n            raise TypeError(\n                \"timeout argument must be int, not {!r}\".format(timeout))\n        else:\n            block_args = [b'BLOCK', timeout]\n        return block_args + count_args + [b'STREAMS'] + streams + latest_ids"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nacquiring a lock. This method blocks until the lock is unlocked and returns True.", "response": "def acquire(self):\n        \"\"\"Acquire a lock.\n        This method blocks until the lock is unlocked, then sets it to\n        locked and returns True.\n        \"\"\"\n        if not self._locked and all(w.cancelled() for w in self._waiters):\n            self._locked = True\n            return True\n\n        fut = self._loop.create_future()\n\n        self._waiters.append(fut)\n        try:\n            yield from fut\n            self._locked = True\n            return True\n        except asyncio.CancelledError:\n            if not self._locked:  # pragma: no cover\n                self._wake_up_first()\n            raise\n        finally:\n            self._waiters.remove(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _wake_up_first(self):\n        for fut in self._waiters:\n            if not fut.done():\n                fut.set_result(True)\n                break", "response": "Wake up the first waiter who isn t cancelled."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nassigns new hash slots to receiving node.", "response": "def cluster_add_slots(self, slot, *slots):\n        \"\"\"Assign new hash slots to receiving node.\"\"\"\n        slots = (slot,) + slots\n        if not all(isinstance(s, int) for s in slots):\n            raise TypeError(\"All parameters must be of type int\")\n        fut = self.execute(b'CLUSTER', b'ADDSLOTS', *slots)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cluster_count_key_in_slots(self, slot):\n        if not isinstance(slot, int):\n            raise TypeError(\"Expected slot to be of type int, got {}\"\n                            .format(type(slot)))\n        return self.execute(b'CLUSTER', b'COUNTKEYSINSLOT', slot)", "response": "Return the number of local keys in the specified hash slot."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets hash slots as unbound in receiving node.", "response": "def cluster_del_slots(self, slot, *slots):\n        \"\"\"Set hash slots as unbound in receiving node.\"\"\"\n        slots = (slot,) + slots\n        if not all(isinstance(s, int) for s in slots):\n            raise TypeError(\"All parameters must be of type int\")\n        fut = self.execute(b'CLUSTER', b'DELSLOTS', *slots)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cluster_forget(self, node_id):\n        fut = self.execute(b'CLUSTER', b'FORGET', node_id)\n        return wait_ok(fut)", "response": "Remove a node from the nodes table."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the local key names in the specified hash slot.", "response": "def cluster_get_keys_in_slots(self, slot, count, *, encoding):\n        \"\"\"Return local key names in the specified hash slot.\"\"\"\n        return self.execute(b'CLUSTER', b'GETKEYSINSLOT', slot, count,\n                            encoding=encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cluster_meet(self, ip, port):\n        fut = self.execute(b'CLUSTER', b'MEET', ip, port)\n        return wait_ok(fut)", "response": "Force a node cluster to handshake with another node."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef cluster_replicate(self, node_id):\n        fut = self.execute(b'CLUSTER', b'REPLICATE', node_id)\n        return wait_ok(fut)", "response": "Reconfigure a node as a slave of the specified master node."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets a Redis Cluster node.", "response": "def cluster_reset(self, *, hard=False):\n        \"\"\"Reset a Redis Cluster node.\"\"\"\n        reset = hard and b'HARD' or b'SOFT'\n        fut = self.execute(b'CLUSTER', b'RESET', reset)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the configuration epoch in a new node.", "response": "def cluster_set_config_epoch(self, config_epoch):\n        \"\"\"Set the configuration epoch in a new node.\"\"\"\n        fut = self.execute(b'CLUSTER', b'SET-CONFIG-EPOCH', config_epoch)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef master_for(self, service):\n        # TODO: make it coroutine and connect minsize connections\n        if service not in self._masters:\n            self._masters[service] = ManagedPool(\n                self, service, is_master=True,\n                db=self._redis_db,\n                password=self._redis_password,\n                encoding=self._redis_encoding,\n                minsize=self._redis_minsize,\n                maxsize=self._redis_maxsize,\n                ssl=self._redis_ssl,\n                parser=self._parser_class,\n                loop=self._loop)\n        return self._masters[service]", "response": "Returns a managed pool for the requested service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef slave_for(self, service):\n        # TODO: make it coroutine and connect minsize connections\n        if service not in self._slaves:\n            self._slaves[service] = ManagedPool(\n                self, service, is_master=False,\n                db=self._redis_db,\n                password=self._redis_password,\n                encoding=self._redis_encoding,\n                minsize=self._redis_minsize,\n                maxsize=self._redis_maxsize,\n                ssl=self._redis_ssl,\n                parser=self._parser_class,\n                loop=self._loop)\n        return self._slaves[service]", "response": "Returns a managed pool for the requested service."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def discover(self, timeout=None):    # TODO: better name?\n        # TODO: check not closed\n        # TODO: discovery must be done with some customizable timeout.\n        if timeout is None:\n            timeout = self.discover_timeout\n        tasks = []\n        pools = []\n        for addr in self._sentinels:    # iterate over unordered set\n            tasks.append(self._connect_sentinel(addr, timeout, pools))\n        done, pending = await asyncio.wait(tasks, loop=self._loop,\n                                           return_when=ALL_COMPLETED)\n        assert not pending, (\"Expected all tasks to complete\", done, pending)\n\n        for task in done:\n            result = task.result()\n            if isinstance(result, Exception):\n                continue    # FIXME\n        if not pools:\n            raise Exception(\"Could not connect to any sentinel\")\n        pools, self._pools[:] = self._pools[:], pools\n        # TODO: close current connections\n        for pool in pools:\n            pool.close()\n            await pool.wait_closed()\n\n        # TODO: discover peer sentinels\n        for pool in self._pools:\n            await pool.execute_pubsub(\n                b'psubscribe', self._monitor.pattern('*'))", "response": "Discover sentinels and all monitored services within given timeout."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntrying to connect to Sentinel returning either connections pool or exception.", "response": "async def _connect_sentinel(self, address, timeout, pools):\n        \"\"\"Try to connect to specified Sentinel returning either\n        connections pool or exception.\n        \"\"\"\n        try:\n            with async_timeout(timeout, loop=self._loop):\n                pool = await create_pool(\n                    address, minsize=1, maxsize=2,\n                    parser=self._parser_class,\n                    loop=self._loop)\n            pools.append(pool)\n            return pool\n        except asyncio.TimeoutError as err:\n            sentinel_logger.debug(\n                \"Failed to connect to Sentinel(%r) within %ss timeout\",\n                address, timeout)\n            return err\n        except Exception as err:\n            sentinel_logger.debug(\n                \"Error connecting to Sentinel(%r): %r\", address, err)\n            return err"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def discover_master(self, service, timeout):\n        # TODO: get lock\n        idle_timeout = timeout\n        # FIXME: single timeout used 4 times;\n        #   meaning discovery can take up to:\n        #   3 * timeout * (sentinels count)\n        #\n        #   having one global timeout also can leed to\n        #   a problem when not all sentinels are checked.\n\n        # use a copy, cause pools can change\n        pools = self._pools[:]\n        for sentinel in pools:\n            try:\n                with async_timeout(timeout, loop=self._loop):\n                    address = await self._get_masters_address(\n                        sentinel, service)\n\n                pool = self._masters[service]\n                with async_timeout(timeout, loop=self._loop), \\\n                        contextlib.ExitStack() as stack:\n                    conn = await pool._create_new_connection(address)\n                    stack.callback(conn.close)\n                    await self._verify_service_role(conn, 'master')\n                    stack.pop_all()\n\n                return conn\n            except asyncio.CancelledError:\n                # we must correctly handle CancelledError(s):\n                #   application may be stopped or function can be cancelled\n                #   by outer timeout, so we must stop the look up.\n                raise\n            except asyncio.TimeoutError:\n                continue\n            except DiscoverError as err:\n                sentinel_logger.debug(\"DiscoverError(%r, %s): %r\",\n                                      sentinel, service, err)\n                await asyncio.sleep(idle_timeout, loop=self._loop)\n                continue\n            except RedisError as err:\n                raise MasterReplyError(\"Service {} error\".format(service), err)\n            except Exception:\n                # TODO: clear (drop) connections to schedule reconnect\n                await asyncio.sleep(idle_timeout, loop=self._loop)\n                continue\n        else:\n            raise MasterNotFoundError(\"No master found for {}\".format(service))", "response": "Perform a master discovery for the specified service."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def discover_slave(self, service, timeout, **kwargs):\n        # TODO: use kwargs to change how slaves are picked up\n        #   (eg: round-robin, priority, random, etc)\n        idle_timeout = timeout\n        pools = self._pools[:]\n        for sentinel in pools:\n            try:\n                with async_timeout(timeout, loop=self._loop):\n                    address = await self._get_slave_address(\n                        sentinel, service)  # add **kwargs\n                pool = self._slaves[service]\n                with async_timeout(timeout, loop=self._loop), \\\n                        contextlib.ExitStack() as stack:\n                    conn = await pool._create_new_connection(address)\n                    stack.callback(conn.close)\n                    await self._verify_service_role(conn, 'slave')\n                    stack.pop_all()\n                return conn\n            except asyncio.CancelledError:\n                raise\n            except asyncio.TimeoutError:\n                continue\n            except DiscoverError:\n                await asyncio.sleep(idle_timeout, loop=self._loop)\n                continue\n            except RedisError as err:\n                raise SlaveReplyError(\"Service {} error\".format(service), err)\n            except Exception:\n                await asyncio.sleep(idle_timeout, loop=self._loop)\n                continue\n        raise SlaveNotFoundError(\"No slave found for {}\".format(service))", "response": "Perform a slave discovery for the specified service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating high - level Redis interface.", "response": "async def create_redis(address, *, db=None, password=None, ssl=None,\n                       encoding=None, commands_factory=Redis,\n                       parser=None, timeout=None,\n                       connection_cls=None, loop=None):\n    \"\"\"Creates high-level Redis interface.\n\n    This function is a coroutine.\n    \"\"\"\n    conn = await create_connection(address, db=db,\n                                   password=password,\n                                   ssl=ssl,\n                                   encoding=encoding,\n                                   parser=parser,\n                                   timeout=timeout,\n                                   connection_cls=connection_cls,\n                                   loop=loop)\n    return commands_factory(conn)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def create_redis_pool(address, *, db=None, password=None, ssl=None,\n                            encoding=None, commands_factory=Redis,\n                            minsize=1, maxsize=10, parser=None,\n                            timeout=None, pool_cls=None,\n                            connection_cls=None, loop=None):\n    \"\"\"Creates high-level Redis interface.\n\n    This function is a coroutine.\n    \"\"\"\n    pool = await create_pool(address, db=db,\n                             password=password,\n                             ssl=ssl,\n                             encoding=encoding,\n                             minsize=minsize,\n                             maxsize=maxsize,\n                             parser=parser,\n                             create_connection_timeout=timeout,\n                             pool_cls=pool_cls,\n                             connection_cls=connection_cls,\n                             loop=loop)\n    return commands_factory(pool)", "response": "Create high - level Redis interface."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef echo(self, message, *, encoding=_NOTSET):\n        return self.execute('ECHO', message, encoding=encoding)", "response": "Echo the given string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ping(self, message=_NOTSET, *, encoding=_NOTSET):\n        if message is not _NOTSET:\n            args = (message,)\n        else:\n            args = ()\n        return self.execute('PING', *args, encoding=encoding)", "response": "Ping the server.\n\n        Accept optional echo message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sadd(self, key, member, *members):\n        return self.execute(b'SADD', key, member, *members)", "response": "Add one or more members to a set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sdiffstore(self, destkey, key, *keys):\n        return self.execute(b'SDIFFSTORE', destkey, key, *keys)", "response": "Subtract multiple sets and store the resulting set in a key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sinterstore(self, destkey, key, *keys):\n        return self.execute(b'SINTERSTORE', destkey, key, *keys)", "response": "Intersect multiple sets and store the resulting set in a key."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef smembers(self, key, *, encoding=_NOTSET):\n        return self.execute(b'SMEMBERS', key, encoding=encoding)", "response": "Get all the members in a set."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef smove(self, sourcekey, destkey, member):\n        return self.execute(b'SMOVE', sourcekey, destkey, member)", "response": "Move a member from one set to another."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef spop(self, key, count=None, *, encoding=_NOTSET):\n        args = [key]\n        if count is not None:\n            args.append(count)\n        return self.execute(b'SPOP', *args, encoding=encoding)", "response": "Remove and return one or multiple random members from a set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef srandmember(self, key, count=None, *, encoding=_NOTSET):\n        args = [key]\n        count is not None and args.append(count)\n        return self.execute(b'SRANDMEMBER', *args, encoding=encoding)", "response": "Get one or multiple random members from a set."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove one or more members from a set.", "response": "def srem(self, key, member, *members):\n        \"\"\"Remove one or more members from a set.\"\"\"\n        return self.execute(b'SREM', key, member, *members)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sunionstore(self, destkey, key, *keys):\n        return self.execute(b'SUNIONSTORE', destkey, key, *keys)", "response": "Add multiple sets and store the resulting set in a key."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the specified elements to the specified HyperLogLog.", "response": "def pfadd(self, key, value, *values):\n        \"\"\"Adds the specified elements to the specified HyperLogLog.\"\"\"\n        return self.execute(b'PFADD', key, value, *values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges N different HyperLogLogs into a single HyperLogLog.", "response": "def pfmerge(self, destkey, sourcekey, *sourcekeys):\n        \"\"\"Merge N different HyperLogLogs into a single one.\"\"\"\n        fut = self.execute(b'PFMERGE', destkey, sourcekey, *sourcekeys)\n        return wait_ok(fut)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a schema from. / schemas and return it.", "response": "def load_schema(name):\n    \"\"\"\n    Load a schema from ./schemas/``name``.json and return it.\n\n    \"\"\"\n\n    data = pkgutil.get_data('jsonschema', \"schemas/{0}.json\".format(name))\n    return json.loads(data.decode(\"utf-8\"))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef find_additional_properties(instance, schema):\n\n    properties = schema.get(\"properties\", {})\n    patterns = \"|\".join(schema.get(\"patternProperties\", {}))\n    for property in instance:\n        if property not in properties:\n            if patterns and re.search(patterns, property):\n                continue\n            yield property", "response": "Find additional properties for the given instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an error message for extra items or properties.", "response": "def extras_msg(extras):\n    \"\"\"\n    Create an error message for extra items or properties.\n\n    \"\"\"\n\n    if len(extras) == 1:\n        verb = \"was\"\n    else:\n        verb = \"were\"\n    return \", \".join(repr(extra) for extra in extras), verb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef types_msg(instance, types):\n\n    reprs = []\n    for type in types:\n        try:\n            reprs.append(repr(type[\"name\"]))\n        except Exception:\n            reprs.append(repr(type))\n    return \"%r is not of type %s\" % (instance, \", \".join(reprs))", "response": "Create an error message for a failure to match the given types."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unbool(element, true=object(), false=object()):\n\n    if element is True:\n        return true\n    elif element is False:\n        return false\n    return element", "response": "A hack to make True and False unique for unique."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uniq(container):\n\n    try:\n        return len(set(unbool(i) for i in container)) == len(container)\n    except TypeError:\n        try:\n            sort = sorted(unbool(i) for i in container)\n            sliced = itertools.islice(sort, 1, None)\n            for i, j in zip(sort, sliced):\n                if i == j:\n                    return False\n        except (NotImplementedError, TypeError):\n            seen = []\n            for e in container:\n                e = unbool(e)\n                if e in seen:\n                    return False\n                seen.append(e)\n    return True", "response": "Check if all of a container s elements are unique."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef checks(self, format, raises=()):\n\n        def _checks(func):\n            self.checkers[format] = (func, raises)\n            return func\n        return _checks", "response": "Decorator to register a function as validating a new format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check(self, instance, format):\n\n        if format not in self.checkers:\n            return\n\n        func, raises = self.checkers[format]\n        result, cause = None, None\n        try:\n            result = func(instance)\n        except raises as e:\n            cause = e\n        if not result:\n            raise FormatError(\n                \"%r is not a %r\" % (instance, format), cause=cause,\n            )", "response": "Checks whether the instance conforms to the given format."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conforms(self, instance, format):\n\n        try:\n            self.check(instance, format)\n        except FormatError:\n            return False\n        else:\n            return True", "response": "Checks whether the instance conforms to the given format."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate legacy type checks out of JSON - type - name - to - type mappings.", "response": "def _generate_legacy_type_checks(types=()):\n    \"\"\"\n    Generate newer-style type checks out of JSON-type-name-to-type mappings.\n\n    Arguments:\n\n        types (dict):\n\n            A mapping of type names to their Python types\n\n    Returns:\n\n        A dictionary of definitions to pass to `TypeChecker`\n    \"\"\"\n    types = dict(types)\n\n    def gen_type_check(pytypes):\n        pytypes = _utils.flatten(pytypes)\n\n        def type_check(checker, instance):\n            if isinstance(instance, bool):\n                if bool not in pytypes:\n                    return False\n            return isinstance(instance, pytypes)\n\n        return type_check\n\n    definitions = {}\n    for typename, pytypes in iteritems(types):\n        definitions[typename] = gen_type_check(pytypes)\n\n    return definitions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nregister the decorated validator for a ``version`` of the specification. Registered validators and their meta schemas will be considered when parsing ``$schema`` properties' URIs. Arguments: version (str): An identifier to use as the version's name Returns: callable: a class decorator to decorate the validator with the version", "response": "def validates(version):\n    \"\"\"\n    Register the decorated validator for a ``version`` of the specification.\n\n    Registered validators and their meta schemas will be considered when\n    parsing ``$schema`` properties' URIs.\n\n    Arguments:\n\n        version (str):\n\n            An identifier to use as the version's name\n\n    Returns:\n\n        callable: a class decorator to decorate the validator with the version\n    \"\"\"\n\n    def _validates(cls):\n        validators[version] = cls\n        meta_schema_id = cls.ID_OF(cls.META_SCHEMA)\n        if meta_schema_id:\n            meta_schemas[meta_schema_id] = cls\n        return cls\n    return _validates"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new validator class for the given meta schema and version.", "response": "def create(\n    meta_schema,\n    validators=(),\n    version=None,\n    default_types=None,\n    type_checker=None,\n    id_of=_id_of,\n):\n    \"\"\"\n    Create a new validator class.\n\n    Arguments:\n\n        meta_schema (collections.Mapping):\n\n            the meta schema for the new validator class\n\n        validators (collections.Mapping):\n\n            a mapping from names to callables, where each callable will\n            validate the schema property with the given name.\n\n            Each callable should take 4 arguments:\n\n                1. a validator instance,\n                2. the value of the property being validated within the\n                   instance\n                3. the instance\n                4. the schema\n\n        version (str):\n\n            an identifier for the version that this validator class will\n            validate. If provided, the returned validator class will have its\n            ``__name__`` set to include the version, and also will have\n            `jsonschema.validators.validates` automatically called for the\n            given version.\n\n        type_checker (jsonschema.TypeChecker):\n\n            a type checker, used when applying the :validator:`type` validator.\n\n            If unprovided, a `jsonschema.TypeChecker` will be created with\n            a set of default types typical of JSON Schema drafts.\n\n        default_types (collections.Mapping):\n\n            .. deprecated:: 3.0.0\n\n                Please use the type_checker argument instead.\n\n            If set, it provides mappings of JSON types to Python types that\n            will be converted to functions and redefined in this object's\n            `jsonschema.TypeChecker`.\n\n        id_of (callable):\n\n            A function that given a schema, returns its ID.\n\n    Returns:\n\n        a new `jsonschema.IValidator` class\n    \"\"\"\n\n    if default_types is not None:\n        if type_checker is not None:\n            raise TypeError(\n                \"Do not specify default_types when providing a type checker.\",\n            )\n        _created_with_default_types = True\n        warn(\n            (\n                \"The default_types argument is deprecated. \"\n                \"Use the type_checker argument instead.\"\n            ),\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        type_checker = _types.TypeChecker(\n            type_checkers=_generate_legacy_type_checks(default_types),\n        )\n    else:\n        default_types = _DEPRECATED_DEFAULT_TYPES\n        if type_checker is None:\n            _created_with_default_types = False\n            type_checker = _TYPE_CHECKER_FOR_DEPRECATED_DEFAULT_TYPES\n        elif type_checker is _TYPE_CHECKER_FOR_DEPRECATED_DEFAULT_TYPES:\n            _created_with_default_types = False\n        else:\n            _created_with_default_types = None\n\n    @add_metaclass(_DefaultTypesDeprecatingMetaClass)\n    class Validator(object):\n\n        VALIDATORS = dict(validators)\n        META_SCHEMA = dict(meta_schema)\n        TYPE_CHECKER = type_checker\n        ID_OF = staticmethod(id_of)\n\n        DEFAULT_TYPES = property(_DEFAULT_TYPES)\n        _DEFAULT_TYPES = dict(default_types)\n        _CREATED_WITH_DEFAULT_TYPES = _created_with_default_types\n\n        def __init__(\n            self,\n            schema,\n            types=(),\n            resolver=None,\n            format_checker=None,\n        ):\n            if types:\n                warn(\n                    (\n                        \"The types argument is deprecated. Provide \"\n                        \"a type_checker to jsonschema.validators.extend \"\n                        \"instead.\"\n                    ),\n                    DeprecationWarning,\n                    stacklevel=2,\n                )\n\n                self.TYPE_CHECKER = self.TYPE_CHECKER.redefine_many(\n                    _generate_legacy_type_checks(types),\n                )\n\n            if resolver is None:\n                resolver = RefResolver.from_schema(schema, id_of=id_of)\n\n            self.resolver = resolver\n            self.format_checker = format_checker\n            self.schema = schema\n\n        @classmethod\n        def check_schema(cls, schema):\n            for error in cls(cls.META_SCHEMA).iter_errors(schema):\n                raise exceptions.SchemaError.create_from(error)\n\n        def iter_errors(self, instance, _schema=None):\n            if _schema is None:\n                _schema = self.schema\n\n            if _schema is True:\n                return\n            elif _schema is False:\n                yield exceptions.ValidationError(\n                    \"False schema does not allow %r\" % (instance,),\n                    validator=None,\n                    validator_value=None,\n                    instance=instance,\n                    schema=_schema,\n                )\n                return\n\n            scope = id_of(_schema)\n            if scope:\n                self.resolver.push_scope(scope)\n            try:\n                ref = _schema.get(u\"$ref\")\n                if ref is not None:\n                    validators = [(u\"$ref\", ref)]\n                else:\n                    validators = iteritems(_schema)\n\n                for k, v in validators:\n                    validator = self.VALIDATORS.get(k)\n                    if validator is None:\n                        continue\n\n                    errors = validator(self, v, instance, _schema) or ()\n                    for error in errors:\n                        # set details if not already set by the called fn\n                        error._set(\n                            validator=k,\n                            validator_value=v,\n                            instance=instance,\n                            schema=_schema,\n                        )\n                        if k != u\"$ref\":\n                            error.schema_path.appendleft(k)\n                        yield error\n            finally:\n                if scope:\n                    self.resolver.pop_scope()\n\n        def descend(self, instance, schema, path=None, schema_path=None):\n            for error in self.iter_errors(instance, schema):\n                if path is not None:\n                    error.path.appendleft(path)\n                if schema_path is not None:\n                    error.schema_path.appendleft(schema_path)\n                yield error\n\n        def validate(self, *args, **kwargs):\n            for error in self.iter_errors(*args, **kwargs):\n                raise error\n\n        def is_type(self, instance, type):\n            try:\n                return self.TYPE_CHECKER.is_type(instance, type)\n            except exceptions.UndefinedTypeCheck:\n                raise exceptions.UnknownType(type, instance, self.schema)\n\n        def is_valid(self, instance, _schema=None):\n            error = next(self.iter_errors(instance, _schema), None)\n            return error is None\n\n    if version is not None:\n        Validator = validates(version)(Validator)\n        Validator.__name__ = version.title().replace(\" \", \"\") + \"Validator\"\n\n    return Validator"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef extend(validator, validators=(), version=None, type_checker=None):\n\n    all_validators = dict(validator.VALIDATORS)\n    all_validators.update(validators)\n\n    if type_checker is None:\n        type_checker = validator.TYPE_CHECKER\n    elif validator._CREATED_WITH_DEFAULT_TYPES:\n        raise TypeError(\n            \"Cannot extend a validator created with default_types \"\n            \"with a type_checker. Update the validator to use a \"\n            \"type_checker when created.\"\n        )\n    return create(\n        meta_schema=validator.META_SCHEMA,\n        validators=all_validators,\n        version=version,\n        type_checker=type_checker,\n        id_of=validator.ID_OF,\n    )", "response": "Create a new validator class by extending an existing one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(instance, schema, cls=None, *args, **kwargs):\n    if cls is None:\n        cls = validator_for(schema)\n\n    cls.check_schema(schema)\n    validator = cls(schema, *args, **kwargs)\n    error = exceptions.best_match(validator.iter_errors(instance))\n    if error is not None:\n        raise error", "response": "Validate an instance under the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validator_for(schema, default=_LATEST_VERSION):\n    if schema is True or schema is False or u\"$schema\" not in schema:\n        return default\n    if schema[u\"$schema\"] not in meta_schemas:\n        warn(\n            (\n                \"The metaschema specified by $schema was not found. \"\n                \"Using the latest draft to validate, but this will raise \"\n                \"an error in the future.\"\n            ),\n            DeprecationWarning,\n            stacklevel=2,\n        )\n    return meta_schemas.get(schema[u\"$schema\"], _LATEST_VERSION)", "response": "Returns the appropriate validator class for validating the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_schema(cls, schema, id_of=_id_of, *args, **kwargs):\n\n        return cls(base_uri=id_of(schema), referrer=schema, *args, **kwargs)", "response": "Construct a RefResolver from a JSON schema object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nresolve the given ``ref`` and enter its resolution scope. Exits the scope on exit of this context manager. Arguments: ref (str): The reference to resolve", "response": "def resolving(self, ref):\n        \"\"\"\n        Resolve the given ``ref`` and enter its resolution scope.\n\n        Exits the scope on exit of this context manager.\n\n        Arguments:\n\n            ref (str):\n\n                The reference to resolve\n        \"\"\"\n\n        url, resolved = self.resolve(ref)\n        self.push_scope(url)\n        try:\n            yield resolved\n        finally:\n            self.pop_scope()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resolve_remote(self, uri):\n        try:\n            import requests\n        except ImportError:\n            requests = None\n\n        scheme = urlsplit(uri).scheme\n\n        if scheme in self.handlers:\n            result = self.handlers[scheme](uri)\n        elif scheme in [u\"http\", u\"https\"] and requests:\n            # Requests has support for detecting the correct encoding of\n            # json over http\n            result = requests.get(uri).json()\n        else:\n            # Otherwise, pass off to urllib and assume utf-8\n            with urlopen(uri) as url:\n                result = json.loads(url.read().decode(\"utf-8\"))\n\n        if self.cache_remote:\n            self.store[uri] = result\n        return result", "response": "Resolve a remote URI."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef total_errors(self):\n\n        child_errors = sum(len(tree) for _, tree in iteritems(self._contents))\n        return len(self.errors) + child_errors", "response": "Returns the total number of errors in the entire tree including children."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup(app):\n\n    app.add_config_value(\"cache_path\", \"_cache\", \"\")\n\n    try:\n        os.makedirs(app.config.cache_path)\n    except OSError as error:\n        if error.errno != errno.EEXIST:\n            raise\n\n    path = os.path.join(app.config.cache_path, \"spec.html\")\n    spec = fetch_or_load(path)\n    app.add_role(\"validator\", docutils_sucks(spec))", "response": "Install the plugin.\n\n    Arguments:\n\n        app (sphinx.application.Sphinx):\n\n            the Sphinx application context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch_or_load(spec_path):\n\n    headers = {}\n\n    try:\n        modified = datetime.utcfromtimestamp(os.path.getmtime(spec_path))\n        date = modified.strftime(\"%a, %d %b %Y %I:%M:%S UTC\")\n        headers[\"If-Modified-Since\"] = date\n    except OSError as error:\n        if error.errno != errno.ENOENT:\n            raise\n\n    request = urllib.Request(VALIDATION_SPEC, headers=headers)\n    response = urllib.urlopen(request, cafile=certifi.where())\n\n    if response.code == 200:\n        with open(spec_path, \"w+b\") as spec:\n            spec.writelines(response)\n            spec.seek(0)\n            return html.parse(spec)\n\n    with open(spec_path) as spec:\n        return html.parse(spec)", "response": "Fetch a new specification or use the cache if it s current."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef docutils_sucks(spec):\n\n    base_url = VALIDATION_SPEC\n    ref_url = \"https://json-schema.org/draft-04/json-schema-core.html#rfc.section.4.1\"\n    schema_url = \"https://json-schema.org/draft-04/json-schema-core.html#rfc.section.6\"\n\n    def validator(name, raw_text, text, lineno, inliner):\n        \"\"\"\n        Link to the JSON Schema documentation for a validator.\n\n        Arguments:\n\n            name (str):\n\n                the name of the role in the document\n\n            raw_source (str):\n\n                the raw text (role with argument)\n\n            text (str):\n\n                the argument given to the role\n\n            lineno (int):\n\n                the line number\n\n            inliner (docutils.parsers.rst.states.Inliner):\n\n                the inliner\n\n        Returns:\n\n            tuple:\n\n                a 2-tuple of nodes to insert into the document and an\n                iterable of system messages, both possibly empty\n\n        \"\"\"\n\n        if text == \"$ref\":\n            return [nodes.reference(raw_text, text, refuri=ref_url)], []\n        elif text == \"$schema\":\n            return [nodes.reference(raw_text, text, refuri=schema_url)], []\n\n        # find the header in the validation spec containing matching text\n        header = spec.xpath(\"//h1[contains(text(), '{0}')]\".format(text))\n\n        if len(header) == 0:\n            inliner.reporter.warning(\n                \"Didn't find a target for {0}\".format(text),\n            )\n            uri = base_url\n        else:\n            if len(header) > 1:\n                inliner.reporter.info(\n                    \"Found multiple targets for {0}\".format(text),\n                )\n\n            # get the href from link in the header\n            uri = base_url + header[0].find('a').attrib[\"href\"]\n\n        reference = nodes.reference(raw_text, text, refuri=uri)\n        return [reference], []\n\n    return validator", "response": "This function is used to create a new object that can be used to create a new object. It can be used to create a new object that can be used to create a new object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _importAndCheckStack(importName):\n    try:\n        return __import__(importName)\n    except ImportError:\n        excType, excValue, excTraceback = sys.exc_info()\n        while excTraceback:\n            execName = excTraceback.tb_frame.f_globals[\"__name__\"]\n            # in Python 2 execName is None when an ImportError is encountered,\n            # where in Python 3 execName is equal to the importName.\n            if execName is None or execName == importName:\n                reraise(excValue, excTraceback)\n            excTraceback = excTraceback.tb_next\n        raise _NoModuleFound()", "response": "Imports the given name as a module then walks the stack to determine whether the module was found."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving a Python object by its fully qualified name.", "response": "def namedAny(name):\n    \"\"\"\n    Retrieve a Python object by its fully qualified name from the global Python\n    module namespace.  The first part of the name, that describes a module,\n    will be discovered and imported.  Each subsequent part of the name is\n    treated as the name of an attribute of the object specified by all of the\n    name which came before it.  For example, the fully-qualified name of this\n    object is 'twisted.python.reflect.namedAny'.\n\n    @type name: L{str}\n    @param name: The name of the object to return.\n\n    @raise InvalidName: If the name is an empty string, starts or ends with\n        a '.', or is otherwise syntactically incorrect.\n\n    @raise ModuleNotFound: If the name is syntactically correct but the\n        module it specifies cannot be imported because it does not appear to\n        exist.\n\n    @raise ObjectNotFound: If the name is syntactically correct, includes at\n        least one '.', but the module it specifies cannot be imported because\n        it does not appear to exist.\n\n    @raise AttributeError: If an attribute of an object along the way cannot be\n        accessed, or a module along the way is not found.\n\n    @return: the Python object identified by 'name'.\n    \"\"\"\n    if not name:\n        raise InvalidName('Empty module name')\n\n    names = name.split('.')\n\n    # if the name starts or ends with a '.' or contains '..', the __import__\n    # will raise an 'Empty module name' error. This will provide a better error\n    # message.\n    if '' in names:\n        raise InvalidName(\n            \"name must be a string giving a '.'-separated list of Python \"\n            \"identifiers, not %r\" % (name,))\n\n    topLevelPackage = None\n    moduleNames = names[:]\n    while not topLevelPackage:\n        if moduleNames:\n            trialname = '.'.join(moduleNames)\n            try:\n                topLevelPackage = _importAndCheckStack(trialname)\n            except _NoModuleFound:\n                moduleNames.pop()\n        else:\n            if len(names) == 1:\n                raise ModuleNotFound(\"No module named %r\" % (name,))\n            else:\n                raise ObjectNotFound('%r does not name an object' % (name,))\n\n    obj = topLevelPackage\n    for n in names[1:]:\n        obj = getattr(obj, n)\n\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of faces that share an edge making them adjacent.", "response": "def face_adjacency(faces=None,\n                   mesh=None,\n                   return_edges=False):\n    \"\"\"\n    Returns an (n,2) list of face indices.\n    Each pair of faces in the list shares an edge, making them adjacent.\n\n\n    Parameters\n    ----------\n    faces : (n, 3) int, or None\n        List of vertex indices representing triangles\n    mesh : Trimesh object\n        If passed will used cached edges instead of faces\n    return_edges : bool\n        Return the edges shared by adjacent faces\n\n    Returns\n    ---------\n    adjacency : (m,2) int\n        Indexes of faces that are adjacent\n    edges: (m,2) int\n        Only returned if return_edges is True\n        Indexes of vertices which make up the\n        edges shared by the adjacent faces\n\n    Examples\n    ----------\n    This is useful for lots of things such as finding\n    face- connected components:\n    >>> graph = nx.Graph()\n    >>> graph.add_edges_from(mesh.face_adjacency)\n    >>> groups = nx.connected_components(graph_connected)\n    \"\"\"\n\n    if mesh is None:\n        # first generate the list of edges for the current faces\n        # also return the index for which face the edge is from\n        edges, edges_face = faces_to_edges(faces,\n                                           return_index=True)\n        # make sure edge rows are sorted\n        edges.sort(axis=1)\n    else:\n        # if passed a mesh, used the cached values\n        edges = mesh.edges_sorted\n        edges_face = mesh.edges_face\n\n    # this will return the indices for duplicate edges\n    # every edge appears twice in a well constructed mesh\n    # so for every row in edge_idx:\n    # edges[edge_idx[*][0]] == edges[edge_idx[*][1]]\n    # in this call to group rows we discard edges which\n    # don't occur twice\n    edge_groups = grouping.group_rows(edges, require_count=2)\n\n    if len(edge_groups) == 0:\n        log.error('No adjacent faces detected! Did you merge vertices?')\n\n    # the pairs of all adjacent faces\n    # so for every row in face_idx, self.faces[face_idx[*][0]] and\n    # self.faces[face_idx[*][1]] will share an edge\n    face_adjacency = edges_face[edge_groups]\n\n    # sort pairs so we can search for indexes with ordered pairs\n    face_adjacency.sort(axis=1)\n\n    if return_edges:\n        face_adjacency_edges = edges[edge_groups[:, 0]]\n        return face_adjacency, face_adjacency_edges\n    return face_adjacency"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef face_adjacency_unshared(mesh):\n\n    # the non- shared vertex index is the same shape as face_adjacnecy\n    # just holding vertex indices rather than face indices\n    vid_unshared = np.zeros_like(mesh.face_adjacency,\n                                 dtype=np.int64)\n    # loop through both columns of face adjacency\n    for i, adjacency in enumerate(mesh.face_adjacency.T):\n        # faces from the current column of face adjacency\n        faces = mesh.faces[adjacency]\n        shared = np.logical_or(\n            faces == mesh.face_adjacency_edges[:, 0].reshape((-1, 1)),\n            faces == mesh.face_adjacency_edges[:, 1].reshape((-1, 1)))\n        vid_unshared[:, i] = faces[np.logical_not(shared)]\n    return vid_unshared", "response": "Returns the vertex index of the two vertices not in the sharedvectory edge between two adjacent faces\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes an approximate radius between adjacent faces.", "response": "def face_adjacency_radius(mesh):\n    \"\"\"\n    Compute an approximate radius between adjacent faces.\n\n    Parameters\n    --------------\n    mesh : trimesh.Trimesh\n\n    Returns\n    -------------\n    radii : (len(self.face_adjacency),) float\n        Approximate radius between faces\n        Parallel faces will have a value of np.inf\n    span :  (len(self.face_adjacency),) float\n        Perpendicular projection distance of two\n        unshared vertices onto the shared edge\n    \"\"\"\n\n    # solve for the radius of the adjacent faces\n    #         distance\n    # R = ------------------\n    #     2 * sin(theta / 2)\n    nonzero = mesh.face_adjacency_angles > np.radians(.01)\n    denominator = np.abs(\n        2.0 * np.sin(mesh.face_adjacency_angles[nonzero] / 1.0))\n\n    # consider the distance between the non- shared vertices of the\n    # face adjacency pair as the key distance\n    point_pairs = mesh.vertices[mesh.face_adjacency_unshared]\n    vectors = np.diff(point_pairs,\n                      axis=1).reshape((-1, 3))\n\n    # the vertex indices of the shared edge for the adjacency pairx\n    edges = mesh.face_adjacency_edges\n    # unit vector along shared the edge\n    edges_vec = util.unitize(np.diff(mesh.vertices[edges],\n                                     axis=1).reshape((-1, 3)))\n\n    # the vector of the perpendicular projection to the shared edge\n    perp = np.subtract(\n        vectors, (util.diagonal_dot(\n            vectors, edges_vec).reshape(\n            (-1, 1)) * edges_vec))\n    # the length of the perpendicular projection\n    span = np.linalg.norm(perp, axis=1)\n\n    # complete the values for non- infinite radii\n    radii = np.ones(len(mesh.face_adjacency)) * np.inf\n    radii[nonzero] = span[nonzero] / denominator\n\n    return radii, span"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vertex_adjacency_graph(mesh):\n    g = nx.Graph()\n    g.add_edges_from(mesh.edges_unique)\n    return g", "response": "Returns a networkx graph representing the vertices and their connections between the given object and the graph containing the neighbors of the vertices and their connections."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving two sets of faces find the edges which are in both sets.", "response": "def shared_edges(faces_a, faces_b):\n    \"\"\"\n    Given two sets of faces, find the edges which are in both sets.\n\n    Parameters\n    ---------\n    faces_a: (n,3) int, set of faces\n    faces_b: (m,3) int, set of faces\n\n    Returns\n    ---------\n    shared: (p, 2) int, set of edges\n    \"\"\"\n    e_a = np.sort(faces_to_edges(faces_a), axis=1)\n    e_b = np.sort(faces_to_edges(faces_b), axis=1)\n    shared = grouping.boolean_rows(e_a, e_b, operation=np.intersect1d)\n    return shared"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives graph G and list of nodes return the list of edges that are connected to nodes", "response": "def connected_edges(G, nodes):\n    \"\"\"\n    Given graph G and list of nodes, return the list of edges that\n    are connected to nodes\n\n    \"\"\"\n    nodes_in_G = collections.deque()\n    for node in nodes:\n        if not G.has_node(node):\n            continue\n        nodes_in_G.extend(nx.node_connected_component(G, node))\n    edges = G.subgraph(nodes_in_G).edges()\n    return edges"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef facets(mesh, engine=None):\n    # what is the radius of a circle that passes through the perpendicular\n    # projection of the vector between the two non- shared vertices\n    # onto the shared edge, with the face normal from the two adjacent faces\n    radii = mesh.face_adjacency_radius\n    # what is the span perpendicular to the shared edge\n    span = mesh.face_adjacency_span\n    # a very arbitrary formula for declaring two adjacent faces\n    # parallel in a way that is hopefully (and anecdotally) robust\n    # to numeric error\n    # a common failure mode is two faces that are very narrow with a slight\n    # angle between them, so here we divide by the perpendicular span\n    # to penalize very narrow faces, and then square it just for fun\n    parallel = np.ones(len(radii), dtype=np.bool)\n    # if span is zero we know faces are small/parallel\n    nonzero = np.abs(span) > tol.zero\n    # faces with a radii/span ratio larger than a threshold pass\n    parallel[nonzero] = (radii[nonzero] /\n                         span[nonzero]) ** 2 > tol.facet_threshold\n\n    # run connected components on the parallel faces to group them\n    components = connected_components(mesh.face_adjacency[parallel],\n                                      nodes=np.arange(len(mesh.faces)),\n                                      min_len=2,\n                                      engine=engine)\n    return components", "response": "Find the list of parallel adjacent faces."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting a Trimesh object into multiple Trimesh objects.", "response": "def split(mesh,\n          only_watertight=True,\n          adjacency=None,\n          engine=None):\n    \"\"\"\n    Split a mesh into multiple meshes from face connectivity.\n\n    If only_watertight is true, it will only return watertight meshes\n    and will attempt single triangle/quad repairs.\n\n    Parameters\n    ----------\n    mesh: Trimesh\n    only_watertight: if True, only return watertight components\n    adjacency: (n,2) list of face adjacency to override using the plain\n               adjacency calculated automatically.\n    engine: str, which engine to use. ('networkx', 'scipy', or 'graphtool')\n\n    Returns\n    ----------\n    meshes: list of Trimesh objects\n    \"\"\"\n\n    if adjacency is None:\n        adjacency = mesh.face_adjacency\n\n    # if only watertight the shortest thing we can split has 3 triangles\n    if only_watertight:\n        min_len = 3\n    else:\n        min_len = 1\n\n    components = connected_components(edges=adjacency,\n                                      nodes=np.arange(len(mesh.faces)),\n                                      min_len=min_len,\n                                      engine=engine)\n    meshes = mesh.submesh(components,\n                          only_watertight=only_watertight)\n    return meshes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connected_components(edges,\n                         min_len=1,\n                         nodes=None,\n                         engine=None):\n    \"\"\"\n    Find groups of connected nodes from an edge list.\n\n    Parameters\n    -----------\n    edges:      (n,2) int, edges between nodes\n    nodes:      (m, ) int, list of nodes that exist\n    min_len:    int, minimum length of a component group to return\n    engine:     str, which graph engine to use.\n                ('networkx', 'scipy', or 'graphtool')\n                If None, will automatically choose fastest available.\n\n    Returns\n    -----------\n    components: (n,) sequence of lists, nodes which are connected\n    \"\"\"\n    def components_networkx():\n        \"\"\"\n        Find connected components using networkx\n        \"\"\"\n        graph = nx.from_edgelist(edges)\n        # make sure every face has a node, so single triangles\n        # aren't discarded (as they aren't adjacent to anything)\n        if min_len <= 1:\n            graph.add_nodes_from(nodes)\n        iterable = nx.connected_components(graph)\n        # newer versions of networkx return sets rather than lists\n        components = np.array([np.array(list(i), dtype=np.int64)\n                               for i in iterable if len(i) >= min_len])\n        return components\n\n    def components_graphtool():\n        \"\"\"\n        Find connected components using graphtool\n        \"\"\"\n        g = GTGraph()\n        # make sure all the nodes are in the graph\n        g.add_vertex(node_count)\n        # add the edge list\n        g.add_edge_list(edges)\n\n        labels = np.array(label_components(g, directed=False)[0].a,\n                          dtype=np.int64)[:node_count]\n\n        # we have to remove results that contain nodes outside\n        # of the specified node set and reindex\n        contained = np.zeros(node_count, dtype=np.bool)\n        contained[nodes] = True\n        index = np.arange(node_count, dtype=np.int64)[contained]\n\n        components = grouping.group(labels[contained], min_len=min_len)\n        components = np.array([index[c] for c in components])\n\n        return components\n\n    def components_csgraph():\n        \"\"\"\n        Find connected components using scipy.sparse.csgraph\n        \"\"\"\n        # label each node\n        labels = connected_component_labels(edges,\n                                            node_count=node_count)\n\n        # we have to remove results that contain nodes outside\n        # of the specified node set and reindex\n        contained = np.zeros(node_count, dtype=np.bool)\n        contained[nodes] = True\n        index = np.arange(node_count, dtype=np.int64)[contained]\n\n        components = grouping.group(labels[contained], min_len=min_len)\n        components = np.array([index[c] for c in components])\n\n        return components\n\n    # check input edges\n    edges = np.asanyarray(edges, dtype=np.int64)\n    # if no nodes were specified just use unique\n    if nodes is None:\n        nodes = np.unique(edges)\n\n    # exit early if we have no nodes\n    if len(nodes) == 0:\n        return np.array([])\n    elif len(edges) == 0:\n        if min_len <= 1:\n            return np.reshape(nodes, (-1, 1))\n        else:\n            return np.array([])\n\n    if not util.is_shape(edges, (-1, 2)):\n        raise ValueError('edges must be (n,2)!')\n\n    # find the maximum index referenced in either nodes or edges\n    counts = [0]\n    if len(edges) > 0:\n        counts.append(edges.max())\n    if len(nodes) > 0:\n        counts.append(nodes.max())\n    node_count = np.max(counts) + 1\n\n    # remove edges that don't have both nodes in the node set\n    mask = np.zeros(node_count, dtype=np.bool)\n    mask[nodes] = True\n    edges_ok = mask[edges].all(axis=1)\n    edges = edges[edges_ok]\n\n    # graphtool is usually faster then scipy by ~10%, however on very\n    # large or very small graphs graphtool outperforms scipy substantially\n    # networkx is pure python and is usually 5-10x slower\n    engines = collections.OrderedDict((('graphtool', components_graphtool),\n                                       ('scipy', components_csgraph),\n                                       ('networkx', components_networkx)))\n\n    # if a graph engine has explicitly been requested use it\n    if engine in engines:\n        return engines[engine]()\n\n    # otherwise, go through our ordered list of graph engines\n    # until we get to one that has actually been installed\n    for function in engines.values():\n        try:\n            return function()\n        # will be raised if the library didn't import correctly above\n        except NameError:\n            continue\n    raise ImportError('No connected component engines available!')", "response": "Returns a sequence of lists of connected components."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of component labels for each node in the graph.", "response": "def connected_component_labels(edges, node_count=None):\n    \"\"\"\n    Label graph nodes from an edge list, using scipy.sparse.csgraph\n\n    Parameters\n    ----------\n    edges : (n, 2) int\n       Edges of a graph\n    node_count : int, or None\n        The largest node in the graph.\n\n    Returns\n    ---------\n    labels : (node_count,) int\n        Component labels for each node\n    \"\"\"\n    matrix = edges_to_coo(edges, node_count)\n    body_count, labels = csgraph.connected_components(\n        matrix, directed=False)\n\n    assert len(labels) == node_count\n\n    return labels"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split_traversal(traversal,\n                    edges,\n                    edges_hash=None):\n    \"\"\"\n    Given a traversal as a list of nodes, split the traversal\n    if a sequential index pair is not in the given edges.\n\n    Parameters\n    --------------\n    edges : (n, 2) int\n       Graph edge indexes\n    traversal : (m,) int\n       Traversal through edges\n    edge_hash : (n,)\n       Edges sorted on axis=1 and\n       passed to grouping.hashable_rows\n\n    Returns\n    ---------------\n    split : sequence of (p,) int\n    \"\"\"\n    traversal = np.asanyarray(traversal,\n                              dtype=np.int64)\n\n    # hash edge rows for contains checks\n    if edges_hash is None:\n        edges_hash = grouping.hashable_rows(\n            np.sort(edges, axis=1))\n\n    # turn the (n,) traversal into (n-1,2) edges\n    trav_edge = np.column_stack((traversal[:-1],\n                                 traversal[1:]))\n    # hash each edge so we can compare to edge set\n    trav_hash = grouping.hashable_rows(\n        np.sort(trav_edge, axis=1))\n    # check if each edge is contained in edge set\n    contained = np.in1d(trav_hash, edges_hash)\n\n    # exit early if every edge of traversal exists\n    if contained.all():\n        # just reshape one traversal\n        split = [traversal]\n    else:\n        # find contiguous groups of contained edges\n        blocks = grouping.blocks(contained,\n                                 min_len=1,\n                                 only_nonzero=True)\n\n        # turn edges back in to sequence of traversals\n        split = [np.append(trav_edge[b][:, 0],\n                           trav_edge[b[-1]][1])\n                 for b in blocks]\n\n    # close traversals if necessary\n    for i, t in enumerate(split):\n        # make sure elements of sequence are numpy arrays\n        split[i] = np.asanyarray(split[i], dtype=np.int64)\n        # don't close if its a single edge\n        if len(t) <= 2:\n            continue\n        # make sure it's not already closed\n        edge = np.sort([t[0], t[-1]])\n        if edge.ptp() == 0:\n            continue\n        close = grouping.hashable_rows(edge.reshape((1, 2)))[0]\n        # if we need the edge add it\n        if close in edges_hash:\n            split[i] = np.append(t, t[0]).astype(np.int64)\n    result = np.array(split)\n\n    return result", "response": "Given a traversal as a list of nodes split the traversal into a sequence of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfilling a list of nodes in a node traversal with a list of edges.", "response": "def fill_traversals(traversals, edges, edges_hash=None):\n    \"\"\"\n    Convert a traversal of a list of edges into a sequence of\n    traversals where every pair of consecutive node indexes\n    is an edge in a passed edge list\n\n    Parameters\n    -------------\n    traversals : sequence of (m,) int\n       Node indexes of traversals of a graph\n    edges : (n, 2) int\n       Pairs of connected node indexes\n    edges_hash : None, or (n,) int\n       Edges sorted along axis 1 then hashed\n       using grouping.hashable_rows\n\n    Returns\n    --------------\n    splits : sequence of (p,) int\n       Node indexes of connected traversals\n    \"\"\"\n    # make sure edges are correct type\n    edges = np.asanyarray(edges, dtype=np.int64)\n    # make sure edges are sorted\n    edges.sort(axis=1)\n\n    # if there are no traversals just return edges\n    if len(traversals) == 0:\n        return edges.copy()\n\n    # hash edges for contains checks\n    if edges_hash is None:\n        edges_hash = grouping.hashable_rows(edges)\n\n    splits = []\n    for nodes in traversals:\n        # split traversals to remove edges\n        # that don't actually exist\n        splits.extend(split_traversal(\n            traversal=nodes,\n            edges=edges,\n            edges_hash=edges_hash))\n    # turn the split traversals back into (n,2) edges\n    included = util.vstack_empty([np.column_stack((i[:-1], i[1:]))\n                                  for i in splits])\n    if len(included) > 0:\n        # sort included edges in place\n        included.sort(axis=1)\n        # make sure any edges not included in split traversals\n        # are just added as a length 2 traversal\n        splits.extend(grouping.boolean_rows(\n            edges,\n            included,\n            operation=np.setdiff1d))\n    else:\n        # no edges were included, so our filled traversal\n        # is just the original edges copied over\n        splits = edges.copy()\n\n    return splits"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives an edge list generate a sequence of ordered DFS or BFS traversals.", "response": "def traversals(edges, mode='bfs'):\n    \"\"\"\n    Given an edge list, generate a sequence of ordered\n    depth first search traversals, using scipy.csgraph routines.\n\n    Parameters\n    ------------\n    edges : (n,2) int, undirected edges of a graph\n    mode :  str, 'bfs', or 'dfs'\n\n    Returns\n    -----------\n    traversals: (m,) sequence of (p,) int,\n                ordered DFS or BFS traversals of the graph.\n    \"\"\"\n    edges = np.asanyarray(edges, dtype=np.int64)\n    if len(edges) == 0:\n        return []\n    elif not util.is_shape(edges, (-1, 2)):\n        raise ValueError('edges are not (n,2)!')\n\n    # pick the traversal method\n    mode = str(mode).lower().strip()\n    if mode == 'bfs':\n        func = csgraph.breadth_first_order\n    elif mode == 'dfs':\n        func = csgraph.depth_first_order\n    else:\n        raise ValueError('traversal mode must be either dfs or bfs')\n\n    # make sure edges are sorted so we can query\n    # an ordered pair later\n    edges.sort(axis=1)\n    # set of nodes to make sure we get every node\n    nodes = set(edges.reshape(-1))\n    # coo_matrix for csgraph routines\n    graph = edges_to_coo(edges)\n\n    # we're going to make a sequence of traversals\n    traversals = []\n\n    while len(nodes) > 0:\n        # starting at any node\n        start = nodes.pop()\n        # get an (n,) ordered traversal\n        ordered = func(graph,\n                       i_start=start,\n                       return_predecessors=False,\n                       directed=False).astype(np.int64)\n\n        traversals.append(ordered)\n        # remove the nodes we've consumed\n        nodes.difference_update(ordered)\n\n    return traversals"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving an edge list return a sparse COO matrix representation of the edges in matrix form.", "response": "def edges_to_coo(edges, count=None, data=None):\n    \"\"\"\n    Given an edge list, return a boolean scipy.sparse.coo_matrix\n    representing the edges in matrix form.\n\n    Parameters\n    ------------\n    edges : (n,2) int\n      Edges of a graph\n    count : int\n      The total number of nodes in the graph\n      if None: count = edges.max() + 1\n    data : (n,) any\n      Assign data to each edge, if None will\n      be bool True for each specified edge\n\n    Returns\n    ------------\n    matrix: (count, count) scipy.sparse.coo_matrix\n      Sparse COO\n    \"\"\"\n    edges = np.asanyarray(edges, dtype=np.int64)\n    if not (len(edges) == 0 or\n            util.is_shape(edges, (-1, 2))):\n        raise ValueError('edges must be (n,2)!')\n\n    # if count isn't specified just set it to largest\n    # value referenced in edges\n    if count is None:\n        count = edges.max() + 1\n    count = int(count)\n\n    # if no data is specified set every specified edge\n    # to True\n    if data is None:\n        data = np.ones(len(edges), dtype=np.bool)\n\n    matrix = coo_matrix((data, edges.T),\n                        dtype=data.dtype,\n                        shape=(count, count))\n    return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef smoothed(mesh, angle):\n    # if the mesh has no adjacent faces return a copy\n    if len(mesh.face_adjacency) == 0:\n        return mesh.copy()\n\n    # face pairs below angle threshold\n    angle_ok = mesh.face_adjacency_angles <= angle\n    # subset of face adjacency\n    adjacency = mesh.face_adjacency[angle_ok]\n    # list of connected groups of faces\n    components = connected_components(adjacency,\n                                      min_len=1,\n                                      nodes=np.arange(len(mesh.faces)))\n    # get a submesh as a single appended Trimesh\n    smooth = mesh.submesh(components,\n                          only_watertight=False,\n                          append=True)\n    return smooth", "response": "Returns a non - watertight version of the mesh which will be smoothed by disconnecting faces at sharp angles."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_watertight(edges, edges_sorted=None):\n    # passing edges_sorted is a speedup only\n    if edges_sorted is None:\n        edges_sorted = np.sort(edges, axis=1)\n\n    # group sorted edges\n    groups = grouping.group_rows(edges_sorted, require_count=2)\n    watertight = bool((len(groups) * 2) == len(edges))\n\n    # are opposing edges reversed\n    opposing = edges[groups].reshape((-1, 4))[:, 1:3].T\n    # wrap the weird numpy bool\n    winding = bool(np.equal(*opposing).all())\n\n    return watertight, winding", "response": "Returns True if every edge is watertight."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a networkx graph into an SVG string using graphviz dot.", "response": "def graph_to_svg(graph):\n    \"\"\"\n    Turn a networkx graph into an SVG string, using graphviz dot.\n\n    Parameters\n    ----------\n    graph: networkx graph\n\n    Returns\n    ---------\n    svg: string, pictoral layout in SVG format\n    \"\"\"\n\n    import tempfile\n    import subprocess\n    with tempfile.NamedTemporaryFile() as dot_file:\n        nx.drawing.nx_agraph.write_dot(graph, dot_file.name)\n        svg = subprocess.check_output(['dot', dot_file.name, '-Tsvg'])\n    return svg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a MultiDiGraph traversal collect attributes along it.", "response": "def multigraph_collect(G, traversal, attrib=None):\n    \"\"\"\n    Given a MultiDiGraph traversal, collect attributes along it.\n\n    Parameters\n    -------------\n    G:          networkx.MultiDiGraph\n    traversal:  (n) list of (node, instance) tuples\n    attrib:     dict key, name to collect. If None, will return all\n\n    Returns\n    -------------\n    collected: (len(traversal) - 1) list of attributes\n    \"\"\"\n\n    collected = []\n    for u, v in util.pairwise(traversal):\n        attribs = G[u[0]][v[0]][v[1]]\n        if attrib is None:\n            collected.append(attribs)\n        else:\n            collected.append(attribs[attrib])\n    return collected"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn a set of keyword arguments into a transformation matrix.", "response": "def kwargs_to_matrix(**kwargs):\n    \"\"\"\n    Turn a set of keyword arguments into a transformation matrix.\n    \"\"\"\n    matrix = np.eye(4)\n    if 'matrix' in kwargs:\n        # a matrix takes precedence over other options\n        matrix = kwargs['matrix']\n    elif 'quaternion' in kwargs:\n        matrix = transformations.quaternion_matrix(kwargs['quaternion'])\n    elif ('axis' in kwargs) and ('angle' in kwargs):\n        matrix = transformations.rotation_matrix(kwargs['angle'],\n                                                 kwargs['axis'])\n    else:\n        raise ValueError('Couldn\\'t update transform!')\n\n    if 'translation' in kwargs:\n        # translation can be used in conjunction with any of the methods of\n        # specifying transforms. In the case a matrix and translation are passed,\n        # we add the translations together rather than picking one.\n        matrix[0:3, 3] += kwargs['translation']\n    return matrix"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate a transform in the tree.", "response": "def update(self, frame_to, frame_from=None, **kwargs):\n        \"\"\"\n        Update a transform in the tree.\n\n        Parameters\n        ---------\n        frame_from : hashable object\n          Usually a string (eg 'world').\n          If left as None it will be set to self.base_frame\n        frame_to :  hashable object\n          Usually a string (eg 'mesh_0')\n        matrix : (4,4) float\n          Homogenous transformation matrix\n        quaternion :  (4,) float\n          Quaternion ordered [w, x, y, z]\n        axis : (3,) float\n          Axis of rotation\n        angle :  float\n          Angle of rotation, in radians\n        translation : (3,) float\n          Distance to translate\n        geometry : hashable\n          Geometry object name, e.g. 'mesh_0'\n        \"\"\"\n        # save a random number for this update\n        self._updated = np.random.random()\n\n        # if no frame specified, use base frame\n        if frame_from is None:\n            frame_from = self.base_frame\n        # convert various kwargs to a single matrix\n        matrix = kwargs_to_matrix(**kwargs)\n\n        # create the edge attributes\n        attr = {'matrix': matrix, 'time': time.time()}\n        # pass through geometry to edge attribute\n        if 'geometry' in kwargs:\n            attr['geometry'] = kwargs['geometry']\n\n        # add the edges\n        changed = self.transforms.add_edge(frame_from,\n                                           frame_to,\n                                           **attr)\n        # set the node attribute with the geometry information\n        if 'geometry' in kwargs:\n            nx.set_node_attributes(\n                self.transforms,\n                name='geometry',\n                values={frame_to: kwargs['geometry']})\n        # if the edge update changed our structure\n        # dump our cache of shortest paths\n        if changed:\n            self._paths = {}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef md5(self):\n        result = str(self._updated) + str(self.base_frame)\n        return result", "response": "Returns the md5 of the current set of items"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a copy of the current TransformForest", "response": "def copy(self):\n        \"\"\"\n        Return a copy of the current TransformForest\n\n        Returns\n        ------------\n        copied: TransformForest\n        \"\"\"\n        copied = TransformForest()\n        copied.base_frame = copy.deepcopy(self.base_frame)\n        copied.transforms = copy.deepcopy(self.transforms)\n\n        return copied"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport the current transform graph as a flattened dictionary.", "response": "def to_flattened(self, base_frame=None):\n        \"\"\"\n        Export the current transform graph as a flattened\n        \"\"\"\n        if base_frame is None:\n            base_frame = self.base_frame\n\n        flat = {}\n        for node in self.nodes:\n            if node == base_frame:\n                continue\n            transform, geometry = self.get(\n                frame_to=node, frame_from=base_frame)\n            flat[node] = {\n                'transform': transform.tolist(),\n                'geometry': geometry\n            }\n        return flat"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_gltf(self, scene):\n        # geometry is an OrderedDict\n        # {geometry key : index}\n        mesh_index = {name: i for i, name\n                      in enumerate(scene.geometry.keys())}\n        # save the output\n        gltf = collections.deque([])\n        for node in self.nodes:\n            # don't include edge for base frame\n            if node == self.base_frame:\n                continue\n            # get the transform and geometry from the graph\n            transform, geometry = self.get(\n                frame_to=node, frame_from=self.base_frame)\n\n            gltf.append({\n                'matrix': transform.T.reshape(-1).tolist(),\n                'name': node})\n            # assign geometry if it exists\n            if geometry is not None:\n                gltf[-1]['mesh'] = mesh_index[geometry]\n\n            if node == scene.camera.name:\n                gltf[-1]['camera'] = 0\n\n        # we have flattened tree, so all nodes will be child of world\n        gltf.appendleft({\n            'name': self.base_frame,\n            'children': list(range(1, 1 + len(gltf)))\n        })\n        result = {'nodes': list(gltf)}\n\n        return result", "response": "Export a transforms as the nodes section of a GLTF dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_edgelist(self):\n        # save cleaned edges\n        export = []\n        # loop through (node, node, edge attributes)\n        for edge in nx.to_edgelist(self.transforms):\n            a, b, c = edge\n            # geometry is a node property but save it to the\n            # edge so we don't need two dictionaries\n            if 'geometry' in self.transforms.node[b]:\n                c['geometry'] = self.transforms.node[b]['geometry']\n            # save the matrix as a float list\n            c['matrix'] = np.asanyarray(c['matrix'], dtype=np.float64).tolist()\n            export.append((a, b, c))\n        return export", "response": "Export the current transforms as a list of edge tuples with the format node_a node_b metadata"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_edgelist(self, edges, strict=True):\n        # loop through each edge\n        for edge in edges:\n            # edge contains attributes\n            if len(edge) == 3:\n                self.update(edge[1], edge[0], **edge[2])\n            # edge just contains nodes\n            elif len(edge) == 2:\n                self.update(edge[1], edge[0])\n            # edge is broken\n            elif strict:\n                raise ValueError('edge incorrect shape: {}'.format(str(edge)))", "response": "Load transform data from an edge list into the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef nodes(self):\n        nodes = np.array(list(self.transforms.nodes()))\n        return nodes", "response": "A list of every node in the graph"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef nodes_geometry(self):\n\n        nodes = np.array([\n            n for n in self.transforms.nodes()\n            if 'geometry' in self.transforms.node[n]\n        ])\n\n        return nodes", "response": "Returns the nodes in the scene graph with geometry attached."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the transform from one frame to another assuming they are connected.", "response": "def get(self, frame_to, frame_from=None):\n        \"\"\"\n        Get the transform from one frame to another, assuming they are connected\n        in the transform tree.\n\n        If the frames are not connected a NetworkXNoPath error will be raised.\n\n        Parameters\n        ---------\n        frame_from: hashable object, usually a string (eg 'world').\n                    If left as None it will be set to self.base_frame\n        frame_to:   hashable object, usually a string (eg 'mesh_0')\n\n        Returns\n        ---------\n        transform:  (4,4) homogenous transformation matrix\n        \"\"\"\n\n        if frame_from is None:\n            frame_from = self.base_frame\n\n        cache_key = str(frame_from) + ':' + str(frame_to)\n        cached = self._cache[cache_key]\n        if cached is not None:\n            return cached\n\n        transform = np.eye(4)\n        path = self._get_path(frame_from, frame_to)\n\n        for i in range(len(path) - 1):\n            data, direction = self.transforms.get_edge_data_direction(\n                path[i], path[i + 1])\n            matrix = data['matrix']\n            if direction < 0:\n                matrix = np.linalg.inv(matrix)\n            transform = np.dot(transform, matrix)\n\n        geometry = None\n        if 'geometry' in self.transforms.node[frame_to]:\n            geometry = self.transforms.node[frame_to]['geometry']\n\n        self._cache[cache_key] = (transform, geometry)\n\n        return transform, geometry"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show(self):\n        import matplotlib.pyplot as plt\n        nx.draw(self.transforms, with_labels=True)\n        plt.show()", "response": "Show the graph layout of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_path(self, frame_from, frame_to):\n        key = (frame_from, frame_to)\n        if not (key in self._paths):\n            path = self.transforms.shortest_path_undirected(\n                frame_from, frame_to)\n            self._paths[key] = path\n        return self._paths[key]", "response": "Find a path between two frames."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_ccw(points):\n    points = np.asanyarray(points, dtype=np.float64)\n\n    if (len(points.shape) != 2 or\n            points.shape[1] != 2):\n        raise ValueError('CCW is only defined for 2D')\n    xd = np.diff(points[:, 0])\n    yd = np.column_stack((\n        points[:, 1],\n        points[:, 1])).reshape(-1)[1:-1].reshape((-1, 2)).sum(axis=1)\n    area = np.sum(xd * yd) * .5\n    ccw = area < 0\n\n    return ccw", "response": "Checks if connected planar points are counterclockwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconcatenating multiple paths into a single path.", "response": "def concatenate(paths):\n    \"\"\"\n    Concatenate multiple paths into a single path.\n\n    Parameters\n    -------------\n    paths: list of Path, Path2D, or Path3D objects\n\n    Returns\n    -------------\n    concat: Path, Path2D, or Path3D object\n    \"\"\"\n    # if only one path object just return copy\n    if len(paths) == 1:\n        return paths[0].copy()\n\n    # length of vertex arrays\n    vert_len = np.array([len(i.vertices) for i in paths])\n    # how much to offset each paths vertex indices by\n    offsets = np.append(0.0, np.cumsum(vert_len))[:-1].astype(np.int64)\n\n    # resulting entities\n    entities = []\n    # resulting vertices\n    vertices = []\n    # resulting metadata\n    metadata = {}\n    for path, offset in zip(paths, offsets):\n        # update metadata\n        metadata.update(path.metadata)\n        # copy vertices, we will stack later\n        vertices.append(path.vertices.copy())\n        # copy entity then reindex points\n        for entity in path.entities:\n            entities.append(entity.copy())\n            entities[-1].points += offset\n\n    # generate the single new concatenated path\n    # use input types so we don't have circular imports\n    concat = type(path)(metadata=metadata,\n                        entities=entities,\n                        vertices=np.vstack(vertices))\n    return concat"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef filter_humphrey(mesh,\n                    alpha=0.1,\n                    beta=0.5,\n                    iterations=10,\n                    laplacian_operator=None):\n    \"\"\"\n    Smooth a mesh in-place using laplacian smoothing\n    and Humphrey filtering.\n\n    Articles\n    \"Improved Laplacian Smoothing of Noisy Surface Meshes\"\n    J. Vollmer, R. Mencl, and H. Muller\n\n    Parameters\n    ------------\n    mesh : trimesh.Trimesh\n      Mesh to be smoothed in place\n    alpha : float\n      Controls shrinkage, range is 0.0 - 1.0\n      If 0.0, not considered\n      If 1.0, no smoothing\n    beta : float\n      Controls how aggressive smoothing is\n      If 0.0, no smoothing\n      If 1.0, full aggressiveness\n    iterations : int\n      Number of passes to run filter\n    laplacian_operator : None or scipy.sparse.coo.coo_matrix\n      Sparse matrix laplacian operator\n      Will be autogenerated if None\n    \"\"\"\n    # if the laplacian operator was not passed create it here\n    if laplacian_operator is None:\n        laplacian_operator = laplacian_calculation(mesh)\n\n    # get mesh vertices as vanilla numpy array\n    vertices = mesh.vertices.copy().view(np.ndarray)\n    # save original unmodified vertices\n    original = vertices.copy()\n\n    # run through iterations of filter\n    for _index in range(iterations):\n        vert_q = vertices.copy()\n        vertices = laplacian_operator.dot(vertices)\n        vert_b = vertices - (alpha * original + (1.0 - alpha) * vert_q)\n        vertices -= (beta * vert_b + (1.0 - beta) *\n                     laplacian_operator.dot(vert_b))\n\n    # assign modified vertices back to mesh\n    mesh.vertices = vertices\n    return mesh", "response": "Filter a mesh using Humphrey smoothing."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef filter_taubin(mesh,\n                  lamb=0.5,\n                  nu=0.5,\n                  iterations=10,\n                  laplacian_operator=None):\n    \"\"\"\n    Smooth a mesh in-place using laplacian smoothing\n    and taubin filtering.\n\n    Articles\n    \"Improved Laplacian Smoothing of Noisy Surface Meshes\"\n    J. Vollmer, R. Mencl, and H. Muller\n\n    Parameters\n    ------------\n    mesh : trimesh.Trimesh\n      Mesh to be smoothed in place.\n    lamb : float\n      Controls shrinkage, range is 0.0 - 1.0\n    nu : float\n      Controls dilation, range is 0.0 - 1.0\n      Nu shall be between 0.0 < 1.0/lambda - 1.0/nu < 0.1\n    iterations : int\n      Number of passes to run the filter\n    laplacian_operator : None or scipy.sparse.coo.coo_matrix\n      Sparse matrix laplacian operator\n      Will be autogenerated if None\n    \"\"\"\n    # if the laplacian operator was not passed create it here\n    if laplacian_operator is None:\n        laplacian_operator = laplacian_calculation(mesh)\n\n    # get mesh vertices as vanilla numpy array\n    vertices = mesh.vertices.copy().view(np.ndarray)\n\n    # run through multiple passes of the filter\n    for index in range(iterations):\n        # do a sparse dot product on the vertices\n        dot = laplacian_operator.dot(vertices) - vertices\n        # alternate shrinkage and dilation\n        if index % 2 == 0:\n            vertices += lamb * dot\n        else:\n            vertices -= nu * dot\n\n    # assign updated vertices back to mesh\n    mesh.vertices = vertices\n    return mesh", "response": "Filter a mesh in - place using laplacian smoothing of Noisy Surface Meshes and taubin filtering."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef laplacian_calculation(mesh, equal_weight=True):\n    # get the vertex neighbors from the cache\n    neighbors = mesh.vertex_neighbors\n    # avoid hitting crc checks in loops\n    vertices = mesh.vertices.view(np.ndarray)\n\n    # stack neighbors to 1D arrays\n    col = np.concatenate(neighbors)\n    row = np.concatenate([[i] * len(n)\n                          for i, n in enumerate(neighbors)])\n\n    if equal_weight:\n        # equal weights for each neighbor\n        data = np.concatenate([[1.0 / len(n)] * len(n)\n                               for n in neighbors])\n    else:\n        # umbrella weights, distance-weighted\n        # use dot product of ones to replace array.sum(axis=1)\n        ones = np.ones(3)\n        # the distance from verticesex to neighbors\n        norms = [1.0 / np.sqrt(np.dot((vertices[i] - vertices[n]) ** 2, ones))\n                 for i, n in enumerate(neighbors)]\n        # normalize group and stack into single array\n        data = np.concatenate([i / i.sum() for i in norms])\n\n    # create the sparse matrix\n    matrix = coo_matrix((data, (row, col)),\n                        shape=[len(vertices)] * 2)\n\n    return matrix", "response": "Calculates a sparse matrix for laplacian operations."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind the intersection between two lines.", "response": "def line_line(origins,\n              directions,\n              plane_normal=None):\n    \"\"\"\n    Find the intersection between two lines.\n    Uses terminology from:\n    http://geomalgorithms.com/a05-_intersect-1.html\n\n    line 1:    P(s) = p_0 + sU\n    line 2:    Q(t) = q_0 + tV\n\n    Parameters\n    ---------\n    origins:      (2, d) float, points on lines (d in [2,3])\n    directions:   (2, d) float, direction vectors\n    plane_normal: (3, ) float, if not passed computed from cross\n\n    Returns\n    ---------\n    intersects:   boolean, whether the lines intersect.\n                  In 2D, false if the lines are parallel\n                  In 3D, false if lines are not coplanar\n    intersection: if intersects: (d) length point of intersection\n                  else:          None\n    \"\"\"\n    # check so we can accept 2D or 3D points\n    origins, is_2D = stack_3D(origins, return_2D=True)\n    directions, is_2D = stack_3D(directions, return_2D=True)\n\n    # unitize direction vectors\n    directions /= np.linalg.norm(directions,\n                                 axis=1).reshape((-1, 1))\n\n    # exit if values are parallel\n    if np.sum(np.abs(np.diff(directions,\n                             axis=0))) < tol.zero:\n        return False, None\n\n    # using notation from docstring\n    q_0, p_0 = origins\n    v, u = directions\n    w = p_0 - q_0\n\n    # recompute plane normal if not passed\n    if plane_normal is None:\n        # the normal of the plane given by the two direction vectors\n        plane_normal = np.cross(u, v)\n        plane_normal /= np.linalg.norm(plane_normal)\n\n    # vectors perpendicular to the two lines\n    v_perp = np.cross(v, plane_normal)\n    v_perp /= np.linalg.norm(v_perp)\n\n    # if the vector from origin to origin is on the plane given by\n    # the direction vector, the dot product with the plane normal\n    # should be within floating point error of zero\n    coplanar = abs(np.dot(plane_normal, w)) < tol.zero\n    if not coplanar:\n        return False, None\n\n    # value of parameter s where intersection occurs\n    s_I = (np.dot(-v_perp, w) /\n           np.dot(v_perp, u))\n    # plug back into the equation of the line to find the point\n    intersection = p_0 + s_I * u\n\n    return True, intersection[:(3 - is_2D)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfitting a circle and reject the fit if fit is acceptable.", "response": "def fit_circle_check(points,\n                     scale,\n                     prior=None,\n                     final=False,\n                     verbose=False):\n    \"\"\"\n    Fit a circle, and reject the fit if:\n    * the radius is larger than tol.radius_min*scale or tol.radius_max*scale\n    * any segment spans more than tol.seg_angle\n    * any segment is longer than tol.seg_frac*scale\n    * the fit deviates by more than tol.radius_frac*radius\n    * the segments on the ends deviate from tangent by more than tol.tangent\n\n    Parameters\n    ---------\n    points:  (n, d) set of points which represent a path\n    prior:   (center, radius) tuple for best guess, or None if unknown\n    scale:   float, what is the overall scale of the set of points\n    verbose: boolean, if True output log.debug messages for the reasons\n             for fit rejection. Potentially generates hundreds of thousands of\n             messages so only suggested in manual debugging.\n\n    Returns\n    ---------\n    if fit is acceptable:\n        (center, radius) tuple\n    else:\n        None\n    \"\"\"\n    # an arc needs at least three points\n    if len(points) < 3:\n        return None\n\n    # do a least squares fit on the points\n    C, R, r_deviation = fit_nsphere(points, prior=prior)\n\n    # check to make sure radius is between min and max allowed\n    if not tol.radius_min < (R / scale) < tol.radius_max:\n        if verbose:\n            log.debug('circle fit error: R %f', R / scale)\n        return None\n\n    # check point radius error\n    r_error = r_deviation / R\n    if r_error > tol.radius_frac:\n        if verbose:\n            log.debug('circle fit error: fit %s', str(r_error))\n        return None\n\n    vectors = np.diff(points, axis=0)\n    segment = np.linalg.norm(vectors, axis=1)\n\n    # approximate angle in radians, segments are linear length\n    # not arc length but this is close and avoids a cosine\n    angle = segment / R\n    if (angle > tol.seg_angle).any():\n        if verbose:\n            log.debug('circle fit error: angle %s', str(angle))\n        return None\n\n    if final and (angle > tol.seg_angle_min).sum() < 3:\n        log.debug('final: angle %s', str(angle))\n        return None\n\n    # check segment length as a fraction of drawing scale\n    scaled = segment / scale\n\n    if (scaled > tol.seg_frac).any():\n        if verbose:\n            log.debug('circle fit error: segment %s', str(scaled))\n        return None\n\n    # check to make sure the line segments on the ends are actually\n    # tangent with the candidate circle fit\n    mid_pt = points[[0, -2]] + (vectors[[0, -1]] * .5)\n    radial = unitize(mid_pt - C)\n    ends = unitize(vectors[[0, -1]])\n    tangent = np.abs(np.arccos(diagonal_dot(radial, ends)))\n    tangent = np.abs(tangent - np.pi / 2).max()\n\n    if tangent > tol.tangent:\n        if verbose:\n            log.debug('circle fit error: tangent %f',\n                      np.degrees(tangent))\n        return None\n\n    result = {'center': C,\n              'radius': R}\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a set of points quickly determine if they represent a circle or not.", "response": "def is_circle(points, scale, verbose=False):\n    \"\"\"\n    Given a set of points, quickly determine if they represent\n    a circle or not.\n\n    Parameters\n    -------------\n    points: (n,2) float, points in space\n    scale:  float, scale of overall drawing\n    verbose: bool, print all fit messages or not\n\n    Returns\n    -------------\n    control: (3,2) float, points in space, OR\n              None, if not a circle\n    \"\"\"\n\n    # make sure input is a numpy array\n    points = np.asanyarray(points)\n    scale = float(scale)\n\n    # can only be a circle if the first and last point are the\n    # same (AKA is a closed path)\n    if np.linalg.norm(points[0] - points[-1]) > tol.merge:\n        return None\n\n    box = points.ptp(axis=0)\n    # the bounding box size of the points\n    # check aspect ratio as an early exit if the path is not a circle\n    aspect = np.divide(*box)\n    if np.abs(aspect - 1.0) > tol.aspect_frac:\n        return None\n\n    # fit a circle with tolerance checks\n    CR = fit_circle_check(points, scale=scale)\n    if CR is None:\n        return None\n\n    # return the circle as three control points\n    control = arc.to_threepoint(**CR)\n    return control"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_colinear(points, scale):\n    points = np.asanyarray(points, dtype=np.float64)\n    scale = float(scale)\n\n    if len(points.shape) != 2 or points.shape[1] != 2:\n        raise ValueError('only for 2D points!')\n\n    # if there's less than 3 points nothing to merge\n    if len(points) < 3:\n        return points.copy()\n\n    # the vector from one point to the next\n    direction = points[1:] - points[:-1]\n    # the length of the direction vector\n    direction_norm = np.linalg.norm(direction, axis=1)\n    # make sure points don't have zero length\n    direction_ok = direction_norm > tol.merge\n\n    # remove duplicate points\n    points = np.vstack((points[0], points[1:][direction_ok]))\n    direction = direction[direction_ok]\n    direction_norm = direction_norm[direction_ok]\n\n    # create a vector between every other point, then turn it perpendicular\n    # if we have points A B C D\n    # and direction vectors A-B, B-C, etc\n    # these will be perpendicular to the vectors A-C, B-D, etc\n    perp = (points[2:] - points[:-2]).T[::-1].T\n    perp_norm = np.linalg.norm(perp, axis=1)\n    perp_nonzero = perp_norm > tol.merge\n    perp[perp_nonzero] /= perp_norm[perp_nonzero].reshape((-1, 1))\n\n    # find the projection of each direction vector\n    # onto the perpendicular vector\n    projection = np.abs(diagonal_dot(perp,\n                                     direction[:-1]))\n\n    projection_ratio = np.max((projection / direction_norm[1:],\n                               projection / direction_norm[:-1]), axis=0)\n\n    mask = np.ones(len(points), dtype=np.bool)\n    # since we took diff, we need to offset by one\n    mask[1:-1][projection_ratio < 1e-4 * scale] = False\n\n    merged = points[mask]\n    return merged", "response": "Given a set of points representing a path in space merge them together."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a spline entity from a set of points.", "response": "def points_to_spline_entity(points, smooth=None, count=None):\n    \"\"\"\n    Create a spline entity from a curve in space\n\n    Parameters\n    -----------\n    points: (n, dimension) float, points in space\n    smooth: float, smoothing amount\n    count:  int, number of samples in result\n\n    Returns\n    ---------\n    entity: entities.BSpline object with points indexed at zero\n    control: (m, dimension) float, new vertices for entity\n    \"\"\"\n\n    from scipy.interpolate import splprep\n    if count is None:\n        count = len(points)\n    if smooth is None:\n        smooth = 0.002\n\n    points = np.asanyarray(points, dtype=np.float64)\n    closed = np.linalg.norm(points[0] - points[-1]) < tol.merge\n\n    knots, control, degree = splprep(points.T, s=smooth)[0]\n    control = np.transpose(control)\n    index = np.arange(len(control))\n\n    if closed:\n        control[0] = control[[0, -1]].mean(axis=0)\n        control = control[:-1]\n        index[-1] = index[0]\n\n    entity = entities.BSpline(points=index,\n                              knots=knots,\n                              closed=closed)\n\n    return entity, control"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef simplify_basic(drawing, process=False, **kwargs):\n\n    if any(i.__class__.__name__ != 'Line'\n           for i in drawing.entities):\n        log.debug('Path contains non- linear entities, skipping')\n        return drawing\n\n    # we are going to do a bookkeeping to avoid having\n    # to recompute literally everything when simplification is ran\n    cache = copy.deepcopy(drawing._cache)\n\n    # store new values\n    vertices_new = collections.deque()\n    entities_new = collections.deque()\n\n    # avoid thrashing cache in loop\n    scale = drawing.scale\n\n    # loop through (n, 2) closed paths\n    for discrete in drawing.discrete:\n        # check to see if the closed entity is a circle\n        circle = is_circle(discrete,\n                           scale=scale)\n        if circle is not None:\n            # the points are circular enough for our high standards\n            # so replace them with a closed Arc entity\n            entities_new.append(entities.Arc(points=np.arange(3) +\n                                             len(vertices_new),\n                                             closed=True))\n            vertices_new.extend(circle)\n        else:\n            # not a circle, so clean up colinear segments\n            # then save it as a single line entity\n            points = merge_colinear(discrete, scale=scale)\n            # references for new vertices\n            indexes = np.arange(len(points)) + len(vertices_new)\n            # discrete curves are always closed\n            indexes[-1] = indexes[0]\n            # append new vertices and entity\n            entities_new.append(entities.Line(points=indexes))\n            vertices_new.extend(points)\n\n    # create the new drawing object\n    simplified = type(drawing)(\n        entities=entities_new,\n        vertices=vertices_new,\n        metadata=copy.deepcopy(drawing.metadata),\n        process=process)\n    # we have changed every path to a single closed entity\n    # either a closed arc, or a closed line\n    # so all closed paths are now represented by a single entity\n    cache.cache.update({\n        'paths': np.arange(len(entities_new)).reshape((-1, 1)),\n        'path_valid': np.ones(len(entities_new), dtype=np.bool),\n        'dangling': np.array([])})\n\n    # force recompute of exact bounds\n    if 'bounds' in cache.cache:\n        cache.cache.pop('bounds')\n\n    simplified._cache = cache\n    # set the cache ID so it won't dump when a value is requested\n    simplified._cache.id_set()\n\n    return simplified", "response": "Simplifies a path2D object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsimplifying a path into a single Path2D object.", "response": "def simplify_spline(path, smooth=None, verbose=False):\n    \"\"\"\n    Replace discrete curves with b-spline or Arc and\n    return the result as a new Path2D object.\n\n    Parameters\n    ------------\n    path : trimesh.path.Path2D\n      Input geometry\n    smooth : float\n      Distance to smooth\n\n    Returns\n    ------------\n    simplified : Path2D\n      Consists of Arc and BSpline entities\n    \"\"\"\n\n    new_vertices = []\n    new_entities = []\n    scale = path.scale\n\n    for discrete in path.discrete:\n        circle = is_circle(discrete,\n                           scale=scale,\n                           verbose=verbose)\n        if circle is not None:\n            # the points are circular enough for our high standards\n            # so replace them with a closed Arc entity\n            new_entities.append(entities.Arc(points=np.arange(3) +\n                                             len(new_vertices),\n                                             closed=True))\n            new_vertices.extend(circle)\n            continue\n\n        # entities for this path\n        entity, vertices = points_to_spline_entity(discrete, smooth=smooth)\n        # reindex returned control points\n        entity.points += len(new_vertices)\n        # save entity and vertices\n        new_vertices.extend(vertices)\n        new_entities.append(entity)\n\n    # create the Path2D object for the result\n    simplified = type(path)(entities=new_entities,\n                            vertices=new_vertices)\n\n    return simplified"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef boolean(meshes, operation='difference'):\n    script = operation + '(){'\n    for i in range(len(meshes)):\n        script += 'import(\\\"$mesh_' + str(i) + '\\\");'\n    script += '}'\n    return interface_scad(meshes, script)", "response": "Run an operation on a set of meshes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_mtl(mtl):\n    # decode bytes if necessary\n    if hasattr(mtl, 'decode'):\n        mtl = mtl.decode('utf-8')\n\n    mtllib = None\n    mtllibs = []\n\n    # use universal newline splitting\n    for line in str.splitlines(str(mtl).strip()):\n        # clean leading/trailing whitespace and split\n        line_split = line.strip().split()\n        # needs to be at least two values\n        if len(line_split) <= 1:\n            continue\n        # store the keys\n        key = line_split[0]\n        if key == 'newmtl':\n            if mtllib:\n                mtllibs.append(mtllib)\n            mtllib = {'newmtl': line_split[1],\n                      'map_Kd': None,\n                      'Kd': None}\n        elif key == 'map_Kd':\n            mtllib[key] = line_split[1]\n        elif key == 'Kd':\n            mtllib[key] = [float(x) for x in line_split[1:]]\n    if mtllib:\n        mtllibs.append(mtllib)\n\n    return mtllibs", "response": "Parse a loaded MTL file and return a list of dicts each dict has keys newmtl map_Kd Kd"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_wavefront(file_obj, resolver=None, **kwargs):\n\n    # make sure text is UTF-8 with only \\n newlines\n    text = file_obj.read()\n    if hasattr(text, 'decode'):\n        text = text.decode('utf-8')\n    text = text.replace('\\r\\n', '\\n').replace('\\r', '\\n') + ' \\n'\n\n    meshes = []\n\n    def append_mesh():\n        # append kwargs for a Trimesh constructor\n        # to our list of meshes\n        if len(current['f']) > 0:\n            # get vertices as clean numpy array\n            vertices = np.array(current['v'],\n                                dtype=np.float64).reshape((-1, 3))\n            # do the same for faces\n            faces = np.array(current['f'],\n                             dtype=np.int64).reshape((-1, 3))\n\n            # get keys and values of remap as numpy arrays\n            # we are going to try to preserve the order as\n            # much as possible by sorting by remap key\n            keys, values = (np.array(list(remap.keys())),\n                            np.array(list(remap.values())))\n\n            try:\n                # if we sort keys as strings they will be an\n                # ordering like (1/1/1, 10/10/10) vs (1/1/1, 2/2/2)\n                # so try to convert to int before sorting\n                split = np.array([i.split('/')[0] for i in keys],\n                                 dtype=np.int)\n                order = split.argsort()\n            except BaseException:\n                # we can still use arbitrary order as a fallback\n                order = keys.argsort()\n\n            # new order of vertices\n            vert_order = values[order]\n\n            # we need to mask to preserve index relationship\n            # between faces and vertices\n            face_order = np.zeros(len(vertices),\n                                  dtype=np.int64)\n            face_order[vert_order] = np.arange(len(vertices),\n                                               dtype=np.int64)\n\n            # apply the ordering and put into kwarg dict\n            loaded = {'vertices': vertices[vert_order],\n                      'faces': face_order[faces],\n                      'metadata': {'object_name': object_name}}\n\n            # handle vertex normals\n            if len(current['vn']) > 0:\n                normals = np.array(current['vn'],\n                                   dtype=np.float64).reshape((-1, 3))\n                loaded['vertex_normals'] = normals[vert_order]\n\n            # build face groups information\n            # faces didn't move around so we don't have to reindex\n            if len(current['g']) > 0:\n                face_groups = np.zeros(len(current['f']) // 3,\n                                       dtype=np.int64)\n                for idx, start_f in current['g']:\n                    face_groups[start_f:] = idx\n                loaded['metadata']['face_groups'] = face_groups\n\n            if len(current['usemtl']) > 0 and any(current['vt_ok']):\n                texture = np.full((len(current['vt_ok']), 3),\n                                  np.nan,\n                                  dtype=np.float64)\n                # make sure mask is numpy array for older numpy\n                vt_ok = np.asanyarray(current['vt_ok'],\n                                      dtype=np.bool)\n                texture[vt_ok] = current['vt']\n\n                for usemtl in current['usemtl']:\n                    try:\n                        findices = usemtl_to_findices[usemtl]\n                        uv = texture[findices]\n                        # what is the file name of the texture image\n                        file_name = mtllibs[usemtl]['map_Kd']\n                        # get the data as bytes\n                        file_data = resolver.get(file_name)\n                        # load the bytes into a PIL image\n                        image = PIL.Image.open(\n                            util.wrap_as_stream(file_data))\n                        # create a texture object\n                        loaded['visual'] = visual.texture.TextureVisuals(\n                            uv=uv, image=image)\n                    except BaseException:\n                        log.error('failed to load texture: {}'.format(usemtl),\n                                  exc_info=True)\n\n            # apply the vertex order to the visual object\n            if 'visual' in loaded:\n                try:\n                    loaded['visual'].update_vertices(vert_order)\n                except BaseException:\n                    log.error('failed to update vertices',\n                              exc_info=True)\n                    loaded.pop('visual')\n\n            # this mesh is done so append the loaded mesh kwarg dict\n            meshes.append(loaded)\n\n    attribs = {k: [] for k in ['v', 'vt', 'vn']}\n    current = {k: [] for k in ['v', 'vt', 'vn',\n                               'f', 'g', 'usemtl',\n                               'vt_ok', 'vn_ok']}\n    # usemtl to 'f' indices\n    usemtl_to_findices = collections.defaultdict(list)\n    mtllibs = {}\n    # remap vertex indexes {str key: int index}\n    remap = {}\n    next_idx = 0\n    group_idx = 0\n    object_name = ''\n\n    for line in text.split(\"\\n\"):\n        line_split = line.strip().split()\n        if len(line_split) < 2:\n            continue\n        if line_split[0] in attribs:\n            # v, vt, or vn\n            # vertex, vertex texture, or vertex normal\n            # only parse 3 values, ignore colors\n            value = [float(x) for x in line_split[1:4]]\n            # vt: u [v] [w]  # v, w is optional and default value is 0\n            if line_split[0] == 'vt' and len(value) != 3:\n                for _ in range(3 - len(value)):\n                    value.append(0)\n            attribs[line_split[0]].append(value)\n        elif line_split[0] == 'f':\n            # a face\n            ft = line_split[1:]\n            if len(ft) == 4:\n                # hasty triangulation of quad\n                ft = [ft[0], ft[1], ft[2], ft[2], ft[3], ft[0]]\n            for f in ft:\n                # loop through each vertex reference of a face\n                # we are reshaping later into (n,3)\n                if f not in remap:\n                    remap[f] = next_idx\n                    next_idx += 1\n                    # faces are \"vertex index\"/\"vertex texture\"/\"vertex normal\"\n                    # you are allowed to leave a value blank, which .split\n                    # will handle by nicely maintaining the index\n                    f_split = f.split('/')\n                    current['v'].append(attribs['v'][int(f_split[0]) - 1])\n                    if len(f_split) > 1 and f_split[1] != '':\n                        current['vt'].append(\n                            attribs['vt'][int(f_split[1]) - 1])\n                        current['vt_ok'].append(True)\n                    else:\n                        current['vt_ok'].append(False)\n                    if len(f_split) > 2:\n                        current['vn'].append(\n                            attribs['vn'][int(f_split[2]) - 1])\n                        current['vn_ok'].append(True)\n                    else:\n                        current['vn_ok'].append(False)\n                    if len(current['usemtl']) > 0:\n                        usemtl_to_findices[current['usemtl']\n                                           [-1]].append(len(current['vt']) - 1)\n                current['f'].append(remap[f])\n        elif line_split[0] == 'o':\n            # defining a new object\n            append_mesh()\n            # reset current to empty lists\n            current = {k: [] for k in current.keys()}\n            usemtl_to_findices = collections.defaultdict(list)\n            remap = {}\n            next_idx = 0\n            group_idx = 0\n            object_name = line_split[1]\n\n        elif line_split[0] == 'g':\n            # defining a new group\n            group_idx += 1\n            current['g'].append((group_idx, len(current['f']) // 3))\n        elif line_split[0] == 'mtllib':\n\n            # the name of the referenced material file\n            mtl_name = line_split[1]\n            try:\n                # fetch bytes containing MTL data\n                mtl_data = resolver.get(mtl_name)\n                # load into a list of dict\n                for mtllib in parse_mtl(mtl_data):\n                    # save new materials\n                    mtllibs[mtllib['newmtl']] = mtllib\n            except BaseException:\n                log.error('unable to load material: {}'.format(mtl_name),\n                          exc_info=True)\n                continue\n\n        elif line_split[0] == 'usemtl':\n            current['usemtl'].append(line_split[1])\n\n    if next_idx > 0:\n        append_mesh()\n\n    return meshes", "response": "Loads an ascii Wavefront OBJ file into kwargs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports a Trimesh object as a Wavefront OBJ file", "response": "def export_wavefront(mesh,\n                     include_normals=True,\n                     include_texture=True):\n    \"\"\"\n    Export a mesh as a Wavefront OBJ file\n\n    Parameters\n    -----------\n    mesh: Trimesh object\n\n    Returns\n    -----------\n    export: str, string of OBJ format output\n    \"\"\"\n    # store the multiple options for formatting\n    # a vertex index for a face\n    face_formats = {('v',): '{}',\n                    ('v', 'vn'): '{}//{}',\n                    ('v', 'vt'): '{}/{}',\n                    ('v', 'vn', 'vt'): '{}/{}/{}'}\n    # we are going to reference face_formats with this\n    face_type = ['v']\n\n    export = 'v '\n    export += util.array_to_string(mesh.vertices,\n                                   col_delim=' ',\n                                   row_delim='\\nv ',\n                                   digits=8) + '\\n'\n\n    if include_normals and 'vertex_normals' in mesh._cache:\n        # if vertex normals are stored in cache export them\n        # these will have been autogenerated if they have ever been called\n        face_type.append('vn')\n        export += 'vn '\n        export += util.array_to_string(mesh.vertex_normals,\n                                       col_delim=' ',\n                                       row_delim='\\nvn ',\n                                       digits=8) + '\\n'\n\n    if (include_texture and\n        'vertex_texture' in mesh.metadata and\n            len(mesh.metadata['vertex_texture']) == len(mesh.vertices)):\n        # if vertex texture exists and is the right shape export here\n        face_type.append('vt')\n        export += 'vt '\n        export += util.array_to_string(mesh.metadata['vertex_texture'],\n                                       col_delim=' ',\n                                       row_delim='\\nvt ',\n                                       digits=8) + '\\n'\n\n    # the format for a single vertex reference of a face\n    face_format = face_formats[tuple(face_type)]\n    faces = 'f ' + util.array_to_string(mesh.faces + 1,\n                                        col_delim=' ',\n                                        row_delim='\\nf ',\n                                        value_format=face_format)\n    # add the exported faces to the export\n    export += faces\n\n    return export"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_resource(name):\n    # get the resource\n    resource = resource_string('trimesh',\n                               os.path.join('resources', name))\n    # make sure we return it as a string\n    if hasattr(resource, 'decode'):\n        return resource.decode('utf-8')\n    return resource", "response": "Get a resource from the trimesh resource folder."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _scale(self):\n        if self._scale_to_box:\n            # scale vertices to approximately a cube to help with\n            # numerical issues at very large/small scales\n            scale = 100.0 / self.mesh.scale\n        else:\n            scale = 1.0\n        return scale", "response": "Returns the scaling factor for precision."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the location of where a ray hits a surface.", "response": "def intersects_location(self,\n                            ray_origins,\n                            ray_directions,\n                            multiple_hits=True):\n        \"\"\"\n        Return the location of where a ray hits a surface.\n\n        Parameters\n        ----------\n        ray_origins:    (n,3) float, origins of rays\n        ray_directions: (n,3) float, direction (vector) of rays\n\n\n        Returns\n        ---------\n        locations: (n) sequence of (m,3) intersection points\n        index_ray: (n,) int, list of ray index\n        index_tri: (n,) int, list of triangle (face) indexes\n        \"\"\"\n        (index_tri,\n         index_ray,\n         locations) = self.intersects_id(\n             ray_origins=ray_origins,\n             ray_directions=ray_directions,\n             multiple_hits=multiple_hits,\n             return_locations=True)\n\n        return locations, index_ray, index_tri"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the triangles that intersect the current mesh with the given set of rays.", "response": "def intersects_id(self,\n                      ray_origins,\n                      ray_directions,\n                      multiple_hits=True,\n                      max_hits=20,\n                      return_locations=False):\n        \"\"\"\n        Find the triangles hit by a list of rays, including\n        optionally multiple hits along a single ray.\n\n        Parameters\n        ----------\n        ray_origins:      (n,3) float, origins of rays\n        ray_directions:   (n,3) float, direction (vector) of rays\n        multiple_hits:    bool, if True will return every hit along the ray\n                                if False will only return first hit\n        return_locations: bool, should we return hit locations or not\n\n        Returns\n        ----------\n        index_tri: (m,) int, index of triangle the ray hit\n        index_ray: (m,) int, index of ray\n        locations: (m,3) float, locations in space\n        \"\"\"\n        # make sure input is _dtype for embree\n        ray_origins = np.asanyarray(\n            deepcopy(ray_origins),\n            dtype=np.float64)\n        ray_directions = np.asanyarray(ray_directions,\n                                       dtype=np.float64)\n        ray_directions = util.unitize(ray_directions)\n\n        # since we are constructing all hits, save them to a deque then\n        # stack into (depth, len(rays)) at the end\n        result_triangle = deque()\n        result_ray_idx = deque()\n        result_locations = deque()\n\n        # the mask for which rays are still active\n        current = np.ones(len(ray_origins), dtype=np.bool)\n\n        if multiple_hits or return_locations:\n            # how much to offset ray to transport to the other side of face\n            distance = np.clip(_ray_offset_factor * self._scale,\n                               _ray_offset_floor,\n                               np.inf)\n            ray_offsets = ray_directions * distance\n\n            # grab the planes from triangles\n            plane_origins = self.mesh.triangles[:, 0, :]\n            plane_normals = self.mesh.face_normals\n\n        # use a for loop rather than a while to ensure this exits\n        # if a ray is offset from a triangle and then is reported\n        # hitting itself this could get stuck on that one triangle\n        for query_depth in range(max_hits):\n            # run the pyembree query\n            # if you set output=1 it will calculate distance along\n            # ray, which is bizzarely slower than our calculation\n            query = self._scene.run(\n                ray_origins[current],\n                ray_directions[current])\n\n            # basically we need to reduce the rays to the ones that hit\n            # something\n            hit = query != -1\n            # which triangle indexes were hit\n            hit_triangle = query[hit]\n\n            # eliminate rays that didn't hit anything from future queries\n            current_index = np.nonzero(current)[0]\n            current_index_no_hit = current_index[np.logical_not(hit)]\n            current_index_hit = current_index[hit]\n            current[current_index_no_hit] = False\n\n            # append the triangle and ray index to the results\n            result_triangle.append(hit_triangle)\n            result_ray_idx.append(current_index_hit)\n\n            # if we don't need all of the hits, return the first one\n            if ((not multiple_hits and\n                 not return_locations) or\n                    not hit.any()):\n                break\n\n            # find the location of where the ray hit the triangle plane\n            new_origins, valid = intersections.planes_lines(\n                plane_origins=plane_origins[hit_triangle],\n                plane_normals=plane_normals[hit_triangle],\n                line_origins=ray_origins[current],\n                line_directions=ray_directions[current])\n\n            if not valid.all():\n                # since a plane intersection was invalid we have to go back and\n                # fix some stuff, we pop the ray index and triangle index,\n                # apply the valid mask then append it right back to keep our\n                # indexes intact\n                result_ray_idx.append(result_ray_idx.pop()[valid])\n                result_triangle.append(result_triangle.pop()[valid])\n\n                # update the current rays to reflect that we couldn't find a\n                # new origin\n                current[current_index_hit[np.logical_not(valid)]] = False\n\n            # since we had to find the intersection point anyway we save it\n            # even if we're not going to return it\n            result_locations.extend(new_origins)\n\n            if multiple_hits:\n                # move the ray origin to the other side of the triangle\n                ray_origins[current] = new_origins + ray_offsets[current]\n            else:\n                break\n\n        # stack the deques into nice 1D numpy arrays\n        index_tri = np.hstack(result_triangle)\n        index_ray = np.hstack(result_ray_idx)\n\n        if return_locations:\n            locations = np.array(result_locations)\n\n            return index_tri, index_ray, locations\n        return index_tri, index_ray"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the index of the first triangle a ray hit.", "response": "def intersects_first(self,\n                         ray_origins,\n                         ray_directions):\n        \"\"\"\n        Find the index of the first triangle a ray hits.\n\n\n        Parameters\n        ----------\n        ray_origins:    (n,3) float, origins of rays\n        ray_directions: (n,3) float, direction (vector) of rays\n\n        Returns\n        ----------\n        triangle_index: (n,) int, index of triangle ray hit, or -1 if not hit\n        \"\"\"\n\n        ray_origins = np.asanyarray(deepcopy(ray_origins))\n        ray_directions = np.asanyarray(ray_directions)\n\n        triangle_index = self._scene.run(ray_origins,\n                                         ray_directions)\n        return triangle_index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef intersects_any(self,\n                       ray_origins,\n                       ray_directions):\n        \"\"\"\n        Check if a list of rays hits the surface.\n\n\n        Parameters\n        ----------\n        ray_origins:    (n,3) float, origins of rays\n        ray_directions: (n,3) float, direction (vector) of rays\n\n        Returns\n        ----------\n        hit:            (n,) bool, did each ray hit the surface\n        \"\"\"\n\n        first = self.intersects_first(ray_origins=ray_origins,\n                                      ray_directions=ray_directions)\n        hit = first != -1\n        return hit", "response": "Check if a list of rays intersect the surface."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_3MF(file_obj,\n             postprocess=True,\n             **kwargs):\n    \"\"\"\n    Load a 3MF formatted file into a Trimesh scene.\n\n    Parameters\n    ------------\n    file_obj:       file object\n\n    Returns\n    ------------\n    kwargs: dict, with keys 'graph', 'geometry', 'base_frame'\n    \"\"\"\n    # dict, {name in archive: BytesIo}\n    archive = util.decompress(file_obj, file_type='zip')\n    # load the XML into an LXML tree\n    tree = etree.XML(archive['3D/3dmodel.model'].read())\n\n    # { mesh id : mesh name}\n    id_name = {}\n    # { mesh id: (n,3) float vertices}\n    v_seq = {}\n    # { mesh id: (n,3) int faces}\n    f_seq = {}\n    # components are objects that contain other objects\n    # {id : [other ids]}\n    components = collections.defaultdict(list)\n\n    for obj in tree.iter('{*}object'):\n        # id is mandatory\n        index = obj.attrib['id']\n        # not required, so use a get call which will return None\n        # if the tag isn't populated\n        if 'name' in obj.attrib:\n            name = obj.attrib['name']\n        else:\n            name = str(index)\n        # store the name by index\n        id_name[index] = name\n\n        # if the object has actual geometry data, store it\n        for mesh in obj.iter('{*}mesh'):\n            vertices = mesh.find('{*}vertices')\n            vertices = np.array([[i.attrib['x'],\n                                  i.attrib['y'],\n                                  i.attrib['z']] for\n                                 i in vertices.iter('{*}vertex')],\n                                dtype=np.float64)\n            v_seq[index] = vertices\n\n            faces = mesh.find('{*}triangles')\n            faces = np.array([[i.attrib['v1'],\n                               i.attrib['v2'],\n                               i.attrib['v3']] for\n                              i in faces.iter('{*}triangle')],\n                             dtype=np.int64)\n            f_seq[index] = faces\n\n        # components are references to other geometries\n        for c in obj.iter('{*}component'):\n            mesh_index = c.attrib['objectid']\n            transform = _attrib_to_transform(c.attrib)\n            components[index].append((mesh_index, transform))\n\n    # load information about the scene graph\n    # each instance is a single geometry\n    build_items = []\n    # scene graph information stored here, aka \"build\" the scene\n    build = tree.find('{*}build')\n    for item in build.iter('{*}item'):\n        # get a transform from the item's attributes\n        transform = _attrib_to_transform(item.attrib)\n        # the index of the geometry this item instantiates\n        build_items.append((item.attrib['objectid'], transform))\n\n    # collect unit information from the tree\n    if 'unit' in tree.attrib:\n        metadata = {'units': tree.attrib['unit']}\n    else:\n        # the default units, defined by the specification\n        metadata = {'units': 'millimeters'}\n\n    # have one mesh per 3MF object\n    # one mesh per geometry ID, store as kwargs for the object\n    meshes = {}\n    for gid in v_seq.keys():\n        name = id_name[gid]\n        meshes[name] = {'vertices': v_seq[gid],\n                        'faces': f_seq[gid],\n                        'metadata': metadata.copy()}\n\n    # turn the item / component representation into\n    # a MultiDiGraph to compound our pain\n    g = nx.MultiDiGraph()\n    # build items are the only things that exist according to 3MF\n    # so we accomplish that by linking them to the base frame\n    for gid, tf in build_items:\n        g.add_edge('world', gid, matrix=tf)\n    # components are instances which need to be linked to base\n    # frame by a build_item\n    for start, group in components.items():\n        for i, (gid, tf) in enumerate(group):\n            g.add_edge(start, gid, matrix=tf)\n\n    # turn the graph into kwargs for a scene graph\n    # flatten the scene structure and simplify to\n    # a single unique node per instance\n    graph_args = []\n    parents = collections.defaultdict(set)\n    for path in graph.multigraph_paths(G=g,\n                                       source='world'):\n        # collect all the transform on the path\n        transforms = graph.multigraph_collect(G=g,\n                                              traversal=path,\n                                              attrib='matrix')\n        # combine them into a single transform\n        if len(transforms) == 1:\n            transform = transforms[0]\n        else:\n            transform = util.multi_dot(transforms)\n\n        # the last element of the path should be the geometry\n        last = path[-1][0]\n        # if someone included an undefined component, skip it\n        if last not in id_name:\n            log.debug('id {} included but not defined!'.format(last))\n            continue\n        # frame names unique\n        name = id_name[last] + util.unique_id()\n        # index in meshes\n        geom = id_name[last]\n\n        # collect parents if we want to combine later\n        if len(path) > 2:\n            parent = path[-2][0]\n            parents[parent].add(last)\n\n        graph_args.append({'frame_from': 'world',\n                           'frame_to': name,\n                           'matrix': transform,\n                           'geometry': geom})\n\n    # solidworks will export each body as its own mesh with the part\n    # name as the parent so optionally rename and combine these bodies\n    if postprocess and all('body' in i.lower() for i in meshes.keys()):\n        # don't rename by default\n        rename = {k: k for k in meshes.keys()}\n        for parent, mesh_name in parents.items():\n            # only handle the case where a parent has a single child\n            # if there are multiple children we would do a combine op\n            if len(mesh_name) != 1:\n                continue\n            # rename the part\n            rename[id_name[next(iter(mesh_name))]] = id_name[parent].split(\n                '(')[0]\n\n        # apply the rename operation meshes\n        meshes = {rename[k]: m for k, m in meshes.items()}\n        # rename geometry references in the scene graph\n        for arg in graph_args:\n            if 'geometry' in arg:\n                arg['geometry'] = rename[arg['geometry']]\n\n    # construct the kwargs to load the scene\n    kwargs = {'base_frame': 'world',\n              'graph': graph_args,\n              'geometry': meshes,\n              'metadata': metadata}\n\n    return kwargs", "response": "Loads a 3MF file into a Trimesh scene."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _attrib_to_transform(attrib):\n\n    transform = np.eye(4, dtype=np.float64)\n    if 'transform' in attrib:\n        # wangle their transform format\n        values = np.array(\n            attrib['transform'].split(),\n            dtype=np.float64).reshape((4, 3)).T\n        transform[:3, :4] = values\n    return transform", "response": "Extract a homogenous transform from a dictionary containing transform"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check(a, b, digits):\n    a = np.array(a, dtype=np.float64)\n    b = np.array(b, dtype=np.float64)\n\n    if a.shape != b.shape or a.shape[-1] != 2:\n        raise ValueError('ranges must be identical and (2,)!')\n\n    # if input was single interval reshape it here\n    is_1D = False\n    if len(a.shape) == 1:\n        a = a.reshape((-1, 2))\n        b = b.reshape((-1, 2))\n        is_1D = True\n\n    # make sure ranges are sorted\n    a.sort(axis=1)\n    b.sort(axis=1)\n\n    # compare in fixed point as integers\n    a_int = (a * 10**digits).round().astype(np.int64)\n    b_int = (b * 10**digits).round().astype(np.int64)\n\n    return a, b, a_int, b_int, is_1D", "response": "Check input ranges convert them to vector form and get a fixed precision integer version of them."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a pair of ranges merge them in to one range if they overlap at all.", "response": "def intersection(a, b, digits=8):\n    \"\"\"\n    Given a pair of ranges, merge them in to\n    one range if they overlap at all\n\n    Parameters\n    --------------\n    a : (2, ) float\n      Start and end of a 1D interval\n    b : (2, ) float\n      Start and end of a 1D interval\n    digits : int\n      How many digits to consider\n\n    Returns\n    --------------\n    intersects : bool or (n,) bool\n      Indicates if the ranges overlap at all\n    new_range : (2, ) or (2, 2) float\n      The unioned range from the two inputs,\n      or both of the original ranges if not overlapping\n    \"\"\"\n    # check shape and convert\n    a, b, a_int, b_int, is_1D = check(a, b, digits)\n\n    # what are the starting and ending points of the overlap\n    overlap = np.zeros(a.shape, dtype=np.float64)\n\n    # A fully overlaps B\n    current = np.logical_and(a_int[:, 0] <= b_int[:, 0],\n                             a_int[:, 1] >= b_int[:, 1])\n    overlap[current] = b[current]\n\n    # B fully overlaps A\n    current = np.logical_and(a_int[:, 0] >= b_int[:, 0],\n                             a_int[:, 1] <= b_int[:, 1])\n    overlap[current] = a[current]\n\n    # A starts B ends\n    # A:, 0   B:, 0     A:, 1        B:, 1\n    current = np.logical_and(\n        np.logical_and(a_int[:, 0] <= b_int[:, 0],\n                       b_int[:, 0] < a_int[:, 1]),\n        a_int[:, 1] < b_int[:, 1])\n    overlap[current] = np.column_stack([b[current][:, 0],\n                                        a[current][:, 1]])\n\n    # B starts A ends\n    # B:, 0  A:, 0    B:, 1  A:, 1\n    current = np.logical_and(\n        np.logical_and(b_int[:, 0] <= a_int[:, 0],\n                       a_int[:, 0] < b_int[:, 1]),\n        b_int[:, 1] < a_int[:, 1])\n    overlap[current] = np.column_stack([a[current][:, 0],\n                                        b[current][:, 1]])\n\n    # is range overlapping at all\n    intersects = overlap.ptp(axis=1) > 10**-digits\n\n    if is_1D:\n        return intersects[0], overlap[0]\n\n    return intersects, overlap"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef geometry_hash(geometry):\n    if hasattr(geometry, 'md5'):\n        # for most of our trimesh objects\n        md5 = geometry.md5()\n    elif hasattr(geometry, 'tostring'):\n        # for unwrapped ndarray objects\n        md5 = str(hash(geometry.tostring()))\n\n    if hasattr(geometry, 'visual'):\n        # if visual properties are defined\n        md5 += str(geometry.visual.crc())\n    return md5", "response": "Returns an MD5 for a geometry object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrender a scene to a PNG format.", "response": "def render_scene(scene,\n                 resolution=(1080, 1080),\n                 visible=True,\n                 **kwargs):\n    \"\"\"\n    Render a preview of a scene to a PNG.\n\n    Parameters\n    ------------\n    scene : trimesh.Scene\n      Geometry to be rendered\n    resolution : (2,) int\n      Resolution in pixels\n    kwargs : **\n      Passed to SceneViewer\n\n    Returns\n    ---------\n    render : bytes\n      Image in PNG format\n    \"\"\"\n    window = SceneViewer(scene,\n                         start_loop=False,\n                         visible=visible,\n                         resolution=resolution,\n                         **kwargs)\n\n    if visible is None:\n        visible = platform.system() != 'Linux'\n\n    # need to run loop twice to display anything\n    for i in range(2):\n        pyglet.clock.tick()\n        window.switch_to()\n        window.dispatch_events()\n        window.dispatch_event('on_draw')\n        window.flip()\n\n    from ..util import BytesIO\n\n    # save the color buffer data to memory\n    file_obj = BytesIO()\n    window.save_image(file_obj)\n    file_obj.seek(0)\n    render = file_obj.read()\n    window.close()\n\n    return render"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_geometry(self, name, geometry, **kwargs):\n        # convert geometry to constructor args\n        args = rendering.convert_to_vertexlist(geometry, **kwargs)\n        # create the indexed vertex list\n        self.vertex_list[name] = self.batch.add_indexed(*args)\n        # save the MD5 of the geometry\n        self.vertex_list_hash[name] = geometry_hash(geometry)\n        # save the rendering mode from the constructor args\n        self.vertex_list_mode[name] = args[1]\n\n        # if a geometry has a texture defined convert it to opengl form and\n        # save\n        if hasattr(geometry, 'visual') and hasattr(\n                geometry.visual, 'material'):\n            tex = rendering.material_to_texture(geometry.visual.material)\n            if tex is not None:\n                self.textures[name] = tex", "response": "Add a geometry to the viewer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreset the view of the object to the default view.", "response": "def reset_view(self, flags=None):\n        \"\"\"\n        Set view to the default view.\n\n        Parameters\n        --------------\n        flags : None or dict\n          If any view key passed override the default\n          e.g. {'cull': False}\n        \"\"\"\n        self.view = {\n            'cull': True,\n            'axis': False,\n            'fullscreen': False,\n            'wireframe': False,\n            'ball': Trackball(\n                pose=self._initial_camera_transform,\n                size=self.scene.camera.resolution,\n                scale=self.scene.scale,\n                target=self.scene.centroid,\n            ),\n        }\n\n        try:\n            # if any flags are passed override defaults\n            if isinstance(flags, dict):\n                for k, v in flags.items():\n                    if k in self.view:\n                        self.view[k] = v\n                self.update_flags()\n        except BaseException:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninitialize the OpenGL scene using pyglet.", "response": "def init_gl(self):\n        \"\"\"\n        Perform the magic incantations to create an\n        OpenGL scene using pyglet.\n        \"\"\"\n\n        # default background color is white-ish\n        background = [.99, .99, .99, 1.0]\n        # if user passed a background color use it\n        if 'background' in self.kwargs:\n            try:\n                # convert to (4,) uint8 RGBA\n                background = to_rgba(self.kwargs['background'])\n                # convert to 0.0 - 1.0 float\n                background = background.astype(np.float64) / 255.0\n            except BaseException:\n                log.error('background color set but wrong!',\n                          exc_info=True)\n\n        self._gl_set_background(background)\n        self._gl_enable_depth(self.scene)\n        self._gl_enable_color_material()\n        self._gl_enable_blending()\n        self._gl_enable_smooth_lines()\n        self._gl_enable_lighting(self.scene)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _gl_enable_lighting(scene):\n        gl.glEnable(gl.GL_LIGHTING)\n        # opengl only supports 7 lights?\n        for i, light in enumerate(scene.lights[:7]):\n            # the index of which light we have\n            lightN = eval('gl.GL_LIGHT{}'.format(i))\n\n            # get the transform for the light by name\n            matrix = scene.graph[light.name][0]\n\n            # convert light object to glLightfv calls\n            multiargs = rendering.light_to_gl(\n                light=light,\n                transform=matrix,\n                lightN=lightN)\n\n            # enable the light in question\n            gl.glEnable(lightN)\n            # run the glLightfv calls\n            for args in multiargs:\n                gl.glLightfv(*args)", "response": "Enable the lights in the current context and apply them as openGL lights."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef toggle_axis(self):\n        # cycle through three axis states\n        states = [False, 'world', 'all']\n        # the state after toggling\n        index = (states.index(self.view['axis']) + 1) % len(states)\n        # update state to next index\n        self.view['axis'] = states[index]\n        # perform gl actions\n        self.update_flags()", "response": "Toggle a rendered XYZ or RGB axis marker"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_flags(self):\n        # view mode, filled vs wirefrom\n        if self.view['wireframe']:\n            gl.glPolygonMode(gl.GL_FRONT_AND_BACK, gl.GL_LINE)\n        else:\n            gl.glPolygonMode(gl.GL_FRONT_AND_BACK, gl.GL_FILL)\n\n        # set fullscreen or windowed\n        self.set_fullscreen(fullscreen=self.view['fullscreen'])\n\n        # backface culling on or off\n        if self.view['cull']:\n            gl.glEnable(gl.GL_CULL_FACE)\n        else:\n            gl.glDisable(gl.GL_CULL_FACE)\n\n        # case where we WANT an axis and NO vertexlist\n        # is stored internally\n        if self.view['axis'] and self._axis is None:\n            from .. import creation\n            # create an axis marker sized relative to the scene\n            axis = creation.axis(origin_size=self.scene.scale / 100)\n            # create ordered args for a vertex list\n            args = rendering.mesh_to_vertexlist(axis)\n            # store the axis as a reference\n            self._axis = self.batch.add_indexed(*args)\n\n        # case where we DON'T want an axis but a vertexlist\n        # IS stored internally\n        elif not self.view['axis'] and self._axis is not None:\n            # remove the axis from the rendering batch\n            self._axis.delete()\n            # set the reference to None\n            self._axis = None", "response": "Update the internal flags of the current object based on the current view flags."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_mouse_press(self, x, y, buttons, modifiers):\n        self.view['ball'].set_state(Trackball.STATE_ROTATE)\n        if (buttons == pyglet.window.mouse.LEFT):\n            ctrl = (modifiers & pyglet.window.key.MOD_CTRL)\n            shift = (modifiers & pyglet.window.key.MOD_SHIFT)\n            if (ctrl and shift):\n                self.view['ball'].set_state(Trackball.STATE_ZOOM)\n            elif shift:\n                self.view['ball'].set_state(Trackball.STATE_ROLL)\n            elif ctrl:\n                self.view['ball'].set_state(Trackball.STATE_PAN)\n        elif (buttons == pyglet.window.mouse.MIDDLE):\n            self.view['ball'].set_state(Trackball.STATE_PAN)\n        elif (buttons == pyglet.window.mouse.RIGHT):\n            self.view['ball'].set_state(Trackball.STATE_ZOOM)\n\n        self.view['ball'].down(np.array([x, y]))\n        self.scene.camera.transform = self.view['ball'].pose", "response": "Set the start point of the drag."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npans or rotate the view.", "response": "def on_mouse_drag(self, x, y, dx, dy, buttons, modifiers):\n        \"\"\"\n        Pan or rotate the view.\n        \"\"\"\n        self.view['ball'].drag(np.array([x, y]))\n        self.scene.camera.transform = self.view['ball'].pose"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when a key is pressed on the main window.", "response": "def on_key_press(self, symbol, modifiers):\n        \"\"\"\n        Call appropriate functions given key presses.\n        \"\"\"\n        magnitude = 10\n        if symbol == pyglet.window.key.W:\n            self.toggle_wireframe()\n        elif symbol == pyglet.window.key.Z:\n            self.reset_view()\n        elif symbol == pyglet.window.key.C:\n            self.toggle_culling()\n        elif symbol == pyglet.window.key.A:\n            self.toggle_axis()\n        elif symbol == pyglet.window.key.Q:\n            self.on_close()\n        elif symbol == pyglet.window.key.M:\n            self.maximize()\n        elif symbol == pyglet.window.key.F:\n            self.toggle_fullscreen()\n\n        if symbol in [\n            pyglet.window.key.LEFT,\n            pyglet.window.key.RIGHT,\n            pyglet.window.key.DOWN,\n            pyglet.window.key.UP,\n        ]:\n            self.view['ball'].down([0, 0])\n            if symbol == pyglet.window.key.LEFT:\n                self.view['ball'].drag([-magnitude, 0])\n            elif symbol == pyglet.window.key.RIGHT:\n                self.view['ball'].drag([magnitude, 0])\n            elif symbol == pyglet.window.key.DOWN:\n                self.view['ball'].drag([0, -magnitude])\n            elif symbol == pyglet.window.key.UP:\n                self.view['ball'].drag([0, magnitude])\n            self.scene.camera.transform = self.view['ball'].pose"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_draw(self):\n        self._update_meshes()\n        gl.glClear(gl.GL_COLOR_BUFFER_BIT | gl.GL_DEPTH_BUFFER_BIT)\n        gl.glLoadIdentity()\n\n        # pull the new camera transform from the scene\n        transform_camera = self.scene.graph.get(\n            frame_to='world',\n            frame_from=self.scene.camera.name)[0]\n\n        # apply the camera transform to the matrix stack\n        gl.glMultMatrixf(rendering.matrix_to_gl(transform_camera))\n\n        # we want to render fully opaque objects first,\n        # followed by objects which have transparency\n        node_names = collections.deque(self.scene.graph.nodes_geometry)\n        # how many nodes did we start with\n        count_original = len(node_names)\n        count = -1\n\n        # if we are rendering an axis marker at the world\n        if self._axis:\n            # we stored it as a vertex list\n            self._axis.draw(mode=gl.GL_TRIANGLES)\n\n        while len(node_names) > 0:\n            count += 1\n            current_node = node_names.popleft()\n\n            # get the transform from world to geometry and mesh name\n            transform, geometry_name = self.scene.graph[current_node]\n\n            # if no geometry at this frame continue without rendering\n            if geometry_name is None:\n                continue\n\n            # if a geometry is marked as fixed apply the inverse view transform\n            if self.fixed is not None and geometry_name in self.fixed:\n                # remove altered camera transform from fixed geometry\n                transform_fix = np.linalg.inv(\n                    np.dot(self._initial_camera_transform, transform_camera))\n                # apply the transform so the fixed geometry doesn't move\n                transform = np.dot(transform, transform_fix)\n\n            # get a reference to the mesh so we can check transparency\n            mesh = self.scene.geometry[geometry_name]\n            if mesh.is_empty:\n                continue\n\n            # add a new matrix to the model stack\n            gl.glPushMatrix()\n            # transform by the nodes transform\n            gl.glMultMatrixf(rendering.matrix_to_gl(transform))\n\n            # draw an axis marker for each mesh frame\n            if self.view['axis'] == 'all':\n                self._axis.draw(mode=gl.GL_TRIANGLES)\n\n            # transparent things must be drawn last\n            if (hasattr(mesh, 'visual') and\n                hasattr(mesh.visual, 'transparency')\n                    and mesh.visual.transparency):\n                # put the current item onto the back of the queue\n                if count < count_original:\n                    # add the node to be drawn last\n                    node_names.append(current_node)\n                    # pop the matrix stack for now\n                    gl.glPopMatrix()\n                    # come back to this mesh later\n                    continue\n\n            # if we have texture enable the target texture\n            texture = None\n            if geometry_name in self.textures:\n                texture = self.textures[geometry_name]\n                gl.glEnable(texture.target)\n                gl.glBindTexture(texture.target, texture.id)\n\n            # get the mode of the current geometry\n            mode = self.vertex_list_mode[geometry_name]\n            # draw the mesh with its transform applied\n            self.vertex_list[geometry_name].draw(mode=mode)\n            # pop the matrix stack as we drew what we needed to draw\n            gl.glPopMatrix()\n\n            # disable texture after using\n            if texture is not None:\n                gl.glDisable(texture.target)", "response": "Update the meshes and the scene and scene scene."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_image(self, file_obj):\n        manager = pyglet.image.get_buffer_manager()\n        colorbuffer = manager.get_color_buffer()\n\n        # if passed a string save by name\n        if hasattr(file_obj, 'write'):\n            colorbuffer.save(file=file_obj)\n        else:\n            colorbuffer.save(filename=file_obj)", "response": "Save the current color buffer to a file object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unit_conversion(current, desired):\n    current = str(current).strip().lower()\n    desired = str(desired).strip().lower()\n    conversion = TO_INCH[current] / TO_INCH[desired]\n    return conversion", "response": "Calculate the conversion from one set of units to another."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to guess the units from the object s metadata and return the string which is the units of the object.", "response": "def units_from_metadata(obj, guess=True):\n    \"\"\"\n    Try to extract hints from metadata and if that fails\n    guess based on the object scale.\n\n\n    Parameters\n    ------------\n    obj: object\n        Has attributes 'metadata' (dict) and 'scale' (float)\n    guess : bool\n        If metadata doesn't indicate units, guess from scale\n\n    Returns\n    ------------\n    units: str\n        A guess of what the units might be\n    \"\"\"\n    # try to guess from metadata\n    for key in ['file_name', 'name']:\n        if key not in obj.metadata:\n            continue\n        # get the string which might contain unit hints\n        hints = obj.metadata[key].lower()\n        if 'unit' in hints:\n            # replace all delimiter options with white space\n            for delim in '_-.':\n                hints = hints.replace(delim, ' ')\n            # loop through each hint\n            for hint in hints.strip().split():\n                # key word is \"unit\" or \"units\"\n                if 'unit' not in hint:\n                    continue\n                # get rid of keyword and whitespace\n                hint = hint.replace(\n                    'units', '').replace(\n                        'unit', '').strip()\n                # if the hint is a valid unit return it\n                if hint in TO_INCH:\n                    return hint\n\n    if not guess:\n        raise ValueError('no units and not allowed to guess')\n\n    # we made it to the wild ass guess section\n    # if the scale is larger than 100 mystery units\n    # declare the model to be millimeters, otherwise inches\n    log.warning('no units: guessing from scale')\n    if float(obj.scale) > 100.0:\n        return 'millimeters'\n    else:\n        return 'inches'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_units(obj, desired, guess=False):\n    if obj.units is None:\n        # try to extract units from metadata\n        # if nothing specified in metadata and not allowed\n        # to guess will raise a ValueError\n        obj.units = units_from_metadata(obj, guess=guess)\n\n    log.info('converting units from %s to %s', obj.units, desired)\n    # float, conversion factor\n    conversion = unit_conversion(obj.units, desired)\n\n    # apply scale uses transforms which preserve\n    # cached properties (rather than just multiplying vertices)\n    obj.apply_scale(conversion)\n    # units are now desired units\n    obj.units = desired", "response": "Convert the units of the object to the desired units."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export_path(path,\n                file_type=None,\n                file_obj=None,\n                **kwargs):\n    \"\"\"\n    Export a Path object to a file- like object, or to a filename\n\n    Parameters\n    ---------\n    file_obj:  None, str, or file object\n      A filename string or a file-like object\n    file_type: None or str\n      File type, e.g.: 'svg', 'dxf'\n    kwargs : passed to loader\n\n    Returns\n    ---------\n    exported : str or bytes\n      Data exported\n    \"\"\"\n    # if file object is a string it is probably a file path\n    # so we can split the extension to set the file type\n    if util.is_string(file_obj):\n        file_type = util.split_extension(file_obj)\n\n    # run the export\n    export = _path_exporters[file_type](path, **kwargs)\n    # if we've been passed files write the data\n    _write_export(export=export, file_obj=file_obj)\n\n    return export", "response": "Exports a Path object to a file - like object or to a filename."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_dict(path):\n    export_entities = [e.to_dict() for e in path.entities]\n    export_object = {'entities': export_entities,\n                     'vertices': path.vertices.tolist()}\n    return export_object", "response": "Export a Path object as a dict of kwargs for the Path constructor."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting a string to a file.", "response": "def _write_export(export, file_obj=None):\n    \"\"\"\n    Write a string to a file.\n    If file_obj isn't specified, return the string\n\n    Parameters\n    ---------\n    export: a string of the export data\n    file_obj: a file-like object or a filename\n    \"\"\"\n\n    if file_obj is None:\n        return export\n\n    if hasattr(file_obj, 'write'):\n        out_file = file_obj\n    else:\n        out_file = open(file_obj, 'wb')\n    try:\n        out_file.write(export)\n    except TypeError:\n        out_file.write(export.encode('utf-8'))\n\n    out_file.close()\n\n    return export"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsampling the surface of a Trimesh object and return the specified number of points in space on the object.", "response": "def sample_surface(mesh, count):\n    \"\"\"\n    Sample the surface of a mesh, returning the specified\n    number of points\n\n    For individual triangle sampling uses this method:\n    http://mathworld.wolfram.com/TrianglePointPicking.html\n\n    Parameters\n    ---------\n    mesh: Trimesh object\n    count: number of points to return\n\n    Returns\n    ---------\n    samples: (count,3) points in space on the surface of mesh\n    face_index: (count,) indices of faces for each sampled point\n    \"\"\"\n\n    # len(mesh.faces) float, array of the areas\n    # of each face of the mesh\n    area = mesh.area_faces\n    # total area (float)\n    area_sum = np.sum(area)\n    # cumulative area (len(mesh.faces))\n    area_cum = np.cumsum(area)\n    face_pick = np.random.random(count) * area_sum\n    face_index = np.searchsorted(area_cum, face_pick)\n\n    # pull triangles into the form of an origin + 2 vectors\n    tri_origins = mesh.triangles[:, 0]\n    tri_vectors = mesh.triangles[:, 1:].copy()\n    tri_vectors -= np.tile(tri_origins, (1, 2)).reshape((-1, 2, 3))\n\n    # pull the vectors for the faces we are going to sample from\n    tri_origins = tri_origins[face_index]\n    tri_vectors = tri_vectors[face_index]\n\n    # randomly generate two 0-1 scalar components to multiply edge vectors by\n    random_lengths = np.random.random((len(tri_vectors), 2, 1))\n\n    # points will be distributed on a quadrilateral if we use 2 0-1 samples\n    # if the two scalar components sum less than 1.0 the point will be\n    # inside the triangle, so we find vectors longer than 1.0 and\n    # transform them to be inside the triangle\n    random_test = random_lengths.sum(axis=1).reshape(-1) > 1.0\n    random_lengths[random_test] -= 1.0\n    random_lengths = np.abs(random_lengths)\n\n    # multiply triangle edge vectors by the random lengths and sum\n    sample_vector = (tri_vectors * random_lengths).sum(axis=1)\n\n    # finally, offset by the origin to generate\n    # (n,3) points in space on the triangle\n    samples = sample_vector + tri_origins\n\n    return samples, face_index"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef volume_mesh(mesh, count):\n    points = (np.random.random((count, 3)) * mesh.extents) + mesh.bounds[0]\n    contained = mesh.contains(points)\n    samples = points[contained][:count]\n    return samples", "response": "Use rejection sampling to produce points randomly distributed\n    in the volume of a Trimesh object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef volume_rectangular(extents,\n                       count,\n                       transform=None):\n    \"\"\"\n    Return random samples inside a rectangular volume.\n\n    Parameters\n    ----------\n    extents:   (3,) float, side lengths of rectangular solid\n    count:     int, number of points to return\n    transform: (4,4) float, transformation matrix\n\n    Returns\n    ---------\n    samples: (count, 3) float, points in volume\n    \"\"\"\n    samples = np.random.random((count, 3)) - .5\n    samples *= extents\n    if transform is not None:\n        samples = transformations.transform_points(samples,\n                                                   transform)\n    return samples", "response": "Return random samples inside a rectangular volume."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsampling the surface of a Trimesh object and return the samples which are approximately evenly spaced.", "response": "def sample_surface_even(mesh, count):\n    \"\"\"\n    Sample the surface of a mesh, returning samples which are\n    approximately evenly spaced.\n\n\n    Parameters\n    ---------\n    mesh: Trimesh object\n    count: number of points to return\n\n    Returns\n    ---------\n    samples: (count,3) points in space on the surface of mesh\n    face_index: (count,) indices of faces for each sampled point\n    \"\"\"\n    from .points import remove_close\n\n    radius = np.sqrt(mesh.area / (2 * count))\n\n    samples, ids = sample_surface(mesh, count * 5)\n    result, mask = remove_close(samples, radius)\n\n    return result, ids[mask]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a list of random points on the surface of a unit sphere.", "response": "def sample_surface_sphere(count):\n    \"\"\"\n    Correctly pick random points on the surface of a unit sphere\n\n    Uses this method:\n    http://mathworld.wolfram.com/SpherePointPicking.html\n\n    Parameters\n    ----------\n    count: int, number of points to return\n\n    Returns\n    ----------\n    points: (count,3) float, list of random points on a unit sphere\n    \"\"\"\n\n    u, v = np.random.random((2, count))\n\n    theta = np.pi * 2 * u\n    phi = np.arccos((2 * v) - 1)\n\n    points = util.spherical_to_vector(np.column_stack((theta, phi)))\n    return points"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef segments_to_parameters(segments):\n    segments = np.asanyarray(segments, dtype=np.float64)\n    if not util.is_shape(segments, (-1, 2, (2, 3))):\n        raise ValueError('incorrect segment shape!',\n                         segments.shape)\n\n    # make the initial origin one of the end points\n    endpoint = segments[:, 0]\n    vectors = segments[:, 1] - endpoint\n    vectors_norm = np.linalg.norm(vectors, axis=1)\n    vectors /= vectors_norm.reshape((-1, 1))\n\n    # find the point along the line nearest the origin\n    offset = util.diagonal_dot(endpoint, vectors)\n    # points nearest [0, 0, 0] will be our new origin\n    origins = endpoint + (offset.reshape((-1, 1)) * -vectors)\n\n    # parametric start and end of line segment\n    parameters = np.column_stack((offset, offset + vectors_norm))\n\n    return origins, vectors, parameters", "response": "Converts a list of 3D line segments into a list of origins vectors and parameters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parameters_to_segments(origins, vectors, parameters):\n    # don't copy input\n    origins = np.asanyarray(origins, dtype=np.float64)\n    vectors = np.asanyarray(vectors, dtype=np.float64)\n    parameters = np.asanyarray(parameters, dtype=np.float64)\n\n    # turn the segments into a reshapable 2D array\n    segments = np.hstack((origins + vectors * parameters[:, :1],\n                          origins + vectors * parameters[:, 1:]))\n\n    return segments.reshape((-1, 2, origins.shape[1]))", "response": "Convert a parametric line segment representation to a two point line segment representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef colinear_pairs(segments,\n                   radius=.01,\n                   angle=.01,\n                   length=None):\n    \"\"\"\n    Find pairs of segments which are colinear.\n\n    Parameters\n    -------------\n    segments : (n, 2, (2, 3)) float\n      Two or three dimensional line segments\n    radius : float\n      Maximum radius line origins can differ\n      and be considered colinear\n    angle : float\n      Maximum angle in radians segments can\n      differ and still be considered colinear\n    length : None or float\n      If specified, will additionally require\n      that pairs have a mean vertex distance less\n      than this value from each other to qualify.\n\n    Returns\n    ------------\n    pairs : (m, 2) int\n      Indexes of segments which are colinear\n    \"\"\"\n    from scipy import spatial\n\n    # convert segments to parameterized origins\n    # which are the closest point on the line to\n    # the actual zero- origin\n    origins, vectors, param = segments_to_parameters(segments)\n\n    # create a kdtree for origins\n    tree = spatial.cKDTree(origins)\n\n    # find origins closer than specified radius\n    pairs = tree.query_pairs(r=radius, output_type='ndarray')\n\n    # calculate angles between pairs\n    angles = geometry.vector_angle(vectors[pairs])\n\n    # angles can be within tolerance of 180 degrees or 0.0 degrees\n    angle_ok = np.logical_or(\n        util.isclose(angles, np.pi, atol=angle),\n        util.isclose(angles, 0.0, atol=angle))\n\n    # apply angle threshold\n    colinear = pairs[angle_ok]\n\n    # if length is specified check endpoint proximity\n    if length is not None:\n        # make sure parameter pairs are ordered\n        param.sort(axis=1)\n        # calculate the mean parameter distance for each colinear pair\n        distance = param[colinear].ptp(axis=1).mean(axis=1)\n        # if the MEAN distance is less than specified length consider\n        # the segment to be identical: worst case single- vertex\n        # distance is 2*length\n        identical = distance < length\n        # remove non- identical pairs\n        colinear = colinear[identical]\n\n    return colinear", "response": "Returns a kdtree of segments which are colinear."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfind any points that lie on a segment (not an endpoint) and then split that segment into two segments. We are basically going to find the distance between point and both segment vertex, and see if it is with tolerance of the segment length. Parameters -------------- segments : (n, 2, (2, 3) float Line segments in space points : (n, (2, 3)) float Points in space atol : float Absolute tolerance for distances Returns ------------- split : (n, 2, (3 | 3) float Line segments in space, split at vertices", "response": "def split(segments, points, atol=1e-5):\n    \"\"\"\n    Find any points that lie on a segment (not an endpoint)\n    and then split that segment into two segments.\n\n    We are basically going to find the distance between\n    point and both segment vertex, and see if it is with\n    tolerance of the segment length.\n\n    Parameters\n    --------------\n    segments : (n, 2, (2, 3) float\n      Line segments in space\n    points : (n, (2, 3)) float\n      Points in space\n    atol : float\n      Absolute tolerance for distances\n\n    Returns\n    -------------\n    split : (n, 2, (3 | 3) float\n      Line segments in space, split at vertices\n    \"\"\"\n\n    points = np.asanyarray(points, dtype=np.float64)\n    segments = np.asanyarray(segments, dtype=np.float64)\n    # reshape to a flat 2D (n, dimension) array\n    seg_flat = segments.reshape((-1, segments.shape[2]))\n\n    # find the length of every segment\n    length = ((segments[:, 0, :] -\n               segments[:, 1, :]) ** 2).sum(axis=1) ** 0.5\n\n    # a mask to remove segments we split at the end\n    keep = np.ones(len(segments), dtype=np.bool)\n    # append new segments to a list\n    new_seg = []\n\n    # loop through every point\n    for p in points:\n        # note that you could probably get a speedup\n        # by using scipy.spatial.distance.cdist here\n\n        # find the distance from point to every segment endpoint\n        pair = ((seg_flat - p) ** 2).sum(\n            axis=1).reshape((-1, 2)) ** 0.5\n        # point is on a segment if it is not on a vertex\n        # and the sum length is equal to the actual segment length\n        on_seg = np.logical_and(\n            util.isclose(length, pair.sum(axis=1), atol=atol),\n            ~util.isclose(pair, 0.0, atol=atol).any(axis=1))\n\n        # if we have any points on the segment split it in twain\n        if on_seg.any():\n            # remove the original segment\n            keep = np.logical_and(keep, ~on_seg)\n            # split every segment that this point lies on\n            for seg in segments[on_seg]:\n                new_seg.append([p, seg[0]])\n                new_seg.append([p, seg[1]])\n\n    if len(new_seg) > 0:\n        return np.vstack((segments[keep], new_seg))\n    else:\n        return segments"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unique(segments, digits=5):\n    segments = np.asanyarray(segments, dtype=np.float64)\n\n    # find segments as unique indexes so we can find duplicates\n    inverse = grouping.unique_rows(\n        segments.reshape((-1, segments.shape[2])),\n        digits=digits)[1].reshape((-1, 2))\n\n    # make sure rows are sorted\n    inverse.sort(axis=1)\n    # find rows that occur once\n    index = grouping.unique_rows(inverse)\n    # apply the unique mask\n    unique = segments[index[0]]\n\n    return unique", "response": "Find unique line segments in space."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding the overlap of two parallel line segments.", "response": "def overlap(origins, vectors, params):\n    \"\"\"\n    Find the overlap of two parallel line segments.\n\n    Parameters\n    ------------\n    origins : (2, 3) float\n       Origin points of lines in space\n    vectors : (2, 3) float\n       Unit direction vectors of lines\n    params : (2, 2) float\n       Two (start, end) distance pairs\n\n    Returns\n    ------------\n    length : float\n       Overlapping length\n    overlap : (n, 2, 3) float\n       Line segments for overlapping distance\n    \"\"\"\n    # copy inputs and make sure shape is correct\n    origins = np.array(origins).reshape((2, 3))\n    vectors = np.array(vectors).reshape((2, 3))\n    params = np.array(params).reshape((2, 2))\n\n    if tol.strict:\n        # convert input to parameters before flipping\n        # to make sure we didn't screw it up\n        truth = parameters_to_segments(origins,\n                                       vectors,\n                                       params)\n\n    # this function only works on parallel lines\n    dot = np.dot(*vectors)\n    assert np.isclose(np.abs(dot), 1.0, atol=.01)\n\n    # if two vectors are reversed\n    if dot < 0.0:\n        # reverse direction vector\n        vectors[1] *= -1.0\n        # negate parameters\n        params[1] *= -1.0\n\n    if tol.strict:\n        # do a check to make sure our reversal didn't\n        # inadvertently give us incorrect segments\n        assert np.allclose(truth,\n                           parameters_to_segments(origins,\n                                                  vectors,\n                                                  params))\n\n    # merge the parameter ranges\n    ok, new_range = interval.intersection(*params)\n\n    if not ok:\n        return 0.0, np.array([])\n\n    # create the overlapping segment pairs (2, 2, 3)\n    segments = np.array([o + v * new_range.reshape((-1, 1))\n                         for o, v in zip(origins, vectors)])\n    # get the length of the new range\n    length = new_range.ptp()\n\n    return length, segments"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads a COLLADA file into a list of dict of kwargs for Trimesh constructor.", "response": "def load_collada(file_obj, resolver=None, **kwargs):\n    \"\"\"\n    Load a COLLADA (.dae) file into a list of trimesh kwargs.\n\n    Parameters\n    ----------\n    file_obj : file object\n      Containing a COLLADA file\n    resolver : trimesh.visual.Resolver or None\n      For loading referenced files, like texture images\n    kwargs : **\n      Passed to trimesh.Trimesh.__init__\n\n    Returns\n    -------\n    loaded : list of dict\n      kwargs for Trimesh constructor\n    \"\"\"\n    # load scene using pycollada\n    c = collada.Collada(file_obj)\n\n    # Create material map from Material ID to trimesh material\n    material_map = {}\n    for m in c.materials:\n        effect = m.effect\n        material_map[m.id] = _parse_material(effect, resolver)\n\n    # name : kwargs\n    meshes = {}\n    # list of dict\n    graph = []\n\n    for node in c.scene.nodes:\n        _parse_node(node=node,\n                    parent_matrix=np.eye(4),\n                    material_map=material_map,\n                    meshes=meshes,\n                    graph=graph,\n                    resolver=resolver)\n\n    # create kwargs for load_kwargs\n    result = {'class': 'Scene',\n              'graph': graph,\n              'geometry': meshes}\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef export_collada(mesh, **kwargs):\n    meshes = mesh\n    if not isinstance(mesh, (list, tuple, set, np.ndarray)):\n        meshes = [mesh]\n\n    c = collada.Collada()\n    nodes = []\n    for i, m in enumerate(meshes):\n\n        # Load uv, colors, materials\n        uv = None\n        colors = None\n        mat = _unparse_material(None)\n        if m.visual.defined:\n            if m.visual.kind == 'texture':\n                mat = _unparse_material(m.visual.material)\n                uv = m.visual.uv\n            elif m.visual.kind == 'vertex':\n                colors = (m.visual.vertex_colors / 255.0)[:, :3]\n        c.effects.append(mat.effect)\n        c.materials.append(mat)\n\n        # Create geometry object\n        vertices = collada.source.FloatSource(\n            'verts-array', m.vertices.flatten(), ('X', 'Y', 'Z'))\n        normals = collada.source.FloatSource(\n            'normals-array', m.vertex_normals.flatten(), ('X', 'Y', 'Z'))\n        input_list = collada.source.InputList()\n        input_list.addInput(0, 'VERTEX', '#verts-array')\n        input_list.addInput(1, 'NORMAL', '#normals-array')\n        arrays = [vertices, normals]\n        if uv is not None:\n            texcoords = collada.source.FloatSource(\n                'texcoords-array', uv.flatten(), ('U', 'V'))\n            input_list.addInput(2, 'TEXCOORD', '#texcoords-array')\n            arrays.append(texcoords)\n        if colors is not None:\n            idx = 2\n            if uv:\n                idx = 3\n            colors = collada.source.FloatSource('colors-array',\n                                                colors.flatten(), ('R', 'G', 'B'))\n            input_list.addInput(idx, 'COLOR', '#colors-array')\n            arrays.append(colors)\n        geom = collada.geometry.Geometry(\n            c, uuid.uuid4().hex, uuid.uuid4().hex, arrays\n        )\n        indices = np.repeat(m.faces.flatten(), len(arrays))\n\n        matref = 'material{}'.format(i)\n        triset = geom.createTriangleSet(indices, input_list, matref)\n        geom.primitives.append(triset)\n        c.geometries.append(geom)\n\n        matnode = collada.scene.MaterialNode(matref, mat, inputs=[])\n        geomnode = collada.scene.GeometryNode(geom, [matnode])\n        node = collada.scene.Node('node{}'.format(i), children=[geomnode])\n        nodes.append(node)\n    scene = collada.scene.Scene('scene', nodes)\n    c.scenes.append(scene)\n    c.scene = scene\n\n    b = io.BytesIO()\n    c.write(b)\n    b.seek(0)\n    return b.read()", "response": "Exports a list of Trimesh objects or a list of Trimesh objects as a COLLADA. dae file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_node(node,\n                parent_matrix,\n                material_map,\n                meshes,\n                graph,\n                resolver=None):\n    \"\"\"\n    Recursively parse COLLADA scene nodes.\n    \"\"\"\n\n    # Parse mesh node\n    if isinstance(node, collada.scene.GeometryNode):\n        geometry = node.geometry\n\n        # Create local material map from material symbol to actual material\n        local_material_map = {}\n        for mn in node.materials:\n            symbol = mn.symbol\n            m = mn.target\n            if m.id in material_map:\n                local_material_map[symbol] = material_map[m.id]\n            else:\n                local_material_map[symbol] = _parse_material(m, resolver)\n\n        # Iterate over primitives of geometry\n        for i, primitive in enumerate(geometry.primitives):\n            if isinstance(primitive, collada.polylist.Polylist):\n                primitive = primitive.triangleset()\n            if isinstance(primitive, collada.triangleset.TriangleSet):\n                vertex = primitive.vertex\n                vertex_index = primitive.vertex_index\n                vertices = vertex[vertex_index].reshape(\n                    len(vertex_index) * 3, 3)\n\n                # Get normals if present\n                normals = None\n                if primitive.normal is not None:\n                    normal = primitive.normal\n                    normal_index = primitive.normal_index\n                    normals = normal[normal_index].reshape(\n                        len(normal_index) * 3, 3)\n\n                # Get colors if present\n                colors = None\n                s = primitive.sources\n                if ('COLOR' in s and len(s['COLOR'])\n                        > 0 and len(primitive.index) > 0):\n                    color = s['COLOR'][0][4].data\n                    color_index = primitive.index[:, :, s['COLOR'][0][0]]\n                    colors = color[color_index].reshape(\n                        len(color_index) * 3, 3)\n\n                faces = np.arange(\n                    vertices.shape[0]).reshape(\n                    vertices.shape[0] // 3, 3)\n\n                # Get UV coordinates if possible\n                vis = None\n                if primitive.material in local_material_map:\n                    material = copy.copy(\n                        local_material_map[primitive.material])\n                    uv = None\n                    if len(primitive.texcoordset) > 0:\n                        texcoord = primitive.texcoordset[0]\n                        texcoord_index = primitive.texcoord_indexset[0]\n                        uv = texcoord[texcoord_index].reshape(\n                            (len(texcoord_index) * 3, 2))\n                    vis = visual.texture.TextureVisuals(\n                        uv=uv, material=material)\n\n                primid = '{}.{}'.format(geometry.id, i)\n                meshes[primid] = {\n                    'vertices': vertices,\n                    'faces': faces,\n                    'vertex_normals': normals,\n                    'vertex_colors': colors,\n                    'visual': vis}\n\n                graph.append({'frame_to': primid,\n                              'matrix': parent_matrix,\n                              'geometry': primid})\n\n    # recurse down tree for nodes with children\n    elif isinstance(node, collada.scene.Node):\n        if node.children is not None:\n            for child in node.children:\n                # create the new matrix\n                matrix = np.dot(parent_matrix, node.matrix)\n                # parse the child node\n                _parse_node(\n                    node=child,\n                    parent_matrix=matrix,\n                    material_map=material_map,\n                    meshes=meshes,\n                    graph=graph,\n                    resolver=resolver)\n\n    elif isinstance(node, collada.scene.CameraNode):\n        # TODO: convert collada cameras to trimesh cameras\n        pass\n    elif isinstance(node, collada.scene.LightNode):\n        # TODO: convert collada lights to trimesh lights\n        pass", "response": "Recursively parse a node and return a list of nodes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a texture from a file into a PIL image.", "response": "def _load_texture(file_name, resolver):\n    \"\"\"\n    Load a texture from a file into a PIL image.\n    \"\"\"\n    file_data = resolver.get(file_name)\n    image = PIL.Image.open(util.wrap_as_stream(file_data))\n    return image"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_material(effect, resolver):\n\n    # Compute base color\n    baseColorFactor = np.ones(4)\n    baseColorTexture = None\n    if isinstance(effect.diffuse, collada.material.Map):\n        try:\n            baseColorTexture = _load_texture(\n                effect.diffuse.sampler.surface.image.path, resolver)\n        except BaseException:\n            log.warning('unable to load base texture',\n                        exc_info=True)\n    elif effect.diffuse is not None:\n        baseColorFactor = effect.diffuse\n\n    # Compute emission color\n    emissiveFactor = np.zeros(3)\n    emissiveTexture = None\n    if isinstance(effect.emission, collada.material.Map):\n        try:\n            emissiveTexture = _load_texture(\n                effect.diffuse.sampler.surface.image.path, resolver)\n        except BaseException:\n            log.warning('unable to load emissive texture',\n                        exc_info=True)\n    elif effect.emission is not None:\n        emissiveFactor = effect.emission[:3]\n\n    # Compute roughness\n    roughnessFactor = 1.0\n    if (not isinstance(effect.shininess, collada.material.Map)\n            and effect.shininess is not None):\n        roughnessFactor = np.sqrt(2.0 / (2.0 + effect.shininess))\n\n    # Compute metallic factor\n    metallicFactor = 0.0\n\n    # Compute normal texture\n    normalTexture = None\n    if effect.bumpmap is not None:\n        try:\n            normalTexture = _load_texture(\n                effect.bumpmap.sampler.surface.image.path, resolver)\n        except BaseException:\n            log.warning('unable to load bumpmap',\n                        exc_info=True)\n\n    return visual.texture.PBRMaterial(\n        emissiveFactor=emissiveFactor,\n        emissiveTexture=emissiveTexture,\n        normalTexture=normalTexture,\n        baseColorTexture=baseColorTexture,\n        baseColorFactor=baseColorFactor,\n        metallicFactor=metallicFactor,\n        roughnessFactor=roughnessFactor\n    )", "response": "Parses a COLLADA effect into a trimesh material."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _unparse_material(material):\n    # TODO EXPORT TEXTURES\n    if isinstance(material, visual.texture.PBRMaterial):\n        diffuse = material.baseColorFactor\n        if diffuse is not None:\n            diffuse = list(diffuse)\n\n        emission = material.emissiveFactor\n        if emission is not None:\n            emission = [float(emission[0]), float(emission[1]),\n                        float(emission[2]), 1.0]\n\n        shininess = material.roughnessFactor\n        if shininess is not None:\n            shininess = 2.0 / shininess**2 - 2.0\n\n        effect = collada.material.Effect(\n            uuid.uuid4().hex, params=[], shadingtype='phong',\n            diffuse=diffuse, emission=emission,\n            specular=[1.0, 1.0, 1.0], shininess=float(shininess)\n        )\n        material = collada.material.Material(\n            uuid.uuid4().hex, 'pbrmaterial', effect\n        )\n    else:\n        effect = collada.material.Effect(\n            uuid.uuid4().hex, params=[], shadingtype='phong'\n        )\n        material = collada.material.Material(\n            uuid.uuid4().hex, 'defaultmaterial', effect\n        )\n    return material", "response": "Turn a trimesh material into a COLLADA material."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_zae(file_obj, resolver=None, **kwargs):\n\n    # a dict, {file name : file object}\n    archive = util.decompress(file_obj,\n                              file_type='zip')\n\n    # load the first file with a .dae extension\n    file_name = next(i for i in archive.keys()\n                     if i.lower().endswith('.dae'))\n\n    # a resolver so the loader can load textures / etc\n    resolver = visual.resolvers.ZipResolver(archive)\n\n    # run the regular collada loader\n    loaded = load_collada(archive[file_name],\n                          resolver=resolver,\n                          **kwargs)\n    return loaded", "response": "Loads a ZAE file which is just a zipped DAE file."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload an OFF file into kwargs for a Trimesh constructor.", "response": "def load_off(file_obj, **kwargs):\n    \"\"\"\n    Load an OFF file into the kwargs for a Trimesh constructor\n\n\n    Parameters\n    ----------\n    file_obj : file object\n                 Contains an OFF file\n\n    Returns\n    ----------\n    loaded : dict\n              kwargs for Trimesh constructor\n    \"\"\"\n    header_string = file_obj.readline()\n    if hasattr(header_string, 'decode'):\n        header_string = header_string.decode('utf-8')\n    header_string = header_string.strip().upper()\n\n    if not header_string == 'OFF':\n        raise NameError('Not an OFF file! Header was ' +\n                        header_string)\n\n    header = np.array(\n        file_obj.readline().strip().split()).astype(np.int64)\n    vertex_count, face_count = header[:2]\n\n    # read the rest of the file\n    blob = np.array(file_obj.read().strip().split())\n    # there should be 3 points per vertex\n    # and 3 indexes + 1 count per face\n    data_ok = np.sum(header * [3, 4, 0]) == len(blob)\n    if not data_ok:\n        raise NameError('Incorrect number of vertices or faces!')\n\n    vertices = blob[:(vertex_count * 3)].astype(\n        np.float64).reshape((-1, 3))\n    # strip the first column which is a per- face count\n    faces = blob[(vertex_count * 3):].astype(\n        np.int64).reshape((-1, 4))[:, 1:]\n\n    kwargs = {'vertices': vertices,\n              'faces': faces}\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_msgpack(blob, **kwargs):\n\n    import msgpack\n    if hasattr(blob, 'read'):\n        data = msgpack.load(blob)\n    else:\n        data = msgpack.loads(blob)\n    loaded = load_dict(data)\n    return loaded", "response": "Load a dict packed with msgpack into kwargs for the Trimesh constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_dict(data, **kwargs):\n    if data is None:\n        raise ValueError('data passed to load_dict was None!')\n    if util.is_instance_named(data, 'Trimesh'):\n        return data\n    if util.is_string(data):\n        if '{' not in data:\n            raise ValueError('Object is not a JSON encoded dictionary!')\n        data = json.loads(data.decode('utf-8'))\n    elif util.is_file(data):\n        data = json.load(data)\n\n    # what shape should the data be to be usable\n    mesh_data = {'vertices': (-1, 3),\n                 'faces': (-1, (3, 4)),\n                 'face_normals': (-1, 3),\n                 'face_colors': (-1, (3, 4)),\n                 'vertex_normals': (-1, 3),\n                 'vertex_colors': (-1, (3, 4))}\n\n    # now go through data structure and if anything is encoded as base64\n    # pull it back into numpy arrays\n    if isinstance(data, dict):\n        loaded = {}\n        data = util.decode_keys(data, 'utf-8')\n        for key, shape in mesh_data.items():\n            if key in data:\n                loaded[key] = util.encoded_to_array(data[key])\n                if not util.is_shape(loaded[key], shape):\n                    raise ValueError('Shape of %s is %s, not %s!',\n                                     key,\n                                     str(loaded[key].shape),\n                                     str(shape))\n        if len(key) == 0:\n            raise ValueError('Unable to extract any mesh data!')\n        return loaded\n    else:\n        raise ValueError('%s object passed to dict loader!',\n                         data.__class__.__name__)", "response": "Load a Trimesh object from a json encoded dict."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef discretize_bezier(points,\n                      count=None,\n                      scale=1.0):\n    \"\"\"\n    Parameters\n    ----------\n    points : (order, dimension) float\n      Control points of the bezier curve\n      For a 2D cubic bezier, order=3, dimension=2\n    count : int, or None\n      Number of segments\n    scale : float\n      Scale of curve\n    Returns\n    ----------\n    discrete: (n,d) list of points, a polyline representation\n              of the bezier curve which respects constants.RES_LENGTH\n    \"\"\"\n    # make sure we have a numpy array\n    points = np.asanyarray(points, dtype=np.float64)\n\n    if count is None:\n        # how much distance does a small percentage of the curve take\n        # this is so we can figure out how finely we have to sample t\n        norm = np.linalg.norm(np.diff(points, axis=0), axis=1).sum()\n        count = np.ceil(norm / (res.seg_frac * scale))\n        count = int(np.clip(count,\n                            res.min_sections * len(points),\n                            res.max_sections * len(points)))\n    count = int(count)\n\n    # parameterize incrementing 0.0 - 1.0\n    t = np.linspace(0.0, 1.0, count)\n    # decrementing 1.0-0.0\n    t_d = 1.0 - t\n    n = len(points) - 1\n    # binomial coefficients, i, and each point\n    iterable = zip(binomial(n), np.arange(len(points)), points)\n    # run the actual interpolation\n    stacked = [((t**i) * (t_d**(n - i))).reshape((-1, 1))\n               * p * c for c, i, p in iterable]\n    result = np.sum(stacked, axis=0)\n\n    # test to make sure end points are correct\n    test = np.sum((result[[0, -1]] - points[[0, -1]])**2, axis=1)\n    assert (test < tol.merge).all()\n    assert len(result) >= 2\n\n    return result", "response": "Discretizes a set of points into a single segment."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a control points and knot vector return a sampled version of the curve.", "response": "def discretize_bspline(control,\n                       knots,\n                       count=None,\n                       scale=1.0):\n    \"\"\"\n    Given a B-Splines control points and knot vector, return\n    a sampled version of the curve.\n\n    Parameters\n    ----------\n    control : (o, d) float\n      Control points of the b- spline\n    knots : (j,) float\n      B-spline knots\n    count : int\n      Number of line segments to discretize the spline\n      If not specified will be calculated as something reasonable\n\n    Returns\n    ----------\n    discrete : (count, dimension) float\n       Points on a polyline version of the B-spline\n    \"\"\"\n\n    # evaluate the b-spline using scipy/fitpack\n    from scipy.interpolate import splev\n    # (n, d) control points where d is the dimension of vertices\n    control = np.asanyarray(control, dtype=np.float64)\n    degree = len(knots) - len(control) - 1\n    if count is None:\n        norm = np.linalg.norm(np.diff(control, axis=0), axis=1).sum()\n        count = int(np.clip(norm / (res.seg_frac * scale),\n                            res.min_sections * len(control),\n                            res.max_sections * len(control)))\n\n    ipl = np.linspace(knots[0], knots[-1], count)\n    discrete = splev(ipl, [knots, control.T, degree])\n    discrete = np.column_stack(discrete)\n\n    return discrete"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef binomial(n):\n    if n == 1:\n        return [1, 1]\n    elif n == 2:\n        return [1, 2, 1]\n    elif n == 3:\n        return [1, 3, 3, 1]\n    elif n == 4:\n        return [1, 4, 6, 4, 1]\n    elif n == 5:\n        return [1, 5, 10, 10, 5, 1]\n    else:\n        from scipy.special import binom\n        return binom(n, np.arange(n + 1))", "response": "Returns a list of binomial coefficients for a given order."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the drawing and return the Path object.", "response": "def process(self):\n        \"\"\"\n        Apply basic cleaning functions to the Path object, in- place.\n        \"\"\"\n        log.debug('Processing drawing')\n        with self._cache:\n            for func in self._process_functions():\n                func()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of layers defined in the current session.", "response": "def layers(self):\n        \"\"\"\n        If entities have a layer defined, return it.\n\n        Returns\n        ---------\n        layers: (len(entities), ) list of str\n        \"\"\"\n        layer = ['NONE'] * len(self.entities)\n        for i, e in enumerate(self.entities):\n            if hasattr(e, 'layer'):\n                layer[i] = str(e.layer)\n        return layer"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef crc(self):\n        # first CRC the points in every entity\n        target = caching.crc32(bytes().join(e._bytes()\n                                            for e in self.entities))\n        # add the CRC for the vertices\n        target ^= self.vertices.crc()\n        return target", "response": "A CRC of the current vertices and entities."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the MD5 hash of the current vertices and entities.", "response": "def md5(self):\n        \"\"\"\n        An MD5 hash of the current vertices and entities.\n\n        Returns\n        ------------\n        md5: str, two appended MD5 hashes\n        \"\"\"\n        # first MD5 the points in every entity\n        target = '{}{}'.format(\n            util.md5_object(bytes().join(e._bytes()\n                                         for e in self.entities)),\n            self.vertices.md5())\n\n        return target"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef paths(self):\n        paths = traversal.closed_paths(self.entities,\n                                       self.vertices)\n        return paths", "response": "Sequence of closed paths encoded by entity index."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dangling(self):\n        if len(self.paths) == 0:\n            return np.arange(len(self.entities))\n        else:\n            included = np.hstack(self.paths)\n        dangling = np.setdiff1d(np.arange(len(self.entities)),\n                                included)\n        return dangling", "response": "Returns the list of entities that aren t included in a closed path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the scale of the species in the world holding this path.", "response": "def scale(self):\n        \"\"\"\n        What is a representitive number that reflects the magnitude\n        of the world holding the paths, for numerical comparisons.\n\n        Returns\n        ----------\n        scale : float\n            Approximate size of the world holding this path\n        \"\"\"\n        # use vertices peak-peak rather than exact extents\n        scale = float((self.vertices.ptp(axis=0) ** 2).sum() ** .5)\n        return scale"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the axis aligned bounding box of the current path.", "response": "def bounds(self):\n        \"\"\"\n        Return the axis aligned bounding box of the current path.\n\n        Returns\n        ----------\n        bounds: (2, dimension) float, (min, max) coordinates\n        \"\"\"\n        # get the exact bounds of each entity\n        # some entities (aka 3- point Arc) have bounds that can't\n        # be generated from just bound box of vertices\n        points = np.array([e.bounds(self.vertices)\n                           for e in self.entities],\n                          dtype=np.float64)\n        # flatten bound extrema into (n, dimension) array\n        points = points.reshape((-1, self.vertices.shape[1]))\n        # get the max and min of all bounds\n        bounds = np.array([points.min(axis=0),\n                           points.max(axis=0)],\n                          dtype=np.float64)\n\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef explode(self):\n        new_entities = collections.deque()\n        for entity in self.entities:\n            new_entities.extend(entity.explode())\n        self.entities = np.array(new_entities)", "response": "Turn every multi - segment entity into single segment entities in - place."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_closed(self):\n        closed = all(i == 2 for i in\n                     dict(self.vertex_graph.degree()).values())\n\n        return closed", "response": "Returns True if all entities connected to other entities are closed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vertex_nodes(self):\n        nodes = np.vstack([e.nodes for e in self.entities])\n        return nodes", "response": "Get a list of which vertex indices are nodes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apply_transform(self, transform):\n        dimension = self.vertices.shape[1]\n        transform = np.asanyarray(transform, dtype=np.float64)\n\n        if transform.shape != (dimension + 1, dimension + 1):\n            raise ValueError('transform is incorrect shape!')\n        elif np.abs(transform - np.eye(dimension + 1)).max() < 1e-8:\n            # if we've been passed an identity matrix do nothing\n            return\n\n        # make sure cache is up to date\n        self._cache.verify()\n        # new cache to transfer items\n        cache = {}\n        # apply transform to discretized paths\n        if 'discrete' in self._cache.cache:\n            cache['discrete'] = np.array([\n                transformations.transform_points(\n                    d, matrix=transform)\n                for d in self.discrete])\n\n        # things we can just straight up copy\n        # as they are topological not geometric\n        for key in ['root',\n                    'paths',\n                    'path_valid',\n                    'dangling',\n                    'vertex_graph',\n                    'enclosure',\n                    'enclosure_shell',\n                    'enclosure_directed']:\n            # if they're in cache save them from the purge\n            if key in self._cache.cache:\n                cache[key] = self._cache.cache[key]\n\n        # transform vertices in place\n        self.vertices = transformations.transform_points(\n            self.vertices,\n            matrix=transform)\n        # explicitly clear the cache\n        self._cache.clear()\n        self._cache.id_set()\n\n        # populate the things we wangled\n        self._cache.cache.update(cache)", "response": "Apply a transformation matrix to the current path in - place and return the new path in - place."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef apply_scale(self, scale):\n        dimension = self.vertices.shape[1]\n        matrix = np.eye(dimension + 1)\n        matrix[:dimension, :dimension] *= scale\n        self.apply_transform(matrix)", "response": "Apply a transformation matrix to the current path in - place."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying a homogenous transformation matrix to the current path in - place.", "response": "def apply_translation(self, offset):\n        \"\"\"\n        Apply a transformation matrix to the current path in- place\n\n        Parameters\n        -----------\n        offset : float or (3,) float\n          Translation to be applied to mesh\n        \"\"\"\n        # work on 2D and 3D paths\n        dimension = self.vertices.shape[1]\n        # make sure offset is correct length and type\n        offset = np.array(\n            offset, dtype=np.float64).reshape(dimension)\n        # create a homogenous transform\n        matrix = np.eye(dimension + 1)\n        # apply the offset\n        matrix[:dimension, dimension] = offset\n\n        self.apply_transform(matrix)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_vertices(self, digits=None):\n        if len(self.vertices) == 0:\n            return\n        if digits is None:\n            digits = util.decimal_to_digits(tol.merge * self.scale,\n                                            min_digits=1)\n\n        unique, inverse = grouping.unique_rows(self.vertices,\n                                               digits=digits)\n        self.vertices = self.vertices[unique]\n\n        entities_ok = np.ones(len(self.entities), dtype=np.bool)\n\n        for index, entity in enumerate(self.entities):\n            # what kind of entity are we dealing with\n            kind = type(entity).__name__\n\n            # entities that don't need runs merged\n            # don't screw up control- point- knot relationship\n            if kind in 'BSpline Bezier Text':\n                entity.points = inverse[entity.points]\n                continue\n            # if we merged duplicate vertices, the entity may\n            # have multiple references to the same vertex\n            points = grouping.merge_runs(inverse[entity.points])\n            # if there are three points and two are identical fix it\n            if kind == 'Line':\n                if len(points) == 3 and points[0] == points[-1]:\n                    points = points[:2]\n                elif len(points) < 2:\n                    # lines need two or more vertices\n                    entities_ok[index] = False\n            elif kind == 'Arc' and len(points) != 3:\n                # three point arcs need three points\n                entities_ok[index] = False\n\n            # store points in entity\n            entity.points = points\n\n        # remove degenerate entities\n        self.entities = self.entities[entities_ok]", "response": "Merges the vertices which are identical and replace references."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace the vertex index references in every entity in the entity with the new ones.", "response": "def replace_vertex_references(self, mask):\n        \"\"\"\n        Replace the vertex index references in every entity.\n\n        Parameters\n        ------------\n        mask : (len(self.vertices), ) int\n          Contains new vertex indexes\n\n        Alters\n        ------------\n        entity.points in self.entities\n          Replaced by mask[entity.points]\n        \"\"\"\n        for entity in self.entities:\n            entity.points = mask[entity.points]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove entities by index.", "response": "def remove_entities(self, entity_ids):\n        \"\"\"\n        Remove entities by index.\n\n        Parameters\n        -----------\n        entity_ids : (n,) int\n          Indexes of self.entities to remove\n        \"\"\"\n        if len(entity_ids) == 0:\n            return\n        keep = np.ones(len(self.entities))\n        keep[entity_ids] = False\n        self.entities = self.entities[keep]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves entities which declare themselves invalid.", "response": "def remove_invalid(self):\n        \"\"\"\n        Remove entities which declare themselves invalid\n\n        Alters\n        ----------\n        self.entities: shortened\n        \"\"\"\n        valid = np.array([i.is_valid for i in self.entities],\n                         dtype=np.bool)\n        self.entities = self.entities[valid]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving entities that are duplicated by the same entity.", "response": "def remove_duplicate_entities(self):\n        \"\"\"\n        Remove entities that are duplicated\n\n        Alters\n        -------\n        self.entities: length same or shorter\n        \"\"\"\n        entity_hashes = np.array([hash(i) for i in self.entities])\n        unique, inverse = grouping.unique_rows(entity_hashes)\n        if len(unique) != len(self.entities):\n            self.entities = self.entities[unique]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the vertices that are referenced by an entity.", "response": "def referenced_vertices(self):\n        \"\"\"\n        Which vertices are referenced by an entity.\n\n        Returns\n        -----------\n        referenced_vertices: (n,) int, indexes of self.vertices\n        \"\"\"\n        # no entities no reference\n        if len(self.entities) == 0:\n            return np.array([], dtype=np.int64)\n        referenced = np.concatenate([e.points for e in self.entities])\n        referenced = np.unique(referenced.astype(np.int64))\n\n        return referenced"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_unreferenced_vertices(self):\n\n        unique = self.referenced_vertices\n\n        mask = np.ones(len(self.vertices), dtype=np.int64) * -1\n        mask[unique] = np.arange(len(unique), dtype=np.int64)\n\n        self.replace_vertex_references(mask=mask)\n        self.vertices = self.vertices[unique]", "response": "Removes all vertices which aren t used by an entity."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of entities return a list of connected points.", "response": "def discretize_path(self, path):\n        \"\"\"\n        Given a list of entities, return a list of connected points.\n\n        Parameters\n        -----------\n        path: (n,) int, indexes of self.entities\n\n        Returns\n        -----------\n        discrete: (m, dimension)\n        \"\"\"\n        discrete = traversal.discretize_path(self.entities,\n                                             self.vertices,\n                                             path,\n                                             scale=self.scale)\n        return discrete"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef discrete(self):\n        discrete = np.array([self.discretize_path(i)\n                             for i in self.paths])\n        return discrete", "response": "Returns a sequence of connected vertices in space corresponding to\n        self. paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef export(self,\n               file_obj=None,\n               file_type=None,\n               **kwargs):\n        \"\"\"\n        Export the path to a file object or return data.\n\n        Parameters\n        ---------------\n        file_obj : None, str, or file object\n          File object or string to export to\n        file_type : None or str\n          Type of file: dxf, dict, svg\n\n        Returns\n        ---------------\n        exported : bytes or str\n          Exported as specified type\n        \"\"\"\n        return export_path(self,\n                           file_type=file_type,\n                           file_obj=file_obj,\n                           **kwargs)", "response": "Returns the path to a file object or return data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy(self):\n\n        metadata = {}\n        # grab all the keys into a list so if something is added\n        # in another thread it probably doesn't stomp on our loop\n        for key in list(self.metadata.keys()):\n            try:\n                metadata[key] = copy.deepcopy(self.metadata[key])\n            except RuntimeError:\n                # multiple threads\n                log.warning('key {} changed during copy'.format(key))\n\n        # copy the core data\n        copied = type(self)(entities=copy.deepcopy(self.entities),\n                            vertices=copy.deepcopy(self.vertices),\n                            metadata=metadata)\n\n        cache = {}\n        # try to copy the cache over to the new object\n        try:\n            # save dict keys before doing slow iteration\n            keys = list(self._cache.cache.keys())\n            # run through each key and copy into new cache\n            for k in keys:\n                cache[k] = copy.deepcopy(self._cache.cache[k])\n        except RuntimeError:\n            # if we have multiple threads this may error and is NBD\n            log.debug('unable to copy cache')\n        except BaseException:\n            # catch and log errors we weren't expecting\n            log.error('unable to copy cache', exc_info=True)\n        copied._cache.cache = cache\n        copied._cache.id_set()\n\n        return copied", "response": "Returns a copy of the current mesh"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_planar(self,\n                  to_2D=None,\n                  normal=None,\n                  check=True):\n        \"\"\"\n        Check to see if current vectors are all coplanar.\n\n        If they are, return a Path2D and a transform which will\n        transform the 2D representation back into 3 dimensions\n\n        Parameters\n        -----------\n        to_2D: (4,4) float\n            Homogenous transformation matrix to apply,\n            If not passed a plane will be fitted to vertices.\n        normal: (3,) float, or None\n           Approximate normal of direction of plane\n           If to_2D is not specified sign\n           will be applied to fit plane normal\n        check:  bool\n            If True: Raise a ValueError if\n            points aren't coplanar\n\n        Returns\n        -----------\n        planar : trimesh.path.Path2D\n                   Current path transformed onto plane\n        to_3D :  (4,4) float\n                   Homeogenous transformation to move planar\n                   back into 3D space\n        \"\"\"\n        # which vertices are actually referenced\n        referenced = self.referenced_vertices\n        # if nothing is referenced return an empty path\n        if len(referenced) == 0:\n            return Path2D(), np.eye(4)\n\n        # no explicit transform passed\n        if to_2D is None:\n            # fit a plane to our vertices\n            C, N = plane_fit(self.vertices[referenced])\n            # apply the normal sign hint\n            if normal is not None:\n                normal = np.asanyarray(normal, dtype=np.float64)\n                if normal.shape == (3,):\n                    N *= np.sign(np.dot(N, normal))\n                    N = normal\n                else:\n                    log.warning(\n                        \"passed normal not used: {}\".format(\n                            normal.shape))\n            # create a transform from fit plane to XY\n            to_2D = plane_transform(origin=C,\n                                    normal=N)\n\n        # make sure we've extracted a transform\n        to_2D = np.asanyarray(to_2D, dtype=np.float64)\n        if to_2D.shape != (4, 4):\n            raise ValueError('unable to create transform!')\n\n        # transform all vertices to 2D plane\n        flat = transformations.transform_points(self.vertices,\n                                                to_2D)\n\n        # Z values of vertices which are referenced\n        heights = flat[referenced][:, 2]\n        # points are not on a plane because Z varies\n        if heights.ptp() > tol.planar:\n            # since Z is inconsistent set height to zero\n            height = 0.0\n            if check:\n                raise ValueError('points are not flat!')\n        else:\n            # if the points were planar store the height\n            height = heights.mean()\n\n        # the transform from 2D to 3D\n        to_3D = np.linalg.inv(to_2D)\n\n        # if the transform didn't move the path to\n        # exactly Z=0 adjust it so the returned transform does\n        if np.abs(height) > tol.planar:\n            # adjust to_3D transform by height\n            adjust = transformations.translation_matrix(\n                [0, 0, height])\n            # apply the height adjustment to_3D\n            to_3D = np.dot(to_3D, adjust)\n\n        # copy metadata to new object\n        metadata = copy.deepcopy(self.metadata)\n        # store transform we used to move it onto the plane\n        metadata['to_3D'] = to_3D\n\n        # create the Path2D with the same entities\n        # and XY values of vertices projected onto the plane\n        planar = Path2D(entities=copy.deepcopy(self.entities),\n                        vertices=flat[:, :2],\n                        metadata=metadata,\n                        process=False)\n\n        return planar, to_3D", "response": "Return a Path2D and a transform which will be applied to the 2D representation of the current path."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_discrete(self, show=False):\n        import matplotlib.pyplot as plt\n        from mpl_toolkits.mplot3d import Axes3D  # NOQA\n        fig = plt.figure()\n        axis = fig.add_subplot(111, projection='3d')\n        for discrete in self.discrete:\n            axis.plot(*discrete.T)\n        if show:\n            plt.show()", "response": "Plot the closed curves of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nplotting discrete version of entities without regards for connectivity.", "response": "def plot_entities(self, show=False):\n        \"\"\"\n        Plot discrete version of entities without regards\n        for connectivity.\n\n        Parameters\n        -------------\n        show : bool\n           If False will not execute matplotlib.pyplot.show\n        \"\"\"\n        import matplotlib.pyplot as plt\n        from mpl_toolkits.mplot3d import Axes3D  # NOQA\n        fig = plt.figure()\n        axis = fig.add_subplot(111, projection='3d')\n        for entity in self.entities:\n            vertices = entity.discrete(self.vertices)\n            axis.plot(*vertices.T)\n        if show:\n            plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show(self, annotations=True):\n        if self.is_closed:\n            self.plot_discrete(show=True, annotations=annotations)\n        else:\n            self.plot_entities(show=True, annotations=annotations)", "response": "Plot the current Path2D object using matplotlib."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransforms the current path so that its OBB is axis aligned and its center is at the origin.", "response": "def apply_obb(self):\n        \"\"\"\n        Transform the current path so that its OBB is axis aligned\n        and OBB center is at the origin.\n        \"\"\"\n        if len(self.root) == 1:\n            matrix, bounds = polygons.polygon_obb(\n                self.polygons_closed[self.root[0]])\n            self.apply_transform(matrix)\n            return matrix\n        else:\n            raise ValueError('Not implemented for multibody geometry')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing rejection sampling to generate random points inside a single polygon.", "response": "def sample(self, count, **kwargs):\n        \"\"\"\n        Use rejection sampling to generate random points inside a\n        polygon.\n\n        Parameters\n        -----------\n        count   : int\n                    Number of points to return\n                    If there are multiple bodies, there will\n                    be up to count * bodies points returned\n        factor  : float\n                    How many points to test per loop\n                    IE, count * factor\n        max_iter : int,\n                    Maximum number of intersection loops\n                    to run, total points sampled is\n                    count * factor * max_iter\n\n        Returns\n        -----------\n        hit : (n, 2) float\n               Random points inside polygon\n        \"\"\"\n\n        poly = self.polygons_full\n        if len(poly) == 0:\n            samples = np.array([])\n        elif len(poly) == 1:\n            samples = polygons.sample(poly[0], count=count, **kwargs)\n        else:\n            samples = util.vstack_empty([\n                polygons.sample(i, count=count, **kwargs)\n                for i in poly])\n\n        return samples"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the current path to 3D path on the XY plane.", "response": "def to_3D(self, transform=None):\n        \"\"\"\n        Convert 2D path to 3D path on the XY plane.\n\n        Parameters\n        -------------\n        transform : (4, 4) float\n            If passed, will transform vertices.\n            If not passed and 'to_3D' is in metadata\n            that transform will be used.\n\n        Returns\n        -----------\n        path_3D: Path3D version of current path\n        \"\"\"\n        # if there is a stored 'to_3D' transform in metadata use it\n        if transform is None and 'to_3D' in self.metadata:\n            transform = self.metadata['to_3D']\n\n        # copy vertices and stack with zeros from (n, 2) to (n, 3)\n        vertices = np.column_stack((copy.deepcopy(self.vertices),\n                                    np.zeros(len(self.vertices))))\n        if transform is not None:\n            vertices = transformations.transform_points(vertices,\n                                                        transform)\n        # make sure everything is deep copied\n        path_3D = Path3D(entities=copy.deepcopy(self.entities),\n                         vertices=vertices,\n                         metadata=copy.deepcopy(self.metadata))\n        return path_3D"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef polygons_full(self):\n        # pre- allocate the list to avoid indexing problems\n        full = [None] * len(self.root)\n        # store the graph to avoid cache thrashing\n        enclosure = self.enclosure_directed\n        # store closed polygons to avoid cache hits\n        closed = self.polygons_closed\n\n        # loop through root curves\n        for i, root in enumerate(self.root):\n            # a list of multiple Polygon objects that\n            # are fully contained by the root curve\n            children = [closed[child]\n                        for child in enclosure[root].keys()]\n            # all polygons_closed are CCW, so for interiors reverse them\n            holes = [np.array(p.exterior.coords)[::-1]\n                     for p in children]\n            # a single Polygon object\n            shell = closed[root].exterior\n            # create a polygon with interiors\n            full[i] = polygons.repair_invalid(Polygon(shell=shell,\n                                                      holes=holes))\n        # so we can use advanced indexing\n        full = np.array(full)\n\n        return full", "response": "Returns a list of shapely. geometry. Polygon objects with interiors created by checking which closed polygons enclose which other polygons."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef area(self):\n        area = float(sum(i.area for i in self.polygons_full))\n        return area", "response": "Returns the area of the polygons interior."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the total discretized length of every entity.", "response": "def length(self):\n        \"\"\"\n        The total discretized length of every entity.\n\n        Returns\n        --------\n        length: float, summed length of every entity\n        \"\"\"\n        length = float(sum(i.length(self.vertices)\n                           for i in self.entities))\n        return length"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextruding the current 2D path into a 3D mesh.", "response": "def extrude(self, height, **kwargs):\n        \"\"\"\n        Extrude the current 2D path into a 3D mesh.\n\n        Parameters\n        ----------\n        height: float, how far to extrude the profile\n        kwargs: passed directly to meshpy.triangle.build:\n                triangle.build(mesh_info,\n                               verbose=False,\n                               refinement_func=None,\n                               attributes=False,\n                               volume_constraints=True,\n                               max_volume=None,\n                               allow_boundary_steiner=True,\n                               allow_volume_steiner=True,\n                               quality_meshing=True,\n                               generate_edges=None,\n                               generate_faces=False,\n                               min_angle=None)\n        Returns\n        --------\n        mesh: trimesh object representing extruded polygon\n        \"\"\"\n        from ..primitives import Extrusion\n        result = [Extrusion(polygon=i, height=height, **kwargs)\n                  for i in self.polygons_full]\n        if len(result) == 1:\n            return result[0]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef triangulate(self, **kwargs):\n        from ..creation import triangulate_polygon\n\n        # append vertices and faces into sequence\n        v_seq = []\n        f_seq = []\n\n        # loop through polygons with interiors\n        for polygon in self.polygons_full:\n            v, f = triangulate_polygon(polygon, **kwargs)\n            v_seq.append(v)\n            f_seq.append(f)\n\n        return util.append_faces(v_seq, f_seq)", "response": "Create a region - aware triangulation of the 2D path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef medial_axis(self, resolution=None, clip=None):\n        if resolution is None:\n            resolution = self.scale / 1000.0\n\n        # convert the edges to Path2D kwargs\n        from .exchange.misc import edges_to_path\n\n        # edges and vertices\n        edge_vert = [polygons.medial_axis(i, resolution, clip)\n                     for i in self.polygons_full]\n        # create a Path2D object for each region\n        medials = [Path2D(**edges_to_path(\n            edges=e, vertices=v)) for e, v in edge_vert]\n\n        # get a single Path2D of medial axis\n        medial = concatenate(medials)\n\n        return medial", "response": "Find the approximate medial axis based on a voronoi diagram of evenly spaced points on the boundary of the polygon."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives an index of self. paths find other paths which overlap with that path.", "response": "def connected_paths(self, path_id, include_self=False):\n        \"\"\"\n        Given an index of self.paths find other paths which\n        overlap with that path.\n\n        Parameters\n        -----------\n        path_id : int\n          Index of self.paths\n        include_self : bool\n          Should the result include path_id or not\n\n        Returns\n        -----------\n        path_ids :  (n, ) int\n          Indexes of self.paths that overlap input path_id\n        \"\"\"\n        if len(self.root) == 1:\n            path_ids = np.arange(len(self.polygons_closed))\n        else:\n            path_ids = list(nx.node_connected_component(\n                self.enclosure,\n                path_id))\n        if include_self:\n            return np.array(path_ids)\n        return np.setdiff1d(path_ids, [path_id])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef simplify_spline(self, path_indexes=None, smooth=.0002):\n        return simplify.simplify_spline(self,\n                                        path_indexes=path_indexes,\n                                        smooth=smooth)", "response": "Simplify a path2D object into a Path2D object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plot_discrete(self, show=False, annotations=True):\n        import matplotlib.pyplot as plt\n        axis = plt.axes()\n        axis.set_aspect('equal', 'datalim')\n\n        for i, points in enumerate(self.discrete):\n            color = ['g', 'k'][i in self.root]\n            axis.plot(*points.T, color=color)\n\n        if annotations:\n            for e in self.entities:\n                if not hasattr(e, 'plot'):\n                    continue\n                e.plot(self.vertices)\n\n        if show:\n            plt.show()\n        return axis", "response": "Plot the closed curves of the path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_entities(self, show=False, annotations=True, color=None):\n        import matplotlib.pyplot as plt\n        plt.axes().set_aspect('equal', 'datalim')\n        eformat = {'Line0': {'color': 'g', 'linewidth': 1},\n                   'Line1': {'color': 'y', 'linewidth': 1},\n                   'Arc0': {'color': 'r', 'linewidth': 1},\n                   'Arc1': {'color': 'b', 'linewidth': 1},\n                   'Bezier0': {'color': 'k', 'linewidth': 1},\n                   'Bezier1': {'color': 'k', 'linewidth': 1},\n                   'BSpline0': {'color': 'm', 'linewidth': 1},\n                   'BSpline1': {'color': 'm', 'linewidth': 1}}\n        for entity in self.entities:\n            if annotations and hasattr(entity, 'plot'):\n                entity.plot(self.vertices)\n                continue\n            discrete = entity.discrete(self.vertices)\n            e_key = entity.__class__.__name__ + str(int(entity.closed))\n            fmt = eformat[e_key]\n            if color is not None:\n                # passed color will override other optons\n                fmt['color'] = color\n            elif hasattr(entity, 'color'):\n                # if entity has specified color use it\n                fmt['color'] = entity.color\n            plt.plot(*discrete.T, **fmt)\n        if show:\n            plt.show()", "response": "Plot the entities of the path with no notion of topology."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef identifier(self):\n        if len(self.polygons_full) != 1:\n            raise TypeError('Identifier only valid for single body')\n        return polygons.polygon_hash(self.polygons_full[0])", "response": "A unique identifier for the path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an MD5 of the identifier", "response": "def identifier_md5(self):\n        \"\"\"\n        Return an MD5 of the identifier\n        \"\"\"\n        as_int = (self.identifier * 1e4).astype(np.int64)\n        hashed = util.md5_object(as_int.tostring(order='C'))\n        return hashed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of arrays where each element in the array is True if the path is valid.", "response": "def path_valid(self):\n        \"\"\"\n        Returns\n        ----------\n        path_valid: (n,) bool, indexes of self.paths self.polygons_closed\n                         which are valid polygons\n        \"\"\"\n        valid = [i is not None for i in self.polygons_closed]\n        valid = np.array(valid, dtype=np.bool)\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Networkx DiGraph of the closed polygons.", "response": "def enclosure_directed(self):\n        \"\"\"\n        Networkx DiGraph of polygon enclosure\n        \"\"\"\n        root, enclosure = polygons.enclosure_tree(self.polygons_closed)\n        self._cache['root'] = root\n        return enclosure"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef log_time(method):\n\n    def timed(*args, **kwargs):\n        tic = time_function()\n        result = method(*args, **kwargs)\n        log.debug('%s executed in %.4f seconds.',\n                  method.__name__,\n                  time_function() - tic)\n        return result\n    timed.__name__ = method.__name__\n    timed.__doc__ = method.__doc__\n    return timed", "response": "A decorator for methods which will time the method and emit a log. debug message with the method name and how long it took to execute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nearby_faces(mesh, points):\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    # an r-tree containing the axis aligned bounding box for every triangle\n    rtree = mesh.triangles_tree\n    # a kd-tree containing every vertex of the mesh\n    kdtree = mesh.kdtree\n\n    # query the distance to the nearest vertex to get AABB of a sphere\n    distance_vertex = kdtree.query(points)[0].reshape((-1, 1))\n    distance_vertex += tol.merge\n\n    # axis aligned bounds\n    bounds = np.column_stack((points - distance_vertex,\n                              points + distance_vertex))\n\n    # faces that intersect axis aligned bounding box\n    candidates = [list(rtree.intersection(b)) for b in bounds]\n\n    return candidates", "response": "Finds nearby faces relatively quickly."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef closest_point_naive(mesh, points):\n    # get triangles from mesh\n    triangles = mesh.triangles.view(np.ndarray)\n    # establish that input points are sane\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('triangles shape incorrect')\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)')\n\n    # create a giant tiled array of each point tiled len(triangles) times\n    points_tiled = np.tile(points, (1, len(triangles)))\n    on_triangle = np.array([closest_point_corresponding(\n        triangles, i.reshape((-1, 3))) for i in points_tiled])\n\n    # distance squared\n    distance_2 = [((i - q)**2).sum(axis=1)\n                  for i, q in zip(on_triangle, points)]\n\n    triangle_id = np.array([i.argmin() for i in distance_2])\n\n    # closest cartesian point\n    closest = np.array([g[i] for i, g in zip(triangle_id, on_triangle)])\n    distance = np.array([g[i] for i, g in zip(triangle_id, distance_2)]) ** .5\n\n    return closest, distance, triangle_id", "response": "Given a mesh and a list of points find the closest point on any triangle and return the closest point on that triangle and the distance between each point and the closest point on that triangle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef closest_point(mesh, points):\n\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    # do a tree- based query for faces near each point\n    candidates = nearby_faces(mesh, points)\n    # view triangles as an ndarray so we don't have to recompute\n    # the MD5 during all of the subsequent advanced indexing\n    triangles = mesh.triangles.view(np.ndarray)\n\n    # create the corresponding list of triangles\n    # and query points to send to the closest_point function\n    query_point = deque()\n    query_tri = deque()\n    for triangle_ids, point in zip(candidates, points):\n        query_point.append(np.tile(point, (len(triangle_ids), 1)))\n        query_tri.append(triangles[triangle_ids])\n\n    # stack points into an (n,3) array\n    query_point = np.vstack(query_point)\n    # stack triangles into an (n,3,3) array\n    query_tri = np.vstack(query_tri)\n\n    # do the computation for closest point\n    query_close = closest_point_corresponding(query_tri, query_point)\n    query_group = np.cumsum(np.array([len(i) for i in candidates]))[:-1]\n\n    distance_2 = ((query_close - query_point) ** 2).sum(axis=1)\n\n    # find the single closest point for each group of candidates\n    result_close = np.zeros((len(points), 3), dtype=np.float64)\n    result_tid = np.zeros(len(points), dtype=np.int64)\n    result_distance = np.zeros(len(points), dtype=np.float64)\n\n    # go through results to get minimum distance result\n    for i, close_points, distance, candidate in zip(\n            np.arange(len(points)),\n            np.array_split(query_close, query_group),\n            np.array_split(distance_2, query_group),\n            candidates):\n\n        # unless some other check is true use the smallest distance\n        idx = distance.argmin()\n\n        # if we have multiple candidates check them\n        if len(candidate) > 1:\n            # (2, ) int, list of 2 closest candidate indices\n            idxs = distance.argsort()[:2]\n            # make sure the two distances are identical\n            check_distance = distance[idxs].ptp() < tol.merge\n            # make sure the magnitude of both distances are nonzero\n            check_magnitude = (np.abs(distance[idxs]) > tol.merge).all()\n\n            # check if query-points are actually off-surface\n            if check_distance and check_magnitude:\n                # get face normals for two points\n                normals = mesh.face_normals[np.array(candidate)[idxs]]\n                # compute normalized surface-point to query-point vectors\n                vectors = ((points[i] - close_points[idxs]) /\n                           distance[idxs, np.newaxis] ** 0.5)\n                # compare enclosed angle for both face normals\n                dots = util.diagonal_dot(normals, vectors)\n                # take the idx with the most positive angle\n                idx = idxs[dots.argmax()]\n\n        # take the single closest value from the group of values\n        result_close[i] = close_points[idx]\n        result_tid[i] = candidate[idx]\n        result_distance[i] = distance[idx]\n\n    # we were comparing the distance squared so\n    # now take the square root in one vectorized operation\n    result_distance **= .5\n\n    return result_close, result_distance, result_tid", "response": "Given a mesh and a list of points find the closest point on any triangle."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds the signed distance from a mesh to a list of points.", "response": "def signed_distance(mesh, points):\n    \"\"\"\n    Find the signed distance from a mesh to a list of points.\n\n    * Points OUTSIDE the mesh will have NEGATIVE distance\n    * Points within tol.merge of the surface will have POSITIVE distance\n    * Points INSIDE the mesh will have POSITIVE distance\n\n    Parameters\n    -----------\n    mesh   : Trimesh object\n    points : (n,3) float, list of points in space\n\n    Returns\n    ----------\n    signed_distance : (n,3) float, signed distance from point to mesh\n    \"\"\"\n    # make sure we have a numpy array\n    points = np.asanyarray(points, dtype=np.float64)\n\n    # find the closest point on the mesh to the queried points\n    closest, distance, triangle_id = closest_point(mesh, points)\n\n    # we only care about nonzero distances\n    nonzero = distance > tol.merge\n\n    if not nonzero.any():\n        return distance\n\n    inside = mesh.ray.contains_points(points[nonzero])\n    sign = (inside.astype(int) * 2) - 1\n\n    # apply sign to previously computed distance\n    distance[nonzero] *= sign\n\n    return distance"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the length of the longest ray in a mesh.", "response": "def longest_ray(mesh, points, directions):\n    \"\"\"\n    Find the lengths of the longest rays which do not intersect the mesh\n    cast from a list of points in the provided directions.\n\n    Parameters\n    -----------\n    points : (n,3) float, list of points in space\n    directions : (n,3) float, directions of rays\n\n    Returns\n    ----------\n    signed_distance : (n,) float, length of rays\n    \"\"\"\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    directions = np.asanyarray(directions, dtype=np.float64)\n    if not util.is_shape(directions, (-1, 3)):\n        raise ValueError('directions must be (n,3)!')\n\n    if len(points) != len(directions):\n        raise ValueError('number of points must equal number of directions!')\n\n    faces, rays, locations = mesh.ray.intersects_id(points, directions,\n                                                    return_locations=True,\n                                                    multiple_hits=True)\n    if len(rays) > 0:\n        distances = np.linalg.norm(locations - points[rays],\n                                   axis=1)\n    else:\n        distances = np.array([])\n\n    # Reject intersections at distance less than tol.planar\n    rays = rays[distances > tol.planar]\n    distances = distances[distances > tol.planar]\n\n    # Add infinite length for those with no valid intersection\n    no_intersections = np.setdiff1d(np.arange(len(points)), rays)\n    rays = np.concatenate((rays, no_intersections))\n    distances = np.concatenate((distances,\n                                np.repeat(np.inf,\n                                          len(no_intersections))))\n    return group_min(rays, distances)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds the center and radius of the sphere which is tangent to the mesh at the given point and at least one more point with no non-tangential intersections with the mesh. Masatomo Inui, Nobuyuki Umezu & Ryohei Shimane (2016) Shrinking sphere: A parallel algorithm for computing the thickness of 3D objects, Computer-Aided Design and Applications, 13:2, 199-207, DOI: 10.1080/16864360.2015.1084186 Parameters ---------- points : (n,3) float, list of points in space inwards : bool, whether to have the sphere inside or outside the mesh normals : (n,3) float, normals of the mesh at the given points None, compute this automatically. Returns ---------- centers : (n,3) float, centers of spheres radii : (n,) float, radii of spheres", "response": "def max_tangent_sphere(mesh,\n                       points,\n                       inwards=True,\n                       normals=None,\n                       threshold=1e-6,\n                       max_iter=100):\n    \"\"\"\n    Find the center and radius of the sphere which is tangent to\n    the mesh at the given point and at least one more point with no\n    non-tangential intersections with the mesh.\n\n    Masatomo Inui, Nobuyuki Umezu & Ryohei Shimane (2016)\n    Shrinking sphere:\n    A parallel algorithm for computing the thickness of 3D objects,\n    Computer-Aided Design and Applications, 13:2, 199-207,\n    DOI: 10.1080/16864360.2015.1084186\n\n    Parameters\n    ----------\n    points : (n,3) float, list of points in space\n    inwards : bool, whether to have the sphere inside or outside the mesh\n    normals : (n,3) float, normals of the mesh at the given points\n              None, compute this automatically.\n\n    Returns\n    ----------\n    centers : (n,3) float, centers of spheres\n    radii : (n,) float, radii of spheres\n\n    \"\"\"\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    if normals is not None:\n        normals = np.asanyarray(normals, dtype=np.float64)\n        if not util.is_shape(normals, (-1, 3)):\n            raise ValueError('normals must be (n,3)!')\n\n        if len(points) != len(normals):\n            raise ValueError('number of points must equal number of normals!')\n    else:\n        normals = mesh.face_normals[closest_point(mesh, points)[2]]\n\n    if inwards:\n        normals = -normals\n\n    # Find initial tangent spheres\n    distances = longest_ray(mesh, points, normals)\n    radii = distances * 0.5\n    not_converged = np.ones(len(points), dtype=np.bool)  # boolean mask\n\n    # If ray is infinite, find the vertex which is furthest from our point\n    # when projected onto the ray. I.e. find v which maximises\n    # (v-p).n = v.n - p.n.\n    # We use a loop rather a vectorised approach to reduce memory cost\n    # it also seems to run faster.\n    for i in np.where(np.isinf(distances))[0]:\n        projections = np.dot(mesh.vertices - points[i], normals[i])\n\n        # If no points lie outside the tangent plane, then the radius is infinite\n        # otherwise we have a point outside the tangent plane, take the one with maximal\n        # projection\n        if projections.max() < tol.planar:\n            radii[i] = np.inf\n            not_converged[i] = False\n        else:\n            vertex = mesh.vertices[projections.argmax()]\n            radii[i] = (np.dot(vertex - points[i], vertex - points[i]) /\n                        (2 * np.dot(vertex - points[i], normals[i])))\n\n    # Compute centers\n    centers = points + normals * np.nan_to_num(radii.reshape(-1, 1))\n    centers[np.isinf(radii)] = [np.nan, np.nan, np.nan]\n\n    # Our iterative process terminates when the difference in sphere\n    # radius is less than threshold*D\n    D = np.linalg.norm(mesh.bounds[1] - mesh.bounds[0])\n    convergence_threshold = threshold * D\n    n_iter = 0\n    while not_converged.sum() > 0 and n_iter < max_iter:\n        n_iter += 1\n        n_points, n_dists, n_faces = mesh.nearest.on_surface(\n            centers[not_converged])\n\n        # If the distance to the nearest point is the same as the distance\n        # to the start point then we are done.\n        done = np.abs(\n            n_dists -\n            np.linalg.norm(\n                centers[not_converged] -\n                points[not_converged],\n                axis=1)) < tol.planar\n        not_converged[np.where(not_converged)[0][done]] = False\n\n        # Otherwise find the radius and center of the sphere tangent to the mesh\n        # at the point and the nearest point.\n        diff = n_points[~done] - points[not_converged]\n        old_radii = radii[not_converged].copy()\n        # np.einsum produces element wise dot product\n        radii[not_converged] = (np.einsum('ij, ij->i',\n                                          diff,\n                                          diff) /\n                                (2 * np.einsum('ij, ij->i',\n                                               diff,\n                                               normals[not_converged])))\n        centers[not_converged] = points[not_converged] + \\\n            normals[not_converged] * radii[not_converged].reshape(-1, 1)\n\n        # If change in radius is less than threshold we have converged\n        cvged = old_radii - radii[not_converged] < convergence_threshold\n        not_converged[np.where(not_converged)[0][cvged]] = False\n\n    return centers, radii"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the thickness of a single object in a single segment.", "response": "def thickness(mesh,\n              points,\n              exterior=False,\n              normals=None,\n              method='max_sphere'):\n    \"\"\"\n    Find the thickness of the mesh at the given points.\n\n    Parameters\n    ----------\n    points : (n,3) float, list of points in space\n    exterior : bool, whether to compute the exterior thickness\n                     (a.k.a. reach)\n    normals : (n,3) float, normals of the mesh at the given points\n              None, compute this automatically.\n    method : string, one of 'max_sphere' or 'ray'\n\n    Returns\n    ----------\n    thickness : (n,) float, thickness\n    \"\"\"\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    if normals is not None:\n        normals = np.asanyarray(normals, dtype=np.float64)\n        if not util.is_shape(normals, (-1, 3)):\n            raise ValueError('normals must be (n,3)!')\n\n        if len(points) != len(normals):\n            raise ValueError('number of points must equal number of normals!')\n    else:\n        normals = mesh.face_normals[closest_point(mesh, points)[2]]\n\n    if method == 'max_sphere':\n        centers, radius = max_tangent_sphere(mesh=mesh,\n                                             points=points,\n                                             inwards=not exterior,\n                                             normals=normals)\n        thickness = radius * 2\n        return thickness\n\n    elif method == 'ray':\n        if exterior:\n            return longest_ray(mesh, points, normals)\n        else:\n            return longest_ray(mesh, points, -normals)\n    else:\n        raise ValueError('Invalid method, use \"max_sphere\" or \"ray\"')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a set of points return the closest vertex index to each point in the set", "response": "def vertex(self, points):\n        \"\"\"\n        Given a set of points, return the closest vertex index to each point\n\n        Parameters\n        ----------\n        points : (n,3) float, list of points in space\n\n        Returns\n        ----------\n        distance  : (n,) float, distance from source point to vertex\n        vertex_id : (n,) int, index of mesh.vertices which is closest\n        \"\"\"\n        tree = self._mesh.kdtree\n        return tree.query(points)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of single - body meshes to test identifiers on.", "response": "def get_meshes(path='../../../models', cutoff=None):\r\n    \"\"\"\r\n    Get a list of single- body meshes to test identifiers on.\r\n\r\n    Parameters\r\n    ------------\r\n    path:   str, location of models\r\n    cutoff: int, number of meshes to stop loading at\r\n\r\n    Returns\r\n    ------------\r\n    meshes: (n,) list of Trimesh objects\r\n    \"\"\"\r\n\r\n    bodies = collections.deque()\r\n    for file_name in os.listdir(path):\r\n        try:\r\n            mesh = trimesh.load(os.path.join(path, file_name))\r\n            split = mesh.split()\r\n            bodies.extend(split)\r\n            if len(split) > 1:\r\n                bodies.append(mesh)\r\n        except BaseException:\r\n            continue\r\n\r\n        if cutoff is not None and len(bodies) > cutoff:\r\n            return np.array(bodies)\r\n\r\n    for i in range(100):\r\n        cylinder = trimesh.creation.cylinder(\r\n            radius=np.random.random() * 100,\r\n            height=np.random.random() * 1000,\r\n            sections=int(np.clip(np.random.random() * 720,\r\n                                 20,\r\n                                 720)))\r\n\r\n        capsule = trimesh.creation.capsule(\r\n            radius=np.random.random() * 100,\r\n            height=np.random.random() * 1000,\r\n            count=np.clip(np.random.random(2) * 720,\r\n                          20,\r\n                          720).astype(int))\r\n        bodies.append(cylinder)\r\n        bodies.append(capsule)\r\n    for i in range(10):\r\n        bodies.append(trimesh.creation.random_soup(\r\n            int(np.clip(np.random.random() * 1000,\r\n                        20,\r\n                        1000))))\r\n    bodies.append(trimesh.creation.icosphere())\r\n    bodies.append(trimesh.creation.uv_sphere())\r\n    bodies.append(trimesh.creation.icosahedron())\r\n\r\n    return np.array(bodies)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new object that is aligned with the other object.", "response": "def mesh_other(mesh,\n               other,\n               samples=500,\n               scale=False,\n               icp_first=10,\n               icp_final=50):\n    \"\"\"\n    Align a mesh with another mesh or a PointCloud using\n    the principal axes of inertia as a starting point which\n    is refined by iterative closest point.\n\n    Parameters\n    ------------\n    mesh : trimesh.Trimesh object\n      Mesh to align with other\n    other : trimesh.Trimesh or (n, 3) float\n      Mesh or points in space\n    samples : int\n      Number of samples from mesh surface to align\n    scale : bool\n      Allow scaling in transform\n    icp_first : int\n      How many ICP iterations for the 9 possible\n      combinations of sign flippage\n    icp_final : int\n      How many ICP iterations for the closest\n      candidate from the wider search\n\n    Returns\n    -----------\n    mesh_to_other : (4, 4) float\n      Transform to align mesh to the other object\n    cost : float\n      Average squared distance per point\n    \"\"\"\n\n    def key_points(m, count):\n        \"\"\"\n        Return a combination of mesh vertices and surface samples\n        with vertices chosen by likelihood to be important\n        to registation.\n        \"\"\"\n        if len(m.vertices) < (count / 2):\n            return np.vstack((\n                m.vertices,\n                m.sample(count - len(m.vertices))))\n        else:\n            return m.sample(count)\n\n    if not util.is_instance_named(mesh, 'Trimesh'):\n        raise ValueError('mesh must be Trimesh object!')\n\n    inverse = True\n    search = mesh\n    # if both are meshes use the smaller one for searching\n    if util.is_instance_named(other, 'Trimesh'):\n        if len(mesh.vertices) > len(other.vertices):\n            # do the expensive tree construction on the\n            # smaller mesh and query the others points\n            search = other\n            inverse = False\n            points = key_points(m=mesh, count=samples)\n            points_mesh = mesh\n        else:\n            points_mesh = other\n            points = key_points(m=other, count=samples)\n\n        if points_mesh.is_volume:\n            points_PIT = points_mesh.principal_inertia_transform\n        else:\n            points_PIT = points_mesh.bounding_box_oriented.principal_inertia_transform\n\n    elif util.is_shape(other, (-1, 3)):\n        # case where other is just points\n        points = other\n        points_PIT = bounds.oriented_bounds(points)[0]\n    else:\n        raise ValueError('other must be mesh or (n, 3) points!')\n\n    # get the transform that aligns the search mesh principal\n    # axes of inertia with the XYZ axis at the origin\n    if search.is_volume:\n        search_PIT = search.principal_inertia_transform\n    else:\n        search_PIT = search.bounding_box_oriented.principal_inertia_transform\n\n    # transform that moves the principal axes of inertia\n    # of the search mesh to be aligned with the best- guess\n    # principal axes of the points\n    search_to_points = np.dot(np.linalg.inv(points_PIT),\n                              search_PIT)\n\n    # permutations of cube rotations\n    # the principal inertia transform has arbitrary sign\n    # along the 3 major axis so try all combinations of\n    # 180 degree rotations with a quick first ICP pass\n    cubes = np.array([np.eye(4) * np.append(diag, 1)\n                      for diag in [[1, 1, 1],\n                                   [1, 1, -1],\n                                   [1, -1, 1],\n                                   [-1, 1, 1],\n                                   [-1, -1, 1],\n                                   [-1, 1, -1],\n                                   [1, -1, -1],\n                                   [-1, -1, -1]]])\n\n    # loop through permutations and run iterative closest point\n    costs = np.ones(len(cubes)) * np.inf\n    transforms = [None] * len(cubes)\n    centroid = search.centroid\n\n    for i, flip in enumerate(cubes):\n        # transform from points to search mesh\n        # flipped around the centroid of search\n        a_to_b = np.dot(\n            transformations.transform_around(flip, centroid),\n            np.linalg.inv(search_to_points))\n\n        # run first pass ICP\n        matrix, junk, cost = icp(a=points,\n                                 b=search,\n                                 initial=a_to_b,\n                                 max_iterations=int(icp_first),\n                                 scale=scale)\n\n        # save transform and costs from ICP\n        transforms[i] = matrix\n        costs[i] = cost\n\n    # run a final ICP refinement step\n    matrix, junk, cost = icp(a=points,\n                             b=search,\n                             initial=transforms[np.argmin(costs)],\n                             max_iterations=int(icp_final),\n                             scale=scale)\n\n    # convert to per- point distance average\n    cost /= len(points)\n\n    # we picked the smaller mesh to construct the tree\n    # on so we may have calculated a transform backwards\n    # to save computation, so just invert matrix here\n    if inverse:\n        mesh_to_other = np.linalg.inv(matrix)\n    else:\n        mesh_to_other = matrix\n\n    return mesh_to_other, cost"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef procrustes(a,\n               b,\n               reflection=True,\n               translation=True,\n               scale=True,\n               return_cost=True):\n    \"\"\"\n    Perform Procrustes' analysis subject to constraints. Finds the\n    transformation T mapping a to b which minimizes the square sum\n    distances between Ta and b, also called the cost.\n\n    Parameters\n    ----------\n    a : (n,3) float\n      List of points in space\n    b : (n,3) float\n      List of points in space\n    reflection : bool\n      If the transformation is allowed reflections\n    translation : bool\n      If the transformation is allowed translations\n    scale : bool\n      If the transformation is allowed scaling\n    return_cost : bool\n      Whether to return the cost and transformed a as well\n\n    Returns\n    ----------\n    matrix : (4,4) float\n      The transformation matrix sending a to b\n    transformed : (n,3) float\n      The image of a under the transformation\n    cost : float\n      The cost of the transformation\n    \"\"\"\n\n    a = np.asanyarray(a, dtype=np.float64)\n    b = np.asanyarray(b, dtype=np.float64)\n    if not util.is_shape(a, (-1, 3)) or not util.is_shape(b, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    if len(a) != len(b):\n        raise ValueError('a and b must contain same number of points!')\n\n    # Remove translation component\n    if translation:\n        acenter = a.mean(axis=0)\n        bcenter = b.mean(axis=0)\n    else:\n        acenter = np.zeros(a.shape[1])\n        bcenter = np.zeros(b.shape[1])\n\n    # Remove scale component\n    if scale:\n        ascale = np.sqrt(((a - acenter)**2).sum() / len(a))\n        bscale = np.sqrt(((b - bcenter)**2).sum() / len(b))\n    else:\n        ascale = 1\n        bscale = 1\n\n    # Use SVD to find optimal orthogonal matrix R\n    # constrained to det(R) = 1 if necessary.\n    u, s, vh = np.linalg.svd(\n        np.dot(((b - bcenter) / bscale).T, ((a - acenter) / ascale)))\n    if reflection:\n        R = np.dot(u, vh)\n    else:\n        R = np.dot(np.dot(u, np.diag(\n            [1, 1, np.linalg.det(np.dot(u, vh))])), vh)\n\n    # Compute our 4D transformation matrix encoding\n    # a -> (R @ (a - acenter)/ascale) * bscale + bcenter\n    #    = (bscale/ascale)R @ a + (bcenter - (bscale/ascale)R @ acenter)\n    translation = bcenter - (bscale / ascale) * np.dot(R, acenter)\n    matrix = np.hstack((bscale / ascale * R, translation.reshape(-1, 1)))\n    matrix = np.vstack(\n        (matrix, np.array([0.] * (a.shape[1]) + [1.]).reshape(1, -1)))\n\n    if return_cost:\n        transformed = transform_points(a, matrix)\n        cost = ((b - transformed)**2).mean()\n        return matrix, transformed, cost\n    else:\n        return matrix", "response": "Perform a procrustes analysis on two sets of points a and b."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef icp(a,\n        b,\n        initial=np.identity(4),\n        threshold=1e-5,\n        max_iterations=20,\n        **kwargs):\n    \"\"\"\n    Apply the iterative closest point algorithm to align a point cloud with\n    another point cloud or mesh. Will only produce reasonable results if the\n    initial transformation is roughly correct. Initial transformation can be\n    found by applying Procrustes' analysis to a suitable set of landmark\n    points (often picked manually).\n\n    Parameters\n    ----------\n    a : (n,3) float\n      List of points in space.\n    b : (m,3) float or Trimesh\n      List of points in space or mesh.\n    initial : (4,4) float\n      Initial transformation.\n    threshold : float\n      Stop when change in cost is less than threshold\n    max_iterations : int\n      Maximum number of iterations\n    kwargs : dict\n      Args to pass to procrustes\n\n    Returns\n    ----------\n    matrix : (4,4) float\n      The transformation matrix sending a to b\n    transformed : (n,3) float\n      The image of a under the transformation\n    cost : float\n      The cost of the transformation\n    \"\"\"\n\n    a = np.asanyarray(a, dtype=np.float64)\n    if not util.is_shape(a, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    is_mesh = util.is_instance_named(b, 'Trimesh')\n    if not is_mesh:\n        b = np.asanyarray(b, dtype=np.float64)\n        if not util.is_shape(b, (-1, 3)):\n            raise ValueError('points must be (n,3)!')\n        btree = cKDTree(b)\n\n    # transform a under initial_transformation\n    a = transform_points(a, initial)\n    total_matrix = initial\n\n    # start with infinite cost\n    old_cost = np.inf\n\n    # avoid looping forever by capping iterations\n    for n_iteration in range(max_iterations):\n        # Closest point in b to each point in a\n        if is_mesh:\n            closest, distance, faces = b.nearest.on_surface(a)\n        else:\n            distances, ix = btree.query(a, 1)\n            closest = b[ix]\n\n        # align a with closest points\n        matrix, transformed, cost = procrustes(a=a,\n                                               b=closest,\n                                               **kwargs)\n\n        # update a with our new transformed points\n        a = transformed\n        total_matrix = np.dot(matrix, total_matrix)\n\n        if old_cost - cost < threshold:\n            break\n        else:\n            old_cost = cost\n\n    return total_matrix, transformed, cost", "response": "This function computes the iterative closest point algorithm to align a point cloud with another point cloud or mesh."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef concatenate(visuals, *args):\n    # get a flat list of ColorVisuals objects\n    if len(args) > 0:\n        visuals = np.append(visuals, args)\n    else:\n        visuals = np.array(visuals)\n\n    # get the type of visuals (vertex or face) removing undefined\n    modes = {v.kind for v in visuals}.difference({None})\n    if len(modes) == 0:\n        # none of the visuals have anything defined\n        return ColorVisuals()\n    else:\n        # if we have visuals with different modes defined\n        # arbitrarily get one of them\n        mode = modes.pop()\n\n    # a linked list to store colors before stacking\n    colors = collections.deque()\n    # a string to evaluate which returns the colors we want\n    append = 'v.{}_colors'.format(mode)\n    for v in visuals:\n        # use an eval so we can use the object property\n        colors.append(eval(append))\n    # use an eval so we can use the constructor\n    concat = eval('ColorVisuals({}_colors=np.vstack(colors))'.format(mode))\n    return concat", "response": "Concatenate multiple visual objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_rgba(colors, dtype=np.uint8):\n    if not util.is_sequence(colors):\n        return\n\n    # colors as numpy array\n    colors = np.asanyarray(colors)\n\n    # integer value for opaque alpha given our datatype\n    opaque = np.iinfo(dtype).max\n\n    if (colors.dtype.kind == 'f' and colors.max() < (1.0 + 1e-8)):\n        colors = (colors * opaque).astype(dtype)\n    elif (colors.max() <= opaque):\n        colors = colors.astype(dtype)\n    else:\n        raise ValueError('colors non- convertible!')\n\n    if util.is_shape(colors, (-1, 3)):\n        # add an opaque alpha for RGB colors\n        colors = np.column_stack((\n            colors,\n            opaque * np.ones(len(colors)))).astype(dtype)\n    elif util.is_shape(colors, (3,)):\n        # if passed a single RGB color add an alpha\n        colors = np.append(colors, opaque)\n\n    if not (util.is_shape(colors, (4,)) or\n            util.is_shape(colors, (-1, 4))):\n        raise ValueError('Colors not of appropriate shape!')\n\n    return colors", "response": "Convert a single or multiple RGB colors to RGBA colors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nturning a string hex color to a 4 - bit RGBA color.", "response": "def hex_to_rgba(color):\n    \"\"\"\n    Turn a string hex color to a (4,) RGBA color.\n\n    Parameters\n    -----------\n    color: str, hex color\n\n    Returns\n    -----------\n    rgba: (4,) np.uint8, RGBA color\n    \"\"\"\n    value = str(color).lstrip('#').strip()\n    if len(value) == 6:\n        rgb = [int(value[i:i + 2], 16) for i in (0, 2, 4)]\n        rgba = np.append(rgb, 255).astype(np.uint8)\n    else:\n        raise ValueError('Only RGB supported')\n\n    return rgba"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a random RGB color using datatype specified.", "response": "def random_color(dtype=np.uint8):\n    \"\"\"\n    Return a random RGB color using datatype specified.\n\n    Parameters\n    ----------\n    dtype: numpy dtype of result\n\n    Returns\n    ----------\n    color: (4,) dtype, random color that looks OK\n    \"\"\"\n    hue = np.random.random() + .61803\n    hue %= 1.0\n    color = np.array(colorsys.hsv_to_rgb(hue, .99, .99))\n    if np.dtype(dtype).kind in 'iu':\n        max_value = (2**(np.dtype(dtype).itemsize * 8)) - 1\n        color *= max_value\n    color = np.append(color, max_value).astype(dtype)\n    return color"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a list of vertex colors to face colors.", "response": "def vertex_to_face_color(vertex_colors, faces):\n    \"\"\"\n    Convert a list of vertex colors to face colors.\n\n    Parameters\n    ----------\n    vertex_colors: (n,(3,4)),  colors\n    faces:         (m,3) int, face indexes\n\n    Returns\n    -----------\n    face_colors: (m,4) colors\n    \"\"\"\n    vertex_colors = to_rgba(vertex_colors)\n    face_colors = vertex_colors[faces].mean(axis=1)\n    return face_colors.astype(np.uint8)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a list of face colors into a list of vertex colors.", "response": "def face_to_vertex_color(mesh, face_colors, dtype=np.uint8):\n    \"\"\"\n    Convert a list of face colors into a list of vertex colors.\n\n    Parameters\n    -----------\n    mesh : trimesh.Trimesh object\n    face_colors: (n, (3,4)) int, face colors\n    dtype:       data type of output\n\n    Returns\n    -----------\n    vertex_colors: (m,4) dtype, colors for each vertex\n    \"\"\"\n    rgba = to_rgba(face_colors)\n    vertex_colors = mesh.faces_sparse.dot(\n        rgba.astype(np.float64))\n    vertex_colors /= mesh.faces_sparse.sum(axis=1)\n    vertex_colors = vertex_colors.astype(dtype)\n\n    return vertex_colors"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef colors_to_materials(colors, count=None):\n\n    # convert RGB to RGBA\n    rgba = to_rgba(colors)\n\n    # if we were only passed a single color\n    if util.is_shape(rgba, (4,)) and count is not None:\n        diffuse = rgba.reshape((-1, 4))\n        index = np.zeros(count, dtype=np.int)\n    elif util.is_shape(rgba, (-1, 4)):\n        # we were passed multiple colors\n        # find the unique colors in the list to save as materials\n        unique, index = grouping.unique_rows(rgba)\n        diffuse = rgba[unique]\n    else:\n        raise ValueError('Colors not convertible!')\n\n    return diffuse, index", "response": "Convert a list of colors into a list of unique materials and material indexes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a 1D list of values return interpolated RGBA colors for the range.", "response": "def interpolate(values, color_map=None, dtype=np.uint8):\n    \"\"\"\n    Given a 1D list of values, return interpolated colors\n    for the range.\n\n    Parameters\n    ---------------\n    values : (n, ) float\n      Values to be interpolated over\n    color_map : None, or str\n      Key to a colormap contained in:\n      matplotlib.pyplot.colormaps()\n      e.g: 'viridis'\n\n    Returns\n    -------------\n    interpolated : (n, 4) dtype\n      Interpolated RGBA colors\n    \"\"\"\n\n    # get a color interpolation function\n    if color_map is None:\n        cmap = linear_color_map\n    else:\n        from matplotlib.pyplot import get_cmap\n        cmap = get_cmap(color_map)\n\n    # make input always float\n    values = np.asanyarray(values, dtype=np.float64).ravel()\n    # scale values to 0.0 - 1.0 and get colors\n    colors = cmap((values - values.min()) / values.ptp())\n    # convert to 0-255 RGBA\n    rgba = to_rgba(colors, dtype=dtype)\n\n    return rgba"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the current object contains any transparency.", "response": "def transparency(self):\n        \"\"\"\n        Does the current object contain any transparency.\n\n        Returns\n        ----------\n        transparency: bool, does the current visual contain transparency\n        \"\"\"\n        if 'vertex_colors' in self._data:\n            a_min = self._data['vertex_colors'][:, 3].min()\n        elif 'face_colors' in self._data:\n            a_min = self._data['face_colors'][:, 3].min()\n        else:\n            return False\n\n        return bool(a_min < 255)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the color mode of the current locale.", "response": "def kind(self):\n        \"\"\"\n        What color mode has been set.\n\n        Returns\n        ----------\n        mode: 'face', 'vertex', or None\n        \"\"\"\n        self._verify_crc()\n        if 'vertex_colors' in self._data:\n            mode = 'vertex'\n        elif 'face_colors' in self._data:\n            mode = 'face'\n        else:\n            mode = None\n\n        return mode"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef copy(self):\n        copied = ColorVisuals()\n        copied._data.data = copy.deepcopy(self._data.data)\n        return copied", "response": "Returns a copy of the current ColorVisuals object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the colors for each face of a mesh.", "response": "def face_colors(self, values):\n        \"\"\"\n        Set the colors for each face of a mesh.\n\n        This will apply these colors and delete any previously specified\n        color information.\n\n        Parameters\n        ------------\n        colors: (len(mesh.faces), 3), set each face to the specified color\n                (len(mesh.faces), 4), set each face to the specified color\n                (3,) int, set the whole mesh this color\n                (4,) int, set the whole mesh this color\n        \"\"\"\n        if values is None:\n            if 'face_colors' in self._data:\n                self._data.data.pop('face_colors')\n            return\n\n        colors = to_rgba(values)\n\n        if (self.mesh is not None and\n                colors.shape == (4,)):\n            count = len(self.mesh.faces)\n            colors = np.tile(colors, (count, 1))\n\n        # if we set any color information, clear the others\n        self._data.clear()\n        self._data['face_colors'] = colors\n        self._cache.verify()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset the colors for each vertex of a mesh and delete any previously specified colors.", "response": "def vertex_colors(self, values):\n        \"\"\"\n        Set the colors for each vertex of a mesh\n\n        This will apply these colors and delete any previously specified\n        color information.\n\n        Parameters\n        ------------\n        colors: (len(mesh.vertices), 3), set each face to the color\n                (len(mesh.vertices), 4), set each face to the color\n                (3,) int, set the whole mesh this color\n                (4,) int, set the whole mesh this color\n        \"\"\"\n        if values is None:\n            if 'vertex_colors' in self._data:\n                self._data.data.pop('vertex_colors')\n            return\n\n        # make sure passed values are numpy array\n        values = np.asanyarray(values)\n        # Ensure the color shape is sane\n        if (self.mesh is not None and not\n                (values.shape == (len(self.mesh.vertices), 3) or\n                 values.shape == (len(self.mesh.vertices), 4) or\n                 values.shape == (3,) or\n                 values.shape == (4,))):\n            return\n\n        colors = to_rgba(values)\n        if (self.mesh is not None and\n                colors.shape == (4,)):\n            count = len(self.mesh.vertices)\n            colors = np.tile(colors, (count, 1))\n\n        # if we set any color information, clear the others\n        self._data.clear()\n        self._data['vertex_colors'] = colors\n        self._cache.verify()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _verify_crc(self):\n        if not hasattr(self, '_cache') or len(self._cache) == 0:\n            return\n\n        for name in ['face', 'vertex']:\n            # the face or vertex colors\n            key_colors = str(name) + '_colors'\n            # the initial crc of the\n            key_crc = key_colors + '_crc'\n\n            if key_colors not in self._cache:\n                continue\n\n            colors = self._cache[key_colors]\n            # if the cached colors have been changed since creation\n            # move them to data\n            if colors.crc() != self._cache[key_crc]:\n                if name == 'face':\n                    self.face_colors = colors\n                elif name == 'vertex':\n                    self.vertex_colors = colors\n                else:\n                    raise ValueError('unsupported name!!!')\n                self._cache.verify()", "response": "Verify the checksums of the cached face and vertex colors of the user s user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a mask of face indices return a sliced version of the color visual.", "response": "def face_subset(self, face_index):\n        \"\"\"\n        Given a mask of face indices, return a sliced version.\n\n        Parameters\n        ----------\n        face_index: (n,) int, mask for faces\n                    (n,) bool, mask for faces\n\n        Returns\n        ----------\n        visual: ColorVisuals object containing a subset of faces.\n        \"\"\"\n        if self.defined:\n            result = ColorVisuals(\n                face_colors=self.face_colors[face_index])\n        else:\n            result = ColorVisuals()\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main_color(self):\n        if self.kind is None:\n            return DEFAULT_COLOR\n        elif self.kind == 'face':\n            colors = self.face_colors\n        elif self.kind == 'vertex':\n            colors = self.vertex_colors\n        else:\n            raise ValueError('color kind incorrect!')\n\n        # find the unique colors\n        unique, inverse = grouping.unique_rows(colors)\n        # the most commonly occurring color, or mode\n        # this will be an index of inverse, not colors\n        mode_index = np.bincount(inverse).argmax()\n        color = colors[unique[mode_index]]\n\n        return color", "response": "Returns the main color of the most commonly occurring color."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef concatenate(self, other, *args):\n        # avoid a circular import\n        from . import objects\n        result = objects.concatenate(self, other, *args)\n        return result", "response": "Concatenate two or more ColorVisuals objects into a single object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _update_key(self, mask, key):\n        mask = np.asanyarray(mask)\n        if key in self._data:\n            self._data[key] = self._data[key][mask]", "response": "Update the key in the internal data structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process(self):\n        # if there are no vertices or faces exit early\n        if self.is_empty:\n            return self\n\n        # avoid clearing the cache during operations\n        with self._cache:\n            self.remove_infinite_values()\n            self.merge_vertices()\n            # if we're cleaning remove duplicate\n            # and degenerate faces\n            if self._validate:\n                self.remove_duplicate_faces()\n                self.remove_degenerate_faces()\n        # since none of our process operations moved vertices or faces,\n        # we can keep face and vertex normals in the cache without recomputing\n        # if faces or vertices have been removed, normals are validated before\n        # being returned so there is no danger of inconsistent dimensions\n        self._cache.clear(exclude=['face_normals',\n                                   'vertex_normals'])\n        self.metadata['processed'] = True\n        return self", "response": "Process the current object and return the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the vertex indexes that make up triangular faces.", "response": "def faces(self, values):\n        \"\"\"\n        Set the vertex indexes that make up triangular faces.\n\n        Parameters\n        --------------\n        values : (n, 3) int\n          Indexes of self.vertices\n        \"\"\"\n        if values is None:\n            values = []\n        values = np.asanyarray(values, dtype=np.int64)\n        # automatically triangulate quad faces\n        if util.is_shape(values, (-1, 4)):\n            log.info('triangulating quad faces')\n            values = geometry.triangulate_quads(values)\n        self._data['faces'] = values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef faces_sparse(self):\n        sparse = geometry.index_sparse(\n            column_count=len(self.vertices),\n            indices=self.faces)\n        return sparse", "response": "A sparse matrix representation of the faces."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef face_normals(self):\n        # check shape of cached normals\n        cached = self._cache['face_normals']\n        if np.shape(cached) == np.shape(self._data['faces']):\n            return cached\n\n        log.debug('generating face normals')\n        # use cached triangle cross products to generate normals\n        # this will always return the correct shape but some values\n        # will be zero or an arbitrary vector if the inputs had\n        # a cross product below machine epsilon\n        normals, valid = triangles.normals(\n            triangles=self.triangles,\n            crosses=self.triangles_cross)\n\n        # if all triangles are valid shape is correct\n        if valid.all():\n            # put calculated face normals into cache manually\n            self._cache['face_normals'] = normals\n            return normals\n\n        # make a padded list of normals for correct shape\n        padded = np.zeros((len(self.triangles), 3),\n                          dtype=np.float64)\n        padded[valid] = normals\n\n        # put calculated face normals into cache manually\n        self._cache['face_normals'] = padded\n\n        return padded", "response": "Return the unit normal vectors for each face."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nassigns values to face normals.", "response": "def face_normals(self, values):\n        \"\"\"\n        Assign values to face normals.\n\n        Parameters\n        -------------\n        values : (len(self.faces), 3) float\n          Unit face normals\n        \"\"\"\n        if values is not None:\n            # make sure face normals are C- contiguous float\n            values = np.asanyarray(values,\n                                   order='C',\n                                   dtype=np.float64)\n\n            # check if any values are larger than tol.merge\n            # this check is equivalent to but 25% faster than:\n            # `np.abs(values) > tol.merge`\n            nonzero = np.logical_or(values > tol.merge,\n                                    values < -tol.merge)\n\n            # don't set the normals if they are all zero\n            if not nonzero.any():\n                log.warning('face_normals all zero, ignoring!')\n                return\n\n            # make sure the first few normals match the first few triangles\n            check, valid = triangles.normals(\n                self.vertices.view(np.ndarray)[self.faces[:20]])\n            compare = np.zeros((len(valid), 3))\n            compare[valid] = check\n\n            if not np.allclose(compare, values[:20]):\n                log.warning('face_normals didn\\'t match triangles, ignoring!')\n                return\n\n        self._cache['face_normals'] = values"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nassigns vertex values to the mesh.", "response": "def vertices(self, values):\n        \"\"\"\n        Assign vertex values to the mesh.\n\n        Parameters\n        --------------\n        values : (n, 3) float\n          Points in space\n        \"\"\"\n        self._data['vertices'] = np.asanyarray(values,\n                                               order='C',\n                                               dtype=np.float64)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the vertex normals of the current object.", "response": "def vertex_normals(self):\n        \"\"\"\n        The vertex normals of the mesh. If the normals were loaded\n        we check to make sure we have the same number of vertex\n        normals and vertices before returning them. If there are\n        no vertex normals defined or a shape mismatch we  calculate\n        the vertex normals from the mean normals of the faces the\n        vertex is used in.\n\n        Returns\n        ----------\n        vertex_normals : (n,3) float\n          Represents the surface normal at each vertex.\n          Where n == len(self.vertices)\n        \"\"\"\n        # make sure we have faces_sparse\n        assert hasattr(self.faces_sparse, 'dot')\n        vertex_normals = geometry.mean_vertex_normals(\n            vertex_count=len(self.vertices),\n            faces=self.faces,\n            face_normals=self.face_normals,\n            sparse=self.faces_sparse)\n        return vertex_normals"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef vertex_normals(self, values):\n        if values is not None:\n            values = np.asanyarray(values,\n                                   order='C',\n                                   dtype=np.float64)\n            if values.shape == self.vertices.shape:\n                self._cache['vertex_normals'] = values", "response": "Assign values to vertex normals"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the axis aligned bounds of the mesh.", "response": "def bounds(self):\n        \"\"\"\n        The axis aligned bounds of the faces of the mesh.\n\n        Returns\n        -----------\n        bounds : (2, 3) float\n          Bounding box with [min, max] coordinates\n        \"\"\"\n        # return bounds including ONLY referenced vertices\n        in_mesh = self.vertices[self.referenced_vertices]\n        # get mesh bounds with min and max\n        mesh_bounds = np.array([in_mesh.min(axis=0),\n                                in_mesh.max(axis=0)])\n        # should not be mutable\n        mesh_bounds.flags.writeable = False\n        return mesh_bounds"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extents(self):\n        extents = self.bounds.ptp(axis=0)\n        extents.flags.writeable = False\n        return extents", "response": "Returns the length width height of the bounding box of the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef centroid(self):\n\n        # use the centroid of each triangle weighted by\n        # the area of the triangle to find the overall centroid\n        centroid = np.average(self.triangles_center,\n                              axis=0,\n                              weights=self.area_faces)\n        centroid.flags.writeable = False\n        return centroid", "response": "Return the center of the overall centroid of the current set of vertices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the density of the mesh", "response": "def density(self, value):\n        \"\"\"\n        Set the density of the mesh.\n\n        Parameters\n        -------------\n        density : float\n          Specify the density of the mesh to be used in inertia calculations\n        \"\"\"\n        self._density = float(value)\n        self._cache.delete('mass_properties')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef principal_inertia_components(self):\n        # both components and vectors from inertia matrix\n        components, vectors = inertia.principal_axis(self.moment_inertia)\n        # store vectors in cache for later\n        self._cache['principal_inertia_vectors'] = vectors\n\n        return components", "response": "Returns the principal components of inertia set in the cache"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef principal_inertia_transform(self):\n        order = np.argsort(self.principal_inertia_components)[1:][::-1]\n        vectors = self.principal_inertia_vectors[order]\n        vectors = np.vstack((vectors, np.cross(*vectors)))\n\n        transform = np.eye(4)\n        transform[:3, :3] = vectors\n        transform = transformations.transform_around(\n            matrix=transform,\n            point=self.centroid)\n        transform[:3, 3] -= self.centroid\n\n        return transform", "response": "A transform which moves the current mesh so the principal inertia vectors are on the X Y and Z axis and the centroid is\n        at the origin."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether a mesh has rotational symmetry.", "response": "def symmetry(self):\n        \"\"\"\n        Check whether a mesh has rotational symmetry.\n\n        Returns\n        -----------\n        symmetry: None         No rotational symmetry\n                  'radial'     Symmetric around an axis\n                  'spherical'  Symmetric around a point\n        \"\"\"\n        symmetry, axis, section = inertia.radial_symmetry(self)\n        self._cache['symmetry_axis'] = axis\n        self._cache['symmetry_section'] = section\n        return symmetry"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef triangles(self):\n        # use of advanced indexing on our tracked arrays will\n        # trigger a change flag which means the MD5 will have to be\n        # recomputed. We can escape this check by viewing the array.\n        triangles = self.vertices.view(np.ndarray)[self.faces]\n        # make triangles (which are derived from faces/vertices) not writeable\n        triangles.flags.writeable = False\n        return triangles", "response": "The actual triangles of the mesh"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of edges of the mesh.", "response": "def edges(self):\n        \"\"\"\n        Edges of the mesh (derived from faces).\n\n        Returns\n        ---------\n        edges : (n, 2) int\n          List of vertex indices making up edges\n        \"\"\"\n        edges, index = geometry.faces_to_edges(self.faces.view(np.ndarray),\n                                               return_index=True)\n        self._cache['edges_face'] = index\n        return edges"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef edges_unique_length(self):\n        vector = np.subtract(*self.vertices[self.edges_unique.T])\n        length = np.linalg.norm(vector, axis=1)\n        return length", "response": "Calculates the length of each unique edge in the tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef edges_sparse(self):\n        sparse = graph.edges_to_coo(self.edges,\n                                    count=len(self.vertices))\n        return sparse", "response": "Returns a sparse graph of the set of nodes in the COO format where connected\n        vertices are True."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef body_count(self):\n        # labels are (len(vertices), int) OB\n        count, labels = graph.csgraph.connected_components(\n            self.edges_sparse,\n            directed=False,\n            return_labels=True)\n        self._cache['vertices_component_label'] = labels\n        return count", "response": "Returns the number of connected vertices in this mesh."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef faces_unique_edges(self):\n        # make sure we have populated unique edges\n        populate = self.edges_unique\n        # we are relying on the fact that edges are stacked in triplets\n        result = self._cache['edges_unique_inverse'].reshape((-1, 3))\n        return result", "response": "Return a list of unique edges for each face"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef euler_number(self):\n        euler = int(self.referenced_vertices.sum() -\n                    len(self.edges_unique) +\n                    len(self.faces))\n        return euler", "response": "Returns the number of the Euler characteristic for the mesh\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of vertices that are referenced by a face.", "response": "def referenced_vertices(self):\n        \"\"\"\n        Which vertices in the current mesh are referenced by a face.\n\n        Returns\n        -------------\n        referenced : (len(self.vertices),) bool\n          Which vertices are referenced by a face\n        \"\"\"\n        referenced = np.zeros(len(self.vertices), dtype=np.bool)\n        referenced[self.faces] = True\n        return referenced"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_units(self, desired, guess=False):\n        units._convert_units(self, desired, guess)\n        return self", "response": "Convert the units of the mesh into a specified unit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef merge_vertices(self, digits=None, textured=True):\n        grouping.merge_vertices(self,\n                                digits=digits,\n                                textured=textured)", "response": "Merge vertices of a mesh with other vertices."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_vertices(self, mask, inverse=None):\n        # if the mesh is already empty we can't remove anything\n        if self.is_empty:\n            return\n\n        # make sure mask is a numpy array\n        mask = np.asanyarray(mask)\n\n        if ((mask.dtype.name == 'bool' and mask.all()) or\n                len(mask) == 0 or self.is_empty):\n            # mask doesn't remove any vertices so exit early\n            return\n\n        # create the inverse mask if not passed\n        if inverse is None:\n            inverse = np.zeros(len(self.vertices), dtype=np.int64)\n            if mask.dtype.kind == 'b':\n                inverse[mask] = np.arange(mask.sum())\n            elif mask.dtype.kind == 'i':\n                inverse[mask] = np.arange(len(mask))\n            else:\n                inverse = None\n\n        # re- index faces from inverse\n        if inverse is not None and util.is_shape(self.faces, (-1, 3)):\n            self.faces = inverse[self.faces.reshape(-1)].reshape((-1, 3))\n\n        # update the visual object with our mask\n        self.visual.update_vertices(mask)\n        # get the normals from cache before dumping\n        cached_normals = self._cache['vertex_normals']\n\n        # actually apply the mask\n        self.vertices = self.vertices[mask]\n\n        # if we had passed vertex normals try to save them\n        if util.is_shape(cached_normals, (-1, 3)):\n            try:\n                self.vertex_normals = cached_normals[mask]\n            except BaseException:\n                pass", "response": "Update the vertices of the current object with a mask."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the faces of the object with the faces in the given mask.", "response": "def update_faces(self, mask):\n        \"\"\"\n        In many cases, we will want to remove specific faces.\n        However, there is additional bookkeeping to do this cleanly.\n        This function updates the set of faces with a validity mask,\n        as well as keeping track of normals and colors.\n\n        Parameters\n        ---------\n        valid : (m) int or (len(self.faces)) bool\n          Mask to remove faces\n        \"\"\"\n        # if the mesh is already empty we can't remove anything\n        if self.is_empty:\n            return\n\n        mask = np.asanyarray(mask)\n        if mask.dtype.name == 'bool' and mask.all():\n            # mask removes no faces so exit early\n            return\n\n        # try to save face normals before dumping cache\n        cached_normals = self._cache['face_normals']\n\n        faces = self._data['faces']\n        # if Trimesh has been subclassed and faces have been moved from data\n        # to cache, get faces from cache.\n        if not util.is_shape(faces, (-1, 3)):\n            faces = self._cache['faces']\n\n        # actually apply the mask\n        self.faces = faces[mask]\n        # apply the mask to the visual object\n        self.visual.update_faces(mask)\n\n        # if our normals were the correct shape apply them\n        if util.is_shape(cached_normals, (-1, 3)):\n            self.face_normals = cached_normals[mask]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nensure that every vertex and face consists of finite numbers.", "response": "def remove_infinite_values(self):\n        \"\"\"\n        Ensure that every vertex and face consists of finite numbers.\n\n        This will remove vertices or faces containing np.nan and np.inf\n\n        Alters\n        ----------\n        self.faces : masked to remove np.inf/np.nan\n        self.vertices : masked to remove np.inf/np.nan\n        \"\"\"\n        if util.is_shape(self.faces, (-1, 3)):\n            # (len(self.faces),) bool, mask for faces\n            face_mask = np.isfinite(self.faces).all(axis=1)\n            self.update_faces(face_mask)\n\n        if util.is_shape(self.vertices, (-1, 3)):\n            # (len(self.vertices),) bool, mask for vertices\n            vertex_mask = np.isfinite(self.vertices).all(axis=1)\n            self.update_vertices(vertex_mask)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove_duplicate_faces(self):\n        unique, inverse = grouping.unique_rows(np.sort(self.faces, axis=1))\n        self.update_faces(unique)", "response": "Remove any duplicate faces from the current mesh."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef split(self, only_watertight=True, adjacency=None, **kwargs):\n        meshes = graph.split(self,\n                             only_watertight=only_watertight,\n                             adjacency=adjacency,\n                             **kwargs)\n        return meshes", "response": "Splits the Trimesh object into individual components."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of faces which share an edge.", "response": "def face_adjacency(self):\n        \"\"\"\n        Find faces that share an edge, which we call here 'adjacent'.\n\n        Returns\n        ----------\n        adjacency : (n,2) int\n          Pairs of faces which share an edge\n\n        Examples\n        ---------\n\n        In [1]: mesh = trimesh.load('models/featuretype.STL')\n\n        In [2]: mesh.face_adjacency\n        Out[2]:\n        array([[   0,    1],\n               [   2,    3],\n               [   0,    3],\n               ...,\n               [1112,  949],\n               [3467, 3475],\n               [1113, 3475]])\n\n        In [3]: mesh.faces[mesh.face_adjacency[0]]\n        Out[3]:\n        TrackedArray([[   1,    0,  408],\n                      [1239,    0,    1]], dtype=int64)\n\n        In [4]: import networkx as nx\n\n        In [5]: graph = nx.from_edgelist(mesh.face_adjacency)\n\n        In [6]: groups = nx.connected_components(graph)\n        \"\"\"\n        adjacency, edges = graph.face_adjacency(mesh=self,\n                                                return_edges=True)\n        self._cache['face_adjacency_edges'] = edges\n        return adjacency"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the angle between adjacent faces", "response": "def face_adjacency_angles(self):\n        \"\"\"\n        Return the angle between adjacent faces\n\n        Returns\n        --------\n        adjacency_angle : (n,) float\n          Angle between adjacent faces\n          Each value corresponds with self.face_adjacency\n        \"\"\"\n        pairs = self.face_normals[self.face_adjacency]\n        angles = geometry.vector_angle(pairs)\n        return angles"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef face_adjacency_radius(self):\n        radii, span = graph.face_adjacency_radius(mesh=self)\n        self._cache['face_adjacency_span'] = span\n        return radii", "response": "Returns the approximate radius of a cylinder that fits inside adjacent faces."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of vertex neighbors of each vertex in the mesh.", "response": "def vertex_neighbors(self):\n        \"\"\"\n        The vertex neighbors of each vertex of the mesh, determined from\n        the cached vertex_adjacency_graph, if already existent.\n\n        Returns\n        ----------\n        vertex_neighbors : (len(self.vertices),) int\n          Represents immediate neighbors of each vertex along\n          the edge of a triangle\n\n        Examples\n        ----------\n        This is useful for getting nearby vertices for a given vertex,\n        potentially for some simple smoothing techniques.\n\n        >>> mesh = trimesh.primitives.Box()\n        >>> mesh.vertex_neighbors[0]\n        [1,2,3,4]\n        \"\"\"\n        graph = self.vertex_adjacency_graph\n        neighbors = [list(graph.neighbors(i)) for\n                     i in range(len(self.vertices))]\n        return np.array(neighbors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_winding_consistent(self):\n        if self.is_empty:\n            return False\n        # consistent winding check is populated into the cache by is_watertight\n        populate = self.is_watertight\n        return self._cache['is_winding_consistent']", "response": "Returns True if the mesh has consistent winding or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_watertight(self):\n        if self.is_empty:\n            return False\n        watertight, winding = graph.is_watertight(\n            edges=self.edges, edges_sorted=self.edges_sorted)\n        self._cache['is_winding_consistent'] = winding\n        return watertight", "response": "Check if a mesh is watertight by making sure every edge is included in two faces."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if a mesh represent a valid volume.", "response": "def is_volume(self):\n        \"\"\"\n        Check if a mesh has all the properties required to represent\n        a valid volume, rather than just a surface.\n\n        These properties include being watertight, having consistent\n        winding and outward facing normals.\n\n        Returns\n        ---------\n        valid : bool\n          Does the mesh represent a volume\n        \"\"\"\n        valid = bool(self.is_watertight and\n                     self.is_winding_consistent and\n                     np.isfinite(self.center_mass).all() and\n                     self.volume > 0.0)\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a mesh is convex or not.", "response": "def is_convex(self):\n        \"\"\"\n        Check if a mesh is convex or not.\n\n        Returns\n        ----------\n        is_convex: bool\n          Is mesh convex or not\n        \"\"\"\n        if self.is_empty:\n            return False\n\n        is_convex = bool(convex.is_convex(self))\n        return is_convex"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kdtree(self):\n\n        from scipy.spatial import cKDTree as KDTree\n        tree = KDTree(self.vertices.view(np.ndarray))\n        return tree", "response": "Return a scipy. spatial. cKDTree containing the vertices of the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nremoving all degenerate faces from the current mesh.", "response": "def remove_degenerate_faces(self, height=tol.merge):\n        \"\"\"\n        Remove degenerate faces (faces without 3 unique vertex indices)\n        from the current mesh.\n\n        If a height is specified, it will remove any face with a 2D oriented\n        bounding box with one edge shorter than that height.\n\n        If not specified, it will remove any face with a zero normal.\n\n        Parameters\n        ------------\n        height : float\n          If specified removes faces with an oriented bounding\n          box shorter than this on one side.\n\n        Returns\n        -------------\n        nondegenerate : (len(self.faces),) bool\n          Mask used to remove faces\n        \"\"\"\n        nondegenerate = triangles.nondegenerate(self.triangles,\n                                                areas=self.area_faces,\n                                                height=height)\n        self.update_faces(nondegenerate)\n\n        return nondegenerate"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an array containing the area of each facet in each group of faces represented by this cache.", "response": "def facets_area(self):\n        \"\"\"\n        Return an array containing the area of each facet.\n\n        Returns\n        ---------\n        area : (len(self.facets),) float\n          Total area of each facet (group of faces)\n        \"\"\"\n        # avoid thrashing the cache inside a loop\n        area_faces = self.area_faces\n        # sum the area of each group of faces represented by facets\n        # use native python sum in tight loop as opposed to array.sum()\n        # as in this case the lower function call overhead of\n        # native sum provides roughly a 50% speedup\n        areas = np.array([sum(area_faces[i])\n                          for i in self.facets],\n                         dtype=np.float64)\n        return areas"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the normal vector of each facet in the cache", "response": "def facets_normal(self):\n        \"\"\"\n        Return the normal of each facet\n\n        Returns\n        ---------\n        normals: (len(self.facets), 3) float\n          A unit normal vector for each facet\n        \"\"\"\n        if len(self.facets) == 0:\n            return np.array([])\n\n        area_faces = self.area_faces\n        # sum the area of each group of faces represented by facets\n\n        # the face index of the first face in each facet\n        index = np.array([i[area_faces[i].argmax()]\n                          for i in self.facets])\n        # (n,3) float, unit normal vectors of facet plane\n        normals = self.face_normals[index]\n        # (n,3) float, points on facet plane\n        origins = self.vertices[self.faces[:, 0][index]]\n        # save origins in cache\n        self._cache['facets_origin'] = origins\n\n        return normals"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef facets_boundary(self):\n        # make each row correspond to a single face\n        edges = self.edges_sorted.reshape((-1, 6))\n        # get the edges for each facet\n        edges_facet = [edges[i].reshape((-1, 2)) for i in self.facets]\n        edges_boundary = np.array([i[grouping.group_rows(i, require_count=1)]\n                                   for i in edges_facet])\n        return edges_boundary", "response": "Return the edges which represent the boundary of each facet\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef facets_on_hull(self):\n        # facets plane, origin and normal\n        normals = self.facets_normal\n        origins = self.facets_origin\n\n        # (n,3) convex hull vertices\n        convex = self.convex_hull.vertices.view(np.ndarray).copy()\n\n        # boolean mask for which facets are on convex hull\n        on_hull = np.zeros(len(self.facets), dtype=np.bool)\n\n        for i, normal, origin in zip(range(len(normals)), normals, origins):\n            # a facet plane is on the convex hull if every vertex\n            # of the convex hull is behind that plane\n            # which we are checking with dot products\n            dot = np.dot(normal, (convex - origin).T)\n            on_hull[i] = (dot < tol.merge).all()\n\n        return on_hull", "response": "Find which facets of the mesh are on the convex hull."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfixing normals in the current set of faces and faces.", "response": "def fix_normals(self, multibody=None):\n        \"\"\"\n        Find and fix problems with self.face_normals and self.faces\n        winding direction.\n\n        For face normals ensure that vectors are consistently pointed\n        outwards, and that self.faces is wound in the correct direction\n        for all connected components.\n\n        Parameters\n        -------------\n        multibody : None or bool\n          Fix normals across multiple bodies\n          if None automatically pick from body_count\n        \"\"\"\n        if multibody is None:\n            multibody = self.body_count > 1\n        repair.fix_normals(self, multibody=multibody)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(self, other, **kwargs):\n        mesh_to_other, cost = registration.mesh_other(mesh=self,\n                                                      other=other,\n                                                      **kwargs)\n        return mesh_to_other, cost", "response": "This function is used to align a mesh with another mesh or a PointCloud."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes stable orientations of a mesh and their quasi-static probabilites. This method samples the location of the center of mass from a multivariate gaussian (mean at com, cov equal to identity times sigma) over n_samples. For each sample, it computes the stable resting poses of the mesh on a a planar workspace and evaulates the probabilities of landing in each pose if the object is dropped onto the table randomly. This method returns the 4x4 homogenous transform matrices that place the shape against the planar surface with the z-axis pointing upwards and a list of the probabilities for each pose. The transforms and probabilties that are returned are sorted, with the most probable pose first. Parameters ---------- center_mass : (3,) float The object center of mass (if None, this method assumes uniform density and watertightness and computes a center of mass explicitly) sigma : float The covariance for the multivariate gaussian used to sample center of mass locations n_samples : int The number of samples of the center of mass location threshold : float The probability value at which to threshold returned stable poses Returns ------- transforms : (n, 4, 4) float The homogenous matrices that transform the object to rest in a stable pose, with the new z-axis pointing upwards from the table and the object just touching the table. probs : (n,) float A probability ranging from 0.0 to 1.0 for each pose", "response": "def compute_stable_poses(self,\n                             center_mass=None,\n                             sigma=0.0,\n                             n_samples=1,\n                             threshold=0.0):\n        \"\"\"\n        Computes stable orientations of a mesh and their quasi-static probabilites.\n\n        This method samples the location of the center of mass from a multivariate\n        gaussian (mean at com, cov equal to identity times sigma) over n_samples.\n        For each sample, it computes the stable resting poses of the mesh on a\n        a planar workspace and evaulates the probabilities of landing in\n        each pose if the object is dropped onto the table randomly.\n\n        This method returns the 4x4 homogenous transform matrices that place\n        the shape against the planar surface with the z-axis pointing upwards\n        and a list of the probabilities for each pose.\n        The transforms and probabilties that are returned are sorted, with the\n        most probable pose first.\n\n        Parameters\n        ----------\n        center_mass : (3,) float\n          The object center of mass (if None, this method\n          assumes uniform density and watertightness and\n          computes a center of mass explicitly)\n        sigma : float\n          The covariance for the multivariate gaussian used\n          to sample center of mass locations\n        n_samples : int\n          The number of samples of the center of mass location\n        threshold : float\n          The probability value at which to threshold\n          returned stable poses\n\n        Returns\n        -------\n        transforms : (n, 4, 4) float\n          The homogenous matrices that transform the\n          object to rest in a stable pose, with the\n          new z-axis pointing upwards from the table\n          and the object just touching the table.\n\n        probs : (n,) float\n          A probability ranging from 0.0 to 1.0 for each pose\n        \"\"\"\n        return poses.compute_stable_poses(mesh=self,\n                                          center_mass=center_mass,\n                                          sigma=sigma,\n                                          n_samples=n_samples,\n                                          threshold=threshold)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef subdivide(self, face_index=None):\n        vertices, faces = remesh.subdivide(vertices=self.vertices,\n                                           faces=self.faces,\n                                           face_index=face_index)\n        return Trimesh(vertices=vertices, faces=faces)", "response": "Subdivide a mesh with each subdivided face replaced with four\n        smaller faces."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a version of the current mesh with smooth shading.", "response": "def smoothed(self, angle=.4):\n        \"\"\"\n        Return a version of the current mesh which will render\n        nicely, without changing source mesh.\n\n        Parameters\n        -------------\n        angle : float\n          Angle in radians, face pairs with angles smaller than\n          this value will appear smoothed\n\n        Returns\n        ---------\n        smoothed : trimesh.Trimesh\n          Non watertight version of current mesh\n          which will render nicely with smooth shading\n        \"\"\"\n\n        # smooth should be recomputed if visuals change\n        self.visual._verify_crc()\n        cached = self.visual._cache['smoothed']\n        if cached is not None:\n            return cached\n\n        smoothed = graph.smoothed(self, angle)\n        self.visual._cache['smoothed'] = smoothed\n        return smoothed"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef section(self,\n                plane_normal,\n                plane_origin):\n        \"\"\"\n        Returns a 3D cross section of the current mesh and a plane\n        defined by origin and normal.\n\n        Parameters\n        ---------\n        plane_normal: (3) vector for plane normal\n          Normal vector of section plane\n        plane_origin : (3,) float\n          Point on the cross section plane\n\n        Returns\n        ---------\n        intersections: Path3D or None\n          Curve of intersection\n        \"\"\"\n        # turn line segments into Path2D/Path3D objects\n        from .exchange.load import load_path\n\n        # return a single cross section in 3D\n        lines, face_index = intersections.mesh_plane(\n            mesh=self,\n            plane_normal=plane_normal,\n            plane_origin=plane_origin,\n            return_faces=True)\n\n        # if the section didn't hit the mesh return None\n        if len(lines) == 0:\n            return None\n\n        # otherwise load the line segments into a Path3D object\n        path = load_path(lines)\n\n        # add the face index info into metadata\n        path.metadata['face_index'] = face_index\n\n        return path", "response": "Returns a 3D cross section of the current mesh and a plane defined by origin and normal."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn multiple parallel cross sections of the current mesh in 2D.", "response": "def section_multiplane(self,\n                           plane_origin,\n                           plane_normal,\n                           heights):\n        \"\"\"\n        Return multiple parallel cross sections of the current\n        mesh in 2D.\n\n        Parameters\n        ---------\n        plane_normal: (3) vector for plane normal\n          Normal vector of section plane\n        plane_origin : (3,) float\n          Point on the cross section plane\n        heights : (n,) float\n          Each section is offset by height along\n          the plane normal.\n\n        Returns\n        ---------\n        paths : (n,) Path2D or None\n          2D cross sections at specified heights.\n          path.metadata['to_3D'] contains transform\n          to return 2D section back into 3D space.\n        \"\"\"\n        # turn line segments into Path2D/Path3D objects\n        from .exchange.load import load_path\n        # do a multiplane intersection\n        lines, transforms, faces = intersections.mesh_multiplane(\n            mesh=self,\n            plane_normal=plane_normal,\n            plane_origin=plane_origin,\n            heights=heights)\n\n        # turn the line segments into Path2D objects\n        paths = [None] * len(lines)\n        for index, L, T in zip(range(len(lines)),\n                               lines,\n                               transforms):\n            if len(L) > 0:\n                paths[index] = load_path(\n                    L, metadata={'to_3D': T})\n        return paths"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef slice_plane(self,\n                    plane_origin,\n                    plane_normal,\n                    **kwargs):\n        \"\"\"\n        Returns another mesh that is the current mesh\n        sliced by the plane defined by origin and normal.\n\n        Parameters\n        ---------\n        plane_normal: (3) vector for plane normal\n          Normal vector of slicing plane\n        plane_origin : (3,) float\n          Point on the slicing plane\n\n        Returns\n        ---------\n        new_mesh: trimesh.Trimesh or None\n          Subset of current mesh sliced by plane\n        \"\"\"\n\n        # return a new mesh\n        new_mesh = intersections.slice_mesh_plane(\n            mesh=self,\n            plane_normal=plane_normal,\n            plane_origin=plane_origin,\n            **kwargs)\n\n        return new_mesh", "response": "Returns another mesh that is the current mesh sliced by the plane defined by origin and normal."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(self, count, return_index=False):\n        samples, index = sample.sample_surface(self, count)\n        if return_index:\n            return samples, index\n        return samples", "response": "Return random samples distributed normally across the base mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves all vertices in the current mesh which are not referenced by a face.", "response": "def remove_unreferenced_vertices(self):\n        \"\"\"\n        Remove all vertices in the current mesh which are not\n        referenced by a face.\n        \"\"\"\n        referenced = np.zeros(len(self.vertices), dtype=np.bool)\n        referenced[self.faces] = True\n\n        inverse = np.zeros(len(self.vertices), dtype=np.int64)\n        inverse[referenced] = np.arange(referenced.sum())\n\n        self.update_vertices(mask=referenced, inverse=inverse)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves all face references so that every vertex is unique and every face is unique.", "response": "def unmerge_vertices(self):\n        \"\"\"\n        Removes all face references so that every face contains\n        three unique vertex indices and no faces are adjacent.\n        \"\"\"\n        # new faces are incrementing so every vertex is unique\n        faces = np.arange(len(self.faces) * 3,\n                          dtype=np.int64).reshape((-1, 3))\n\n        # use update_vertices to apply mask to\n        # all properties that are per-vertex\n        self.update_vertices(self.faces.reshape(-1))\n        # set faces to incrementing indexes\n        self.faces = faces\n        # keep face normals as the haven't changed\n        self._cache.clear(exclude=['face_normals'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\napplies the oriented bounding box transform to the current mesh.", "response": "def apply_obb(self):\n        \"\"\"\n        Apply the oriented bounding box transform to the current mesh.\n\n        This will result in a mesh with an AABB centered at the\n        origin and the same dimensions as the OBB.\n\n        Returns\n        ----------\n        matrix : (4, 4) float\n          Transformation matrix that was applied\n          to mesh to move it into OBB frame\n        \"\"\"\n        matrix = self.bounding_box_oriented.primitive.transform\n        matrix = np.linalg.inv(matrix)\n        self.apply_transform(matrix)\n        return matrix"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_transform(self, matrix):\n        # get c-order float64 matrix\n        matrix = np.asanyarray(matrix,\n                               order='C',\n                               dtype=np.float64)\n\n        # only support homogenous transformations\n        if matrix.shape != (4, 4):\n            raise ValueError('Transformation matrix must be (4,4)!')\n\n        # exit early if we've been passed an identity matrix\n        # np.allclose is surprisingly slow so do this test\n        elif np.abs(matrix - np.eye(4)).max() < 1e-8:\n            log.debug('apply_tranform passed identity matrix')\n            return\n\n        # new vertex positions\n        new_vertices = transformations.transform_points(\n            self.vertices,\n            matrix=matrix)\n\n        # overridden center of mass\n        if self._center_mass is not None:\n            self._center_mass = transformations.transform_points(\n                np.array([self._center_mass, ]),\n                matrix)[0]\n\n        # preserve face normals if we have them stored\n        new_face_normals = None\n        if 'face_normals' in self._cache:\n            # transform face normals by rotation component\n            new_face_normals = util.unitize(\n                transformations.transform_points(\n                    self.face_normals,\n                    matrix=matrix,\n                    translate=False))\n\n        # preserve vertex normals if we have them stored\n        new_vertex_normals = None\n        if 'vertex_normals' in self._cache:\n            new_vertex_normals = util.unitize(\n                transformations.transform_points(\n                    self.vertex_normals,\n                    matrix=matrix,\n                    translate=False))\n\n        # a test triangle pre and post transform\n        triangle_pre = self.vertices[self.faces[:5]]\n        # we don't care about scale so make sure they aren't tiny\n        triangle_pre /= np.abs(triangle_pre).max()\n\n        # do the same for the post- transform test\n        triangle_post = new_vertices[self.faces[:5]]\n        triangle_post /= np.abs(triangle_post).max()\n\n        # compute triangle normal before and after transform\n        normal_pre, valid_pre = triangles.normals(triangle_pre)\n        normal_post, valid_post = triangles.normals(triangle_post)\n\n        # check the first few faces against normals to check winding\n        aligned_pre = triangles.windings_aligned(triangle_pre[valid_pre],\n                                                 normal_pre)\n        # windings aligned after applying transform\n        aligned_post = triangles.windings_aligned(triangle_post[valid_post],\n                                                  normal_post)\n\n        # convert multiple face checks to single bool, allowing outliers\n        pre = (aligned_pre.sum() / float(len(aligned_pre))) > .6\n        post = (aligned_post.sum() / float(len(aligned_post))) > .6\n\n        if pre != post:\n            log.debug('transform flips winding')\n            # fliplr will make array non C contiguous, which will\n            # cause hashes to be more expensive than necessary\n            self.faces = np.ascontiguousarray(np.fliplr(self.faces))\n\n        # assign the new values\n        self.vertices = new_vertices\n        # may be None if we didn't have them previously\n        self.face_normals = new_face_normals\n        self.vertex_normals = new_vertex_normals\n\n        # preserve normals and topology in cache\n        # while dumping everything else\n        self._cache.clear(exclude=[\n            'face_normals',   # transformed by us\n            'face_adjacency',  # topological\n            'face_adjacency_edges',\n            'face_adjacency_unshared',\n            'edges',\n            'edges_sorted',\n            'edges_unique',\n            'edges_sparse',\n            'body_count',\n            'faces_unique_edges',\n            'euler_number',\n            'vertex_normals'])\n        # set the cache ID with the current hash value\n        self._cache.id_set()\n\n        log.debug('mesh transformed by matrix')\n        return self", "response": "Apply a homogenous transformation matrix to the current object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Voxel object representing the current mesh discretized into voxels at the specified pitch.", "response": "def voxelized(self, pitch, **kwargs):\n        \"\"\"\n        Return a Voxel object representing the current mesh\n        discretized into voxels at the specified pitch\n\n        Parameters\n        ----------\n        pitch : float\n          The edge length of a single voxel\n\n        Returns\n        ----------\n        voxelized : Voxel object\n          Representing the current mesh\n        \"\"\"\n        voxelized = voxel.VoxelMesh(self,\n                                    pitch=pitch,\n                                    **kwargs)\n        return voxelized"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a list of faces find the outline of those faces and return the path in 3D.", "response": "def outline(self, face_ids=None, **kwargs):\n        \"\"\"\n        Given a list of face indexes find the outline of those\n        faces and return it as a Path3D.\n\n        The outline is defined here as every edge which is only\n        included by a single triangle.\n\n        Note that this implies a non-watertight mesh as the\n        outline of a watertight mesh is an empty path.\n\n        Parameters\n        ----------\n        face_ids : (n,) int\n          Indices to compute the outline of.\n          If None, outline of full mesh will be computed.\n        **kwargs: passed to Path3D constructor\n\n        Returns\n        ----------\n        path : Path3D\n          Curve in 3D of the outline\n        \"\"\"\n        from .path.exchange.misc import faces_to_path\n        from .path.exchange.load import _create_path\n\n        path = _create_path(**faces_to_path(self,\n                                            face_ids,\n                                            **kwargs))\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef area_faces(self):\n        area_faces = triangles.area(crosses=self.triangles_cross,\n                                    sum=False)\n        return area_faces", "response": "Returns the area of each face in the mesh."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the mass properties of the current mesh.", "response": "def mass_properties(self):\n        \"\"\"\n        Returns the mass properties of the current mesh.\n\n        Assumes uniform density, and result is probably garbage if mesh\n        isn't watertight.\n\n        Returns\n        ----------\n        properties : dict\n          With keys:\n          'volume'      : in global units^3\n          'mass'        : From specified density\n          'density'     : Included again for convenience (same as kwarg density)\n          'inertia'     : Taken at the center of mass and aligned with global\n                         coordinate system\n          'center_mass' : Center of mass location, in global coordinate system\n        \"\"\"\n        mass = triangles.mass_properties(triangles=self.triangles,\n                                         crosses=self.triangles_cross,\n                                         density=self._density,\n                                         center_mass=self._center_mass,\n                                         skip_inertia=False)\n\n        # if magical clean- up mode is enabled\n        # and mesh is watertight/wound correctly but with negative\n        # volume it means that every triangle is probably facing\n        # inwards, so we invert it in- place without dumping cache\n        if (self._validate and\n            self.is_watertight and\n            self.is_winding_consistent and\n            np.linalg.det(mass['inertia']) < 0.0 and\n            mass['mass'] < 0.0 and\n                mass['volume'] < 0.0):\n\n            # negate mass properties so we don't need to recalculate\n            mass['inertia'] = -mass['inertia']\n            mass['mass'] = -mass['mass']\n            mass['volume'] = -mass['volume']\n            # invert the faces and normals of the mesh\n            self.invert()\n        return mass"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef invert(self):\n        with self._cache:\n            if 'face_normals' in self._cache:\n                self.face_normals *= -1.0\n            if 'vertex_normals' in self._cache:\n                self.vertex_normals *= -1.0\n            self.faces = np.fliplr(self.faces)\n        # save our normals\n        self._cache.clear(exclude=['face_normals',\n                                   'vertex_normals'])", "response": "Invert the mesh in - place by reversing the winding of every atom face and negating normals."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a subset of the mesh.", "response": "def submesh(self, faces_sequence, **kwargs):\n        \"\"\"\n        Return a subset of the mesh.\n\n        Parameters\n        ----------\n        faces_sequence : sequence (m,) int\n          Face indices of mesh\n        only_watertight : bool\n          Only return submeshes which are watertight\n        append : bool\n          Return a single mesh which has the faces appended.\n          if this flag is set, only_watertight is ignored\n\n        Returns\n        ---------\n        if append : trimesh.Trimesh object\n        else :      list of trimesh.Trimesh objects\n        \"\"\"\n        return util.submesh(mesh=self,\n                            faces_sequence=faces_sequence,\n                            **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports the current mesh to a file object.", "response": "def export(self, file_obj=None, file_type=None, **kwargs):\n        \"\"\"\n        Export the current mesh to a file object.\n        If file_obj is a filename, file will be written there.\n\n        Supported formats are stl, off, ply, collada, json, dict, glb,\n        dict64, msgpack.\n\n        Parameters\n        ---------\n        file_obj: open writeable file object\n          str, file name where to save the mesh\n          None, if you would like this function to return the export blob\n        file_type: str\n          Which file type to export as.\n          If file name is passed this is not required\n        \"\"\"\n        return export_mesh(mesh=self,\n                           file_obj=file_obj,\n                           file_type=file_type,\n                           **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes an approximate convex decomposition of a mesh.", "response": "def convex_decomposition(self, maxhulls=20, **kwargs):\n        \"\"\"\n        Compute an approximate convex decomposition of a mesh.\n\n        testVHACD Parameters which can be passed as kwargs:\n\n        Name                                        Default\n        -----------------------------------------------------\n        resolution                                  100000\n        max. concavity                              0.001\n        plane down-sampling                         4\n        convex-hull down-sampling                   4\n        alpha                                       0.05\n        beta                                        0.05\n        maxhulls                                    10\n        pca                                         0\n        mode                                        0\n        max. vertices per convex-hull               64\n        min. volume to add vertices to convex-hulls 0.0001\n        convex-hull approximation                   1\n        OpenCL acceleration                         1\n        OpenCL platform ID                          0\n        OpenCL device ID                            0\n        output                                      output.wrl\n        log                                         log.txt\n\n\n        Parameters\n        ----------\n        maxhulls :  int\n          Maximum number of convex hulls to return\n        **kwargs :  testVHACD keyword arguments\n\n        Returns\n        -------\n        meshes : list of trimesh.Trimesh\n          List of convex meshes that approximate the original\n        \"\"\"\n        result = decomposition.convex_decomposition(self,\n                                                    maxhulls=maxhulls,\n                                                    **kwargs)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a boolean intersection between this mesh and n other meshes", "response": "def intersection(self, other, engine=None):\n        \"\"\"\n        Boolean intersection between this mesh and n other meshes\n\n        Parameters\n        ---------\n        other : trimesh.Trimesh, or list of trimesh.Trimesh objects\n          Meshes to calculate intersections with\n\n        Returns\n        ---------\n        intersection : trimesh.Trimesh\n          Mesh of the volume contained by all passed meshes\n        \"\"\"\n        result = boolean.intersection(meshes=np.append(self, other),\n                                      engine=engine)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining whether or not each point is inside the mesh.", "response": "def contains(self, points):\n        \"\"\"\n        Given a set of points, determine whether or not they are inside the mesh.\n        This raises an error if called on a non- watertight mesh.\n\n        Parameters\n        ---------\n        points : (n, 3) float\n          Points in cartesian space\n\n        Returns\n        ---------\n        contains : (n, ) bool\n          Whether or not each point is inside the mesh\n        \"\"\"\n        if not self.is_watertight:\n            log.warning('Mesh is non- watertight for contained point query!')\n        contains = self.ray.contains_points(points)\n        return contains"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a deep copy of the current mesh.", "response": "def copy(self):\n        \"\"\"\n        Safely get a copy of the current mesh.\n\n        Copied objects will have emptied caches to avoid memory\n        issues and so may be slow on initial operations until\n        caches are regenerated.\n\n        Current object will *not* have its cache cleared.\n\n        Returns\n        ---------\n        copied : trimesh.Trimesh\n          Copy of current mesh\n        \"\"\"\n        copied = Trimesh()\n\n        # copy vertex and face data\n        copied._data.data = copy.deepcopy(self._data.data)\n        # copy visual information\n        copied.visual = self.visual.copy()\n        # get metadata\n        copied.metadata = copy.deepcopy(self.metadata)\n        # get center_mass and density\n        if self._center_mass is not None:\n            copied.center_mass = self.center_mass\n        copied._density = self._density\n\n        # make sure cache is set from here\n        copied._cache.clear()\n\n        return copied"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef eval_cached(self, statement, *args):\n\n        statement = str(statement)\n        key = 'eval_cached_' + statement\n        key += '_'.join(str(i) for i in args)\n\n        if key in self._cache:\n            return self._cache[key]\n\n        result = eval(statement)\n        self._cache[key] = result\n        return result", "response": "Evaluate a statement and cache the result before returning."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntraverse and change the winding of a Trimesh object to make sure that the winding of the object is correct.", "response": "def fix_winding(mesh):\n    \"\"\"\n    Traverse and change mesh faces in-place to make sure winding\n    is correct, with edges on adjacent faces in\n    opposite directions.\n\n    Parameters\n    -------------\n    mesh: Trimesh object\n\n    Alters\n    -------------\n    mesh.face: will reverse columns of certain faces\n    \"\"\"\n    # anything we would fix is already done\n    if mesh.is_winding_consistent:\n        return\n\n    graph_all = nx.from_edgelist(mesh.face_adjacency)\n    flipped = 0\n\n    faces = mesh.faces.view(np.ndarray).copy()\n\n    # we are going to traverse the graph using BFS\n    # start a traversal for every connected component\n    for components in nx.connected_components(graph_all):\n        # get a subgraph for this component\n        g = graph_all.subgraph(components)\n        # get the first node in the graph in a way that works on nx's\n        # new API and their old API\n        start = next(iter(g.nodes()))\n\n        # we traverse every pair of faces in the graph\n        # we modify mesh.faces and mesh.face_normals in place\n        for face_pair in nx.bfs_edges(g, start):\n            # for each pair of faces, we convert them into edges,\n            # find the edge that both faces share and then see if edges\n            # are reversed in order as you would expect\n            # (2, ) int\n            face_pair = np.ravel(face_pair)\n            # (2, 3) int\n            pair = faces[face_pair]\n            # (6, 2) int\n            edges = faces_to_edges(pair)\n            overlap = group_rows(np.sort(edges, axis=1),\n                                 require_count=2)\n            if len(overlap) == 0:\n                # only happens on non-watertight meshes\n                continue\n            edge_pair = edges[overlap[0]]\n            if edge_pair[0][0] == edge_pair[1][0]:\n                # if the edges aren't reversed, invert the order of one face\n                flipped += 1\n                faces[face_pair[1]] = faces[face_pair[1]][::-1]\n\n    if flipped > 0:\n        mesh.faces = faces\n\n    log.debug('flipped %d/%d edges', flipped, len(mesh.faces) * 3)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fix_inversion(mesh, multibody=False):\n    if multibody:\n        groups = graph.connected_components(mesh.face_adjacency)\n        # escape early for single body\n        if len(groups) == 1:\n            if mesh.volume < 0.0:\n                mesh.invert()\n            return\n        # mask of faces to flip\n        flip = np.zeros(len(mesh.faces), dtype=np.bool)\n        # save these to avoid thrashing cache\n        tri = mesh.triangles\n        cross = mesh.triangles_cross\n        # indexes of mesh.faces, not actual faces\n        for faces in groups:\n            # calculate the volume of the submesh faces\n            volume = triangles.mass_properties(\n                tri[faces],\n                crosses=cross[faces],\n                skip_inertia=True)['volume']\n            # if that volume is negative it is either\n            # inverted or just total garbage\n            if volume < 0.0:\n                flip[faces] = True\n        # one or more faces needs flipping\n        if flip.any():\n            with mesh._cache:\n                # flip normals of necessary faces\n                if 'face_normals' in mesh._cache:\n                    mesh.face_normals[flip] *= -1.0\n                # flip faces\n                mesh.faces[flip] = np.fliplr(mesh.faces[flip])\n            # save wangled normals\n            mesh._cache.clear(exclude=['face_normals'])\n\n    elif mesh.volume < 0.0:\n        # reverse every triangles and flip every normals\n        mesh.invert()", "response": "Fix inversion of a mesh."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fix_normals(mesh, multibody=False):\n    # traverse face adjacency to correct winding\n    fix_winding(mesh)\n    # check to see if a mesh is inverted\n    fix_inversion(mesh, multibody=multibody)", "response": "Fix the winding and direction of a mesh face and face normals in - place."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef broken_faces(mesh, color=None):\n    adjacency = nx.from_edgelist(mesh.face_adjacency)\n    broken = [k for k, v in dict(adjacency.degree()).items()\n              if v != 3]\n    broken = np.array(broken)\n    if color is not None:\n        # if someone passed a broken color\n        color = np.array(color)\n        if not (color.shape == (4,) or color.shape == (3,)):\n            color = [255, 0, 0, 255]\n        mesh.visual.face_colors[broken] = color\n    return broken", "response": "Returns the index of faces in the mesh which break the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfilling single - triangle holes on a mesh by adding new triangles to the new triangles and then assigning them to the new triangles.", "response": "def fill_holes(mesh):\n    \"\"\"\n    Fill single- triangle holes on triangular meshes by adding\n    new triangles to fill the holes. New triangles will have\n    proper winding and normals, and if face colors exist the color\n    of the last face will be assigned to the new triangles.\n\n    Parameters\n    ---------\n    mesh : trimesh.Trimesh\n      Mesh will be repaired in- place\n    \"\"\"\n\n    def hole_to_faces(hole):\n        \"\"\"\n        Given a loop of vertex indices  representing a hole, turn it into\n        triangular faces.\n        If unable to do so, return None\n\n        Parameters\n        ---------\n        hole:     ordered loop of vertex indices\n\n        Returns\n        ---------\n        (n, 3) new faces\n        (m, 3) new vertices\n        \"\"\"\n        hole = np.asanyarray(hole)\n        # the case where the hole is just a single missing triangle\n        if len(hole) == 3:\n            return [hole], []\n        # the hole is a quad, which we fill with two triangles\n        if len(hole) == 4:\n            face_A = hole[[0, 1, 2]]\n            face_B = hole[[2, 3, 0]]\n            return [face_A, face_B], []\n        return [], []\n\n    if len(mesh.faces) < 3:\n        return False\n\n    if mesh.is_watertight:\n        return True\n\n    # we know that in a watertight mesh every edge will be included twice\n    # thus every edge which appears only once is part of a hole boundary\n    boundary_groups = group_rows(mesh.edges_sorted, require_count=1)\n\n    if len(boundary_groups) < 3:\n        watertight = len(boundary_groups) == 0\n        return watertight\n\n    boundary_edges = mesh.edges[boundary_groups]\n    index_as_dict = [{'index': i} for i in boundary_groups]\n\n    # we create a graph of the boundary edges, and find cycles.\n    g = nx.from_edgelist(np.column_stack((boundary_edges,\n                                          index_as_dict)))\n    cycles = np.array(nx.cycle_basis(g))\n\n    new_faces = []\n    new_vertex = []\n    for hole in cycles:\n        # convert the hole, which is a polygon of vertex indices\n        # to triangles and new vertices\n        faces, vertex = hole_to_faces(hole=hole)\n        if len(faces) == 0:\n            continue\n        # remeshing returns new vertices as negative indices, so change those\n        # to absolute indices which won't be screwed up by the later appends\n        faces = np.array(faces)\n        faces[faces < 0] += len(new_vertex) + len(mesh.vertices) + len(vertex)\n        new_vertex.extend(vertex)\n        new_faces.extend(faces)\n    new_faces = np.array(new_faces)\n    new_vertex = np.array(new_vertex)\n\n    if len(new_faces) == 0:\n        # no new faces have been added, so nothing further to do\n        # the mesh is NOT watertight, as boundary groups exist\n        # but we didn't add any new faces to fill them in\n        return False\n\n    for face_index, face in enumerate(new_faces):\n        # we compare the edge from the new face with\n        # the boundary edge from the source mesh\n        edge_test = face[0:2]\n        edge_boundary = mesh.edges[g.get_edge_data(*edge_test)['index']]\n\n        # in a well construced mesh, the winding is such that adjacent triangles\n        # have reversed edges to each other. Here we check to make sure the\n        # edges are reversed, and if they aren't we simply reverse the face\n        reversed = edge_test[0] == edge_boundary[1]\n        if not reversed:\n            new_faces[face_index] = face[::-1]\n\n    # stack vertices into clean (n, 3) float\n    if len(new_vertex) != 0:\n        new_vertices = np.vstack((mesh.vertices, new_vertex))\n    else:\n        new_vertices = mesh.vertices\n\n    # try to save face normals if we can\n    if 'face_normals' in mesh._cache.cache:\n        cached_normals = mesh._cache.cache['face_normals']\n    else:\n        cached_normals = None\n\n    # also we can remove any zero are triangles by masking here\n    new_normals, valid = triangles.normals(new_vertices[new_faces])\n    # all the added faces were broken\n    if not valid.any():\n        return False\n\n    # apply the new faces and vertices\n    mesh.faces = np.vstack((mesh._data['faces'], new_faces[valid]))\n    mesh.vertices = new_vertices\n\n    # dump the cache and set id to the new hash\n    mesh._cache.verify()\n\n    # save us a normals recompute if we can\n    if cached_normals is not None:\n        mesh.face_normals = np.vstack((cached_normals,\n                                       new_normals))\n\n    # this is usually the case where two vertices of a triangle are just\n    # over tol.merge apart, but the normal calculation is screwed up\n    # these could be fixed by merging the vertices in question here:\n    # if not valid.all():\n    if mesh.visual.defined and mesh.visual.kind == 'face':\n        # if face colors exist, assign the last face color to the new faces\n        # note that this is a little cheesey, but it is very inexpensive and\n        # is the right thing to do if the mesh is a single color.\n        stored = mesh.visual._data['face_colors']\n        color_shape = np.shape(stored)\n        if len(color_shape) == 2:\n            new_colors = np.tile(stored[-1], (np.sum(valid), 1))\n            new_colors = np.vstack((stored,\n                                    new_colors))\n            mesh.visual.face_colors = new_colors\n\n    log.debug('Filled in mesh with %i triangles', np.sum(valid))\n    return mesh.is_watertight"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef mesh_plane(mesh,\n               plane_normal,\n               plane_origin,\n               return_faces=False,\n               cached_dots=None):\n    \"\"\"\n    Find a the intersections between a mesh and a plane,\n    returning a set of line segments on that plane.\n\n    Parameters\n    ---------\n    mesh : Trimesh object\n        Source mesh to slice\n    plane_normal : (3,) float\n        Normal vector of plane to intersect with mesh\n    plane_origin:  (3,) float\n        Point on plane to intersect with mesh\n    return_faces:  bool\n        If True return face index each line is from\n    cached_dots : (n, 3) float\n        If an external function has stored dot\n        products pass them here to avoid recomputing\n\n    Returns\n    ----------\n    lines :  (m, 2, 3) float\n        List of 3D line segments in space\n    face_index : (m,) int\n        Index of mesh.faces for each line\n        Only returned if return_faces was True\n    \"\"\"\n\n    def triangle_cases(signs):\n        \"\"\"\n        Figure out which faces correspond to which intersection\n        case from the signs of the dot product of each vertex.\n        Does this by bitbang each row of signs into an 8 bit\n        integer.\n\n        code : signs      : intersects\n        0    : [-1 -1 -1] : No\n        2    : [-1 -1  0] : No\n        4    : [-1 -1  1] : Yes; 2 on one side, 1 on the other\n        6    : [-1  0  0] : Yes; one edge fully on plane\n        8    : [-1  0  1] : Yes; one vertex on plane, 2 on different sides\n        12   : [-1  1  1] : Yes; 2 on one side, 1 on the other\n        14   : [0 0 0]    : No (on plane fully)\n        16   : [0 0 1]    : Yes; one edge fully on plane\n        20   : [0 1 1]    : No\n        28   : [1 1 1]    : No\n\n        Parameters\n        ----------\n        signs: (n,3) int, all values are -1,0, or 1\n               Each row contains the dot product of all three vertices\n               in a face with respect to the plane\n\n        Returns\n        ---------\n        basic:      (n,) bool, which faces are in the basic intersection case\n        one_vertex: (n,) bool, which faces are in the one vertex case\n        one_edge:   (n,) bool, which faces are in the one edge case\n        \"\"\"\n\n        signs_sorted = np.sort(signs, axis=1)\n        coded = np.zeros(len(signs_sorted), dtype=np.int8) + 14\n        for i in range(3):\n            coded += signs_sorted[:, i] << 3 - i\n\n        # one edge fully on the plane\n        # note that we are only accepting *one* of the on- edge cases,\n        # where the other vertex has a positive dot product (16) instead\n        # of both on- edge cases ([6,16])\n        # this is so that for regions that are co-planar with the the section plane\n        # we don't end up with an invalid boundary\n        key = np.zeros(29, dtype=np.bool)\n        key[16] = True\n        one_edge = key[coded]\n\n        # one vertex on plane, other two on different sides\n        key[:] = False\n        key[8] = True\n        one_vertex = key[coded]\n\n        # one vertex on one side of the plane, two on the other\n        key[:] = False\n        key[[4, 12]] = True\n        basic = key[coded]\n\n        return basic, one_vertex, one_edge\n\n    def handle_on_vertex(signs, faces, vertices):\n        # case where one vertex is on plane, two are on different sides\n        vertex_plane = faces[signs == 0]\n        edge_thru = faces[signs != 0].reshape((-1, 2))\n        point_intersect, valid = plane_lines(plane_origin,\n                                             plane_normal,\n                                             vertices[edge_thru.T],\n                                             line_segments=False)\n        lines = np.column_stack((vertices[vertex_plane[valid]],\n                                 point_intersect)).reshape((-1, 2, 3))\n        return lines\n\n    def handle_on_edge(signs, faces, vertices):\n        # case where two vertices are on the plane and one is off\n        edges = faces[signs == 0].reshape((-1, 2))\n        points = vertices[edges]\n        return points\n\n    def handle_basic(signs, faces, vertices):\n        # case where one vertex is on one side and two are on the other\n        unique_element = grouping.unique_value_in_row(\n            signs, unique=[-1, 1])\n        edges = np.column_stack(\n            (faces[unique_element],\n             faces[np.roll(unique_element, 1, axis=1)],\n             faces[unique_element],\n             faces[np.roll(unique_element, 2, axis=1)])).reshape(\n            (-1, 2))\n        intersections, valid = plane_lines(plane_origin,\n                                           plane_normal,\n                                           vertices[edges.T],\n                                           line_segments=False)\n        # since the data has been pre- culled, any invalid intersections at all\n        # means the culling was done incorrectly and thus things are\n        # mega-fucked\n        assert valid.all()\n        return intersections.reshape((-1, 2, 3))\n\n    # check input plane\n    plane_normal = np.asanyarray(plane_normal,\n                                 dtype=np.float64)\n    plane_origin = np.asanyarray(plane_origin,\n                                 dtype=np.float64)\n    if plane_origin.shape != (3,) or plane_normal.shape != (3,):\n        raise ValueError('Plane origin and normal must be (3,)!')\n\n    if cached_dots is not None:\n        dots = cached_dots\n    else:\n        # dot product of each vertex with the plane normal indexed by face\n        # so for each face the dot product of each vertex is a row\n        # shape is the same as mesh.faces (n,3)\n        dots = np.dot(plane_normal,\n                      (mesh.vertices - plane_origin).T)[mesh.faces]\n\n    # sign of the dot product is -1, 0, or 1\n    # shape is the same as mesh.faces (n,3)\n    signs = np.zeros(mesh.faces.shape, dtype=np.int8)\n    signs[dots < -tol.merge] = -1\n    signs[dots > tol.merge] = 1\n\n    # figure out which triangles are in the cross section,\n    # and which of the three intersection cases they are in\n    cases = triangle_cases(signs)\n    # handlers for each case\n    handlers = (handle_basic,\n                handle_on_vertex,\n                handle_on_edge)\n\n    # the (m, 2, 3) line segments\n    lines = np.vstack([h(signs[c],\n                         mesh.faces[c],\n                         mesh.vertices)\n                       for c, h in zip(cases, handlers)])\n\n    log.debug('mesh_cross_section found %i intersections',\n              len(lines))\n\n    if return_faces:\n        face_index = np.hstack([np.nonzero(c)[0] for c in cases])\n        return lines, face_index\n    return lines", "response": "Returns a set of 3D line segments that intersect a mesh and a plane."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mesh_multiplane(mesh,\n                    plane_origin,\n                    plane_normal,\n                    heights):\n    \"\"\"\n    A utility function for slicing a mesh by multiple\n    parallel planes, which caches the dot product operation.\n\n    Parameters\n    -------------\n    mesh : trimesh.Trimesh\n        Geometry to be sliced by planes\n    plane_normal : (3,) float\n        Normal vector of plane\n    plane_origin : (3,) float\n        Point on a plane\n    heights : (m,) float\n        Offset distances from plane to slice at\n\n    Returns\n    --------------\n    lines : (m,) sequence of (n, 2, 2) float\n        Lines in space for m planes\n    to_3D : (m, 4, 4) float\n        Transform to move each section back to 3D\n    face_index : (m,) sequence of (n,) int\n        Indexes of mesh.faces for each segment\n    \"\"\"\n    # check input plane\n    plane_normal = util.unitize(plane_normal)\n    plane_origin = np.asanyarray(plane_origin,\n                                 dtype=np.float64)\n    heights = np.asanyarray(heights, dtype=np.float64)\n\n    # dot product of every vertex with plane\n    vertex_dots = np.dot(plane_normal,\n                         (mesh.vertices - plane_origin).T)\n\n    # reconstruct transforms for each 2D section\n    base_transform = geometry.plane_transform(origin=plane_origin,\n                                              normal=plane_normal)\n    base_transform = np.linalg.inv(base_transform)\n\n    # alter translation Z inside loop\n    translation = np.eye(4)\n\n    # store results\n    transforms = []\n    face_index = []\n    segments = []\n\n    # loop through user specified heights\n    for height in heights:\n        # offset the origin by the height\n        new_origin = plane_origin + (plane_normal * height)\n        # offset the dot products by height and index by faces\n        new_dots = (vertex_dots - height)[mesh.faces]\n        # run the intersection with the cached dot products\n        lines, index = mesh_plane(mesh=mesh,\n                                  plane_origin=new_origin,\n                                  plane_normal=plane_normal,\n                                  return_faces=True,\n                                  cached_dots=new_dots)\n\n        # get the transforms to 3D space and back\n        translation[2, 3] = height\n        to_3D = np.dot(base_transform, translation)\n        to_2D = np.linalg.inv(to_3D)\n        transforms.append(to_3D)\n\n        # transform points to 2D frame\n        lines_2D = transformations.transform_points(\n            lines.reshape((-1, 3)),\n            to_2D)\n\n        # if we didn't screw up the transform all\n        # of the Z values should be zero\n        assert np.allclose(lines_2D[:, 2], 0.0)\n\n        # reshape back in to lines and discard Z\n        lines_2D = lines_2D[:, :2].reshape((-1, 2, 2))\n        # store (n, 2, 2) float lines\n        segments.append(lines_2D)\n        # store (n,) int indexes of mesh.faces\n        face_index.append(face_index)\n\n    # (n, 4, 4) transforms from 2D to 3D\n    transforms = np.array(transforms, dtype=np.float64)\n\n    return segments, transforms, face_index", "response": "This function is used to slicing a mesh by multiple parallel planes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the intersection of a set of lines on a plane.", "response": "def plane_lines(plane_origin,\n                plane_normal,\n                endpoints,\n                line_segments=True):\n    \"\"\"\n    Calculate plane-line intersections\n\n    Parameters\n    ---------\n    plane_origin : (3,) float\n        Point on plane\n    plane_normal : (3,) float\n        Plane normal vector\n    endpoints : (2, n, 3) float\n        Points defining lines to be tested\n    line_segments : bool\n        If True, only returns intersections as valid if\n        vertices from endpoints are on different sides\n        of the plane.\n\n    Returns\n    ---------\n    intersections : (m, 3) float\n        Cartesian intersection points\n    valid : (n, 3) bool\n        Indicate whether a valid intersection exists\n        for each input line segment\n    \"\"\"\n    endpoints = np.asanyarray(endpoints)\n    plane_origin = np.asanyarray(plane_origin).reshape(3)\n    line_dir = util.unitize(endpoints[1] - endpoints[0])\n    plane_normal = util.unitize(np.asanyarray(plane_normal).reshape(3))\n\n    t = np.dot(plane_normal, (plane_origin - endpoints[0]).T)\n    b = np.dot(plane_normal, line_dir.T)\n\n    # If the plane normal and line direction are perpendicular, it means\n    # the vector is 'on plane', and there isn't a valid intersection.\n    # We discard on-plane vectors by checking that the dot product is nonzero\n    valid = np.abs(b) > tol.zero\n    if line_segments:\n        test = np.dot(plane_normal,\n                      np.transpose(plane_origin - endpoints[1]))\n        different_sides = np.sign(t) != np.sign(test)\n        nonzero = np.logical_or(np.abs(t) > tol.zero,\n                                np.abs(test) > tol.zero)\n        valid = np.logical_and(valid, different_sides)\n        valid = np.logical_and(valid, nonzero)\n\n    d = np.divide(t[valid], b[valid])\n    intersection = endpoints[0][valid]\n    intersection = intersection + np.reshape(d, (-1, 1)) * line_dir[valid]\n\n    return intersection, valid"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngive one line per plane find the intersection points.", "response": "def planes_lines(plane_origins,\n                 plane_normals,\n                 line_origins,\n                 line_directions):\n    \"\"\"\n    Given one line per plane, find the intersection points.\n\n    Parameters\n    -----------\n    plane_origins : (n,3) float\n        Point on each plane\n    plane_normals : (n,3) float\n        Normal vector of each plane\n    line_origins : (n,3) float\n        Point at origin of each line\n    line_directions : (n,3) float\n        Direction vector of each line\n\n    Returns\n    ----------\n    on_plane : (n,3) float\n        Points on specified planes\n    valid : (n,) bool\n        Did plane intersect line or not\n    \"\"\"\n\n    # check input types\n    plane_origins = np.asanyarray(plane_origins, dtype=np.float64)\n    plane_normals = np.asanyarray(plane_normals, dtype=np.float64)\n    line_origins = np.asanyarray(line_origins, dtype=np.float64)\n    line_directions = np.asanyarray(line_directions, dtype=np.float64)\n\n    # vector from line to plane\n    origin_vectors = plane_origins - line_origins\n\n    projection_ori = util.diagonal_dot(origin_vectors, plane_normals)\n    projection_dir = util.diagonal_dot(line_directions, plane_normals)\n\n    valid = np.abs(projection_dir) > tol.merge\n\n    distance = np.divide(projection_ori[valid],\n                         projection_dir[valid])\n\n    on_plane = line_directions[valid] * distance.reshape((-1, 1))\n    on_plane += line_origins[valid]\n\n    return on_plane, valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef slice_faces_plane(vertices,\n                      faces,\n                      plane_normal,\n                      plane_origin,\n                      cached_dots=None):\n    \"\"\"\n    Slice a mesh (given as a set of faces and vertices) with a plane, returning a\n    new mesh (again as a set of faces and vertices) that is the\n    portion of the original mesh to the positive normal side of the plane.\n\n    Parameters\n    ---------\n    vertices : (n, 3) float\n        Vertices of source mesh to slice\n    faces : (n, 3) int\n        Faces of source mesh to slice\n    plane_normal : (3,) float\n        Normal vector of plane to intersect with mesh\n    plane_origin:  (3,) float\n        Point on plane to intersect with mesh\n    cached_dots : (n, 3) float\n        If an external function has stored dot\n        products pass them here to avoid recomputing\n    Returns\n    ----------\n    new_vertices : (n, 3) float\n        Vertices of sliced mesh\n    new_faces : (n, 3) int\n        Faces of sliced mesh\n    \"\"\"\n\n    if len(vertices) == 0:\n        return vertices, faces\n\n    if cached_dots is not None:\n        dots = cached_dots\n    else:\n        # dot product of each vertex with the plane normal indexed by face\n        # so for each face the dot product of each vertex is a row\n        # shape is the same as faces (n,3)\n        dots = np.einsum('i,ij->j', plane_normal,\n                         (vertices - plane_origin).T)[faces]\n\n    # Find vertex orientations w.r.t. faces for all triangles:\n    #  -1 -> vertex \"inside\" plane (positive normal direction)\n    #   0 -> vertex on plane\n    #   1 -> vertex \"outside\" plane (negative normal direction)\n    signs = np.zeros(faces.shape, dtype=np.int8)\n    signs[dots < -tol.merge] = 1\n    signs[dots > tol.merge] = -1\n    signs[np.logical_and(dots >= -tol.merge, dots <= tol.merge)] = 0\n\n    # Find all triangles that intersect this plane\n    # onedge <- indices of all triangles intersecting the plane\n    # inside <- indices of all triangles \"inside\" the plane (positive normal)\n    signs_sum = signs.sum(axis=1, dtype=np.int8)\n    signs_asum = np.abs(signs).sum(axis=1, dtype=np.int8)\n\n    # Cases:\n    # (0,0,0),  (-1,0,0),  (-1,-1,0), (-1,-1,-1) <- inside\n    # (1,0,0),  (1,1,0),   (1,1,1)               <- outside\n    # (1,0,-1), (1,-1,-1), (1,1,-1)              <- onedge\n    onedge = np.logical_and(signs_asum >= 2,\n                            np.abs(signs_sum) <= 1)\n    inside = (signs_sum == -signs_asum)\n\n    # Automatically include all faces that are \"inside\"\n    new_faces = faces[inside]\n\n    # Separate faces on the edge into two cases: those which will become\n    # quads (two vertices inside plane) and those which will become triangles\n    # (one vertex inside plane)\n    triangles = vertices[faces]\n    cut_triangles = triangles[onedge]\n    cut_faces_quad = faces[np.logical_and(onedge, signs_sum < 0)]\n    cut_faces_tri = faces[np.logical_and(onedge, signs_sum >= 0)]\n    cut_signs_quad = signs[np.logical_and(onedge, signs_sum < 0)]\n    cut_signs_tri = signs[np.logical_and(onedge, signs_sum >= 0)]\n\n    # If no faces to cut, the surface is not in contact with this plane.\n    # Thus, return a mesh with only the inside faces\n    if len(cut_faces_quad) + len(cut_faces_tri) == 0:\n\n        if len(new_faces) == 0:\n            # if no new faces at all return empty arrays\n            empty = (np.zeros((0, 3), dtype=np.float64),\n                     np.zeros((0, 3), dtype=np.int64))\n            return empty\n\n        try:\n            # count the number of occurrences of each value\n            counts = np.bincount(new_faces.flatten(), minlength=len(vertices))\n            unique_verts = counts > 0\n            unique_index = np.where(unique_verts)[0]\n        except TypeError:\n            # casting failed on 32 bit windows\n            log.error('casting failed!', exc_info=True)\n            # fall back to numpy unique\n            unique_index = np.unique(new_faces.flatten())\n            # generate a mask for cumsum\n            unique_verts = np.zeros(len(vertices), dtype=np.bool)\n            unique_verts[unique_index] = True\n\n        unique_faces = (np.cumsum(unique_verts) - 1)[new_faces]\n        return vertices[unique_index], unique_faces\n\n    # Extract the intersections of each triangle's edges with the plane\n    o = cut_triangles                               # origins\n    d = np.roll(o, -1, axis=1) - o                  # directions\n    num = (plane_origin - o).dot(plane_normal)      # compute num/denom\n    denom = np.dot(d, plane_normal)\n    denom[denom == 0.0] = 1e-12                     # prevent division by zero\n    dist = np.divide(num, denom)\n    # intersection points for each segment\n    int_points = np.einsum('ij,ijk->ijk', dist, d) + o\n\n    # Initialize the array of new vertices with the current vertices\n    new_vertices = vertices\n\n    # Handle the case where a new quad is formed by the intersection\n    # First, extract the intersection points belonging to a new quad\n    quad_int_points = int_points[(signs_sum < 0)[onedge], :, :]\n    num_quads = len(quad_int_points)\n    if num_quads > 0:\n        # Extract the vertex on the outside of the plane, then get the vertices\n        # (in CCW order of the inside vertices)\n        quad_int_inds = np.where(cut_signs_quad == 1)[1]\n        quad_int_verts = cut_faces_quad[\n            np.stack((range(num_quads), range(num_quads)), axis=1),\n            np.stack(((quad_int_inds + 1) % 3, (quad_int_inds + 2) % 3), axis=1)]\n\n        # Fill out new quad faces with the intersection points as vertices\n        new_quad_faces = np.append(\n            quad_int_verts,\n            np.arange(len(new_vertices),\n                      len(new_vertices) +\n                      2 * num_quads).reshape(num_quads, 2), axis=1)\n\n        # Extract correct intersection points from int_points and order them in\n        # the same way as they were added to faces\n        new_quad_vertices = quad_int_points[\n            np.stack((range(num_quads), range(num_quads)), axis=1),\n            np.stack((((quad_int_inds + 2) % 3).T, quad_int_inds.T),\n                     axis=1), :].reshape(2 * num_quads, 3)\n\n        # Add new vertices to existing vertices, triangulate quads, and add the\n        # resulting triangles to the new faces\n        new_vertices = np.append(new_vertices, new_quad_vertices, axis=0)\n        new_tri_faces_from_quads = geometry.triangulate_quads(new_quad_faces)\n        new_faces = np.append(new_faces, new_tri_faces_from_quads, axis=0)\n\n    # Handle the case where a new triangle is formed by the intersection\n    # First, extract the intersection points belonging to a new triangle\n    tri_int_points = int_points[(signs_sum >= 0)[onedge], :, :]\n    num_tris = len(tri_int_points)\n    if num_tris > 0:\n        # Extract the single vertex for each triangle inside the plane and get the\n        # inside vertices (CCW order)\n        tri_int_inds = np.where(cut_signs_tri == -1)[1]\n        tri_int_verts = cut_faces_tri[range(\n            num_tris), tri_int_inds].reshape(num_tris, 1)\n\n        # Fill out new triangles with the intersection points as vertices\n        new_tri_faces = np.append(\n            tri_int_verts,\n            np.arange(len(new_vertices),\n                      len(new_vertices) +\n                      2 * num_tris).reshape(num_tris, 2),\n            axis=1)\n\n        # Extract correct intersection points and order them in the same way as\n        # the vertices were added to the faces\n        new_tri_vertices = tri_int_points[\n            np.stack((range(num_tris), range(num_tris)), axis=1),\n            np.stack((tri_int_inds.T, ((tri_int_inds + 2) % 3).T),\n                     axis=1),\n            :].reshape(2 * num_tris, 3)\n\n        # Append new vertices and new faces\n        new_vertices = np.append(new_vertices, new_tri_vertices, axis=0)\n        new_faces = np.append(new_faces, new_tri_faces, axis=0)\n\n    # find the unique indices in the new faces\n    # using an integer- only unique function\n    unique, inverse = grouping.unique_bincount(new_faces.reshape(-1),\n                                               minlength=len(new_vertices),\n                                               return_inverse=True)\n\n    # use the unique indexes for our final vertex and faces\n    final_vert = new_vertices[unique]\n    final_face = inverse.reshape((-1, 3))\n\n    return final_vert, final_face", "response": "Slice a mesh with a plane."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nslice a mesh with a plane", "response": "def slice_mesh_plane(mesh,\n                     plane_normal,\n                     plane_origin,\n                     **kwargs):\n    \"\"\"\n    Slice a mesh with a plane, returning a new mesh that is the\n    portion of the original mesh to the positive normal side of the plane\n\n    Parameters\n    ---------\n    mesh : Trimesh object\n        Source mesh to slice\n    plane_normal : (3,) float\n        Normal vector of plane to intersect with mesh\n    plane_origin:  (3,) float\n        Point on plane to intersect with mesh\n    cached_dots : (n, 3) float\n        If an external function has stored dot\n        products pass them here to avoid recomputing\n    Returns\n    ----------\n    new_mesh : Trimesh object\n        Sliced mesh\n    \"\"\"\n    # check input for none\n    if mesh is None:\n        return None\n\n    # avoid circular import\n    from .base import Trimesh\n\n    # check input plane\n    plane_normal = np.asanyarray(plane_normal,\n                                 dtype=np.float64)\n    plane_origin = np.asanyarray(plane_origin,\n                                 dtype=np.float64)\n\n    # check to make sure origins and normals have acceptable shape\n    shape_ok = ((plane_origin.shape == (3,) or\n                 util.is_shape(plane_origin, (-1, 3))) and\n                (plane_normal.shape == (3,) or\n                 util.is_shape(plane_normal, (-1, 3))) and\n                plane_origin.shape == plane_normal.shape)\n    if not shape_ok:\n        raise ValueError('plane origins and normals must be (n, 3)!')\n\n    # start with original vertices and faces\n    vertices = mesh.vertices.copy()\n    faces = mesh.faces.copy()\n\n    # slice away specified planes\n    for origin, normal in zip(plane_origin.reshape((-1, 3)),\n                              plane_normal.reshape((-1, 3))):\n        # save the new vertices and faces\n        vertices, faces = slice_faces_plane(vertices=vertices,\n                                            faces=faces,\n                                            plane_normal=normal,\n                                            plane_origin=origin,\n                                            **kwargs)\n\n    # create a mesh from the sliced result\n    new_mesh = Trimesh(vertices=vertices,\n                       faces=faces,\n                       process=False)\n    return new_mesh"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a scene with a Fuze bottle some cubes and an axis.", "response": "def create_scene():\n    \"\"\"\n    Create a scene with a Fuze bottle, some cubes, and an axis.\n\n    Returns\n    ----------\n    scene : trimesh.Scene\n      Object with geometry\n    \"\"\"\n    scene = trimesh.Scene()\n\n    # plane\n    geom = trimesh.creation.box((0.5, 0.5, 0.01))\n    geom.apply_translation((0, 0, -0.005))\n    geom.visual.face_colors = (.6, .6, .6)\n    scene.add_geometry(geom)\n\n    # axis\n    geom = trimesh.creation.axis(0.02)\n    scene.add_geometry(geom)\n\n    box_size = 0.1\n\n    # box1\n    geom = trimesh.creation.box((box_size,) * 3)\n    geom.visual.face_colors = np.random.uniform(\n        0, 1, (len(geom.faces), 3))\n    transform = tf.translation_matrix([0.1, 0.1, box_size / 2])\n    scene.add_geometry(geom, transform=transform)\n\n    # box2\n    geom = trimesh.creation.box((box_size,) * 3)\n    geom.visual.face_colors = np.random.uniform(\n        0, 1, (len(geom.faces), 3))\n    transform = tf.translation_matrix([-0.1, 0.1, box_size / 2])\n    scene.add_geometry(geom, transform=transform)\n\n    # fuze\n    geom = trimesh.load(str(here / '../models/fuze.obj'))\n    transform = tf.translation_matrix([-0.1, -0.1, 0])\n    scene.add_geometry(geom, transform=transform)\n\n    # sphere\n    geom = trimesh.creation.icosphere(radius=0.05)\n    geom.visual.face_colors = np.random.uniform(\n        0, 1, (len(geom.faces), 3))\n    transform = tf.translation_matrix([0.1, -0.1, box_size / 2])\n    scene.add_geometry(geom, transform=transform)\n\n    return scene"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vertex_defects(mesh):\n    angle_sum = np.asarray(mesh.face_angles_sparse.sum(axis=1)).flatten()\n    defect = (2 * np.pi) - angle_sum\n    return defect", "response": "Returns the vertex defects at every vertex in the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the discrete gaussian curvature measure of a sphere centered at a point.", "response": "def discrete_gaussian_curvature_measure(mesh, points, radius):\n    \"\"\"\n    Return the discrete gaussian curvature measure of a sphere centered\n    at a point as detailed in 'Restricted Delaunay triangulations and normal\n    cycle', Cohen-Steiner and Morvan.\n\n    Parameters\n    ----------\n    points : (n,3) float, list of points in space\n    radius : float, the sphere radius\n\n    Returns\n    --------\n    gaussian_curvature: (n,) float, discrete gaussian curvature measure.\n    \"\"\"\n\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    nearest = mesh.kdtree.query_ball_point(points, radius)\n    gauss_curv = [mesh.vertex_defects[vertices].sum() for vertices in nearest]\n\n    return np.asarray(gauss_curv)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef discrete_mean_curvature_measure(mesh, points, radius):\n\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)!')\n\n    # axis aligned bounds\n    bounds = np.column_stack((points - radius,\n                              points + radius))\n\n    # line segments that intersect axis aligned bounding box\n    candidates = [list(mesh.face_adjacency_tree.intersection(b))\n                  for b in bounds]\n\n    mean_curv = np.empty(len(points))\n    for i, (x, x_candidates) in enumerate(zip(points, candidates)):\n        endpoints = mesh.vertices[mesh.face_adjacency_edges[x_candidates]]\n        lengths = line_ball_intersection(\n            endpoints[:, 0],\n            endpoints[:, 1],\n            center=x,\n            radius=radius)\n        angles = mesh.face_adjacency_angles[x_candidates]\n        signs = np.where(mesh.face_adjacency_convex[x_candidates], 1, -1)\n        mean_curv[i] = (lengths * angles * signs).sum() / 2\n\n    return mean_curv", "response": "Return the discrete mean curvature measure of a sphere centered at a point."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the length of the intersection of a line segment with a ball.", "response": "def line_ball_intersection(start_points, end_points, center, radius):\n    \"\"\"\n    Compute the length of the intersection of a line segment with a ball.\n\n    Parameters\n    ----------\n    start_points : (n,3) float, list of points in space\n    end_points   : (n,3) float, list of points in space\n    center       : (3,) float, the sphere center\n    radius       : float, the sphere radius\n\n    Returns\n    --------\n    lengths: (n,) float, the lengths.\n\n    \"\"\"\n\n    # We solve for the intersection of |x-c|**2 = r**2 and\n    # x = o + dL. This yields\n    # d = (-l.(o-c) +- sqrt[ l.(o-c)**2 - l.l((o-c).(o-c) - r^**2) ]) / l.l\n    L = end_points - start_points\n    oc = start_points - center  # o-c\n    r = radius\n    ldotl = np.einsum('ij, ij->i', L, L)  # l.l\n    ldotoc = np.einsum('ij, ij->i', L, oc)  # l.(o-c)\n    ocdotoc = np.einsum('ij, ij->i', oc, oc)  # (o-c).(o-c)\n    discrims = ldotoc**2 - ldotl * (ocdotoc - r**2)\n\n    # If discriminant is non-positive, then we have zero length\n    lengths = np.zeros(len(start_points))\n    # Otherwise we solve for the solns with d2 > d1.\n    m = discrims > 0  # mask\n    d1 = (-ldotoc[m] - np.sqrt(discrims[m])) / ldotl[m]\n    d2 = (-ldotoc[m] + np.sqrt(discrims[m])) / ldotl[m]\n\n    # Line segment means we have 0 <= d <= 1\n    d1 = np.clip(d1, 0, 1)\n    d2 = np.clip(d2, 0, 1)\n\n    # Length is |o + d2 l - o + d1 l|  = (d2 - d1) |l|\n    lengths[m] = (d2 - d1) * np.sqrt(ldotl[m])\n\n    return lengths"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sphere_ball_intersection(R, r):\n    x = (2 * R**2 - r**2) / (2 * R)  # x coord of plane\n    if x >= -R:\n        return 2 * np.pi * R * (R - x)\n    if x < -R:\n        return 4 * np.pi * R**2", "response": "Compute the surface area of the intersection of a sphere with a ball of radius r centered at ( 0 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the color at each of the UV coordinates on a texture image.", "response": "def uv_to_color(uv, image):\n    \"\"\"\n    Get the color in a texture image.\n\n    Parameters\n    -------------\n    uv : (n, 2) float\n      UV coordinates on texture image\n    image : PIL.Image\n      Texture image\n\n    Returns\n    ----------\n    colors : (n, 4) float\n      RGBA color at each of the UV coordinates\n    \"\"\"\n    if image is None or uv is None:\n        return None\n\n    # UV coordinates should be (n, 2) float\n    uv = np.asanyarray(uv, dtype=np.float64)\n\n    # get texture image pixel positions of UV coordinates\n    x = (uv[:, 0] * (image.width - 1))\n    y = ((1 - uv[:, 1]) * (image.height - 1))\n\n    # convert to int and wrap to image\n    # size in the manner of GL_REPEAT\n    x = x.round().astype(np.int64) % image.width\n    y = y.round().astype(np.int64) % image.height\n\n    # access colors from pixel locations\n    # make sure image is RGBA before getting values\n    colors = np.asanyarray(image.convert('RGBA'))[y, x]\n\n    # conversion to RGBA should have corrected shape\n    assert colors.ndim == 2 and colors.shape[1] == 4\n\n    return colors"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the UV coordinates of the current object.", "response": "def uv(self, values):\n        \"\"\"\n        Set the UV coordinates.\n\n        Parameters\n        --------------\n        values : (n, 2) float\n          Pixel locations on a texture per- vertex\n        \"\"\"\n        if values is None:\n            self._data.clear()\n        else:\n            self._data['uv'] = np.asanyarray(values, dtype=np.float64)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a copy of the current TextureVisuals object.", "response": "def copy(self):\n        \"\"\"\n        Return a copy of the current TextureVisuals object.\n\n        Returns\n        ----------\n        copied : TextureVisuals\n          Contains the same information in a new object\n        \"\"\"\n        uv = self.uv\n        if uv is not None:\n            uv = uv.copy()\n        copied = TextureVisuals(\n            uv=uv,\n            material=copy.deepcopy(self.material))\n\n        return copied"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts textured visuals to a ColorVisuals with vertex color calculated from texture.", "response": "def to_color(self):\n        \"\"\"\n        Convert textured visuals to a ColorVisuals with vertex\n        color calculated from texture.\n\n        Returns\n        -----------\n        vis : trimesh.visuals.ColorVisuals\n          Contains vertex color from texture\n        \"\"\"\n        # find the color at each UV coordinate\n        colors = self.material.to_color(self.uv)\n        # create ColorVisuals from result\n        vis = color.ColorVisuals(vertex_colors=colors)\n        return vis"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef update_vertices(self, mask):\n        if self.uv is not None:\n            self.uv = self.uv[mask]", "response": "Update the vertex properties of a set of vertices."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_stl(file_obj, file_type=None, **kwargs):\n    # save start of file obj\n    file_pos = file_obj.tell()\n    try:\n        # check the file for a header which matches the file length\n        # if that is true, it is almost certainly a binary STL file\n        # if the header doesn't match the file length a HeaderError will be\n        # raised\n        return load_stl_binary(file_obj)\n    except HeaderError:\n        # move the file back to where it was initially\n        file_obj.seek(file_pos)\n        # try to load the file as an ASCII STL\n        # if the header doesn't match the file length a HeaderError will be\n        # raised\n        return load_stl_ascii(file_obj)", "response": "Loads an STL file from a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload a binary STL file from a file - like object.", "response": "def load_stl_binary(file_obj):\n    \"\"\"\n    Load a binary STL file from a file object.\n\n    Parameters\n    ----------\n    file_obj: open file- like object\n\n    Returns\n    ----------\n    loaded: kwargs for a Trimesh constructor with keys:\n              vertices:     (n,3) float, vertices\n              faces:        (m,3) int, indexes of vertices\n              face_normals: (m,3) float, normal vector of each face\n    \"\"\"\n    # the header is always 84 bytes long, we just reference the dtype.itemsize\n    # to be explicit about where that magical number comes from\n    header_length = _stl_dtype_header.itemsize\n    header_data = file_obj.read(header_length)\n    if len(header_data) < header_length:\n        raise HeaderError('Binary STL shorter than a fixed header!')\n\n    try:\n        header = np.frombuffer(header_data,\n                               dtype=_stl_dtype_header)\n    except BaseException:\n        raise HeaderError('Binary header incorrect type')\n\n    try:\n        # save the header block as a string\n        # there could be any garbage in there so wrap in try\n        metadata = {'header':\n                    bytes(header['header'][0]).decode('utf-8').strip()}\n    except BaseException:\n        metadata = {}\n\n    # now we check the length from the header versus the length of the file\n    # data_start should always be position 84, but hard coding that felt ugly\n    data_start = file_obj.tell()\n    # this seeks to the end of the file\n    # position 0, relative to the end of the file 'whence=2'\n    file_obj.seek(0, 2)\n    # we save the location of the end of the file and seek back to where we\n    # started from\n    data_end = file_obj.tell()\n    file_obj.seek(data_start)\n\n    # the binary format has a rigidly defined structure, and if the length\n    # of the file doesn't match the header, the loaded version is almost\n    # certainly going to be garbage.\n    len_data = data_end - data_start\n    len_expected = header['face_count'] * _stl_dtype.itemsize\n\n    # this check is to see if this really is a binary STL file.\n    # if we don't do this and try to load a file that isn't structured properly\n    # we will be producing garbage or crashing hard\n    # so it's much better to raise an exception here.\n    if len_data != len_expected:\n        raise HeaderError('Binary STL has incorrect length in header!')\n    blob = np.frombuffer(file_obj.read(), dtype=_stl_dtype)\n\n    # all of our vertices will be loaded in order\n    # so faces are just sequential indices reshaped.\n    faces = np.arange(header['face_count'] * 3).reshape((-1, 3))\n\n    result = {'vertices': blob['vertices'].reshape((-1, 3)),\n              'face_normals': blob['normals'].reshape((-1, 3)),\n              'faces': faces,\n              'metadata': metadata}\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload an ASCII STL file into Trimesh objects.", "response": "def load_stl_ascii(file_obj):\n    \"\"\"\n    Load an ASCII STL file from a file object.\n\n    Parameters\n    ----------\n    file_obj: open file- like object\n\n    Returns\n    ----------\n    loaded: kwargs for a Trimesh constructor with keys:\n              vertices:     (n,3) float, vertices\n              faces:        (m,3) int, indexes of vertices\n              face_normals: (m,3) float, normal vector of each face\n    \"\"\"\n\n    # the first line is the header\n    header = file_obj.readline()\n    # make sure header is a string, not bytes\n    if hasattr(header, 'decode'):\n        try:\n            header = header.decode('utf-8')\n        except BaseException:\n            header = ''\n    # save header to metadata\n    metadata = {'header': header}\n\n    # read all text into one string\n    text = file_obj.read()\n    # convert bytes to string\n    if hasattr(text, 'decode'):\n        text = text.decode('utf-8')\n    # split by endsolid keyword\n    text = text.lower().split('endsolid')[0]\n    # create array of splits\n    blob = np.array(text.strip().split())\n\n    # there are 21 'words' in each face\n    face_len = 21\n\n    # length of blob should be multiple of face_len\n    if (len(blob) % face_len) != 0:\n        raise HeaderError('Incorrect length STL file!')\n    face_count = int(len(blob) / face_len)\n\n    # this offset is to be added to a fixed set of tiled indices\n    offset = face_len * np.arange(face_count).reshape((-1, 1))\n    normal_index = np.tile([2, 3, 4], (face_count, 1)) + offset\n    vertex_index = np.tile([8, 9, 10,\n                            12, 13, 14,\n                            16, 17, 18], (face_count, 1)) + offset\n\n    # faces are groups of three sequential vertices\n    faces = np.arange(face_count * 3).reshape((-1, 3))\n    face_normals = blob[normal_index].astype('<f8')\n    vertices = blob[vertex_index.reshape((-1, 3))].astype('<f8')\n\n    return {'vertices': vertices,\n            'faces': faces,\n            'metadata': metadata,\n            'face_normals': face_normals}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export_stl(mesh):\n    header = np.zeros(1, dtype=_stl_dtype_header)\n    header['face_count'] = len(mesh.faces)\n\n    packed = np.zeros(len(mesh.faces), dtype=_stl_dtype)\n    packed['normals'] = mesh.face_normals\n    packed['vertices'] = mesh.triangles\n\n    export = header.tostring()\n    export += packed.tostring()\n\n    return export", "response": "Convert a Trimesh object into a binary STL file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_stl_ascii(mesh):\n\n    # move all the data that's going into the STL file into one array\n    blob = np.zeros((len(mesh.faces), 4, 3))\n    blob[:, 0, :] = mesh.face_normals\n    blob[:, 1:, :] = mesh.triangles\n\n    # create a lengthy format string for the data section of the file\n    format_string = 'facet normal {} {} {}\\nouter loop\\n'\n    format_string += 'vertex {} {} {}\\n' * 3\n    format_string += 'endloop\\nendfacet\\n'\n    format_string *= len(mesh.faces)\n\n    # concatenate the header, data, and footer\n    export = 'solid \\n'\n    export += format_string.format(*blob.reshape(-1))\n    export += 'endsolid'\n\n    return export", "response": "Convert a Trimesh object into an ASCII STL file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef vertex_graph(entities):\n    graph = nx.Graph()\n    closed = []\n    for index, entity in enumerate(entities):\n        if entity.closed:\n            closed.append(index)\n        else:\n            graph.add_edges_from(entity.nodes,\n                                 entity_index=index)\n    return graph, np.array(closed)", "response": "Generates a networkx. Graph containing the vertices of the given set of entity objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vertex_to_entity_path(vertex_path,\n                          graph,\n                          entities,\n                          vertices=None):\n    \"\"\"\n    Convert a path of vertex indices to a path of entity indices.\n\n    Parameters\n    ----------\n    vertex_path : (n,) int\n        Ordered list of vertex indices representing a path\n    graph : nx.Graph\n        Vertex connectivity\n    entities : (m,) list\n        Entity objects\n    vertices :  (p, dimension) float\n        Vertex points in space\n\n    Returns\n    ----------\n    entity_path : (q,) int\n        Entity indices which make up vertex_path\n    \"\"\"\n    def edge_direction(a, b):\n        \"\"\"\n        Given two edges, figure out if the first needs to be\n         reversed to keep the progression forward.\n\n         [1,0] [1,2] -1  1\n         [1,0] [2,1] -1 -1\n         [0,1] [1,2]  1  1\n         [0,1] [2,1]  1 -1\n\n        Parameters\n        ------------\n        a : (2,) int\n        b : (2,) int\n\n        Returns\n        ------------\n        a_direction : int\n        b_direction : int\n        \"\"\"\n        if a[0] == b[0]:\n            return -1, 1\n        elif a[0] == b[1]:\n            return -1, -1\n        elif a[1] == b[0]:\n            return 1, 1\n        elif a[1] == b[1]:\n            return 1, -1\n        else:\n            msg = 'edges not connected!'\n            msg += '\\nvertex_path: {}'.format(vertex_path)\n            msg += '\\nentity_path: {}'.format(entity_path)\n            msg += '\\nentity[a]: {}'.format(entities[ea].points)\n            msg += '\\nentity[b]: {}'.format(entities[eb].points)\n            constants.log.warning(msg)\n            return None, None\n\n    if vertices is None or vertices.shape[1] != 2:\n        ccw_direction = 1\n    else:\n        ccw_check = is_ccw(vertices[np.append(vertex_path,\n                                              vertex_path[0])])\n        ccw_direction = (ccw_check * 2) - 1\n\n    # make sure vertex path is correct type\n    vertex_path = np.asanyarray(vertex_path, dtype=np.int64)\n    # we will be saving entity indexes\n    entity_path = []\n    # loop through pairs of vertices\n    for i in np.arange(len(vertex_path) + 1):\n        # get two wrapped vertex positions\n        vertex_path_pos = np.mod(np.arange(2) + i, len(vertex_path))\n        vertex_index = vertex_path[vertex_path_pos]\n        entity_index = graph.get_edge_data(*vertex_index)['entity_index']\n        entity_path.append(entity_index)\n    # remove duplicate entities and order CCW\n    entity_path = grouping.unique_ordered(entity_path)[::ccw_direction]\n    # check to make sure there is more than one entity\n    if len(entity_path) == 1:\n        # apply CCW reverse in place if necessary\n        if ccw_direction < 0:\n            index = entity_path[0]\n            entities[index].reverse()\n\n        return entity_path\n    # traverse the entity path and reverse entities in place to\n    # align with this path ordering\n    round_trip = np.append(entity_path, entity_path[0])\n    round_trip = zip(round_trip[:-1], round_trip[1:])\n    for ea, eb in round_trip:\n        da, db = edge_direction(entities[ea].end_points,\n                                entities[eb].end_points)\n        if da is not None:\n            entities[ea].reverse(direction=da)\n            entities[eb].reverse(direction=db)\n\n    entity_path = np.array(entity_path)\n\n    return entity_path", "response": "Convert a path of vertex indices to a path of entity indices."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef closed_paths(entities, vertices):\n    # get a networkx graph of entities\n    graph, closed = vertex_graph(entities)\n    # add entities that are closed as single- entity paths\n    entity_paths = np.reshape(closed, (-1, 1)).tolist()\n    # look for cycles in the graph, or closed loops\n    vertex_paths = np.array(nx.cycles.cycle_basis(graph))\n\n    # loop through every vertex cycle\n    for vertex_path in vertex_paths:\n        # a path has no length if it has fewer than 2 vertices\n        if len(vertex_path) < 2:\n            continue\n        # convert vertex indices to entity indices\n        entity_paths.append(\n            vertex_to_entity_path(vertex_path,\n                                  graph,\n                                  entities,\n                                  vertices))\n    entity_paths = np.array(entity_paths)\n    return entity_paths", "response": "Generate a list of entity paths that are closed by the given entities and vertices."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef discretize_path(entities, vertices, path, scale=1.0):\n    # make sure vertices are numpy array\n    vertices = np.asanyarray(vertices)\n    path_len = len(path)\n    if path_len == 0:\n        raise ValueError('Cannot discretize empty path!')\n    if path_len == 1:\n        # case where we only have one entity\n        discrete = np.asanyarray(entities[path[0]].discrete(\n            vertices,\n            scale=scale))\n    else:\n        # run through path appending each entity\n        discrete = []\n        for i, entity_id in enumerate(path):\n            # the current (n, dimension) discrete curve of an entity\n            current = entities[entity_id].discrete(vertices, scale=scale)\n            # check if we are on the final entity\n            if i >= (path_len - 1):\n                # if we are on the last entity include the last point\n                discrete.append(current)\n            else:\n                # slice off the last point so we don't get duplicate\n                # points from the end of one entity and the start of another\n                discrete.append(current[:-1])\n        # stack all curves to one nice (n, dimension) curve\n        discrete = np.vstack(discrete)\n    # make sure 2D curves are are counterclockwise\n    if vertices.shape[1] == 2 and not is_ccw(discrete):\n        # reversing will make array non c- contiguous\n        discrete = np.ascontiguousarray(discrete[::-1])\n\n    return discrete", "response": "Discretizes a list of entity indices into a list of connected points."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a path along (n,d) points, resample them such that the distance traversed along the path is constant in between each of the resampled points. Note that this can produce clipping at corners, as the original vertices are NOT guaranteed to be in the new, resampled path. ONLY ONE of count or step can be specified Result can be uniformly distributed (np.linspace) by specifying count Result can have a specific distance (np.arange) by specifying step Parameters ---------- points: (n, d) float Points in space count : int, Number of points to sample evenly (aka np.linspace) step : float Distance each step should take along the path (aka np.arange) Returns ---------- resampled : (j,d) float Points on the path", "response": "def resample_path(points,\n                  count=None,\n                  step=None,\n                  step_round=True):\n    \"\"\"\n    Given a path along (n,d) points, resample them such that the\n    distance traversed along the path is constant in between each\n    of the resampled points. Note that this can produce clipping at\n    corners, as the original vertices are NOT guaranteed to be in the\n    new, resampled path.\n\n    ONLY ONE of count or step can be specified\n    Result can be uniformly distributed (np.linspace) by specifying count\n    Result can have a specific distance (np.arange) by specifying step\n\n\n    Parameters\n    ----------\n    points:   (n, d) float\n        Points in space\n    count : int,\n        Number of points to sample evenly (aka np.linspace)\n    step : float\n        Distance each step should take along the path (aka np.arange)\n\n    Returns\n    ----------\n    resampled : (j,d) float\n        Points on the path\n    \"\"\"\n\n    points = np.array(points, dtype=np.float)\n    # generate samples along the perimeter from kwarg count or step\n    if (count is not None) and (step is not None):\n        raise ValueError('Only step OR count can be specified')\n    if (count is None) and (step is None):\n        raise ValueError('Either step or count must be specified')\n\n    sampler = PathSample(points)\n    if step is not None and step_round:\n        if step >= sampler.length:\n            return points[[0, -1]]\n\n        count = int(np.ceil(sampler.length / step))\n\n    if count is not None:\n        samples = np.linspace(0, sampler.length, count)\n    elif step is not None:\n        samples = np.arange(0, sampler.length, step)\n\n    resampled = sampler.sample(samples)\n\n    check = np.linalg.norm(points[[0, -1]] - resampled[[0, -1]], axis=1)\n    assert check[0] < constants.tol_path.merge\n    if count is not None:\n        assert check[1] < constants.tol_path.merge\n\n    return resampled"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting a Path2D into multiple Path2D objects where each Path2D object has exactly one root curve.", "response": "def split(self):\n    \"\"\"\n    Split a Path2D into multiple Path2D objects where each\n    one has exactly one root curve.\n\n    Parameters\n    --------------\n    self : trimesh.path.Path2D\n      Input geometry\n\n    Returns\n    -------------\n    split : list of trimesh.path.Path2D\n      Original geometry as separate paths\n    \"\"\"\n    # avoid a circular import by referencing class of self\n    Path2D = type(self)\n\n    # save the results of the split to an array\n    split = []\n\n    # get objects from cache to avoid a bajillion\n    # cache checks inside the tight loop\n    paths = self.paths\n    discrete = self.discrete\n    polygons_closed = self.polygons_closed\n    enclosure_directed = self.enclosure_directed\n\n    for root_index, root in enumerate(self.root):\n        # get a list of the root curve's children\n        connected = list(enclosure_directed[root].keys())\n        # add the root node to the list\n        connected.append(root)\n\n        # store new paths and entities\n        new_paths = []\n        new_entities = []\n\n        for index in connected:\n            path = paths[index]\n            # add a path which is just sequential indexes\n            new_paths.append(np.arange(len(path)) +\n                             len(new_entities))\n            # save the entity indexes\n            new_entities.extend(path)\n\n        # store the root index from the original drawing\n        metadata = copy.deepcopy(self.metadata)\n        metadata['split_2D'] = root_index\n        # we made the root path the last index of connected\n        new_root = np.array([len(new_paths) - 1])\n\n        # prevents the copying from nuking our cache\n        with self._cache:\n            # create the Path2D\n            split.append(Path2D(\n                entities=copy.deepcopy(self.entities[new_entities]),\n                vertices=copy.deepcopy(self.vertices),\n                metadata=metadata))\n            # add back expensive things to the cache\n            split[-1]._cache.update(\n                {'paths': new_paths,\n                 'polygons_closed': polygons_closed[connected],\n                 'discrete': discrete[connected],\n                 'root': new_root})\n            # set the cache ID\n            split[-1]._cache.id_set()\n\n    return np.array(split)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a truncated version of the path.", "response": "def truncate(self, distance):\n        \"\"\"\n        Return a truncated version of the path.\n        Only one vertex (at the endpoint) will be added.\n        \"\"\"\n        position = np.searchsorted(self._cum_norm, distance)\n        offset = distance - self._cum_norm[position - 1]\n\n        if offset < constants.tol_path.merge:\n            truncated = self._points[:position + 1]\n        else:\n            vector = unitize(np.diff(self._points[np.arange(2) + position],\n                                     axis=0).reshape(-1))\n            vector *= offset\n            endpoint = self._points[position] + vector\n            truncated = np.vstack((self._points[:position + 1],\n                                   endpoint))\n\n        assert (np.linalg.norm(np.diff(truncated, axis=0),\n                               axis=1).sum() - distance) < constants.tol_path.merge\n\n        return truncated"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ray_triangle_id(triangles,\n                    ray_origins,\n                    ray_directions,\n                    triangles_normal=None,\n                    tree=None,\n                    multiple_hits=True):\n    \"\"\"\n    Find the intersections between a group of triangles and rays\n\n    Parameters\n    -------------\n    triangles : (n, 3, 3) float\n      Triangles in space\n    ray_origins : (m, 3) float\n      Ray origin points\n    ray_directions : (m, 3) float\n      Ray direction vectors\n    triangles_normal : (n, 3) float\n      Normal vector of triangles, optional\n    tree : rtree.Index\n      Rtree object holding triangle bounds\n\n    Returns\n    -----------\n    index_triangle : (h,) int\n      Index of triangles hit\n    index_ray : (h,) int\n      Index of ray that hit triangle\n    locations : (h, 3) float\n      Position of intersection in space\n    \"\"\"\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    ray_origins = np.asanyarray(ray_origins, dtype=np.float64)\n    ray_directions = np.asanyarray(ray_directions, dtype=np.float64)\n\n    # if we didn't get passed an r-tree for the bounds of each\n    # triangle create one here\n    if tree is None:\n        tree = triangles_mod.bounds_tree(triangles)\n\n    # find the list of likely triangles and which ray they\n    # correspond with, via rtree queries\n    ray_candidates, ray_id = ray_triangle_candidates(\n        ray_origins=ray_origins,\n        ray_directions=ray_directions,\n        tree=tree)\n\n    # get subsets which are corresponding rays and triangles\n    # (c,3,3) triangle candidates\n    triangle_candidates = triangles[ray_candidates]\n    # (c,3) origins and vectors for the rays\n    line_origins = ray_origins[ray_id]\n    line_directions = ray_directions[ray_id]\n\n    # get the plane origins and normals from the triangle candidates\n    plane_origins = triangle_candidates[:, 0, :]\n    if triangles_normal is None:\n        plane_normals, triangle_ok = triangles_mod.normals(\n            triangle_candidates)\n        if not triangle_ok.all():\n            raise ValueError('Invalid triangles!')\n    else:\n        plane_normals = triangles_normal[ray_candidates]\n\n    # find the intersection location of the rays with the planes\n    location, valid = intersections.planes_lines(\n        plane_origins=plane_origins,\n        plane_normals=plane_normals,\n        line_origins=line_origins,\n        line_directions=line_directions)\n\n    if (len(triangle_candidates) == 0 or\n            not valid.any()):\n        return [], [], []\n\n    # find the barycentric coordinates of each plane intersection on the\n    # triangle candidates\n    barycentric = triangles_mod.points_to_barycentric(\n        triangle_candidates[valid], location)\n\n    # the plane intersection is inside the triangle if all barycentric coordinates\n    # are between 0.0 and 1.0\n    hit = np.logical_and((barycentric > -tol.zero).all(axis=1),\n                         (barycentric < (1 + tol.zero)).all(axis=1))\n\n    # the result index of the triangle is a candidate with a valid plane intersection and\n    # a triangle which contains the plane intersection point\n    index_tri = ray_candidates[valid][hit]\n    # the ray index is a subset with a valid plane intersection and contained\n    # by a triangle\n    index_ray = ray_id[valid][hit]\n    # locations are already valid plane intersections, just mask by hits\n    location = location[hit]\n\n    # only return points that are forward from the origin\n    vector = location - ray_origins[index_ray]\n    distance = util.diagonal_dot(vector, ray_directions[index_ray])\n    forward = distance > -1e-6\n\n    index_tri = index_tri[forward]\n    index_ray = index_ray[forward]\n    location = location[forward]\n    distance = distance[forward]\n\n    if multiple_hits:\n        return index_tri, index_ray, location\n\n    # since we are not returning multiple hits, we need to\n    # figure out which hit is first\n    if len(index_ray) == 0:\n        return index_tri, index_ray, location\n\n    first = np.zeros(len(index_ray), dtype=np.bool)\n    groups = grouping.group(index_ray)\n    for group in groups:\n        index = group[distance[group].argmin()]\n        first[index] = True\n\n    return index_tri[first], index_ray[first], location[first]", "response": "Find the intersections between a set of triangles and a set of rays."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndoes broad - phase search for triangles that intersect the given rays.", "response": "def ray_triangle_candidates(ray_origins,\n                            ray_directions,\n                            tree):\n    \"\"\"\n    Do broad- phase search for triangles that the rays\n    may intersect.\n\n    Does this by creating a bounding box for the ray as it\n    passes through the volume occupied by the tree\n\n    Parameters\n    ------------\n    ray_origins:      (m,3) float, ray origin points\n    ray_directions:   (m,3) float, ray direction vectors\n    tree:             rtree object, contains AABB of each triangle\n\n    Returns\n    ----------\n    ray_candidates: (n,) int, triangle indexes\n    ray_id:         (n,) int, corresponding ray index for a triangle candidate\n    \"\"\"\n    ray_bounding = ray_bounds(ray_origins=ray_origins,\n                              ray_directions=ray_directions,\n                              bounds=tree.bounds)\n    ray_candidates = [[]] * len(ray_origins)\n    ray_id = [[]] * len(ray_origins)\n\n    for i, bounds in enumerate(ray_bounding):\n        ray_candidates[i] = np.array(list(tree.intersection(bounds)),\n                                     dtype=np.int)\n        ray_id[i] = np.ones(len(ray_candidates[i]), dtype=np.int) * i\n\n    ray_id = np.hstack(ray_id)\n    ray_candidates = np.hstack(ray_candidates)\n\n    return ray_candidates, ray_id"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ray_bounds(ray_origins,\n               ray_directions,\n               bounds,\n               buffer_dist=1e-5):\n    \"\"\"\n    Given a set of rays and a bounding box for the volume of interest\n    where the rays will be passing through, find the bounding boxes\n    of the rays as they pass through the volume.\n\n    Parameters\n    ------------\n    ray_origins:      (m,3) float, ray origin points\n    ray_directions:   (m,3) float, ray direction vectors\n    bounds:           (2,3) bounding box (min, max)\n    buffer_dist:      float, distance to pad zero width bounding boxes\n\n    Returns\n    ---------\n    ray_bounding: (n) set of AABB of rays passing through volume\n    \"\"\"\n\n    ray_origins = np.asanyarray(ray_origins, dtype=np.float64)\n    ray_directions = np.asanyarray(ray_directions, dtype=np.float64)\n\n    # bounding box we are testing against\n    bounds = np.asanyarray(bounds)\n\n    # find the primary axis of the vector\n    axis = np.abs(ray_directions).argmax(axis=1)\n    axis_bound = bounds.reshape((2, -1)).T[axis]\n    axis_ori = np.array([ray_origins[i][a]\n                         for i, a in enumerate(axis)]).reshape((-1, 1))\n    axis_dir = np.array([ray_directions[i][a]\n                         for i, a in enumerate(axis)]).reshape((-1, 1))\n\n    # parametric equation of a line\n    # point = direction*t + origin\n    # p = dt + o\n    # t = (p-o)/d\n    t = (axis_bound - axis_ori) / axis_dir\n\n    # prevent the bounding box from including triangles\n    # behind the ray origin\n    t[t < buffer_dist] = buffer_dist\n\n    # the value of t for both the upper and lower bounds\n    t_a = t[:, 0].reshape((-1, 1))\n    t_b = t[:, 1].reshape((-1, 1))\n\n    # the cartesion point for where the line hits the plane defined by\n    # axis\n    on_a = (ray_directions * t_a) + ray_origins\n    on_b = (ray_directions * t_b) + ray_origins\n\n    on_plane = np.column_stack(\n        (on_a, on_b)).reshape(\n        (-1, 2, ray_directions.shape[1]))\n\n    ray_bounding = np.hstack((on_plane.min(axis=1),\n                              on_plane.max(axis=1)))\n    # pad the bounding box by TOL_BUFFER\n    # not sure if this is necessary, but if the ray is  axis aligned\n    # this function will otherwise return zero volume bounding boxes\n    # which may or may not screw up the r-tree intersection queries\n    ray_bounding += np.array([-1, -1, -1, 1, 1, 1]) * buffer_dist\n\n    return ray_bounding", "response": "This function takes a set of rays and a bounding box for the volume of interest and returns the bounding boxes that are used to pass through the volume."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the intersections between the current mesh and a list of rays. Parameters ------------ ray_origins: (m,3) float, ray origin points ray_directions: (m,3) float, ray direction vectors multiple_hits: bool, consider multiple hits of each ray or not return_locations: bool, return hit locations or not Returns ----------- index_triangle: (h,) int, index of triangles hit index_ray: (h,) int, index of ray that hit triangle locations: (h,3) float, (optional) position of intersection in space", "response": "def intersects_id(self,\n                      ray_origins,\n                      ray_directions,\n                      return_locations=False,\n                      multiple_hits=True,\n                      **kwargs):\n        \"\"\"\n        Find the intersections between the current mesh and a list of rays.\n\n        Parameters\n        ------------\n        ray_origins:      (m,3) float, ray origin points\n        ray_directions:   (m,3) float, ray direction vectors\n        multiple_hits:    bool, consider multiple hits of each ray or not\n        return_locations: bool, return hit locations or not\n\n        Returns\n        -----------\n        index_triangle: (h,) int,    index of triangles hit\n        index_ray:      (h,) int,    index of ray that hit triangle\n        locations:      (h,3) float, (optional) position of intersection in space\n        \"\"\"\n        (index_tri,\n         index_ray,\n         locations) = ray_triangle_id(triangles=self.mesh.triangles,\n                                      ray_origins=ray_origins,\n                                      ray_directions=ray_directions,\n                                      tree=self.mesh.triangles_tree,\n                                      multiple_hits=multiple_hits,\n                                      triangles_normal=self.mesh.face_normals)\n        if return_locations:\n            if len(index_tri) == 0:\n                return index_tri, index_ray, locations\n            unique = grouping.unique_rows(np.column_stack((locations,\n                                                           index_ray)))[0]\n            return index_tri[unique], index_ray[unique], locations[unique]\n        return index_tri, index_ray"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if any triangle on the mesh intersects the object.", "response": "def intersects_any(self,\n                       ray_origins,\n                       ray_directions,\n                       **kwargs):\n        \"\"\"\n        Find out if each ray hit any triangle on the mesh.\n\n        Parameters\n        ------------\n        ray_origins:      (m,3) float, ray origin points\n        ray_directions:   (m,3) float, ray direction vectors\n\n        Returns\n        ---------\n        hit: boolean, whether any ray hit any triangle on the mesh\n        \"\"\"\n        index_tri, index_ray = self.intersects_id(ray_origins,\n                                                  ray_directions)\n        hit_any = np.zeros(len(ray_origins), dtype=np.bool)\n        hit_idx = np.unique(index_ray)\n        if len(hit_idx) > 0:\n            hit_any[hit_idx] = True\n        return hit_any"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dict_to_path(as_dict):\n    # start kwargs with initial value\n    result = as_dict.copy()\n    # map of constructors\n    loaders = {'Arc': Arc, 'Line': Line}\n    # pre- allocate entity array\n    entities = [None] * len(as_dict['entities'])\n    # run constructor for dict kwargs\n    for entity_index, entity in enumerate(as_dict['entities']):\n        entities[entity_index] = loaders[entity['type']](\n            points=entity['points'], closed=entity['closed'])\n    result['entities'] = entities\n\n    return result", "response": "Turn a pure dict into a dict containing entity objects that can be sent directly to a Path constructor."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lines_to_path(lines):\n    lines = np.asanyarray(lines, dtype=np.float64)\n\n    if util.is_shape(lines, (-1, (2, 3))):\n        # the case where we have a list of points\n        # we are going to assume they are connected\n        result = {'entities': np.array([Line(np.arange(len(lines)))]),\n                  'vertices': lines}\n        return result\n    elif util.is_shape(lines, (-1, 2, (2, 3))):\n        # case where we have line segments in 2D or 3D\n        dimension = lines.shape[-1]\n        # convert lines to even number of (n, dimension) points\n        lines = lines.reshape((-1, dimension))\n        # merge duplicate vertices\n        unique, inverse = grouping.unique_rows(lines)\n        # use scipy edges_to_path to skip creating\n        # a bajillion individual line entities which\n        # will be super slow vs. fewer polyline entities\n        return edges_to_path(edges=inverse.reshape((-1, 2)),\n                             vertices=lines[unique])\n    else:\n        raise ValueError('Lines must be (n,(2|3)) or (n,2,(2|3))')\n    return result", "response": "Converts a list of lines into a Path2D or Path3D object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload shapely. polygon. Polygon objects into a trimesh. path. Path2D object", "response": "def polygon_to_path(polygon):\n    \"\"\"\n    Load shapely Polygon objects into a trimesh.path.Path2D object\n\n    Parameters\n    -------------\n    polygon : shapely.geometry.Polygon\n      Input geometry\n\n    Returns\n    -------------\n    kwargs : dict\n      Keyword arguments for Path2D constructor\n    \"\"\"\n    # start with a single polyline for the exterior\n    entities = deque([Line(points=np.arange(\n        len(polygon.exterior.coords)))])\n    # start vertices\n    vertices = np.array(polygon.exterior.coords).tolist()\n\n    # append interiors as single Line objects\n    for boundary in polygon.interiors:\n        entities.append(Line(np.arange(len(boundary.coords)) +\n                             len(vertices)))\n        # append the new vertex array\n        vertices.extend(boundary.coords)\n\n    # make sure result arrays are numpy\n    kwargs = {'entities': np.array(entities),\n              'vertices': np.array(vertices)}\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef linestrings_to_path(multi):\n    # append to result as we go\n    entities = []\n    vertices = []\n\n    if not util.is_sequence(multi):\n        multi = [multi]\n\n    for line in multi:\n        # only append geometry with points\n        if hasattr(line, 'coords'):\n            coords = np.array(line.coords)\n            if len(coords) < 2:\n                continue\n            entities.append(Line(np.arange(len(coords)) +\n                                 len(vertices)))\n            vertices.extend(coords)\n\n    kwargs = {'entities': np.array(entities),\n              'vertices': np.array(vertices)}\n    return kwargs", "response": "Load shapely LineString objects into a trimesh. path. Path2D object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a mesh and face indices find the outline edges and turn them into a Path3D.", "response": "def faces_to_path(mesh, face_ids=None, **kwargs):\n    \"\"\"\n    Given a mesh and face indices find the outline edges and\n    turn them into a Path3D.\n\n    Parameters\n    ---------\n    mesh : trimesh.Trimesh\n      Triangulated surface in 3D\n    face_ids : (n,) int\n      Indexes referencing mesh.faces\n\n    Returns\n    ---------\n    kwargs : dict\n      Kwargs for Path3D constructor\n    \"\"\"\n    if face_ids is None:\n        edges = mesh.edges_sorted\n    else:\n        # take advantage of edge ordering to index as single row\n        edges = mesh.edges_sorted.reshape(\n            (-1, 6))[face_ids].reshape((-1, 2))\n    # an edge which occurs onely once is on the boundary\n    unique_edges = grouping.group_rows(\n        edges, require_count=1)\n    # add edges and vertices to kwargs\n    kwargs.update(edges_to_path(edges=edges[unique_edges],\n                                vertices=mesh.vertices))\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef edges_to_path(edges,\n                  vertices,\n                  **kwargs):\n    \"\"\"\n    Given an edge list of indices and associated vertices\n    representing lines, generate kwargs for a Path object.\n\n    Parameters\n    -----------\n    edges : (n, 2) int\n      Vertex indices of line segments\n    vertices : (m, dimension) float\n      Vertex positions where dimension is 2 or 3\n\n    Returns\n    ----------\n    kwargs: dict, kwargs for Path constructor\n    \"\"\"\n    # sequence of ordered traversals\n    dfs = graph.traversals(edges, mode='dfs')\n    # make sure every consecutive index in DFS\n    # traversal is an edge in the source edge list\n    dfs_connected = graph.fill_traversals(dfs, edges=edges)\n    # kwargs for Path constructor\n    # turn traversals into Line objects\n    lines = [Line(d) for d in dfs_connected]\n\n    kwargs.update({'entities': lines,\n                   'vertices': vertices})\n    return kwargs", "response": "Given an edge list of indices and associated vertices representing lines generate kwargs for a Path object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the minimum perpendicular distance from a point to a plane.", "response": "def point_plane_distance(points,\n                         plane_normal,\n                         plane_origin=[0.0, 0.0, 0.0]):\n    \"\"\"\n    The minimum perpendicular distance of a point to a plane.\n\n    Parameters\n    -----------\n    points:       (n, 3) float, points in space\n    plane_normal: (3,) float, normal vector\n    plane_origin: (3,) float, plane origin in space\n\n    Returns\n    ------------\n    distances:     (n,) float, distance from point to plane\n    \"\"\"\n    points = np.asanyarray(points, dtype=np.float64)\n    w = points - plane_origin\n    distances = np.dot(plane_normal, w.T) / np.linalg.norm(plane_normal)\n    return distances"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an approximate vector representing the major axis of points in the sequence.", "response": "def major_axis(points):\n    \"\"\"\n    Returns an approximate vector representing the major axis of points\n\n    Parameters\n    -------------\n    points: (n, dimension) float, points in space\n\n    Returns\n    -------------\n    axis: (dimension,) float, vector along approximate major axis\n    \"\"\"\n    U, S, V = np.linalg.svd(points)\n    axis = util.unitize(np.dot(S, V))\n    return axis"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef plane_fit(points):\n    # make sure input is numpy array\n    points = np.asanyarray(points, dtype=np.float64)\n    # make the plane origin the mean of the points\n    C = points.mean(axis=0)\n    # points offset by the plane origin\n    x = points - C\n    # create a (3, 3) matrix\n    M = np.dot(x.T, x)\n    # run SVD\n    N = np.linalg.svd(M)[0][:, -1]\n\n    return C, N", "response": "Given a set of points find an origin and normal vector of the next plane."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef radial_sort(points,\n                origin,\n                normal):\n    \"\"\"\n    Sorts a set of points radially (by angle) around an\n    origin/normal.\n\n    Parameters\n    --------------\n    points: (n,3) float, points in space\n    origin: (3,)  float, origin to sort around\n    normal: (3,)  float, vector to sort around\n\n    Returns\n    --------------\n    ordered: (n,3) flot, re- ordered points in space\n    \"\"\"\n\n    # create two axis perpendicular to each other and the normal,\n    # and project the points onto them\n    axis0 = [normal[0], normal[2], -normal[1]]\n    axis1 = np.cross(normal, axis0)\n    ptVec = points - origin\n    pr0 = np.dot(ptVec, axis0)\n    pr1 = np.dot(ptVec, axis1)\n\n    # calculate the angles of the points on the axis\n    angles = np.arctan2(pr0, pr1)\n\n    # return the points sorted by angle\n    return points[[np.argsort(angles)]]", "response": "Radial sort of a set of points around an anatomical dataset."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nproject a set of points onto a plane.", "response": "def project_to_plane(points,\n                     plane_normal=[0, 0, 1],\n                     plane_origin=[0, 0, 0],\n                     transform=None,\n                     return_transform=False,\n                     return_planar=True):\n    \"\"\"\n    Projects a set of (n,3) points onto a plane.\n\n    Parameters\n    ---------\n    points:           (n,3) array of points\n    plane_normal:     (3) normal vector of plane\n    plane_origin:     (3) point on plane\n    transform:        None or (4,4) matrix. If specified, normal/origin are ignored\n    return_transform: bool, if true returns the (4,4) matrix used to project points\n                      onto a plane\n    return_planar:    bool, if True, returns (n,2) points. If False, returns\n                      (n,3), where the Z column consists of zeros\n    \"\"\"\n\n    if np.all(np.abs(plane_normal) < tol.zero):\n        raise NameError('Normal must be nonzero!')\n\n    if transform is None:\n        transform = plane_transform(plane_origin, plane_normal)\n\n    transformed = transformations.transform_points(points, transform)\n    transformed = transformed[:, 0:(3 - int(return_planar))]\n\n    if return_transform:\n        polygon_to_3D = np.linalg.inv(transform)\n        return transformed, polygon_to_3D\n    return transformed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of points where no point is closer than radius.", "response": "def remove_close(points, radius):\n    \"\"\"\n    Given an (n, m) set of points where n=(2|3) return a list of points\n    where no point is closer than radius.\n\n    Parameters\n    ------------\n    points : (n, dimension) float\n      Points in space\n    radius : float\n      Minimum radius between result points\n\n    Returns\n    ------------\n    culled : (m, dimension) float\n      Points in space\n    mask : (n,) bool\n      Which points from the original set were returned\n    \"\"\"\n    from scipy.spatial import cKDTree as KDTree\n\n    tree = KDTree(points)\n    consumed = np.zeros(len(points), dtype=np.bool)\n    unique = np.zeros(len(points), dtype=np.bool)\n    for i in range(len(points)):\n        if consumed[i]:\n            continue\n        neighbors = tree.query_ball_point(points[i], r=radius)\n        consumed[neighbors] = True\n        unique[i] = True\n\n    return points[unique], unique"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef k_means(points, k, **kwargs):\n    from scipy.cluster.vq import kmeans\n    from scipy.spatial import cKDTree\n\n    points = np.asanyarray(points, dtype=np.float64)\n    points_std = points.std(axis=0)\n    whitened = points / points_std\n    centroids_whitened, distortion = kmeans(whitened, k, **kwargs)\n    centroids = centroids_whitened * points_std\n\n    # find which centroid each point is closest to\n    tree = cKDTree(centroids)\n    labels = tree.query(points, k=1)[1]\n\n    return centroids, labels", "response": "Find k - centroids that attempt to minimize the k - means problem"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind an ordering of points where each is visited and the next point is the closest in euclidean distance, and if there are multiple points with equal distance go to an arbitrary one. Assumes every point is visitable from every other point, i.e. the travelling salesman problem on a fully connected graph. It is not a MINIMUM traversal; rather it is a \"not totally goofy traversal, quickly.\" On random points this traversal is often ~20x shorter than random ordering. Parameters --------------- points : (n, dimension) float ND points in space start : int The index of points we should start at Returns --------------- traversal : (n,) int Ordered traversal visiting every point distances : (n - 1,) float The euclidean distance between points in traversal", "response": "def tsp(points, start=0):\n    \"\"\"\n    Find an ordering of points where each is visited and\n    the next point is the closest in euclidean distance,\n    and if there are multiple points with equal distance\n    go to an arbitrary one.\n\n    Assumes every point is visitable from every other point,\n    i.e. the travelling salesman problem on a fully connected\n    graph. It is not a MINIMUM traversal; rather it is a\n    \"not totally goofy traversal, quickly.\" On random points\n    this traversal is often ~20x shorter than random ordering.\n\n    Parameters\n    ---------------\n    points : (n, dimension) float\n      ND points in space\n    start : int\n      The index of points we should start at\n\n    Returns\n    ---------------\n    traversal : (n,) int\n      Ordered traversal visiting every point\n    distances : (n - 1,) float\n      The euclidean distance between points in traversal\n    \"\"\"\n    # points should be float\n    points = np.asanyarray(points, dtype=np.float64)\n\n    if len(points.shape) != 2:\n        raise ValueError('points must be (n, dimension)!')\n\n    # start should be an index\n    start = int(start)\n\n    # a mask of unvisited points by index\n    unvisited = np.ones(len(points), dtype=np.bool)\n    unvisited[start] = False\n\n    # traversal of points by index\n    traversal = np.zeros(len(points), dtype=np.int64) - 1\n    traversal[0] = start\n    # list of distances\n    distances = np.zeros(len(points) - 1, dtype=np.float64)\n    # a mask of indexes in order\n    index_mask = np.arange(len(points), dtype=np.int64)\n\n    # in the loop we want to call distances.sum(axis=1)\n    # a lot and it's actually kind of slow for \"reasons\"\n    # dot products with ones is equivalent and ~2x faster\n    sum_ones = np.ones(points.shape[1])\n\n    # loop through all points\n    for i in range(len(points) - 1):\n        # which point are we currently on\n        current = points[traversal[i]]\n\n        # do NlogN distance query\n        # use dot instead of .sum(axis=1) or np.linalg.norm\n        # as it is faster, also don't square root here\n        dist = np.dot((points[unvisited] - current) ** 2,\n                      sum_ones)\n\n        # minimum distance index\n        min_index = dist.argmin()\n        # successor is closest unvisited point\n        successor = index_mask[unvisited][min_index]\n        # update the mask\n        unvisited[successor] = False\n        # store the index to the traversal\n        traversal[i + 1] = successor\n        # store the distance\n        distances[i] = dist[min_index]\n\n    # we were comparing distance^2 so take square root\n    distances **= 0.5\n\n    return traversal, distances"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot an n - dimensional list of points using matplotlib", "response": "def plot_points(points, show=True):\n    \"\"\"\n    Plot an (n,3) list of points using matplotlib\n\n    Parameters\n    -------------\n    points : (n, 3) float\n      Points in space\n    show : bool\n      If False, will not show until plt.show() is called\n    \"\"\"\n    import matplotlib.pyplot as plt\n    from mpl_toolkits.mplot3d import Axes3D  # NOQA\n\n    points = np.asanyarray(points, dtype=np.float64)\n\n    if len(points.shape) != 2:\n        raise ValueError('Points must be (n, 2|3)!')\n\n    if points.shape[1] == 3:\n        fig = plt.figure()\n        ax = fig.add_subplot(111, projection='3d')\n        ax.scatter(*points.T)\n    elif points.shape[1] == 2:\n        plt.scatter(*points.T)\n    else:\n        raise ValueError('points not 2D/3D: {}'.format(\n            points.shape))\n\n    if show:\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self):\n        copied = PointCloud(vertices=None)\n\n        # copy vertex and face data\n        copied._data.data = copy.deepcopy(self._data.data)\n        # get metadata\n        copied.metadata = copy.deepcopy(self.metadata)\n\n        # make sure cache is set from here\n        copied._cache.clear()\n\n        return copied", "response": "Returns a deep copy of the current point cloud."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge vertices closer than tol. merge", "response": "def merge_vertices(self):\n        \"\"\"\n        Merge vertices closer than tol.merge (default: 1e-8)\n        \"\"\"\n        # run unique rows\n        unique, inverse = grouping.unique_rows(self.vertices)\n\n        # apply unique mask to vertices\n        self.vertices = self.vertices[unique]\n\n        # apply unique mask to colors\n        if (self.colors is not None and\n                len(self.colors) == len(inverse)):\n            self.colors = self.colors[unique]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef apply_transform(self, transform):\n        self.vertices = transformations.transform_points(self.vertices,\n                                                         matrix=transform)", "response": "Apply a homogenous transformation to the PointCloud\n        object in - place."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the axis aligned bounds of the PointCloud", "response": "def bounds(self):\n        \"\"\"\n        The axis aligned bounds of the PointCloud\n\n        Returns\n        ------------\n        bounds : (2, 3) float\n          Miniumum, Maximum verteex\n        \"\"\"\n        return np.array([self.vertices.min(axis=0),\n                         self.vertices.max(axis=0)])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn matrix to mirror at plane defined by point and normal vector.", "response": "def reflection_matrix(point, normal):\n    \"\"\"Return matrix to mirror at plane defined by point and normal vector.\n\n    >>> v0 = np.random.random(4) - 0.5\n    >>> v0[3] = 1.\n    >>> v1 = np.random.random(3) - 0.5\n    >>> R = reflection_matrix(v0, v1)\n    >>> np.allclose(2, np.trace(R))\n    True\n    >>> np.allclose(v0, np.dot(R, v0))\n    True\n    >>> v2 = v0.copy()\n    >>> v2[:3] += v1\n    >>> v3 = v0.copy()\n    >>> v2[:3] -= v1\n    >>> np.allclose(v2, np.dot(R, v3))\n    True\n\n    \"\"\"\n    normal = unit_vector(normal[:3])\n    M = np.identity(4)\n    M[:3, :3] -= 2.0 * np.outer(normal, normal)\n    M[:3, 3] = (2.0 * np.dot(point[:3], normal)) * normal\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reflection_from_matrix(matrix):\n    M = np.array(matrix, dtype=np.float64, copy=False)\n    # normal: unit eigenvector corresponding to eigenvalue -1\n    w, V = np.linalg.eig(M[:3, :3])\n    i = np.where(abs(np.real(w) + 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\"no unit eigenvector corresponding to eigenvalue -1\")\n    normal = np.real(V[:, i[0]]).squeeze()\n    # point: any unit eigenvector corresponding to eigenvalue 1\n    w, V = np.linalg.eig(M)\n    i = np.where(abs(np.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\"no unit eigenvector corresponding to eigenvalue 1\")\n    point = np.real(V[:, i[-1]]).squeeze()\n    point /= point[3]\n    return point, normal", "response": "Return mirror plane point and normal vector from reflection matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn matrix to rotate about axis defined by point and direction.", "response": "def rotation_matrix(angle, direction, point=None):\n    \"\"\"\n    Return matrix to rotate about axis defined by point and\n    direction.\n\n    Parameters\n    -------------\n    angle     : float, or sympy.Symbol\n                Angle, in radians or symbolic angle\n    direction : (3,) float\n                Unit vector along rotation axis\n    point     : (3, ) float, or None\n                Origin point of rotation axis\n\n    Returns\n    -------------\n    matrix : (4, 4) float, or (4, 4) sympy.Matrix\n             Homogenous transformation matrix\n\n    Examples\n    -------------\n    >>> R = rotation_matrix(math.pi/2, [0, 0, 1], [1, 0, 0])\n    >>> np.allclose(np.dot(R, [0, 0, 0, 1]), [1, -1, 0, 1])\n    True\n    >>> angle = (random.random() - 0.5) * (2*math.pi)\n    >>> direc = np.random.random(3) - 0.5\n    >>> point = np.random.random(3) - 0.5\n    >>> R0 = rotation_matrix(angle, direc, point)\n    >>> R1 = rotation_matrix(angle-2*math.pi, direc, point)\n    >>> is_same_transform(R0, R1)\n    True\n    >>> R0 = rotation_matrix(angle, direc, point)\n    >>> R1 = rotation_matrix(-angle, -direc, point)\n    >>> is_same_transform(R0, R1)\n    True\n    >>> I = np.identity(4, np.float64)\n    >>> np.allclose(I, rotation_matrix(math.pi*2, direc))\n    True\n    >>> np.allclose(2, np.trace(rotation_matrix(math.pi/2,direc,point)))\n    True\n\n    \"\"\"\n    # special case sympy symbolic angles\n    if type(angle).__name__ == 'Symbol':\n        import sympy as sp\n        sina = sp.sin(angle)\n        cosa = sp.cos(angle)\n    else:\n        sina = math.sin(angle)\n        cosa = math.cos(angle)\n\n    direction = unit_vector(direction[:3])\n    # rotation matrix around unit vector\n    M = np.diag([cosa, cosa, cosa, 1.0])\n    M[:3, :3] += np.outer(direction, direction) * (1.0 - cosa)\n\n    direction = direction * sina\n    M[:3, :3] += np.array([[0.0, -direction[2], direction[1]],\n                           [direction[2], 0.0, -direction[0]],\n                           [-direction[1], direction[0], 0.0]])\n\n    # if point is specified, rotation is not around origin\n    if point is not None:\n        point = np.array(point[:3], dtype=np.float64, copy=False)\n        M[:3, 3] = point - np.dot(M[:3, :3], point)\n\n    # return symbolic angles as sympy Matrix objects\n    if type(angle).__name__ == 'Symbol':\n        return sp.Matrix(M)\n\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn matrix to obtain normalized device coordinates from frustum.", "response": "def clip_matrix(left, right, bottom, top, near, far, perspective=False):\n    \"\"\"Return matrix to obtain normalized device coordinates from frustum.\n\n    The frustum bounds are axis-aligned along x (left, right),\n    y (bottom, top) and z (near, far).\n\n    Normalized device coordinates are in range [-1, 1] if coordinates are\n    inside the frustum.\n\n    If perspective is True the frustum is a truncated pyramid with the\n    perspective point at origin and direction along z axis, otherwise an\n    orthographic canonical view volume (a box).\n\n    Homogeneous coordinates transformed by the perspective clip matrix\n    need to be dehomogenized (divided by w coordinate).\n\n    >>> frustum = np.random.rand(6)\n    >>> frustum[1] += frustum[0]\n    >>> frustum[3] += frustum[2]\n    >>> frustum[5] += frustum[4]\n    >>> M = clip_matrix(perspective=False, *frustum)\n    >>> a = np.dot(M, [frustum[0], frustum[2], frustum[4], 1])\n    >>> np.allclose(a, [-1., -1., -1.,  1.])\n    True\n    >>> b = np.dot(M, [frustum[1], frustum[3], frustum[5], 1])\n    >>> np.allclose(b, [ 1.,  1.,  1.,  1.])\n    True\n    >>> M = clip_matrix(perspective=True, *frustum)\n    >>> v = np.dot(M, [frustum[0], frustum[2], frustum[4], 1])\n    >>> c = v / v[3]\n    >>> np.allclose(c, [-1., -1., -1.,  1.])\n    True\n    >>> v = np.dot(M, [frustum[1], frustum[3], frustum[4], 1])\n    >>> d = v / v[3]\n    >>> np.allclose(d, [ 1.,  1., -1.,  1.])\n    True\n\n    \"\"\"\n    if left >= right or bottom >= top or near >= far:\n        raise ValueError(\"invalid frustum\")\n    if perspective:\n        if near <= _EPS:\n            raise ValueError(\"invalid frustum: near <= 0\")\n        t = 2.0 * near\n        M = [[t / (left - right), 0.0, (right + left) / (right - left), 0.0],\n             [0.0, t / (bottom - top), (top + bottom) / (top - bottom), 0.0],\n             [0.0, 0.0, (far + near) / (near - far), t * far / (far - near)],\n             [0.0, 0.0, -1.0, 0.0]]\n    else:\n        M = [[2.0 / (right - left), 0.0, 0.0, (right + left) / (left - right)],\n             [0.0, 2.0 / (top - bottom), 0.0, (top + bottom) / (bottom - top)],\n             [0.0, 0.0, 2.0 / (far - near), (far + near) / (near - far)],\n             [0.0, 0.0, 0.0, 1.0]]\n    return np.array(M)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns matrix to shear by angle along direction vector on shear plane.", "response": "def shear_matrix(angle, direction, point, normal):\n    \"\"\"Return matrix to shear by angle along direction vector on shear plane.\n\n    The shear plane is defined by a point and normal vector. The direction\n    vector must be orthogonal to the plane's normal vector.\n\n    A point P is transformed by the shear matrix into P\" such that\n    the vector P-P\" is parallel to the direction vector and its extent is\n    given by the angle of P-P'-P\", where P' is the orthogonal projection\n    of P onto the shear plane.\n\n    >>> angle = (random.random() - 0.5) * 4*math.pi\n    >>> direct = np.random.random(3) - 0.5\n    >>> point = np.random.random(3) - 0.5\n    >>> normal = np.cross(direct, np.random.random(3))\n    >>> S = shear_matrix(angle, direct, point, normal)\n    >>> np.allclose(1, np.linalg.det(S))\n    True\n\n    \"\"\"\n    normal = unit_vector(normal[:3])\n    direction = unit_vector(direction[:3])\n    if abs(np.dot(normal, direction)) > 1e-6:\n        raise ValueError(\"direction and normal vectors are not orthogonal\")\n    angle = math.tan(angle)\n    M = np.identity(4)\n    M[:3, :3] += angle * np.outer(direction, normal)\n    M[:3, 3] = -angle * np.dot(point[:3], normal) * direction\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn shear angle direction and plane from shear matrix.", "response": "def shear_from_matrix(matrix):\n    \"\"\"Return shear angle, direction and plane from shear matrix.\n\n    >>> angle  = np.pi / 2.0\n    >>> direct = [0.0, 1.0, 0.0]\n    >>> point  = [0.0, 0.0, 0.0]\n    >>> normal = np.cross(direct, np.roll(direct,1))\n    >>> S0 = shear_matrix(angle, direct, point, normal)\n    >>> angle, direct, point, normal = shear_from_matrix(S0)\n    >>> S1 = shear_matrix(angle, direct, point, normal)\n    >>> is_same_transform(S0, S1)\n    True\n\n    \"\"\"\n    M = np.array(matrix, dtype=np.float64, copy=False)\n    M33 = M[:3, :3]\n    # normal: cross independent eigenvectors corresponding to the eigenvalue 1\n    w, V = np.linalg.eig(M33)\n\n    i = np.where(abs(np.real(w) - 1.0) < 1e-4)[0]\n    if len(i) < 2:\n        raise ValueError(\"no two linear independent eigenvectors found %s\" % w)\n    V = np.real(V[:, i]).squeeze().T\n    lenorm = -1.0\n    for i0, i1 in ((0, 1), (0, 2), (1, 2)):\n        n = np.cross(V[i0], V[i1])\n        w = vector_norm(n)\n        if w > lenorm:\n            lenorm = w\n            normal = n\n    normal /= lenorm\n    # direction and angle\n    direction = np.dot(M33 - np.identity(3), normal)\n    angle = vector_norm(direction)\n    direction /= angle\n    angle = math.atan(angle)\n    # point: eigenvector corresponding to eigenvalue 1\n    w, V = np.linalg.eig(M)\n\n    i = np.where(abs(np.real(w) - 1.0) < 1e-8)[0]\n    if not len(i):\n        raise ValueError(\"no eigenvector corresponding to eigenvalue 1\")\n    point = np.real(V[:, i[-1]]).squeeze()\n    point /= point[3]\n    return angle, direction, point, normal"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compose_matrix(scale=None, shear=None, angles=None, translate=None,\n                   perspective=None):\n    \"\"\"Return transformation matrix from sequence of transformations.\n\n    This is the inverse of the decompose_matrix function.\n\n    Sequence of transformations:\n        scale : vector of 3 scaling factors\n        shear : list of shear factors for x-y, x-z, y-z axes\n        angles : list of Euler angles about static x, y, z axes\n        translate : translation vector along x, y, z axes\n        perspective : perspective partition of matrix\n\n    >>> scale = np.random.random(3) - 0.5\n    >>> shear = np.random.random(3) - 0.5\n    >>> angles = (np.random.random(3) - 0.5) * (2*math.pi)\n    >>> trans = np.random.random(3) - 0.5\n    >>> persp = np.random.random(4) - 0.5\n    >>> M0 = compose_matrix(scale, shear, angles, trans, persp)\n    >>> result = decompose_matrix(M0)\n    >>> M1 = compose_matrix(*result)\n    >>> is_same_transform(M0, M1)\n    True\n\n    \"\"\"\n    M = np.identity(4)\n    if perspective is not None:\n        P = np.identity(4)\n        P[3, :] = perspective[:4]\n        M = np.dot(M, P)\n    if translate is not None:\n        T = np.identity(4)\n        T[:3, 3] = translate[:3]\n        M = np.dot(M, T)\n    if angles is not None:\n        R = euler_matrix(angles[0], angles[1], angles[2], 'sxyz')\n        M = np.dot(M, R)\n    if shear is not None:\n        Z = np.identity(4)\n        Z[1, 2] = shear[2]\n        Z[0, 2] = shear[1]\n        Z[0, 1] = shear[0]\n        M = np.dot(M, Z)\n    if scale is not None:\n        S = np.identity(4)\n        S[0, 0] = scale[0]\n        S[1, 1] = scale[1]\n        S[2, 2] = scale[2]\n        M = np.dot(M, S)\n    M /= M[3, 3]\n    return M", "response": "Return a transformation matrix from sequence of transformations."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef euler_matrix(ai, aj, ak, axes='sxyz'):\n    try:\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes]\n    except (AttributeError, KeyError):\n        _TUPLE2AXES[axes]  # validation\n        firstaxis, parity, repetition, frame = axes\n\n    i = firstaxis\n    j = _NEXT_AXIS[i + parity]\n    k = _NEXT_AXIS[i - parity + 1]\n\n    if frame:\n        ai, ak = ak, ai\n    if parity:\n        ai, aj, ak = -ai, -aj, -ak\n\n    si, sj, sk = math.sin(ai), math.sin(aj), math.sin(ak)\n    ci, cj, ck = math.cos(ai), math.cos(aj), math.cos(ak)\n    cc, cs = ci * ck, ci * sk\n    sc, ss = si * ck, si * sk\n\n    M = np.identity(4)\n    if repetition:\n        M[i, i] = cj\n        M[i, j] = sj * si\n        M[i, k] = sj * ci\n        M[j, i] = sj * sk\n        M[j, j] = -cj * ss + cc\n        M[j, k] = -cj * cs - sc\n        M[k, i] = -sj * ck\n        M[k, j] = cj * sc + cs\n        M[k, k] = cj * cc - ss\n    else:\n        M[i, i] = cj * ck\n        M[i, j] = sj * sc - cs\n        M[i, k] = sj * cc + ss\n        M[j, i] = cj * sk\n        M[j, j] = sj * ss + cc\n        M[j, k] = sj * cs - sc\n        M[k, i] = -sj\n        M[k, j] = cj * si\n        M[k, k] = cj * ci\n    return M", "response": "Return homogeneous rotation matrix from Euler angles and axis sequence."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef euler_from_matrix(matrix, axes='sxyz'):\n    try:\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes.lower()]\n    except (AttributeError, KeyError):\n        _TUPLE2AXES[axes]  # validation\n        firstaxis, parity, repetition, frame = axes\n\n    i = firstaxis\n    j = _NEXT_AXIS[i + parity]\n    k = _NEXT_AXIS[i - parity + 1]\n\n    M = np.array(matrix, dtype=np.float64, copy=False)[:3, :3]\n    if repetition:\n        sy = math.sqrt(M[i, j] * M[i, j] + M[i, k] * M[i, k])\n        if sy > _EPS:\n            ax = math.atan2(M[i, j], M[i, k])\n            ay = math.atan2(sy, M[i, i])\n            az = math.atan2(M[j, i], -M[k, i])\n        else:\n            ax = math.atan2(-M[j, k], M[j, j])\n            ay = math.atan2(sy, M[i, i])\n            az = 0.0\n    else:\n        cy = math.sqrt(M[i, i] * M[i, i] + M[j, i] * M[j, i])\n        if cy > _EPS:\n            ax = math.atan2(M[k, j], M[k, k])\n            ay = math.atan2(-M[k, i], cy)\n            az = math.atan2(M[j, i], M[i, i])\n        else:\n            ax = math.atan2(-M[j, k], M[j, j])\n            ay = math.atan2(-M[k, i], cy)\n            az = 0.0\n\n    if parity:\n        ax, ay, az = -ax, -ay, -az\n    if frame:\n        ax, az = az, ax\n    return ax, ay, az", "response": "Return Euler angles from rotation matrix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a quaternion from Euler angles and axis sequence.", "response": "def quaternion_from_euler(ai, aj, ak, axes='sxyz'):\n    \"\"\"Return quaternion from Euler angles and axis sequence.\n\n    ai, aj, ak : Euler's roll, pitch and yaw angles\n    axes : One of 24 axis sequences as string or encoded tuple\n\n    >>> q = quaternion_from_euler(1, 2, 3, 'ryxz')\n    >>> np.allclose(q, [0.435953, 0.310622, -0.718287, 0.444435])\n    True\n\n    \"\"\"\n    try:\n        firstaxis, parity, repetition, frame = _AXES2TUPLE[axes.lower()]\n    except (AttributeError, KeyError):\n        _TUPLE2AXES[axes]  # validation\n        firstaxis, parity, repetition, frame = axes\n\n    i = firstaxis + 1\n    j = _NEXT_AXIS[i + parity - 1] + 1\n    k = _NEXT_AXIS[i - parity] + 1\n\n    if frame:\n        ai, ak = ak, ai\n    if parity:\n        aj = -aj\n\n    ai /= 2.0\n    aj /= 2.0\n    ak /= 2.0\n    ci = math.cos(ai)\n    si = math.sin(ai)\n    cj = math.cos(aj)\n    sj = math.sin(aj)\n    ck = math.cos(ak)\n    sk = math.sin(ak)\n    cc = ci * ck\n    cs = ci * sk\n    sc = si * ck\n    ss = si * sk\n\n    q = np.empty((4, ))\n    if repetition:\n        q[0] = cj * (cc - ss)\n        q[i] = cj * (cs + sc)\n        q[j] = sj * (cc + ss)\n        q[k] = sj * (cs - sc)\n    else:\n        q[0] = cj * cc + sj * ss\n        q[i] = cj * sc - sj * cs\n        q[j] = cj * ss + sj * cc\n        q[k] = cj * cs - sj * sc\n    if parity:\n        q[j] *= -1.0\n\n    return q"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn quaternion about axis.", "response": "def quaternion_about_axis(angle, axis):\n    \"\"\"Return quaternion for rotation about axis.\n\n    >>> q = quaternion_about_axis(0.123, [1, 0, 0])\n    >>> np.allclose(q, [0.99810947, 0.06146124, 0, 0])\n    True\n\n    \"\"\"\n    q = np.array([0.0, axis[0], axis[1], axis[2]])\n    qlen = vector_norm(q)\n    if qlen > _EPS:\n        q *= math.sin(angle / 2.0) / qlen\n    q[0] = math.cos(angle / 2.0)\n    return q"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef quaternion_conjugate(quaternion):\n    q = np.array(quaternion, dtype=np.float64, copy=True)\n    np.negative(q[1:], q[1:])\n    return q", "response": "Return conjugate of quaternion."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning inverse of quaternion.", "response": "def quaternion_inverse(quaternion):\n    \"\"\"Return inverse of quaternion.\n\n    >>> q0 = random_quaternion()\n    >>> q1 = quaternion_inverse(q0)\n    >>> np.allclose(quaternion_multiply(q0, q1), [1, 0, 0, 0])\n    True\n\n    \"\"\"\n    q = np.array(quaternion, dtype=np.float64, copy=True)\n    np.negative(q[1:], q[1:])\n    return q / np.dot(q, q)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning imaginary part of quaternion.", "response": "def quaternion_imag(quaternion):\n    \"\"\"Return imaginary part of quaternion.\n\n    >>> quaternion_imag([3, 0, 1, 2])\n    array([0., 1., 2.])\n\n    \"\"\"\n    return np.array(quaternion[1:4], dtype=np.float64, copy=True)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef random_quaternion(rand=None):\n    if rand is None:\n        rand = np.random.rand(3)\n    else:\n        assert len(rand) == 3\n    r1 = np.sqrt(1.0 - rand[0])\n    r2 = np.sqrt(rand[0])\n    pi2 = math.pi * 2.0\n    t1 = pi2 * rand[1]\n    t2 = pi2 * rand[2]\n    return np.array([np.cos(t2) * r2, np.sin(t1) * r1,\n                     np.cos(t1) * r1, np.sin(t2) * r2])", "response": "Return uniform random unit quaternion."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef arcball_constrain_to_axis(point, axis):\n    v = np.array(point, dtype=np.float64, copy=True)\n    a = np.array(axis, dtype=np.float64, copy=True)\n    v -= a * np.dot(a, v)  # on plane\n    n = vector_norm(v)\n    if n > _EPS:\n        if v[2] < 0.0:\n            np.negative(v, v)\n        v /= n\n        return v\n    if a[2] == 1.0:\n        return np.array([1.0, 0.0, 0.0])\n    return unit_vector([-a[1], a[0], 0.0])", "response": "Return sphere point perpendicular to axis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arcball_nearest_axis(point, axes):\n    point = np.array(point, dtype=np.float64, copy=False)\n    nearest = None\n    mx = -1.0\n    for axis in axes:\n        t = np.dot(arcball_constrain_to_axis(point, axis), point)\n        if t > mx:\n            nearest = axis\n            mx = t\n    return nearest", "response": "Return axis which arc is nearest to point."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn length i. e. Euclidean norm of ndarray along axis.", "response": "def vector_norm(data, axis=None, out=None):\n    \"\"\"Return length, i.e. Euclidean norm, of ndarray along axis.\n\n    >>> v = np.random.random(3)\n    >>> n = vector_norm(v)\n    >>> np.allclose(n, np.linalg.norm(v))\n    True\n    >>> v = np.random.rand(6, 5, 3)\n    >>> n = vector_norm(v, axis=-1)\n    >>> np.allclose(n, np.sqrt(np.sum(v*v, axis=2)))\n    True\n    >>> n = vector_norm(v, axis=1)\n    >>> np.allclose(n, np.sqrt(np.sum(v*v, axis=1)))\n    True\n    >>> v = np.random.rand(5, 4, 3)\n    >>> n = np.empty((5, 3))\n    >>> vector_norm(v, axis=1, out=n)\n    >>> np.allclose(n, np.sqrt(np.sum(v*v, axis=1)))\n    True\n    >>> vector_norm([])\n    0.0\n    >>> vector_norm([1])\n    1.0\n\n    \"\"\"\n    data = np.array(data, dtype=np.float64, copy=True)\n    if out is None:\n        if data.ndim == 1:\n            return math.sqrt(np.dot(data, data))\n        data *= data\n        out = np.atleast_1d(np.sum(data, axis=axis))\n        np.sqrt(out, out)\n        return out\n    else:\n        data *= data\n        np.sum(data, axis=axis, out=out)\n        np.sqrt(out, out)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn vector perpendicular to vectors.", "response": "def vector_product(v0, v1, axis=0):\n    \"\"\"Return vector perpendicular to vectors.\n\n    >>> v = vector_product([2, 0, 0], [0, 3, 0])\n    >>> np.allclose(v, [0, 0, 6])\n    True\n    >>> v0 = [[2, 0, 0, 2], [0, 2, 0, 2], [0, 0, 2, 2]]\n    >>> v1 = [[3], [0], [0]]\n    >>> v = vector_product(v0, v1)\n    >>> np.allclose(v, [[0, 0, 0, 0], [0, 0, 6, 6], [0, -6, 0, -6]])\n    True\n    >>> v0 = [[2, 0, 0], [2, 0, 0], [0, 2, 0], [2, 0, 0]]\n    >>> v1 = [[0, 3, 0], [0, 0, 3], [0, 0, 3], [3, 3, 3]]\n    >>> v = vector_product(v0, v1, axis=1)\n    >>> np.allclose(v, [[0, 0, 6], [0, -6, 0], [6, 0, 0], [0, -6, 6]])\n    True\n\n    \"\"\"\n    return np.cross(v0, v1, axis=axis)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning concatenation of series of transformation matrices.", "response": "def concatenate_matrices(*matrices):\n    \"\"\"Return concatenation of series of transformation matrices.\n\n    >>> M = np.random.rand(16).reshape((4, 4)) - 0.5\n    >>> np.allclose(M, concatenate_matrices(M))\n    True\n    >>> np.allclose(np.dot(M, M.T), concatenate_matrices(M, M.T))\n    True\n\n    \"\"\"\n    M = np.identity(4)\n    for i in matrices:\n        M = np.dot(M, i)\n    return M"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if two matrices perform same transformation.", "response": "def is_same_transform(matrix0, matrix1):\n    \"\"\"Return True if two matrices perform same transformation.\n\n    >>> is_same_transform(np.identity(4), np.identity(4))\n    True\n    >>> is_same_transform(np.identity(4), random_rotation_matrix())\n    False\n\n    \"\"\"\n    matrix0 = np.array(matrix0, dtype=np.float64, copy=True)\n    matrix0 /= matrix0[3, 3]\n    matrix1 = np.array(matrix1, dtype=np.float64, copy=True)\n    matrix1 /= matrix1[3, 3]\n    return np.allclose(matrix0, matrix1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if two quaternions are equal.", "response": "def is_same_quaternion(q0, q1):\n    \"\"\"Return True if two quaternions are equal.\"\"\"\n    q0 = np.array(q0)\n    q1 = np.array(q1)\n    return np.allclose(q0, q1) or np.allclose(q0, -q1)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform_around(matrix, point):\n    point = np.asanyarray(point)\n    matrix = np.asanyarray(matrix)\n    dim = len(point)\n    if matrix.shape != (dim + 1,\n                        dim + 1):\n        raise ValueError('matrix must be (d+1, d+1)')\n\n    translate = np.eye(dim + 1)\n    translate[:dim, dim] = -point\n    result = np.dot(matrix, translate)\n    translate[:dim, dim] = point\n    result = np.dot(translate, result)\n\n    return result", "response": "Given a transformation matrix apply its rotation\n    around a point in space."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a 2D homogenous transformation matrix for a single page of the page.", "response": "def planar_matrix(offset=None,\n                  theta=None,\n                  point=None):\n    \"\"\"\n    2D homogeonous transformation matrix\n\n    Parameters\n    ----------\n    offset : (2,) float\n      XY offset\n    theta : float\n      Rotation around Z in radians\n    point :  (2, ) float\n      point to rotate around\n\n    Returns\n    ----------\n    matrix : (3, 3) flat\n      Homogenous 2D transformation matrix\n    \"\"\"\n    if offset is None:\n        offset = [0.0, 0.0]\n    if theta is None:\n        theta = 0.0\n    offset = np.asanyarray(offset, dtype=np.float64)\n    theta = float(theta)\n    if not np.isfinite(theta):\n        raise ValueError('theta must be finite angle!')\n    if offset.shape != (2,):\n        raise ValueError('offset must be length 2!')\n\n    T = np.eye(3, dtype=np.float64)\n    s = np.sin(theta)\n    c = np.cos(theta)\n\n    T[0, 0:2] = [c, s]\n    T[1, 0:2] = [-s, c]\n    T[0:2, 2] = offset\n\n    if point is not None:\n        T = transform_around(matrix=T, point=point)\n\n    return T"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef planar_matrix_to_3D(matrix_2D):\n\n    matrix_2D = np.asanyarray(matrix_2D, dtype=np.float64)\n    if matrix_2D.shape != (3, 3):\n        raise ValueError('Homogenous 2D transformation matrix required!')\n\n    matrix_3D = np.eye(4)\n    # translation\n    matrix_3D[0:2, 3] = matrix_2D[0:2, 2]\n    # rotation from 2D to around Z\n    matrix_3D[0:2, 0:2] = matrix_2D[0:2, 0:2]\n\n    return matrix_3D", "response": "Convert a 2D homogenous rotation matrix to a 3D rotation matrix that is rotating around the Z axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a spherical coordinate vector, find the rotation that will transform a [0,0,1] vector to those coordinates Parameters ----------- theta: float, rotation angle in radians phi: float, rotation angle in radians Returns ---------- matrix: (4,4) rotation matrix where the following will be a cartesian vector in the direction of the input spherical coordinats: np.dot(matrix, [0,0,1,0])", "response": "def spherical_matrix(theta, phi, axes='sxyz'):\n    \"\"\"\n    Give a spherical coordinate vector, find the rotation that will\n    transform a [0,0,1] vector to those coordinates\n\n    Parameters\n    -----------\n    theta: float, rotation angle in radians\n    phi:   float, rotation angle in radians\n\n    Returns\n    ----------\n    matrix: (4,4) rotation matrix where the following will\n             be a cartesian vector in the direction of the\n             input spherical coordinats:\n                np.dot(matrix, [0,0,1,0])\n\n    \"\"\"\n    result = euler_matrix(0.0, phi, theta, axes=axes)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform a set of points into a new set of points.", "response": "def transform_points(points,\n                     matrix,\n                     translate=True):\n    \"\"\"\n    Returns points, rotated by transformation matrix\n\n    If points is (n,2), matrix must be (3,3)\n    if points is (n,3), matrix must be (4,4)\n\n    Parameters\n    ----------\n    points : (n, d) float\n      Points where d is 2 or 3\n    matrix : (3,3) or (4,4) float\n      Homogenous rotation matrix\n    translate : bool\n      Apply translation from matrix or not\n\n    Returns\n    ----------\n    transformed : (n,d) float\n      Transformed points\n    \"\"\"\n    points = np.asanyarray(points, dtype=np.float64)\n    # no points no cry\n    if len(points) == 0:\n        return points.copy()\n\n    matrix = np.asanyarray(matrix, dtype=np.float64)\n    if (len(points.shape) != 2 or\n            (points.shape[1] + 1 != matrix.shape[1])):\n        raise ValueError('matrix shape ({}) doesn\\'t match points ({})'.format(\n            matrix.shape,\n            points.shape))\n\n    # check to see if we've been passed an identity matrix\n    identity = np.abs(matrix - np.eye(matrix.shape[0])).max()\n    if identity < 1e-8:\n        return np.ascontiguousarray(points.copy())\n\n    dimension = points.shape[1]\n    column = np.zeros(len(points)) + int(bool(translate))\n    stacked = np.column_stack((points, column))\n    transformed = np.dot(matrix, stacked.T).T[:, :dimension]\n    transformed = np.ascontiguousarray(transformed)\n    return transformed"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks to make sure a homogeonous transformation matrix is a rigid body transform.", "response": "def is_rigid(matrix):\n    \"\"\"\n    Check to make sure a homogeonous transformation matrix is\n    a rigid body transform.\n\n    Parameters\n    -----------\n    matrix: possibly a transformation matrix\n\n    Returns\n    -----------\n    check: bool, True if matrix is a valid (4,4) rigid body transform.\n    \"\"\"\n\n    matrix = np.asanyarray(matrix, dtype=np.float64)\n\n    if matrix.shape != (4, 4):\n        return False\n\n    if not np.allclose(matrix[-1], [0, 0, 0, 1]):\n        return False\n\n    check = np.dot(matrix[:3, :3],\n                   matrix[:3, :3].T)\n\n    return np.allclose(check, np.eye(3))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplacing Arcball in trackball center and radius.", "response": "def place(self, center, radius):\n        \"\"\"Place Arcball, e.g. when window size changes.\n\n        center : sequence[2]\n            Window coordinates of trackball center.\n        radius : float\n            Radius of trackball in window coordinates.\n\n        \"\"\"\n        self._radius = float(radius)\n        self._center[0] = center[0]\n        self._center[1] = center[1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setaxes(self, *axes):\n        if axes is None:\n            self._axes = None\n        else:\n            self._axes = [unit_vector(axis) for axis in axes]", "response": "Set axes to constrain rotations."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef down(self, point):\n        self._vdown = arcball_map_to_sphere(point, self._center, self._radius)\n        self._qdown = self._qpre = self._qnow\n        if self._constrain and self._axes is not None:\n            self._axis = arcball_nearest_axis(self._vdown, self._axes)\n            self._vdown = arcball_constrain_to_axis(self._vdown, self._axis)\n        else:\n            self._axis = None", "response": "Set initial cursor window coordinates and pick constrain - axis."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef next(self, acceleration=0.0):\n        q = quaternion_slerp(self._qpre, self._qnow, 2.0 + acceleration, False)\n        self._qpre, self._qnow = self._qnow, q", "response": "Continue rotation in direction of last drag."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate a shapely. geometry. Polygon object.", "response": "def validate_polygon(obj):\n    \"\"\"\n    Make sure an input can be returned as a valid polygon.\n\n    Parameters\n    -------------\n    obj : shapely.geometry.Polygon, str (wkb), or (n, 2) float\n      Object which might be a polygon\n\n    Returns\n    ------------\n    polygon : shapely.geometry.Polygon\n      Valid polygon object\n\n    Raises\n    -------------\n    ValueError\n      If a valid finite- area polygon isn't available\n    \"\"\"\n    if isinstance(obj, Polygon):\n        polygon = obj\n    elif util.is_shape(obj, (-1, 2)):\n        polygon = Polygon(obj)\n    elif util.is_string(obj):\n        polygon = load_wkb(obj)\n    else:\n        raise ValueError('Input not a polygon!')\n\n    if (not polygon.is_valid or\n            polygon.area < tol.zero):\n        raise ValueError('Polygon is zero- area or invalid!')\n    return polygon"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extrude_polygon(polygon,\n                    height,\n                    **kwargs):\n    \"\"\"\n    Extrude a 2D shapely polygon into a 3D mesh\n\n    Parameters\n    ----------\n    polygon : shapely.geometry.Polygon\n      2D geometry to extrude\n    height : float\n      Distance to extrude polygon along Z\n    **kwargs:\n        passed to Trimesh\n\n    Returns\n    ----------\n    mesh : trimesh.Trimesh\n      Resulting extrusion as watertight body\n    \"\"\"\n    vertices, faces = triangulate_polygon(polygon, **kwargs)\n    mesh = extrude_triangulation(vertices=vertices,\n                                 faces=faces,\n                                 height=height,\n                                 **kwargs)\n    return mesh", "response": "Extrude a shapely polygon into a 3D mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sweep_polygon(polygon,\n                  path,\n                  angles=None,\n                  **kwargs):\n    \"\"\"\n    Extrude a 2D shapely polygon into a 3D mesh along an\n    arbitrary 3D path. Doesn't handle sharp curvature.\n\n\n    Parameters\n    ----------\n    polygon : shapely.geometry.Polygon\n      Profile to sweep along path\n    path : (n, 3) float\n      A path in 3D\n    angles :  (n,) float\n      Optional rotation angle relative to prior vertex\n      at each vertex\n\n    Returns\n    -------\n    mesh : trimesh.Trimesh\n      Geometry of result\n    \"\"\"\n\n    path = np.asanyarray(path, dtype=np.float64)\n    if not util.is_shape(path, (-1, 3)):\n        raise ValueError('Path must be (n, 3)!')\n\n    # Extract 2D vertices and triangulation\n    verts_2d = np.array(polygon.exterior)[:-1]\n    base_verts_2d, faces_2d = triangulate_polygon(polygon, **kwargs)\n    n = len(verts_2d)\n\n    # Create basis for first planar polygon cap\n    x, y, z = util.generate_basis(path[0] - path[1])\n    tf_mat = np.ones((4, 4))\n    tf_mat[:3, :3] = np.c_[x, y, z]\n    tf_mat[:3, 3] = path[0]\n\n    # Compute 3D locations of those vertices\n    verts_3d = np.c_[verts_2d, np.zeros(n)]\n    verts_3d = transformations.transform_points(verts_3d, tf_mat)\n    base_verts_3d = np.c_[base_verts_2d,\n                          np.zeros(len(base_verts_2d))]\n    base_verts_3d = transformations.transform_points(base_verts_3d,\n                                                     tf_mat)\n\n    # keep matching sequence of vertices and 0- indexed faces\n    vertices = [base_verts_3d]\n    faces = [faces_2d]\n\n    # Compute plane normals for each turn --\n    # each turn induces a plane halfway between the two vectors\n    v1s = util.unitize(path[1:-1] - path[:-2])\n    v2s = util.unitize(path[1:-1] - path[2:])\n    norms = np.cross(np.cross(v1s, v2s), v1s + v2s)\n    norms[(norms == 0.0).all(1)] = v1s[(norms == 0.0).all(1)]\n    norms = util.unitize(norms)\n    final_v1 = util.unitize(path[-1] - path[-2])\n    norms = np.vstack((norms, final_v1))\n    v1s = np.vstack((v1s, final_v1))\n\n    # Create all side walls by projecting the 3d vertices into each plane\n    # in succession\n    for i in range(len(norms)):\n        verts_3d_prev = verts_3d\n\n        # Rotate if needed\n        if angles is not None:\n            tf_mat = transformations.rotation_matrix(angles[i],\n                                                     norms[i],\n                                                     path[i])\n            verts_3d_prev = transformations.transform_points(verts_3d_prev,\n                                                             tf_mat)\n\n        # Project vertices onto plane in 3D\n        ds = np.einsum('ij,j->i', (path[i + 1] - verts_3d_prev), norms[i])\n        ds = ds / np.dot(v1s[i], norms[i])\n\n        verts_3d_new = np.einsum('i,j->ij', ds, v1s[i]) + verts_3d_prev\n\n        # Add to face and vertex lists\n        new_faces = [[i + n, (i + 1) % n, i] for i in range(n)]\n        new_faces.extend([[(i - 1) % n + n, i + n, i] for i in range(n)])\n\n        # save faces and vertices into a sequence\n        faces.append(np.array(new_faces))\n        vertices.append(np.vstack((verts_3d, verts_3d_new)))\n\n        verts_3d = verts_3d_new\n\n    # do the main stack operation from a sequence to (n,3) arrays\n    # doing one vstack provides a substantial speedup by\n    # avoiding a bunch of temporary  allocations\n    vertices, faces = util.append_faces(vertices, faces)\n\n    # Create final cap\n    x, y, z = util.generate_basis(path[-1] - path[-2])\n    vecs = verts_3d - path[-1]\n    coords = np.c_[np.einsum('ij,j->i', vecs, x),\n                   np.einsum('ij,j->i', vecs, y)]\n    base_verts_2d, faces_2d = triangulate_polygon(Polygon(coords))\n    base_verts_3d = (np.einsum('i,j->ij', base_verts_2d[:, 0], x) +\n                     np.einsum('i,j->ij', base_verts_2d[:, 1], y)) + path[-1]\n    faces = np.vstack((faces, faces_2d + len(vertices)))\n    vertices = np.vstack((vertices, base_verts_3d))\n\n    return Trimesh(vertices, faces)", "response": "Given a 2D shapely polygon and a 3D path sweeps along a 3D path."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a set of vertices faces and height extrude a 2D triangulation into a watertight Trimesh.", "response": "def extrude_triangulation(vertices,\n                          faces,\n                          height,\n                          **kwargs):\n    \"\"\"\n    Turn a 2D triangulation into a watertight Trimesh.\n\n    Parameters\n    ----------\n    vertices : (n, 2) float\n      2D vertices\n    faces : (m, 3) int\n      Triangle indexes of vertices\n    height : float\n      Distance to extrude triangulation\n    **kwargs:\n        passed to Trimesh\n\n    Returns\n    ---------\n    mesh : trimesh.Trimesh\n      Mesh created from extrusion\n    \"\"\"\n    vertices = np.asanyarray(vertices, dtype=np.float64)\n    height = float(height)\n    faces = np.asanyarray(faces, dtype=np.int64)\n\n    if not util.is_shape(vertices, (-1, 2)):\n        raise ValueError('Vertices must be (n,2)')\n    if not util.is_shape(faces, (-1, 3)):\n        raise ValueError('Faces must be (n,3)')\n    if np.abs(height) < tol.merge:\n        raise ValueError('Height must be nonzero!')\n\n    # make sure triangulation winding is pointing up\n    normal_test = normals(\n        [util.stack_3D(vertices[faces[0]])])[0]\n\n    normal_dot = np.dot(normal_test,\n                        [0.0, 0.0, np.sign(height)])[0]\n\n    # make sure the triangulation is aligned with the sign of\n    # the height we've been passed\n    if normal_dot < 0.0:\n        faces = np.fliplr(faces)\n\n    # stack the (n,3) faces into (3*n, 2) edges\n    edges = faces_to_edges(faces)\n    edges_sorted = np.sort(edges, axis=1)\n    # edges which only occur once are on the boundary of the polygon\n    # since the triangulation may have subdivided the boundary of the\n    # shapely polygon, we need to find it again\n    edges_unique = grouping.group_rows(edges_sorted, require_count=1)\n\n    # (n, 2, 2) set of line segments (positions, not references)\n    boundary = vertices[edges[edges_unique]]\n\n    # we are creating two vertical  triangles for every 2D line segment\n    # on the boundary of the 2D triangulation\n    vertical = np.tile(boundary.reshape((-1, 2)), 2).reshape((-1, 2))\n    vertical = np.column_stack((vertical,\n                                np.tile([0, height, 0, height],\n                                        len(boundary))))\n    vertical_faces = np.tile([3, 1, 2, 2, 1, 0],\n                             (len(boundary), 1))\n    vertical_faces += np.arange(len(boundary)).reshape((-1, 1)) * 4\n    vertical_faces = vertical_faces.reshape((-1, 3))\n\n    # stack the (n,2) vertices with zeros to make them (n, 3)\n    vertices_3D = util.stack_3D(vertices)\n\n    # a sequence of zero- indexed faces, which will then be appended\n    # with offsets to create the final mesh\n    faces_seq = [faces[:, ::-1],\n                 faces.copy(),\n                 vertical_faces]\n    vertices_seq = [vertices_3D,\n                    vertices_3D.copy() + [0.0, 0, height],\n                    vertical]\n\n    mesh = Trimesh(*util.append_faces(vertices_seq,\n                                      faces_seq),\n                   process=True,\n                   **kwargs)\n\n    assert mesh.volume > 0.0\n\n    return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef triangulate_polygon(polygon,\n                        triangle_args='pq30',\n                        engine='auto',\n                        **kwargs):\n    \"\"\"\n    Given a shapely polygon create a triangulation using one\n    of the python interfaces to triangle.c:\n    > pip install meshpy\n    > pip install triangle\n\n    Parameters\n    ---------\n    polygon : Shapely.geometry.Polygon\n        Polygon object to be triangulated\n    triangle_args : str\n        Passed to triangle.triangulate\n    engine : str\n        'meshpy', 'triangle', or 'auto'\n    kwargs: passed directly to meshpy.triangle.build:\n            triangle.build(mesh_info,\n                           verbose=False,\n                           refinement_func=None,\n                           attributes=False,\n                           volume_constraints=True,\n                           max_volume=None,\n                           allow_boundary_steiner=True,\n                           allow_volume_steiner=True,\n                           quality_meshing=True,\n                           generate_edges=None,\n                           generate_faces=False,\n                           min_angle=None)\n\n    Returns\n    --------------\n    vertices : (n, 2) float\n       Points in space\n    faces :    (n, 3) int\n       Index of vertices that make up triangles\n    \"\"\"\n\n    # turn the polygon in to vertices, segments, and hole points\n    arg = _polygon_to_kwargs(polygon)\n\n    try:\n        if str(engine).strip() in ['auto', 'triangle']:\n            from triangle import triangulate\n            result = triangulate(arg, triangle_args)\n            return result['vertices'], result['triangles']\n    except ImportError:\n        # no `triangle` so move on to `meshpy`\n        pass\n    except BaseException as E:\n        # if we see an exception log it and move on\n        log.error('failed to triangulate using triangle!',\n                  exc_info=True)\n        # if we are running unit tests exit here and fail\n        if tol.strict:\n            raise E\n\n    # do the import here, as sometimes this import can segfault\n    # which is not catchable with a try/except block\n    from meshpy import triangle\n    # call meshpy.triangle on our cleaned representation\n    info = triangle.MeshInfo()\n    info.set_points(arg['vertices'])\n    info.set_facets(arg['segments'])\n    # not all polygons have holes\n    if 'holes' in arg:\n        info.set_holes(arg['holes'])\n    # build mesh and pass kwargs to triangle\n    mesh = triangle.build(info, **kwargs)\n    # (n, 2) float vertices\n    vertices = np.array(mesh.points, dtype=np.float64)\n    # (m, 3) int faces\n    faces = np.array(mesh.elements, dtype=np.int64)\n\n    return vertices, faces", "response": "Given a shapely polygon create a triangulation of the vertices segments and hole points"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _polygon_to_kwargs(polygon):\n\n    if not polygon.is_valid:\n        raise ValueError('invalid shapely polygon passed!')\n\n    def round_trip(start, length):\n        \"\"\"\n        Given a start index and length, create a series of (n, 2) edges which\n        create a closed traversal.\n\n        Examples\n        ---------\n        start, length = 0, 3\n        returns:  [(0,1), (1,2), (2,0)]\n        \"\"\"\n        tiled = np.tile(np.arange(start, start + length).reshape((-1, 1)), 2)\n        tiled = tiled.reshape(-1)[1:-1].reshape((-1, 2))\n        tiled = np.vstack((tiled, [tiled[-1][-1], tiled[0][0]]))\n        return tiled\n\n    def add_boundary(boundary, start):\n        # coords is an (n, 2) ordered list of points on the polygon boundary\n        # the first and last points are the same, and there are no\n        # guarantees on points not being duplicated (which will\n        # later cause meshpy/triangle to shit a brick)\n        coords = np.array(boundary.coords)\n        # find indices points which occur only once, and sort them\n        # to maintain order\n        unique = np.sort(grouping.unique_rows(coords)[0])\n        cleaned = coords[unique]\n\n        vertices.append(cleaned)\n        facets.append(round_trip(start, len(cleaned)))\n\n        # holes require points inside the region of the hole, which we find\n        # by creating a polygon from the cleaned boundary region, and then\n        # using a representative point. You could do things like take the mean of\n        # the points, but this is more robust (to things like concavity), if\n        # slower.\n        test = Polygon(cleaned)\n        holes.append(np.array(test.representative_point().coords)[0])\n\n        return len(cleaned)\n\n    # sequence of (n,2) points in space\n    vertices = collections.deque()\n    # sequence of (n,2) indices of vertices\n    facets = collections.deque()\n    # list of (2) vertices in interior of hole regions\n    holes = collections.deque()\n\n    start = add_boundary(polygon.exterior, 0)\n    for interior in polygon.interiors:\n        try:\n            start += add_boundary(interior, start)\n        except BaseException:\n            log.warn('invalid interior, continuing')\n            continue\n\n    # create clean (n,2) float array of vertices\n    # and (m, 2) int array of facets\n    # by stacking the sequence of (p,2) arrays\n    vertices = np.vstack(vertices)\n    facets = np.vstack(facets).tolist()\n\n    # shapely polygons can include a Z component\n    # strip it out for the triangulation\n    if vertices.shape[1] == 3:\n        vertices = vertices[:, :2]\n\n    result = {'vertices': vertices,\n              'segments': facets}\n    # holes in meshpy lingo are a (h, 2) list of (x,y) points\n    # which are inside the region of the hole\n    # we added a hole for the exterior, which we slice away here\n    holes = np.array(holes)[1:]\n    if len(holes) > 0:\n        result['holes'] = holes\n\n    return result", "response": "Given a shapely polygon generate the data to pass to\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a cuboid. A box is a simple simplex - triangulation of the cuboid.", "response": "def box(extents=None, transform=None, **kwargs):\n    \"\"\"\n    Return a cuboid.\n\n    Parameters\n    ------------\n    extents : float, or (3,) float\n      Edge lengths\n    transform: (4, 4) float\n      Transformation matrix\n    **kwargs:\n        passed to Trimesh to create box\n\n    Returns\n    ------------\n    geometry : trimesh.Trimesh\n      Mesh of a cuboid\n    \"\"\"\n    # vertices of the cube\n    vertices = [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n                1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n    vertices = np.array(vertices,\n                        order='C',\n                        dtype=np.float64).reshape((-1, 3))\n    vertices -= 0.5\n\n    # resize cube based on passed extents\n    if extents is not None:\n        extents = np.asanyarray(extents, dtype=np.float64)\n        if extents.shape != (3,):\n            raise ValueError('Extents must be (3,)!')\n        vertices *= extents\n\n    faces = [1, 3, 0, 4, 1, 0, 0, 3, 2, 2, 4, 0, 1, 7, 3, 5, 1, 4,\n             5, 7, 1, 3, 7, 2, 6, 4, 2, 2, 7, 6, 6, 5, 4, 7, 5, 6]\n    faces = np.array(faces,\n                     order='C', dtype=np.int64).reshape((-1, 3))\n\n    face_normals = [-1, 0, 0, 0, -1, 0, -1, 0, 0, 0, 0, -1, 0, 0, 1, 0, -1,\n                    0, 0, 0, 1, 0, 1, 0, 0, 0, -1, 0, 1, 0, 1, 0, 0, 1, 0, 0]\n    face_normals = np.asanyarray(face_normals,\n                                 order='C',\n                                 dtype=np.float64).reshape(-1, 3)\n\n    box = Trimesh(vertices=vertices,\n                  faces=faces,\n                  face_normals=face_normals,\n                  process=False,\n                  **kwargs)\n\n    # do the transform here to preserve face normals\n    if transform is not None:\n        box.apply_transform(transform)\n\n    return box"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an icosahedron with a 20 faced polyhedron.", "response": "def icosahedron():\n    \"\"\"\n    Create an icosahedron, a 20 faced polyhedron.\n\n    Returns\n    -------------\n    ico : trimesh.Trimesh\n      Icosahederon centered at the origin.\n    \"\"\"\n    t = (1.0 + 5.0**.5) / 2.0\n    vertices = [-1, t, 0, 1, t, 0, -1, -t, 0, 1, -t, 0, 0, -1, t, 0, 1, t,\n                0, -1, -t, 0, 1, -t, t, 0, -1, t, 0, 1, -t, 0, -1, -t, 0, 1]\n    faces = [0, 11, 5, 0, 5, 1, 0, 1, 7, 0, 7, 10, 0, 10, 11,\n             1, 5, 9, 5, 11, 4, 11, 10, 2, 10, 7, 6, 7, 1, 8,\n             3, 9, 4, 3, 4, 2, 3, 2, 6, 3, 6, 8, 3, 8, 9,\n             4, 9, 5, 2, 4, 11, 6, 2, 10, 8, 6, 7, 9, 8, 1]\n    # scale vertices so each vertex radius is 1.0\n    vertices = np.reshape(vertices, (-1, 3)) / np.sqrt(2.0 + t)\n    faces = np.reshape(faces, (-1, 3))\n    mesh = Trimesh(vertices=vertices,\n                   faces=faces,\n                   process=False)\n    return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef icosphere(subdivisions=3, radius=1.0, color=None):\n    def refine_spherical():\n        vectors = ico.vertices\n        scalar = (vectors ** 2).sum(axis=1)**.5\n        unit = vectors / scalar.reshape((-1, 1))\n        offset = radius - scalar\n        ico.vertices += unit * offset.reshape((-1, 1))\n    ico = icosahedron()\n    ico._validate = False\n    for j in range(subdivisions):\n        ico = ico.subdivide()\n        refine_spherical()\n    ico._validate = True\n    if color is not None:\n        ico.visual.face_colors = color\n    return ico", "response": "Create an isophere centered at the origin."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a UV sphere with specified radius and count.", "response": "def uv_sphere(radius=1.0,\n              count=[32, 32],\n              theta=None,\n              phi=None):\n    \"\"\"\n    Create a UV sphere (latitude + longitude) centered at the\n    origin. Roughly one order of magnitude faster than an\n    icosphere but slightly uglier.\n\n    Parameters\n    ----------\n    radius : float\n      Radius of sphere\n    count : (2,) int\n      Number of latitude and longitude lines\n    theta : (n,) float\n      Optional theta angles in radians\n    phi :   (n,) float\n      Optional phi angles in radians\n\n    Returns\n    ----------\n    mesh : trimesh.Trimesh\n       Mesh of UV sphere with specified parameters\n    \"\"\"\n\n    count = np.array(count, dtype=np.int)\n    count += np.mod(count, 2)\n    count[1] *= 2\n\n    # generate vertices on a sphere using spherical coordinates\n    if theta is None:\n        theta = np.linspace(0, np.pi, count[0])\n    if phi is None:\n        phi = np.linspace(0, np.pi * 2, count[1])[:-1]\n    spherical = np.dstack((np.tile(phi, (len(theta), 1)).T,\n                           np.tile(theta, (len(phi), 1)))).reshape((-1, 2))\n    vertices = util.spherical_to_vector(spherical) * radius\n\n    # generate faces by creating a bunch of pie wedges\n    c = len(theta)\n    # a quad face as two triangles\n    pairs = np.array([[c, 0, 1],\n                      [c + 1, c, 1]])\n\n    # increment both triangles in each quad face by the same offset\n    incrementor = np.tile(np.arange(c - 1), (2, 1)).T.reshape((-1, 1))\n    # create the faces for a single pie wedge of the sphere\n    strip = np.tile(pairs, (c - 1, 1))\n    strip += incrementor\n    # the first and last faces will be degenerate since the first\n    # and last vertex are identical in the two rows\n    strip = strip[1:-1]\n\n    # tile pie wedges into a sphere\n    faces = np.vstack([strip + (i * c) for i in range(len(phi))])\n\n    # poles are repeated in every strip, so a mask to merge them\n    mask = np.arange(len(vertices))\n    # the top pole are all the same vertex\n    mask[0::c] = 0\n    # the bottom pole are all the same vertex\n    mask[c - 1::c] = c - 1\n\n    # faces masked to remove the duplicated pole vertices\n    # and mod to wrap to fill in the last pie wedge\n    faces = mask[np.mod(faces, len(vertices))]\n\n    # we save a lot of time by not processing again\n    # since we did some bookkeeping mesh is watertight\n    mesh = Trimesh(vertices=vertices, faces=faces, process=False)\n    return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef capsule(height=1.0,\n            radius=1.0,\n            count=[32, 32]):\n    \"\"\"\n    Create a mesh of a capsule, or a cylinder with hemispheric ends.\n\n    Parameters\n    ----------\n    height : float\n      Center to center distance of two spheres\n    radius : float\n      Radius of the cylinder and hemispheres\n    count : (2,) int\n      Number of sections on latitude and longitude\n\n    Returns\n    ----------\n    capsule : trimesh.Trimesh\n      Capsule geometry with:\n        - cylinder axis is along Z\n        - one hemisphere is centered at the origin\n        - other hemisphere is centered along the Z axis at height\n    \"\"\"\n    height = float(height)\n    radius = float(radius)\n    count = np.array(count, dtype=np.int)\n    count += np.mod(count, 2)\n\n    # create a theta where there is a double band around the equator\n    # so that we can offset the top and bottom of a sphere to\n    # get a nicely meshed capsule\n    theta = np.linspace(0, np.pi, count[0])\n    center = np.clip(np.arctan(tol.merge / radius),\n                     tol.merge, np.inf)\n    offset = np.array([-center, center]) + (np.pi / 2)\n    theta = np.insert(theta,\n                      int(len(theta) / 2),\n                      offset)\n\n    capsule = uv_sphere(radius=radius,\n                        count=count,\n                        theta=theta)\n\n    top = capsule.vertices[:, 2] > tol.zero\n    capsule.vertices[top] += [0, 0, height]\n\n    return capsule", "response": "Create a mesh of a cylinder with hemispheric ends."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cylinder(radius=1.0,\n             height=1.0,\n             sections=32,\n             segment=None,\n             transform=None,\n             **kwargs):\n    \"\"\"\n    Create a mesh of a cylinder along Z centered at the origin.\n\n    Parameters\n    ----------\n    radius : float\n      The radius of the cylinder\n    height : float\n      The height of the cylinder\n    sections : int\n      How many pie wedges should the cylinder have\n    segment : (2, 3) float\n      Endpoints of axis, overrides transform and height\n    transform : (4, 4) float\n      Transform to apply\n    **kwargs:\n        passed to Trimesh to create cylinder\n\n    Returns\n    ----------\n    cylinder: trimesh.Trimesh\n      Resulting mesh of a cylinder\n    \"\"\"\n\n    if segment is not None:\n        segment = np.asanyarray(segment, dtype=np.float64)\n        if segment.shape != (2, 3):\n            raise ValueError('segment must be 2 3D points!')\n        vector = segment[1] - segment[0]\n        # override height with segment length\n        height = np.linalg.norm(vector)\n        # point in middle of line\n        midpoint = segment[0] + (vector * 0.5)\n        # align Z with our desired direction\n        rotation = align_vectors([0, 0, 1], vector)\n        # translate to midpoint of segment\n        translation = transformations.translation_matrix(midpoint)\n        # compound the rotation and translation\n        transform = np.dot(translation, rotation)\n\n    # create a 2D pie out of wedges\n    theta = np.linspace(0, np.pi * 2, sections)\n    vertices = np.column_stack((np.sin(theta),\n                                np.cos(theta))) * radius\n    # the single vertex at the center of the circle\n    # we're overwriting the duplicated start/end vertex\n    vertices[0] = [0, 0]\n\n    # whangle indexes into a triangulation of the pie wedges\n    index = np.arange(1, len(vertices) + 1).reshape((-1, 1))\n    index[-1] = 1\n    faces = np.tile(index, (1, 2)).reshape(-1)[1:-1].reshape((-1, 2))\n    faces = np.column_stack((np.zeros(len(faces), dtype=np.int), faces))\n\n    # extrude the 2D triangulation into a Trimesh object\n    cylinder = extrude_triangulation(vertices=vertices,\n                                     faces=faces,\n                                     height=height,\n                                     **kwargs)\n    # the extrusion was along +Z, so move the cylinder\n    # center of mass back to the origin\n    cylinder.vertices[:, 2] -= height * .5\n    if transform is not None:\n        # apply a transform here before any cache stuff is generated\n        # and would have to be dumped after the transform is applied\n        cylinder.apply_transform(transform)\n\n    return cylinder", "response": "Create a 2D cylinder with a radius height and a transform."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an annular cylinder with a center radius r_min r_max height height.", "response": "def annulus(r_min=1.0,\n            r_max=2.0,\n            height=1.0,\n            sections=32,\n            transform=None,\n            **kwargs):\n    \"\"\"\n    Create a mesh of an annular cylinder along Z,\n    centered at the origin.\n\n    Parameters\n    ----------\n    r_min : float\n      The inner radius of the annular cylinder\n    r_max : float\n      The outer radius of the annular cylinder\n    height : float\n      The height of the annular cylinder\n    sections : int\n      How many pie wedges should the annular cylinder have\n    **kwargs:\n        passed to Trimesh to create annulus\n\n    Returns\n    ----------\n    annulus : trimesh.Trimesh\n      Mesh of annular cylinder\n    \"\"\"\n    r_min = abs(float(r_min))\n    r_max = abs(float(r_max))\n    height = float(height)\n    sections = int(sections)\n\n    # if center radius is zero this is a cylinder\n    if r_min < tol.merge:\n        return cylinder(radius=r_max,\n                        height=height,\n                        sections=sections,\n                        transform=transform)\n\n    # create a 2D pie out of wedges\n    theta = np.linspace(0, np.pi * 2, sections)[:-1]\n    unit = np.column_stack((np.sin(theta),\n                            np.cos(theta)))\n    assert len(unit) == sections - 1\n\n    vertices = np.vstack((unit * r_min,\n                          unit * r_max))\n\n    # one flattened triangulated quad covering one slice\n    face = np.array([0, sections - 1, 1,\n                     1, sections - 1, sections])\n\n    # tile one quad into lots of quads\n    faces = (np.tile(face, (sections - 1, 1)) +\n             np.arange(sections - 1).reshape((-1, 1))).reshape((-1, 3))\n\n    # stitch the last and first triangles with correct winding\n    faces[-1] = [sections - 1, 0, sections - 2]\n\n    # extrude the 2D profile into a mesh\n    annulus = extrude_triangulation(vertices=vertices,\n                                    faces=faces,\n                                    height=height,\n                                    **kwargs)\n\n    # move the annulus so the centroid is at the origin\n    annulus.vertices[:, 2] -= height * .5\n    if transform is not None:\n        # apply a transform here before any cache stuff is generated\n        # and would have to be dumped after the transform is applied\n        annulus.apply_transform(transform)\n\n    return annulus"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn random triangles as a Trimesh object", "response": "def random_soup(face_count=100):\n    \"\"\"\n    Return random triangles as a Trimesh\n\n    Parameters\n    -----------\n    face_count : int\n      Number of faces desired in mesh\n\n    Returns\n    -----------\n    soup : trimesh.Trimesh\n      Geometry with face_count random faces\n    \"\"\"\n    vertices = np.random.random((face_count * 3, 3)) - 0.5\n    faces = np.arange(face_count * 3).reshape((-1, 3))\n    soup = Trimesh(vertices=vertices, faces=faces)\n    return soup"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef axis(origin_size=0.04,\n         transform=None,\n         origin_color=None,\n         axis_radius=None,\n         axis_length=None):\n    \"\"\"\n    Return an XYZ axis marker as a  Trimesh, which represents position\n    and orientation. If you set the origin size the other parameters\n    will be set relative to it.\n\n    Parameters\n    ----------\n    transform : (4, 4) float\n      Transformation matrix\n    origin_size : float\n      Radius of sphere that represents the origin\n    origin_color : (3,) float or int, uint8 or float\n      Color of the origin\n    axis_radius : float\n      Radius of cylinder that represents x, y, z axis\n    axis_length: float\n      Length of cylinder that represents x, y, z axis\n\n    Returns\n    -------\n    marker : trimesh.Trimesh\n      Mesh geometry of axis indicators\n    \"\"\"\n    # the size of the ball representing the origin\n    origin_size = float(origin_size)\n\n    # set the transform and use origin-relative\n    # sized for other parameters if not specified\n    if transform is None:\n        transform = np.eye(4)\n    if origin_color is None:\n        origin_color = [255, 255, 255, 255]\n    if axis_radius is None:\n        axis_radius = origin_size / 5.0\n    if axis_length is None:\n        axis_length = origin_size * 10.0\n\n    # generate a ball for the origin\n    axis_origin = uv_sphere(radius=origin_size,\n                            count=[10, 10])\n    axis_origin.apply_transform(transform)\n\n    # apply color to the origin ball\n    axis_origin.visual.face_colors = origin_color\n\n    # create the cylinder for the z-axis\n    translation = transformations.translation_matrix(\n        [0, 0, axis_length / 2])\n    z_axis = cylinder(\n        radius=axis_radius,\n        height=axis_length,\n        transform=transform.dot(translation))\n    # XYZ->RGB, Z is blue\n    z_axis.visual.face_colors = [0, 0, 255]\n\n    # create the cylinder for the y-axis\n    translation = transformations.translation_matrix(\n        [0, 0, axis_length / 2])\n    rotation = transformations.rotation_matrix(np.radians(-90),\n                                               [1, 0, 0])\n    y_axis = cylinder(\n        radius=axis_radius,\n        height=axis_length,\n        transform=transform.dot(rotation).dot(translation))\n    # XYZ->RGB, Y is green\n    y_axis.visual.face_colors = [0, 255, 0]\n\n    # create the cylinder for the x-axis\n    translation = transformations.translation_matrix(\n        [0, 0, axis_length / 2])\n    rotation = transformations.rotation_matrix(np.radians(90),\n                                               [0, 1, 0])\n    x_axis = cylinder(\n        radius=axis_radius,\n        height=axis_length,\n        transform=transform.dot(rotation).dot(translation))\n    # XYZ->RGB, X is red\n    x_axis.visual.face_colors = [255, 0, 0]\n\n    # append the sphere and three cylinders\n    marker = util.concatenate([axis_origin,\n                               x_axis,\n                               y_axis,\n                               z_axis])\n    return marker", "response": "Returns a marker that represents the XYZ axis of the specified origin size radius and length."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef camera_marker(camera,\n                  marker_height=0.4,\n                  origin_size=None):\n    \"\"\"\n    Create a visual marker for a camera object, including an axis and FOV.\n\n    Parameters\n    ---------------\n    camera : trimesh.scene.Camera\n      Camera object with FOV and transform defined\n    marker_height : float\n      How far along the camera Z should FOV indicators be\n    origin_size : float\n      Sphere radius of the origin (default: marker_height / 10.0)\n\n    Returns\n    ------------\n    meshes : list\n      Contains Trimesh and Path3D objects which can be visualized\n    \"\"\"\n\n    camera_transform = camera.transform\n    if camera_transform is None:\n        camera_transform = np.eye(4)\n\n    # append the visualizations to an array\n    meshes = [axis(origin_size=marker_height / 10.0)]\n    meshes[0].apply_transform(camera_transform)\n\n    try:\n        # path is a soft dependency\n        from .path.exchange.load import load_path\n    except ImportError:\n        # they probably don't have shapely installed\n        log.warning('unable to create FOV visualization!',\n                    exc_info=True)\n        return meshes\n\n    # create sane origin size from marker height\n    if origin_size is None:\n        origin_size = marker_height / 10.0\n\n    # calculate vertices from camera FOV angles\n    x = marker_height * np.tan(np.deg2rad(camera.fov[0]) / 2.0)\n    y = marker_height * np.tan(np.deg2rad(camera.fov[1]) / 2.0)\n    z = marker_height\n\n    # combine the points into the vertices of an FOV visualization\n    points = np.array(\n        [(0, 0, 0),\n         (-x, -y, z),\n         (x, -y, z),\n         (x, y, z),\n         (-x, y, z)],\n        dtype=float)\n\n    # create line segments for the FOV visualization\n    # a segment from the origin to each bound of the FOV\n    segments = np.column_stack(\n        (np.zeros_like(points), points)).reshape(\n        (-1, 3))\n\n    # add a loop for the outside of the FOV then reshape\n    # the whole thing into multiple line segments\n    segments = np.vstack((segments,\n                          points[[1, 2,\n                                  2, 3,\n                                  3, 4,\n                                  4, 1]])).reshape((-1, 2, 3))\n\n    # add a single Path3D object for all line segments\n    meshes.append(load_path(segments))\n    meshes[-1].apply_transform(camera_transform)\n\n    return meshes", "response": "Create a visual marker for a camera object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new Trimesh object representing the convex hull of the current mesh.", "response": "def convex_hull(obj, qhull_options='QbB Pp QJn'):\n    \"\"\"\n    Get a new Trimesh object representing the convex hull of the\n    current mesh, with proper normals and watertight.\n    Requires scipy >.12.\n\n    Arguments\n    --------\n    obj : Trimesh, or (n,3) float\n      Mesh or cartesian points\n\n    Returns\n    --------\n    convex : Trimesh\n      Mesh of convex hull\n    \"\"\"\n    from .base import Trimesh\n\n    if isinstance(obj, Trimesh):\n        points = obj.vertices.view(np.ndarray)\n    else:\n        # will remove subclassing\n        points = np.asarray(obj, dtype=np.float64)\n        if not util.is_shape(points, (-1, 3)):\n            raise ValueError('Object must be Trimesh or (n,3) points!')\n\n    hull = spatial.ConvexHull(points,\n                              qhull_options=qhull_options)\n\n    # hull object doesn't remove unreferenced vertices\n    # create a mask to re- index faces for only referenced vertices\n    vid = np.sort(hull.vertices)\n    mask = np.zeros(len(hull.points), dtype=np.int64)\n    mask[vid] = np.arange(len(vid))\n    # remove unreferenced vertices here\n    faces = mask[hull.simplices].copy()\n\n    # rescale vertices back to original size\n    vertices = hull.points[vid].copy()\n\n    # qhull returns faces with random winding\n    # calculate the returned normal of each face\n    crosses = triangles.cross(vertices[faces])\n\n    # qhull returns zero magnitude faces like an asshole\n    normals, valid = util.unitize(crosses, check_valid=True)\n\n    # remove zero magnitude faces\n    faces = faces[valid]\n    crosses = crosses[valid]\n\n    # each triangle area and mean center\n    triangles_area = triangles.area(crosses=crosses, sum=False)\n    triangles_center = vertices[faces].mean(axis=1)\n\n    # since the convex hull is (hopefully) convex, the vector from\n    # the centroid to the center of each face\n    # should have a positive dot product with the normal of that face\n    # if it doesn't it is probably backwards\n    # note that this sometimes gets screwed up by precision issues\n    centroid = np.average(triangles_center,\n                          weights=triangles_area,\n                          axis=0)\n    # a vector from the centroid to a point on each face\n    test_vector = triangles_center - centroid\n    # check the projection against face normals\n    backwards = util.diagonal_dot(normals,\n                                  test_vector) < 0.0\n\n    # flip the winding outward facing\n    faces[backwards] = np.fliplr(faces[backwards])\n    # flip the normal\n    normals[backwards] *= -1.0\n\n    # save the work we did to the cache so it doesn't have to be recomputed\n    initial_cache = {'triangles_cross': crosses,\n                     'triangles_center': triangles_center,\n                     'area_faces': triangles_area,\n                     'centroid': centroid}\n\n    # create the Trimesh object for the convex hull\n    convex = Trimesh(vertices=vertices,\n                     faces=faces,\n                     face_normals=normals,\n                     initial_cache=initial_cache,\n                     process=True,\n                     validate=False)\n\n    # we did the gross case above, but sometimes precision issues\n    # leave some faces backwards anyway\n    # this call will exit early if the winding is consistent\n    # and if not will fix it by traversing the adjacency graph\n    convex.fix_normals(multibody=False)\n\n    # sometimes the QbB option will cause precision issues\n    # so try the hull again without it and\n    # check for qhull_options is None to avoid infinite recursion\n    if (qhull_options is not None and\n            not convex.is_winding_consistent):\n        return convex_hull(convex, qhull_options=None)\n\n    return convex"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef adjacency_projections(mesh):\n    # normals and origins from the first column of face adjacency\n    normals = mesh.face_normals[mesh.face_adjacency[:, 0]]\n    # one of the vertices on the shared edge\n    origins = mesh.vertices[mesh.face_adjacency_edges[:, 0]]\n\n    # faces from the second column of face adjacency\n    vid_other = mesh.face_adjacency_unshared[:, 1]\n    vector_other = mesh.vertices[vid_other] - origins\n\n    # get the projection with a dot product\n    dots = util.diagonal_dot(vector_other, normals)\n\n    return dots", "response": "Test if a mesh is convex by projecting the vertices of the adjacent vertex onto the normal of its adjacent face."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_convex(mesh):\n    # don't consider zero- area faces\n    nonzero = mesh.area_faces > tol.merge\n\n    # adjacencies with two nonzero faces\n    adj_ok = nonzero[mesh.face_adjacency].all(axis=1)\n\n    # make threshold of convexity scale- relative\n    threshold = tol.planar * mesh.scale\n    # if projections of vertex onto plane of adjacent\n    # face is negative, it means the face pair is locally\n    # convex, and if that is true for all faces the mesh is convex\n    convex = bool(mesh.face_adjacency_projections[adj_ok].max() < threshold)\n\n    return convex", "response": "Check if a mesh is convex"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntrying to extract a convex set of points from multiple input formats.", "response": "def hull_points(obj, qhull_options='QbB Pp'):\n    \"\"\"\n    Try to extract a convex set of points from multiple input formats.\n\n    Parameters\n    ---------\n    obj: Trimesh object\n         (n,d) points\n         (m,) Trimesh objects\n\n    Returns\n    --------\n    points: (o,d) convex set of points\n    \"\"\"\n    if hasattr(obj, 'convex_hull'):\n        return obj.convex_hull.vertices\n\n    initial = np.asanyarray(obj, dtype=np.float64)\n    if len(initial.shape) != 2:\n        raise ValueError('points must be (n, dimension)!')\n\n    hull = spatial.ConvexHull(initial, qhull_options=qhull_options)\n    points = hull.points[hull.vertices]\n\n    return points"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef closed(self):\n        closed = (len(self.points) > 2 and\n                  self.points[0] == self.points[-1])\n        return closed", "response": "Returns True if the entity is closed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of nodes or vertices on the path.", "response": "def nodes(self):\n        \"\"\"\n        Returns an (n,2) list of nodes, or vertices on the path.\n        Note that this generic class function assumes that all of the\n        reference points are on the path which is true for lines and\n        three point arcs.\n\n        If you were to define another class where that wasn't the case\n        (for example, the control points of a bezier curve),\n        you would need to implement an entity- specific version of this\n        function.\n\n        The purpose of having a list of nodes is so that they can then be\n        added as edges to a graph so we can use functions to check\n        connectivity, extract paths, etc.\n\n        The slicing on this function is essentially just tiling points\n        so the first and last vertices aren't repeated. Example:\n\n        self.points = [0,1,2]\n        returns:      [[0,1], [1,2]]\n        \"\"\"\n        return np.column_stack((self.points,\n                                self.points)).reshape(\n                                    -1)[1:-1].reshape((-1, 2))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the total length of the entity.", "response": "def length(self, vertices):\n        \"\"\"\n        Return the total length of the entity.\n\n        Returns\n        ---------\n        length: float, total length of entity\n        \"\"\"\n        length = ((np.diff(self.discrete(vertices),\n                           axis=0)**2).sum(axis=1)**.5).sum()\n        return length"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self, vertices, show=False):\n        if vertices.shape[1] != 2:\n            raise ValueError('only for 2D points!')\n\n        import matplotlib.pyplot as plt\n\n        # get rotation angle in degrees\n        angle = np.degrees(self.angle(vertices))\n\n        plt.text(*vertices[self.origin],\n                 s=self.text,\n                 rotation=angle,\n                 ha=self.align[0],\n                 va=self.align[1],\n                 size=18)\n\n        if show:\n            plt.show()", "response": "Plot the text using matplotlib. pyplot."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the angle in radians between the origin and the vector of the point", "response": "def angle(self, vertices):\n        \"\"\"\n        If Text is 2D, get the rotation angle in radians.\n\n        Parameters\n        -----------\n        vertices : (n, 2) float\n          Vertices in space referenced by self.points\n\n        Returns\n        ---------\n        angle : float\n          Rotation angle in radians\n        \"\"\"\n\n        if vertices.shape[1] != 2:\n            raise ValueError('angle only valid for 2D points!')\n\n        # get the vector from origin\n        direction = vertices[self.vector] - vertices[self.origin]\n        # get the rotation angle in radians\n        angle = np.arctan2(*direction[::-1])\n\n        return angle"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the current entity is well formed and False otherwise.", "response": "def is_valid(self):\n        \"\"\"\n        Is the current entity valid.\n\n        Returns\n        -----------\n        valid : bool\n          Is the current entity well formed\n        \"\"\"\n        valid = np.any((self.points - self.points[0]) != 0)\n        return valid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef explode(self):\n        points = np.column_stack((\n            self.points,\n            self.points)).ravel()[1:-1].reshape((-1, 2))\n        exploded = [Line(i) for i in points]\n        return exploded", "response": "Explode the current Line entity into n Line entities."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef discrete(self, vertices, scale=1.0):\n        discrete = discretize_arc(vertices[self.points],\n                                  close=self.closed,\n                                  scale=scale)\n        return self._orient(discrete)", "response": "Discretize the arc entity into line sections."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the AABB of the arc entity.", "response": "def bounds(self, vertices):\n        \"\"\"\n        Return the AABB of the arc entity.\n\n        Parameters\n        -----------\n        vertices: (n,dimension) float, vertices in space\n\n        Returns\n        -----------\n        bounds: (2, dimension) float, (min, max) coordinate of AABB\n        \"\"\"\n        if util.is_shape(vertices, (-1, 2)) and self.closed:\n            # if we have a closed arc (a circle), we can return the actual bounds\n            # this only works in two dimensions, otherwise this would return the\n            # AABB of an sphere\n            info = self.center(vertices)\n            bounds = np.array([info['center'] - info['radius'],\n                               info['center'] + info['radius']],\n                              dtype=np.float64)\n        else:\n            # since the AABB of a partial arc is hard, approximate\n            # the bounds by just looking at the discrete values\n            discrete = self.discrete(vertices)\n            bounds = np.array([discrete.min(axis=0),\n                               discrete.max(axis=0)],\n                              dtype=np.float64)\n        return bounds"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef discrete(self, vertices, count=None, scale=1.0):\n        discrete = discretize_bspline(\n            control=vertices[self.points],\n            knots=self.knots,\n            count=count,\n            scale=scale)\n        return self._orient(discrete)", "response": "Discretize the B - Spline curve."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_dict(self):\n        return {'type': self.__class__.__name__,\n                'points': self.points.tolist(),\n                'knots': self.knots.tolist(),\n                'closed': self.closed}", "response": "Returns a dictionary with all of the information\n        about the entity."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfills the gaps of a 3D line segment with the given distance.", "response": "def fill_gaps(path, distance=.025):\n    \"\"\"\n    For 3D line segments defined by two points, turn\n    them in to an origin defined as the closest point along\n    the line to the zero origin as well as a direction vector\n    and start and end parameter.\n\n    Parameters\n    ------------\n    segments : (n, 2, 3) float\n       Line segments defined by start and end points\n\n    Returns\n    --------------\n    origins : (n, 3) float\n       Point on line closest to [0, 0, 0]\n    vectors : (n, 3) float\n       Unit line directions\n    parameters : (n, 2) float\n       Start and end distance pairs for each line\n    \"\"\"\n\n    # find any vertex without degree 2 (connected to two things)\n    broken = np.array([\n        k for k, d in dict(path.vertex_graph.degree()).items()\n        if d != 2])\n\n    # if all vertices have correct connectivity, exit\n    if len(broken) == 0:\n        return\n\n    # first find broken vertices with distance\n    tree = cKDTree(path.vertices[broken])\n    pairs = tree.query_pairs(r=distance, output_type='ndarray')\n\n    connect_seg = []\n    if len(pairs) > 0:\n        end_points = {tuple(sorted(e.end_points)) for e in path.entities}\n        pair_set = {tuple(i) for i in np.sort(broken[pairs], axis=1)}\n\n        # we don't want to connect entities to themselves so do a set\n        # difference\n        mask = np.array(list(pair_set.difference(end_points)))\n\n        if len(mask) > 0:\n            connect_seg = path.vertices[mask]\n\n    # a set of values we can query intersections with quickly\n    broken_set = set(broken)\n    # query end points set vs path.dangling to avoid having\n    # to compute every single path and discrete curve\n    dangle = [i for i, e in enumerate(path.entities) if\n              len(broken_set.intersection(e.end_points)) > 0]\n\n    segs = []\n    # mask for which entities to keep\n    keep = np.ones(len(path.entities), dtype=np.bool)\n    # save a reference to the line class to avoid circular import\n    line_class = None\n\n    for entity_index in dangle:\n        # only consider line entities\n        if path.entities[entity_index].__class__.__name__ != 'Line':\n            continue\n\n        if line_class is None:\n            line_class = path.entities[entity_index].__class__\n\n        # get discrete version of entity\n        points = path.entities[entity_index].discrete(path.vertices)\n        # turn connected curve into segments\n        seg_idx = util.stack_lines(np.arange(len(points)))\n        # append the segments to our collection\n        segs.append(points[seg_idx])\n        # remove this entity and replace with segments\n        keep[entity_index] = False\n\n    # combine segments with connection segments\n    all_segs = util.vstack_empty((util.vstack_empty(segs),\n                                  connect_seg))\n\n    # go home early\n    if len(all_segs) == 0:\n        return\n\n    # split segments at broken vertices so topology can happen\n    split = segments.split(all_segs, path.vertices[broken])\n    # merge duplicate segments\n    final_seg = segments.unique(split)\n\n    # add line segments in as line entities\n    entities = []\n    for i, seg in enumerate(final_seg):\n        entities.append(\n            line_class(\n                points=np.arange(2) + (i * 2) + len(path.vertices)))\n\n    # replace entities with new entities\n    path.entities = np.append(path.entities[keep], entities)\n    path.vertices = np.vstack((path.vertices, np.vstack(final_seg)))\n    path._cache.clear()\n    path.process()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads a 3D XAML file into a dict of dicts.", "response": "def load_XAML(file_obj, *args, **kwargs):\n    \"\"\"\n    Load a 3D XAML file.\n\n    Parameters\n    ----------\n    file_obj : file object\n                Open, containing XAML file\n\n    Returns\n    ----------\n    result : dict\n                kwargs for a trimesh constructor, including:\n                vertices:       (n,3) np.float64, points in space\n                faces:          (m,3) np.int64, indices of vertices\n                face_colors:    (m,4) np.uint8, RGBA colors\n                vertex_normals: (n,3) np.float64, vertex normals\n    \"\"\"\n    def element_to_color(element):\n        \"\"\"\n        Turn an XML element into a (4,) np.uint8 RGBA color\n        \"\"\"\n        if element is None:\n            return visual.DEFAULT_COLOR\n        hexcolor = int(element.attrib['Color'].replace('#', ''), 16)\n        opacity = float(element.attrib['Opacity'])\n        rgba = [(hexcolor >> 16) & 0xFF,\n                (hexcolor >> 8) & 0xFF,\n                (hexcolor & 0xFF),\n                opacity * 0xFF]\n        rgba = np.array(rgba, dtype=np.uint8)\n        return rgba\n\n    def element_to_transform(element):\n        \"\"\"\n        Turn an XML element into a (4,4) np.float64\n        transformation matrix.\n        \"\"\"\n        try:\n            matrix = next(element.iter(\n                tag=ns + 'MatrixTransform3D')).attrib['Matrix']\n            matrix = np.array(matrix.split(),\n                              dtype=np.float64).reshape((4, 4)).T\n            return matrix\n        except StopIteration:\n            # this will be raised if the MatrixTransform3D isn't in the passed\n            # elements tree\n            return np.eye(4)\n\n    # read the file and parse XML\n    file_data = file_obj.read()\n    root = etree.XML(file_data)\n\n    # the XML namespace\n    ns = root.tag.split('}')[0] + '}'\n\n    # the linked lists our results are going in\n    vertices = collections.deque()\n    faces = collections.deque()\n    colors = collections.deque()\n    normals = collections.deque()\n\n    # iterate through the element tree\n    # the GeometryModel3D tag contains a material and geometry\n    for geometry in root.iter(tag=ns + 'GeometryModel3D'):\n\n        # get the diffuse and specular colors specified in the material\n        color_search = './/{ns}{color}Material/*/{ns}SolidColorBrush'\n        diffuse = geometry.find(color_search.format(ns=ns,\n                                                    color='Diffuse'))\n        specular = geometry.find(color_search.format(ns=ns,\n                                                     color='Specular'))\n\n        # convert the element into a (4,) np.uint8 RGBA color\n        diffuse = element_to_color(diffuse)\n        specular = element_to_color(specular)\n\n        # to get the final transform of a component we'll have to traverse\n        # all the way back to the root node and save transforms we find\n        current = geometry\n        transforms = collections.deque()\n        # when the root node is reached its parent will be None and we stop\n        while current is not None:\n            # element.find will only return elements that are direct children\n            # of the current element as opposed to element.iter,\n            # which will return any depth of child\n            transform_element = current.find(ns + 'ModelVisual3D.Transform')\n            if transform_element is not None:\n                # we are traversing the tree backwards, so append new\n                # transforms to the left of the deque\n                transforms.appendleft(element_to_transform(transform_element))\n            # we are going from the lowest level of the tree to the highest\n            # this avoids having to traverse any branches that don't have\n            # geometry\n            current = current.getparent()\n\n        if len(transforms) == 0:\n            # no transforms in the tree mean an identity matrix\n            transform = np.eye(4)\n        elif len(transforms) == 1:\n            # one transform in the tree we can just use\n            transform = transforms.pop()\n        else:\n            # multiple transforms we apply all of them in order\n            transform = util.multi_dot(transforms)\n\n        # iterate through the contained mesh geometry elements\n        for g in geometry.iter(tag=ns + 'MeshGeometry3D'):\n            c_normals = np.array(g.attrib['Normals'].replace(',', ' ').split(),\n                                 dtype=np.float64).reshape((-1, 3))\n\n            c_vertices = np.array(\n                g.attrib['Positions'].replace(\n                    ',', ' ').split(), dtype=np.float64).reshape(\n                (-1, 3))\n            # bake in the transform as we're saving\n            c_vertices = transformations.transform_points(c_vertices,\n                                                          transform)\n\n            c_faces = np.array(\n                g.attrib['TriangleIndices'].replace(\n                    ',', ' ').split(), dtype=np.int64).reshape(\n                (-1, 3))\n\n            # save data to a sequence\n            vertices.append(c_vertices)\n            faces.append(c_faces)\n            colors.append(np.tile(diffuse, (len(c_faces), 1)))\n            normals.append(c_normals)\n\n    # compile the results into clean numpy arrays\n    result = dict()\n    result['vertices'], result['faces'] = util.append_faces(vertices,\n                                                            faces)\n    result['face_colors'] = np.vstack(colors)\n    result['vertex_normals'] = np.vstack(normals)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload a 3DXML file into kwargs.", "response": "def load_3DXML(file_obj, *args, **kwargs):\n    \"\"\"\n    Load a 3DXML scene into kwargs.\n\n    Parameters\n    ------------\n    file_obj : file object\n      Open and containing 3DXML data\n\n    Returns\n    -----------\n    kwargs : dict\n      Can be passed to trimesh.exchange.load.load_kwargs\n    \"\"\"\n    archive = util.decompress(file_obj, file_type='zip')\n\n    # a dictionary of file name : lxml etree\n    as_etree = {}\n    for k, v in archive.items():\n        # wrap in try statement, as sometimes 3DXML\n        # contains non- xml files, like JPG previews\n        try:\n            as_etree[k] = etree.XML(v.read())\n        except etree.XMLSyntaxError:\n            # move the file object back to the file start\n            v.seek(0)\n\n    # the file name of the root scene\n    root_file = as_etree['Manifest.xml'].find('{*}Root').text\n    # the etree of the scene layout\n    tree = as_etree[root_file]\n    # index of root element of directed acyclic graph\n    root_id = tree.find('{*}ProductStructure').attrib['root']\n\n    # load the materials library from the materials elements\n    colors = {}\n    material_tree = as_etree['CATMaterialRef.3dxml']\n    for MaterialDomain in material_tree.iter('{*}MaterialDomain'):\n        material_id = MaterialDomain.attrib['id']\n        material_file = MaterialDomain.attrib['associatedFile'].split(\n            'urn:3DXML:')[-1]\n        rend = as_etree[material_file].find(\n            \"{*}Feature[@Alias='RenderingFeature']\")\n        diffuse = rend.find(\"{*}Attr[@Name='DiffuseColor']\")\n        # specular = rend.find(\"{*}Attr[@Name='SpecularColor']\")\n        # emissive = rend.find(\"{*}Attr[@Name='EmissiveColor']\")\n        rgb = (np.array(json.loads(\n            diffuse.attrib['Value'])) * 255).astype(np.uint8)\n        colors[material_id] = rgb\n\n    # copy indexes for instances of colors\n    for MaterialDomainInstance in material_tree.iter(\n            '{*}MaterialDomainInstance'):\n        instance = MaterialDomainInstance.find('{*}IsInstanceOf')\n        # colors[b.attrib['id']] = colors[instance.text]\n        for aggregate in MaterialDomainInstance.findall('{*}IsAggregatedBy'):\n            colors[aggregate.text] = colors[instance.text]\n\n    # references which hold the 3DXML scene structure as a dict\n    # element id : {key : value}\n    references = collections.defaultdict(dict)\n\n    # the 3DXML can specify different visual properties for  occurrences\n    view = tree.find('{*}DefaultView')\n    for ViewProp in view.iter('{*}DefaultViewProperty'):\n        color = ViewProp.find('{*}GraphicProperties/' +\n                              '{*}SurfaceAttributes/{*}Color')\n        if (color is None or\n                'RGBAColorType' not in color.attrib.values()):\n            continue\n        rgba = np.array([color.attrib[i]\n                         for i in ['red',\n                                   'green',\n                                   'blue',\n                                   'alpha']],\n                        dtype=np.float)\n        rgba = (rgba * 255).astype(np.uint8)\n        for occurrence in ViewProp.findall('{*}OccurenceId/{*}id'):\n            reference_id = occurrence.text.split('#')[-1]\n            references[reference_id]['color'] = rgba\n\n    # geometries will hold meshes\n    geometries = dict()\n\n    # get geometry\n    for ReferenceRep in tree.iter(tag='{*}ReferenceRep'):\n        # the str of an int that represents this meshes unique ID\n        part_id = ReferenceRep.attrib['id']\n        # which part file in the archive contains the geometry we care about\n        part_file = ReferenceRep.attrib['associatedFile'].split(':')[-1]\n\n        # load actual geometry\n        mesh_faces = []\n        mesh_colors = []\n        mesh_normals = []\n        mesh_vertices = []\n\n        # the geometry is stored in a Rep\n        for Rep in as_etree[part_file].iter('{*}Rep'):\n            faces = Rep.find('{*}Faces/{*}Face')\n            vertices = Rep.find('{*}VertexBuffer/{*}Positions')\n\n            if faces is None or vertices is None:\n                continue\n\n            # these are vertex normals\n            normals = Rep.find('{*}VertexBuffer/{*}Normals')\n            material = Rep.find('{*}SurfaceAttributes/' +\n                                '{*}MaterialApplication/' +\n                                '{*}MaterialId')\n\n            (material_file, material_id) = material.attrib['id'].split(\n                'urn:3DXML:')[-1].split('#')\n\n            # triangle strips, sequence of arbitrary length lists\n            # np.fromstring is substantially faster than np.array(i.split())\n            # inside the list comprehension\n            strips = [np.fromstring(i, sep=' ', dtype=np.int64)\n                      for i in faces.attrib['strips'].split(',')]\n\n            # convert strips to (m,3) int\n            mesh_faces.append(util.triangle_strips_to_faces(strips))\n\n            # they mix delimiters like we couldn't figure it out from the\n            # shape :(\n            # load vertices into (n, 3) float64\n            mesh_vertices.append(np.fromstring(\n                vertices.text.replace(',', ' '),\n                sep=' ',\n                dtype=np.float64).reshape((-1, 3)))\n\n            # load vertex normals into (n, 3) float64\n            mesh_normals.append(np.fromstring(\n                normals.text.replace(',', ' '),\n                sep=' ',\n                dtype=np.float64).reshape((-1, 3)))\n\n            # store the material information as (m,3) uint8 FACE COLORS\n            mesh_colors.append(np.tile(colors[material_id],\n                                       (len(mesh_faces[-1]), 1)))\n\n        # save each mesh as the kwargs for a trimesh.Trimesh constructor\n        # aka, a Trimesh object can be created with trimesh.Trimesh(**mesh)\n        # this avoids needing trimesh- specific imports in this IO function\n        mesh = dict()\n        (mesh['vertices'],\n         mesh['faces']) = util.append_faces(mesh_vertices,\n                                            mesh_faces)\n        mesh['vertex_normals'] = np.vstack(mesh_normals)\n        mesh['face_colors'] = np.vstack(mesh_colors)\n\n        # as far as I can tell, all 3DXML files are exported as\n        # implicit millimeters (it isn't specified in the file)\n        mesh['metadata'] = {'units': 'mm'}\n        mesh['class'] = 'Trimesh'\n\n        geometries[part_id] = mesh\n        references[part_id]['geometry'] = part_id\n\n    # a Reference3D maps to a subassembly or assembly\n    for Reference3D in tree.iter('{*}Reference3D'):\n        references[Reference3D.attrib['id']] = {\n            'name': Reference3D.attrib['name'],\n            'type': 'Reference3D'}\n\n    # a node that is the connectivity between a geometry and the Reference3D\n    for InstanceRep in tree.iter('{*}InstanceRep'):\n        current = InstanceRep.attrib['id']\n        instance = InstanceRep.find('{*}IsInstanceOf').text\n        aggregate = InstanceRep.find('{*}IsAggregatedBy').text\n\n        references[current].update({'aggregate': aggregate,\n                                    'instance': instance,\n                                    'type': 'InstanceRep'})\n\n    # an Instance3D maps basically to a part\n    for Instance3D in tree.iter('{*}Instance3D'):\n        matrix = np.eye(4)\n        relative = Instance3D.find('{*}RelativeMatrix')\n        if relative is not None:\n            relative = np.array(relative.text.split(),\n                                dtype=np.float64)\n\n            # rotation component\n            matrix[:3, :3] = relative[:9].reshape((3, 3)).T\n            # translation component\n            matrix[:3, 3] = relative[9:]\n\n        current = Instance3D.attrib['id']\n        name = Instance3D.attrib['name']\n        instance = Instance3D.find('{*}IsInstanceOf').text\n        aggregate = Instance3D.find('{*}IsAggregatedBy').text\n\n        references[current].update({'aggregate': aggregate,\n                                    'instance': instance,\n                                    'matrix': matrix,\n                                    'name': name,\n                                    'type': 'Instance3D'})\n\n    # turn references into directed graph for path finding\n    graph = nx.DiGraph()\n    for k, v in references.items():\n        # IsAggregatedBy points up to a parent\n        if 'aggregate' in v:\n            graph.add_edge(v['aggregate'], k)\n        # IsInstanceOf indicates a child\n        if 'instance' in v:\n            graph.add_edge(k, v['instance'])\n\n    # the 3DXML format is stored as a directed acyclic graph that needs all\n    # paths from the root to a geometry to generate the tree of the scene\n    paths = collections.deque()\n    for geometry_id in geometries.keys():\n        paths.extend(nx.all_simple_paths(graph,\n                                         source=root_id,\n                                         target=geometry_id))\n\n    # the name of the root frame\n    root_name = references[root_id]['name']\n    # create a list of kwargs to send to the scene.graph.update function\n    # start with a transform from the graphs base frame to our root name\n    graph_kwargs = collections.deque([{'frame_to': root_name,\n                                       'matrix': np.eye(4)}])\n\n    # we are going to collect prettier geometry names as we traverse paths\n    geom_names = {}\n    # loop through every simple path and generate transforms tree\n    # note that we are flattening the transform tree here\n    for path_index, path in enumerate(paths):\n        name = ''\n        if 'name' in references[path[-3]]:\n            name = references[path[-3]]['name']\n            geom_names[path[-1]] = name\n        # we need a unique node name for our geometry instance frame\n        # due to the nature of the DAG names specified by the file may not\n        # be unique, so we add an Instance3D name then append the path ids\n        node_name = name + '#' + ':'.join(path)\n\n        # pull all transformations in the path\n        matrices = [references[i]['matrix']\n                    for i in path if 'matrix' in references[i]]\n        if len(matrices) == 0:\n            matrix = np.eye(4)\n        elif len(matrices) == 1:\n            matrix = matrices[0]\n        else:\n            matrix = util.multi_dot(matrices)\n\n        graph_kwargs.append({'matrix': matrix,\n                             'frame_from': root_name,\n                             'frame_to': node_name,\n                             'geometry': path[-1]})\n\n    # remap geometry names from id numbers to the name string\n    # we extracted from the 3DXML tree\n    geom_final = {}\n    for key, value in geometries.items():\n        if key in geom_names:\n            geom_final[geom_names[key]] = value\n    # change geometry names in graph kwargs in place\n    for kwarg in graph_kwargs:\n        if 'geometry' not in kwarg:\n            continue\n        kwarg['geometry'] = geom_names[kwarg['geometry']]\n\n    # create the kwargs for load_kwargs\n    result = {'class': 'Scene',\n              'geometry': geom_final,\n              'graph': graph_kwargs}\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef print_element(element):\n    pretty = etree.tostring(\n        element, pretty_print=True).decode('utf-8')\n    print(pretty)\n    return pretty", "response": "Pretty - print an lxml. etree element."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmerging vertices of a Trimesh object with the same positions and UV coordinates.", "response": "def merge_vertices(mesh,\n                   digits=None,\n                   textured=True,\n                   uv_digits=4):\n    \"\"\"\n    Removes duplicate vertices based on integer hashes of\n    each row.\n\n    Parameters\n    -------------\n    mesh : Trimesh object\n      Mesh to merge vertices on\n    digits : int\n      How many digits to consider for vertices\n      If not specified uses tol.merge\n    textured : bool\n      If True, for textured meshes only merge vertices\n      with identical positions AND UV coordinates.\n      No effect on untextured meshes\n    uv_digits : int\n      Number of digits to consider for UV coordinates.\n    \"\"\"\n\n    if not isinstance(digits, int):\n        digits = util.decimal_to_digits(tol.merge)\n\n    # UV texture visuals require us to update the\n    # vertices and normals differently\n    if (textured and\n        mesh.visual.defined and\n        mesh.visual.kind == 'texture' and\n            mesh.visual.uv is not None):\n\n        # get an array with vertices and UV coordinates\n        # converted to integers at requested precision\n        stacked = np.column_stack((\n            mesh.vertices * (10 ** digits),\n            mesh.visual.uv * (10 ** uv_digits))).round().astype(np.int64)\n        # Merge vertices with identical positions and UVs\n        # we don't merge vertices just based on position\n        # because that can corrupt textures at seams.\n        unique, inverse = unique_rows(stacked)\n        mesh.update_vertices(unique, inverse)\n\n        # Now, smooth out the vertex normals at the duplicate vertices.\n        # For now, we just use the first vertex's normal in a duplicate group.\n        # It would be better to average these, but that's slower.\n        unique, inverse = unique_rows(mesh.vertices,\n                                      digits=digits)\n        try:\n            mesh.vertex_normals = mesh.vertex_normals[unique[inverse]]\n        except BaseException:\n            pass\n\n    # In normal usage, just merge vertices that are close.\n    else:\n        # if we have a ton of unreferenced vertices it will\n        # make the unique_rows call super slow so cull first\n        if hasattr(mesh, 'faces') and len(mesh.faces) > 0:\n            referenced = np.zeros(len(mesh.vertices), dtype=np.bool)\n            referenced[mesh.faces] = True\n        else:\n            # this is used for PointCloud objects\n            referenced = np.ones(len(mesh.vertices), dtype=np.bool)\n\n        # check unique rows of referenced vertices\n        u, i = unique_rows(mesh.vertices[referenced],\n                           digits=digits)\n\n        # construct an inverse using the subset\n        inverse = np.zeros(len(mesh.vertices), dtype=np.int64)\n        inverse[referenced] = i\n        # get the vertex mask\n        mask = np.nonzero(referenced)[0][u]\n        # run the update\n        mesh.update_vertices(mask=mask, inverse=inverse)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the indices of values that are identical to min_len and max_len", "response": "def group(values, min_len=0, max_len=np.inf):\n    \"\"\"\n    Return the indices of values that are identical\n\n    Parameters\n    ----------\n    values:     1D array\n    min_len:    int, the shortest group allowed\n                All groups will have len >= min_length\n    max_len:    int, the longest group allowed\n                All groups will have len <= max_length\n\n    Returns\n    ----------\n    groups: sequence of indices to form groups\n            IE [0,1,0,1] returns [[0,2], [1,3]]\n    \"\"\"\n    original = np.asanyarray(values)\n\n    # save the sorted order and then apply it\n    order = original.argsort()\n    values = original[order]\n\n    # find the indexes which are duplicates\n    if values.dtype.kind == 'f':\n        # for floats in a sorted array, neighbors are not duplicates\n        # if the difference between them is greater than approximate zero\n        nondupe = np.greater(np.abs(np.diff(values)), tol.zero)\n    else:\n        # for ints and strings we can check exact non- equality\n        # for all other types this will only work if they defined\n        # an __eq__\n        nondupe = values[1:] != values[:-1]\n\n    dupe_idx = np.append(0, np.nonzero(nondupe)[0] + 1)\n    dupe_len = np.diff(np.concatenate((dupe_idx, [len(values)])))\n    dupe_ok = np.logical_and(np.greater_equal(dupe_len, min_len),\n                             np.less_equal(dupe_len, max_len))\n    groups = [order[i:(i + j)]\n              for i, j in zip(dupe_idx[dupe_ok],\n                              dupe_len[dupe_ok])]\n    groups = np.array(groups)\n\n    return groups"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a numpy array of float or bool to int.", "response": "def float_to_int(data, digits=None, dtype=np.int32):\n    \"\"\"\n    Given a numpy array of float/bool/int, return as integers.\n\n    Parameters\n    -------------\n    data :  (n, d) float, int, or bool\n      Input data\n    digits : float or int\n      Precision for float conversion\n    dtype : numpy.dtype\n      What datatype should result be returned as\n\n    Returns\n    -------------\n    as_int : (n, d) int\n      Data as integers\n    \"\"\"\n    # convert to any numpy array\n    data = np.asanyarray(data)\n\n    # if data is already an integer or boolean we're done\n    # if the data is empty we are also done\n    if data.dtype.kind in 'ib' or data.size == 0:\n        return data.astype(dtype)\n\n    # populate digits from kwargs\n    if digits is None:\n        digits = util.decimal_to_digits(tol.merge)\n    elif isinstance(digits, float) or isinstance(digits, np.float):\n        digits = util.decimal_to_digits(digits)\n    elif not (isinstance(digits, int) or isinstance(digits, np.integer)):\n        log.warn('Digits were passed as %s!', digits.__class__.__name__)\n        raise ValueError('Digits must be None, int, or float!')\n\n    # data is float so convert to large integers\n    data_max = np.abs(data).max() * 10**digits\n    # ignore passed dtype if we have something large\n    dtype = [np.int32, np.int64][int(data_max > 2**31)]\n    # multiply by requested power of ten\n    # then subtract small epsilon to avoid \"go either way\" rounding\n    # then do the rounding and convert to integer\n    as_int = np.round((data * 10 ** digits) - 1e-6).astype(dtype)\n\n    return as_int"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unique_ordered(data):\n    data = np.asanyarray(data)\n    order = np.sort(np.unique(data, return_index=True)[1])\n    result = data[order]\n    return result", "response": "Returns the same as np. unique but ordered as per the\n    first occurrence of the unique value in data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef merge_runs(data, digits=None):\n    data = np.asanyarray(data)\n    mask = np.abs(np.diff(data)) > tol.merge\n    mask = np.concatenate((np.array([True]), mask))\n\n    return data[mask]", "response": "Merge duplicate sequential values. This differs from unique_ordered\n    in that it is not possible to merge multiple consecutive repeats."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unique_float(data,\n                 return_index=False,\n                 return_inverse=False,\n                 digits=None):\n    \"\"\"\n    Identical to the numpy.unique command, except evaluates floating point\n    numbers, using a specified number of digits.\n\n    If digits isn't specified, the library default TOL_MERGE will be used.\n    \"\"\"\n    data = np.asanyarray(data)\n    as_int = float_to_int(data, digits)\n    _junk, unique, inverse = np.unique(as_int,\n                                       return_index=True,\n                                       return_inverse=True)\n\n    if (not return_index) and (not return_inverse):\n        return data[unique]\n\n    result = [data[unique]]\n\n    if return_index:\n        result.append(unique)\n    if return_inverse:\n        result.append(inverse)\n    return tuple(result)", "response": "Returns the unique number of the given data."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unique_rows(data, digits=None):\n    hashes = hashable_rows(data, digits=digits)\n    garbage, unique, inverse = np.unique(hashes,\n                                         return_index=True,\n                                         return_inverse=True)\n    return unique, inverse", "response": "Returns indices of unique rows in a sequence of data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of unique entries in a tree from a list of data rows.", "response": "def group_rows(data, require_count=None, digits=None):\n    \"\"\"\n    Returns index groups of duplicate rows, for example:\n    [[1,2], [3,4], [1,2]] will return [[0,2], [1]]\n\n    Parameters\n    ----------\n    data:          (n,m) array\n    require_count: only returns groups of a specified length, eg:\n                   require_count =  2\n                   [[1,2], [3,4], [1,2]] will return [[0,2]]\n\n                   Note that using require_count allows numpy advanced indexing\n                   to be used in place of looping and checking hashes, and as a\n                   consequence is ~10x faster.\n\n    digits:        If data is floating point, how many decimals to look at.\n                   If this is None, the value in TOL_MERGE will be turned into a\n                   digit count and used.\n\n    Returns\n    ----------\n    groups:        List or sequence of indices from data indicating identical rows.\n                   If require_count != None, shape will be (j, require_count)\n                   If require_count is None, shape will be irregular (AKA a sequence)\n    \"\"\"\n\n    def group_dict():\n        \"\"\"\n        Simple hash table based grouping.\n        The loop and appends make this rather slow on very large arrays,\n        but it works on irregular groups.\n        \"\"\"\n        observed = dict()\n        hashable = hashable_rows(data, digits=digits)\n        for index, key in enumerate(hashable):\n            key_string = key.tostring()\n            if key_string in observed:\n                observed[key_string].append(index)\n            else:\n                observed[key_string] = [index]\n        return np.array(list(observed.values()))\n\n    def group_slice():\n        # create a representation of the rows that can be sorted\n        hashable = hashable_rows(data, digits=digits)\n        # record the order of the rows so we can get the original indices back\n        # later\n        order = np.argsort(hashable)\n        # but for now, we want our hashes sorted\n        hashable = hashable[order]\n        # this is checking each neighbour for equality, example:\n        # example: hashable = [1, 1, 1]; dupe = [0, 0]\n        dupe = hashable[1:] != hashable[:-1]\n        # we want the first index of a group, so we can slice from that location\n        # example: hashable = [0 1 1]; dupe = [1,0]; dupe_idx = [0,1]\n        dupe_idx = np.append(0, np.nonzero(dupe)[0] + 1)\n        # if you wanted to use this one function to deal with non- regular groups\n        # you could use: np.array_split(dupe_idx)\n        # this is roughly 3x slower than using the group_dict method above.\n        start_ok = np.diff(\n            np.concatenate((dupe_idx, [len(hashable)]))) == require_count\n        groups = np.tile(dupe_idx[start_ok].reshape((-1, 1)),\n                         require_count) + np.arange(require_count)\n        groups_idx = order[groups]\n        if require_count == 1:\n            return groups_idx.reshape(-1)\n        return groups_idx\n\n    if require_count is None:\n        return group_dict()\n    else:\n        return group_slice()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef boolean_rows(a, b, operation=np.intersect1d):\n    a = np.asanyarray(a, dtype=np.int64)\n    b = np.asanyarray(b, dtype=np.int64)\n\n    av = a.view([('', a.dtype)] * a.shape[1]).ravel()\n    bv = b.view([('', b.dtype)] * b.shape[1]).ravel()\n    shared = operation(av, bv).view(a.dtype).reshape(-1, a.shape[1])\n\n    return shared", "response": "Find the rows in two arrays which occur in both rows."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef group_vectors(vectors,\n                  angle=1e-4,\n                  include_negative=False):\n    \"\"\"\n    Group vectors based on an angle tolerance, with the option to\n    include negative vectors.\n\n    Parameters\n    -----------\n    vectors : (n,3) float\n        Direction vector\n    angle : float\n        Group vectors closer than this angle in radians\n    include_negative : bool\n        If True consider the same:\n        [0,0,1] and [0,0,-1]\n\n    Returns\n    ------------\n    new_vectors : (m,3) float\n        Direction vector\n    groups : (m,) sequence of int\n        Indices of source vectors\n    \"\"\"\n\n    vectors = np.asanyarray(vectors, dtype=np.float64)\n    angle = float(angle)\n\n    if include_negative:\n        vectors = util.vector_hemisphere(vectors)\n\n    spherical = util.vector_to_spherical(vectors)\n    angles, groups = group_distance(spherical, angle)\n    new_vectors = util.spherical_to_vector(angles)\n    return new_vectors, groups", "response": "Group vectors by an angle tolerance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of unique and groups of points which have farther than radius apart.", "response": "def group_distance(values, distance):\n    \"\"\"\n    Find groups of points which have neighbours closer than radius,\n    where no two points in a group are farther than distance apart.\n\n    Parameters\n    ---------\n    points :   (n, d) float\n        Points of dimension d\n    distance : float\n        Max distance between points in a cluster\n\n    Returns\n    ----------\n    unique : (m, d) float\n        Median value of each group\n    groups : (m) sequence of int\n        Indexes of points that make up a group\n\n    \"\"\"\n    values = np.asanyarray(values,\n                           dtype=np.float64)\n\n    consumed = np.zeros(len(values),\n                        dtype=np.bool)\n    tree = cKDTree(values)\n\n    # (n, d) set of values that are unique\n    unique = []\n    # (n) sequence of indices in values\n    groups = []\n\n    for index, value in enumerate(values):\n        if consumed[index]:\n            continue\n        group = np.array(tree.query_ball_point(value, distance),\n                         dtype=np.int)\n        consumed[group] = True\n        unique.append(np.median(values[group], axis=0))\n        groups.append(group)\n    return np.array(unique), np.array(groups)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef clusters(points, radius):\n    from . import graph\n    tree = cKDTree(points)\n\n    # some versions return pairs as a set of tuples\n    pairs = tree.query_pairs(r=radius, output_type='ndarray')\n    # group connected components\n    groups = graph.connected_components(pairs)\n\n    return groups", "response": "Returns a set of clusters of points which have neighbours closer than radius"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef blocks(data,\n           min_len=2,\n           max_len=np.inf,\n           digits=None,\n           only_nonzero=False):\n    \"\"\"\n    Given an array, find the indices of contiguous blocks\n    of equal values.\n\n    Parameters\n    ---------\n    data:    (n) array\n    min_len: int, the minimum length group to be returned\n    max_len: int, the maximum length group to be retuurned\n    digits:  if dealing with floats, how many digits to use\n    only_nonzero: bool, only return blocks of non- zero values\n\n    Returns\n    ---------\n    blocks: (m) sequence of indices referencing data\n    \"\"\"\n    data = float_to_int(data, digits=digits)\n\n    # find the inflection points, or locations where the array turns\n    # from True to False.\n    infl = np.concatenate(([0],\n                           np.nonzero(np.diff(data))[0] + 1,\n                           [len(data)]))\n    infl_len = np.diff(infl)\n    infl_ok = np.logical_and(infl_len >= min_len,\n                             infl_len <= max_len)\n\n    if only_nonzero:\n        # check to make sure the values of each contiguous block are True,\n        # by checking the first value of each block\n        infl_ok = np.logical_and(infl_ok,\n                                 data[infl[:-1]])\n\n    # inflate start/end indexes into full ranges of values\n    blocks = [np.arange(infl[i], infl[i + 1])\n              for i, ok in enumerate(infl_ok) if ok]\n    return blocks", "response": "Given an array find the indices of contiguous blocks of equal values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef group_min(groups, data):\n    # sort with major key groups, minor key data\n    order = np.lexsort((data, groups))\n    groups = groups[order]  # this is only needed if groups is unsorted\n    data = data[order]\n    # construct an index which marks borders between groups\n    index = np.empty(len(groups), 'bool')\n    index[0] = True\n    index[1:] = groups[1:] != groups[:-1]\n    return data[index]", "response": "Given a list of groups find the minimum element of data within each group"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef minimum_nsphere(obj):\n    # reduce the input points or mesh to the vertices of the convex hull\n    # since we are computing the furthest site voronoi diagram this reduces\n    # the input complexity substantially and returns the same value\n    points = convex.hull_points(obj)\n\n    # we are scaling the mesh to a unit cube\n    # this used to pass qhull_options 'QbB' to Voronoi however this had a bug somewhere\n    # to avoid this we scale to a unit cube ourselves inside this function\n    points_origin = points.min(axis=0)\n    points_scale = points.ptp(axis=0).min()\n    points = (points - points_origin) / points_scale\n\n    # if all of the points are on an n-sphere already the voronoi\n    # method will fail so we check a least squares fit before\n    # bothering to compute the voronoi diagram\n    fit_C, fit_R, fit_E = fit_nsphere(points)\n    # return fit radius and center to global scale\n    fit_R = (((points - fit_C)**2).sum(axis=1).max() ** .5) * points_scale\n    fit_C = (fit_C * points_scale) + points_origin\n\n    if fit_E < 1e-6:\n        log.debug('Points were on an n-sphere, returning fit')\n        return fit_C, fit_R\n\n    # calculate a furthest site voronoi diagram\n    # this will fail if the points are ALL on the surface of\n    # the n-sphere but hopefully the least squares check caught those cases\n    # , qhull_options='QbB Pp')\n    voronoi = spatial.Voronoi(points, furthest_site=True)\n\n    # find the maximum radius^2 point for each of the voronoi vertices\n    # this is worst case quite expensive but we have taken\n    # convex hull to reduce n for this operation\n    # we are doing comparisons on the radius squared then rooting once\n    try:\n        # cdist is massivly faster than looping or tiling methods\n        # although it does create a very large intermediate array\n        # first, get an order of magnitude memory size estimate\n        # a float64 would be 8 bytes per entry plus overhead\n        memory_estimate = len(voronoi.vertices) * len(points) * 9\n        if memory_estimate > _MAX_MEMORY():\n            raise MemoryError\n        radii_2 = spatial.distance.cdist(\n            voronoi.vertices, points,\n            metric='sqeuclidean').max(axis=1)\n    except MemoryError:\n        # log the MemoryError\n        log.warning('MemoryError: falling back to slower check!')\n        # fall back to a potentially very slow list comprehension\n        radii_2 = np.array([((points - v) ** 2).sum(axis=1).max()\n                            for v in voronoi.vertices])\n\n    # we want the smallest sphere so take the min of the radii\n    radii_idx = radii_2.argmin()\n\n    # return voronoi radius and center to global scale\n    radius_v = np.sqrt(radii_2[radii_idx]) * points_scale\n    center_v = (voronoi.vertices[radii_idx] *\n                points_scale) + points_origin\n\n    if radius_v > fit_R:\n        return fit_C, fit_R\n\n    return center_v, radius_v", "response": "Compute the minimum n - sphere for a mesh or a set of points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfit an n - sphere to a set of points using least squares.", "response": "def fit_nsphere(points, prior=None):\n    \"\"\"\n    Fit an n-sphere to a set of points using least squares.\n\n    Parameters\n    ---------\n    points : (n, d) float\n      Points in space\n    prior : (d,) float\n      Best guess for center of nsphere\n\n    Returns\n    ---------\n    center : (d,) float\n      Location of center\n    radius : float\n      Mean radius across circle\n    error : float\n      Peak to peak value of deviation from mean radius\n    \"\"\"\n    # make sure points are numpy array\n    points = np.asanyarray(points, dtype=np.float64)\n    # create ones so we can dot instead of using slower sum\n    ones = np.ones(points.shape[1])\n\n    def residuals(center):\n        # do the axis sum with a dot\n        # this gets called a LOT so worth optimizing\n        radii_sq = np.dot((points - center) ** 2, ones)\n        # residuals are difference between mean\n        # use our sum mean vs .mean() as it is slightly faster\n        return radii_sq - (radii_sq.sum() / len(radii_sq))\n\n    if prior is None:\n        guess = points.mean(axis=0)\n    else:\n        guess = np.asanyarray(prior)\n\n    center_result, return_code = leastsq(residuals,\n                                         guess,\n                                         xtol=1e-8)\n\n    if not (return_code in [1, 2, 3, 4]):\n        raise ValueError('Least square fit failed!')\n\n    radii = np.linalg.norm(points - center_result, axis=1)\n    radius = radii.mean()\n    error = radii.ptp()\n    return center_result, radius, error"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a list of points is an nsphere.", "response": "def is_nsphere(points):\n    \"\"\"\n    Check if a list of points is an nsphere.\n\n    Parameters\n    -----------\n    points : (n, dimension) float\n      Points in space\n\n    Returns\n    -----------\n    check : bool\n      True if input points are on an nsphere\n    \"\"\"\n    center, radius, error = fit_nsphere(points)\n    check = error < tol.merge\n    return check"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if a mesh contains a set of points.", "response": "def contains_points(intersector,\n                    points,\n                    check_direction=None):\n    \"\"\"\n    Check if a mesh contains a set of points, using ray tests.\n\n    If the point is on the surface of the mesh, behavior is\n    undefined.\n\n    Parameters\n    ---------\n    mesh: Trimesh object\n    points: (n,3) points in space\n\n    Returns\n    ---------\n    contains : (n) bool\n                  Whether point is inside mesh or not\n    \"\"\"\n    # convert points to float and make sure they are 3D\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(points, (-1, 3)):\n        raise ValueError('points must be (n,3)')\n\n    # placeholder result with no hits we'll fill in later\n    contains = np.zeros(len(points), dtype=np.bool)\n\n    # cull points outside of the axis aligned bounding box\n    # this avoids running ray tests unless points are close\n    inside_aabb = bounds.contains(intersector.mesh.bounds,\n                                  points)\n\n    # if everything is outside the AABB, exit early\n    if not inside_aabb.any():\n        return contains\n\n    # default ray direction is random, but we are not generating\n    # uniquely each time so the behavior of this function is easier to debug\n    default_direction = np.array([0.4395064455,\n                                  0.617598629942,\n                                  0.652231566745])\n    if check_direction is None:\n        # if no check direction is specified use the default\n        # stack it only for points inside the AABB\n        ray_directions = np.tile(default_direction,\n                                 (inside_aabb.sum(), 1))\n    else:\n        # if a direction is passed use it\n        ray_directions = np.tile(\n            np.array(check_direction).reshape(3),\n            (inside_aabb.sum(), 1))\n\n    # cast a ray both forwards and backwards\n    location, index_ray, c = intersector.intersects_location(\n        np.vstack(\n            (points[inside_aabb],\n             points[inside_aabb])),\n        np.vstack(\n            (ray_directions,\n             -ray_directions)))\n\n    # if we hit nothing in either direction just return with no hits\n    if len(index_ray) == 0:\n        return contains\n\n    # reshape so bi_hits[0] is the result in the forward direction and\n    #            bi_hits[1] is the result in the backwards directions\n    bi_hits = np.bincount(\n        index_ray,\n        minlength=len(ray_directions) * 2).reshape((2, -1))\n    # a point is probably inside if it hits a surface an odd number of times\n    bi_contains = np.mod(bi_hits, 2) == 1\n\n    # if the mod of the hit count is the same in both\n    # directions, we can save that result and move on\n    agree = np.equal(*bi_contains)\n\n    # in order to do an assignment we can only have one\n    # level of boolean indexes, for example this doesn't work:\n    # contains[inside_aabb][agree] = bi_contains[0][agree]\n    # no error is thrown, but nothing gets assigned\n    # to get around that, we create a single mask for assignment\n    mask = inside_aabb.copy()\n    mask[mask] = agree\n\n    # set contains flags for things inside the AABB and who have\n    # ray tests that agree in both directions\n    contains[mask] = bi_contains[0][agree]\n\n    # if one of the rays in either direction hit nothing\n    # it is a very solid indicator we are in free space\n    # as the edge cases we are working around tend to\n    # add hits rather than miss hits\n    one_freespace = (bi_hits == 0).any(axis=0)\n\n    # rays where they don't agree and one isn't in free space\n    # are deemed to be broken\n    broken = np.logical_and(np.logical_not(agree),\n                            np.logical_not(one_freespace))\n\n    # if all rays agree return\n    if not broken.any():\n        return contains\n\n    # try to run again with a new random vector\n    # only do it if check_direction isn't specified\n    # to avoid infinite recursion\n    if check_direction is None:\n        # we're going to run the check again in a random direction\n        new_direction = util.unitize(np.random.random(3) - .5)\n        # do the mask trick again to be able to assign results\n        mask = inside_aabb.copy()\n        mask[mask] = broken\n\n        contains[mask] = contains_points(\n            intersector,\n            points[inside_aabb][broken],\n            check_direction=new_direction)\n\n        constants.log.debug(\n            'detected %d broken contains test, attempted to fix',\n            broken.sum())\n\n    return contains"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef mesh_to_BVH(mesh):\n    bvh = fcl.BVHModel()\n    bvh.beginModel(num_tris_=len(mesh.faces),\n                   num_vertices_=len(mesh.vertices))\n    bvh.addSubModel(verts=mesh.vertices,\n                    triangles=mesh.faces)\n    bvh.endModel()\n    return bvh", "response": "Create a BVHModel object from a Trimesh object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scene_to_collision(scene):\n    manager = CollisionManager()\n    objects = {}\n    for node in scene.graph.nodes_geometry:\n        T, geometry = scene.graph[node]\n        objects[node] = manager.add_object(name=node,\n                                           mesh=scene.geometry[geometry],\n                                           transform=T)\n    return manager, objects", "response": "Create a collision object from a trimesh. Scene object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an object to the collision manager.", "response": "def add_object(self,\n                   name,\n                   mesh,\n                   transform=None):\n        \"\"\"\n        Add an object to the collision manager.\n\n        If an object with the given name is already in the manager,\n        replace it.\n\n        Parameters\n        ----------\n        name : str\n          An identifier for the object\n        mesh : Trimesh object\n          The geometry of the collision object\n        transform : (4,4) float\n          Homogenous transform matrix for the object\n        \"\"\"\n\n        # if no transform passed, assume identity transform\n        if transform is None:\n            transform = np.eye(4)\n        transform = np.asanyarray(transform, dtype=np.float32)\n        if transform.shape != (4, 4):\n            raise ValueError('transform must be (4,4)!')\n\n        # create or recall from cache BVH\n        bvh = self._get_BVH(mesh)\n        # create the FCL transform from (4,4) matrix\n        t = fcl.Transform(transform[:3, :3], transform[:3, 3])\n        o = fcl.CollisionObject(bvh, t)\n\n        # Add collision object to set\n        if name in self._objs:\n            self._manager.unregisterObject(self._objs[name])\n        self._objs[name] = {'obj': o,\n                            'geom': bvh}\n        # store the name of the geometry\n        self._names[id(bvh)] = name\n\n        self._manager.registerObject(o)\n        self._manager.update()\n        return o"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove an object from the collision manager.", "response": "def remove_object(self, name):\n        \"\"\"\n        Delete an object from the collision manager.\n\n        Parameters\n        ----------\n        name : str\n          The identifier for the object\n        \"\"\"\n        if name in self._objs:\n            self._manager.unregisterObject(self._objs[name]['obj'])\n            self._manager.update(self._objs[name]['obj'])\n            # remove objects from _objs\n            geom_id = id(self._objs.pop(name)['geom'])\n            # remove names\n            self._names.pop(geom_id)\n        else:\n            raise ValueError('{} not in collision manager!'.format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the homogenous transform for one of the objects in the manager.", "response": "def set_transform(self, name, transform):\n        \"\"\"\n        Set the transform for one of the manager's objects.\n        This replaces the prior transform.\n\n        Parameters\n        ----------\n        name : str\n          An identifier for the object already in the manager\n        transform : (4,4) float\n          A new homogenous transform matrix for the object\n        \"\"\"\n        if name in self._objs:\n            o = self._objs[name]['obj']\n            o.setRotation(transform[:3, :3])\n            o.setTranslation(transform[:3, 3])\n            self._manager.update(o)\n        else:\n            raise ValueError('{} not in collision manager!'.format(name))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks a single object for collisions with the manager.", "response": "def in_collision_single(self, mesh, transform=None,\n                            return_names=False, return_data=False):\n        \"\"\"\n        Check a single object for collisions against all objects in the\n        manager.\n\n        Parameters\n        ----------\n        mesh : Trimesh object\n          The geometry of the collision object\n        transform : (4,4) float\n          Homogenous transform matrix\n        return_names : bool\n          If true, a set is returned containing the names\n          of all objects in collision with the object\n        return_data :  bool\n          If true, a list of ContactData is returned as well\n\n        Returns\n        ------------\n        is_collision : bool\n          True if a collision occurs and False otherwise\n        names : set of str\n          The set of names of objects that collided with the\n          provided one\n        contacts : list of ContactData\n          All contacts detected\n        \"\"\"\n        if transform is None:\n            transform = np.eye(4)\n\n        # Create FCL data\n        b = self._get_BVH(mesh)\n        t = fcl.Transform(transform[:3, :3], transform[:3, 3])\n        o = fcl.CollisionObject(b, t)\n\n        # Collide with manager's objects\n        cdata = fcl.CollisionData()\n        if return_names or return_data:\n            cdata = fcl.CollisionData(request=fcl.CollisionRequest(\n                num_max_contacts=100000,\n                enable_contact=True))\n\n        self._manager.collide(o, cdata, fcl.defaultCollisionCallback)\n        result = cdata.result.is_collision\n\n        # If we want to return the objects that were collision, collect them.\n        objs_in_collision = set()\n        contact_data = []\n        if return_names or return_data:\n            for contact in cdata.result.contacts:\n                cg = contact.o1\n                if cg == b:\n                    cg = contact.o2\n                name = self._extract_name(cg)\n\n                names = (name, '__external')\n                if cg == contact.o2:\n                    names = reversed(names)\n\n                if return_names:\n                    objs_in_collision.add(name)\n                if return_data:\n                    contact_data.append(ContactData(names, contact))\n\n        if return_names and return_data:\n            return result, objs_in_collision, contact_data\n        elif return_names:\n            return result, objs_in_collision\n        elif return_data:\n            return result, contact_data\n        else:\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if any object from this manager collides with any object from another manager.", "response": "def in_collision_other(self, other_manager,\n                           return_names=False, return_data=False):\n        \"\"\"\n        Check if any object from this manager collides with any object\n        from another manager.\n\n        Parameters\n        -------------------\n        other_manager : CollisionManager\n          Another collision manager object\n        return_names : bool\n          If true, a set is returned containing the names\n          of all pairs of objects in collision.\n        return_data : bool\n          If true, a list of ContactData is returned as well\n\n        Returns\n        -------------\n        is_collision : bool\n          True if a collision occurred between any pair of objects\n          and False otherwise\n        names : set of 2-tup\n          The set of pairwise collisions. Each tuple\n          contains two names (first from this manager,\n          second from the other_manager) indicating\n          that the two corresponding objects are in collision.\n        contacts : list of ContactData\n          All contacts detected\n        \"\"\"\n        cdata = fcl.CollisionData()\n        if return_names or return_data:\n            cdata = fcl.CollisionData(\n                request=fcl.CollisionRequest(\n                    num_max_contacts=100000,\n                    enable_contact=True))\n        self._manager.collide(other_manager._manager,\n                              cdata,\n                              fcl.defaultCollisionCallback)\n        result = cdata.result.is_collision\n\n        objs_in_collision = set()\n        contact_data = []\n        if return_names or return_data:\n            for contact in cdata.result.contacts:\n                reverse = False\n                names = (self._extract_name(contact.o1),\n                         other_manager._extract_name(contact.o2))\n                if names[0] is None:\n                    names = (self._extract_name(contact.o2),\n                             other_manager._extract_name(contact.o1))\n                    reverse = True\n\n                if return_names:\n                    objs_in_collision.add(names)\n                if return_data:\n                    if reverse:\n                        names = reversed(names)\n                    contact_data.append(ContactData(names, contact))\n\n        if return_names and return_data:\n            return result, objs_in_collision, contact_data\n        elif return_names:\n            return result, objs_in_collision\n        elif return_data:\n            return result, contact_data\n        else:\n            return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef min_distance_single(self,\n                            mesh,\n                            transform=None,\n                            return_name=False,\n                            return_data=False):\n        \"\"\"\n        Get the minimum distance between a single object and any\n        object in the manager.\n\n        Parameters\n        ---------------\n        mesh : Trimesh object\n          The geometry of the collision object\n        transform : (4,4) float\n          Homogenous transform matrix for the object\n        return_names : bool\n          If true, return name of the closest object\n        return_data : bool\n          If true, a DistanceData object is returned as well\n\n        Returns\n        -------------\n        distance : float\n          Min distance between mesh and any object in the manager\n        name : str\n          The name of the object in the manager that was closest\n        data : DistanceData\n          Extra data about the distance query\n        \"\"\"\n        if transform is None:\n            transform = np.eye(4)\n\n        # Create FCL data\n        b = self._get_BVH(mesh)\n\n        t = fcl.Transform(transform[:3, :3], transform[:3, 3])\n        o = fcl.CollisionObject(b, t)\n\n        # Collide with manager's objects\n        ddata = fcl.DistanceData()\n        if return_data:\n            ddata = fcl.DistanceData(\n                fcl.DistanceRequest(enable_nearest_points=True),\n                fcl.DistanceResult()\n            )\n\n        self._manager.distance(o, ddata, fcl.defaultDistanceCallback)\n\n        distance = ddata.result.min_distance\n\n        # If we want to return the objects that were collision, collect them.\n        name, data = None, None\n        if return_name or return_data:\n            cg = ddata.result.o1\n            if cg == b:\n                cg = ddata.result.o2\n\n            name = self._extract_name(cg)\n\n            names = (name, '__external')\n            if cg == ddata.result.o2:\n                names = reversed(names)\n            data = DistanceData(names, ddata.result)\n\n        if return_name and return_data:\n            return distance, name, data\n        elif return_name:\n            return distance, name\n        elif return_data:\n            return distance, data\n        else:\n            return distance", "response": "Get the minimum distance between a single object and any object in the manager."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef min_distance_internal(self, return_names=False, return_data=False):\n        ddata = fcl.DistanceData()\n        if return_data:\n            ddata = fcl.DistanceData(\n                fcl.DistanceRequest(enable_nearest_points=True),\n                fcl.DistanceResult()\n            )\n\n        self._manager.distance(ddata, fcl.defaultDistanceCallback)\n\n        distance = ddata.result.min_distance\n\n        names, data = None, None\n        if return_names or return_data:\n            names = (self._extract_name(ddata.result.o1),\n                     self._extract_name(ddata.result.o2))\n            data = DistanceData(names, ddata.result)\n            names = tuple(sorted(names))\n\n        if return_names and return_data:\n            return distance, names, data\n        elif return_names:\n            return distance, names\n        elif return_data:\n            return distance, data\n        else:\n            return distance", "response": "Returns the minimum distance between any pair of managed objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the minimum distance between two objects in each manager and another.", "response": "def min_distance_other(self, other_manager,\n                           return_names=False, return_data=False):\n        \"\"\"\n        Get the minimum distance between any pair of objects,\n        one in each manager.\n\n        Parameters\n        ----------\n        other_manager : CollisionManager\n          Another collision manager object\n        return_names : bool\n          If true, a 2-tuple is returned containing\n          the names of the closest objects.\n        return_data : bool\n          If true, a DistanceData object is returned as well\n\n        Returns\n        -----------\n        distance : float\n          The min distance between a pair of objects,\n          one from each manager.\n        names : 2-tup of str\n          A 2-tuple containing two names (first from this manager,\n          second from the other_manager) indicating\n          the two closest objects.\n        data : DistanceData\n          Extra data about the distance query\n        \"\"\"\n        ddata = fcl.DistanceData()\n        if return_data:\n            ddata = fcl.DistanceData(\n                fcl.DistanceRequest(enable_nearest_points=True),\n                fcl.DistanceResult()\n            )\n\n        self._manager.distance(other_manager._manager,\n                               ddata,\n                               fcl.defaultDistanceCallback)\n\n        distance = ddata.result.min_distance\n\n        names, data = None, None\n        if return_names or return_data:\n            reverse = False\n            names = (self._extract_name(ddata.result.o1),\n                     other_manager._extract_name(ddata.result.o2))\n            if names[0] is None:\n                reverse = True\n                names = (self._extract_name(ddata.result.o2),\n                         other_manager._extract_name(ddata.result.o1))\n\n            dnames = tuple(names)\n            if reverse:\n                dnames = reversed(dnames)\n            data = DistanceData(dnames, ddata.result)\n\n        if return_names and return_data:\n            return distance, names, data\n        elif return_names:\n            return distance, names\n        elif return_data:\n            return distance, data\n        else:\n            return distance"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cylinder_inertia(mass, radius, height, transform=None):\n    h2, r2 = height ** 2, radius ** 2\n    diagonal = np.array([((mass * h2) / 12) + ((mass * r2) / 4),\n                         ((mass * h2) / 12) + ((mass * r2) / 4),\n                         (mass * r2) / 2])\n    inertia = diagonal * np.eye(3)\n\n    if transform is not None:\n        inertia = transform_inertia(transform, inertia)\n\n    return inertia", "response": "Returns the inertia tensor of a cylinder."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the principal components and row vectors pointing along the principal axes of the inertia tensor.", "response": "def principal_axis(inertia):\n    \"\"\"\n    Find the principal components and principal axis\n    of inertia from the inertia tensor.\n\n    Parameters\n    ------------\n    inertia : (3,3) float\n      Inertia tensor\n\n    Returns\n    ------------\n    components : (3,) float\n      Principal components of inertia\n    vectors : (3,3) float\n      Row vectors pointing along the\n      principal axes of inertia\n    \"\"\"\n    inertia = np.asanyarray(inertia, dtype=np.float64)\n    if inertia.shape != (3, 3):\n        raise ValueError('inertia tensor must be (3,3)!')\n\n    # you could any of the following to calculate this:\n    # np.linalg.svd, np.linalg.eig, np.linalg.eigh\n    # moment of inertia is square symmetric matrix\n    # eigh has the best numeric precision in tests\n    components, vectors = np.linalg.eigh(inertia * negate_nondiagonal)\n\n    # eigh returns them as column vectors, change them to row vectors\n    vectors = vectors.T\n\n    return components, vectors"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms an inertia tensor to a new frame.", "response": "def transform_inertia(transform, inertia_tensor):\n    \"\"\"\n    Transform an inertia tensor to a new frame.\n\n    More details in OCW PDF:\n    MIT16_07F09_Lec26.pdf\n\n    Parameters\n    ------------\n    transform : (3, 3) or (4, 4) float\n      Transformation matrix\n    inertia_tensor : (3, 3) float\n      Inertia tensor\n\n    Returns\n    ------------\n    transformed : (3, 3) float\n      Inertia tensor in new frame\n    \"\"\"\n    # check inputs and extract rotation\n    transform = np.asanyarray(transform, dtype=np.float64)\n    if transform.shape == (4, 4):\n        rotation = transform[:3, :3]\n    elif transform.shape == (3, 3):\n        rotation = transform\n    else:\n        raise ValueError('transform must be (3,3) or (4,4)!')\n\n    inertia_tensor = np.asanyarray(inertia_tensor, dtype=np.float64)\n    if inertia_tensor.shape != (3, 3):\n        raise ValueError('inertia_tensor must be (3,3)!')\n\n    transformed = util.multi_dot([rotation,\n                                  inertia_tensor * negate_nondiagonal,\n                                  rotation.T])\n    transformed *= negate_nondiagonal\n    return transformed"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck whether a mesh has radial symmetry.", "response": "def radial_symmetry(mesh):\n    \"\"\"\n    Check whether a mesh has rotational symmetry.\n\n    Returns\n    -----------\n    symmetry : None or str\n         None         No rotational symmetry\n         'radial'     Symmetric around an axis\n         'spherical'  Symmetric around a point\n    axis : None or (3,) float\n      Rotation axis or point\n    section : None or (3, 2) float\n      If radial symmetry provide vectors\n      to get cross section\n    \"\"\"\n\n    # if not a volume this is meaningless\n    if not mesh.is_volume:\n        return None, None, None\n\n    # the sorted order of the principal components of inertia (3,) float\n    order = mesh.principal_inertia_components.argsort()\n\n    # we are checking if a geometry has radial symmetry\n    # if 2 of the PCI are equal, it is a revolved 2D profile\n    # if 3 of the PCI (all of them) are equal it is a sphere\n    # thus we take the diff of the sorted PCI, scale it as a ratio\n    # of the largest PCI, and then scale to the tolerance we care about\n    # if tol is 1e-3, that means that 2 components are identical if they\n    # are within .1% of the maximum PCI.\n    diff = np.abs(np.diff(mesh.principal_inertia_components[order]))\n    diff /= np.abs(mesh.principal_inertia_components).max()\n    # diffs that are within tol of zero\n    diff_zero = (diff / 1e-3).astype(int) == 0\n\n    if diff_zero.all():\n        # this is the case where all 3 PCI are identical\n        # this means that the geometry is symmetric about a point\n        # examples of this are a sphere, icosahedron, etc\n        axis = mesh.principal_inertia_vectors[0]\n        section = mesh.principal_inertia_vectors[1:]\n\n        return 'spherical', axis, section\n\n    elif diff_zero.any():\n        # this is the case for 2/3 PCI are identical\n        # this means the geometry is symmetric about an axis\n        # probably a revolved 2D profile\n\n        # we know that only 1/2 of the diff values are True\n        # if the first diff is 0, it means if we take the first element\n        # in the ordered PCI we will have one of the non- revolve axis\n        # if the second diff is 0, we take the last element of\n        # the ordered PCI for the section axis\n        # if we wanted the revolve axis we would just switch [0,-1] to\n        # [-1,0]\n\n        # since two vectors are the same, we know the middle\n        # one is one of those two\n        section_index = order[np.array([[0, 1],\n                                        [1, -1]])[diff_zero]].flatten()\n        section = mesh.principal_inertia_vectors[section_index]\n\n        # we know the rotation axis is the sole unique value\n        # and is either first or last of the sorted values\n        axis_index = order[np.array([-1, 0])[diff_zero]][0]\n        axis = mesh.principal_inertia_vectors[axis_index]\n        return 'radial', axis, section\n\n    return None, None, None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef autolight(scene):\n\n    # create two default point lights\n    lights = [PointLight(), PointLight()]\n\n    # create two translation matrices for bounds corners\n    transforms = [transformations.translation_matrix(b)\n                  for b in scene.bounds]\n\n    return lights, transforms", "response": "Generate a list of lights for a scene that looks decent."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tracked_array(array, dtype=None):\n    # if someone passed us None, just create an empty array\n    if array is None:\n        array = []\n    # make sure it is contiguous then view it as our subclass\n    tracked = np.ascontiguousarray(\n        array, dtype=dtype).view(TrackedArray)\n    # should always be contiguous here\n    assert tracked.flags['C_CONTIGUOUS']\n\n    return tracked", "response": "Returns a TrackedArray containing the contents of the given array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cache_decorator(function):\n\n    # use wraps to preseve docstring\n    @wraps(function)\n    def get_cached(*args, **kwargs):\n        \"\"\"\n        Only execute the function if its value isn't stored\n        in cache already.\n        \"\"\"\n        self = args[0]\n        # use function name as key in cache\n        name = function.__name__\n        # do the dump logic ourselves to avoid\n        # verifying cache twice per call\n        self._cache.verify()\n        # access cache dict to avoid automatic verification\n        if name in self._cache.cache:\n            # already stored so return value\n            return self._cache.cache[name]\n\n        # time execution\n        tic = time.time()\n        # value not in cache so execute the function\n        value = function(*args, **kwargs)\n        # store the value\n        self._cache.cache[name] = value\n        # debug log execution time\n        # this is nice for debugging as you can see when\n        # cache is getting dumped all the time\n        log.debug('%s was not in cache, executed in %.6f',\n                  name,\n                  time.time() - tic)\n        return value\n\n    # all cached values are also properties\n    # so they can be accessed like value attributes\n    # rather than functions\n    return property(get_cached)", "response": "A decorator for class methods replaces properties and functions with objects and stores and retrieve the value in the object cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _fast_crc(count=50):\n    import timeit\n\n    setup = 'import numpy, zlib;'\n    setup += 'd = numpy.random.random((500,3));'\n\n    crc32 = timeit.timeit(setup=setup,\n                          stmt='zlib.crc32(d)',\n                          number=count)\n    adler32 = timeit.timeit(setup=setup,\n                            stmt='zlib.adler32(d)',\n                            number=count)\n    if adler32 < crc32:\n        return zlib.adler32\n    else:\n        return zlib.crc32", "response": "This function is used to determine the fastest hashing function available in zlib."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an MD5 hash of the current array.", "response": "def md5(self):\n        \"\"\"\n        Return an MD5 hash of the current array.\n\n        Returns\n        -----------\n        md5: str, hexadecimal MD5 of the array\n        \"\"\"\n        if self._modified_m or not hasattr(self, '_hashed_md5'):\n            if self.flags['C_CONTIGUOUS']:\n                hasher = hashlib.md5(self)\n                self._hashed_md5 = hasher.hexdigest()\n            else:\n                # the case where we have sliced our nice\n                # contiguous array into a non- contiguous block\n                # for example (note slice *after* track operation):\n                # t = util.tracked_array(np.random.random(10))[::-1]\n                contiguous = np.ascontiguousarray(self)\n                hasher = hashlib.md5(contiguous)\n                self._hashed_md5 = hasher.hexdigest()\n        self._modified_m = False\n        return self._hashed_md5"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _xxhash(self):\n        # repeat the bookkeeping to get a contiguous array inside\n        # the function to avoid additional function calls\n        # these functions are called millions of times so everything helps\n        if self._modified_x or not hasattr(self, '_hashed_xx'):\n            if self.flags['C_CONTIGUOUS']:\n                hasher = xxhash.xxh64(self)\n                self._hashed_xx = hasher.intdigest()\n            else:\n                # the case where we have sliced our nice\n                # contiguous array into a non- contiguous block\n                # for example (note slice *after* track operation):\n                # t = util.tracked_array(np.random.random(10))[::-1]\n                contiguous = np.ascontiguousarray(self)\n                hasher = xxhash.xxh64(contiguous)\n                self._hashed_xx = hasher.intdigest()\n        self._modified_x = False\n        return self._hashed_xx", "response": "Returns the xxhash. b64 hash of the array."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a key from the cache.", "response": "def delete(self, key):\n        \"\"\"\n        Remove a key from the cache.\n        \"\"\"\n        if key in self.cache:\n            self.cache.pop(key, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nverifying that the cached values are still for the same value of id_function and delete all stored items if they have changed.", "response": "def verify(self):\n        \"\"\"\n        Verify that the cached values are still for the same\n        value of id_function and delete all stored items if\n        the value of id_function has changed.\n        \"\"\"\n        # if we are in a lock don't check anything\n        if self._lock != 0:\n            return\n\n        # check the hash of our data\n        id_new = self._id_function()\n\n        # things changed\n        if id_new != self.id_current:\n            if len(self.cache) > 0:\n                log.debug('%d items cleared from cache: %s',\n                          len(self.cache),\n                          str(list(self.cache.keys())))\n            # hash changed, so dump the cache\n            # do it manually rather than calling clear()\n            # as we are internal logic and can avoid function calls\n            self.cache = {}\n            # set the id to the new data hash\n            self.id_current = id_new"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear(self, exclude=None):\n        if exclude is None:\n            self.cache = {}\n        else:\n            self.cache = {k: v for k, v in self.cache.items()\n                          if k in exclude}", "response": "Remove all elements in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nam the current DataStore empty or not.", "response": "def is_empty(self):\n        \"\"\"\n        Is the current DataStore empty or not.\n\n        Returns\n        ----------\n        empty: bool, False if there are items in the DataStore\n        \"\"\"\n        if len(self.data) == 0:\n            return True\n        for v in self.data.values():\n            if is_sequence(v):\n                if len(v) == 0:\n                    return True\n                else:\n                    return False\n            elif bool(np.isreal(v)):\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an MD5 reflecting everything in the DataStore. Returns the MD5 in hexadecimal", "response": "def md5(self):\n        \"\"\"\n        Get an MD5 reflecting everything in the DataStore.\n\n        Returns\n        ----------\n        md5: str, MD5 in hexadecimal\n        \"\"\"\n        hasher = hashlib.md5()\n        for key in sorted(self.data.keys()):\n            hasher.update(self.data[key].md5().encode('utf-8'))\n        md5 = hasher.hexdigest()\n        return md5"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a CRC reflecting everything in the DataStore.", "response": "def crc(self):\n        \"\"\"\n        Get a CRC reflecting everything in the DataStore.\n\n        Returns\n        ----------\n        crc: int, CRC of data\n        \"\"\"\n        crc = sum(i.crc() for i in self.data.values())\n        return crc"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget a CRC32 or xxhash. xxh64 reflecting the DataStore.", "response": "def fast_hash(self):\n        \"\"\"\n        Get a CRC32 or xxhash.xxh64 reflecting the DataStore.\n\n        Returns\n        ------------\n        hashed: int, checksum of data\n        \"\"\"\n        fast = sum(i.fast_hash() for i in self.data.values())\n        return fast"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef identifier_simple(mesh):\n    # verify the cache once\n    mesh._cache.verify()\n\n    # don't check hashes during identifier as we aren't\n    # changing any data values of the mesh inside block\n    # if we did change values in cache block things would break\n    with mesh._cache:\n        # pre-allocate identifier so indexes of values can't move around\n        # like they might if we used hstack or something else\n        identifier = np.zeros(6, dtype=np.float64)\n        # avoid thrashing the cache unnecessarily\n        mesh_area = mesh.area\n        # start with properties that are valid regardless of watertightness\n        # note that we're going to try to make all parameters relative\n        # to area so other values don't get blown up at weird scales\n        identifier[0] = mesh_area\n\n        # topological constant and the only thing we can really\n        # trust in this fallen world\n        identifier[1] = mesh.euler_number\n\n        # if we have a watertight mesh include volume and inertia\n        if mesh.is_volume:\n            # side length of a cube ratio\n            # 1.0 for cubes, different values for other things\n            identifier[2] = (((mesh_area / 6.0) ** (1.0 / 2.0)) /\n                             (mesh.volume ** (1.0 / 3.0)))\n            # save vertices for radius calculation\n            vertices = mesh.vertices - mesh.center_mass\n            # we are going to special case radially symmetric meshes\n            # to replace their surface area with ratio of their\n            # surface area to a primitive sphere or cylinder surface area\n            # this is because tessellated curved surfaces are really rough\n            # to reliably hash as they are very sensitive to floating point\n            # and tessellation error. By making area proportionate to a fit\n            # primitive area we are able to reliably hash at more sigfigs\n            if mesh.symmetry == 'radial':\n                # cylinder height\n                h = np.dot(vertices, mesh.symmetry_axis).ptp()\n                # section radius\n                R2 = (np.dot(vertices, mesh.symmetry_section.T)\n                      ** 2).sum(axis=1).max()\n                # area of a cylinder primitive\n                area = (2 * np.pi * (R2**.5) * h) + (2 * np.pi * R2)\n                # replace area in this case with area ratio\n                identifier[0] = mesh_area / area\n            elif mesh.symmetry == 'spherical':\n                # handle a spherically symmetric mesh\n                R2 = (vertices ** 2).sum(axis=1).max()\n                area = 4 * np.pi * R2\n                identifier[0] = mesh_area / area\n        else:\n            # if we don't have a watertight mesh add information about the\n            # convex hull, which is slow to compute and unreliable\n            # just what we're looking for in a hash but hey\n            identifier[3] = mesh_area / mesh.convex_hull.area\n            # cube side length ratio for the hull\n            identifier[4] = (((mesh.convex_hull.area / 6.0) ** (1.0 / 2.0)) /\n                             (mesh.convex_hull.volume ** (1.0 / 3.0)))\n            vertices = mesh.vertices - mesh.centroid\n\n            # add in max radius^2 to area ratio\n            R2 = (vertices ** 2).sum(axis=1).max()\n            identifier[5] = R2 / mesh_area\n\n    return identifier", "response": "Return a basic identifier for a mesh with a simple set of properties."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhashing an identifier array to a specified number of significant figures.", "response": "def identifier_hash(identifier, sigfig=None):\n    \"\"\"\n    Hash an identifier array to a specified number of\n    significant figures.\n\n    Parameters\n    ----------\n    identifier : (n,) float\n      Vector of properties\n    sigfig : (n,) int\n      Number of sigfigs per property\n\n    Returns\n    ----------\n    md5 : str\n      MD5 hash of identifier\n    \"\"\"\n    if sigfig is None:\n        sigfig = id_sigfig\n\n    # convert identifier to integers and order of magnitude\n    as_int, multiplier = util.sigfig_int(identifier, sigfig)\n    # make all scales positive\n    if (multiplier < 0).any():\n        multiplier += np.abs(multiplier.min())\n    hashable = (as_int * (10 ** multiplier)).astype(np.int64)\n    md5 = util.md5_object(hashable)\n    return md5"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef face_ordering(mesh):\n\n    # the length of each edge in faces\n    norms = mesh.edges_unique_length[\n        mesh.edges_unique_inverse].reshape((-1, 3))\n\n    # the per- row index of the shortest edge\n    small = norms.argmin(axis=1)\n\n    # the ordered index for the medium and large edge norm\n    # arranged to reference flattened norms for indexing\n    MLidx = np.column_stack((small + 1, small + 2)) % 3\n    MLidx += (np.arange(len(small)) * 3).reshape((-1, 1))\n\n    # subtract the two largest edge lengths from each other\n    diff = np.subtract(*norms.reshape(-1)[MLidx.T])\n\n    # mark by sign but keep zero values zero\n    order = np.zeros(len(norms), dtype=np.int64)\n    order[diff < tol.merge] = -1\n    order[diff > tol.merge] = 1\n\n    return order", "response": "This function calculates the order of each face in a non - empty tesselation triangle in a mesh."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef convert_to_vertexlist(geometry, **kwargs):\n    if util.is_instance_named(geometry, 'Trimesh'):\n        return mesh_to_vertexlist(geometry, **kwargs)\n    elif util.is_instance_named(geometry, 'Path'):\n        # works for Path3D and Path2D\n        # both of which inherit from Path\n        return path_to_vertexlist(geometry, **kwargs)\n    elif util.is_instance_named(geometry, 'PointCloud'):\n        # pointcloud objects contain colors\n        return points_to_vertexlist(geometry.vertices,\n                                    colors=geometry.colors,\n                                    **kwargs)\n    elif util.is_instance_named(geometry, 'ndarray'):\n        # (n,2) or (n,3) points\n        return points_to_vertexlist(geometry, **kwargs)\n    else:\n        raise ValueError('Geometry passed is not a viewable type!')", "response": "Convert a pyglet geometry object to a list of vertex lists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a Trimesh object to a list of vertex lists.", "response": "def mesh_to_vertexlist(mesh,\n                       group=None,\n                       smooth=True,\n                       smooth_threshold=60000):\n    \"\"\"\n    Convert a Trimesh object to arguments for an\n    indexed vertex list constructor.\n\n    Parameters\n    -------------\n    mesh : trimesh.Trimesh\n      Mesh to be rendered\n    group : str\n      Rendering group for the vertex list\n    smooth : bool\n      Should we try to smooth shade the mesh\n    smooth_threshold : int\n      Maximum number of faces to smooth shade\n\n    Returns\n    --------------\n    args : (7,) tuple\n      Args for vertex list constructor\n    \"\"\"\n\n    if hasattr(mesh.visual, 'uv') and mesh.visual.uv is not None:\n        # if the mesh has texture defined pass it to pyglet\n        vertex_count = len(mesh.vertices)\n        normals = mesh.vertex_normals.reshape(-1).tolist()\n        faces = mesh.faces.reshape(-1).tolist()\n        vertices = mesh.vertices.reshape(-1).tolist()\n\n        # get the per- vertex UV coordinates\n        uv = mesh.visual.uv\n        # if someone passed (n, 3) UVR cut it off here\n        if uv.shape[1] > 2:\n            uv = uv[:, :2]\n        # texcoord as (2,) float\n        color_gl = ('t2f/static',\n                    uv.astype(np.float64).reshape(-1).tolist())\n\n    elif smooth and len(mesh.faces) < smooth_threshold:\n        # if we have a small number of faces and colors defined\n        # smooth the  mesh by merging vertices of faces below\n        # the threshold angle\n        mesh = mesh.smoothed()\n        vertex_count = len(mesh.vertices)\n        normals = mesh.vertex_normals.reshape(-1).tolist()\n        faces = mesh.faces.reshape(-1).tolist()\n        vertices = mesh.vertices.reshape(-1).tolist()\n        color_gl = colors_to_gl(mesh.visual.vertex_colors,\n                                vertex_count)\n    else:\n        # we don't have textures or want to smooth so\n        # send a polygon soup of disconnected triangles to opengl\n        vertex_count = len(mesh.triangles) * 3\n        normals = np.tile(mesh.face_normals,\n                          (1, 3)).reshape(-1).tolist()\n        vertices = mesh.triangles.reshape(-1).tolist()\n        faces = np.arange(vertex_count).tolist()\n        colors = np.tile(mesh.visual.face_colors,\n                         (1, 3)).reshape((-1, 4))\n        color_gl = colors_to_gl(colors, vertex_count)\n\n    # create the ordered tuple for pyglet, use like:\n    # `batch.add_indexed(*args)`\n    args = (vertex_count,    # number of vertices\n            GL_TRIANGLES,    # mode\n            group,           # group\n            faces,           # indices\n            ('v3f/static', vertices),\n            ('n3f/static', normals),\n            color_gl)\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a Path3D object to a tuple of vertex list arguments.", "response": "def path_to_vertexlist(path, group=None, colors=None, **kwargs):\n    \"\"\"\n    Convert a Path3D object to arguments for an\n    indexed vertex list constructor.\n\n    Parameters\n    -------------\n    path : trimesh.path.Path3D object\n      Mesh to be rendered\n    group : str\n      Rendering group for the vertex list\n\n    Returns\n    --------------\n    args : (7,) tuple\n      Args for vertex list constructor\n    \"\"\"\n    # avoid cache check inside tight loop\n    vertices = path.vertices\n\n    # get (n, 2, (2|3)) lines\n    lines = np.vstack([util.stack_lines(e.discrete(vertices))\n                       for e in path.entities])\n    count = len(lines)\n\n    # stack zeros for 2D lines\n    if util.is_shape(vertices, (-1, 2)):\n        lines = lines.reshape((-1, 2))\n        lines = np.column_stack((lines, np.zeros(len(lines))))\n\n    # index for GL is one per point\n    index = np.arange(count).tolist()\n\n    args = (count,    # number of lines\n            GL_LINES,  # mode\n            group,    # group\n            index,    # indices\n            ('v3f/static', lines.reshape(-1)),\n            colors_to_gl(colors, count=count))  # default colors\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a numpy array of 3D points to a vertex list.", "response": "def points_to_vertexlist(points,\n                         colors=None,\n                         group=None,\n                         **kwargs):\n    \"\"\"\n    Convert a numpy array of 3D points to args for\n    a vertex list constructor.\n\n    Parameters\n    -------------\n    points : (n, 3) float\n      Points to be rendered\n    colors : (n, 3) or (n, 4) float\n      Colors for each point\n    group : str\n      Rendering group for the vertex list\n\n    Returns\n    --------------\n    args : (7,) tuple\n      Args for vertex list constructor\n    \"\"\"\n    points = np.asanyarray(points, dtype=np.float64)\n\n    if util.is_shape(points, (-1, 2)):\n        points = np.column_stack((points, np.zeros(len(points))))\n    elif not util.is_shape(points, (-1, 3)):\n        raise ValueError('Pointcloud must be (n,3)!')\n\n    index = np.arange(len(points)).tolist()\n\n    args = (len(points),  # number of vertices\n            GL_POINTS,   # mode\n            group,       # group\n            index,       # indices\n            ('v3f/static', points.reshape(-1)),\n            colors_to_gl(colors, len(points)))\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of colors return a GL - acceptable list of colors", "response": "def colors_to_gl(colors, count):\n    \"\"\"\n    Given a list of colors (or None) return a GL- acceptable list of colors\n\n    Parameters\n    ------------\n    colors: (count, (3 or 4)) float\n      Input colors as an array\n\n    Returns\n    ---------\n    colors_type : str\n      Color type\n    colors_gl : (count,) list\n      Colors to pass to pyglet\n    \"\"\"\n\n    colors = np.asanyarray(colors)\n    count = int(count)\n    if util.is_shape(colors, (count, (3, 4))):\n        # convert the numpy dtype code to an opengl one\n        colors_dtype = {'f': 'f',\n                        'i': 'B',\n                        'u': 'B'}[colors.dtype.kind]\n        # create the data type description string pyglet expects\n        colors_type = 'c' + str(colors.shape[1]) + colors_dtype + '/static'\n        # reshape the 2D array into a 1D one and then convert to a python list\n        colors = colors.reshape(-1).tolist()\n    else:\n        # case where colors are wrong shape, use a default color\n        colors = np.tile([.5, .10, .20], (count, 1)).reshape(-1).tolist()\n        colors_type = 'c3f/static'\n\n    return colors_type, colors"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef material_to_texture(material):\n\n    # try to extract a PIL image from material\n    if hasattr(material, 'image'):\n        img = material.image\n    else:\n        img = material.baseColorTexture\n\n    if img is None:\n        return None\n\n    # use a PNG export to exchange into pyglet\n    # probably a way to do this with a PIL converter\n    with util.BytesIO() as f:\n        # export PIL image as PNG\n        img.save(f, format='png')\n        f.seek(0)\n        # filename used for format guess\n        gl_image = pyglet.image.load(filename='.png', file=f)\n\n    # turn image into pyglet texture\n    texture = gl_image.get_texture()\n\n    return texture", "response": "Convert a trimesh. visual. texture. Material object into a pyglet - compatible texture object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts a numpy row - major homogenous transformation matrixto a flat column - major GLfloat transformation matrix.", "response": "def matrix_to_gl(matrix):\n    \"\"\"\n    Convert a numpy row- major homogenous transformation matrix\n    to a flat column- major GLfloat transformation.\n\n    Parameters\n    -------------\n    matrix : (4,4) float\n      Row- major homogenous transform\n\n    Returns\n    -------------\n    glmatrix : (16,) gl.GLfloat\n      Transform in pyglet format\n    \"\"\"\n    matrix = np.asanyarray(matrix, dtype=np.float64)\n    if matrix.shape != (4, 4):\n        raise ValueError('matrix must be (4,4)!')\n\n    # switch to column major and flatten to (16,)\n    column = matrix.T.flatten()\n    # convert to GLfloat\n    glmatrix = (gl.GLfloat * 16)(*column)\n\n    return glmatrix"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef vector_to_gl(array, *args):\n    array = np.array(array)\n    if len(args) > 0:\n        array = np.append(array, args)\n    vector = (gl.GLfloat * len(array))(*array)\n    return vector", "response": "Convert an array and an optional set of args into a\n    flat vector of gl. GLfloat"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert light to GL objects", "response": "def light_to_gl(light, transform, lightN):\n    \"\"\"\n    Convert trimesh.scene.lighting.Light objects into\n    args for gl.glLightFv calls\n\n    Parameters\n    --------------\n    light : trimesh.scene.lighting.Light\n      Light object to be converted to GL\n    transform : (4, 4) float\n      Transformation matrix of light\n    lightN : int\n      Result of gl.GL_LIGHT0, gl.GL_LIGHT1, etc\n\n    Returns\n    --------------\n    multiarg : [tuple]\n      List of args to pass to gl.glLightFv eg:\n      [gl.glLightfb(*a) for a in multiarg]\n    \"\"\"\n\n    # convert color to opengl\n    gl_color = vector_to_gl(light.color.astype(np.float64) / 255.0)\n    assert len(gl_color) == 4\n\n    # cartesian translation from matrix\n    gl_position = vector_to_gl(transform[:3, 3])\n\n    # create the different position and color arguments\n    args = [(lightN, gl.GL_POSITION, gl_position),\n            (lightN, gl.GL_SPECULAR, gl_color),\n            (lightN, gl.GL_DIFFUSE, gl_color),\n            (lightN, gl.GL_AMBIENT, gl_color)]\n\n    return args"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrecords an initial mouse press at a given point.", "response": "def down(self, point):\n        \"\"\"Record an initial mouse press at a given point.\n\n        Parameters\n        ----------\n        point : (2,) int\n            The x and y pixel coordinates of the mouse press.\n        \"\"\"\n        self._pdown = np.array(point, dtype=np.float32)\n        self._pose = self._n_pose\n        self._target = self._n_target"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef drag(self, point):\n        point = np.array(point, dtype=np.float32)\n        dx, dy = point - self._pdown\n        mindim = 0.3 * np.min(self._size)\n\n        target = self._target\n        x_axis = self._pose[:3, 0].flatten()\n        y_axis = self._pose[:3, 1].flatten()\n        z_axis = self._pose[:3, 2].flatten()\n        eye = self._pose[:3, 3].flatten()\n\n        # Interpret drag as a rotation\n        if self._state == Trackball.STATE_ROTATE:\n            x_angle = -dx / mindim\n            x_rot_mat = transformations.rotation_matrix(\n                x_angle, y_axis, target\n            )\n\n            y_angle = dy / mindim\n            y_rot_mat = transformations.rotation_matrix(\n                y_angle, x_axis, target\n            )\n\n            self._n_pose = y_rot_mat.dot(x_rot_mat.dot(self._pose))\n\n        # Interpret drag as a roll about the camera axis\n        elif self._state == Trackball.STATE_ROLL:\n            center = self._size / 2.0\n            v_init = self._pdown - center\n            v_curr = point - center\n            v_init = v_init / np.linalg.norm(v_init)\n            v_curr = v_curr / np.linalg.norm(v_curr)\n\n            theta = (-np.arctan2(v_curr[1], v_curr[0]) +\n                     np.arctan2(v_init[1], v_init[0]))\n\n            rot_mat = transformations.rotation_matrix(theta, z_axis, target)\n\n            self._n_pose = rot_mat.dot(self._pose)\n\n        # Interpret drag as a camera pan in view plane\n        elif self._state == Trackball.STATE_PAN:\n            dx = -dx / (5.0 * mindim) * self._scale\n            dy = -dy / (5.0 * mindim) * self._scale\n\n            translation = dx * x_axis + dy * y_axis\n            self._n_target = self._target + translation\n            t_tf = np.eye(4)\n            t_tf[:3, 3] = translation\n            self._n_pose = t_tf.dot(self._pose)\n\n        # Interpret drag as a zoom motion\n        elif self._state == Trackball.STATE_ZOOM:\n            radius = np.linalg.norm(eye - target)\n            ratio = 0.0\n            if dy > 0:\n                ratio = np.exp(abs(dy) / (0.5 * self._size[1])) - 1.0\n            elif dy < 0:\n                ratio = 1.0 - np.exp(dy / (0.5 * (self._size[1])))\n            translation = -np.sign(dy) * ratio * radius * z_axis\n            t_tf = np.eye(4)\n            t_tf[:3, 3] = translation\n            self._n_pose = t_tf.dot(self._pose)", "response": "Update the tracball with the current x and y coordinates of the mouse."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef scroll(self, clicks):\n        target = self._target\n        ratio = 0.90\n\n        mult = 1.0\n        if clicks > 0:\n            mult = ratio**clicks\n        elif clicks < 0:\n            mult = (1.0 / ratio)**abs(clicks)\n\n        z_axis = self._n_pose[:3, 2].flatten()\n        eye = self._n_pose[:3, 3].flatten()\n        radius = np.linalg.norm(eye - target)\n        translation = (mult * radius - radius) * z_axis\n        t_tf = np.eye(4)\n        t_tf[:3, 3] = translation\n        self._n_pose = t_tf.dot(self._n_pose)\n\n        z_axis = self._pose[:3, 2].flatten()\n        eye = self._pose[:3, 3].flatten()\n        radius = np.linalg.norm(eye - target)\n        translation = (mult * radius - radius) * z_axis\n        t_tf = np.eye(4)\n        t_tf[:3, 3] = translation\n        self._pose = t_tf.dot(self._pose)", "response": "Zoom using a mouse scroll wheel motion."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrotate the trackball about the Up axis by azimuth radians.", "response": "def rotate(self, azimuth, axis=None):\n        \"\"\"Rotate the trackball about the \"Up\" axis by azimuth radians.\n\n        Parameters\n        ----------\n        azimuth : float\n            The number of radians to rotate.\n        \"\"\"\n        target = self._target\n\n        y_axis = self._n_pose[:3, 1].flatten()\n        if axis is not None:\n            y_axis = axis\n        x_rot_mat = transformations.rotation_matrix(azimuth, y_axis, target)\n        self._n_pose = x_rot_mat.dot(self._n_pose)\n\n        y_axis = self._pose[:3, 1].flatten()\n        if axis is not None:\n            y_axis = axis\n        x_rot_mat = transformations.rotation_matrix(azimuth, y_axis, target)\n        self._pose = x_rot_mat.dot(self._pose)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef symbolic_barycentric(function):\n\n    class evaluator:\n\n        def __init__(self, expr, expr_args):\n            self.lambdified = sp.lambdify(args=expr_args,\n                                          expr=expr,\n                                          modules=['numpy', 'sympy'])\n\n        def __call__(self, mesh, *args):\n            \"\"\"\n            Quickly evaluate the surface integral across a mesh\n\n            Parameters\n            ----------\n            mesh: Trimesh object\n\n            Returns\n            ----------\n            integrated: (len(faces),) float, integral evaluated for each face\n            \"\"\"\n            integrated = self.lambdified(*mesh.triangles.reshape((-1, 9)).T)\n            integrated *= 2 * mesh.area_faces\n            return integrated\n\n    function, symbols = substitute_barycentric(function)\n\n    b1, b2, x1, x2, x3, y1, y2, y3, z1, z2, z3 = symbols\n    # do the first integral for b1\n    integrated_1 = sp.integrate(function, b1)\n    integrated_1 = (integrated_1.subs({b1: 1 - b2}) -\n                    integrated_1.subs({b1: 0}))\n\n    integrated_2 = sp.integrate(integrated_1, b2)\n    integrated_2 = (integrated_2.subs({b2: 1}) -\n                    integrated_2.subs({b2: 0}))\n\n    lambdified = evaluator(expr=integrated_2,\n                           expr_args=[x1, y1, z1, x2, y2, z2, x3, y3, z3])\n\n    return lambdified, integrated_2", "response": "This function is a function that integrates a function across a triangle and returns a barycentric representation of the function."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a list of vectors that look at a set of points in the camera s field of view.", "response": "def look_at(points, fov, rotation=None, distance=None, center=None):\n    \"\"\"\n    Generate transform for a camera to keep a list\n    of points in the camera's field of view.\n\n    Parameters\n    -------------\n    points : (n, 3) float\n      Points in space\n    fov : (2,) float\n      Field of view, in DEGREES\n    rotation : None, or (4, 4) float\n      Rotation matrix for initial rotation\n    center : None, or (3,) float\n      Center of field of view.\n\n    Returns\n    --------------\n    transform : (4, 4) float\n      Transformation matrix with points in view\n    \"\"\"\n\n    if rotation is None:\n        rotation = np.eye(4)\n    else:\n        rotation = np.asanyarray(rotation, dtype=np.float64)\n    points = np.asanyarray(points, dtype=np.float64)\n\n    # Transform points to camera frame (just use the rotation part)\n    rinv = rotation[:3, :3].T\n    points_c = rinv.dot(points.T).T\n\n    if center is None:\n        # Find the center of the points' AABB in camera frame\n        center_c = points_c.min(axis=0) + 0.5 * points_c.ptp(axis=0)\n    else:\n        # Transform center to camera frame\n        center_c = rinv.dot(center)\n\n    # Re-center the points around the camera-frame origin\n    points_c -= center_c\n\n    # Find the minimum distance for the camera from the origin\n    # so that all points fit in the view frustrum\n    tfov = np.tan(np.radians(fov) / 2.0)\n\n    if distance is None:\n        distance = np.max(np.abs(points_c[:, :2]) /\n                          tfov + points_c[:, 2][:, np.newaxis])\n\n    # set the pose translation\n    center_w = rotation[:3, :3].dot(center_c)\n    cam_pose = rotation.copy()\n    cam_pose[:3, 3] = center_w + distance * cam_pose[:3, 2]\n\n    return cam_pose"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef camera_to_rays(camera):\n    # radians of half the field of view\n    half = np.radians(camera.fov / 2.0)\n    # scale it down by two pixels to keep image under resolution\n    half *= (camera.resolution - 2) / camera.resolution\n\n    # create an evenly spaced list of angles\n    angles = util.grid_linspace(bounds=[-half, half],\n                                count=camera.resolution)\n\n    # turn the angles into unit vectors\n    vectors = util.unitize(np.column_stack((\n        np.sin(angles),\n        np.ones(len(angles)))))\n\n    # flip the camera transform to change sign of Z\n    transform = np.dot(\n        camera.transform,\n        align_vectors([1, 0, 0], [-1, 0, 0]))\n\n    # apply the rotation to the direction vectors\n    vectors = transformations.transform_points(\n        vectors,\n        transform,\n        translate=False)\n\n    # camera origin is single point, extract from transform\n    origin = transformations.translation_from_matrix(transform)\n    # tile it into corresponding list of ray vectorsy\n    origins = np.ones_like(vectors) * origin\n\n    return origins, vectors, angles", "response": "Convert a trimesh. scene. Camera object to ray origins vectors and angles. Will return one ray per pixel and the list of rays per pixel."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the camera resolution in pixels.", "response": "def resolution(self, values):\n        \"\"\"\n        Set the camera resolution in pixels.\n\n        Parameters\n        ------------\n        resolution (2,) float\n          Camera resolution in pixels\n        \"\"\"\n        values = np.asanyarray(values, dtype=np.int64)\n        if values.shape != (2,):\n            raise ValueError('resolution must be (2,) float')\n        # assign passed values to focal length\n        self._resolution = values"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scene(self, value):\n\n        # save the scene reference\n        self._scene = value\n\n        # check if we have local not None transform\n        # an if we can apply it to the scene graph\n        # also check here that scene is a real scene\n        if (hasattr(self, '_transform') and\n            self._transform is not None and\n                hasattr(value, 'graph')):\n            # set scene transform to locally saved transform\n            self._scene.graph[self.name] = self._transform\n            # set local transform to None\n            self._transform = None", "response": "Set the reference to the scene that this camera is attached to."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef transform(self):\n        # no scene set\n        if self._scene is None:\n            # no transform saved locally\n            if not hasattr(self, '_transform') or self._transform is None:\n                return np.eye(4)\n            # transform saved locally\n            return self._transform\n\n        # get the transform from the scene\n        return self._scene.graph[self.name][0]", "response": "Get the homogenous transformation from the world frame to the camera object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the focal length in pixels for the camera.", "response": "def focal(self):\n        \"\"\"\n        Get the focal length in pixels for the camera.\n\n        Returns\n        ------------\n        focal : (2,) float\n          Focal length in pixels\n        \"\"\"\n        if self._focal is None:\n            # calculate focal length from FOV\n            focal = [(px / 2.0) / np.tan(np.radians(fov / 2.0))\n                     for px, fov in zip(self._resolution, self.fov)]\n            # store as correct dtype\n            self._focal = np.asanyarray(focal, dtype=np.float64)\n\n        return self._focal"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the intrinsic matrix for the camera object.", "response": "def K(self):\n        \"\"\"\n        Get the intrinsic matrix for the Camera object.\n\n        Returns\n        -----------\n        K : (3, 3) float\n          Intrinsic matrix for camera\n        \"\"\"\n        K = np.eye(3, dtype=np.float64)\n        K[0, 0] = self.focal[0]\n        K[1, 1] = self.focal[1]\n        K[0, 2] = self.resolution[0] / 2.0\n        K[1, 2] = self.resolution[1] / 2.0\n        return K"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the field of view in degrees.", "response": "def fov(self):\n        \"\"\"\n        Get the field of view in degrees.\n\n        Returns\n        -------------\n        fov : (2,) float\n          XY field of view in degrees\n        \"\"\"\n        if self._fov is None:\n            fov = [2.0 * np.degrees(np.arctan((px / 2.0) / f))\n                   for px, f in zip(self._resolution, self._focal)]\n            fov = np.asanyarray(fov, dtype=np.float64)\n            self._fov = fov\n        return self._fov"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fov(self, values):\n        if values is None:\n            self._fov = None\n        else:\n            values = np.asanyarray(values, dtype=np.float64)\n            if values.shape != (2,):\n                raise ValueError('focal length must be (2,) int')\n            # assign passed values to FOV\n            self._fov = values\n            # fov overrides focal\n            self._focal = None", "response": "Sets the field of view in degrees."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sinwave(scene):\n    # create an empty homogenous transformation\n    matrix = np.eye(4)\n    # set Y as cos of time\n    matrix[1][3] = np.cos(time.time()) * 2\n    # set Z as sin of time\n    matrix[2][3] = np.sin(time.time()) * 3\n\n    # take one of the two spheres arbitrarily\n    node = s.graph.nodes_geometry[0]\n    # apply the transform to the node\n    scene.graph.update(node, matrix=matrix)", "response": "This callback is used to update the sinwave transform of a scene viewer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_gmsh(file_name, gmsh_args=None):\n    # use STL as an intermediate format\n    from ..exchange.stl import load_stl\n\n    # start with default args for the meshing step\n    # Mesh.Algorithm=2 MeshAdapt/Delaunay, there are others but they may include quads\n    # With this planes are meshed using Delaunay and cylinders are meshed using MeshAdapt\n    args = [(\"Mesh.Algorithm\", 2),\n            (\"Mesh.CharacteristicLengthFromCurvature\", 1),\n            (\"Mesh.MinimumCirclePoints\", 32)]\n    # add passed argument tuples last so we can override defaults\n    if gmsh_args is not None:\n        args.extend(gmsh_args)\n\n    # formats GMSH can load\n    supported = ['.brep', '.stp', '.step', '.igs', '.iges',\n                 '.bdf', '.msh', '.inp', '.diff', '.mesh']\n\n    # check extensions to make sure it is supported format\n    if file_name is not None:\n        if not any(file_name.lower().endswith(e)\n                   for e in supported):\n            raise ValueError(\n                'Supported formats are: BREP (.brep), STEP (.stp or .step), ' +\n                'IGES (.igs or .iges), Nastran (.bdf), Gmsh (.msh), Abaqus (*.inp), ' +\n                'Diffpack (*.diff), Inria Medit (*.mesh)')\n    else:\n        raise ValueError('No import since no file was provided!')\n\n    # if we initialize with sys.argv it could be anything\n    gmsh.initialize()\n    gmsh.option.setNumber(\"General.Terminal\", 1)\n    gmsh.model.add('Surface_Mesh_Generation')\n    gmsh.open(file_name)\n\n    # create a temporary file for the results\n    out_data = tempfile.NamedTemporaryFile(suffix='.stl', delete=False)\n    # windows gets mad if two processes try to open the same file\n    out_data.close()\n\n    # we have to mesh the surface as these are analytic BREP formats\n    if any(file_name.lower().endswith(e)\n           for e in ['.brep', '.stp', '.step', '.igs', '.iges']):\n        gmsh.model.geo.synchronize()\n        # loop through our numbered args which do things, stuff\n        for arg in args:\n            gmsh.option.setNumber(*arg)\n        # generate the mesh\n        gmsh.model.mesh.generate(2)\n        # write to the temporary file\n        gmsh.write(out_data.name)\n    else:\n        gmsh.plugin.run(\"NewView\")\n        gmsh.plugin.run(\"Skin\")\n        gmsh.view.write(1, out_data.name)\n\n    # load the data from the temporary outfile\n    with open(out_data.name, 'rb') as f:\n        kwargs = load_stl(f)\n\n    return kwargs", "response": "Loads a 3D volume mesh from a GMSH file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a mesh to a 3D volume mesh.", "response": "def to_volume(mesh,\n              file_name=None,\n              max_element=None,\n              mesher_id=1):\n    \"\"\"\n    Convert a surface mesh to a 3D volume mesh generated by gmsh.\n\n    An easy way to install the gmsh sdk is through the gmsh-sdk\n    package on pypi, which downloads and sets up gmsh:\n        pip install gmsh-sdk\n\n    Algorithm details, although check gmsh docs for more information:\n    The \"Delaunay\" algorithm is split into three separate steps.\n    First, an initial mesh of the union of all the volumes in the model is performed,\n    without inserting points in the volume. The surface mesh is then recovered using H.\n    Si's boundary recovery algorithm Tetgen/BR. Then a three-dimensional version of the\n    2D Delaunay algorithm described above is applied to insert points in the volume to\n    respect the mesh size constraints.\n\n    The Frontal\" algorithm uses J. Schoeberl's Netgen algorithm.\n    The \"HXT\" algorithm is a new efficient and parallel reimplementaton\n    of the Delaunay algorithm.\n    The \"MMG3D\" algorithm (experimental) allows to generate\n    anisotropic tetrahedralizations\n\n\n    Parameters\n    --------------\n    mesh : trimesh.Trimesh\n      Surface mesh of input geometry\n    file_name : str or None\n      Location to save output, in .msh (gmsh) or .bdf (Nastran) format\n    max_element : float or None\n      Maximum length of an element in the volume mesh\n    mesher_id : int\n      3D unstructured algorithms:\n      1: Delaunay, 4: Frontal, 7: MMG3D, 10: HXT\n\n    Returns\n    ------------\n    data : None or bytes\n      MSH data, only returned if file_name is None\n\n    \"\"\"\n\n    # checks mesher selection\n    if mesher_id not in [1, 4, 7, 10]:\n        raise ValueError('unavilable mesher selected!')\n    else:\n        mesher_id = int(mesher_id)\n\n    # set max element length to a best guess if not specified\n    if max_element is None:\n        max_element = np.sqrt(np.mean(mesh.area_faces))\n\n    if file_name is not None:\n        # check extensions to make sure it is supported format\n        if not any(file_name.lower().endswith(e)\n                   for e in ['.bdf', '.msh', '.inp', '.diff', '.mesh']):\n            raise ValueError(\n                'Only Nastran (.bdf), Gmsh (.msh), Abaqus (*.inp), ' +\n                'Diffpack (*.diff) and Inria Medit (*.mesh) formats ' +\n                'are available!')\n\n    # exports to disk for gmsh to read using a temp file\n    mesh_file = tempfile.NamedTemporaryFile(suffix='.stl', delete=False)\n    mesh_file.close()\n    mesh.export(mesh_file.name)\n\n    # starts Gmsh Python API script\n    gmsh.initialize()\n    gmsh.option.setNumber(\"General.Terminal\", 1)\n    gmsh.model.add('Nastran_stl')\n\n    gmsh.merge(mesh_file.name)\n    dimtag = gmsh.model.getEntities()[0]\n    dim = dimtag[0]\n    tag = dimtag[1]\n\n    surf_loop = gmsh.model.geo.addSurfaceLoop([tag])\n    gmsh.model.geo.addVolume([surf_loop])\n    gmsh.model.geo.synchronize()\n\n    # We can then generate a 3D mesh...\n    gmsh.option.setNumber(\"Mesh.Algorithm3D\", mesher_id)\n    gmsh.option.setNumber(\"Mesh.CharacteristicLengthMax\", max_element)\n    gmsh.model.mesh.generate(3)\n\n    dimtag2 = gmsh.model.getEntities()[1]\n    dim2 = dimtag2[0]\n    tag2 = dimtag2[1]\n    p2 = gmsh.model.addPhysicalGroup(dim2, [tag2])\n    gmsh.model.setPhysicalName(dim, p2, 'Nastran_bdf')\n\n    data = None\n    # if file name is None, return msh data using a tempfile\n    if file_name is None:\n        out_data = tempfile.NamedTemporaryFile(suffix='.msh', delete=False)\n        # windows gets mad if two processes try to open the same file\n        out_data.close()\n        gmsh.write(out_data.name)\n        with open(out_data.name, 'rb') as f:\n            data = f.read()\n    else:\n        gmsh.write(file_name)\n\n    # close up shop\n    gmsh.finalize()\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a Trimesh object into an URDF file.", "response": "def export_urdf(mesh,\n                directory,\n                scale=1.0,\n                color=[0.75, 0.75, 0.75],\n                **kwargs):\n    \"\"\"\n    Convert a Trimesh object into a URDF package for physics simulation.\n    This breaks the mesh into convex pieces and writes them to the same\n    directory as the .urdf file.\n\n    Parameters\n    ---------\n    mesh      : Trimesh object\n    directory : str\n                  The directory path for the URDF package\n\n    Returns\n    ---------\n    mesh : Trimesh object\n             Multi-body mesh containing convex decomposition\n    \"\"\"\n\n    import lxml.etree as et\n    # TODO: fix circular import\n    from .export import export_mesh\n    # Extract the save directory and the file name\n    fullpath = os.path.abspath(directory)\n    name = os.path.basename(fullpath)\n    _, ext = os.path.splitext(name)\n\n    if ext != '':\n        raise ValueError('URDF path must be a directory!')\n\n    # Create directory if needed\n    if not os.path.exists(fullpath):\n        os.mkdir(fullpath)\n    elif not os.path.isdir(fullpath):\n        raise ValueError('URDF path must be a directory!')\n\n    # Perform a convex decomposition\n    try:\n        convex_pieces = convex_decomposition(mesh, **kwargs)\n        if not isinstance(convex_pieces, list):\n            convex_pieces = [convex_pieces]\n    except BaseException:\n        log.error('problem with convex decomposition, using hull',\n                  exc_info=True)\n        convex_pieces = [mesh.convex_hull]\n\n    # Get the effective density of the mesh\n    effective_density = mesh.volume / sum([\n        m.volume for m in convex_pieces])\n\n    # open an XML tree\n    root = et.Element('robot', name='root')\n\n    # Loop through all pieces, adding each as a link\n    prev_link_name = None\n    for i, piece in enumerate(convex_pieces):\n\n        # Save each nearly convex mesh out to a file\n        piece_name = '{}_convex_piece_{}'.format(name, i)\n        piece_filename = '{}.obj'.format(piece_name)\n        piece_filepath = os.path.join(fullpath, piece_filename)\n        export_mesh(piece, piece_filepath)\n\n        # Set the mass properties of the piece\n        piece.center_mass = mesh.center_mass\n        piece.density = effective_density * mesh.density\n\n        link_name = 'link_{}'.format(piece_name)\n        geom_name = '{}'.format(piece_filename)\n        I = [['{:.2E}'.format(y) for y in x]  # NOQA\n             for x in piece.moment_inertia]\n\n        # Write the link out to the XML Tree\n        link = et.SubElement(root, 'link', name=link_name)\n\n        # Inertial information\n        inertial = et.SubElement(link, 'inertial')\n        et.SubElement(inertial, 'origin', xyz=\"0 0 0\", rpy=\"0 0 0\")\n        et.SubElement(inertial, 'mass', value='{:.2E}'.format(piece.mass))\n        et.SubElement(\n            inertial,\n            'inertia',\n            ixx=I[0][0],\n            ixy=I[0][1],\n            ixz=I[0][2],\n            iyy=I[1][1],\n            iyz=I[1][2],\n            izz=I[2][2])\n        # Visual Information\n        visual = et.SubElement(link, 'visual')\n        et.SubElement(visual, 'origin', xyz=\"0 0 0\", rpy=\"0 0 0\")\n        geometry = et.SubElement(visual, 'geometry')\n        et.SubElement(geometry, 'mesh', filename=geom_name,\n                      scale=\"{:.4E} {:.4E} {:.4E}\".format(scale,\n                                                          scale,\n                                                          scale))\n        material = et.SubElement(visual, 'material', name='')\n        et.SubElement(material,\n                      'color',\n                      rgba=\"{:.2E} {:.2E} {:.2E} 1\".format(color[0],\n                                                           color[1],\n                                                           color[2]))\n\n        # Collision Information\n        collision = et.SubElement(link, 'collision')\n        et.SubElement(collision, 'origin', xyz=\"0 0 0\", rpy=\"0 0 0\")\n        geometry = et.SubElement(collision, 'geometry')\n        et.SubElement(geometry, 'mesh', filename=geom_name,\n                      scale=\"{:.4E} {:.4E} {:.4E}\".format(scale,\n                                                          scale,\n                                                          scale))\n\n        # Create rigid joint to previous link\n        if prev_link_name is not None:\n            joint_name = '{}_joint'.format(link_name)\n            joint = et.SubElement(root,\n                                  'joint',\n                                  name=joint_name,\n                                  type='fixed')\n            et.SubElement(joint, 'origin', xyz=\"0 0 0\", rpy=\"0 0 0\")\n            et.SubElement(joint, 'parent', link=prev_link_name)\n            et.SubElement(joint, 'child', link=link_name)\n\n        prev_link_name = link_name\n\n    # Write URDF file\n    tree = et.ElementTree(root)\n    urdf_filename = '{}.urdf'.format(name)\n    tree.write(os.path.join(fullpath, urdf_filename),\n               pretty_print=True)\n\n    # Write Gazebo config file\n    root = et.Element('model')\n    model = et.SubElement(root, 'name')\n    model.text = name\n    version = et.SubElement(root, 'version')\n    version.text = '1.0'\n    sdf = et.SubElement(root, 'sdf', version='1.4')\n    sdf.text = '{}.urdf'.format(name)\n\n    author = et.SubElement(root, 'author')\n    et.SubElement(author, 'name').text = 'trimesh {}'.format(trimesh_version)\n    et.SubElement(author, 'email').text = 'blank@blank.blank'\n\n    description = et.SubElement(root, 'description')\n    description.text = name\n\n    tree = et.ElementTree(root)\n    tree.write(os.path.join(fullpath, 'model.config'))\n\n    return np.sum(convex_pieces)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef voxelize_subdivide(mesh,\n                       pitch,\n                       max_iter=10,\n                       edge_factor=2.0):\n    \"\"\"\n    Voxelize a surface by subdividing a mesh until every edge is\n    shorter than: (pitch / edge_factor)\n\n    Parameters\n    -----------\n    mesh:        Trimesh object\n    pitch:       float, side length of a single voxel cube\n    max_iter:    int, cap maximum subdivisions or None for no limit.\n    edge_factor: float,\n\n    Returns\n    -----------\n    voxels_sparse:   (n,3) int, (m,n,p) indexes of filled cells\n    origin_position: (3,) float, position of the voxel\n                                 grid origin in space\n    \"\"\"\n    max_edge = pitch / edge_factor\n\n    if max_iter is None:\n        longest_edge = np.linalg.norm(mesh.vertices[mesh.edges[:, 0]] -\n                                      mesh.vertices[mesh.edges[:, 1]],\n                                      axis=1).max()\n        max_iter = max(int(np.ceil(np.log2(longest_edge / max_edge))), 0)\n\n    # get the same mesh sudivided so every edge is shorter\n    # than a factor of our pitch\n    v, f = remesh.subdivide_to_size(mesh.vertices,\n                                    mesh.faces,\n                                    max_edge=max_edge,\n                                    max_iter=max_iter)\n\n    # convert the vertices to their voxel grid position\n    hit = v / pitch\n\n    # Provided edge_factor > 1 and max_iter is large enough, this is\n    # sufficient to preserve 6-connectivity at the level of voxels.\n    hit = np.round(hit).astype(int)\n\n    # remove duplicates\n    unique, inverse = grouping.unique_rows(hit)\n\n    # get the voxel centers in model space\n    occupied_index = hit[unique]\n\n    origin_index = occupied_index.min(axis=0)\n    origin_position = origin_index * pitch\n\n    voxels_sparse = (occupied_index - origin_index)\n\n    return voxels_sparse, origin_position", "response": "Voxelize a surface by subdividing every edge of every voxel in the mesh until every edge is shorter than the pitch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef local_voxelize(mesh, point, pitch, radius, fill=True, **kwargs):\n    from scipy import ndimage\n\n    # make sure point is correct type/shape\n    point = np.asanyarray(point, dtype=np.float64).reshape(3)\n    # this is a gotcha- radius sounds a lot like it should be in\n    # float model space, not int voxel space so check\n    if not isinstance(radius, int):\n        raise ValueError('radius needs to be an integer number of cubes!')\n\n    # Bounds of region\n    bounds = np.concatenate((point - (radius + 0.5) * pitch,\n                             point + (radius + 0.5) * pitch))\n\n    # faces that intersect axis aligned bounding box\n    faces = list(mesh.triangles_tree.intersection(bounds))\n\n    # didn't hit anything so exit\n    if len(faces) == 0:\n        return np.array([], dtype=np.bool), np.zeros(3)\n\n    local = mesh.submesh([[f] for f in faces], append=True)\n\n    # Translate mesh so point is at 0,0,0\n    local.apply_translation(-point)\n\n    sparse, origin = voxelize_subdivide(local, pitch, **kwargs)\n    matrix = sparse_to_matrix(sparse)\n\n    # Find voxel index for point\n    center = np.round(-origin / pitch).astype(np.int64)\n\n    # pad matrix if necessary\n    prepad = np.maximum(radius - center, 0)\n    postpad = np.maximum(center + radius + 1 - matrix.shape, 0)\n\n    matrix = np.pad(matrix, np.stack((prepad, postpad), axis=-1),\n                    mode='constant')\n    center += prepad\n\n    # Extract voxels within the bounding box\n    voxels = matrix[center[0] - radius:center[0] + radius + 1,\n                    center[1] - radius:center[1] + radius + 1,\n                    center[2] - radius:center[2] + radius + 1]\n    local_origin = point - radius * pitch  # origin of local voxels\n\n    # Fill internal regions\n    if fill:\n        regions, n = ndimage.measurements.label(~voxels)\n        distance = ndimage.morphology.distance_transform_cdt(~voxels)\n        representatives = [np.unravel_index((distance * (regions == i)).argmax(),\n                                            distance.shape) for i in range(1, n + 1)]\n        contains = mesh.contains(\n            np.asarray(representatives) *\n            pitch +\n            local_origin)\n\n        where = np.where(contains)[0] + 1\n        # use in1d vs isin for older numpy versions\n        internal = np.in1d(regions.flatten(), where).reshape(regions.shape)\n\n        voxels = np.logical_or(voxels, internal)\n\n    return voxels, local_origin", "response": "Returns a non - watertight voxelized version of a mesh in the local space."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef voxelize_ray(mesh,\n                 pitch,\n                 per_cell=[2, 2],\n                 **kwargs):\n    \"\"\"\n    Voxelize a mesh using ray queries.\n\n    Parameters\n    -------------\n    mesh     : Trimesh object\n                 Mesh to be voxelized\n    pitch    : float\n                 Length of voxel cube\n    per_cell : (2,) int\n                 How many ray queries to make per cell\n\n    Returns\n    -------------\n    voxels : (n, 3) int\n                 Voxel positions\n    origin : (3, ) int\n                 Origin of voxels\n    \"\"\"\n    # how many rays per cell\n    per_cell = np.array(per_cell).astype(np.int).reshape(2)\n    # edge length of cube voxels\n    pitch = float(pitch)\n\n    # create the ray origins in a grid\n    bounds = mesh.bounds[:, :2].copy()\n    # offset start so we get the requested number per cell\n    bounds[0] += pitch / (1.0 + per_cell)\n    # offset end so arange doesn't short us\n    bounds[1] += pitch\n    # on X we are doing multiple rays per voxel step\n    step = pitch / per_cell\n    # 2D grid\n    ray_ori = util.grid_arange(bounds, step=step)\n    # a Z position below the mesh\n    z = np.ones(len(ray_ori)) * (mesh.bounds[0][2] - pitch)\n    ray_ori = np.column_stack((ray_ori, z))\n    # all rays are along positive Z\n    ray_dir = np.ones_like(ray_ori) * [0, 0, 1]\n\n    # if you have pyembree this should be decently fast\n    hits = mesh.ray.intersects_location(ray_ori, ray_dir)[0]\n\n    # just convert hit locations to integer positions\n    voxels = np.round(hits / pitch).astype(np.int64)\n\n    # offset voxels by min, so matrix isn't huge\n    origin = voxels.min(axis=0)\n    voxels -= origin\n\n    return voxels, origin", "response": "Voxelize a Trimesh object using ray queries."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fill_voxelization(occupied):\n    # validate inputs\n    occupied = np.asanyarray(occupied, dtype=np.int64)\n    if not util.is_shape(occupied, (-1, 3)):\n        raise ValueError('incorrect shape')\n\n    # create grid and mark inner voxels\n    max_value = occupied.max() + 3\n\n    grid = np.zeros((max_value,\n                     max_value,\n                     max_value),\n                    dtype=np.int64)\n    voxels_sparse = np.add(occupied, 1)\n\n    grid.__setitem__(tuple(voxels_sparse.T), 1)\n\n    for i in range(max_value):\n        check_dir2 = False\n        for j in range(0, max_value - 1):\n            idx = []\n            # find transitions first\n            # transition positions are from 0 to 1 and from 1 to 0\n            eq = np.equal(grid[i, j, :-1], grid[i, j, 1:])\n            idx = np.where(np.logical_not(eq))[0] + 1\n            c = len(idx)\n            check_dir2 = (c % 4) > 0 and c > 4\n            if c < 4:\n                continue\n            for s in range(0, c - c % 4, 4):\n                grid[i, j, idx[s]:idx[s + 3]] = 1\n        if not check_dir2:\n            continue\n\n        # check another direction for robustness\n        for k in range(0, max_value - 1):\n            idx = []\n            # find transitions first\n            eq = np.equal(grid[i, :-1, k], grid[i, 1:, k])\n            idx = np.where(np.logical_not(eq))[0] + 1\n            c = len(idx)\n            if c < 4:\n                continue\n            for s in range(0, c - c % 4, 4):\n                grid[i, idx[s]:idx[s + 3], k] = 1\n\n    # generate new voxels\n    idx = np.where(grid == 1)\n    filled = np.array([[idx[0][i] - 1,\n                        idx[1][i] - 1,\n                        idx[2][i] - 1]\n                       for i in range(len(idx[0]))])\n\n    return filled", "response": "Given a sparse surface voxelization fill in between columns."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts center points of an anatomy matrix into its indices.", "response": "def points_to_indices(points, pitch, origin):\n    \"\"\"\n    Convert center points of an (n,m,p) matrix into its indices.\n\n    Parameters\n    ----------\n    points: (q, 3) float, center points of voxel matrix (n,m,p)\n    pitch: float, what pitch was the voxel matrix computed with\n    origin: (3,) float, what is the origin of the voxel matrix\n\n    Returns\n    ----------\n    indices: (q, 3) int, list of indices\n    \"\"\"\n    points = np.asanyarray(points, dtype=np.float64)\n    origin = np.asanyarray(origin, dtype=np.float64)\n    pitch = float(pitch)\n\n    if points.shape != (points.shape[0], 3):\n        raise ValueError('shape of points must be (q, 3)')\n\n    if origin.shape != (3,):\n        raise ValueError('shape of origin must be (3,)')\n\n    indices = np.round((points - origin) / pitch).astype(int)\n    return indices"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert indices of an array of voxel center points into a list of points.", "response": "def indices_to_points(indices, pitch, origin):\n    \"\"\"\n    Convert indices of an (n,m,p) matrix into a set of voxel center points.\n\n    Parameters\n    ----------\n    indices: (q, 3) int, index of voxel matrix (n,m,p)\n    pitch: float, what pitch was the voxel matrix computed with\n    origin: (3,) float, what is the origin of the voxel matrix\n\n    Returns\n    ----------\n    points: (q, 3) float, list of points\n    \"\"\"\n    indices = np.asanyarray(indices, dtype=np.float64)\n    origin = np.asanyarray(origin, dtype=np.float64)\n    pitch = float(pitch)\n\n    if indices.shape != (indices.shape[0], 3):\n        from IPython import embed\n        embed()\n        raise ValueError('shape of indices must be (q, 3)')\n\n    if origin.shape != (3,):\n        raise ValueError('shape of origin must be (3,)')\n\n    points = indices * pitch + origin\n    return points"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef matrix_to_points(matrix, pitch, origin):\n    indices = np.column_stack(np.nonzero(matrix))\n    points = indices_to_points(indices=indices,\n                               pitch=pitch,\n                               origin=origin)\n    return points", "response": "Convert a matrix into a list of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef matrix_to_marching_cubes(matrix, pitch, origin):\n    from skimage import measure\n    from .base import Trimesh\n\n    matrix = np.asanyarray(matrix, dtype=np.bool)\n\n    rev_matrix = np.logical_not(matrix)  # Takes set about 0.\n    # Add in padding so marching cubes can function properly with\n    # voxels on edge of AABB\n    pad_width = 1\n    rev_matrix = np.pad(rev_matrix,\n                        pad_width=(pad_width),\n                        mode='constant',\n                        constant_values=(1))\n\n    # pick between old and new API\n    if hasattr(measure, 'marching_cubes_lewiner'):\n        func = measure.marching_cubes_lewiner\n    else:\n        func = measure.marching_cubes\n\n    # Run marching cubes.\n    meshed = func(volume=rev_matrix,\n                  level=.5,  # it is a boolean voxel grid\n                  spacing=(pitch,\n                           pitch,\n                           pitch))\n\n    # allow results from either marching cubes function in skimage\n    # binaries available for python 3.3 and 3.4 appear to use the classic\n    # method\n    if len(meshed) == 2:\n        log.warning('using old marching cubes, may not be watertight!')\n        vertices, faces = meshed\n        normals = None\n    elif len(meshed) == 4:\n        vertices, faces, normals, vals = meshed\n\n    # Return to the origin, add in the pad_width\n    vertices = np.subtract(np.add(vertices, origin), pad_width * pitch)\n    # create the mesh\n    mesh = Trimesh(vertices=vertices,\n                   faces=faces,\n                   vertex_normals=normals)\n    return mesh", "response": "Convert a matrix into a mesh using marching cubes algorithm."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sparse_to_matrix(sparse):\n\n    sparse = np.asanyarray(sparse, dtype=np.int)\n    if not util.is_shape(sparse, (-1, 3)):\n        raise ValueError('sparse must be (n,3)!')\n\n    shape = sparse.max(axis=0) + 1\n    matrix = np.zeros(np.product(shape), dtype=np.bool)\n    multiplier = np.array([np.product(shape[1:]), shape[2], 1])\n\n    index = (sparse * multiplier).sum(axis=1)\n    matrix[index] = True\n\n    dense = matrix.reshape(shape)\n    return dense", "response": "Takes a sparse list of integer indexes of filled cells and turns it into a dense matrix."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef multibox(centers, pitch, colors=None):\n    from . import primitives\n    from .base import Trimesh\n\n    b = primitives.Box(extents=[pitch, pitch, pitch])\n\n    v = np.tile(centers, (1, len(b.vertices))).reshape((-1, 3))\n    v += np.tile(b.vertices, (len(centers), 1))\n\n    f = np.tile(b.faces, (len(centers), 1))\n    f += np.tile(np.arange(len(centers)) * len(b.vertices),\n                 (len(b.faces), 1)).T.reshape((-1, 1))\n\n    face_colors = None\n    if colors is not None:\n        colors = np.asarray(colors)\n        if colors.ndim == 1:\n            colors = colors[None].repeat(len(centers), axis=0)\n        if colors.ndim == 2 and len(colors) == len(centers):\n            face_colors = colors.repeat(12, axis=0)\n\n    mesh = Trimesh(vertices=v,\n                   faces=f,\n                   face_colors=face_colors)\n\n    return mesh", "response": "Returns a Trimesh object representing a multibox."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds common rows between two arrays very quickly using 3D boolean sparse matrices.", "response": "def boolean_sparse(a, b, operation=np.logical_and):\n    \"\"\"\n    Find common rows between two arrays very quickly\n    using 3D boolean sparse matrices.\n\n    Parameters\n    -----------\n    a: (n, d)  int, coordinates in space\n    b: (m, d)  int, coordinates in space\n    operation: numpy operation function, ie:\n                  np.logical_and\n                  np.logical_or\n\n    Returns\n    -----------\n    coords: (q, d) int, coordinates in space\n    \"\"\"\n    # 3D sparse arrays, using wrapped scipy.sparse\n    # pip install sparse\n    import sparse\n\n    # find the bounding box of both arrays\n    extrema = np.array([a.min(axis=0),\n                        a.max(axis=0),\n                        b.min(axis=0),\n                        b.max(axis=0)])\n    origin = extrema.min(axis=0) - 1\n    size = tuple(extrema.ptp(axis=0) + 2)\n\n    # put nearby voxel arrays into same shape sparse array\n    sp_a = sparse.COO((a - origin).T,\n                      data=np.ones(len(a), dtype=np.bool),\n                      shape=size)\n    sp_b = sparse.COO((b - origin).T,\n                      data=np.ones(len(b), dtype=np.bool),\n                      shape=size)\n\n    # apply the logical operation\n    # get a sparse matrix out\n    applied = operation(sp_a, sp_b)\n    # reconstruct the original coordinates\n    coords = np.column_stack(applied.coords) + origin\n\n    return coords"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the marching cubes representation of the voxels.", "response": "def marching_cubes(self):\n        \"\"\"\n        A marching cubes Trimesh representation of the voxels.\n\n        No effort was made to clean or smooth the result in any way;\n        it is merely the result of applying the scikit-image\n        measure.marching_cubes function to self.matrix.\n\n        Returns\n        ---------\n        meshed: Trimesh object representing the current voxel\n                        object, as returned by marching cubes algorithm.\n        \"\"\"\n        meshed = matrix_to_marching_cubes(matrix=self.matrix,\n                                          pitch=self.pitch,\n                                          origin=self.origin)\n        return meshed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef points(self):\n        points = matrix_to_points(matrix=self.matrix,\n                                  pitch=self.pitch,\n                                  origin=self.origin)\n        return points", "response": "Returns a list of points in the center of each filled cell as a list of points."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef point_to_index(self, point):\n        indices = points_to_indices(points=[point],\n                                    pitch=self.pitch,\n                                    origin=self.origin)\n        index = tuple(indices[0])\n        return index", "response": "Convert a point in the matrix array to an index in the matrix array."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_filled(self, point):\n        index = self.point_to_index(point)\n        in_range = (np.array(index) < np.array(self.shape)).all()\n        if in_range:\n            is_filled = self.matrix[index]\n        else:\n            is_filled = False\n        return is_filled", "response": "Query a point to see if the voxel cell it lies in is filled or not."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef as_boxes(self, colors=None):\n        matrix = self._data['matrix']\n        centers = matrix_to_points(\n            matrix=matrix,\n            pitch=self._data['pitch'],\n            origin=self._data['origin'])\n\n        if colors is not None:\n            colors = np.asanyarray(colors)\n            if (colors.ndim == 4 and\n                colors.shape[:3] == matrix.shape and\n                    colors.shape[3] in [3, 4]):\n                colors = colors[matrix > 0]\n            elif not (colors.shape == (3,) or colors.shape == (4,)):\n                log.warning('colors incorrect shape!')\n                colors = None\n\n        mesh = multibox(centers=centers,\n                        pitch=self.pitch,\n                        colors=colors)\n        return mesh", "response": "Returns a Trimesh representation of the voxels with a box per filled voxel."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sparse_surface(self):\n        if self._method == 'ray':\n            func = voxelize_ray\n        elif self._method == 'subdivide':\n            func = voxelize_subdivide\n        else:\n            raise ValueError('voxelization method incorrect')\n\n        voxels, origin = func(\n            mesh=self._data['mesh'],\n            pitch=self._data['pitch'],\n            max_iter=self._data['max_iter'][0])\n        self._cache['origin'] = origin\n\n        return voxels", "response": "Returns the set of voxels on the surface of the mesh."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a Trimesh object representation of the voxels with a box containing each filled voxel.", "response": "def as_boxes(self, solid=False):\n        \"\"\"\n        A rough Trimesh representation of the voxels with a box\n        for each filled voxel.\n\n        Parameters\n        -----------\n        solid: bool, if True return boxes for sparse_solid\n\n        Returns\n        ---------\n        mesh: Trimesh object made up of one box per filled cell.\n        \"\"\"\n        if solid:\n            filled = self.sparse_solid\n        else:\n            filled = self.sparse_surface\n        # center points of voxels\n        centers = indices_to_points(indices=filled,\n                                    pitch=self.pitch,\n                                    origin=self.origin)\n        mesh = multibox(centers=centers, pitch=self.pitch)\n        return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a mesh that is a rectangular solid with noise with random transform", "response": "def simulated_brick(face_count, extents, noise, max_iter=10):\n    \"\"\"\n    Produce a mesh that is a rectangular solid with noise\n    with a random transform.\n\n    Parameters\n    -------------\n    face_count : int\n      Approximate number of faces desired\n    extents : (n,3) float\n      Dimensions of brick\n    noise : float\n      Magnitude of vertex noise to apply\n    \"\"\"\n\n    # create the mesh as a simple box\n    mesh = trimesh.creation.box(extents=extents)\n\n    # add some systematic error pre- tesselation\n    mesh.vertices[0] += mesh.vertex_normals[0] + (noise * 2)\n\n    # subdivide until we have more faces than we want\n    for i in range(max_iter):\n        if len(mesh.vertices) > face_count:\n            break\n        mesh = mesh.subdivide()\n\n    # apply tesselation and random noise\n    mesh = mesh.permutate.noise(noise)\n\n    # randomly rotation with translation\n    transform = trimesh.transformations.random_rotation_matrix()\n    transform[:3, 3] = (np.random.random(3) - .5) * 1000\n\n    mesh.apply_transform(transform)\n\n    return mesh"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unitize(vectors,\n            check_valid=False,\n            threshold=None):\n    \"\"\"\n    Unitize a vector or an array or row- vectors.\n\n    Parameters\n    ---------\n    vectors : (n,m) or (j) float\n       Vector or vectors to be unitized\n    check_valid :  bool\n       If set, will return mask of nonzero vectors\n    threshold : float\n       Cutoff for a value to be considered zero.\n\n    Returns\n    ---------\n    unit :  (n,m) or (j) float\n       Input vectors but unitized\n    valid : (n,) bool or bool\n        Mask of nonzero vectors returned if `check_valid`\n    \"\"\"\n    # make sure we have a numpy array\n    vectors = np.asanyarray(vectors)\n\n    # allow user to set zero threshold\n    if threshold is None:\n        threshold = TOL_ZERO\n\n    if len(vectors.shape) == 2:\n        # for (m, d) arrays take the per- row unit vector\n        # using sqrt and avoiding exponents is slightly faster\n        # also dot with ones is faser than .sum(axis=1)\n        norm = np.sqrt(np.dot(vectors * vectors,\n                              [1.0] * vectors.shape[1]))\n        # non-zero norms\n        valid = norm > threshold\n        # in-place reciprocal of nonzero norms\n        norm[valid] **= -1\n        # tile reciprocal of norm\n        tiled = np.tile(norm, (vectors.shape[1], 1)).T\n        # multiply by reciprocal of norm\n        unit = vectors * tiled\n    elif len(vectors.shape) == 1:\n        # treat 1D arrays as a single vector\n        norm = np.sqrt((vectors * vectors).sum())\n        valid = norm > threshold\n        if valid:\n            unit = vectors / norm\n        else:\n            unit = vectors.copy()\n    else:\n        raise ValueError('vectors must be (n, ) or (n, d)!')\n\n    if check_valid:\n        return unit[valid], valid\n    return unit", "response": "Unitize a vector or an array or row - vectors."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the Euclidean distance between vectors a and b.", "response": "def euclidean(a, b):\n    \"\"\"\n    Euclidean distance between vectors a and b.\n\n    Parameters\n    ------------\n    a : (n,) float\n       First vector\n    b : (n,) float\n       Second vector\n\n    Returns\n    ------------\n    distance : float\n        Euclidean distance between A and B\n    \"\"\"\n    a = np.asanyarray(a, dtype=np.float64)\n    b = np.asanyarray(b, dtype=np.float64)\n    return np.sqrt(((a - b) ** 2).sum())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_none(obj):\n    if obj is None:\n        return True\n    if (is_sequence(obj) and\n        len(obj) == 1 and\n            obj[0] is None):\n        return True\n    return False", "response": "Checks to see if an object is None or not."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if an object is a sequence or not.", "response": "def is_sequence(obj):\n    \"\"\"\n    Check if an object is a sequence or not.\n\n    Parameters\n    -------------\n    obj : object\n      Any object type to be checked\n\n    Returns\n    -------------\n    is_sequence : bool\n        True if object is sequence\n    \"\"\"\n    seq = (not hasattr(obj, \"strip\") and\n           hasattr(obj, \"__getitem__\") or\n           hasattr(obj, \"__iter__\"))\n\n    # check to make sure it is not a set, string, or dictionary\n    seq = seq and all(not isinstance(obj, i) for i in (dict,\n                                                       set,\n                                                       basestring))\n\n    # PointCloud objects can look like an array but are not\n    seq = seq and type(obj).__name__ not in ['PointCloud']\n\n    # numpy sometimes returns objects that are single float64 values\n    # but sure look like sequences, so we check the shape\n    if hasattr(obj, 'shape'):\n        seq = seq and obj.shape != ()\n\n    return seq"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_shape(obj, shape):\n\n    # if the obj.shape is different length than\n    # the goal shape it means they have different number\n    # of dimensions and thus the obj is not the query shape\n    if (not hasattr(obj, 'shape') or\n            len(obj.shape) != len(shape)):\n        return False\n\n    # loop through each integer of the two shapes\n    # multiple values are sequences\n    # wildcards are less than zero (i.e. -1)\n    for i, target in zip(obj.shape, shape):\n        # check if current field has multiple acceptable values\n        if is_sequence(target):\n            if i in target:\n                # obj shape is in the accepted values\n                continue\n            else:\n                return False\n\n        # check if current field is a wildcard\n        if target < 0:\n            if i == 0:\n                # if a dimension is 0, we don't allow\n                # that to match to a wildcard\n                # it would have to be explicitly called out as 0\n                return False\n            else:\n                continue\n        # since we have a single target and a single value,\n        # if they are not equal we have an answer\n        if target != i:\n            return False\n\n    # since none of the checks failed the obj.shape\n    # matches the pattern\n    return True", "response": "Checks if the shape of a numpy. ndarray is the same length as the input shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_sequence(obj):\n    if is_sequence(obj):\n        return np.array(list(obj))\n    else:\n        return np.array([obj])", "response": "Makes a sequence of objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vector_hemisphere(vectors, return_sign=False):\n    # vectors as numpy array\n    vectors = np.asanyarray(vectors, dtype=np.float64)\n\n    if is_shape(vectors, (-1, 2)):\n        # 2D vector case\n        # check the Y value and reverse vector\n        # direction if negative.\n        negative = vectors < -TOL_ZERO\n        zero = np.logical_not(\n            np.logical_or(negative, vectors > TOL_ZERO))\n\n        signs = np.ones(len(vectors), dtype=np.float64)\n        # negative Y values are reversed\n        signs[negative[:, 1]] = -1.0\n\n        # zero Y and negative X are reversed\n        signs[np.logical_and(zero[:, 1], negative[:, 0])] = -1.0\n\n    elif is_shape(vectors, (-1, 3)):\n        # 3D vector case\n        negative = vectors < -TOL_ZERO\n        zero = np.logical_not(\n            np.logical_or(negative, vectors > TOL_ZERO))\n        # move all                          negative Z to positive\n        # then for zero Z vectors, move all negative Y to positive\n        # then for zero Y vectors, move all negative X to positive\n        signs = np.ones(len(vectors), dtype=np.float64)\n        # all vectors with negative Z values\n        signs[negative[:, 2]] = -1.0\n        # all on-plane vectors with negative Y values\n        signs[np.logical_and(zero[:, 2], negative[:, 1])] = -1.0\n        # all on-plane vectors with zero Y values\n        # and negative X values\n        signs[np.logical_and(np.logical_and(zero[:, 2],\n                                            zero[:, 1]),\n                             negative[:, 0])] = -1.0\n\n    else:\n        raise ValueError('vectors must be (n,3)!')\n\n    # apply the signs to the vectors\n    oriented = vectors * signs.reshape((-1, 1))\n\n    if return_sign:\n        return oriented, signs\n\n    return oriented", "response": "Returns a base - 3D array that contains the set of 3D vectors in the same hemisphere."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a set of cartesian points to spherical unit vectors.", "response": "def vector_to_spherical(cartesian):\n    \"\"\"\n    Convert a set of cartesian points to (n,2) spherical unit\n    vectors.\n\n    Parameters\n    ------------\n    cartesian : (n, 3) float\n       Points in space\n\n    Returns\n    ------------\n    spherical : (n, 2) float\n       Angles, in radians\n    \"\"\"\n    cartesian = np.asanyarray(cartesian, dtype=np.float64)\n    if not is_shape(cartesian, (-1, 3)):\n        raise ValueError('Cartesian points must be (n,3)!')\n\n    unit, valid = unitize(cartesian, check_valid=True)\n    unit[np.abs(unit) < TOL_MERGE] = 0.0\n\n    x, y, z = unit.T\n    spherical = np.zeros((len(cartesian), 2), dtype=np.float64)\n    spherical[valid] = np.column_stack((np.arctan2(y, x),\n                                        np.arccos(z)))\n    return spherical"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef spherical_to_vector(spherical):\n    spherical = np.asanyarray(spherical, dtype=np.float64)\n    if not is_shape(spherical, (-1, 2)):\n        raise ValueError('spherical coordinates must be (n, 2)!')\n\n    theta, phi = spherical.T\n    st, ct = np.sin(theta), np.cos(theta)\n    sp, cp = np.sin(phi), np.cos(phi)\n    vectors = np.column_stack((ct * sp,\n                               st * sp,\n                               cp))\n    return vectors", "response": "Convert a set of spherical vectors to a set of 3 - dimensional vectors."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of pairs of sequential values in a single order.", "response": "def pairwise(iterable):\n    \"\"\"\n    For an iterable, group values into pairs.\n\n    Parameters\n    -----------\n    iterable : (m, ) list\n       A sequence of values\n\n    Returns\n    -----------\n    pairs: (n, 2)\n      Pairs of sequential values\n\n    Example\n    -----------\n    In [1]: data\n    Out[1]: [0, 1, 2, 3, 4, 5, 6]\n\n    In [2]: list(trimesh.util.pairwise(data))\n    Out[2]: [(0, 1), (1, 2), (2, 3), (3, 4), (4, 5), (5, 6)]\n\n    \"\"\"\n    # looping through a giant numpy array would be dumb\n    # so special case ndarrays and use numpy operations\n    if isinstance(iterable, np.ndarray):\n        iterable = iterable.reshape(-1)\n        stacked = np.column_stack((iterable, iterable))\n        pairs = stacked.reshape(-1)[1:-1].reshape((-1, 2))\n        return pairs\n\n    # if we have a normal iterable use itertools\n    import itertools\n    a, b = itertools.tee(iterable)\n    # pop the first element of the second item\n    next(b)\n\n    return zip(a, b)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndots product by row of a and b. There are a lot of ways to do this though performance varies very widely. This method uses the dot product to sum the row and avoids function calls if at all possible. Comparing performance of some equivalent versions: ``` In [1]: import numpy as np; import trimesh In [2]: a = np.random.random((10000, 3)) In [3]: b = np.random.random((10000, 3)) In [4]: %timeit (a * b).sum(axis=1) 1000 loops, best of 3: 181 us per loop In [5]: %timeit np.einsum('ij,ij->i', a, b) 10000 loops, best of 3: 62.7 us per loop In [6]: %timeit np.diag(np.dot(a, b.T)) 1 loop, best of 3: 429 ms per loop In [7]: %timeit np.dot(a * b, np.ones(a.shape[1])) 10000 loops, best of 3: 61.3 us per loop In [8]: %timeit trimesh.util.diagonal_dot(a, b) 10000 loops, best of 3: 55.2 us per loop ``` Parameters ------------ a : (m, d) float First array b : (m, d) float Second array Returns ------------- result : (m,) float Dot product of each row", "response": "def diagonal_dot(a, b):\n    \"\"\"\n    Dot product by row of a and b.\n\n    There are a lot of ways to do this though\n    performance varies very widely. This method\n    uses the dot product to sum the row and avoids\n    function calls if at all possible.\n\n    Comparing performance of some equivalent versions:\n    ```\n    In [1]: import numpy as np; import trimesh\n\n    In [2]: a = np.random.random((10000, 3))\n\n    In [3]: b = np.random.random((10000, 3))\n\n    In [4]: %timeit (a * b).sum(axis=1)\n    1000 loops, best of 3: 181 us per loop\n\n    In [5]: %timeit np.einsum('ij,ij->i', a, b)\n    10000 loops, best of 3: 62.7 us per loop\n\n    In [6]: %timeit np.diag(np.dot(a, b.T))\n    1 loop, best of 3: 429 ms per loop\n\n    In [7]: %timeit np.dot(a * b, np.ones(a.shape[1]))\n    10000 loops, best of 3: 61.3 us per loop\n\n    In [8]: %timeit trimesh.util.diagonal_dot(a, b)\n    10000 loops, best of 3: 55.2 us per loop\n    ```\n\n    Parameters\n    ------------\n    a : (m, d) float\n      First array\n    b : (m, d) float\n      Second array\n\n    Returns\n    -------------\n    result : (m,) float\n      Dot product of each row\n    \"\"\"\n    # make sure `a` is numpy array\n    # doing it for `a` will force the multiplication to\n    # convert `b` if necessary and avoid function call otherwise\n    a = np.asanyarray(a)\n    # 3x faster than (a * b).sum(axis=1)\n    # avoiding np.ones saves 5-10% sometimes\n    result = np.dot(a * b, [1.0] * a.shape[1])\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stack_3D(points, return_2D=False):\n    points = np.asanyarray(points, dtype=np.float64)\n    shape = points.shape\n\n    if len(shape) != 2:\n        raise ValueError('Points must be 2D array!')\n\n    if shape[1] == 2:\n        points = np.column_stack((points,\n                                  np.zeros(len(points))))\n        is_2D = True\n    elif shape[1] == 3:\n        is_2D = False\n    else:\n        raise ValueError('Points must be (n,2) or (n,3)!')\n\n    if return_2D:\n        return points, is_2D\n\n    return points", "response": "This function stacks a list of 3D points on the XY plane."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a grid from an array of points in a specified step distance apart.", "response": "def grid_arange(bounds, step):\n    \"\"\"\n    Return a grid from an (2,dimension) bounds with samples step distance apart.\n\n    Parameters\n    ---------\n    bounds: (2,dimension) list of [[min x, min y, etc], [max x, max y, etc]]\n    step:   float, or (dimension) floats, separation between points\n\n    Returns\n    -------\n    grid: (n, dimension), points inside the specified bounds\n    \"\"\"\n    bounds = np.asanyarray(bounds, dtype=np.float64)\n    if len(bounds) != 2:\n        raise ValueError('bounds must be (2, dimension!')\n\n    # allow single float or per-dimension spacing\n    step = np.asanyarray(step, dtype=np.float64)\n    if step.shape == ():\n        step = np.tile(step, bounds.shape[1])\n\n    grid_elements = [np.arange(*b, step=s) for b, s in zip(bounds.T, step)]\n    grid = np.vstack(np.meshgrid(*grid_elements)\n                     ).reshape(bounds.shape[1], -1).T\n    return grid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef grid_linspace(bounds, count):\n    bounds = np.asanyarray(bounds, dtype=np.float64)\n    if len(bounds) != 2:\n        raise ValueError('bounds must be (2, dimension!')\n\n    count = np.asanyarray(count, dtype=np.int)\n    if count.shape == ():\n        count = np.tile(count, bounds.shape[1])\n\n    grid_elements = [np.linspace(*b, num=c) for b, c in zip(bounds.T, count)]\n    grid = np.vstack(np.meshgrid(*grid_elements)\n                     ).reshape(bounds.shape[1], -1).T\n    return grid", "response": "Returns a grid spaced inside a bounding box with edges spaced using np. linspace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef multi_dict(pairs):\n    result = collections.defaultdict(list)\n    for k, v in pairs:\n        result[k].append(v)\n    return result", "response": "Given a set of key value pairs create a dictionary that contains all the values stored in the last key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning True if file is a binary file.", "response": "def is_binary_file(file_obj):\n    \"\"\"\n    Returns True if file has non-ASCII characters (> 0x7F, or 127)\n    Should work in both Python 2 and 3\n    \"\"\"\n    start = file_obj.tell()\n    fbytes = file_obj.read(1024)\n    file_obj.seek(start)\n    is_str = isinstance(fbytes, str)\n    for fbyte in fbytes:\n        if is_str:\n            code = ord(fbyte)\n        else:\n            code = fbyte\n        if code > 127:\n            return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef distance_to_end(file_obj):\n    position_current = file_obj.tell()\n    file_obj.seek(0, 2)\n    position_end = file_obj.tell()\n    file_obj.seek(position_current)\n    distance = position_end - position_current\n    return distance", "response": "Returns the distance from the end of the file object to the current file object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the number of digits to the first nonzero decimal.", "response": "def decimal_to_digits(decimal, min_digits=None):\n    \"\"\"\n    Return the number of digits to the first nonzero decimal.\n\n    Parameters\n    -----------\n    decimal:    float\n    min_digits: int, minimum number of digits to return\n\n    Returns\n    -----------\n\n    digits: int, number of digits to the first nonzero decimal\n    \"\"\"\n    digits = abs(int(np.log10(decimal)))\n    if min_digits is not None:\n        digits = np.clip(digits, min_digits, 20)\n    return digits"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the hash of an open file - like object.", "response": "def hash_file(file_obj,\n              hash_function=hashlib.md5):\n    \"\"\"\n    Get the hash of an open file- like object.\n\n    Parameters\n    ---------\n    file_obj: file like object\n    hash_function: function to use to hash data\n\n    Returns\n    ---------\n    hashed: str, hex version of result\n    \"\"\"\n    # before we read the file data save the current position\n    # in the file (which is probably 0)\n    file_position = file_obj.tell()\n    # create an instance of the hash object\n    hasher = hash_function()\n    # read all data from the file into the hasher\n    hasher.update(file_obj.read())\n    # get a hex version of the result\n    hashed = hasher.hexdigest()\n    # return the file object to its original position\n    file_obj.seek(file_position)\n\n    return hashed"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the MD5 of an object.", "response": "def md5_object(obj):\n    \"\"\"\n    If an object is hashable, return the string of the MD5.\n\n    Parameters\n    -----------\n    obj: object\n\n    Returns\n    ----------\n    md5: str, MD5 hash\n    \"\"\"\n    hasher = hashlib.md5()\n    if isinstance(obj, basestring) and PY3:\n        # in python3 convert strings to bytes before hashing\n        hasher.update(obj.encode('utf-8'))\n    else:\n        hasher.update(obj)\n\n    md5 = hasher.hexdigest()\n    return md5"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nattaches a stream handler to all loggers.", "response": "def attach_to_log(level=logging.DEBUG,\n                  handler=None,\n                  loggers=None,\n                  colors=True,\n                  capture_warnings=True,\n                  blacklist=None):\n    \"\"\"\n    Attach a stream handler to all loggers.\n\n    Parameters\n    ------------\n    level:     logging level\n    handler:   log handler object\n    loggers:   list of loggers to attach to\n                 if None, will try to attach to all available\n    colors:    bool, if True try to use colorlog formatter\n    blacklist: list of str, names of loggers NOT to attach to\n    \"\"\"\n\n    if blacklist is None:\n        blacklist = ['TerminalIPythonApp',\n                     'PYREADLINE',\n                     'pyembree',\n                     'shapely.geos',\n                     'shapely.speedups._speedups',\n                     'parso.cache',\n                     'parso.python.diff']\n\n    # make sure we log warnings from the warnings module\n    logging.captureWarnings(capture_warnings)\n\n    formatter = logging.Formatter(\n        \"[%(asctime)s] %(levelname)-7s (%(filename)s:%(lineno)3s) %(message)s\",\n        \"%Y-%m-%d %H:%M:%S\")\n    if colors:\n        try:\n            from colorlog import ColoredFormatter\n            formatter = ColoredFormatter(\n                (\"%(log_color)s%(levelname)-8s%(reset)s \" +\n                 \"%(filename)17s:%(lineno)-4s  %(blue)4s%(message)s\"),\n                datefmt=None,\n                reset=True,\n                log_colors={'DEBUG': 'cyan',\n                            'INFO': 'green',\n                            'WARNING': 'yellow',\n                            'ERROR': 'red',\n                            'CRITICAL': 'red'})\n        except ImportError:\n            pass\n\n    # if no handler was passed, use a StreamHandler\n    if handler is None:\n        handler = logging.StreamHandler()\n\n    # add the formatters and set the level\n    handler.setFormatter(formatter)\n    handler.setLevel(level)\n\n    # if nothing passed use all available loggers\n    if loggers is None:\n        # de- duplicate loggers using a set\n        loggers = set(logging.Logger.manager.loggerDict.values())\n    # add the warnings logging\n    loggers.add(logging.getLogger('py.warnings'))\n\n    # disable pyembree warnings\n    logging.getLogger('pyembree').disabled = True\n\n    # loop through all available loggers\n    for logger in loggers:\n        # skip loggers on the blacklist\n        if (logger.__class__.__name__ != 'Logger' or\n                logger.name in blacklist):\n            continue\n        logger.addHandler(handler)\n        logger.setLevel(level)\n\n    # set nicer numpy print options\n    np.set_printoptions(precision=5, suppress=True)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stack_lines(indices):\n    indices = np.asanyarray(indices)\n    if is_sequence(indices[0]):\n        shape = (-1, len(indices[0]))\n    else:\n        shape = (-1, 2)\n    return np.column_stack((indices[:-1],\n                            indices[1:])).reshape(shape)", "response": "Returns a list of values that represent a polyline into a single line segment with duplicated consecutive values."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a sequence of zero - indexed faces and vertices and faces combine them into a single array of faces and a single array of vertices.", "response": "def append_faces(vertices_seq, faces_seq):\n    \"\"\"\n    Given a sequence of zero- indexed faces and vertices\n    combine them into a single array of faces and\n    a single array of vertices.\n\n    Parameters\n    -----------\n    vertices_seq : (n, ) sequence of (m, d) float\n      Multiple arrays of verticesvertex arrays\n    faces_seq : (n, ) sequence of (p, j) int\n      Zero indexed faces for matching vertices\n\n    Returns\n    ----------\n    vertices : (i, d) float\n      Points in space\n    faces : (j, 3) int\n      Reference vertex indices\n    \"\"\"\n    # the length of each vertex array\n    vertices_len = np.array([len(i) for i in vertices_seq])\n    # how much each group of faces needs to be offset\n    face_offset = np.append(0, np.cumsum(vertices_len)[:-1])\n\n    new_faces = []\n    for offset, faces in zip(face_offset, faces_seq):\n        if len(faces) == 0:\n            continue\n        # apply the index offset\n        new_faces.append(faces + offset)\n    # stack to clean (n, 3) float\n    vertices = vstack_empty(vertices_seq)\n    # stack to clean (n, 3) int\n    faces = vstack_empty(new_faces)\n\n    return vertices, faces"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a 1D numpy array into a string with a specified number of digits and delimiter.", "response": "def array_to_string(array,\n                    col_delim=' ',\n                    row_delim='\\n',\n                    digits=8,\n                    value_format='{}'):\n    \"\"\"\n    Convert a 1 or 2D array into a string with a specified number\n    of digits and delimiter. The reason this exists is that the\n    basic numpy array to string conversions are surprisingly bad.\n\n    Parameters\n    ----------\n    array : (n,) or (n, d) float or int\n       Data to be converted\n       If shape is (n,) only column delimiter will be used\n    col_delim : str\n      What string should separate values in a column\n    row_delim : str\n      What string should separate values in a row\n    digits : int\n      How many digits should floating point numbers include\n    value_format : str\n       Format string for each value or sequence of values\n       If multiple values per value_format it must divide\n       into array evenly.\n\n    Returns\n    ----------\n    formatted : str\n       String representation of original array\n    \"\"\"\n    # convert inputs to correct types\n    array = np.asanyarray(array)\n    digits = int(digits)\n    row_delim = str(row_delim)\n    col_delim = str(col_delim)\n    value_format = str(value_format)\n\n    # abort for non- flat arrays\n    if len(array.shape) > 2:\n        raise ValueError('conversion only works on 1D/2D arrays not %s!',\n                         str(array.shape))\n\n    # allow a value to be repeated in a value format\n    repeats = value_format.count('{}')\n\n    if array.dtype.kind == 'i':\n        # integer types don't need a specified precision\n        format_str = value_format + col_delim\n    elif array.dtype.kind == 'f':\n        # add the digits formatting to floats\n        format_str = value_format.replace(\n            '{}', '{:.' + str(digits) + 'f}') + col_delim\n    else:\n        raise ValueError('dtype %s not convertible!',\n                         array.dtype.name)\n\n    # length of extra delimiters at the end\n    end_junk = len(col_delim)\n    # if we have a 2D array add a row delimiter\n    if len(array.shape) == 2:\n        format_str *= array.shape[1]\n        # cut off the last column delimiter and add a row delimiter\n        format_str = format_str[:-len(col_delim)] + row_delim\n        end_junk = len(row_delim)\n\n    # expand format string to whole array\n    format_str *= len(array)\n\n    # if an array is repeated in the value format\n    # do the shaping here so we don't need to specify indexes\n    shaped = np.tile(array.reshape((-1, 1)),\n                     (1, repeats)).reshape(-1)\n\n    # run the format operation and remove the extra delimiters\n    formatted = format_str.format(*shaped)[:-end_junk]\n\n    return formatted"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef array_to_encoded(array, dtype=None, encoding='base64'):\n    array = np.asanyarray(array)\n    shape = array.shape\n    # ravel also forces contiguous\n    flat = np.ravel(array)\n    if dtype is None:\n        dtype = array.dtype\n\n    encoded = {'dtype': np.dtype(dtype).str,\n               'shape': shape}\n    if encoding in ['base64', 'dict64']:\n        packed = base64.b64encode(flat.astype(dtype).tostring())\n        if hasattr(packed, 'decode'):\n            packed = packed.decode('utf-8')\n        encoded['base64'] = packed\n    elif encoding == 'binary':\n        encoded['binary'] = array.tostring(order='C')\n    else:\n        raise ValueError('encoding {} is not available!'.format(encoding))\n    return encoded", "response": "Export a numpy array to a compact serializable dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding the keys in a dictionary to ASCII strings.", "response": "def decode_keys(store, encoding='utf-8'):\n    \"\"\"\n    If a dictionary has keys that are bytes decode them to a str.\n\n    Parameters\n    ---------\n    store : dict\n      Dictionary with data\n\n    Returns\n    ---------\n    result : dict\n      Values are untouched but keys that were bytes\n      are converted to ASCII strings.\n\n    Example\n    -----------\n    In [1]: d\n    Out[1]: {1020: 'nah', b'hi': 'stuff'}\n\n    In [2]: trimesh.util.decode_keys(d)\n    Out[2]: {1020: 'nah', 'hi': 'stuff'}\n    \"\"\"\n    keys = store.keys()\n    for key in keys:\n        if hasattr(key, 'decode'):\n            decoded = key.decode(encoding)\n            if key != decoded:\n                store[key.decode(encoding)] = store[key]\n                store.pop(key)\n    return store"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef encoded_to_array(encoded):\n\n    if not isinstance(encoded, dict):\n        if is_sequence(encoded):\n            as_array = np.asanyarray(encoded)\n            return as_array\n        else:\n            raise ValueError('Unable to extract numpy array from input')\n\n    encoded = decode_keys(encoded)\n\n    dtype = np.dtype(encoded['dtype'])\n    if 'base64' in encoded:\n        array = np.frombuffer(base64.b64decode(encoded['base64']),\n                              dtype)\n    elif 'binary' in encoded:\n        array = np.frombuffer(encoded['binary'],\n                              dtype=dtype)\n    if 'shape' in encoded:\n        array = array.reshape(encoded['shape'])\n    return array", "response": "Turn a dictionary with base64 encoded strings back into a numpy array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the bases of the object passed.", "response": "def type_bases(obj, depth=4):\n    \"\"\"\n    Return the bases of the object passed.\n    \"\"\"\n    bases = collections.deque([list(obj.__class__.__bases__)])\n    for i in range(depth):\n        bases.append([i.__base__ for i in bases[-1] if i is not None])\n    try:\n        bases = np.hstack(bases)\n    except IndexError:\n        bases = []\n    # we do the hasattr as None/NoneType can be in the list of bases\n    bases = [i for i in bases if hasattr(i, '__name__')]\n    return np.array(bases)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the class of the object with the given name.", "response": "def type_named(obj, name):\n    \"\"\"\n    Similar to the type() builtin, but looks in class bases\n    for named instance.\n\n    Parameters\n    ----------\n    obj: object to look for class of\n    name : str, name of class\n\n    Returns\n    ----------\n    named class, or None\n    \"\"\"\n    # if obj is a member of the named class, return True\n    name = str(name)\n    if obj.__class__.__name__ == name:\n        return obj.__class__\n    for base in type_bases(obj):\n        if base.__name__ == name:\n            return base\n    raise ValueError('Unable to extract class of name ' + name)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef concatenate(a, b=None):\n    if b is None:\n        b = []\n    # stack meshes into flat list\n    meshes = np.append(a, b)\n\n    # extract the trimesh type to avoid a circular import\n    # and assert that both inputs are Trimesh objects\n    trimesh_type = type_named(meshes[0], 'Trimesh')\n\n    # append faces and vertices of meshes\n    vertices, faces = append_faces(\n        [m.vertices.copy() for m in meshes],\n        [m.faces.copy() for m in meshes])\n\n    # only save face normals if already calculated\n    face_normals = None\n    if all('face_normals' in m._cache for m in meshes):\n        face_normals = np.vstack([m.face_normals\n                                  for m in meshes])\n\n    # concatenate visuals\n    visual = meshes[0].visual.concatenate(\n        [m.visual for m in meshes[1:]])\n\n    # create the mesh object\n    mesh = trimesh_type(vertices=vertices,\n                        faces=faces,\n                        face_normals=face_normals,\n                        visual=visual,\n                        process=False)\n\n    return mesh", "response": "Concatenate two or more Trimesh objects."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef submesh(mesh,\n            faces_sequence,\n            only_watertight=False,\n            append=False):\n    \"\"\"\n    Return a subset of a mesh.\n\n    Parameters\n    ----------\n    mesh : Trimesh\n       Source mesh to take geometry from\n    faces_sequence : sequence (p,) int\n        Indexes of mesh.faces\n    only_watertight : bool\n        Only return submeshes which are watertight.\n    append : bool\n        Return a single mesh which has the faces appended,\n        if this flag is set, only_watertight is ignored\n\n    Returns\n    ---------\n    if append : Trimesh object\n    else        list of Trimesh objects\n    \"\"\"\n    # evaluate generators so we can escape early\n    faces_sequence = list(faces_sequence)\n\n    if len(faces_sequence) == 0:\n        return []\n\n    # check to make sure we're not doing a whole bunch of work\n    # to deliver a subset which ends up as the whole mesh\n    if len(faces_sequence[0]) == len(mesh.faces):\n        all_faces = np.array_equal(np.sort(faces_sequence),\n                                   np.arange(len(faces_sequence)))\n        if all_faces:\n            log.debug('entire mesh requested, returning copy')\n            return mesh.copy()\n\n    # avoid nuking the cache on the original mesh\n    original_faces = mesh.faces.view(np.ndarray)\n    original_vertices = mesh.vertices.view(np.ndarray)\n\n    faces = []\n    vertices = []\n    normals = []\n    visuals = []\n\n    # for reindexing faces\n    mask = np.arange(len(original_vertices))\n\n    for faces_index in faces_sequence:\n        # sanitize indices in case they are coming in as a set or tuple\n        faces_index = np.asanyarray(faces_index, dtype=np.int64)\n        if len(faces_index) == 0:\n            continue\n        faces_current = original_faces[faces_index]\n        unique = np.unique(faces_current.reshape(-1))\n\n        # redefine face indices from zero\n        mask[unique] = np.arange(len(unique))\n        normals.append(mesh.face_normals[faces_index])\n        faces.append(mask[faces_current])\n        vertices.append(original_vertices[unique])\n        visuals.append(mesh.visual.face_subset(faces_index))\n\n    # we use type(mesh) rather than importing Trimesh from base\n    # to avoid a circular import\n    trimesh_type = type_named(mesh, 'Trimesh')\n    if append:\n        if all(hasattr(i, 'concatenate')\n               for i in visuals):\n            visuals = np.array(visuals)\n            visual = visuals[0].concatenate(visuals[1:])\n        else:\n            visual = None\n\n        vertices, faces = append_faces(vertices, faces)\n        appended = trimesh_type(\n            vertices=vertices,\n            faces=faces,\n            face_normals=np.vstack(normals),\n            visual=visual,\n            process=False)\n        return appended\n\n    # generate a list of Trimesh objects\n    result = [trimesh_type(\n        vertices=v,\n        faces=f,\n        face_normals=n,\n        visual=c,\n        metadata=copy.deepcopy(mesh.metadata),\n        process=False) for v, f, n, c in zip(vertices,\n                                             faces,\n                                             normals,\n                                             visuals)]\n    result = np.array(result)\n    if len(result) > 0 and only_watertight:\n        # fill_holes will attempt a repair and returns the\n        # watertight status at the end of the repair attempt\n        watertight = np.array([i.fill_holes() and len(i.faces) >= 4\n                               for i in result])\n        # remove unrepairable meshes\n        result = result[watertight]\n\n    return result", "response": "Returns a subset of a Trimesh object from a source mesh."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a 1D array where the first count elements are zero.", "response": "def zero_pad(data, count, right=True):\n    \"\"\"\n    Parameters\n    --------\n    data : (n,)\n      1D array\n    count : int\n      Minimum length of result array\n\n    Returns\n    --------\n    padded : (m,)\n      1D array where m >= count\n    \"\"\"\n    if len(data) == 0:\n        return np.zeros(count)\n    elif len(data) < count:\n        padded = np.zeros(count)\n        if right:\n            padded[-len(data):] = data\n        else:\n            padded[:len(data)] = data\n        return padded\n    else:\n        return np.asanyarray(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef convert_like(item, like):\n    if isinstance(like, np.ndarray):\n        return np.asanyarray(item, dtype=like.dtype)\n\n    if isinstance(item, like.__class__) or is_none(like):\n        return item\n\n    if (is_sequence(item) and\n        len(item) == 1 and\n            isinstance(item[0], like.__class__)):\n        return item[0]\n\n    item = like.__class__(item)\n    return item", "response": "Convert an item to have the dtype of another item"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef bounds_tree(bounds):\n    bounds = np.asanyarray(copy.deepcopy(bounds), dtype=np.float64)\n    if len(bounds.shape) != 2:\n        raise ValueError('Bounds must be (n,dimension*2)!')\n\n    dimension = bounds.shape[1]\n    if (dimension % 2) != 0:\n        raise ValueError('Bounds must be (n,dimension*2)!')\n    dimension = int(dimension / 2)\n\n    import rtree\n    # some versions of rtree screw up indexes on stream loading\n    # do a test here so we know if we are free to use stream loading\n    # or if we have to do a loop to insert things which is 5x slower\n    rtree_test = rtree.index.Index([(1564, [0, 0, 0, 10, 10, 10], None)],\n                                   properties=rtree.index.Property(dimension=3))\n    rtree_stream_ok = next(rtree_test.intersection([1, 1, 1, 2, 2, 2])) == 1564\n\n    properties = rtree.index.Property(dimension=dimension)\n    if rtree_stream_ok:\n        # stream load was verified working on inport above\n        tree = rtree.index.Index(zip(np.arange(len(bounds)),\n                                     bounds,\n                                     [None] * len(bounds)),\n                                 properties=properties)\n    else:\n        # in some rtree versions stream loading goofs the index\n        log.warning('rtree stream loading broken! Try upgrading rtree!')\n        tree = rtree.index.Index(properties=properties)\n        for i, b in enumerate(bounds):\n            tree.insert(i, b)\n    return tree", "response": "Create an r - tree object for a set of axis aligned bounds."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wrap_as_stream(item):\n    if not PY3:\n        return StringIO(item)\n    if isinstance(item, str):\n        return StringIO(item)\n    elif isinstance(item, bytes):\n        return BytesIO(item)\n    raise ValueError('{} is not wrappable!'.format(type(item).__name__))", "response": "Wraps a string or bytes object as a file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nround a single value to a specified number of significant figures.", "response": "def sigfig_round(values, sigfig=1):\n    \"\"\"\n    Round a single value to a specified number of significant figures.\n\n    Parameters\n    ----------\n    values: float, value to be rounded\n    sigfig: int, number of significant figures to reduce to\n\n\n    Returns\n    ----------\n    rounded: values, but rounded to the specified number of significant figures\n\n\n    Examples\n    ----------\n    In [1]: trimesh.util.round_sigfig(-232453.00014045456, 1)\n    Out[1]: -200000.0\n\n    In [2]: trimesh.util.round_sigfig(.00014045456, 1)\n    Out[2]: 0.0001\n\n    In [3]: trimesh.util.round_sigfig(.00014045456, 4)\n    Out[3]: 0.0001405\n    \"\"\"\n    as_int, multiplier = sigfig_int(values, sigfig)\n    rounded = as_int * (10 ** multiplier)\n\n    return rounded"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sigfig_int(values, sigfig):\n    values = np.asanyarray(values).reshape(-1)\n    sigfig = np.asanyarray(sigfig, dtype=np.int).reshape(-1)\n\n    if sigfig.shape != values.shape:\n        raise ValueError('sigfig must match identifier')\n\n    exponent = np.zeros(len(values))\n    nonzero = np.abs(values) > TOL_ZERO\n    exponent[nonzero] = np.floor(np.log10(np.abs(values[nonzero])))\n\n    multiplier = exponent - sigfig + 1\n\n    as_int = np.round(values / (10**multiplier)).astype(np.int32)\n\n    return as_int, multiplier", "response": "Convert a set of floating point values into integers with a specified number of significant figures and an exponent."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives an open file object and a file type, return all components of the archive as open file objects in a dict. Parameters ----------- file_obj : file-like Containing compressed data file_type : str File extension, 'zip', 'tar.gz', etc Returns --------- decompressed : dict Data from archive in format {file name : file-like}", "response": "def decompress(file_obj, file_type):\n    \"\"\"\n    Given an open file object and a file type, return all components\n    of the archive as open file objects in a dict.\n\n    Parameters\n    -----------\n    file_obj : file-like\n      Containing compressed data\n    file_type : str\n      File extension, 'zip', 'tar.gz', etc\n\n    Returns\n    ---------\n    decompressed : dict\n      Data from archive in format {file name : file-like}\n    \"\"\"\n\n    def is_zip():\n        archive = zipfile.ZipFile(file_obj)\n        result = {name: wrap_as_stream(archive.read(name))\n                  for name in archive.namelist()}\n        return result\n\n    def is_tar():\n        import tarfile\n        archive = tarfile.open(fileobj=file_obj, mode='r')\n        result = {name: archive.extractfile(name)\n                  for name in archive.getnames()}\n        return result\n\n    file_type = str(file_type).lower()\n    if isinstance(file_obj, bytes):\n        file_obj = wrap_as_stream(file_obj)\n\n    if file_type[-3:] == 'zip':\n        return is_zip()\n    if 'tar' in file_type[-6:]:\n        return is_tar()\n    raise ValueError('Unsupported type passed!')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compress(info):\n    if PY3:\n        file_obj = BytesIO()\n    else:\n        file_obj = StringIO()\n\n    with zipfile.ZipFile(\n            file_obj,\n            mode='w',\n            compression=zipfile.ZIP_DEFLATED) as zipper:\n        for name, data in info.items():\n            if hasattr(data, 'read'):\n                # if we were passed a file object, read it\n                data = data.read()\n            zipper.writestr(name, data)\n    file_obj.seek(0)\n    compressed = file_obj.read()\n    return compressed", "response": "Compresses the data stored in a dict into a byte - like object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split_extension(file_name, special=['tar.bz2', 'tar.gz']):\n    file_name = str(file_name)\n\n    if file_name.endswith(tuple(special)):\n        for end in special:\n            if file_name.endswith(end):\n                return end\n    return file_name.split('.')[-1]", "response": "Returns the file extension of a file name including support for multipart file extensions."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef triangle_strips_to_faces(strips):\n\n    # save the length of each list in the list of lists\n    lengths = np.array([len(i) for i in strips])\n    # looping through a list of lists is extremely slow\n    # combine all the sequences into a blob we can manipulate\n    blob = np.concatenate(strips)\n\n    # preallocate and slice the blob into rough triangles\n    tri = np.zeros((len(blob) - 2, 3), dtype=np.int)\n    for i in range(3):\n        tri[:len(blob) - 3, i] = blob[i:-3 + i]\n    # the last triangle is left off from the slicing, add it back\n    tri[-1] = blob[-3:]\n\n    # remove the triangles which were implicit but not actually there\n    # because we combined everything into one big array for speed\n    length_index = np.cumsum(lengths)[:-1]\n    keep = np.ones(len(tri), dtype=np.bool)\n    keep[length_index - 2] = False\n    keep[length_index - 1] = False\n    tri = tri[keep]\n\n    # flip every other triangle so they generate correct normals/winding\n    length_index = np.append(0, np.cumsum(lengths - 2))\n    flip = np.zeros(length_index[-1], dtype=np.bool)\n    for i in range(len(length_index) - 1):\n        flip[length_index[i] + 1:length_index[i + 1]][::2] = True\n    tri[flip] = np.fliplr(tri[flip])\n\n    return tri", "response": "Given a sequence of triangle strips convert them to n faces."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vstack_empty(tup):\n    # filter out empty arrays\n    stackable = [i for i in tup if len(i) > 0]\n    # if we only have one array just return it\n    if len(stackable) == 1:\n        return np.asanyarray(stackable[0])\n    # if we have nothing return an empty numpy array\n    elif len(stackable) == 0:\n        return np.array([])\n    # otherwise just use vstack as normal\n    return np.vstack(stackable)", "response": "A thin wrapper for numpy. vstack that ignores empty lists."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwriting a string or bytes to a file in binary mode and returns a list of stuff written.", "response": "def write_encoded(file_obj,\n                  stuff,\n                  encoding='utf-8'):\n    \"\"\"\n    If a file is open in binary mode and a string is passed, encode and write\n    If a file is open in text   mode and bytes are passed, decode and write\n\n    Parameters\n    -----------\n    file_obj: file object,  with 'write' and 'mode'\n    stuff:    str or bytes, stuff to be written\n    encoding: str,          encoding of text\n\n    \"\"\"\n    binary_file = 'b' in file_obj.mode\n    string_stuff = isinstance(stuff, basestring)\n    binary_stuff = isinstance(stuff, bytes)\n\n    if not PY3:\n        file_obj.write(stuff)\n    elif binary_file and string_stuff:\n        file_obj.write(stuff.encode(encoding))\n    elif not binary_file and binary_stuff:\n        file_obj.write(stuff.decode(encoding))\n    else:\n        file_obj.write(stuff)\n    file_obj.flush()\n    return stuff"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a decent looking alphanumeric unique identifier.", "response": "def unique_id(length=12, increment=0):\n    \"\"\"\n    Generate a decent looking alphanumeric unique identifier.\n    First 16 bits are time- incrementing, followed by randomness.\n\n    This function is used as a nicer looking alternative to:\n    >>> uuid.uuid4().hex\n\n    Follows the advice in:\n    https://eager.io/blog/how-long-does-an-id-need-to-be/\n\n    Parameters\n    ------------\n    length:    int, length of resulting identifier\n    increment: int, number to add to header uint16\n                    useful if calling this function repeatedly\n                    in a tight loop executing faster than time\n                    can increment the header\n    Returns\n    ------------\n    unique: str, unique alphanumeric identifier\n    \"\"\"\n    # head the identifier with 16 bits of time information\n    # this provides locality and reduces collision chances\n    head = np.array((increment + time.time() * 10) % 2**16,\n                    dtype=np.uint16).tostring()\n    # get a bunch of random bytes\n    random = np.random.random(int(np.ceil(length / 5))).tostring()\n    # encode the time header and random information as base64\n    # replace + and / with spaces\n    unique = base64.b64encode(head + random,\n                              b'  ').decode('utf-8')\n    # remove spaces and cut to length\n    unique = unique.replace(' ', '')[:length]\n    return unique"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating an arbitrary basis for a given z - axis vector.", "response": "def generate_basis(z):\n    \"\"\"\n    Generate an arbitrary basis (also known as a coordinate frame)\n    from a given z-axis vector.\n\n    Parameters\n    ----------\n    z: (3,) float\n      A vector along the positive z-axis\n\n    Returns\n    -------\n    x : (3,) float\n      Vector along x axis\n    y : (3,) float\n      Vector along y axis\n    z : (3,) float\n      Vector along z axis\n    \"\"\"\n    # get a copy of input vector\n    z = np.array(z, dtype=np.float64, copy=True)\n    # must be a 3D vector\n    if z.shape != (3,):\n        raise ValueError('z must be (3,) float!')\n\n    # normalize vector in- place\n    z /= np.linalg.norm(z)\n    # X as arbitrary perpendicular vector\n    x = np.array([-z[1], z[0], 0.0])\n    # avoid degenerate case\n    if np.isclose(np.linalg.norm(x), 0.0):\n        # Z is already along Z [0, 0, 1]\n        # so a perpendicular X is just X\n        x = np.array([1.0, 0.0, 0.0])\n    else:\n        # otherwise normalize X in- place\n        x /= np.linalg.norm(x)\n    # get perpendicular Y with cross product\n    y = np.cross(z, x)\n    # append result values into vector\n    result = np.array([x, y, z], dtype=np.float64)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload an SVG file into a Path2D object.", "response": "def svg_to_path(file_obj, file_type=None):\n    \"\"\"\n    Load an SVG file into a Path2D object.\n\n    Parameters\n    -----------\n    file_obj : open file object\n      Contains SVG data\n    file_type: None\n      Not used\n\n    Returns\n    -----------\n    loaded : dict\n      With kwargs for Path2D constructor\n    \"\"\"\n\n    def element_transform(e, max_depth=100):\n        \"\"\"\n        Find a transformation matrix for an XML element.\n        \"\"\"\n        matrices = []\n        current = e\n        for i in range(max_depth):\n            if 'transform' in current.attrib:\n                mat = transform_to_matrices(current.attrib['transform'])\n                matrices.extend(mat)\n                # cached[current] = mat\n            current = current.getparent()\n            if current is None:\n                break\n\n        if len(matrices) == 0:\n            return np.eye(3)\n        elif len(matrices) == 1:\n            return matrices[0]\n        else:\n            return util.multi_dot(matrices[::-1])\n\n    # first parse the XML\n    xml = etree.fromstring(file_obj.read())\n\n    # store paths and transforms as\n    # (path string, 3x3 matrix)\n    paths = []\n\n    # store every path element\n    for element in xml.iter('{*}path'):\n        paths.append((element.attrib['d'],\n                      element_transform(element)))\n\n    return _svg_path_convert(paths)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting an SVG transform string to an array of matrices.", "response": "def transform_to_matrices(transform):\n    \"\"\"\n    Convert an SVG transform string to an array of matrices.\n\n\n    > transform = \"rotate(-10 50 100)\n                   translate(-36 45.5)\n                   skewX(40)\n                   scale(1 0.5)\"\n\n    Parameters\n    -----------\n    transform : str\n      Contains transformation information in SVG form\n\n    Returns\n    -----------\n    matrices : (n, 3, 3) float\n      Multiple transformation matrices from input transform string\n    \"\"\"\n    # split the transform string in to components of:\n    # (operation, args) i.e. (translate, '-1.0, 2.0')\n    components = [\n        [j.strip() for j in i.strip().split('(') if len(j) > 0]\n        for i in transform.lower().split(')') if len(i) > 0]\n    # store each matrix without dotting\n    matrices = []\n    for line in components:\n        if len(line) == 0:\n            continue\n        elif len(line) != 2:\n            raise ValueError('should always have two components!')\n        key, args = line\n        # convert string args to array of floats\n        # support either comma or space delimiter\n        values = np.array([float(i) for i in\n                           args.replace(',', ' ').split()])\n        if key == 'translate':\n            # convert translation to a (3, 3) homogenous matrix\n            matrices.append(np.eye(3))\n            matrices[-1][:2, 2] = values\n        elif key == 'matrix':\n            # [a b c d e f] ->\n            # [[a c e],\n            #  [b d f],\n            #  [0 0 1]]\n            matrices.append(np.vstack((\n                values.reshape((3, 2)).T, [0, 0, 1])))\n        elif key == 'rotate':\n            # SVG rotations are in degrees\n            angle = np.degrees(values[0])\n            # if there are three values rotate around point\n            if len(values) == 3:\n                point = values[1:]\n            else:\n                point = None\n            matrices.append(planar_matrix(theta=angle,\n                                          point=point))\n        elif key == 'scale':\n            # supports (x_scale, y_scale) or (scale)\n            mat = np.eye(3)\n            mat[:2, :2] *= values\n            matrices.append(mat)\n        else:\n            log.warning('unknown SVG transform: {}'.format(key))\n\n    return matrices"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _svg_path_convert(paths):\n    def complex_to_float(values):\n        return np.array([[i.real, i.imag] for i in values])\n\n    def load_line(svg_line):\n        points = complex_to_float([svg_line.point(0.0),\n                                   svg_line.point(1.0)])\n        if starting:\n            # return every vertex and use it\n            return (entities_mod.Line(np.arange(2) + len(vertices)), points)\n        else:\n            # we are not starting so use the last referenced vertex as the\n            # start point\n            return (entities_mod.Line(\n                np.arange(2) + len(vertices) - 1), points[1:])\n\n    def load_arc(svg_arc):\n        points = complex_to_float([svg_arc.start,\n                                   svg_arc.point(.5),\n                                   svg_arc.end])\n        if starting:\n            # return every vertex and use it\n            return (entities_mod.Arc(np.arange(3) + len(vertices)), points)\n        else:\n            # we are not starting so use the last referenced vertex as the\n            # start point\n            return (entities_mod.Arc(np.arange(3) +\n                                     len(vertices) - 1), points[1:])\n\n    def load_quadratic(svg_quadratic):\n        points = complex_to_float([svg_quadratic.start,\n                                   svg_quadratic.control,\n                                   svg_quadratic.end])\n        if starting:\n            # return every vertex and use it\n            return (entities_mod.Bezier(np.arange(3) + len(vertices)), points)\n        else:\n            # we are not starting so use the last referenced vertex as the\n            # start point\n            return (entities_mod.Bezier(\n                np.arange(3) + len(vertices) - 1), points[1:])\n\n    def load_cubic(svg_cubic):\n        points = complex_to_float([svg_cubic.start,\n                                   svg_cubic.control1,\n                                   svg_cubic.control2,\n                                   svg_cubic.end])\n        if starting:\n            # return every vertex and use it\n            return (entities_mod.Bezier(np.arange(4) + len(vertices)), points)\n        else:\n            # we are not starting so use the last referenced vertex as the\n            # start point\n            return (entities_mod.Bezier(\n                np.arange(4) + len(vertices) - 1), points[1:])\n\n    # store loaded values here\n    entities = []\n    vertices = []\n    loaders = {'Arc': load_arc,\n               'Line': load_line,\n               'CubicBezier': load_cubic,\n               'QuadraticBezier': load_quadratic}\n\n    for path_string, matrix in paths:\n        starting = True\n        for svg_entity in parse_path(path_string):\n            type_name = svg_entity.__class__.__name__\n            if type_name in loaders:\n                e, v = loaders[type_name](svg_entity)\n                entities.append(e)\n                vertices.extend(transform_points(v, matrix))\n    # store results as kwargs\n    loaded = {'entities': np.array(entities),\n              'vertices': np.array(vertices)}\n    return loaded", "response": "Convert an SVG path string into a Path2D object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef export_svg(drawing,\n               return_path=False,\n               layers=None,\n               **kwargs):\n    \"\"\"\n    Export a Path2D object into an SVG file.\n\n    Parameters\n    -----------\n    drawing : Path2D\n     Source geometry\n    return_path : bool\n      If True return only path string\n    layers : None, or [str]\n      Only export specified layers\n\n    Returns\n    -----------\n    as_svg: str, XML formatted as SVG\n\n    \"\"\"\n    if not util.is_instance_named(drawing, 'Path2D'):\n        raise ValueError('drawing must be Path2D object!')\n\n    points = drawing.vertices.view(np.ndarray).copy()\n\n    def circle_to_svgpath(center, radius, reverse):\n        radius_str = format(radius, res.export)\n        path_str = ' M ' + format(center[0] - radius, res.export) + ','\n        path_str += format(center[1], res.export)\n        path_str += ' a ' + radius_str + ',' + radius_str\n        path_str += ',0,1,' + str(int(reverse)) + ','\n        path_str += format(2 * radius, res.export) + ',0'\n        path_str += ' a ' + radius_str + ',' + radius_str\n        path_str += ',0,1,' + str(int(reverse)) + ','\n        path_str += format(-2 * radius, res.export) + ',0 Z'\n        return path_str\n\n    def svg_arc(arc, reverse):\n        \"\"\"\n        arc string: (rx ry x-axis-rotation large-arc-flag sweep-flag x y)+\n        large-arc-flag: greater than 180 degrees\n        sweep flag: direction (cw/ccw)\n        \"\"\"\n        arc_idx = arc.points[::((reverse * -2) + 1)]\n        vertices = points[arc_idx]\n        vertex_start, vertex_mid, vertex_end = vertices\n        center_info = arc_center(vertices)\n        C, R, angle = (center_info['center'],\n                       center_info['radius'],\n                       center_info['span'])\n        if arc.closed:\n            return circle_to_svgpath(C, R, reverse)\n\n        large_flag = str(int(angle > np.pi))\n        sweep_flag = str(int(np.cross(vertex_mid - vertex_start,\n                                      vertex_end - vertex_start) > 0.0))\n\n        arc_str = move_to(arc_idx[0])\n        arc_str += 'A {},{} 0 {}, {} {},{}'.format(R,\n                                                   R,\n                                                   large_flag,\n                                                   sweep_flag,\n                                                   vertex_end[0],\n                                                   vertex_end[1])\n        return arc_str\n\n    def move_to(vertex_id):\n        x_ex = format(points[vertex_id][0], res.export)\n        y_ex = format(points[vertex_id][1], res.export)\n        move_str = ' M ' + x_ex + ',' + y_ex\n        return move_str\n\n    def svg_discrete(entity, reverse):\n        \"\"\"\n        Use an entities discrete representation to export a\n        curve as a polyline\n        \"\"\"\n        discrete = entity.discrete(points)\n        # if entity contains no geometry return\n        if len(discrete) == 0:\n            return ''\n        # are we reversing the entity\n        if reverse:\n            discrete = discrete[::-1]\n        # the format string for the SVG path\n        template = ' M {},{} ' + (' L {},{}' * (len(discrete) - 1))\n        # apply the data from the discrete curve\n        result = template.format(*discrete.reshape(-1))\n        return result\n\n    def convert_path(path,\n                     reverse=False,\n                     close=True):\n        \"\"\"\n        Convert a list of entity indices to SVG.\n\n        Parameters\n        ----------------\n        path : [int]\n          List of entity indices\n        reverse : bool\n          Reverse exported path\n        close : bool\n          If True, connect last vertex to first\n\n        Returns\n        -------------\n        as_svg : str\n          SVG path string of input path\n        \"\"\"\n        # if we are only exporting some layers check here\n        if layers is not None:\n            # only export if every entity is on layer whitelist\n            if not all(drawing.layers[i] in layers for i in path):\n                return ''\n\n        path = path[::(reverse * -2) + 1]\n        converted = []\n        for i, entity_id in enumerate(path):\n            # the entity object\n            entity = drawing.entities[entity_id]\n            # the class name of the entity\n            etype = entity.__class__.__name__\n            if etype in converters:\n                # export the exact version of the entity\n                converted.append(converters[etype](entity,\n                                                   reverse))\n            else:\n                # just export the polyline version of the entity\n                converted.append(svg_discrete(entity,\n                                              reverse))\n\n        # remove leading and trailing whitespace\n        as_svg = ' '.join(converted) + ' '\n        return as_svg\n\n    # only converters where we want to do something\n    # other than export a curve as a polyline\n    converters = {'Arc': svg_arc}\n\n    converted = []\n    for index, path in enumerate(drawing.paths):\n        # holes are determined by winding\n        # trimesh makes all paths clockwise\n        reverse = not (index in drawing.root)\n        converted.append(convert_path(path,\n                                      reverse=reverse,\n                                      close=True))\n\n    # entities which haven't been included in a closed path\n    converted.append(convert_path(drawing.dangling,\n                                  reverse=False,\n                                  close=False))\n\n    # append list of converted into a string\n    path_str = ''.join(converted).strip()\n\n    # return path string without XML wrapping\n    if return_path:\n        return path_str\n\n    # format as XML\n    if 'stroke_width' in kwargs:\n        stroke_width = float(kwargs['stroke_width'])\n    else:\n        stroke_width = drawing.extents.max() / 800.0\n    subs = {'PATH_STRING': path_str,\n            'MIN_X': points[:, 0].min(),\n            'MIN_Y': points[:, 1].min(),\n            'WIDTH': drawing.extents[0],\n            'HEIGHT': drawing.extents[1],\n            'STROKE': stroke_width}\n    result = _template_svg.substitute(subs)\n    return result", "response": "Export a Path2D object into an SVG file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compute_stable_poses(mesh,\n                         center_mass=None,\n                         sigma=0.0,\n                         n_samples=1,\n                         threshold=0.0):\n    \"\"\"\n    Computes stable orientations of a mesh and their quasi-static probabilites.\n\n    This method samples the location of the center of mass from a multivariate\n    gaussian with the mean at the center of mass, and a covariance\n    equal to and identity matrix times sigma, over n_samples.\n\n    For each sample, it computes the stable resting poses of the mesh on a\n    a planar workspace and evaulates the probabilities of landing in\n    each pose if the object is dropped onto the table randomly.\n\n    This method returns the 4x4 homogenous transform matrices that place\n    the shape against the planar surface with the z-axis pointing upwards\n    and a list of the probabilities for each pose.\n\n    The transforms and probabilties that are returned are sorted, with the\n    most probable pose first.\n\n    Parameters\n    ----------\n    mesh : trimesh.Trimesh\n      The target mesh\n    com : (3,) float\n      Rhe object center of mass. If None, this method\n      assumes uniform density and watertightness and\n      computes a center of mass explicitly\n    sigma : float\n      Rhe covariance for the multivariate gaussian used\n      to sample center of mass locations\n    n_samples : int\n      The number of samples of the center of mass location\n    threshold : float\n      The probability value at which to threshold\n      returned stable poses\n\n    Returns\n    -------\n    transforms : (n, 4, 4) float\n      The homogenous matrices that transform the\n      object to rest in a stable pose, with the\n      new z-axis pointing upwards from the table\n      and the object just touching the table.\n    probs : (n,) float\n      Probability in (0, 1) for each pose\n    \"\"\"\n\n    # save convex hull mesh to avoid a cache check\n    cvh = mesh.convex_hull\n\n    if center_mass is None:\n        center_mass = mesh.center_mass\n\n    # Sample center of mass, rejecting points outside of conv hull\n    sample_coms = []\n    while len(sample_coms) < n_samples:\n        remaining = n_samples - len(sample_coms)\n        coms = np.random.multivariate_normal(center_mass,\n                                             sigma * np.eye(3),\n                                             remaining)\n        for c in coms:\n            dots = np.einsum('ij,ij->i',\n                             c - cvh.triangles_center,\n                             cvh.face_normals)\n            if np.all(dots < 0):\n                sample_coms.append(c)\n\n    norms_to_probs = {}  # Map from normal to probabilities\n\n    # For each sample, compute the stable poses\n    for sample_com in sample_coms:\n\n        # Create toppling digraph\n        dg = _create_topple_graph(cvh, sample_com)\n\n        # Propagate probabilites to sink nodes with a breadth-first traversal\n        nodes = [n for n in dg.nodes() if dg.in_degree(n) == 0]\n        n_iters = 0\n        while len(nodes) > 0 and n_iters <= len(mesh.faces):\n            new_nodes = []\n            for node in nodes:\n                if dg.out_degree(node) == 0:\n                    continue\n                successor = next(iter(dg.successors(node)))\n                dg.node[successor]['prob'] += dg.node[node]['prob']\n                dg.node[node]['prob'] = 0.0\n                new_nodes.append(successor)\n            nodes = new_nodes\n            n_iters += 1\n\n        # Collect stable poses\n        for node in dg.nodes():\n            if dg.node[node]['prob'] > 0.0:\n                normal = cvh.face_normals[node]\n                prob = dg.node[node]['prob']\n                key = tuple(np.around(normal, decimals=3))\n                if key in norms_to_probs:\n                    norms_to_probs[key]['prob'] += 1.0 / n_samples * prob\n                else:\n                    norms_to_probs[key] = {\n                        'prob': 1.0 / n_samples * prob,\n                        'normal': normal\n                    }\n\n    transforms = []\n    probs = []\n\n    # Filter stable poses\n    for key in norms_to_probs:\n        prob = norms_to_probs[key]['prob']\n        if prob > threshold:\n            tf = np.eye(4)\n\n            # Compute a rotation matrix for this stable pose\n            z = -1.0 * norms_to_probs[key]['normal']\n            x = np.array([-z[1], z[0], 0])\n            if np.linalg.norm(x) == 0.0:\n                x = np.array([1, 0, 0])\n            else:\n                x = x / np.linalg.norm(x)\n            y = np.cross(z, x)\n            y = y / np.linalg.norm(y)\n            tf[:3, :3] = np.array([x, y, z])\n\n            # Compute the necessary translation for this stable pose\n            m = cvh.copy()\n            m.apply_transform(tf)\n            z = -m.bounds[0][2]\n            tf[:3, 3] = np.array([0, 0, z])\n\n            transforms.append(tf)\n            probs.append(prob)\n\n    # Sort the results\n    transforms = np.array(transforms)\n    probs = np.array(probs)\n    inds = np.argsort(-probs)\n\n    return transforms[inds], probs[inds]", "response": "This method computes the stable orientations of a mesh and its quasi - static probabilites."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _orient3dfast(plane, pd):\n    pa, pb, pc = plane\n    adx = pa[0] - pd[0]\n    bdx = pb[0] - pd[0]\n    cdx = pc[0] - pd[0]\n    ady = pa[1] - pd[1]\n    bdy = pb[1] - pd[1]\n    cdy = pc[1] - pd[1]\n    adz = pa[2] - pd[2]\n    bdz = pb[2] - pd[2]\n    cdz = pc[2] - pd[2]\n\n    return (adx * (bdy * cdz - bdz * cdy)\n            + bdx * (cdy * adz - cdz * ady)\n            + cdx * (ady * bdz - adz * bdy))", "response": "A fast 3D orientation test for the given three points in space that define a single point."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_static_prob(tri, com):\n    sv = [(v - com) / np.linalg.norm(v - com) for v in tri]\n\n    # Use L'Huilier's Formula to compute spherical area\n    a = np.arccos(min(1, max(-1, np.dot(sv[0], sv[1]))))\n    b = np.arccos(min(1, max(-1, np.dot(sv[1], sv[2]))))\n    c = np.arccos(min(1, max(-1, np.dot(sv[2], sv[0]))))\n    s = (a + b + c) / 2.0\n\n    # Prevents weirdness with arctan\n    try:\n        return 1.0 / np.pi * np.arctan(np.sqrt(np.tan(s / 2) * np.tan(\n            (s - a) / 2) * np.tan((s - b) / 2) * np.tan((s - c) / 2)))\n    except BaseException:\n        s = s + 1e-8\n        return 1.0 / np.pi * np.arctan(np.sqrt(np.tan(s / 2) * np.tan(\n            (s - a) / 2) * np.tan((s - b) / 2) * np.tan((s - c) / 2)))", "response": "Compute the probability that the given triangle is the first hit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_topple_graph(cvh_mesh, com):\n    adj_graph = nx.Graph()\n    topple_graph = nx.DiGraph()\n\n    # Create face adjacency graph\n    face_pairs = cvh_mesh.face_adjacency\n    edges = cvh_mesh.face_adjacency_edges\n\n    graph_edges = []\n    for fp, e in zip(face_pairs, edges):\n        verts = cvh_mesh.vertices[e]\n        graph_edges.append([fp[0], fp[1], {'verts': verts}])\n\n    adj_graph.add_edges_from(graph_edges)\n\n    # Compute static probabilities of landing on each face\n    for i, tri in enumerate(cvh_mesh.triangles):\n        prob = _compute_static_prob(tri, com)\n        topple_graph.add_node(i, prob=prob)\n\n    # Compute COM projections onto planes of each triangle in cvh_mesh\n    proj_dists = np.einsum('ij,ij->i', cvh_mesh.face_normals,\n                           com - cvh_mesh.triangles[:, 0])\n    proj_coms = com - np.einsum('i,ij->ij', proj_dists, cvh_mesh.face_normals)\n    barys = points_to_barycentric(cvh_mesh.triangles, proj_coms)\n    unstable_face_indices = np.where(np.any(barys < 0, axis=1))[0]\n\n    # For each unstable face, compute the face it topples to\n    for fi in unstable_face_indices:\n        proj_com = proj_coms[fi]\n        centroid = cvh_mesh.triangles_center[fi]\n        norm = cvh_mesh.face_normals[fi]\n\n        for tfi in adj_graph[fi]:\n            v1, v2 = adj_graph[fi][tfi]['verts']\n            if np.dot(np.cross(v1 - centroid, v2 - centroid), norm) < 0:\n                tmp = v2\n                v2 = v1\n                v1 = tmp\n            plane1 = [centroid, v1, v1 + norm]\n            plane2 = [centroid, v2 + norm, v2]\n            if _orient3dfast(plane1, proj_com) >= 0 and _orient3dfast(\n                    plane2, proj_com) >= 0:\n                break\n\n        topple_graph.add_edge(fi, tfi)\n\n    return topple_graph", "response": "Creates a toppling digraph for the given convex hull mesh and target shape."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a permutated version of a Trimesh object with random rotatating and translating a random mesh.", "response": "def transform(mesh, translation_scale=1000.0):\n    \"\"\"\n    Return a permutated variant of a mesh by randomly reording faces\n    and rotatating + translating a mesh by a random matrix.\n\n    Parameters\n    ----------\n    mesh:   Trimesh object (input will not be altered by this function)\n\n    Returns\n    ----------\n    permutated: Trimesh object, same faces as input mesh but\n                rotated and reordered.\n    \"\"\"\n    matrix = transformations.random_rotation_matrix()\n    matrix[0:3, 3] = np.random.random(3) * translation_scale\n\n    triangles = np.random.permutation(mesh.triangles).reshape((-1, 3))\n    triangles = transformations.transform_points(triangles, matrix)\n\n    mesh_type = util.type_named(mesh, 'Trimesh')\n    permutated = mesh_type(\n        **triangles_module.to_kwargs(triangles.reshape((-1, 3, 3))))\n\n    return permutated"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds gaussian noise to every vertex of a mesh.", "response": "def noise(mesh, magnitude=None):\n    \"\"\"\n    Add gaussian noise to every vertex of a mesh.\n    Makes no effort to maintain topology or sanity.\n\n    Parameters\n    ----------\n    mesh:      Trimesh object (will not be mutated)\n    magnitude: float, what is the maximum distance per axis we can displace a vertex.\n               Default value is mesh.scale/100.0\n\n    Returns\n    ----------\n    permutated: Trimesh object, input mesh with noise applied\n    \"\"\"\n    if magnitude is None:\n        magnitude = mesh.scale / 100.0\n\n    random = (np.random.random(mesh.vertices.shape) - .5) * magnitude\n    vertices_noise = mesh.vertices.copy() + random\n\n    # make sure we've re- ordered faces randomly\n    triangles = np.random.permutation(vertices_noise[mesh.faces])\n\n    mesh_type = util.type_named(mesh, 'Trimesh')\n    permutated = mesh_type(**triangles_module.to_kwargs(triangles))\n\n    return permutated"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef tessellation(mesh):\n    # create random barycentric coordinates for each face\n    # pad all coordinates by a small amount to bias new vertex towards center\n    barycentric = np.random.random(mesh.faces.shape) + .05\n    barycentric /= barycentric.sum(axis=1).reshape((-1, 1))\n\n    # create one new vertex somewhere in a face\n    vertex_face = (barycentric.reshape((-1, 3, 1))\n                   * mesh.triangles).sum(axis=1)\n    vertex_face_id = np.arange(len(vertex_face)) + len(mesh.vertices)\n\n    # new vertices are the old vertices stacked on the vertices in the faces\n    vertices = np.vstack((mesh.vertices, vertex_face))\n    # there are three new faces per old face, and we maintain correct winding\n    faces = np.vstack((np.column_stack((mesh.faces[:, [0, 1]], vertex_face_id)),\n                       np.column_stack(\n                           (mesh.faces[:, [1, 2]], vertex_face_id)),\n                       np.column_stack((mesh.faces[:, [2, 0]], vertex_face_id))))\n    # make sure the order of the faces is permutated\n    faces = np.random.permutation(faces)\n\n    mesh_type = util.type_named(mesh, 'Trimesh')\n    permutated = mesh_type(vertices=vertices,\n                           faces=faces)\n    return permutated", "response": "Returns a Trimesh object with the same surface area and volume as the input mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_ply(file_obj,\n             resolver=None,\n             fix_texture=True,\n             *args,\n             **kwargs):\n    \"\"\"\n    Load a PLY file from an open file object.\n\n    Parameters\n    ---------\n    file_obj : an open file- like object\n      Source data, ASCII or binary PLY\n    resolver : trimesh.visual.resolvers.Resolver\n      Object which can resolve assets\n    fix_texture : bool\n      If True, will re- index vertices and faces\n      so vertices with different UV coordinates\n      are disconnected.\n\n    Returns\n    ---------\n    mesh_kwargs : dict\n      Data which can be passed to\n      Trimesh constructor, eg: a = Trimesh(**mesh_kwargs)\n    \"\"\"\n\n    # OrderedDict which is populated from the header\n    elements, is_ascii, image_name = parse_header(file_obj)\n\n    # functions will fill in elements from file_obj\n    if is_ascii:\n        ply_ascii(elements, file_obj)\n    else:\n        ply_binary(elements, file_obj)\n\n    # try to load the referenced image\n    image = None\n    if image_name is not None:\n        try:\n            data = resolver.get(image_name)\n            image = PIL.Image.open(util.wrap_as_stream(data))\n        except BaseException:\n            log.warning('unable to load image!',\n                        exc_info=True)\n\n    kwargs = elements_to_kwargs(elements,\n                                fix_texture=fix_texture,\n                                image=image)\n\n    return kwargs", "response": "Loads a PLY file into a Trimesh object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexports a Trimesh object in the PLY format.", "response": "def export_ply(mesh,\n               encoding='binary',\n               vertex_normal=None):\n    \"\"\"\n    Export a mesh in the PLY format.\n\n    Parameters\n    ----------\n    mesh : Trimesh object\n    encoding : ['ascii'|'binary_little_endian']\n    vertex_normal : include vertex normals\n\n    Returns\n    ----------\n    export : bytes of result\n    \"\"\"\n    # evaluate input args\n    # allow a shortcut for binary\n    if encoding == 'binary':\n        encoding = 'binary_little_endian'\n    elif encoding not in ['binary_little_endian', 'ascii']:\n        raise ValueError('encoding must be binary or ascii')\n    # if vertex normals aren't specifically asked for\n    # only export them if they are stored in cache\n    if vertex_normal is None:\n        vertex_normal = 'vertex_normal' in mesh._cache\n\n    # custom numpy dtypes for exporting\n    dtype_face = [('count', '<u1'),\n                  ('index', '<i4', (3))]\n    dtype_vertex = [('vertex', '<f4', (3))]\n    # will be appended to main dtype if needed\n    dtype_vertex_normal = ('normals', '<f4', (3))\n    dtype_color = ('rgba', '<u1', (4))\n\n    # get template strings in dict\n    templates = json.loads(get_resource('ply.template'))\n    # start collecting elements into a string for the header\n    header = templates['intro']\n    header += templates['vertex']\n\n    # if we're exporting vertex normals add them\n    # to the header and dtype\n    if vertex_normal:\n        header += templates['vertex_normal']\n        dtype_vertex.append(dtype_vertex_normal)\n\n    # if mesh has a vertex coloradd it to the header\n    if mesh.visual.kind == 'vertex' and encoding != 'ascii':\n        header += templates['color']\n        dtype_vertex.append(dtype_color)\n\n    # create and populate the custom dtype for vertices\n    vertex = np.zeros(len(mesh.vertices),\n                      dtype=dtype_vertex)\n    vertex['vertex'] = mesh.vertices\n    if vertex_normal:\n        vertex['normals'] = mesh.vertex_normals\n    if mesh.visual.kind == 'vertex':\n        vertex['rgba'] = mesh.visual.vertex_colors\n\n    header += templates['face']\n    if mesh.visual.kind == 'face' and encoding != 'ascii':\n        header += templates['color']\n        dtype_face.append(dtype_color)\n\n    # put mesh face data into custom dtype to export\n    faces = np.zeros(len(mesh.faces), dtype=dtype_face)\n    faces['count'] = 3\n    faces['index'] = mesh.faces\n    if mesh.visual.kind == 'face' and encoding != 'ascii':\n        faces['rgba'] = mesh.visual.face_colors\n\n    header += templates['outro']\n\n    header_params = {'vertex_count': len(mesh.vertices),\n                     'face_count': len(mesh.faces),\n                     'encoding': encoding}\n\n    export = Template(header).substitute(header_params).encode('utf-8')\n\n    if encoding == 'binary_little_endian':\n        export += vertex.tostring()\n        export += faces.tostring()\n    elif encoding == 'ascii':\n        # ply format is: (face count, v0, v1, v2)\n        fstack = np.column_stack((np.ones(len(mesh.faces),\n                                          dtype=np.int64) * 3,\n                                  mesh.faces))\n\n        # if we're exporting vertex normals they get stacked\n        if vertex_normal:\n            vstack = np.column_stack((mesh.vertices,\n                                      mesh.vertex_normals))\n        else:\n            vstack = mesh.vertices\n\n        # add the string formatted vertices and faces\n        export += (util.array_to_string(vstack,\n                                        col_delim=' ',\n                                        row_delim='\\n') +\n                   '\\n' +\n                   util.array_to_string(fstack,\n                                        col_delim=' ',\n                                        row_delim='\\n')).encode('utf-8')\n    else:\n        raise ValueError('encoding must be ascii or binary!')\n\n    return export"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the ASCII header of a PLY file and returns the elements and data types populated by the elements.", "response": "def parse_header(file_obj):\n    \"\"\"\n    Read the ASCII header of a PLY file, and leave the file object\n    at the position of the start of data but past the header.\n\n    Parameters\n    -----------\n    file_obj : open file object\n      Positioned at the start of the file\n\n    Returns\n    -----------\n    elements : collections.OrderedDict\n      Fields and data types populated\n    is_ascii : bool\n      Whether the data is ASCII or binary\n    image_name : None or str\n      File name of TextureFile\n    \"\"\"\n\n    if 'ply' not in str(file_obj.readline()):\n        raise ValueError('not a ply file!')\n\n    # collect the encoding: binary or ASCII\n    encoding = file_obj.readline().decode('utf-8').strip().lower()\n    is_ascii = 'ascii' in encoding\n\n    # big or little endian\n    endian = ['<', '>'][int('big' in encoding)]\n    elements = collections.OrderedDict()\n\n    # store file name of TextureFiles in the header\n    image_name = None\n\n    while True:\n        line = file_obj.readline()\n        if line is None:\n            raise ValueError(\"Header not terminated properly!\")\n        line = line.decode('utf-8').strip().split()\n\n        # we're done\n        if 'end_header' in line:\n            break\n\n        # elements are groups of properties\n        if 'element' in line[0]:\n            # we got a new element so add it\n            name, length = line[1:]\n            elements[name] = {\n                'length': int(length),\n                'properties': collections.OrderedDict()}\n        # a property is a member of an element\n        elif 'property' in line[0]:\n            # is the property a simple single value, like:\n            # `propert float x`\n            if len(line) == 3:\n                dtype, field = line[1:]\n                elements[name]['properties'][\n                    str(field)] = endian + dtypes[dtype]\n            # is the property a painful list, like:\n            # `property list uchar int vertex_indices`\n            elif 'list' in line[1]:\n                dtype_count, dtype, field = line[2:]\n                elements[name]['properties'][\n                    str(field)] = (\n                    endian +\n                    dtypes[dtype_count] +\n                    ', ($LIST,)' +\n                    endian +\n                    dtypes[dtype])\n        # referenced as a file name\n        elif 'TextureFile' in line:\n            # textures come listed like:\n            # `comment TextureFile fuze_uv.jpg`\n            index = line.index('TextureFile') + 1\n            if index < len(line):\n                image_name = line[index]\n\n    return elements, is_ascii, image_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives an elements data structure extract the keyword arguments that a Trimesh object constructor will expect.", "response": "def elements_to_kwargs(elements, fix_texture, image):\n    \"\"\"\n    Given an elements data structure, extract the keyword\n    arguments that a Trimesh object constructor will expect.\n\n    Parameters\n    ------------\n    elements: OrderedDict object, with fields and data loaded\n\n    Returns\n    -----------\n    kwargs: dict, with keys for Trimesh constructor.\n            eg: mesh = trimesh.Trimesh(**kwargs)\n    \"\"\"\n\n    kwargs = {'metadata': {'ply_raw': elements}}\n\n    vertices = np.column_stack([elements['vertex']['data'][i]\n                                for i in 'xyz'])\n\n    if not util.is_shape(vertices, (-1, 3)):\n        raise ValueError('Vertices were not (n,3)!')\n\n    try:\n        face_data = elements['face']['data']\n    except (KeyError, ValueError):\n        # some PLY files only include vertices\n        face_data = None\n        faces = None\n\n    # what keys do in-the-wild exporters use for vertices\n    index_names = ['vertex_index',\n                   'vertex_indices']\n    texcoord = None\n\n    if util.is_shape(face_data, (-1, (3, 4))):\n        faces = face_data\n    elif isinstance(face_data, dict):\n        # get vertex indexes\n        for i in index_names:\n            if i in face_data:\n                faces = face_data[i]\n                break\n        # if faces have UV coordinates defined use them\n        if 'texcoord' in face_data:\n            texcoord = face_data['texcoord']\n\n    elif isinstance(face_data, np.ndarray):\n        face_blob = elements['face']['data']\n        # some exporters set this name to 'vertex_index'\n        # and some others use 'vertex_indices' but we really\n        # don't care about the name unless there are multiple\n        if len(face_blob.dtype.names) == 1:\n            name = face_blob.dtype.names[0]\n        elif len(face_blob.dtype.names) > 1:\n            # loop through options\n            for i in face_blob.dtype.names:\n                if i in index_names:\n                    name = i\n                    break\n        # get faces\n        faces = face_blob[name]['f1']\n\n        try:\n            texcoord = face_blob['texcoord']['f1']\n        except (ValueError, KeyError):\n            # accessing numpy arrays with named fields\n            # incorrectly is a ValueError\n            pass\n\n    # PLY stores texture coordinates per- face which is\n    # slightly annoying, as we have to then figure out\n    # which vertices have the same position but different UV\n    expected = (faces.shape[0], faces.shape[1] * 2)\n    if (image is not None and\n        texcoord is not None and\n            texcoord.shape == expected):\n\n        # vertices with the same position but different\n        # UV coordinates can't be merged without it\n        # looking like it went through a woodchipper\n        # in- the- wild PLY comes with things merged that\n        # probably shouldn't be so disconnect vertices\n        if fix_texture:\n            # reshape to correspond with flattened faces\n            uv = texcoord.reshape((-1, 2))\n\n            # round UV to OOM 10^4 as they are pixel coordinates\n            # and more precision is not necessary or desirable\n            search = np.column_stack((\n                vertices[faces.reshape(-1)],\n                (uv * 1e4).round()))\n\n            # find vertices which have the same position AND UV\n            unique, inverse = grouping.unique_rows(search)\n\n            # set vertices, faces, and UV to the new values\n            vertices = search[:, :3][unique]\n            faces = inverse.reshape((-1, 3))\n            uv = uv[unique]\n        else:\n            # don't alter vertices, UV will look like crap\n            # if it was exported with vertices merged\n            uv = np.zeros((len(vertices), 2))\n            uv[faces.reshape(-1)] = texcoord.reshape((-1, 2))\n\n        # create the visuals object for the texture\n        kwargs['visual'] = visual.texture.TextureVisuals(\n            uv=uv, image=image)\n\n    # kwargs for Trimesh or PointCloud\n    kwargs.update({'faces': faces,\n                   'vertices': vertices})\n\n    # if both vertex and face color are defined pick the one\n    # with the most going on\n    colors = []\n    signal = []\n    if kwargs['faces'] is not None:\n        f_color, f_signal = element_colors(elements['face'])\n        colors.append({'face_colors': f_color})\n        signal.append(f_signal)\n    if kwargs['vertices'] is not None:\n        v_color, v_signal = element_colors(elements['vertex'])\n        colors.append({'vertex_colors': v_color})\n        signal.append(v_signal)\n\n    # add the winning colors to the result\n    kwargs.update(colors[np.argmax(signal)])\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef element_colors(element):\n    keys = ['red', 'green', 'blue', 'alpha']\n    candidate_colors = [element['data'][i]\n                        for i in keys if i in element['properties']]\n\n    if len(candidate_colors) >= 3:\n        colors = np.column_stack(candidate_colors)\n        signal = colors.ptp(axis=0).sum()\n        return colors, signal\n\n    return None, 0.0", "response": "Given an element try to extract RGBA color from the properties and return them as an array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ply_ascii(elements, file_obj):\n\n    # get the file contents as a string\n    text = str(file_obj.read().decode('utf-8'))\n\n    # split by newlines\n    lines = str.splitlines(text)\n\n    # get each line as an array split by whitespace\n    array = np.array([np.fromstring(i, sep=' ')\n                      for i in lines])\n\n    # store the line position in the file\n    position = 0\n\n    # loop through data we need\n    for key, values in elements.items():\n        # will store (start, end) column index of data\n        columns = collections.deque()\n        # will store the total number of rows\n        rows = 0\n\n        for name, dtype in values['properties'].items():\n            if '$LIST' in dtype:\n                # if an element contains a list property handle it here\n\n                row = array[position]\n                list_count = int(row[rows])\n\n                # ignore the count and take the data\n                columns.append([rows + 1,\n                                rows + 1 + list_count])\n                rows += list_count + 1\n                # change the datatype to just the dtype for data\n\n                values['properties'][name] = dtype.split('($LIST,)')[-1]\n            else:\n                # a single column data field\n                columns.append([rows, rows + 1])\n                rows += 1\n\n        # get the lines as a 2D numpy array\n        data = np.vstack(array[position:position + values['length']])\n        # offset position in file\n        position += values['length']\n\n        # store columns we care about by name and convert to data type\n        elements[key]['data'] = {n: data[:, c[0]:c[1]].astype(dt)\n                                 for n, dt, c in zip(\n            values['properties'].keys(),    # field name\n            values['properties'].values(),  # data type of field\n            columns)}", "response": "Load data from an ASCII PLY file into an existing elements data structure."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ply_binary(elements, file_obj):\n\n    def populate_listsize(file_obj, elements):\n        \"\"\"\n        Given a set of elements populated from the header if there are any\n        list properties seek in the file the length of the list.\n\n        Note that if you have a list where each instance is different length\n        (if for example you mixed triangles and quads) this won't work at all\n        \"\"\"\n        p_start = file_obj.tell()\n        p_current = file_obj.tell()\n        for element_key, element in elements.items():\n            props = element['properties']\n            prior_data = ''\n            for k, dtype in props.items():\n                if '$LIST' in dtype:\n                    # every list field has two data types:\n                    # the list length (single value), and the list data (multiple)\n                    # here we are only reading the single value for list length\n                    field_dtype = np.dtype(dtype.split(',')[0])\n                    if len(prior_data) == 0:\n                        offset = 0\n                    else:\n                        offset = np.dtype(prior_data).itemsize\n                    file_obj.seek(p_current + offset)\n                    size = np.frombuffer(file_obj.read(field_dtype.itemsize),\n                                         dtype=field_dtype)[0]\n                    props[k] = props[k].replace('$LIST', str(size))\n                prior_data += props[k] + ','\n            itemsize = np.dtype(', '.join(props.values())).itemsize\n            p_current += element['length'] * itemsize\n        file_obj.seek(p_start)\n\n    def populate_data(file_obj, elements):\n        \"\"\"\n        Given the data type and field information from the header,\n        read the data and add it to a 'data' field in the element.\n        \"\"\"\n        for key in elements.keys():\n            items = list(elements[key]['properties'].items())\n            dtype = np.dtype(items)\n            data = file_obj.read(elements[key]['length'] * dtype.itemsize)\n            elements[key]['data'] = np.frombuffer(data,\n                                                  dtype=dtype)\n        return elements\n\n    def elements_size(elements):\n        \"\"\"\n        Given an elements data structure populated from the header,\n        calculate how long the file should be if it is intact.\n        \"\"\"\n        size = 0\n        for element in elements.values():\n            dtype = np.dtype(','.join(element['properties'].values()))\n            size += element['length'] * dtype.itemsize\n        return size\n\n    # some elements are passed where the list dimensions\n    # are not included in the header, so this function goes\n    # into the meat of the file and grabs the list dimensions\n    # before we to the main data read as a single operation\n    populate_listsize(file_obj, elements)\n\n    # how many bytes are left in the file\n    size_file = util.distance_to_end(file_obj)\n    # how many bytes should the data structure described by\n    # the header take up\n    size_elements = elements_size(elements)\n\n    # if the number of bytes is not the same the file is probably corrupt\n    if size_file != size_elements:\n        raise ValueError('File is unexpected length!')\n\n    # with everything populated and a reasonable confidence the file\n    # is intact, read the data fields described by the header\n    populate_data(file_obj, elements)", "response": "Load the data from a binary PLY file into the elements data structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports a Trimesh object to a Draco compressed format.", "response": "def export_draco(mesh):\n    \"\"\"\n    Export a mesh using Google's Draco compressed format.\n\n    Only works if draco_encoder is in your PATH:\n    https://github.com/google/draco\n\n    Parameters\n    ----------\n    mesh : Trimesh object\n\n    Returns\n    ----------\n    data : str or bytes\n      DRC file bytes\n    \"\"\"\n    with tempfile.NamedTemporaryFile(suffix='.ply') as temp_ply:\n        temp_ply.write(export_ply(mesh))\n        temp_ply.flush()\n        with tempfile.NamedTemporaryFile(suffix='.drc') as encoded:\n            subprocess.check_output([draco_encoder,\n                                     '-qp',  # bits of quantization for position\n                                     '28',  # since our tol.merge is 1e-8, 25 bits\n                                            # more has a machine epsilon\n                                            # smaller than that\n                                     '-i',\n                                     temp_ply.name,\n                                     '-o',\n                                     encoded.name])\n            encoded.seek(0)\n            data = encoded.read()\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_draco(file_obj, **kwargs):\n\n    with tempfile.NamedTemporaryFile(suffix='.drc') as temp_drc:\n        temp_drc.write(file_obj.read())\n        temp_drc.flush()\n\n        with tempfile.NamedTemporaryFile(suffix='.ply') as temp_ply:\n            subprocess.check_output([draco_decoder,\n                                     '-i',\n                                     temp_drc.name,\n                                     '-o',\n                                     temp_ply.name])\n            temp_ply.seek(0)\n            kwargs = load_ply(temp_ply)\n    return kwargs", "response": "Loads a Trimesh object from Google s Draco format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convex_decomposition(mesh, **kwargs):\n    # decompositions require testVHACD\n    if interfaces.vhacd.exists:\n        return interfaces.vhacd.convex_decomposition(mesh, **kwargs)\n    else:\n        raise ValueError('convex compositions require testVHACD installed!')", "response": "Compute an approximate convex decomposition of a mesh."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef boolean_automatic(meshes, operation):\n    if interfaces.blender.exists:\n        result = interfaces.blender.boolean(meshes, operation)\n    elif interfaces.scad.exists:\n        result = interfaces.scad.boolean(meshes, operation)\n    else:\n        raise ValueError('No backends available for boolean operations!')\n    return result", "response": "Automatic pick an engine for booleans based on availability."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subdivide(vertices,\n              faces,\n              face_index=None):\n    \"\"\"\n    Subdivide a mesh into smaller triangles.\n\n    Note that if `face_index` is passed, only those faces will\n    be subdivided and their neighbors won't be modified making\n    the mesh no longer \"watertight.\"\n\n    Parameters\n    ----------\n    vertices : (n, 3) float\n      Vertices in space\n    faces : (n, 3) int\n      Indexes of vertices which make up triangular faces\n    face_index : faces to subdivide.\n      if None: all faces of mesh will be subdivided\n      if (n,) int array of indices: only specified faces\n\n    Returns\n    ----------\n    new_vertices : (n, 3) float\n      Vertices in space\n    new_faces : (n, 3) int\n      Remeshed faces\n    \"\"\"\n    if face_index is None:\n        face_index = np.arange(len(faces))\n    else:\n        face_index = np.asanyarray(face_index)\n\n    # the (c,3) int set of vertex indices\n    faces = faces[face_index]\n    # the (c, 3, 3) float set of points in the triangles\n    triangles = vertices[faces]\n    # the 3 midpoints of each triangle edge\n    # stacked to a (3 * c, 3) float\n    mid = np.vstack([triangles[:, g, :].mean(axis=1)\n                     for g in [[0, 1],\n                               [1, 2],\n                               [2, 0]]])\n\n    # for adjacent faces we are going to be generating\n    # the same midpoint twice so merge them here\n    mid_idx = (np.arange(len(face_index) * 3)).reshape((3, -1)).T\n    unique, inverse = grouping.unique_rows(mid)\n    mid = mid[unique]\n    mid_idx = inverse[mid_idx] + len(vertices)\n\n    # the new faces with correct winding\n    f = np.column_stack([faces[:, 0],\n                         mid_idx[:, 0],\n                         mid_idx[:, 2],\n                         mid_idx[:, 0],\n                         faces[:, 1],\n                         mid_idx[:, 1],\n                         mid_idx[:, 2],\n                         mid_idx[:, 1],\n                         faces[:, 2],\n                         mid_idx[:, 0],\n                         mid_idx[:, 1],\n                         mid_idx[:, 2]]).reshape((-1, 3))\n    # add the 3 new faces per old face\n    new_faces = np.vstack((faces, f[len(face_index):]))\n    # replace the old face with a smaller face\n    new_faces[face_index] = f[:len(face_index)]\n\n    new_vertices = np.vstack((vertices, mid))\n\n    return new_vertices, new_faces", "response": "Subdivide a mesh into smaller triangles."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef subdivide_to_size(vertices,\n                      faces,\n                      max_edge,\n                      max_iter=10):\n    \"\"\"\n    Subdivide a mesh until every edge is shorter than a\n    specified length.\n\n    Will return a triangle soup, not a nicely structured mesh.\n\n    Parameters\n    ------------\n    vertices : (n, 3) float\n      Vertices in space\n    faces : (m, 3) int\n      Indices of vertices which make up triangles\n    max_edge : float\n      Maximum length of any edge in the result\n    max_iter : int\n      The maximum number of times to run subdivision\n\n    Returns\n    ------------\n    vertices : (j, 3) float\n      Vertices in space\n    faces : (q, 3) int\n      Indices of vertices\n    \"\"\"\n    # store completed\n    done_face = []\n    done_vert = []\n\n    # copy inputs and make sure dtype is correct\n    current_faces = np.array(faces,\n                             dtype=np.int64,\n                             copy=True)\n    current_vertices = np.array(vertices,\n                                dtype=np.float64,\n                                copy=True)\n\n    # loop through iteration cap\n    for i in range(max_iter + 1):\n        # (n, 3, 3) float triangle soup\n        triangles = current_vertices[current_faces]\n\n        # compute the length of every triangle edge\n        edge_lengths = (np.diff(triangles[:, [0, 1, 2, 0]],\n                                axis=1) ** 2).sum(axis=2) ** .5\n        too_long = (edge_lengths > max_edge).any(axis=1)\n\n        # clean up the faces a little bit so we don't\n        # store a ton of unused vertices\n        unique, inverse = np.unique(\n            current_faces[np.logical_not(too_long)],\n            return_inverse=True)\n\n        # store vertices and faces meeting criteria\n        done_vert.append(current_vertices[unique])\n        done_face.append(inverse.reshape((-1, 3)))\n\n        # met our goals so abort\n        if not too_long.any():\n            break\n\n        # run subdivision again\n        (current_vertices,\n         current_faces) = subdivide(current_vertices,\n                                    current_faces[too_long])\n\n    # stack sequence into nice (n, 3) arrays\n    vertices, faces = util.append_faces(done_vert,\n                                        done_face)\n\n    return vertices, faces", "response": "Subdivide a mesh until every edge is shorter than max_edge."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_dxf(file_obj, **kwargs):\n\n    def info(e):\n        \"\"\"\n        Pull metadata based on group code, and return as a dict.\n        \"\"\"\n        # which keys should we extract from the entity data\n        # DXF group code : our metadata key\n        get = {'8': 'layer'}\n\n        # replace group codes with names and only\n        # take info from the entity dict if it is in cand\n        renamed = {get[k]: util.make_sequence(v)[0] for k,\n                   v in e.items() if k in get}\n\n        return renamed\n\n    def convert_line(e):\n        \"\"\"\n        Convert DXF LINE entities into trimesh Line entities.\n        \"\"\"\n        # create a single Line entity\n        entities.append(Line(points=len(vertices) + np.arange(2),\n                             **info(e)))\n        # add the vertices to our collection\n        vertices.extend(np.array([[e['10'], e['20']],\n                                  [e['11'], e['21']]],\n                                 dtype=np.float64))\n\n    def convert_circle(e):\n        \"\"\"\n        Convert DXF CIRCLE entities into trimesh Circle entities\n        \"\"\"\n        R = float(e['40'])\n        C = np.array([e['10'],\n                      e['20']]).astype(np.float64)\n        points = to_threepoint(center=C[0:2],\n                               radius=R)\n        entities.append(Arc(points=(len(vertices) + np.arange(3)),\n                            closed=True,\n                            **info(e)))\n        vertices.extend(points)\n\n    def convert_arc(e):\n        \"\"\"\n        Convert DXF ARC entities into into trimesh Arc entities.\n        \"\"\"\n        # the radius of the circle\n        R = float(e['40'])\n        # the center point of the circle\n        C = np.array([e['10'],\n                      e['20']], dtype=np.float64)\n        # the start and end angle of the arc, in degrees\n        # this may depend on an AUNITS header data\n        A = np.radians(np.array([e['50'],\n                                 e['51']], dtype=np.float64))\n        # convert center/radius/angle representation\n        # to three points on the arc representation\n        points = to_threepoint(center=C[0:2],\n                               radius=R,\n                               angles=A)\n        # add a single Arc entity\n        entities.append(Arc(points=len(vertices) + np.arange(3),\n                            closed=False,\n                            **info(e)))\n        # add the three vertices\n        vertices.extend(points)\n\n    def convert_polyline(e):\n        \"\"\"\n        Convert DXF LWPOLYLINE entities into trimesh Line entities.\n        \"\"\"\n        # load the points in the line\n        lines = np.column_stack((\n            e['10'], e['20'])).astype(np.float64)\n\n        # save entity info so we don't have to recompute\n        polyinfo = info(e)\n\n        # 70 is the closed flag for polylines\n        # if the closed flag is set make sure to close\n        is_closed = '70' in e and int(e['70'][0]) & 1\n        if is_closed:\n            lines = np.vstack((lines, lines[:1]))\n\n        # 42 is the vertex bulge flag for LWPOLYLINE entities\n        # \"bulge\" is autocad for \"add a stupid arc using flags\n        # in my otherwise normal polygon\", it's like SVG arc\n        # flags but somehow even more annoying\n        if '42' in e:\n            # get the actual bulge float values\n            bulge = np.array(e['42'], dtype=np.float64)\n            # what position were vertices stored at\n            vid = np.nonzero(chunk[:, 0] == '10')[0]\n            # what position were bulges stored at in the chunk\n            bid = np.nonzero(chunk[:, 0] == '42')[0]\n            # filter out endpoint bulge if we're not closed\n            if not is_closed:\n                bid_ok = bid < vid.max()\n                bid = bid[bid_ok]\n                bulge = bulge[bid_ok]\n            # which vertex index is bulge value associated with\n            bulge_idx = np.searchsorted(vid, bid)\n            # convert stupid bulge to Line/Arc entities\n            v, e = bulge_to_arcs(lines=lines,\n                                 bulge=bulge,\n                                 bulge_idx=bulge_idx,\n                                 is_closed=is_closed)\n            for i in e:\n                # offset added entities by current vertices length\n                i.points += len(vertices)\n            vertices.extend(v)\n            entities.extend(e)\n            # done with this polyline\n            return\n\n        # we have a normal polyline so just add it\n        # as single line entity and vertices\n        entities.append(Line(\n            points=np.arange(len(lines)) + len(vertices),\n            **polyinfo))\n        vertices.extend(lines)\n\n    def convert_bspline(e):\n        \"\"\"\n        Convert DXF Spline entities into trimesh BSpline entities.\n        \"\"\"\n        # in the DXF there are n points and n ordered fields\n        # with the same group code\n\n        points = np.column_stack((e['10'],\n                                  e['20'])).astype(np.float64)\n        knots = np.array(e['40']).astype(np.float64)\n\n        # if there are only two points, save it as a line\n        if len(points) == 2:\n            # create a single Line entity\n            entities.append(Line(points=len(vertices) +\n                                 np.arange(2),\n                                 **info(e)))\n            # add the vertices to our collection\n            vertices.extend(points)\n            return\n\n        # check bit coded flag for closed\n        # closed = bool(int(e['70'][0]) & 1)\n        # check euclidean distance to see if closed\n        closed = np.linalg.norm(points[0] -\n                                points[-1]) < tol.merge\n\n        # create a BSpline entity\n        entities.append(BSpline(\n            points=np.arange(len(points)) + len(vertices),\n            knots=knots,\n            closed=closed,\n            **info(e)))\n        # add the vertices\n        vertices.extend(points)\n\n    def convert_text(e):\n        \"\"\"\n        Convert a DXF TEXT entity into a native text entity.\n        \"\"\"\n        if '50' in e:\n            # rotation angle converted to radians\n            angle = np.radians(float(e['50']))\n        else:\n            # otherwise no rotation\n            angle = 0.0\n\n        # text with leading and trailing whitespace removed\n        text = e['1'].strip()\n\n        # height of text\n        if '40' in e:\n            height = float(e['40'])\n        else:\n            height = None\n\n        # origin point\n        origin = np.array([e['10'],\n                           e['20']]).astype(np.float64)\n\n        # an origin- relative point (so transforms work)\n        vector = origin + [np.cos(angle), np.sin(angle)]\n\n        # try to extract a (horizontal, vertical) text alignment\n        align = ['center', 'center']\n        try:\n            align[0] = ['left', 'center', 'right'][int(e['72'])]\n        except BaseException:\n            pass\n\n        # append the entity\n        entities.append(Text(origin=len(vertices),\n                             vector=len(vertices) + 1,\n                             height=height,\n                             text=text,\n                             align=align))\n        # append the text origin and direction\n        vertices.append(origin)\n        vertices.append(vector)\n\n    # in a DXF file, lines come in pairs,\n    # a group code then the next line is the value\n    # we are removing all whitespace then splitting with the\n    # splitlines function which uses the universal newline method\n    raw = file_obj.read()\n    # if we've been passed bytes\n    if hasattr(raw, 'decode'):\n        # search for the sentinel string indicating binary DXF\n        # do it by encoding sentinel to bytes and subset searching\n        if raw[:22].find(b'AutoCAD Binary DXF') != -1:\n            if _teigha is None:\n                # no converter to ASCII DXF available\n                raise ValueError('binary DXF not supported!')\n            else:\n                # convert binary DXF to R14 ASCII DXF\n                raw = _teigha_convert(raw, extension='dxf')\n        else:\n            # we've been passed bytes that don't have the\n            # header for binary DXF so try decoding as UTF-8\n            raw = raw.decode('utf-8', errors='ignore')\n\n    # remove trailing whitespace\n    raw = str(raw).strip()\n    # without any spaces and in upper case\n    cleaned = raw.replace(' ', '').strip().upper()\n\n    # blob with spaces and original case\n    blob_raw = np.array(str.splitlines(raw)).reshape((-1, 2))\n    # if this reshape fails, it means the DXF is malformed\n    blob = np.array(str.splitlines(cleaned)).reshape((-1, 2))\n\n    # get the section which contains the header in the DXF file\n    endsec = np.nonzero(blob[:, 1] == 'ENDSEC')[0]\n\n    # get the section which contains entities in the DXF file\n    entity_start = np.nonzero(blob[:, 1] == 'ENTITIES')[0][0]\n    entity_end = endsec[np.searchsorted(endsec, entity_start)]\n    entity_blob = blob[entity_start:entity_end]\n    # store the entity blob with original case\n    entity_raw = blob_raw[entity_start:entity_end]\n\n    # store metadata\n    metadata = {}\n\n    # try reading the header, which may be malformed\n    header_start = np.nonzero(blob[:, 1] == 'HEADER')[0]\n    if len(header_start) > 0:\n        header_end = endsec[np.searchsorted(endsec, header_start[0])]\n        header_blob = blob[header_start[0]:header_end]\n\n        # store some properties from the DXF header\n        metadata['DXF_HEADER'] = {}\n        for key, group in [('$ACADVER', '1'),\n                           ('$DIMSCALE', '40'),\n                           ('$DIMALT', '70'),\n                           ('$DIMALTF', '40'),\n                           ('$DIMUNIT', '70'),\n                           ('$INSUNITS', '70'),\n                           ('$LUNITS', '70')]:\n            value = get_key(header_blob,\n                            key,\n                            group)\n            if value is not None:\n                metadata['DXF_HEADER'][key] = value\n\n        # store unit data pulled from the header of the DXF\n        # prefer LUNITS over INSUNITS\n        # I couldn't find a table for LUNITS values but they\n        # look like they are 0- indexed versions of\n        # the INSUNITS keys, so for now offset the key value\n        for offset, key in [(-1, '$LUNITS'),\n                            (0, '$INSUNITS')]:\n            # get the key from the header blob\n            units = get_key(header_blob, key, '70')\n            # if it exists add the offset\n            if units is None:\n                continue\n            metadata[key] = units\n            units += offset\n            # if the key is in our list of units store it\n            if units in _DXF_UNITS:\n                metadata['units'] = _DXF_UNITS[units]\n        # warn on drawings with no units\n        if 'units' not in metadata:\n            log.warning('DXF doesn\\'t have units specified!')\n\n    # find the start points of entities\n    group_check = entity_blob[:, 0] == '0'\n    inflection = np.nonzero(group_check)[0]\n\n    # DXF object to trimesh object converters\n    loaders = {'LINE': (dict, convert_line),\n               'LWPOLYLINE': (util.multi_dict, convert_polyline),\n               'ARC': (dict, convert_arc),\n               'CIRCLE': (dict, convert_circle),\n               'SPLINE': (util.multi_dict, convert_bspline)}\n\n    # store loaded vertices\n    vertices = []\n    # store loaded entities\n    entities = []\n\n    # an old-style polyline entity strings its data across\n    # multiple vertex entities like a real asshole\n    polyline = None\n\n    # loop through chunks of entity information\n    for index in np.array_split(np.arange(len(entity_blob)),\n                                inflection):\n\n        # if there is only a header continue\n        if len(index) < 1:\n            continue\n\n        # chunk will be an (n, 2) array of (group code, data) pairs\n        chunk = entity_blob[index]\n\n        # the string representing entity type\n        entity_type = chunk[0][1]\n\n        ############\n        # special case old- style polyline entities\n        if entity_type == 'POLYLINE':\n            polyline = [dict(chunk)]\n        # if we are collecting vertex entities\n        elif polyline is not None and entity_type == 'VERTEX':\n            polyline.append(dict(chunk))\n        # the end of a polyline\n        elif polyline is not None and entity_type == 'SEQEND':\n            # pull the geometry information for the entity\n            lines = np.array([[i['10'], i['20']]\n                              for i in polyline[1:]],\n                             dtype=np.float64)\n\n            # check for a closed flag on the polyline\n            if '70' in polyline[0]:\n                # flag is bit- coded integer\n                flag = int(polyline[0]['70'])\n                # first bit represents closed\n                is_closed = bool(flag & 1)\n                if is_closed:\n                    lines = np.vstack((lines, lines[:1]))\n\n            # get the index of each bulged vertices\n            bulge_idx = np.array([i for i, e in enumerate(polyline)\n                                  if '42' in e],\n                                 dtype=np.int64)\n            # get the actual bulge value\n            bulge = np.array([float(e['42'])\n                              for i, e in enumerate(polyline)\n                              if '42' in e],\n                             dtype=np.float64)\n            # convert bulge to new entities\n            v, e = bulge_to_arcs(lines=lines,\n                                 bulge=bulge,\n                                 bulge_idx=bulge_idx,\n                                 is_closed=is_closed)\n            for i in e:\n                # offset entities by existing vertices\n                i.points += len(vertices)\n            vertices.extend(v)\n            entities.extend(e)\n\n            # we no longer have an active polyline\n            polyline = None\n        elif entity_type == 'TEXT':\n            # text entities need spaces preserved so take\n            # group codes from clean representation (0- column)\n            # and data from the raw representation (1- column)\n            chunk_raw = entity_raw[index]\n            # if we didn't use clean group codes we wouldn't\n            # be able to access them by key as whitespace\n            # is random and crazy, like: '  1 '\n            chunk_raw[:, 0] = entity_blob[index][:, 0]\n            try:\n                convert_text(dict(chunk_raw))\n            except BaseException:\n                log.warning('failed to load text entity!',\n                            exc_info=True)\n        # if the entity contains all relevant data we can\n        # cleanly load it from inside a single function\n        elif entity_type in loaders:\n            # the chunker converts an (n,2) list into a dict\n            chunker, loader = loaders[entity_type]\n            # convert data to dict\n            entity_data = chunker(chunk)\n            # append data to the lists we're collecting\n            loader(entity_data)\n        else:\n            log.debug('Entity type %s not supported',\n                      entity_type)\n\n    # stack vertices into single array\n    vertices = util.vstack_empty(vertices).astype(np.float64)\n\n    # return result as kwargs for trimesh.path.Path2D constructor\n    result = {'vertices': vertices,\n              'entities': np.array(entities),\n              'metadata': metadata}\n\n    return result", "response": "Load a DXF file into a dictionary containing vertices and metadata."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_dxf(path, layers=None):\n\n    def format_points(points,\n                      as_2D=False,\n                      increment=True):\n        \"\"\"\n        Format points into DXF- style point string.\n\n        Parameters\n        -----------\n        points : (n,2) or (n,3) float\n          Points in space\n        as_2D : bool\n          If True only output 2 points per vertex\n        increment : bool\n          If True increment group code per point\n          Example:\n            [[X0, Y0, Z0], [X1, Y1, Z1]]\n          Result, new lines replaced with spaces:\n            True  -> 10 X0 20 Y0 30 Z0 11 X1 21 Y1 31 Z1\n            False -> 10 X0 20 Y0 30 Z0 10 X1 20 Y1 30 Z1\n\n        Returns\n        -----------\n        packed : str\n          Points formatted with group code\n        \"\"\"\n        points = np.asanyarray(points, dtype=np.float64)\n        # get points in 3D\n        three = util.stack_3D(points)\n        if increment:\n            group = np.tile(\n                np.arange(len(three), dtype=np.int).reshape((-1, 1)),\n                (1, 3))\n        else:\n            group = np.zeros((len(three), 3), dtype=np.int)\n        group += [10, 20, 30]\n\n        if as_2D:\n            group = group[:, :2]\n            three = three[:, :2]\n\n        packed = '\\n'.join('{:d}\\n{:.12f}'.format(g, v)\n                           for g, v in zip(group.reshape(-1),\n                                           three.reshape(-1)))\n\n        return packed\n\n    def entity_info(entity):\n        \"\"\"\n        Pull layer, color, and name information about an entity\n\n        Parameters\n        -----------\n        entity : entity object\n          Source entity to pull metadata\n\n        Returns\n        ----------\n        subs : dict\n          Has keys 'COLOR', 'LAYER', 'NAME'\n        \"\"\"\n        subs = {'COLOR': 255,  # default is ByLayer\n                'LAYER': 0,\n                'NAME': str(id(entity))[:16]}\n\n        if hasattr(entity, 'color'):\n            # all colors must be integers between 0-255\n            color = str(entity.color)\n            if str.isnumeric(color):\n                subs['COLOR'] = int(color) % 256\n\n        if hasattr(entity, 'layer'):\n            subs['LAYER'] = str(entity.layer)\n\n        return subs\n\n    def convert_line(line, vertices):\n        \"\"\"\n        Convert an entity to a discrete polyline\n\n        Parameters\n        -------------\n        line : entity\n          Entity which has 'e.discrete' method\n        vertices : (n, 2) float\n          Vertices in space\n\n        Returns\n        -----------\n        as_dxf : str\n          Entity exported as a DXF\n        \"\"\"\n        # get a discrete representation of entity\n        points = line.discrete(vertices)\n        # if one or fewer points return nothing\n        if len(points) <= 1:\n            return ''\n\n        # generate a substitution dictionary for template\n        subs = entity_info(line)\n        subs['POINTS'] = format_points(points,\n                                       as_2D=True,\n                                       increment=False)\n        subs['TYPE'] = 'LWPOLYLINE'\n        subs['VCOUNT'] = len(points)\n        # 1 is closed\n        # 0 is default (open)\n        subs['FLAG'] = int(bool(line.closed))\n\n        result = TEMPLATES['line'].substitute(subs)\n        return result\n\n    def convert_arc(arc, vertices):\n        info = arc.center(vertices)\n        subs = entity_info(arc)\n\n        center = info['center']\n        if len(center) == 2:\n            center = np.append(center, 0.0)\n        data = '10\\n{:.12f}\\n20\\n{:.12f}\\n30\\n{:.12f}'.format(*center)\n        data += '\\n40\\n{:.12f}'.format(info['radius'])\n\n        if arc.closed:\n            subs['TYPE'] = 'CIRCLE'\n        else:\n            subs['TYPE'] = 'ARC'\n            # an arc is the same as a circle, with an added start\n            # and end angle field\n            data += '\\n100\\nAcDbArc'\n            data += '\\n50\\n{:.12f}\\n51\\n{:.12f}'.format(\n                *np.degrees(info['angles']))\n        subs['DATA'] = data\n\n        result = TEMPLATES['arc'].substitute(subs)\n\n        return result\n\n    def convert_bspline(spline, vertices):\n        # points formatted with group code\n        points = format_points(vertices[spline.points],\n                               increment=False)\n\n        # (n,) float knots, formatted with group code\n        knots = ('40\\n{:.12f}\\n' * len(spline.knots)\n                 ).format(*spline.knots)[:-1]\n\n        # bit coded\n        flags = {'closed': 1,\n                 'periodic': 2,\n                 'rational': 4,\n                 'planar': 8,\n                 'linear': 16}\n\n        flag = flags['planar']\n        if spline.closed:\n            flag = flag | flags['closed']\n\n        normal = [0.0, 0.0, 1.0]\n        n_code = [210, 220, 230]\n        n_str = '\\n'.join('{:d}\\n{:.12f}'.format(i, j)\n                          for i, j in zip(n_code, normal))\n\n        subs = entity_info(spline)\n        subs.update({'TYPE': 'SPLINE',\n                     'POINTS': points,\n                     'KNOTS': knots,\n                     'NORMAL': n_str,\n                     'DEGREE': 3,\n                     'FLAG': flag,\n                     'FCOUNT': 0,\n                     'KCOUNT': len(spline.knots),\n                     'PCOUNT': len(spline.points)})\n        # format into string template\n        result = TEMPLATES['bspline'].substitute(subs)\n\n        return result\n\n    def convert_text(txt, vertices):\n        \"\"\"\n        Convert a Text entity to DXF string.\n        \"\"\"\n        sub = entity_info(txt)\n\n        # get the origin point of the text\n        sub['ORIGIN'] = format_points(vertices[[txt.origin]],\n                                      increment=False)\n        # rotation angle in degrees\n        sub['ANGLE'] = np.degrees(txt.angle(vertices))\n        # actual string of text with spaces escaped\n        sub['TEXT'] = txt.text.replace(' ', _SAFESPACE)\n        # height of text\n        sub['HEIGHT'] = txt.height\n\n        return TEMPLATES['text'].substitute(sub)\n\n    def convert_generic(entity, vertices):\n        \"\"\"\n        For entities we don't know how to handle, return their\n        discrete form as a polyline\n        \"\"\"\n        return convert_line(entity, vertices)\n\n    # make sure we're not losing a ton of\n    # precision in the string conversion\n    np.set_printoptions(precision=12)\n    # trimesh entity to DXF entity converters\n    conversions = {'Line': convert_line,\n                   'Text': convert_text,\n                   'Arc': convert_arc,\n                   'Bezier': convert_generic,\n                   'BSpline': convert_bspline}\n    collected = []\n    for e, layer in zip(path.entities, path.layers):\n        name = type(e).__name__\n        # only export specified layers\n        if layers is not None:\n            if layer not in layers:\n                continue\n        if name in conversions:\n            converted = conversions[name](e, path.vertices).strip()\n            # only save if we converted something\n            if len(converted) > 0:\n                collected.append(converted)\n        else:\n            log.debug('Entity type %s not exported!', name)\n\n    entities_str = '\\n'.join(collected)\n\n    hsub = {'BOUNDS_MIN': format_points([path.bounds[0]]),\n            'BOUNDS_MAX': format_points([path.bounds[1]]),\n            'LUNITS': '1'}\n    if path.units in _UNITS_TO_DXF:\n        hsub['LUNITS'] = _UNITS_TO_DXF[path.units]\n\n    # sections of the DXF\n    header = TEMPLATES['header'].substitute(hsub)\n    # entities section\n    entities = TEMPLATES['entities'].substitute({\n        'ENTITIES': entities_str})\n    footer = TEMPLATES['footer'].substitute()\n\n    # filter out empty sections\n    # random whitespace causes AutoCAD to fail to load\n    # although Draftsight, LibreCAD, and Inkscape don't care\n    # what a giant legacy piece of shit\n    # strip out all leading and trailing whitespace\n    sections = [i.strip() for i in [header,\n                                    entities,\n                                    footer]\n                if len(i) > 0]\n\n    blob = '\\n'.join(sections).replace(_SAFESPACE, ' ')\n\n    # run additional self- checks\n    if tol.strict:\n        # check that every line pair is (group code, value)\n        lines = str.splitlines(str(blob))\n\n        # should be even number of lines\n        assert (len(lines) % 2) == 0\n\n        # group codes should all be convertible to int and positive\n        assert all(int(i) >= 0 for i in lines[::2])\n\n    return blob", "response": "Export a 2D path object to a DXF file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads the DWG files by converting them to DXF files using TeighaFileConverter.", "response": "def load_dwg(file_obj, **kwargs):\n    \"\"\"\n    Load DWG files by converting them to DXF files using\n    TeighaFileConverter.\n\n    Parameters\n    -------------\n    file_obj : file- like object\n\n    Returns\n    -------------\n    loaded : dict\n        kwargs for a Path2D constructor\n    \"\"\"\n    # read the DWG data into a bytes object\n    data = file_obj.read()\n    # convert data into R14 ASCII DXF\n    converted = _teigha_convert(data)\n    # load into kwargs for Path2D constructor\n    result = load_dxf(util.wrap_as_stream(converted))\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bulge_to_arcs(lines,\n                  bulge,\n                  bulge_idx,\n                  is_closed=False,\n                  metadata=None):\n    \"\"\"\n    Polylines can have \"vertex bulge,\" which means the polyline\n    has an arc tangent to segments, rather than meeting at a\n    vertex.\n\n    From Autodesk reference:\n    The bulge is the tangent of one fourth the included\n    angle for an arc segment, made negative if the arc\n    goes clockwise from the start point to the endpoint.\n    A bulge of 0 indicates a straight segment, and a\n    bulge of 1 is a semicircle.\n\n    Parameters\n    ----------------\n    lines : (n, 2) float\n      Polyline vertices in order\n    bulge : (m,) float\n      Vertex bulge value\n    bulge_idx : (m,) float\n      Which index of lines is bulge associated with\n    is_closed : bool\n      Is segment closed\n    metadata : None, or dict\n      Entity metadata to add\n\n    Returns\n    ---------------\n    vertices : (a, 2) float\n      New vertices for poly-arc\n    entities : (b,) entities.Entity\n      New entities, either line or arc\n    \"\"\"\n    # make sure lines are 2D array\n    lines = np.asanyarray(lines, dtype=np.float64)\n\n    # make sure inputs are numpy arrays\n    bulge = np.asanyarray(bulge, dtype=np.float64)\n    bulge_idx = np.asanyarray(bulge_idx, dtype=np.int64)\n\n    # filter out zero- bulged polylines\n    ok = np.abs(bulge) > 1e-5\n    bulge = bulge[ok]\n    bulge_idx = bulge_idx[ok]\n\n    # metadata to apply to new entities\n    if metadata is None:\n        metadata = {}\n\n    # if there's no bulge, just return the input curve\n    if len(bulge) == 0:\n        index = np.arange(len(lines))\n        # add a single line entity and vertices\n        entities = [Line(index, **metadata)]\n        return lines, entities\n\n    # use bulge to calculate included angle of the arc\n    angle = np.arctan(bulge) * 4.0\n    # the indexes making up a bulged segment\n    tid = np.column_stack((bulge_idx, bulge_idx - 1))\n    # if it's a closed segment modulus to start vertex\n    if is_closed:\n        tid %= len(lines)\n\n    # the vector connecting the two ends of the arc\n    vector = lines[tid[:, 0]] - lines[tid[:, 1]]\n\n    # the length of the connector segment\n    length = (np.linalg.norm(vector, axis=1))\n\n    # perpendicular vectors by crossing vector with Z\n    perp = np.cross(\n        np.column_stack((vector, np.zeros(len(vector)))),\n        np.ones((len(vector), 3)) * [0, 0, 1])\n    # strip the zero Z\n    perp = util.unitize(perp[:, :2])\n\n    # midpoint of each line\n    midpoint = lines[tid].mean(axis=1)\n\n    # calculate the signed radius of each arc segment\n    radius = (length / 2.0) / np.sin(angle / 2.0)\n\n    # offset magnitude to point on arc\n    offset = radius - np.cos(angle / 2) * radius\n\n    # convert each arc to three points:\n    # start, any point on arc, end\n    three = np.column_stack((\n        lines[tid[:, 0]],\n        midpoint + perp * offset.reshape((-1, 1)),\n        lines[tid[:, 1]])).reshape((-1, 3, 2))\n\n    # if we're in strict mode make sure our arcs\n    # have the same magnitude as the input data\n    if tol.strict:\n        from ..arc import arc_center\n        check_angle = [arc_center(i)['span']\n                       for i in three]\n        assert np.allclose(np.abs(angle),\n                           np.abs(check_angle))\n\n        check_radii = [arc_center(i)['radius']\n                       for i in three]\n        assert np.allclose(check_radii, np.abs(radius))\n\n    # collect new entities and vertices\n    entities, vertices = [], []\n    # add the entities for each new arc\n    for arc_points in three:\n        entities.append(Arc(\n            points=np.arange(3) + len(vertices),\n            **metadata))\n        vertices.extend(arc_points)\n\n    # if there are unconsumed line\n    # segments add them to drawing\n    if (len(lines) - 1) > len(bulge):\n        # indexes of line segments\n        existing = util.stack_lines(np.arange(len(lines)))\n        # remove line segments replaced with arcs\n        for line_idx in grouping.boolean_rows(\n                existing,\n                np.sort(tid, axis=1),\n                np.setdiff1d):\n            # add a single line entity and vertices\n            entities.append(Line(\n                points=np.arange(2) + len(vertices),\n                **metadata))\n            vertices.extend(lines[line_idx].copy())\n\n    # make sure vertices are clean numpy array\n    vertices = np.array(vertices, dtype=np.float64)\n\n    return vertices, entities", "response": "Converts bulge values to arc tangent coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_key(blob, field, code):\n    try:\n        line = blob[np.nonzero(blob[:, 1] == field)[0][0] + 1]\n    except IndexError:\n        return None\n    if line[0] == code:\n        try:\n            return int(line[1])\n        except ValueError:\n            return line[1]\n    else:\n        return None", "response": "Given a loaded blob and a field name get a value by code."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting any DXF or DWG file to R14 ASCII DXF using Teigha Converter.", "response": "def _teigha_convert(data, extension='dwg'):\n    \"\"\"\n    Convert any DXF/DWG to R14 ASCII DXF using Teigha Converter.\n\n    Parameters\n    ---------------\n    data : str or bytes\n       The contents of a DXF or DWG file\n    extension : str\n       The format of data: 'dwg' or 'dxf'\n\n    Returns\n    --------------\n    converted : str\n       Result as R14 ASCII DXF\n    \"\"\"\n    # temp directory for DWG file\n    dir_dwg = tempfile.mkdtemp()\n    # temp directory for DXF output\n    dir_out = tempfile.mkdtemp()\n\n    # put together the subprocess command\n    cmd = [_xvfb_run,  # suppress the GUI QT status bar\n           '-a',      # use an automatic screen\n           _teigha,   # run the converter\n           dir_dwg,   # the directory containing DWG files\n           dir_out,   # the directory for output DXF files\n           'ACAD14',  # the revision of DXF\n           'DXF',     # the output format\n           '1',       # recurse input folder\n           '1']       # audit each file\n\n    # if Xvfb is already running it probably\n    # has a working configuration so use it\n    running = b'Xvfb' in subprocess.check_output(['ps', '-eaf'])\n    # chop off XVFB if it isn't installed or is running\n    if running or _xvfb_run is None:\n        cmd = cmd[2:]\n\n    # create file in correct mode for data\n    if hasattr(data, 'encode'):\n        # data is a string which can be encoded to bytes\n        mode = 'w'\n    else:\n        # data is already bytes\n        mode = 'wb'\n\n    # write the file_obj in the temp directory\n    dwg_name = os.path.join(dir_dwg, 'drawing.' + extension)\n    with open(dwg_name, mode) as f:\n        f.write(data)\n\n    # run the conversion\n    output = subprocess.check_output(cmd)\n\n    # load the ASCII DXF produced from the conversion\n    name_result = os.path.join(dir_out, 'drawing.dxf')\n    # if the conversion failed log things before failing\n    if not os.path.exists(name_result):\n        log.error('teigha convert failed!\\nls {}: {}\\n\\n {}'.format(\n            dir_out,\n            os.listdir(dir_out),\n            output))\n        raise ValueError('conversion using Teigha failed!')\n\n    # load converted file into a string\n    with open(name_result, 'r') as f:\n        converted = f.read()\n\n    # remove the temporary directories\n    shutil.rmtree(dir_out)\n    shutil.rmtree(dir_dwg)\n\n    return converted"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cross(triangles):\n    vectors = np.diff(triangles, axis=1)\n    crosses = np.cross(vectors[:, 0], vectors[:, 1])\n    return crosses", "response": "Returns the cross product of two edges from input triangles"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the sum area of a set of triangles.", "response": "def area(triangles=None, crosses=None, sum=False):\n    \"\"\"\n    Calculates the sum area of input triangles\n\n    Parameters\n    ----------\n    triangles : (n, 3, 3) float\n      Vertices of triangles\n    crosses : (n, 3) float or None\n      As a speedup don't re- compute cross products\n    sum : bool\n      Return summed area or individual triangle area\n\n    Returns\n    ----------\n    area : (n,) float or float\n      Individual or summed area depending on `sum` argument\n    \"\"\"\n    if crosses is None:\n        crosses = cross(triangles)\n    area = (np.sum(crosses**2, axis=1)**.5) * .5\n    if sum:\n        return np.sum(area)\n    return area"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normals(triangles=None, crosses=None):\n    if crosses is None:\n        crosses = cross(triangles)\n    # unitize the cross product vectors\n    unit, valid = util.unitize(crosses, check_valid=True)\n    return unit, valid", "response": "Calculates the normals of a set of triangles."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef angles(triangles):\n\n    # get a vector for each edge of the triangle\n    u = triangles[:, 1] - triangles[:, 0]\n    v = triangles[:, 2] - triangles[:, 0]\n    w = triangles[:, 2] - triangles[:, 1]\n\n    # normalize each vector in place\n    u /= np.linalg.norm(u, axis=1, keepdims=True)\n    v /= np.linalg.norm(v, axis=1, keepdims=True)\n    w /= np.linalg.norm(w, axis=1, keepdims=True)\n\n    # run the cosine and an einsum that definitely does something\n    a = np.arccos(np.clip(np.einsum('ij, ij->i', u, v), -1, 1))\n    b = np.arccos(np.clip(np.einsum('ij, ij->i', -u, w), -1, 1))\n    c = np.pi - a - b\n\n    return np.column_stack([a, b, c])", "response": "Calculates the angles of a set of triangles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck to see if a list of triangles are all coplanar", "response": "def all_coplanar(triangles):\n    \"\"\"\n    Check to see if a list of triangles are all coplanar\n\n    Parameters\n    ----------------\n    triangles: (n, 3, 3) float\n      Vertices of triangles\n\n    Returns\n    ---------------\n    all_coplanar : bool\n      True if all triangles are coplanar\n    \"\"\"\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n\n    test_normal = normals(triangles)[0]\n    test_vertex = triangles[0][0]\n    distances = point_plane_distance(points=triangles[1:].reshape((-1, 3)),\n                                     plane_normal=test_normal,\n                                     plane_origin=test_vertex)\n    all_coplanar = np.all(np.abs(distances) < tol.zero)\n    return all_coplanar"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef mass_properties(triangles,\n                    crosses=None,\n                    density=1.0,\n                    center_mass=None,\n                    skip_inertia=False):\n    \"\"\"\n    Calculate the mass properties of a group of triangles.\n\n    Implemented from:\n    http://www.geometrictools.com/Documentation/PolyhedralMassProperties.pdf\n\n    Parameters\n    ----------\n    triangles : (n, 3, 3) float\n      Triangle vertices in space\n    crosses : (n,) float\n      Optional cross products of triangles\n    density : float\n      Optional override for density\n    center_mass :  (3,) float\n      Optional override for center mass\n    skip_inertia : bool\n      if True will not return moments matrix\n\n    Returns\n    ---------\n    info : dict\n      Mass properties\n    \"\"\"\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n\n    if crosses is None:\n        crosses = cross(triangles)\n\n    # these are the subexpressions of the integral\n    f1 = triangles.sum(axis=1)\n\n    # for the the first vertex of every triangle:\n    # triangles[:,0,:] will give rows like [[x0, y0, z0], ...]\n\n    # for the x coordinates of every triangle\n    # triangles[:,:,0] will give rows like [[x0, x1, x2], ...]\n    f2 = (triangles[:, 0, :]**2 +\n          triangles[:, 1, :]**2 +\n          triangles[:, 0, :] * triangles[:, 1, :] +\n          triangles[:, 2, :] * f1)\n    f3 = ((triangles[:, 0, :]**3) +\n          (triangles[:, 0, :]**2) * (triangles[:, 1, :]) +\n          (triangles[:, 0, :]) * (triangles[:, 1, :]**2) +\n          (triangles[:, 1, :]**3) +\n          (triangles[:, 2, :] * f2))\n    g0 = (f2 + (triangles[:, 0, :] + f1) * triangles[:, 0, :])\n    g1 = (f2 + (triangles[:, 1, :] + f1) * triangles[:, 1, :])\n    g2 = (f2 + (triangles[:, 2, :] + f1) * triangles[:, 2, :])\n    integral = np.zeros((10, len(f1)))\n    integral[0] = crosses[:, 0] * f1[:, 0]\n    integral[1:4] = (crosses * f2).T\n    integral[4:7] = (crosses * f3).T\n    for i in range(3):\n        triangle_i = np.mod(i + 1, 3)\n        integral[i + 7] = crosses[:, i] * ((triangles[:, 0, triangle_i] * g0[:, i]) +\n                                           (triangles[:, 1, triangle_i] * g1[:, i]) +\n                                           (triangles[:, 2, triangle_i] * g2[:, i]))\n\n    coefficients = 1.0 / np.array([6, 24, 24, 24, 60, 60, 60, 120, 120, 120],\n                                  dtype=np.float64)\n    integrated = integral.sum(axis=1) * coefficients\n\n    volume = integrated[0]\n\n    if center_mass is None:\n        if np.abs(volume) < tol.zero:\n            center_mass = np.zeros(3)\n        else:\n            center_mass = integrated[1:4] / volume\n\n    mass = density * volume\n\n    result = {'density': density,\n              'mass': mass,\n              'volume': volume,\n              'center_mass': center_mass}\n\n    if skip_inertia:\n        return result\n\n    inertia = np.zeros((3, 3))\n    inertia[0, 0] = integrated[5] + integrated[6] - \\\n        (volume * (center_mass[[1, 2]]**2).sum())\n    inertia[1, 1] = integrated[4] + integrated[6] - \\\n        (volume * (center_mass[[0, 2]]**2).sum())\n    inertia[2, 2] = integrated[4] + integrated[5] - \\\n        (volume * (center_mass[[0, 1]]**2).sum())\n    inertia[0, 1] = (\n        integrated[7] - (volume * np.product(center_mass[[0, 1]])))\n    inertia[1, 2] = (\n        integrated[8] - (volume * np.product(center_mass[[1, 2]])))\n    inertia[0, 2] = (\n        integrated[9] - (volume * np.product(center_mass[[0, 2]])))\n    inertia[2, 0] = inertia[0, 2]\n    inertia[2, 1] = inertia[1, 2]\n    inertia[1, 0] = inertia[0, 1]\n    inertia *= density\n    result['inertia'] = inertia\n\n    return result", "response": "Calculate the mass properties of a group of triangles."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef windings_aligned(triangles, normals_compare):\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n\n    calculated, valid = normals(triangles)\n    difference = util.diagonal_dot(calculated,\n                                   normals_compare[valid])\n\n    aligned = np.zeros(len(triangles), dtype=np.bool)\n    aligned[valid] = difference > 0.0\n\n    return aligned", "response": "Given a list of triangles and a list of normals determine if the two windings are aligned with the triangles."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bounds_tree(triangles):\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n\n    # the (n,6) interleaved bounding box for every triangle\n    triangle_bounds = np.column_stack((triangles.min(axis=1),\n                                       triangles.max(axis=1)))\n    tree = util.bounds_tree(triangle_bounds)\n    return tree", "response": "Returns an r - tree for broad - phase cross - section detection for a list of triangles"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nondegenerate(triangles, areas=None, height=None):\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n\n    if height is None:\n        height = tol.merge\n\n    # if both edges of the triangles OBB are longer than tol.merge\n    # we declare them to be nondegenerate\n    ok = (extents(triangles=triangles,\n                  areas=areas) > height).all(axis=1)\n\n    return ok", "response": "Returns a non - degenerate version of the given triangles."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef extents(triangles, areas=None):\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n\n    if areas is None:\n        areas = area(triangles=triangles,\n                     sum=False)\n\n    # the edge vectors which define the triangle\n    a = triangles[:, 1] - triangles[:, 0]\n    b = triangles[:, 2] - triangles[:, 0]\n\n    # length of the edge vectors\n    length_a = (a**2).sum(axis=1)**.5\n    length_b = (b**2).sum(axis=1)**.5\n\n    # which edges are acceptable length\n    nonzero_a = length_a > tol.merge\n    nonzero_b = length_b > tol.merge\n\n    # find the two heights of the triangle\n    # essentially this is the side length of an\n    # oriented bounding box, per triangle\n    box = np.zeros((len(triangles), 2), dtype=np.float64)\n    box[:, 0][nonzero_a] = (areas[nonzero_a] * 2) / length_a[nonzero_a]\n    box[:, 1][nonzero_b] = (areas[nonzero_b] * 2) / length_b[nonzero_b]\n\n    return box", "response": "Returns the 2D bounding box size of each triangle in the input space."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a list of barycentric coordinates on a list of triangles on a list of triangles into a list of cartesian points.", "response": "def barycentric_to_points(triangles, barycentric):\n    \"\"\"\n    Convert a list of barycentric coordinates on a list of triangles\n    to cartesian points.\n\n    Parameters\n    ------------\n    triangles : (n, 3, 3) float\n      Triangles in space\n    barycentric : (n, 2) float\n      Barycentric coordinates\n\n    Returns\n    -----------\n    points : (m, 3) float\n      Points in space\n    \"\"\"\n    barycentric = np.asanyarray(barycentric, dtype=np.float64)\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n    if barycentric.shape == (2,):\n        barycentric = np.ones((len(triangles), 2),\n                              dtype=np.float64) * barycentric\n    if util.is_shape(barycentric, (len(triangles), 2)):\n        barycentric = np.column_stack((barycentric,\n                                       1.0 - barycentric.sum(axis=1)))\n    elif not util.is_shape(barycentric, (len(triangles), 3)):\n        raise ValueError('Barycentric shape incorrect!')\n\n    barycentric /= barycentric.sum(axis=1).reshape((-1, 1))\n    points = (triangles * barycentric.reshape((-1, 3, 1))).sum(axis=1)\n\n    return points"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the barycentric coordinates of a set of points relative to a set of triangles.", "response": "def points_to_barycentric(triangles,\n                          points,\n                          method='cramer'):\n    \"\"\"\n    Find the barycentric coordinates of points relative to triangles.\n\n    The Cramer's rule solution implements:\n        http://blackpawn.com/texts/pointinpoly\n\n    The cross product solution implements:\n        https://www.cs.ubc.ca/~heidrich/Papers/JGT.05.pdf\n\n\n    Parameters\n    -----------\n    triangles : (n, 3, 3) float\n      Triangles vertices in space\n    points : (n, 3) float\n      Point in space associated with a triangle\n    method :  str\n      Which method to compute the barycentric coordinates with:\n        - 'cross': uses a method using cross products, roughly 2x slower but\n                  different numerical robustness properties\n        - anything else: uses a cramer's rule solution\n\n    Returns\n    -----------\n    barycentric : (n, 3) float\n      Barycentric coordinates of each point\n    \"\"\"\n\n    def method_cross():\n        n = np.cross(edge_vectors[:, 0], edge_vectors[:, 1])\n        denominator = util.diagonal_dot(n, n)\n\n        barycentric = np.zeros((len(triangles), 3), dtype=np.float64)\n        barycentric[:, 2] = util.diagonal_dot(\n            np.cross(edge_vectors[:, 0], w), n) / denominator\n        barycentric[:, 1] = util.diagonal_dot(\n            np.cross(w, edge_vectors[:, 1]), n) / denominator\n        barycentric[:, 0] = 1 - barycentric[:, 1] - barycentric[:, 2]\n        return barycentric\n\n    def method_cramer():\n        dot00 = util.diagonal_dot(edge_vectors[:, 0], edge_vectors[:, 0])\n        dot01 = util.diagonal_dot(edge_vectors[:, 0], edge_vectors[:, 1])\n        dot02 = util.diagonal_dot(edge_vectors[:, 0], w)\n        dot11 = util.diagonal_dot(edge_vectors[:, 1], edge_vectors[:, 1])\n        dot12 = util.diagonal_dot(edge_vectors[:, 1], w)\n\n        inverse_denominator = 1.0 / (dot00 * dot11 - dot01 * dot01)\n\n        barycentric = np.zeros((len(triangles), 3), dtype=np.float64)\n        barycentric[:, 2] = (dot00 * dot12 - dot01 *\n                             dot02) * inverse_denominator\n        barycentric[:, 1] = (dot11 * dot02 - dot01 *\n                             dot12) * inverse_denominator\n        barycentric[:, 0] = 1 - barycentric[:, 1] - barycentric[:, 2]\n        return barycentric\n\n    # establish that input triangles and points are sane\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('triangles shape incorrect')\n    if not util.is_shape(points, (len(triangles), 3)):\n        raise ValueError('triangles and points must correspond')\n\n    edge_vectors = triangles[:, 1:] - triangles[:, :1]\n    w = points - triangles[:, 0].reshape((-1, 3))\n\n    if method == 'cross':\n        return method_cross()\n    return method_cramer()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the point on the surface of each triangle that is closest to each point in the triangles.", "response": "def closest_point(triangles, points):\n    \"\"\"\n    Return the closest point on the surface of each triangle for a\n    list of corresponding points.\n\n    Implements the method from \"Real Time Collision Detection\" and\n    use the same variable names as \"ClosestPtPointTriangle\" to avoid\n    being any more confusing.\n\n\n    Parameters\n    ----------\n    triangles : (n, 3, 3) float\n      Triangle vertices in space\n    points : (n, 3) float\n      Points in space\n\n    Returns\n    ----------\n    closest : (n, 3) float\n      Point on each triangle closest to each point\n    \"\"\"\n\n    # check input triangles and points\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    points = np.asanyarray(points, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('triangles shape incorrect')\n    if not util.is_shape(points, (len(triangles), 3)):\n        raise ValueError('need same number of triangles and points!')\n\n    # store the location of the closest point\n    result = np.zeros_like(points)\n    # which points still need to be handled\n    remain = np.ones(len(points), dtype=np.bool)\n\n    # if we dot product this against a (n, 3)\n    # it is equivalent but faster than array.sum(axis=1)\n    ones = [1.0, 1.0, 1.0]\n\n    # get the three points of each triangle\n    # use the same notation as RTCD to avoid confusion\n    a = triangles[:, 0, :]\n    b = triangles[:, 1, :]\n    c = triangles[:, 2, :]\n\n    # check if P is in vertex region outside A\n    ab = b - a\n    ac = c - a\n    ap = points - a\n    # this is a faster equivalent of:\n    # util.diagonal_dot(ab, ap)\n    d1 = np.dot(ab * ap, ones)\n    d2 = np.dot(ac * ap, ones)\n\n    # is the point at A\n    is_a = np.logical_and(d1 < tol.zero, d2 < tol.zero)\n    if is_a.any():\n        result[is_a] = a[is_a]\n        remain[is_a] = False\n\n    # check if P in vertex region outside B\n    bp = points - b\n    d3 = np.dot(ab * bp, ones)\n    d4 = np.dot(ac * bp, ones)\n\n    # do the logic check\n    is_b = (d3 > -tol.zero) & (d4 <= d3) & remain\n    if is_b.any():\n        result[is_b] = b[is_b]\n        remain[is_b] = False\n\n    # check if P in edge region of AB, if so return projection of P onto A\n    vc = (d1 * d4) - (d3 * d2)\n    is_ab = ((vc < tol.zero) &\n             (d1 > -tol.zero) &\n             (d3 < tol.zero) & remain)\n    if is_ab.any():\n        v = (d1[is_ab] / (d1[is_ab] - d3[is_ab])).reshape((-1, 1))\n        result[is_ab] = a[is_ab] + (v * ab[is_ab])\n        remain[is_ab] = False\n\n    # check if P in vertex region outside C\n    cp = points - c\n    d5 = np.dot(ab * cp, ones)\n    d6 = np.dot(ac * cp, ones)\n    is_c = (d6 > -tol.zero) & (d5 <= d6) & remain\n    if is_c.any():\n        result[is_c] = c[is_c]\n        remain[is_c] = False\n\n    # check if P in edge region of AC, if so return projection of P onto AC\n    vb = (d5 * d2) - (d1 * d6)\n    is_ac = (vb < tol.zero) & (d2 > -tol.zero) & (d6 < tol.zero) & remain\n    if is_ac.any():\n        w = (d2[is_ac] / (d2[is_ac] - d6[is_ac])).reshape((-1, 1))\n        result[is_ac] = a[is_ac] + w * ac[is_ac]\n        remain[is_ac] = False\n\n    # check if P in edge region of BC, if so return projection of P onto BC\n    va = (d3 * d6) - (d5 * d4)\n    is_bc = ((va < tol.zero) &\n             ((d4 - d3) > - tol.zero) &\n             ((d5 - d6) > -tol.zero) & remain)\n    if is_bc.any():\n        d43 = d4[is_bc] - d3[is_bc]\n        w = (d43 / (d43 + (d5[is_bc] - d6[is_bc]))).reshape((-1, 1))\n        result[is_bc] = b[is_bc] + w * (c[is_bc] - b[is_bc])\n        remain[is_bc] = False\n\n    # any remaining points must be inside face region\n    if remain.any():\n        # point is inside face region\n        denom = 1.0 / (va[remain] + vb[remain] + vc[remain])\n        v = (vb[remain] * denom).reshape((-1, 1))\n        w = (vc[remain] * denom).reshape((-1, 1))\n        # compute Q through its barycentric coordinates\n        result[remain] = a[remain] + (ab[remain] * v) + (ac[remain] * w)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a list of triangles to the kwargs for the Trimesh constructor.", "response": "def to_kwargs(triangles):\n    \"\"\"\n    Convert a list of triangles to the kwargs for the Trimesh\n    constructor.\n\n    Parameters\n    ---------\n    triangles : (n, 3, 3) float\n      Triangles in space\n\n    Returns\n    ---------\n    kwargs : dict\n      Keyword arguments for the trimesh.Trimesh constructor\n      Includes keys 'vertices' and 'faces'\n\n    Examples\n    ---------\n    >>> mesh = trimesh.Trimesh(**trimesh.triangles.to_kwargs(triangles))\n    \"\"\"\n    triangles = np.asanyarray(triangles, dtype=np.float64)\n    if not util.is_shape(triangles, (-1, 3, 3)):\n        raise ValueError('Triangles must be (n,3,3)!')\n\n    vertices = triangles.reshape((-1, 3))\n    faces = np.arange(len(vertices)).reshape((-1, 3))\n    kwargs = {'vertices': vertices,\n              'faces': faces}\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef copy(self):\n        result = copy.deepcopy(self)\n        result._cache.clear()\n        return result", "response": "Return a copy of the Primitive object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_mesh(self):\n        result = Trimesh(vertices=self.vertices.copy(),\n                         faces=self.faces.copy(),\n                         face_normals=self.face_normals.copy(),\n                         process=False)\n        return result", "response": "Return a copy of the Primitive object as a Trimesh object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef apply_transform(self, matrix):\n        matrix = np.asanyarray(matrix, order='C', dtype=np.float64)\n        if matrix.shape != (4, 4):\n            raise ValueError('Transformation matrix must be (4,4)!')\n\n        if np.allclose(matrix, np.eye(4)):\n            log.debug('apply_tranform received identity matrix')\n            return\n\n        new_transform = np.dot(matrix, self.primitive.transform)\n\n        self.primitive.transform = new_transform", "response": "Apply a homogenous transformation to the current primitive."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef volume(self):\n        volume = ((np.pi * self.primitive.radius ** 2) *\n                  self.primitive.height)\n        return volume", "response": "Returns the analytic volume of the cylinder primitive."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn an analytic inertia tensor of the cylinder primitive.", "response": "def moment_inertia(self):\n        \"\"\"\n        The analytic inertia tensor of the cylinder primitive.\n\n        Returns\n        ----------\n        tensor: (3,3) float, 3D inertia tensor\n        \"\"\"\n\n        tensor = inertia.cylinder_inertia(\n            mass=self.volume,\n            radius=self.primitive.radius,\n            height=self.primitive.height,\n            transform=self.primitive.transform)\n        return tensor"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef buffer(self, distance):\n        distance = float(distance)\n\n        buffered = Cylinder(\n            height=self.primitive.height + distance * 2,\n            radius=self.primitive.radius + distance,\n            transform=self.primitive.transform.copy())\n        return buffered", "response": "Return a buffered cylinder that covers the source cylinder at a given distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies a transform to the sphere primitive", "response": "def apply_transform(self, matrix):\n        \"\"\"\n        Apply a transform to the sphere primitive\n\n        Parameters\n        ------------\n        matrix: (4,4) float, homogenous transformation\n        \"\"\"\n        matrix = np.asanyarray(matrix, dtype=np.float64)\n        if matrix.shape != (4, 4):\n            raise ValueError('shape must be 4,4')\n\n        center = np.dot(matrix,\n                        np.append(self.primitive.center, 1.0))[:3]\n        self.primitive.center = center"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the analytic inertia tensor of the sphere primitive.", "response": "def moment_inertia(self):\n        \"\"\"\n        The analytic inertia tensor of the sphere primitive.\n\n        Returns\n        ----------\n        tensor: (3,3) float, 3D inertia tensor\n        \"\"\"\n        tensor = inertia.sphere_inertia(mass=self.volume,\n                                        radius=self.primitive.radius)\n        return tensor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample_volume(self, count):\n        samples = sample.volume_rectangular(\n            extents=self.primitive.extents,\n            count=count,\n            transform=self.primitive.transform)\n        return samples", "response": "Return random samples from inside the volume of the box."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a 3D grid containing the points in the box.", "response": "def sample_grid(self, count=None, step=None):\n        \"\"\"\n        Return a 3D grid which is contained by the box.\n        Samples are either 'step' distance apart, or there are\n        'count' samples per box side.\n\n        Parameters\n        -----------\n        count : int or (3,) int\n          If specified samples are spaced with np.linspace\n        step : float or (3,) float\n          If specified samples are spaced with np.arange\n\n        Returns\n        -----------\n        grid : (n, 3) float\n          Points inside the box\n        \"\"\"\n\n        if (count is not None and\n                step is not None):\n            raise ValueError('only step OR count can be specified!')\n\n        # create pre- transform bounds from extents\n        bounds = np.array([-self.primitive.extents,\n                           self.primitive.extents]) * .5\n\n        if step is not None:\n            grid = util.grid_arange(bounds, step=step)\n        elif count is not None:\n            grid = util.grid_linspace(bounds, count=count)\n        else:\n            raise ValueError('either count or step must be specified!')\n\n        transformed = transformations.transform_points(\n            grid, matrix=self.primitive.transform)\n        return transformed"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_oriented(self):\n        if util.is_shape(self.primitive.transform, (4, 4)):\n            return not np.allclose(self.primitive.transform[\n                                   0:3, 0:3], np.eye(3))\n        else:\n            return False", "response": "Returns whether or not the current box is oriented at all."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the volume of the box Primitive.", "response": "def volume(self):\n        \"\"\"\n        Volume of the box Primitive.\n\n        Returns\n        --------\n        volume: float, volume of box\n        \"\"\"\n        volume = float(np.product(self.primitive.extents))\n        return volume"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the surface area of the primitive extrusion.", "response": "def area(self):\n        \"\"\"\n        The surface area of the primitive extrusion.\n\n        Calculated from polygon and height to avoid mesh creation.\n\n        Returns\n        ----------\n        area: float, surface area of 3D extrusion\n        \"\"\"\n        # area of the sides of the extrusion\n        area = abs(self.primitive.height *\n                   self.primitive.polygon.length)\n        # area of the two caps of the extrusion\n        area += self.primitive.polygon.area * 2\n        return area"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the volume of the primitive extrusion.", "response": "def volume(self):\n        \"\"\"\n        The volume of the primitive extrusion.\n\n        Calculated from polygon and height to avoid mesh creation.\n\n        Returns\n        ----------\n        volume: float, volume of 3D extrusion\n        \"\"\"\n        volume = abs(self.primitive.polygon.area *\n                     self.primitive.height)\n        return volume"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef direction(self):\n        direction = np.dot(self.primitive.transform[:3, :3],\n                           [0.0, 0.0, np.sign(self.primitive.height)])\n        return direction", "response": "Returns the direction of the object in the system."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nalters the transform of the current extrusion to slide it along its extrude_direction vector Parameters ----------- distance: float, distance along self.extrude_direction to move", "response": "def slide(self, distance):\n        \"\"\"\n        Alter the transform of the current extrusion to slide it\n        along its extrude_direction vector\n\n        Parameters\n        -----------\n        distance: float, distance along self.extrude_direction to move\n        \"\"\"\n        distance = float(distance)\n        translation = np.eye(4)\n        translation[2, 3] = distance\n        new_transform = np.dot(self.primitive.transform.copy(),\n                               translation.copy())\n        self.primitive.transform = new_transform"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a new Extrusion object which is expanded in profile and in height by a specified distance.", "response": "def buffer(self, distance):\n        \"\"\"\n        Return a new Extrusion object which is expanded in profile and\n        in height by a specified distance.\n\n        Returns\n        ----------\n        buffered: Extrusion object\n        \"\"\"\n        distance = float(distance)\n\n        # start with current height\n        height = self.primitive.height\n        # if current height is negative offset by negative amount\n        height += np.sign(height) * 2.0 * distance\n\n        buffered = Extrusion(\n            transform=self.primitive.transform.copy(),\n            polygon=self.primitive.polygon.buffer(distance),\n            height=height)\n\n        # slide the stock along the axis\n        buffered.slide(-np.sign(height) * distance)\n\n        return buffered"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of shapely polygons with only exteriors, find which curves represent the exterior shell or root curve and which represent holes which penetrate the exterior. This is done with an R-tree for rough overlap detection, and then exact polygon queries for a final result. Parameters ----------- polygons : (n,) shapely.geometry.Polygon Polygons which only have exteriors and may overlap Returns ----------- roots : (m,) int Index of polygons which are root contains : networkx.DiGraph Edges indicate a polygon is contained by another polygon", "response": "def enclosure_tree(polygons):\n    \"\"\"\n    Given a list of shapely polygons with only exteriors,\n    find which curves represent the exterior shell or root curve\n    and which represent holes which penetrate the exterior.\n\n    This is done with an R-tree for rough overlap detection,\n    and then exact polygon queries for a final result.\n\n    Parameters\n    -----------\n    polygons : (n,) shapely.geometry.Polygon\n       Polygons which only have exteriors and may overlap\n\n    Returns\n    -----------\n    roots : (m,) int\n        Index of polygons which are root\n    contains : networkx.DiGraph\n       Edges indicate a polygon is\n       contained by another polygon\n    \"\"\"\n    tree = Rtree()\n    # nodes are indexes in polygons\n    contains = nx.DiGraph()\n    for i, polygon in enumerate(polygons):\n        # if a polygon is None it means creation\n        # failed due to weird geometry so ignore it\n        if polygon is None or len(polygon.bounds) != 4:\n            continue\n        # insert polygon bounds into rtree\n        tree.insert(i, polygon.bounds)\n        # make sure every valid polygon has a node\n        contains.add_node(i)\n\n    # loop through every polygon\n    for i in contains.nodes():\n        polygon = polygons[i]\n        # we first query for bounding box intersections from the R-tree\n        for j in tree.intersection(polygon.bounds):\n            # if we are checking a polygon against itself continue\n            if (i == j):\n                continue\n            # do a more accurate polygon in polygon test\n            # for the enclosure tree information\n            if polygons[i].contains(polygons[j]):\n                contains.add_edge(i, j)\n            elif polygons[j].contains(polygons[i]):\n                contains.add_edge(j, i)\n\n    # a root or exterior curve has an even number of parents\n    # wrap in dict call to avoid networkx view\n    degree = dict(contains.in_degree())\n\n    # convert keys and values to numpy arrays\n    indexes = np.array(list(degree.keys()))\n    degrees = np.array(list(degree.values()))\n\n    # roots are curves with an even inward degree (parent count)\n    roots = indexes[(degrees % 2) == 0]\n\n    # if there are multiple nested polygons split the graph\n    # so the contains logic returns the individual polygons\n    if len(degrees) > 0 and degrees.max() > 1:\n        # collect new edges for graph\n        edges = []\n        # find edges of subgraph for each root and children\n        for root in roots:\n            children = indexes[degrees == degree[root] + 1]\n            edges.extend(contains.subgraph(np.append(children, root)).edges())\n        # stack edges into new directed graph\n        contains = nx.from_edgelist(edges, nx.DiGraph())\n        # if roots have no children add them anyway\n        contains.add_nodes_from(roots)\n\n    return roots, contains"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives an edge list of indices and associated vertices generate a list of polygons.", "response": "def edges_to_polygons(edges, vertices):\n    \"\"\"\n    Given an edge list of indices and associated vertices\n    representing lines, generate a list of polygons.\n\n    Parameters\n    -----------\n    edges : (n, 2) int\n      Indexes of vertices which represent lines\n    vertices : (m, 2) float\n      Vertices in 2D space\n\n    Returns\n    ----------\n    polygons : (p,) shapely.geometry.Polygon\n      Polygon objects with interiors\n    \"\"\"\n\n    # create closed polygon objects\n    polygons = []\n    # loop through a sequence of ordered traversals\n    for dfs in graph.traversals(edges, mode='dfs'):\n        try:\n            # try to recover polygons before they are more complicated\n            polygons.append(repair_invalid(Polygon(vertices[dfs])))\n        except ValueError:\n            continue\n\n    # if there is only one polygon, just return it\n    if len(polygons) == 1:\n        return polygons\n\n    # find which polygons contain which other polygons\n    roots, tree = enclosure_tree(polygons)\n\n    # generate list of polygons with proper interiors\n    complete = []\n    for root in roots:\n        interior = list(tree[root].keys())\n        shell = polygons[root].exterior.coords\n        holes = [polygons[i].exterior.coords for i in interior]\n        complete.append(Polygon(shell=shell,\n                                holes=holes))\n    return complete"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the OBBs for a list of shapely. geometry. Polygons", "response": "def polygons_obb(polygons):\n    \"\"\"\n    Find the OBBs for a list of shapely.geometry.Polygons\n    \"\"\"\n    rectangles = [None] * len(polygons)\n    transforms = [None] * len(polygons)\n    for i, p in enumerate(polygons):\n        transforms[i], rectangles[i] = polygon_obb(p)\n    return np.array(transforms), np.array(rectangles)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the oriented bounding box of a shapely. geometry. Polygon object", "response": "def polygon_obb(polygon):\n    \"\"\"\n    Find the oriented bounding box of a Shapely polygon.\n\n    The OBB is always aligned with an edge of the convex hull of the polygon.\n\n    Parameters\n    -------------\n    polygons: shapely.geometry.Polygon\n\n    Returns\n    -------------\n    transform: (3,3) float, transformation matrix\n               which will move input polygon from its original position\n               to the first quadrant where the AABB is the OBB\n    extents:   (2,) float, extents of transformed polygon\n    \"\"\"\n    if hasattr(polygon, 'exterior'):\n        points = np.asanyarray(polygon.exterior.coords)\n    elif isinstance(polygon, np.ndarray):\n        points = polygon\n    else:\n        raise ValueError('polygon or points must be provided')\n    return bounds.oriented_bounds_2D(points)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transform_polygon(polygon, matrix):\n    matrix = np.asanyarray(matrix, dtype=np.float64)\n\n    if util.is_sequence(polygon):\n        result = [transform_polygon(p, t)\n                  for p, t in zip(polygon, matrix)]\n        return result\n    # transform the outer shell\n    shell = transform_points(np.array(polygon.exterior.coords),\n                             matrix)[:, :2]\n    # transform the interiors\n    holes = [transform_points(np.array(i.coords),\n                              matrix)[:, :2]\n             for i in polygon.interiors]\n    # create a new polygon with the result\n    result = Polygon(shell=shell, holes=holes)\n    return result", "response": "Transform a shapely. geometry. Polygon polygon by a 2D homogenous transformation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nplot a shapely polygon using matplotlib.", "response": "def plot_polygon(polygon, show=True, **kwargs):\n    \"\"\"\n    Plot a shapely polygon using matplotlib.\n\n    Parameters\n    ------------\n    polygon : shapely.geometry.Polygon\n      Polygon to be plotted\n    show : bool\n      If True will display immediately\n    **kwargs\n      Passed to plt.plot\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    def plot_single(single):\n        plt.plot(*single.exterior.xy, **kwargs)\n        for interior in single.interiors:\n            plt.plot(*interior.xy, **kwargs)\n    # make aspect ratio non- stupid\n    plt.axes().set_aspect('equal', 'datalim')\n    if util.is_sequence(polygon):\n        [plot_single(i) for i in polygon]\n    else:\n        plot_single(polygon)\n\n    if show:\n        plt.show()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resample_boundaries(polygon, resolution, clip=None):\n    def resample_boundary(boundary):\n        # add a polygon.exterior or polygon.interior to\n        # the deque after resampling based on our resolution\n        count = boundary.length / resolution\n        count = int(np.clip(count, *clip))\n        return resample_path(boundary.coords, count=count)\n    if clip is None:\n        clip = [8, 200]\n    # create a sequence of [(n,2)] points\n    kwargs = {'shell': resample_boundary(polygon.exterior),\n              'holes': deque()}\n    for interior in polygon.interiors:\n        kwargs['holes'].append(resample_boundary(interior))\n    kwargs['holes'] = np.array(kwargs['holes'])\n    return kwargs", "response": "Resample a polygon with boundaries resampled to a specified resolution."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stack_boundaries(boundaries):\n    if len(boundaries['holes']) == 0:\n        return boundaries['shell']\n    result = np.vstack((boundaries['shell'],\n                        np.vstack(boundaries['holes'])))\n    return result", "response": "Stack the boundaries of a single\n    into a single single\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a shapely polygon find the approximate medial axis of the polygon.", "response": "def medial_axis(polygon,\n                resolution=None,\n                clip=None):\n    \"\"\"\n    Given a shapely polygon, find the approximate medial axis\n    using a voronoi diagram of evenly spaced points on the\n    boundary of the polygon.\n\n    Parameters\n    ----------\n    polygon : shapely.geometry.Polygon\n      The source geometry\n    resolution : float\n      Distance between each sample on the polygon boundary\n    clip : None, or (2,) int\n      Clip sample count to min of clip[0] and max of clip[1]\n\n    Returns\n    ----------\n    edges : (n, 2) int\n      Vertex indices representing line segments\n      on the polygon's medial axis\n    vertices : (m, 2) float\n      Vertex positions in space\n    \"\"\"\n    from scipy.spatial import Voronoi\n\n    if resolution is None:\n        resolution = .01\n\n    # get evenly spaced points on the polygons boundaries\n    samples = resample_boundaries(polygon=polygon,\n                                  resolution=resolution,\n                                  clip=clip)\n    # stack the boundary into a (m,2) float array\n    samples = stack_boundaries(samples)\n    # create the voronoi diagram on 2D points\n    voronoi = Voronoi(samples)\n    # which voronoi vertices are contained inside the polygon\n    contains = vectorized.contains(polygon, *voronoi.vertices.T)\n    # ridge vertices of -1 are outside, make sure they are False\n    contains = np.append(contains, False)\n    # make sure ridge vertices is numpy array\n    ridge = np.asanyarray(voronoi.ridge_vertices, dtype=np.int64)\n    # only take ridges where every vertex is contained\n    edges = ridge[contains[ridge].all(axis=1)]\n\n    return edges, voronoi.vertices"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef polygon_hash(polygon):\n    result = np.array(\n        [len(polygon.interiors),\n         polygon.convex_hull.area,\n         polygon.convex_hull.length,\n         polygon.area,\n         polygon.length,\n         polygon.exterior.length],\n        dtype=np.float64)\n    return result", "response": "Returns a vector containing values representitive of a particular polygon."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a random polygon with a maximum number of sides and approximate radius.", "response": "def random_polygon(segments=8, radius=1.0):\n    \"\"\"\n    Generate a random polygon with a maximum number of sides and approximate radius.\n\n    Parameters\n    ---------\n    segments: int, the maximum number of sides the random polygon will have\n    radius:   float, the approximate radius of the polygon desired\n\n    Returns\n    ---------\n    polygon: shapely.geometry.Polygon object with random exterior, and no interiors.\n    \"\"\"\n    angles = np.sort(np.cumsum(np.random.random(\n        segments) * np.pi * 2) % (np.pi * 2))\n    radii = np.random.random(segments) * radius\n    points = np.column_stack(\n        (np.cos(angles), np.sin(angles))) * radii.reshape((-1, 1))\n    points = np.vstack((points, points[0]))\n    polygon = Polygon(points).buffer(0.0)\n    if util.is_sequence(polygon):\n        return polygon[0]\n    return polygon"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef polygon_scale(polygon):\n    extents = np.reshape(polygon.bounds, (2, 2)).ptp(axis=0)\n    scale = (extents ** 2).sum() ** .5\n\n    return scale", "response": "Returns the length of the AABB in the polygon."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a sequence of closed points turn them into shapely Polygons.", "response": "def paths_to_polygons(paths, scale=None):\n    \"\"\"\n    Given a sequence of connected points turn them into\n    valid shapely Polygon objects.\n\n    Parameters\n    -----------\n    paths : (n,) sequence\n        Of (m,2) float, closed paths\n    scale: float\n        Approximate scale of drawing for precision\n\n    Returns\n    -----------\n    polys: (p,) list\n        shapely.geometry.Polygon\n        None\n    \"\"\"\n    polygons = [None] * len(paths)\n    for i, path in enumerate(paths):\n        if len(path) < 4:\n            # since the first and last vertices are identical in\n            # a closed loop a 4 vertex path is the minimum for\n            # non-zero area\n            continue\n        try:\n            polygons[i] = repair_invalid(Polygon(path), scale)\n        except ValueError:\n            # raised if a polygon is unrecoverable\n            continue\n        except BaseException:\n            log.error('unrecoverable polygon', exc_info=True)\n    polygons = np.array(polygons)\n    return polygons"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample(polygon, count, factor=1.5, max_iter=10):\n    bounds = np.reshape(polygon.bounds, (2, 2))\n    extents = bounds.ptp(axis=0)\n\n    hit = []\n    hit_count = 0\n    per_loop = int(count * factor)\n\n    for i in range(max_iter):\n        # generate points inside polygons AABB\n        points = np.random.random((per_loop, 2))\n        points = (points * extents) + bounds[0]\n\n        # do the point in polygon test and append resulting hits\n        mask = vectorized.contains(polygon, *points.T)\n        hit.append(points[mask])\n\n        # keep track of how many points we've collected\n        hit_count += len(hit[-1])\n\n        # if we have enough points exit the loop\n        if hit_count > count:\n            break\n\n    # stack the hits into an (n,2) array and truncate\n    hit = np.vstack(hit)[:count]\n\n    return hit", "response": "Use rejection sampling to generate random points inside a polygon."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a shapely. geometry. Polygon attempt to return a valid version of the polygon.", "response": "def repair_invalid(polygon, scale=None, rtol=.5):\n    \"\"\"\n    Given a shapely.geometry.Polygon, attempt to return a\n    valid version of the polygon through buffering tricks.\n\n    Parameters\n    -----------\n    polygon: shapely.geometry.Polygon object\n    rtol:    float, how close does a perimeter have to be\n    scale:   float, or None\n\n    Returns\n    ----------\n    repaired: shapely.geometry.Polygon object\n\n    Raises\n    ----------\n    ValueError: if polygon can't be repaired\n    \"\"\"\n    if hasattr(polygon, 'is_valid') and polygon.is_valid:\n        return polygon\n\n    # basic repair involves buffering the polygon outwards\n    # this will fix a subset of problems.\n    basic = polygon.buffer(tol.zero)\n    # if it returned multiple polygons check the largest\n    if util.is_sequence(basic):\n        basic = basic[np.argmax([i.area for i in basic])]\n\n    # check perimeter of result against original perimeter\n    if basic.is_valid and np.isclose(basic.length,\n                                     polygon.length,\n                                     rtol=rtol):\n        return basic\n\n    if scale is None:\n        distance = tol.buffer * polygon_scale(polygon)\n    else:\n        distance = tol.buffer * scale\n\n    # if there are no interiors, we can work with just the exterior\n    # ring, which is often more reliable\n    if len(polygon.interiors) == 0:\n        # try buffering the exterior of the polygon\n        # the interior will be offset by -tol.buffer\n        rings = polygon.exterior.buffer(distance).interiors\n        if len(rings) == 1:\n            # reconstruct a single polygon from the interior ring\n            recon = Polygon(shell=rings[0]).buffer(distance)\n            # check perimeter of result against original perimeter\n            if recon.is_valid and np.isclose(recon.length,\n                                             polygon.length,\n                                             rtol=rtol):\n                return recon\n\n    # buffer and unbuffer the whole polygon\n    buffered = polygon.buffer(distance).buffer(-distance)\n    # if it returned multiple polygons check the largest\n    if util.is_sequence(buffered):\n        buffered = buffered[np.argmax([i.area for i in buffered])]\n    # check perimeter of result against original perimeter\n    if buffered.is_valid and np.isclose(buffered.length,\n                                        polygon.length,\n                                        rtol=rtol):\n        log.debug('Recovered invalid polygon through double buffering')\n        return buffered\n\n    raise ValueError('unable to recover polygon!')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting a scene object as a GLTF file.", "response": "def export_gltf(scene,\n                extras=None,\n                include_normals=False):\n    \"\"\"\n    Export a scene object as a GLTF directory.\n\n    This puts each mesh into a separate file (i.e. a `buffer`)\n    as opposed to one larger file.\n\n    Parameters\n    -----------\n    scene : trimesh.Scene\n      Scene to be exported\n\n    Returns\n    ----------\n    export : dict\n      Format: {file name : file data}\n    \"\"\"\n    # if we were passed a bare Trimesh or Path3D object\n    if (not util.is_instance_named(scene, \"Scene\")\n            and hasattr(scene, \"scene\")):\n        scene = scene.scene()\n\n    # create the header and buffer data\n    tree, buffer_items = _create_gltf_structure(\n        scene=scene,\n        extras=extras,\n        include_normals=include_normals)\n\n    # store files as {name : data}\n    files = {}\n    # make one buffer per buffer_items\n    buffers = [None] * len(buffer_items)\n    # A bufferView is a slice of a file\n    views = [None] * len(buffer_items)\n    # create the buffer views\n    for i, item in enumerate(buffer_items):\n        views[i] = {\n            \"buffer\": i,\n            \"byteOffset\": 0,\n            \"byteLength\": len(item)}\n\n        buffer_data = _byte_pad(bytes().join(buffer_items[i: i + 2]))\n        buffer_name = \"gltf_buffer_{}.bin\".format(i)\n        buffers[i] = {\n            \"uri\": buffer_name,\n            \"byteLength\": len(buffer_data)}\n        files[buffer_name] = buffer_data\n\n    tree[\"buffers\"] = buffers\n    tree[\"bufferViews\"] = views\n    files[\"model.gltf\"] = json.dumps(tree).encode(\"utf-8\")\n    return files"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef export_glb(scene, extras=None, include_normals=False):\n    # if we were passed a bare Trimesh or Path3D object\n    if not util.is_instance_named(scene, \"Scene\") and hasattr(scene, \"scene\"):\n        # generate a scene with just that mesh in it\n        scene = scene.scene()\n\n    tree, buffer_items = _create_gltf_structure(\n        scene=scene,\n        extras=extras,\n        include_normals=include_normals)\n\n    # A bufferView is a slice of a file\n    views = []\n    # create the buffer views\n    current_pos = 0\n    for current_item in buffer_items:\n        views.append(\n            {\"buffer\": 0,\n             \"byteOffset\": current_pos,\n             \"byteLength\": len(current_item)}\n        )\n        current_pos += len(current_item)\n\n    buffer_data = bytes().join(buffer_items)\n\n    tree[\"buffers\"] = [{\"byteLength\": len(buffer_data)}]\n    tree[\"bufferViews\"] = views\n\n    # export the tree to JSON for the content of the file\n    content = json.dumps(tree)\n    # add spaces to content, so the start of the data\n    # is 4 byte aligned as per spec\n    content += (4 - ((len(content) + 20) % 4)) * \" \"\n    content = content.encode(\"utf-8\")\n    # make sure we didn't screw it up\n    assert (len(content) % 4) == 0\n\n    # the initial header of the file\n    header = _byte_pad(\n        np.array([_magic[\"gltf\"],  # magic, turns into glTF\n                  2,               # GLTF version\n                  # length is the total length of the Binary glTF\n                  # including Header and all Chunks, in bytes.\n                  len(content) + len(buffer_data) + 28,\n                  # contentLength is the length, in bytes,\n                  # of the glTF content (JSON)\n                  len(content),\n                  # magic number which is 'JSON'\n                  1313821514],\n                 dtype=\"<u4\",\n                 ).tobytes())\n\n    # the header of the binary data section\n    bin_header = _byte_pad(\n        np.array([len(buffer_data), 0x004E4942],\n                 dtype=\"<u4\").tobytes())\n\n    exported = bytes().join([header,\n                             content,\n                             bin_header,\n                             buffer_data])\n\n    return exported", "response": "Export a scene as a binary GLTF file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_gltf(file_obj=None,\n              resolver=None,\n              **mesh_kwargs):\n    \"\"\"\n    Load a GLTF file, which consists of a directory structure\n    with multiple files.\n\n    Parameters\n    -------------\n    file_obj : None or file-like\n      Object containing header JSON, or None\n    resolver : trimesh.visual.Resolver\n      Object which can be used to load other files by name\n    **mesh_kwargs : dict\n      Passed to mesh constructor\n\n    Returns\n    --------------\n    kwargs : dict\n      Arguments to create scene\n    \"\"\"\n    try:\n        # see if we've been passed the GLTF header file\n        tree = json.load(file_obj)\n    except BaseException:\n        # otherwise header should be in 'model.gltf'\n        data = resolver['model.gltf']\n        # old versions of python/json need strings\n        try:\n            tree = json.loads(data)\n        except BaseException:\n            tree = json.loads(data.decode('utf-8'))\n\n    # use the resolver to get data from file names\n    buffers = [resolver[b['uri']] for b in tree['buffers']]\n\n    # turn the layout header and data into kwargs\n    # that can be used to instantiate a trimesh.Scene object\n    kwargs = _read_buffers(header=tree,\n                           buffers=buffers,\n                           mesh_kwargs=mesh_kwargs)\n    return kwargs", "response": "Loads a GLTF file and returns a dict containing the kwargs and the scene object that can be used to instantiate a trimesh. Scene object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_glb(file_obj, resolver=None, **mesh_kwargs):\n\n    # save the start position of the file for referencing\n    # against lengths\n    start = file_obj.tell()\n    # read the first 20 bytes which contain section lengths\n    head_data = file_obj.read(20)\n    head = np.frombuffer(head_data, dtype=\"<u4\")\n\n    # check to make sure first index is gltf\n    # and second is 2, for GLTF 2.0\n    if head[0] != _magic[\"gltf\"] or head[1] != 2:\n        raise ValueError(\"file is not GLTF 2.0\")\n\n    # overall file length\n    # first chunk length\n    # first chunk type\n    length, chunk_length, chunk_type = head[2:]\n\n    # first chunk should be JSON header\n    if chunk_type != _magic[\"json\"]:\n        raise ValueError(\"no initial JSON header!\")\n\n    # uint32 causes an error in read, so we convert to native int\n    # for the length passed to read, for the JSON header\n    json_data = file_obj.read(int(chunk_length))\n    # convert to text\n    if hasattr(json_data, \"decode\"):\n        json_data = json_data.decode(\"utf-8\")\n    # load the json header to native dict\n    header = json.loads(json_data)\n\n    # read the binary data referred to by GLTF as 'buffers'\n    buffers = []\n    while (file_obj.tell() - start) < length:\n        # the last read put us past the JSON chunk\n        # we now read the chunk header, which is 8 bytes\n        chunk_head = file_obj.read(8)\n        if len(chunk_head) != 8:\n            # double check to make sure we didn't\n            # read the whole file\n            break\n\n        chunk_length, chunk_type = np.frombuffer(chunk_head, dtype=\"<u4\")\n        # make sure we have the right data type\n        if chunk_type != _magic[\"bin\"]:\n            raise ValueError(\"not binary GLTF!\")\n        # read the chunk\n        chunk_data = file_obj.read(int(chunk_length))\n        if len(chunk_data) != chunk_length:\n            raise ValueError(\"chunk was not expected length!\")\n        buffers.append(chunk_data)\n\n    # turn the layout header and data into kwargs\n    # that can be used to instantiate a trimesh.Scene object\n    kwargs = _read_buffers(header=header,\n                           buffers=buffers,\n                           mesh_kwargs=mesh_kwargs)\n    return kwargs", "response": "Load a GLTF file into a trimesh. Scene object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts a mesh to a GLTF material", "response": "def _mesh_to_material(mesh, metallic=0.0, rough=0.0):\n    \"\"\"\n    Create a simple GLTF material for a mesh using the most\n    commonly occurring color in that mesh.\n\n    Parameters\n    ------------\n    mesh: trimesh.Trimesh\n      Mesh to create a material from\n\n    Returns\n    ------------\n    material: dict\n      In GLTF material format\n    \"\"\"\n\n    try:\n        # just get the most commonly occurring color\n        color = mesh.visual.main_color\n    except BaseException:\n        color = np.array([100, 100, 100, 255], dtype=np.uint8)\n\n    # convert uint color to 0.0-1.0 float color\n    color = color.astype(float32) / np.iinfo(color.dtype).max\n\n    material = {\n        \"pbrMetallicRoughness\": {\n            \"baseColorFactor\": color.tolist(),\n            \"metallicFactor\": metallic,\n            \"roughnessFactor\": rough}\n    }\n\n    return material"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a GLTF structure from a scene.", "response": "def _create_gltf_structure(scene,\n                           extras=None,\n                           include_normals=False):\n    \"\"\"\n    Generate a GLTF header.\n\n    Parameters\n    -------------\n    scene : trimesh.Scene\n      Input scene data\n    extras : JSON serializable\n      Will be stored in the extras field\n    include_normals : bool\n      Include vertex normals in output file?\n\n    Returns\n    ---------------\n    tree : dict\n      Contains required keys for a GLTF scene\n    buffer_items : list\n      Contains bytes of data\n    \"\"\"\n    # we are defining a single scene, and will be setting the\n    # world node to the 0th index\n    tree = {\n        \"scene\": 0,\n        \"scenes\": [{\"nodes\": [0]}],\n        \"asset\": {\"version\": \"2.0\",\n                  \"generator\": \"github.com/mikedh/trimesh\"},\n        \"accessors\": [],\n        \"meshes\": [],\n        \"materials\": [],\n        \"cameras\": [_convert_camera(scene.camera)]}\n\n    if extras is not None:\n        tree['extras'] = extras\n\n    # grab the flattened scene graph in GLTF's format\n    nodes = scene.graph.to_gltf(scene=scene)\n    tree.update(nodes)\n\n    buffer_items = []\n    for name, geometry in scene.geometry.items():\n        if util.is_instance_named(geometry, \"Trimesh\"):\n            # add the mesh\n            _append_mesh(\n                mesh=geometry,\n                name=name,\n                tree=tree,\n                buffer_items=buffer_items,\n                include_normals=include_normals)\n        elif util.is_instance_named(geometry, \"Path\"):\n            # add Path2D and Path3D objects\n            _append_path(\n                path=geometry,\n                name=name,\n                tree=tree,\n                buffer_items=buffer_items)\n    # if nothing defined a material remove it from the structure\n    if len(tree[\"materials\"]) == 0:\n        tree.pop(\"materials\")\n\n    return tree, buffer_items"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nappends a mesh to the scene structure and put it into buffer_items.", "response": "def _append_mesh(mesh,\n                 name,\n                 tree,\n                 buffer_items,\n                 include_normals):\n    \"\"\"\n    Append a mesh to the scene structure and put the\n    data into buffer_items.\n\n    Parameters\n    -------------\n    mesh : trimesh.Trimesh\n      Source geometry\n    name : str\n      Name of geometry\n    tree : dict\n      Will be updated with data from mesh\n    buffer_items\n      Will have buffer appended with mesh data\n    include_normals : bool\n      Include vertex normals in export or not\n    \"\"\"\n    # meshes reference accessor indexes\n    # mode 4 is GL_TRIANGLES\n    tree[\"meshes\"].append({\n        \"name\": name,\n        \"primitives\": [{\n            \"attributes\": {\"POSITION\": len(tree[\"accessors\"]) + 1},\n            \"indices\": len(tree[\"accessors\"]),\n            \"mode\": 4}]})\n\n    # if units are defined, store them as an extra\n    # the GLTF spec says everything is implicit meters\n    # we're not doing that as our unit conversions are expensive\n    # although that might be better, implicit works for 3DXML\n    # https://github.com/KhronosGroup/glTF/tree/master/extensions\n    if mesh.units is not None and 'meter' not in mesh.units:\n        tree[\"meshes\"][-1][\"extras\"] = {\"units\": str(mesh.units)}\n\n    # accessors refer to data locations\n    # mesh faces are stored as flat list of integers\n    tree[\"accessors\"].append({\n        \"bufferView\": len(buffer_items),\n        \"componentType\": 5125,\n        \"count\": len(mesh.faces) * 3,\n        \"max\": [int(mesh.faces.max())],\n        \"min\": [0],\n        \"type\": \"SCALAR\"})\n    # convert mesh data to the correct dtypes\n    # faces: 5125 is an unsigned 32 bit integer\n    buffer_items.append(_byte_pad(\n        mesh.faces.astype(uint32).tobytes()))\n\n    # the vertex accessor\n    tree[\"accessors\"].append({\n        \"bufferView\": len(buffer_items),\n        \"componentType\": 5126,\n        \"count\": len(mesh.vertices),\n        \"type\": \"VEC3\",\n        \"byteOffset\": 0,\n        \"max\": mesh.vertices.max(axis=0).tolist(),\n        \"min\": mesh.vertices.min(axis=0).tolist()})\n    # vertices: 5126 is a float32\n    buffer_items.append(_byte_pad(\n        mesh.vertices.astype(float32).tobytes()))\n\n    assert len(buffer_items) >= tree['accessors'][-1]['bufferView']\n\n    # for now cheap hack to display\n    # crappy version of textured meshes\n    if hasattr(mesh.visual, \"uv\"):\n        visual = mesh.visual.to_color()\n    else:\n        visual = mesh.visual\n    color_ok = (\n        visual.kind in ['vertex', 'face'] and\n        visual.vertex_colors.shape == (len(mesh.vertices), 4))\n\n    # make sure to append colors after other stuff to\n    # not screw up the indexes of accessors or buffers\n    if color_ok:\n        # make sure colors are RGBA, this should always be true\n        vertex_colors = visual.vertex_colors\n\n        # add the reference for vertex color\n        tree[\"meshes\"][-1][\"primitives\"][0][\"attributes\"][\n            \"COLOR_0\"] = len(tree[\"accessors\"])\n\n        # convert color data to bytes\n        color_data = _byte_pad(vertex_colors.astype(uint8).tobytes())\n\n        # the vertex color accessor data\n        tree[\"accessors\"].append({\n            \"bufferView\": len(buffer_items),\n            \"componentType\": 5121,\n            \"normalized\": True,\n            \"count\": len(vertex_colors),\n            \"type\": \"VEC4\",\n            \"byteOffset\": 0})\n        # the actual color data\n        buffer_items.append(color_data)\n    else:\n        # if no colors, set a material\n        tree[\"meshes\"][-1][\"primitives\"][0][\"material\"] = len(\n            tree[\"materials\"])\n        # add a default- ish material\n        tree[\"materials\"].append(_mesh_to_material(mesh))\n\n    if include_normals:\n        # add the reference for vertex color\n        tree[\"meshes\"][-1][\"primitives\"][0][\"attributes\"][\n            \"NORMAL\"] = len(tree[\"accessors\"])\n        normal_data = _byte_pad(mesh.vertex_normals.astype(\n            float32).tobytes())\n        # the vertex color accessor data\n        tree[\"accessors\"].append({\n            \"bufferView\": len(buffer_items),\n            \"componentType\": 5126,\n            \"count\": len(mesh.vertices),\n            \"type\": \"VEC3\",\n            \"byteOffset\": 0})\n        # the actual color data\n        buffer_items.append(normal_data)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _byte_pad(data, bound=4):\n    bound = int(bound)\n    if len(data) % bound != 0:\n        pad = bytes(bound - (len(data) % bound))\n        result = bytes().join([data, pad])\n        assert (len(result) % bound) == 0\n        return result\n\n    return data", "response": "This function pads the data to a specified boundary size."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nappending a path to the scene structure and put the data into buffer_items.", "response": "def _append_path(path, name, tree, buffer_items):\n    \"\"\"\n    Append a 2D or 3D path to the scene structure and put the\n    data into buffer_items.\n\n    Parameters\n    -------------\n    path : trimesh.Path2D or trimesh.Path3D\n      Source geometry\n    name : str\n      Name of geometry\n    tree : dict\n      Will be updated with data from path\n    buffer_items\n      Will have buffer appended with path data\n    \"\"\"\n\n    # convert the path to the unnamed args for\n    # a pyglet vertex list\n    vxlist = rendering.path_to_vertexlist(path)\n\n    tree[\"meshes\"].append({\n        \"name\": name,\n        \"primitives\": [{\n            \"attributes\": {\"POSITION\": len(tree[\"accessors\"])},\n            \"mode\": 1,  # mode 1 is GL_LINES\n            \"material\": len(tree[\"materials\"])}]})\n\n    # if units are defined, store them as an extra:\n    # https://github.com/KhronosGroup/glTF/tree/master/extensions\n    if path.units is not None and 'meter' not in path.units:\n        tree[\"meshes\"][-1][\"extras\"] = {\"units\": str(path.units)}\n\n    tree[\"accessors\"].append(\n        {\n            \"bufferView\": len(buffer_items),\n            \"componentType\": 5126,\n            \"count\": vxlist[0],\n            \"type\": \"VEC3\",\n            \"byteOffset\": 0,\n            \"max\": path.vertices.max(axis=0).tolist(),\n            \"min\": path.vertices.min(axis=0).tolist(),\n        }\n    )\n\n    # TODO add color support to Path object\n    # this is just exporting everying as black\n    tree[\"materials\"].append(_default_material)\n\n    # data is the second value of the fourth field\n    # which is a (data type, data) tuple\n    buffer_items.append(_byte_pad(\n        vxlist[4][1].astype(float32).tobytes()))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts materials and images stored in a GLTF header and buffer views into PBRMaterial objects.", "response": "def _parse_materials(header, views):\n    \"\"\"\n    Convert materials and images stored in a GLTF header\n    and buffer views to PBRMaterial objects.\n\n    Parameters\n    ------------\n    header : dict\n      Contains layout of file\n    views : (n,) bytes\n      Raw data\n\n    Returns\n    ------------\n    materials : list\n      List of trimesh.visual.texture.Material objects\n    \"\"\"\n    try:\n        import PIL.Image\n    except ImportError:\n        log.warning(\"unable to load textures without pillow!\")\n        return None\n\n    # load any images\n    images = None\n    if \"images\" in header:\n        # images are referenced by index\n        images = [None] * len(header[\"images\"])\n        # loop through images\n        for i, img in enumerate(header[\"images\"]):\n            # get the bytes representing an image\n            blob = views[img[\"bufferView\"]]\n            # i.e. 'image/jpeg'\n            # mime = img['mimeType']\n            try:\n                # load the buffer into a PIL image\n                images[i] = PIL.Image.open(util.wrap_as_stream(blob))\n            except BaseException:\n                log.error(\"failed to load image!\", exc_info=True)\n\n    # store materials which reference images\n    materials = []\n    if \"materials\" in header:\n        for mat in header[\"materials\"]:\n            # flatten key structure so we can loop it\n            loopable = mat.copy()\n            # this key stores another dict of crap\n            if \"pbrMetallicRoughness\" in loopable:\n                # add keys of keys to top level dict\n                loopable.update(loopable.pop(\"pbrMetallicRoughness\"))\n\n            # save flattened keys we can use for kwargs\n            pbr = {}\n            for k, v in loopable.items():\n                if not isinstance(v, dict):\n                    pbr[k] = v\n                elif \"index\" in v:\n                    # get the index of image for texture\n                    idx = header[\"textures\"][v[\"index\"]][\"source\"]\n                    # store the actual image as the value\n                    pbr[k] = images[idx]\n            # create a PBR material object for the GLTF material\n            materials.append(visual.texture.PBRMaterial(**pbr))\n\n    return materials"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads a list of binary data and a layout and return a list of kwargs for a scene object.", "response": "def _read_buffers(header, buffers, mesh_kwargs):\n    \"\"\"\n    Given a list of binary data and a layout, return the\n    kwargs to create a scene object.\n\n    Parameters\n    -----------\n    header : dict\n      With GLTF keys\n    buffers : list of bytes\n      Stored data\n    passed : dict\n      Kwargs for mesh constructors\n\n    Returns\n    -----------\n    kwargs : dict\n      Can be passed to load_kwargs for a trimesh.Scene\n    \"\"\"\n    # split buffer data into buffer views\n    views = [None] * len(header[\"bufferViews\"])\n    for i, view in enumerate(header[\"bufferViews\"]):\n        if \"byteOffset\" in view:\n            start = view[\"byteOffset\"]\n        else:\n            start = 0\n        end = start + view[\"byteLength\"]\n        views[i] = buffers[view[\"buffer\"]][start:end]\n\n        assert len(views[i]) == view[\"byteLength\"]\n\n    # load data from buffers and bufferviews into numpy arrays\n    # using the layout described by accessors\n    access = []\n    for a in header[\"accessors\"]:\n        data = views[a[\"bufferView\"]]\n        dtype = _types[a[\"componentType\"]]\n        shape = _shapes[a[\"type\"]]\n\n        # is the accessor offset in a buffer\n        if \"byteOffset\" in a:\n            start = a[\"byteOffset\"]\n        else:\n            start = 0\n        # basically the number of columns\n        per_count = np.abs(np.product(shape))\n        # length is the number of bytes per item times total\n        length = np.dtype(dtype).itemsize * a[\"count\"] * per_count\n        end = start + length\n\n        array = np.frombuffer(\n            data[start:end], dtype=dtype).reshape(shape)\n\n        assert len(array) == a[\"count\"]\n        access.append(array)\n\n    # load images and textures into material objects\n    materials = _parse_materials(header, views)\n\n    mesh_prim = collections.defaultdict(list)\n    # load data from accessors into Trimesh objects\n    meshes = collections.OrderedDict()\n    for index, m in enumerate(header[\"meshes\"]):\n\n        metadata = {}\n        try:\n            # try loading units from the GLTF extra\n            metadata['units'] = str(m[\"extras\"][\"units\"])\n        except BaseException:\n            # GLTF spec indicates the default units are meters\n            metadata['units'] = 'meters'\n\n        for j, p in enumerate(m[\"primitives\"]):\n            # if we don't have a triangular mesh continue\n            # if not specified assume it is a mesh\n            if \"mode\" in p and p[\"mode\"] != 4:\n                continue\n\n            # store those units\n            kwargs = {\"metadata\": {}}\n            kwargs.update(mesh_kwargs)\n            kwargs[\"metadata\"].update(metadata)\n\n            # get faces from accessors and reshape\n            kwargs[\"faces\"] = access[p[\"indices\"]].reshape((-1, 3))\n            # get vertices from accessors\n            kwargs[\"vertices\"] = access[p[\"attributes\"][\"POSITION\"]]\n\n            # do we have UV coordinates\n            if \"material\" in p:\n                if materials is None:\n                    log.warning('no materials! `pip install pillow`')\n                else:\n                    uv = None\n                    if \"TEXCOORD_0\" in p[\"attributes\"]:\n                        # flip UV's top- bottom to move origin to lower-left:\n                        # https://github.com/KhronosGroup/glTF/issues/1021\n                        uv = access[p[\"attributes\"][\"TEXCOORD_0\"]].copy()\n                        uv[:, 1] = 1.0 - uv[:, 1]\n                        # create a texture visual\n                    kwargs[\"visual\"] = visual.texture.TextureVisuals(\n                        uv=uv, material=materials[p[\"material\"]])\n\n            # create a unique mesh name per- primitive\n            if \"name\" in m:\n                name = m[\"name\"]\n            else:\n                name = \"GLTF_geometry\"\n\n            # each primitive gets it's own Trimesh object\n            if len(m[\"primitives\"]) > 1:\n                name += \"_{}\".format(j)\n            meshes[name] = kwargs\n            mesh_prim[index].append(name)\n\n    # make it easier to reference nodes\n    nodes = header[\"nodes\"]\n    # nodes are referenced by index\n    # save their string names if they have one\n    # node index (int) : name (str)\n    names = {}\n    for i, n in enumerate(nodes):\n        if \"name\" in n:\n            names[i] = n[\"name\"]\n        else:\n            names[i] = str(i)\n\n    # make sure we have a unique base frame name\n    base_frame = \"world\"\n    if base_frame in names:\n        base_frame = str(int(np.random.random() * 1e10))\n    names[base_frame] = base_frame\n\n    # visited, kwargs for scene.graph.update\n    graph = collections.deque()\n    # unvisited, pairs of node indexes\n    queue = collections.deque()\n\n    # start the traversal from the base frame to the roots\n    for root in header[\"scenes\"][header[\"scene\"]][\"nodes\"]:\n        # add transform from base frame to these root nodes\n        queue.append([base_frame, root])\n\n    # go through the nodes tree to populate\n    # kwargs for scene graph loader\n    while len(queue) > 0:\n        # (int, int) pair of node indexes\n        a, b = queue.pop()\n\n        # dict of child node\n        # parent = nodes[a]\n        child = nodes[b]\n        # add edges of children to be processed\n        if \"children\" in child:\n            queue.extend([[b, i] for i in child[\"children\"]])\n\n        # kwargs to be passed to scene.graph.update\n        kwargs = {\"frame_from\": names[a], \"frame_to\": names[b]}\n\n        # grab matrix from child\n        # parent -> child relationships have matrix stored in child\n        # for the transform from parent to child\n        if \"matrix\" in child:\n            kwargs[\"matrix\"] = (\n                np.array(child[\"matrix\"],\n                         dtype=np.float64).reshape((4, 4)).T\n            )\n        else:\n            # if no matrix set identity\n            kwargs[\"matrix\"] = np.eye(4)\n\n        # Now apply keyword translations\n        # GLTF applies these in order: T * R * S\n        if \"translation\" in child:\n            kwargs[\"matrix\"] = np.dot(\n                kwargs[\"matrix\"],\n                transformations.translation_matrix(child[\"translation\"]),\n            )\n        if \"rotation\" in child:\n            # GLTF rotations are stored as (4,) XYZW unit quaternions\n            # we need to re- order to our quaternion style, WXYZ\n            quat = np.reshape(child[\"rotation\"], 4)[[3, 0, 1, 2]]\n\n            # add the rotation to the matrix\n            kwargs[\"matrix\"] = np.dot(\n                kwargs[\"matrix\"], transformations.quaternion_matrix(quat)\n            )\n\n        # append the nodes for connectivity without the mesh\n        graph.append(kwargs.copy())\n        if \"mesh\" in child:\n            # append a new node per- geometry instance\n            geometries = mesh_prim[child[\"mesh\"]]\n            for name in geometries:\n                kwargs[\"geometry\"] = name\n                kwargs[\"frame_to\"] = \"{}_{}\".format(\n                    name, util.unique_id(\n                        length=6, increment=len(graph)).upper()\n                )\n                # append the edge with the mesh frame\n                graph.append(kwargs.copy())\n\n    # kwargs to be loaded\n    result = {\n        \"class\": \"Scene\",\n        \"geometry\": meshes,\n        \"graph\": graph,\n        \"base_frame\": base_frame,\n    }\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_camera(camera):\n    result = {\n        \"name\": camera.name,\n        \"type\": \"perspective\",\n        \"perspective\": {\n            \"aspectRatio\": camera.fov[0] / camera.fov[1],\n            \"yfov\": np.radians(camera.fov[1]),\n        },\n    }\n    return result", "response": "Convert a trimesh camera to a GLTF camera."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting an asset. ArcGIS file.", "response": "def get(self, name):\n        \"\"\"\n        Get an asset.\n\n        Parameters\n        -------------\n        name : str\n          Name of the asset\n\n        Returns\n        ------------\n        data : bytes\n          Loaded data from asset\n        \"\"\"\n        # load the file by path name\n        with open(os.path.join(self.parent,\n                               name.strip()), 'rb') as f:\n            data = f.read()\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, name):\n        # not much we can do with that\n        if name is None:\n            return\n\n        # if name isn't in archive try some similar values\n        if name not in self.archive:\n            if hasattr(name, 'decode'):\n                name = name.decode('utf-8')\n            # try with cleared whitespace, split paths\n            for option in [name,\n                           name.lstrip('./'),\n                           name.strip(),\n                           name.split('/')[-1]]:\n                if option in self.archive:\n                    name = option\n                    break\n        # read file object from beginning\n        self.archive[name].seek(0)\n        # data is stored as a file object\n        data = self.archive[name].read()\n\n        return data", "response": "Get an asset from the ZIP archive."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a resource from the remote site.", "response": "def get(self, name):\n        \"\"\"\n        Get a resource from the remote site.\n\n        Parameters\n        -------------\n        name : str\n          Asset name, i.e. 'quadknot.obj.mtl'\n        \"\"\"\n        # do import here to keep soft dependency\n        import requests\n\n        # append base url to requested name\n        url = urljoin(self.base_url, name)\n        # fetch the data from the remote url\n        response = requests.get(url)\n        # return the bytes of the response\n        return response.content"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_translation(self, translation):\n        translation = np.asanyarray(translation, dtype=np.float64)\n        if translation.shape != (3,):\n            raise ValueError('Translation must be (3,)!')\n\n        matrix = np.eye(4)\n        matrix[:3, 3] = translation\n        self.apply_transform(matrix)", "response": "Apply a translation matrix to the current mesh."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_scale(self, scaling):\n        scaling = float(scaling)\n        if not np.isfinite(scaling):\n            raise ValueError('Scaling factor must be finite number!')\n\n        matrix = np.eye(4)\n        matrix[:3, :3] *= scaling\n        # apply_transform will work nicely even on negative scales\n        self.apply_transform(matrix)", "response": "Scale the mesh equally on all axes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the axis aligned bounding box for the current mesh.", "response": "def bounding_box(self):\n        \"\"\"\n        An axis aligned bounding box for the current mesh.\n\n        Returns\n        ----------\n        aabb : trimesh.primitives.Box\n          Box object with transform and extents defined\n          representing the axis aligned bounding box of the mesh\n        \"\"\"\n        from . import primitives\n\n        transform = np.eye(4)\n        # translate to center of axis aligned bounds\n        transform[:3, 3] = self.bounds.mean(axis=0)\n\n        aabb = primitives.Box(transform=transform,\n                              extents=self.extents,\n                              mutable=False)\n        return aabb"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bounding_box_oriented(self):\n        from . import primitives, bounds\n        to_origin, extents = bounds.oriented_bounds(self)\n        obb = primitives.Box(transform=np.linalg.inv(to_origin),\n                             extents=extents,\n                             mutable=False)\n        return obb", "response": "Returns the oriented bounding box for the current mesh."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bounding_sphere(self):\n        from . import primitives, nsphere\n        center, radius = nsphere.minimum_nsphere(self)\n        minball = primitives.Sphere(center=center,\n                                    radius=radius,\n                                    mutable=False)\n        return minball", "response": "Returns the minimum volume bounding sphere for the current mesh."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the minimum volume bounding cylinder for the current mesh.", "response": "def bounding_cylinder(self):\n        \"\"\"\n        A minimum volume bounding cylinder for the current mesh.\n\n        Returns\n        --------\n        mincyl : trimesh.primitives.Cylinder\n          Cylinder primitive containing current mesh\n        \"\"\"\n        from . import primitives, bounds\n        kwargs = bounds.minimum_cylinder(self)\n        mincyl = primitives.Cylinder(mutable=False, **kwargs)\n        return mincyl"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bounding_primitive(self):\n        options = [self.bounding_box_oriented,\n                   self.bounding_sphere,\n                   self.bounding_cylinder]\n        volume_min = np.argmin([i.volume for i in options])\n        bounding_primitive = options[volume_min]\n        return bounding_primitive", "response": "Returns the minimum volume primitive that bounds the mesh with the smallest volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nexport a Trimesh object to a file - like object or to a filename.", "response": "def export_mesh(mesh, file_obj, file_type=None, **kwargs):\n    \"\"\"\n    Export a Trimesh object to a file- like object, or to a filename\n\n    Parameters\n    ---------\n    file_obj : str, file-like\n      Where should mesh be exported to\n    file_type : str or None\n      Represents file type (eg: 'stl')\n\n    Returns\n    ----------\n    exported : bytes or str\n      Result of exporter\n    \"\"\"\n    # if we opened a file object in this function\n    # we will want to close it when we're done\n    was_opened = False\n\n    if util.is_string(file_obj):\n        if file_type is None:\n            file_type = (str(file_obj).split('.')[-1]).lower()\n        if file_type in _mesh_exporters:\n            was_opened = True\n            file_obj = open(file_obj, 'wb')\n    file_type = str(file_type).lower()\n\n    if not (file_type in _mesh_exporters):\n        raise ValueError('%s exporter not available!', file_type)\n\n    if isinstance(mesh, (list, tuple, set, np.ndarray)):\n        faces = 0\n        for m in mesh:\n            faces += len(m.faces)\n        log.debug('Exporting %d meshes with a total of %d faces as %s',\n                  len(mesh), faces, file_type.upper())\n    else:\n        log.debug('Exporting %d faces as %s', len(mesh.faces),\n                  file_type.upper())\n    export = _mesh_exporters[file_type](mesh, **kwargs)\n\n    if hasattr(file_obj, 'write'):\n        result = util.write_encoded(file_obj, export)\n    else:\n        result = export\n\n    if was_opened:\n        file_obj.close()\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexports a mesh as an OFF file a simple text format", "response": "def export_off(mesh, digits=10):\n    \"\"\"\n    Export a mesh as an OFF file, a simple text format\n\n    Parameters\n    -----------\n    mesh : trimesh.Trimesh\n      Geometry to export\n    digits : int\n      Number of digits to include on floats\n\n    Returns\n    -----------\n    export : str\n      OFF format output\n    \"\"\"\n    # make sure specified digits is an int\n    digits = int(digits)\n    # prepend a 3 (face count) to each face\n    faces_stacked = np.column_stack((np.ones(len(mesh.faces)) * 3,\n                                     mesh.faces)).astype(np.int64)\n    export = 'OFF\\n'\n    # the header is vertex count, face count, another number\n    export += str(len(mesh.vertices)) + ' ' + str(len(mesh.faces)) + ' 0\\n'\n    export += util.array_to_string(\n        mesh.vertices, col_delim=' ', row_delim='\\n', digits=digits) + '\\n'\n    export += util.array_to_string(\n        faces_stacked, col_delim=' ', row_delim='\\n')\n    return export"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexports a Trimesh object to a dict", "response": "def export_dict(mesh, encoding=None):\n    \"\"\"\n    Export a mesh to a dict\n\n    Parameters\n    ------------\n    mesh : Trimesh object\n             Mesh to be exported\n    encoding : str, or None\n                 'base64'\n\n    Returns\n    -------------\n\n    \"\"\"\n\n    def encode(item, dtype=None):\n        if encoding is None:\n            return item.tolist()\n        else:\n            if dtype is None:\n                dtype = item.dtype\n            return util.array_to_encoded(item, dtype=dtype, encoding=encoding)\n\n    # metadata keys we explicitly want to preserve\n    # sometimes there are giant datastructures we don't\n    # care about in metadata which causes exports to be\n    # extremely slow, so skip all but known good keys\n    meta_keys = ['units', 'file_name', 'file_path']\n    metadata = {k: v for k, v in mesh.metadata.items() if k in meta_keys}\n\n    export = {\n        'metadata': metadata,\n        'faces': encode(mesh.faces),\n        'face_normals': encode(mesh.face_normals),\n        'vertices': encode(mesh.vertices)\n    }\n    if mesh.visual.kind == 'face':\n        export['face_colors'] = encode(mesh.visual.face_colors)\n    elif mesh.visual.kind == 'vertex':\n        export['vertex_colors'] = encode(mesh.visual.vertex_colors)\n\n    return export"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef minify(path):\n\n    if 'http' in path:\n        data = requests.get(path).content.decode(\n            'ascii', errors='ignore')\n    else:\n        with open(path, 'rb') as f:\n            # some of these assholes use unicode spaces -_-\n            data = f.read().decode('ascii',\n                                   errors='ignore')\n    # don't re- minify\n    if '.min.' in path:\n        return data\n\n    try:\n        return jsmin.jsmin(data)\n    except BaseException:\n        return data", "response": "Load a javascript file and minify it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a Path2D representing a circular pattern.", "response": "def circle_pattern(pattern_radius,\n                   circle_radius,\n                   count,\n                   center=[0.0, 0.0],\n                   angle=None,\n                   **kwargs):\n    \"\"\"\n    Create a Path2D representing a circle pattern.\n\n    Parameters\n    ------------\n    pattern_radius : float\n      Radius of circle centers\n    circle_radius : float\n      The radius of each circle\n    count : int\n      Number of circles in the pattern\n    center : (2,) float\n      Center of pattern\n    angle :  float\n      If defined pattern will span this angle\n      If None, pattern will be evenly spaced\n\n    Returns\n    -------------\n    pattern : trimesh.path.Path2D\n      Path containing circular pattern\n    \"\"\"\n    from .path import Path2D\n\n    if angle is None:\n        angles = np.linspace(0.0, np.pi * 2.0, count + 1)[:-1]\n    elif isinstance(angle, float) or isinstance(angle, int):\n        angles = np.linspace(0.0, angle, count)\n    else:\n        raise ValueError('angle must be float or int!')\n\n    # centers of circles\n    centers = np.column_stack((\n        np.cos(angles), np.sin(angles))) * pattern_radius\n\n    vert = []\n    ents = []\n    for circle_center in centers:\n        # (3,3) center points of arc\n        three = arc.to_threepoint(angles=[0, np.pi],\n                                  center=circle_center,\n                                  radius=circle_radius)\n        # add a single circle entity\n        ents.append(\n            entities.Arc(\n                points=np.arange(3) + len(vert),\n                closed=True))\n        # keep flat array by extend instead of append\n        vert.extend(three)\n\n    # translate vertices to pattern center\n    vert = np.array(vert) + center\n    pattern = Path2D(entities=ents,\n                     vertices=vert,\n                     **kwargs)\n    return pattern"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef circle(radius=None, center=None, **kwargs):\n    from .path import Path2D\n\n    if center is None:\n        center = [0.0, 0.0]\n    else:\n        center = np.asanyarray(center, dtype=np.float64)\n    if radius is None:\n        radius = 1.0\n    else:\n        radius = float(radius)\n\n    # (3, 2) float, points on arc\n    three = arc.to_threepoint(angles=[0, np.pi],\n                              center=center,\n                              radius=radius) + center\n\n    result = Path2D(entities=[entities.Arc(points=np.arange(3), closed=True)],\n                    vertices=three,\n                    **kwargs)\n    return result", "response": "Create a Path2D containing a circle with the specified radius and center."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Path2D containing a single or multiple rectangles with the specified bounds.", "response": "def rectangle(bounds, **kwargs):\n    \"\"\"\n    Create a Path2D containing a single or multiple rectangles\n    with the specified bounds.\n\n    Parameters\n    --------------\n    bounds : (2, 2) float, or (m, 2, 2) float\n      Minimum XY, Maximum XY\n\n    Returns\n    -------------\n    rect : Path2D\n      Path containing specified rectangles\n    \"\"\"\n    from .path import Path2D\n\n    # data should be float\n    bounds = np.asanyarray(bounds, dtype=np.float64)\n\n    # bounds are extents, re- shape to origin- centered rectangle\n    if bounds.shape == (2,):\n        half = np.abs(bounds) / 2.0\n        bounds = np.array([-half, half])\n\n    # should have one bounds or multiple bounds\n    if not (util.is_shape(bounds, (2, 2)) or\n            util.is_shape(bounds, (-1, 2, 2))):\n        raise ValueError('bounds must be (m, 2, 2) or (2, 2)')\n\n    # hold entities.Line objects\n    lines = []\n    # hold (n, 2) cartesian points\n    vertices = []\n\n    # loop through each rectangle\n    for lower, upper in bounds.reshape((-1, 2, 2)):\n        lines.append(entities.Line((np.arange(5) % 4) + len(vertices)))\n        vertices.extend([lower,\n                         [upper[0], lower[1]],\n                         upper,\n                         [lower[0], upper[1]]])\n\n    # create the Path2D with specified rectangles\n    rect = Path2D(entities=lines,\n                  vertices=vertices,\n                  **kwargs)\n\n    return rect"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef box_outline(extents=None, transform=None, **kwargs):\n    from .exchange.load import load_path\n\n    # create vertices for the box\n    vertices = [0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1,\n                1, 1, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1]\n    vertices = np.array(vertices,\n                        order='C',\n                        dtype=np.float64).reshape((-1, 3))\n    vertices -= 0.5\n\n    # resize the vertices based on passed size\n    if extents is not None:\n        extents = np.asanyarray(extents, dtype=np.float64)\n        if extents.shape != (3,):\n            raise ValueError('Extents must be (3,)!')\n        vertices *= extents\n\n    # apply transform if passed\n    if transform is not None:\n        vertices = transformations.transform_points(vertices, transform)\n\n    # vertex indices\n    indices = [0, 1, 3, 2, 0, 4, 5, 7, 6, 4, 0, 2, 6, 7, 3, 1, 5]\n    outline = load_path(vertices[indices])\n\n    return outline", "response": "Returns a path outline of a cuboid."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plane_transform(origin, normal):\n    transform = align_vectors(normal, [0, 0, 1])\n    transform[0:3, 3] = -np.dot(transform,\n                                np.append(origin, 1))[0:3]\n    return transform", "response": "Find the transform that will move the point onto the XY plane."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef align_vectors(a, b, return_angle=False):\n    # copy of input vectors\n    a = np.array(a, dtype=np.float64, copy=True)\n    b = np.array(b, dtype=np.float64, copy=True)\n\n    # make sure vectors are 3D\n    if a.shape != (3,) or b.shape != (3,):\n        raise ValueError('only works for (3,) vectors')\n\n    # unitize input vectors\n    a /= np.linalg.norm(a)\n    b /= np.linalg.norm(b)\n\n    # projection of a onto b\n    dot = np.dot(a, b)\n\n    # are vectors just reversed\n    if dot < (tol.zero - 1):\n        # a reversed vector is 180 degrees\n        angle = np.pi\n\n        # get an arbitrary perpendicular vector to a\n        perp = util.generate_basis(a)[0] * np.eye(3)\n\n        # (3, 3) rotation from a to b\n        rotation = (2 * np.dot(perp, perp.T)) - np.eye(3)\n\n    # are vectors already the same\n    elif dot > (1 - tol.zero):\n        angle = 0.0\n        # no rotation\n        rotation = np.eye(3)\n\n    # vectors are at some angle to each other\n    else:\n        # we already handled values out of the range [-1.0, 1.0]\n        angle = np.arccos(dot)\n\n        # (3,) vector perpendicular to both a and b\n        w = np.cross(a, b)\n\n        # a float between 0.5 and 1.0\n        c = 1.0 / (1.0 + dot)\n\n        # (3, 3) skew- symmetric matrix from the (3,) vector w\n        # the matrix has the property: wx == -wx.T\n        wx = np.array([[0, -w[2], w[1]],\n                       [w[2], 0, -w[0]],\n                       [-w[1], w[0], 0]])\n\n        # (3, 3) rotation from a to b\n        rotation = np.eye(3) + wx + (np.dot(wx, wx) * c)\n\n    # put rotation into homogenous transformation matrix\n    transform = np.eye(4)\n    transform[:3, :3] = rotation\n\n    if return_angle:\n        return transform, angle\n\n    return transform", "response": "This function aligns two 3D vectors into a single 3D tree."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a list of faces return a list of edges", "response": "def faces_to_edges(faces, return_index=False):\n    \"\"\"\n    Given a list of faces (n,3), return a list of edges (n*3,2)\n\n    Parameters\n    -----------\n    faces : (n, 3) int\n      Vertex indices representing faces\n\n    Returns\n    -----------\n    edges : (n*3, 2) int\n      Vertex indices representing edges\n    \"\"\"\n    faces = np.asanyarray(faces)\n\n    # each face has three edges\n    edges = faces[:, [0, 1, 1, 2, 2, 0]].reshape((-1, 2))\n\n    if return_index:\n        # edges are in order of faces due to reshape\n        face_index = np.tile(np.arange(len(faces)),\n                             (3, 1)).T.reshape(-1)\n        return edges, face_index\n    return edges"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vector_angle(pairs):\n    pairs = np.asanyarray(pairs, dtype=np.float64)\n    if len(pairs) == 0:\n        return np.array([])\n    elif util.is_shape(pairs, (2, 3)):\n        pairs = pairs.reshape((-1, 2, 3))\n    elif not util.is_shape(pairs, (-1, 2, (2, 3))):\n        raise ValueError('pairs must be (n,2,(2|3))!')\n\n    # do the dot product between vectors\n    dots = util.diagonal_dot(pairs[:, 0], pairs[:, 1])\n    # clip for floating point error\n    dots = np.clip(dots, -1.0, 1.0)\n    # do cos and remove arbitrary sign\n    angles = np.abs(np.arccos(dots))\n\n    return angles", "response": "Find the angles between unit vectors in radians."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngive a set of quad faces return them as triangle faces.", "response": "def triangulate_quads(quads):\n    \"\"\"\n    Given a set of quad faces, return them as triangle faces.\n\n    Parameters\n    -----------\n    quads: (n, 4) int\n      Vertex indices of quad faces\n\n    Returns\n    -----------\n    faces : (m, 3) int\n      Vertex indices of triangular faces\n    \"\"\"\n    if len(quads) == 0:\n        return quads\n    quads = np.asanyarray(quads)\n    faces = np.vstack((quads[:, [0, 1, 2]],\n                       quads[:, [2, 3, 0]]))\n    return faces"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfinding vertex normals from the mean of the faces that contain that vertex. Parameters ----------- vertex_count : int The number of vertices faces refer to faces : (n, 3) int List of vertex indices face_normals : (n, 3) float Normal vector for each face Returns ----------- vertex_normals : (vertex_count, 3) float Normals for every vertex Vertices unreferenced by faces will be zero.", "response": "def mean_vertex_normals(vertex_count,\n                        faces,\n                        face_normals,\n                        **kwargs):\n    \"\"\"\n    Find vertex normals from the mean of the faces that contain\n    that vertex.\n\n    Parameters\n    -----------\n    vertex_count : int\n      The number of vertices faces refer to\n    faces : (n, 3) int\n      List of vertex indices\n    face_normals : (n, 3) float\n      Normal vector for each face\n\n    Returns\n    -----------\n    vertex_normals : (vertex_count, 3) float\n      Normals for every vertex\n      Vertices unreferenced by faces will be zero.\n    \"\"\"\n    def summed_sparse():\n        # use a sparse matrix of which face contains each vertex to\n        # figure out the summed normal at each vertex\n        # allow cached sparse matrix to be passed\n        if 'sparse' in kwargs:\n            sparse = kwargs['sparse']\n        else:\n            sparse = index_sparse(vertex_count, faces)\n        summed = sparse.dot(face_normals)\n        return summed\n\n    def summed_loop():\n        # loop through every face, in tests was ~50x slower than\n        # doing this with a sparse matrix\n        summed = np.zeros((vertex_count, 3))\n        for face, normal in zip(faces, face_normals):\n            summed[face] += normal\n        return summed\n\n    try:\n        summed = summed_sparse()\n    except BaseException:\n        log.warning(\n            'unable to generate sparse matrix! Falling back!',\n            exc_info=True)\n        summed = summed_loop()\n\n    # invalid normals will be returned as zero\n    vertex_normals = util.unitize(summed)\n\n    return vertex_normals"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef index_sparse(column_count, indices):\n    indices = np.asanyarray(indices)\n    column_count = int(column_count)\n\n    row = indices.reshape(-1)\n    col = np.tile(np.arange(len(indices)).reshape(\n        (-1, 1)), (1, indices.shape[1])).reshape(-1)\n\n    shape = (column_count, len(indices))\n    data = np.ones(len(col), dtype=np.bool)\n    sparse = coo_matrix((data, (row, col)),\n                        shape=shape,\n                        dtype=np.bool)\n    return sparse", "response": "This function returns a sparse matrix for which vertices are contained in which faces."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the JSON blob into native objects", "response": "def get_json(file_name='../dxf.json.template'):\n    \"\"\"\n    Load the JSON blob into native objects\n    \"\"\"\n    with open(file_name, 'r') as f:\n        t = json.load(f)\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwrite a native object to a JSON blob", "response": "def write_json(template, file_name='../dxf.json.template'):\n    \"\"\"\n    Write a native object to a JSON blob\n    \"\"\"\n    with open(file_name, 'w') as f:\n        json.dump(template, f, indent=4)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreplaces non - strippable whitepace in a string with a safe space", "response": "def replace_whitespace(text, SAFE_SPACE='|<^>|', insert=True):\n    \"\"\"\n    Replace non-strippable whitepace in a string with a safe space\n    \"\"\"\n    if insert:\n        # replace whitespace with safe space chr\n        args = (' ', SAFE_SPACE)\n    else:\n        # replace safe space chr with whitespace\n        args = (SAFE_SPACE, ' ')\n\n    return '\\n'.join(line.strip().replace(*args)\n                     for line in str.splitlines(text))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_files(template, destination='./dxf'):\n    os.makedirs(destination)\n    for key, value in template.items():\n        with open(os.path.join(destination, key), 'w') as f:\n            f.write(replace_whitespace(value, insert=False))", "response": "Write the files from a dictionary to a directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a directory full of files and return a dict with file_name : text", "response": "def read_files(path):\n    \"\"\"\n    For a directory full of files, retrieve it\n    as a dict with file_name:text\n    \"\"\"\n    template = {}\n    for file_name in os.listdir(path):\n        with open(os.path.join(path, file_name), 'r') as f:\n            template[file_name] = replace_whitespace(\n                f.read(), insert=True)\n\n    return template"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive three points on an arc find: center, radius, normal, and angle. This uses the fact that the intersection of the perp bisectors of the segments between the control points is the center of the arc. Parameters --------- points : (3, dimension) float Points in space, where dimension is either 2 or 3 Returns --------- result : dict Has keys: 'center': (d,) float, cartesian center of the arc 'radius': float, radius of the arc 'normal': (3,) float, the plane normal. 'angle': (2,) float, angle of start and end, in radians 'span' : float, angle swept by the arc, in radians", "response": "def arc_center(points):\n    \"\"\"\n    Given three points on an arc find:\n    center, radius, normal, and angle.\n\n    This uses the fact that the intersection of the perp\n    bisectors of the segments between the control points\n    is the center of the arc.\n\n    Parameters\n    ---------\n    points : (3, dimension) float\n      Points in space, where dimension is either 2 or 3\n\n    Returns\n    ---------\n    result : dict\n      Has keys:\n        'center':   (d,) float, cartesian center of the arc\n        'radius':   float, radius of the arc\n        'normal':   (3,) float, the plane normal.\n        'angle':    (2,) float, angle of start and end, in radians\n        'span' :    float, angle swept by the arc, in radians\n    \"\"\"\n    # it's a lot easier to treat 2D as 3D with a zero Z value\n    points, is_2D = util.stack_3D(points, return_2D=True)\n\n    # find the two edge vectors of the triangle\n    edge_direction = np.diff(points, axis=0)\n    edge_midpoints = (edge_direction * 0.5) + points[:2]\n\n    # three points define a plane, so we find its normal vector\n    plane_normal = np.cross(*edge_direction[::-1])\n    plane_normal /= np.linalg.norm(plane_normal)\n\n    # unit vector along edges\n    vector_edge = (edge_direction /\n                   np.linalg.norm(edge_direction, axis=1).reshape((-1, 1)))\n\n    # perpendicular cector to each segment\n    vector_perp = np.cross(vector_edge, plane_normal)\n    vector_perp /= np.linalg.norm(vector_perp, axis=1).reshape((-1, 1))\n\n    # run the line- line intersection to find the point\n    intersects, center = line_line(origins=edge_midpoints,\n                                   directions=vector_perp,\n                                   plane_normal=plane_normal)\n\n    if not intersects:\n        raise ValueError('Segments do not intersect!')\n\n    # radius is euclidean distance\n    radius = ((points[0] - center) ** 2).sum() ** .5\n\n    # vectors from points on arc to center point\n    vector = points - center\n    vector /= np.linalg.norm(vector, axis=1).reshape((-1, 1))\n\n    angle = np.arccos(np.clip(np.dot(*vector[[0, 2]]), -1.0, 1.0))\n    large_arc = (abs(angle) > tol.zero and\n                 np.dot(*edge_direction) < 0.0)\n    if large_arc:\n        angle = (np.pi * 2) - angle\n\n    angles = np.arctan2(*vector[:, :2].T[::-1]) + np.pi * 2\n    angles_sorted = np.sort(angles[[0, 2]])\n    reverse = angles_sorted[0] < angles[1] < angles_sorted[1]\n    angles_sorted = angles_sorted[::(1 - int(not reverse) * 2)]\n\n    result = {'center': center[:(3 - is_2D)],\n              'radius': radius,\n              'normal': plane_normal,\n              'span': angle,\n              'angles': angles_sorted}\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef discretize_arc(points,\n                   close=False,\n                   scale=1.0):\n    \"\"\"\n    Returns a version of a three point arc consisting of\n    line segments.\n\n    Parameters\n    ---------\n    points : (3, d) float\n      Points on the arc where d in [2,3]\n    close :  boolean\n      If True close the arc into a circle\n    scale : float\n      What is the approximate overall drawing scale\n      Used to establish order of magnitude for precision\n\n    Returns\n    ---------\n    discrete : (m, d) float\n      Connected points in space\n    \"\"\"\n    # make sure points are (n, 3)\n    points, is_2D = util.stack_3D(points, return_2D=True)\n    # find the center of the points\n    center_info = arc_center(points)\n    center, R, N, angle = (center_info['center'],\n                           center_info['radius'],\n                           center_info['normal'],\n                           center_info['span'])\n\n    # if requested, close arc into a circle\n    if close:\n        angle = np.pi * 2\n\n    # the number of facets, based on the angle criteria\n    count_a = angle / res.seg_angle\n    count_l = ((R * angle)) / (res.seg_frac * scale)\n\n    # figure out the number of line segments\n    count = np.max([count_a, count_l])\n    # force at LEAST 4 points for the arc\n    # otherwise the endpoints will diverge\n    count = np.clip(count, 4, np.inf)\n    count = int(np.ceil(count))\n\n    V1 = util.unitize(points[0] - center)\n    V2 = util.unitize(np.cross(-N, V1))\n    t = np.linspace(0, angle, count)\n\n    discrete = np.tile(center, (count, 1))\n    discrete += R * np.cos(t).reshape((-1, 1)) * V1\n    discrete += R * np.sin(t).reshape((-1, 1)) * V2\n\n    # do an in-process check to make sure result endpoints\n    # match the endpoints of the source arc\n    if not close:\n        arc_dist = np.linalg.norm(points[[0, -1]] -\n                                  discrete[[0, -1]], axis=1)\n        arc_ok = (arc_dist < tol.merge).all()\n        if not arc_ok:\n            log.warn(\n                'failed to discretize arc (endpoint distance %s)',\n                str(arc_dist))\n            log.warn('Failed arc points: %s', str(points))\n            raise ValueError('Arc endpoints diverging!')\n    discrete = discrete[:, :(3 - is_2D)]\n\n    return discrete", "response": "Return a three point arc consisting of line segments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting 2D arcs to three Arc control points on the plane.", "response": "def to_threepoint(center, radius, angles=None):\n    \"\"\"\n    For 2D arcs, given a center and radius convert them to three\n    points on the arc.\n\n    Parameters\n    -----------\n    center : (2,) float\n      Center point on the plane\n    radius : float\n      Radius of arc\n    angles : (2,) float\n      Angles in radians for start and end angle\n      if not specified, will default to (0.0, pi)\n\n    Returns\n    ----------\n    three : (3, 2) float\n      Arc control points\n    \"\"\"\n    # if no angles provided assume we want a half circle\n    if angles is None:\n        angles = [0.0, np.pi]\n    # force angles to float64\n    angles = np.asanyarray(angles, dtype=np.float64)\n    if angles.shape != (2,):\n        raise ValueError('angles must be (2,)!')\n    # provide the wrap around\n    if angles[1] < angles[0]:\n        angles[1] += np.pi * 2\n\n    center = np.asanyarray(center, dtype=np.float64)\n    if center.shape != (2,):\n        raise ValueError('only valid on 2D arcs!')\n\n    # turn the angles of [start, end]\n    # into [start, middle, end]\n    angles = np.array([angles[0],\n                       angles.mean(),\n                       angles[1]],\n                      dtype=np.float64)\n    # turn angles into (3,2) points\n    three = np.column_stack((np.cos(angles),\n                             np.sin(angles))) * radius\n    three += center\n\n    return three"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef abspath(rel):\n    return os.path.abspath(os.path.join(cwd, rel))", "response": "Take paths relative to the current file and convert them to absolute paths."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a mesh from a file object or file name.", "response": "def load_pyassimp(file_obj,\n                  file_type=None,\n                  resolver=None,\n                  **kwargs):\n    \"\"\"\n    Use the pyassimp library to load a mesh from a file object\n    and type or file name if file_obj is a string\n\n    Parameters\n    ---------\n    file_obj: str, or file object\n      File path or object containing mesh data\n    file_type : str\n      File extension, aka 'stl'\n    resolver : trimesh.visual.resolvers.Resolver\n      Used to load referenced data (like texture files)\n    kwargs : dict\n      Passed through to mesh constructor\n\n    Returns\n    ---------\n    scene : trimesh.Scene\n      Native trimesh copy of assimp scene\n    \"\"\"\n\n    def LP_to_TM(lp):\n        # try to get the vertex colors attribute\n        colors = (np.reshape(lp.colors, (-1, 4))\n                  [:, :3] * 255).round().astype(np.uint8)\n        # If no vertex colors, try to extract them from the material\n        if len(colors) == 0:\n            if 'diffuse' in lp.material.properties.keys():\n                colors = np.array(lp.material.properties['diffuse'])\n\n        # pass kwargs through to mesh constructor\n        mesh_kwargs = copy.deepcopy(kwargs)\n        # add data from the LP_Mesh\n        mesh_kwargs.update({'vertices': lp.vertices,\n                            'vertex_normals': lp.normals,\n                            'faces': lp.faces,\n                            'vertex_colors': colors})\n        return mesh_kwargs\n\n    # did we open the file inside this function\n    opened = False\n    # not a file object\n    if not hasattr(file_obj, 'read'):\n        # if there is no read attribute\n        # we assume we've been passed a file name\n        file_type = (str(file_obj).split('.')[-1]).lower()\n        file_obj = open(file_obj, 'rb')\n        opened = True\n    # we need files to be bytes\n    elif not hasattr(file_obj, 'mode') or file_obj.mode != 'rb':\n        # assimp will crash on anything that isn't binary\n        # so if we have a text mode file or anything else\n        # grab the data, encode as bytes, and then use stream\n        data = file_obj.read()\n        if hasattr(data, 'encode'):\n            data = data.encode('utf-8')\n        file_obj = util.wrap_as_stream(data)\n\n    # load the scene using pyassimp\n    scene = pyassimp.load(file_obj,\n                          file_type=file_type)\n\n    # save a record of mesh names used so we\n    # don't have to do queries on mesh_id.values()\n    mesh_names = set()\n    # save a mapping for {id(mesh) : name}\n    mesh_id = {}\n    # save results as {name : Trimesh}\n    meshes = {}\n    # loop through scene LPMesh objects\n    for m in scene.meshes:\n        # skip meshes without tri/quad faces\n        if m.faces.shape[1] not in [3, 4]:\n            continue\n        # if this mesh has the name of an existing mesh\n        if m.name in mesh_names:\n            # make it the name plus the unique ID of the object\n            name = m.name + str(id(m))\n        else:\n            # otherwise just use the name it calls itself by\n            name = m.name\n\n        # save the name to mark as consumed\n        mesh_names.add(name)\n        # save the id:name mapping\n        mesh_id[id(m)] = name\n        # convert the mesh to a trimesh object\n        meshes[name] = LP_to_TM(m)\n\n    # now go through and collect the transforms from the scene\n    # we are going to save them as a list of dict kwargs\n    transforms = []\n    # nodes as (parent, node) tuples\n    # use deque so we can pop from both ends\n    queue = collections.deque(\n        [('world', n) for\n         n in scene.rootnode.children])\n\n    # consume the queue\n    while len(queue) > 0:\n        # parent name, node object\n        parent, node = queue.pop()\n\n        # assimp uses weirdly duplicate node names\n        # object ID's are actually unique and consistent\n        node_name = id(node)\n        transforms.append({'frame_from': parent,\n                           'frame_to': node_name,\n                           'matrix': node.transformation})\n\n        # loop through meshes this node uses\n        # note that they are the SAME objects as converted\n        # above so we can find their reference using id()\n        for m in node.meshes:\n            if id(m) not in mesh_id:\n                continue\n\n            # create kwargs for graph.update\n            edge = {'frame_from': node_name,\n                    'frame_to': str(id(m)) + str(node_name),\n                    'matrix': np.eye(4),\n                    'geometry': mesh_id[id(m)]}\n            transforms.append(edge)\n\n        # add any children to the queue to be visited\n        for child in node.children:\n            queue.appendleft((node_name, child))\n\n    # release the loaded scene\n    pyassimp.release(scene)\n\n    # if we opened the file in this function close it\n    if opened:\n        file_obj.close()\n\n    # create kwargs for trimesh.exchange.load.load_kwargs\n    result = {'class': 'Scene',\n              'geometry': meshes,\n              'graph': transforms,\n              'base_frame': 'world'}\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_cyassimp(file_obj,\n                  file_type=None,\n                  resolver=None,\n                  **kwargs):\n    \"\"\"\n    Load a file using the cyassimp bindings.\n\n    The easiest way to install these is with conda:\n    conda install -c menpo/label/master cyassimp\n\n    Parameters\n    ---------\n    file_obj: str, or file object\n      File path or object containing mesh data\n    file_type : str\n      File extension, aka 'stl'\n    resolver : trimesh.visual.resolvers.Resolver\n      Used to load referenced data (like texture files)\n    kwargs : dict\n      Passed through to mesh constructor\n\n    Returns\n    ---------\n    meshes : (n,) list of dict\n      Contain kwargs for Trimesh constructor\n    \"\"\"\n\n    if hasattr(file_obj, 'read'):\n        # if it has a read attribute it is probably a file object\n        with tempfile.NamedTemporaryFile(\n                suffix=str(file_type)) as file_temp:\n\n            file_temp.write(file_obj.read())\n            # file name should be bytes\n            scene = cyassimp.AIImporter(\n                file_temp.name.encode('utf-8'))\n            scene.build_scene()\n    else:\n        scene = cyassimp.AIImporter(file_obj.encode('utf-8'))\n        scene.build_scene()\n\n    meshes = []\n    for m in scene.meshes:\n        mesh_kwargs = kwargs.copy()\n        mesh_kwargs.update({'vertices': m.points,\n                            'faces': m.trilist})\n        meshes.append(mesh_kwargs)\n\n    if len(meshes) == 1:\n        return meshes[0]\n    return meshes", "response": "Loads a file using the cyassimp bindings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_path(obj, file_type=None, **kwargs):\n\n    if isinstance(obj, Path):\n        # we have been passed a Path object so\n        # do nothing and return the passed object\n        return obj\n    elif util.is_file(obj):\n        # for open file objects use loaders\n        loaded = path_loaders[file_type](obj,\n                                         file_type=file_type)\n        obj.close()\n    elif util.is_string(obj):\n        # strings passed are evaluated as file objects\n        with open(obj, 'rb') as file_obj:\n            # get the file type from the extension\n            file_type = os.path.splitext(obj)[-1][1:].lower()\n            # call the loader\n            loaded = path_loaders[file_type](file_obj,\n                                             file_type=file_type)\n    elif util.is_instance_named(obj, 'Polygon'):\n        # convert from shapely polygons to Path2D\n        loaded = misc.polygon_to_path(obj)\n    elif util.is_instance_named(obj, 'MultiLineString'):\n        # convert from shapely LineStrings to Path2D\n        loaded = misc.linestrings_to_path(obj)\n    elif util.is_instance_named(obj, 'dict'):\n        # load as kwargs\n        loaded = misc.dict_to_path(obj)\n    elif util.is_sequence(obj):\n        # load as lines in space\n        loaded = misc.lines_to_path(obj)\n    else:\n        raise ValueError('Not a supported object type!')\n\n    # pass kwargs through to path loader\n    kwargs.update(loaded)\n    # convert the kwargs to a Path2D or Path3D object\n    path = _create_path(**kwargs)\n\n    return path", "response": "Load a file to a Path object."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a Path2D or Path3D object based on the dimension of vertices.", "response": "def _create_path(entities,\n                 vertices,\n                 metadata=None,\n                 **kwargs):\n    \"\"\"\n    Turn entities and vertices into a Path2D or a Path3D\n    object depending on dimension of vertices.\n\n    Parameters\n    -----------\n    entities : list\n        Entity objects that reference vertex indices\n    vertices : (n, 2) or (n, 3) float\n        Vertices in space\n    metadata : dict\n        Any metadata about the path object\n\n    Returns\n    -----------\n    as_path : Path2D or Path3D object\n        Args in native trimesh object form\n\n    \"\"\"\n    # make sure vertices are numpy array\n    vertices = np.asanyarray(vertices, dtype=np.float64)\n\n    # check dimension of vertices to decide on object type\n    if vertices.shape[1] == 2:\n        path_type = Path2D\n    elif vertices.shape[1] == 3:\n        path_type = Path3D\n    else:\n        # weird or empty vertices, just use default Path object\n        path_type = Path\n\n    # create the object\n    as_path = path_type(entities=entities,\n                        vertices=vertices,\n                        metadata=metadata,\n                        **kwargs)\n    return as_path"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a geometry list of geometries or a Scene return them as a single Scene object.", "response": "def split_scene(geometry):\n    \"\"\"\n    Given a geometry, list of geometries, or a Scene\n    return them as a single Scene object.\n\n    Parameters\n    ----------\n    geometry : splittable\n\n    Returns\n    ---------\n    scene: trimesh.Scene\n    \"\"\"\n    # already a scene, so return it\n    if util.is_instance_named(geometry, 'Scene'):\n        return geometry\n\n    # a list of things\n    if util.is_sequence(geometry):\n        metadata = {}\n        for g in geometry:\n            try:\n                metadata.update(g.metadata)\n            except BaseException:\n                continue\n        return Scene(geometry,\n                     metadata=metadata)\n\n    # a single geometry so we are going to split\n    split = collections.deque()\n\n    metadata = {}\n    for g in util.make_sequence(geometry):\n        split.extend(g.split())\n        metadata.update(g.metadata)\n\n    # if there is only one geometry in the mesh\n    # name it from the file name\n    if len(split) == 1 and 'file_name' in metadata:\n        split = {metadata['file_name']: split[0]}\n\n    scene = Scene(split, metadata=metadata)\n\n    return scene"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconcatenates multiple scene objects into one scene.", "response": "def append_scenes(iterable, common=['world']):\n    \"\"\"\n    Concatenate multiple scene objects into one scene.\n\n    Parameters\n    -------------\n    iterable : (n,) Trimesh or Scene\n       Geometries that should be appended\n    common : (n,) str\n       Nodes that shouldn't be remapped\n\n    Returns\n    ------------\n    result : trimesh.Scene\n       Scene containing all geometry\n    \"\"\"\n    if isinstance(iterable, Scene):\n        return iterable\n\n    # save geometry in dict\n    geometry = {}\n    # save transforms as edge tuples\n    edges = []\n\n    # nodes which shouldn't be remapped\n    common = set(common)\n    # nodes which are consumed and need to be remapped\n    consumed = set()\n\n    def node_remap(node):\n        \"\"\"\n        Remap node to new name if necessary\n\n        Parameters\n        -------------\n        node : hashable\n           Node name in original scene\n\n        Returns\n        -------------\n        name : hashable\n           Node name in concatenated scene\n        \"\"\"\n\n        # if we've already remapped a node use it\n        if node in map_node:\n            return map_node[node]\n\n        # if a node is consumed and isn't one of the nodes\n        # we're going to hold common between scenes remap it\n        if node not in common and node in consumed:\n            name = str(node) + '-' + util.unique_id().upper()\n            map_node[node] = name\n            node = name\n\n        # keep track of which nodes have been used\n        # in the current scene\n        current.add(node)\n        return node\n\n    # loop through every geometry\n    for s in iterable:\n        # allow Trimesh/Path2D geometry to be passed\n        if hasattr(s, 'scene'):\n            s = s.scene()\n        # if we don't have a scene raise an exception\n        if not isinstance(s, Scene):\n            raise ValueError('{} is not a scene!'.format(\n                type(s).__name__))\n\n        # remap geometries if they have been consumed\n        map_geom = {}\n        for k, v in s.geometry.items():\n            # if a geometry already exists add a UUID to the name\n            if k in geometry:\n                name = str(k) + '-' + util.unique_id().upper()\n            else:\n                name = k\n            # store name mapping\n            map_geom[k] = name\n            # store geometry with new name\n            geometry[name] = v\n\n        # remap nodes and edges so duplicates won't\n        # stomp all over each other\n        map_node = {}\n        # the nodes used in this scene\n        current = set()\n        for a, b, attr in s.graph.to_edgelist():\n            # remap node names from local names\n            a, b = node_remap(a), node_remap(b)\n            # remap geometry keys\n            # if key is not in map_geom it means one of the scenes\n            # referred to geometry that doesn't exist\n            # rather than crash here we ignore it as the user\n            # possibly intended to add in geometries back later\n            if 'geometry' in attr and attr['geometry'] in map_geom:\n                attr['geometry'] = map_geom[attr['geometry']]\n            # save the new edge\n            edges.append((a, b, attr))\n        # mark nodes from current scene as consumed\n        consumed.update(current)\n\n    # add all data to a new scene\n    result = Scene()\n    result.graph.from_edgelist(edges)\n    result.geometry.update(geometry)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a geometry to the current scene.", "response": "def add_geometry(self,\n                     geometry,\n                     node_name=None,\n                     geom_name=None,\n                     parent_node_name=None,\n                     transform=None):\n        \"\"\"\n        Add a geometry to the scene.\n\n        If the mesh has multiple transforms defined in its\n        metadata, they will all be copied into the\n        TransformForest of the current scene automatically.\n\n        Parameters\n        ----------\n        geometry : Trimesh, Path2D, Path3D PointCloud or list\n          Geometry to initially add to the scene\n        base_frame : str or hashable\n          Name of base frame\n        metadata : dict\n          Any metadata about the scene\n        graph : TransformForest or None\n          A passed transform graph to use\n\n        Returns\n        ----------\n        node_name : str\n          Name of node in self.graph\n        \"\"\"\n\n        if geometry is None:\n            return\n        # PointCloud objects will look like a sequence\n        elif util.is_sequence(geometry):\n            # if passed a sequence add all elements\n            for value in geometry:\n                self.add_geometry(\n                    geometry=value,\n                    node_name=node_name,\n                    geom_name=geom_name,\n                    parent_node_name=parent_node_name,\n                    transform=transform,\n                )\n            return\n\n        elif isinstance(geometry, dict):\n            # if someone passed us a dict of geometry\n            for key, value in geometry.items():\n                self.add_geometry(value, geom_name=key)\n            return\n\n        # get or create a name to reference the geometry by\n        if geom_name is not None:\n            # if name is passed use it\n            name = geom_name\n        elif 'name' in geometry.metadata:\n            # if name is in metadata use it\n            name = geometry.metadata['name']\n        elif 'file_name' in geometry.metadata:\n            name = geometry.metadata['file_name']\n        else:\n            # try to create a simple name\n            name = 'geometry_' + str(len(self.geometry))\n\n        # if its already taken add a unique random string to it\n        if name in self.geometry:\n            name += ':' + util.unique_id().upper()\n\n        # save the geometry reference\n        self.geometry[name] = geometry\n\n        # create a unique node name if not passed\n        if node_name is None:\n            # a random unique identifier\n            unique = util.unique_id(increment=len(self.geometry))\n            # geometry name + UUID\n            node_name = name + '_' + unique.upper()\n\n        if transform is None:\n            # create an identity transform from parent_node\n            transform = np.eye(4)\n\n        self.graph.update(frame_to=node_name,\n                          frame_from=parent_node_name,\n                          matrix=transform,\n                          geometry=name,\n                          geometry_flags={'visible': True})\n        return node_name"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef md5(self):\n        # start with transforms hash\n        hashes = [self.graph.md5()]\n        for g in self.geometry.values():\n            if hasattr(g, 'md5'):\n                hashes.append(g.md5())\n            elif hasattr(g, 'tostring'):\n                hashes.append(str(hash(g.tostring())))\n            else:\n                # try to just straight up hash\n                # this may raise errors\n                hashes.append(str(hash(g)))\n\n        md5 = util.md5_object(''.join(hashes))\n\n        return md5", "response": "Returns the MD5 of the current object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the current node is valid.", "response": "def is_valid(self):\n        \"\"\"\n        Is every geometry connected to the root node.\n\n        Returns\n        -----------\n        is_valid : bool\n          Does every geometry have a transform\n        \"\"\"\n        if len(self.geometry) == 0:\n            return True\n\n        try:\n            referenced = {self.graph[i][1]\n                          for i in self.graph.nodes_geometry}\n        except BaseException:\n            # if connectivity to world frame is broken return false\n            return False\n\n        # every geometry is referenced\n        ok = referenced == set(self.geometry.keys())\n\n        return ok"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of points that represent the corners of the AABB of every geometry in the scene.", "response": "def bounds_corners(self):\n        \"\"\"\n        A list of points that represent the corners of the\n        AABB of every geometry in the scene.\n\n        This can be useful if you want to take the AABB in\n        a specific frame.\n\n        Returns\n        -----------\n        corners: (n, 3) float, points in space\n        \"\"\"\n        # the saved corners of each instance\n        corners_inst = []\n        # (n, 3) float corners of each geometry\n        corners_geom = {k: bounds_module.corners(v.bounds)\n                        for k, v in self.geometry.items()}\n\n        for node_name in self.graph.nodes_geometry:\n            # access the transform and geometry name from node\n            transform, geometry_name = self.graph[node_name]\n            # not all nodes have associated geometry\n            if geometry_name is None:\n                continue\n            # transform geometry corners into where\n            # the instance of the geometry is located\n            corners_inst.extend(\n                transformations.transform_points(\n                    corners_geom[geometry_name],\n                    transform))\n        # make corners numpy array\n        corners_inst = np.array(corners_inst,\n                                dtype=np.float64)\n        return corners_inst"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bounds(self):\n        corners = self.bounds_corners\n        bounds = np.array([corners.min(axis=0),\n                           corners.max(axis=0)])\n        return bounds", "response": "Returns the overall bounding box of the scene."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a correctly transformed polygon soup of the current scene.", "response": "def triangles(self):\n        \"\"\"\n        Return a correctly transformed polygon soup of the\n        current scene.\n\n        Returns\n        ----------\n        triangles: (n,3,3) float, triangles in space\n        \"\"\"\n        triangles = collections.deque()\n        triangles_node = collections.deque()\n\n        for node_name in self.graph.nodes_geometry:\n            # which geometry does this node refer to\n            transform, geometry_name = self.graph[node_name]\n\n            # get the actual potential mesh instance\n            geometry = self.geometry[geometry_name]\n            if not hasattr(geometry, 'triangles'):\n                continue\n            # append the (n, 3, 3) triangles to a sequence\n            triangles.append(\n                transformations.transform_points(\n                    geometry.triangles.copy().reshape((-1, 3)),\n                    matrix=transform))\n            # save the node names for each triangle\n            triangles_node.append(\n                np.tile(node_name,\n                        len(geometry.triangles)))\n        # save the resulting nodes to the cache\n        self._cache['triangles_node'] = np.hstack(triangles_node)\n        triangles = np.vstack(triangles).reshape((-1, 3, 3))\n        return triangles"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a dict of identifiers for all geometries in the current session.", "response": "def geometry_identifiers(self):\n        \"\"\"\n        Look up geometries by identifier MD5\n\n        Returns\n        ---------\n        identifiers: dict, identifier md5: key in self.geometry\n        \"\"\"\n        identifiers = {mesh.identifier_md5: name\n                       for name, mesh in self.geometry.items()}\n        return identifiers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a sequence of node keys that represent identical meshes.", "response": "def duplicate_nodes(self):\n        \"\"\"\n        Return a sequence of node keys of identical meshes.\n\n        Will combine meshes duplicated by copying in space with different keys in\n        self.geometry, as well as meshes repeated by self.nodes.\n\n        Returns\n        -----------\n        duplicates: (m) sequence of keys to self.nodes that represent\n                     identical geometry\n        \"\"\"\n        # if there is no geometry we can have no duplicate nodes\n        if len(self.geometry) == 0:\n            return []\n\n        # geometry name : md5 of mesh\n        mesh_hash = {k: int(m.identifier_md5, 16)\n                     for k, m in self.geometry.items()}\n\n        # the name of nodes in the scene graph with geometry\n        node_names = np.array(self.graph.nodes_geometry)\n        # the geometry names for each node in the same order\n        node_geom = np.array([self.graph[i][1] for i in node_names])\n\n        # the mesh md5 for each node in the same order\n        node_hash = np.array([mesh_hash[v] for v in node_geom])\n\n        # indexes of identical hashes\n        node_groups = grouping.group(node_hash)\n\n        # sequence of node names, where each sublist has identical geometry\n        duplicates = [np.sort(node_names[g]).tolist() for g in node_groups]\n        return duplicates"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_camera(self,\n                   angles=None,\n                   distance=None,\n                   center=None,\n                   resolution=None,\n                   fov=None):\n        \"\"\"\n        Create a camera object for self.camera, and add\n        a transform to self.graph for it.\n\n        If arguments are not passed sane defaults will be figured\n        out which show the mesh roughly centered.\n\n        Parameters\n        -----------\n        angles : (3,) float\n          Initial euler angles in radians\n        distance : float\n          Distance from centroid\n        center : (3,) float\n          Point camera should be center on\n        camera : Camera object\n          Object that stores camera parameters\n        \"\"\"\n\n        if fov is None:\n            fov = np.array([60, 45])\n\n        # if no geometry nothing to set camera to\n        if len(self.geometry) == 0:\n            return\n\n        # set with no rotation by default\n        if angles is None:\n            angles = np.zeros(3)\n\n        rotation = transformations.euler_matrix(*angles)\n        transform = cameras.look_at(self.bounds_corners,\n                                    fov=fov,\n                                    rotation=rotation,\n                                    distance=distance,\n                                    center=center)\n\n        if hasattr(self, '_camera') and self._camera is not None:\n            self.camera.fov = fov\n            self.camera._scene = self\n            self.camera.transform = transform\n        else:\n            # create a new camera object\n            self.camera = cameras.Camera(fov=fov,\n                                         scene=self,\n                                         transform=transform)\n        return self.camera", "response": "Set the camera object for this object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the single camera for the scene.", "response": "def camera(self):\n        \"\"\"\n        Get the single camera for the scene. If not manually\n        set one will abe automatically generated.\n\n        Returns\n        ----------\n        camera : trimesh.scene.Camera\n          Camera object defined for the scene\n        \"\"\"\n        # no camera set for the scene yet\n        if not hasattr(self, '_camera') or self._camera is None:\n            # will create a camera with everything in view\n            return self.set_camera()\n\n        return self._camera"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a list of the lights in the scene.", "response": "def lights(self):\n        \"\"\"\n        Get a list of the lights in the scene. If nothing is\n        set it will generate some automatically.\n\n        Returns\n        -------------\n        lights : [trimesh.scene.lighting.Light]\n          Lights in the scene.\n        \"\"\"\n        if not hasattr(self, '_lights') or self._lights is None:\n            # do some automatic lighting\n            lights, transforms = lighting.autolight(self)\n            # assign the transforms to the scene graph\n            for L, T in zip(lights, transforms):\n                self.graph[L.name] = T\n            # set the lights\n            self._lights = lights\n        return self._lights"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rezero(self):\n        if self.is_empty or np.allclose(self.centroid, 0.0):\n            # early exit since what we want already exists\n            return\n\n        # the transformation to move the overall scene to AABB centroid\n        matrix = np.eye(4)\n        matrix[:3, 3] = -self.centroid\n\n        # we are going to change the base frame\n        new_base = str(self.graph.base_frame) + '_I'\n        self.graph.update(frame_from=new_base,\n                          frame_to=self.graph.base_frame,\n                          matrix=matrix)\n        self.graph.base_frame = new_base", "response": "Move the current scene so that the AABB of the whole tree is centered at the origin."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps all meshes in scene to a list of Trimesh objects transformed to their location the scene. graph is a list of Trimesh objects", "response": "def dump(self):\n        \"\"\"\n        Append all meshes in scene to a list of meshes.\n\n        Returns\n        ----------\n        dumped: (n,) list, of Trimesh objects transformed to their\n                           location the scene.graph\n        \"\"\"\n        result = collections.deque()\n\n        for node_name in self.graph.nodes_geometry:\n            transform, geometry_name = self.graph[node_name]\n\n            current = self.geometry[geometry_name].copy()\n            current.apply_transform(transform)\n            result.append(current)\n        return np.array(result)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the convex hull of the whole scene", "response": "def convex_hull(self):\n        \"\"\"\n        The convex hull of the whole scene\n\n        Returns\n        ---------\n        hull: Trimesh object, convex hull of all meshes in scene\n        \"\"\"\n        points = util.vstack_empty([m.vertices for m in self.dump()])\n        hull = convex.convex_hull(points)\n        return hull"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef export(self, file_type=None):\n\n        file_type = str(file_type).strip().lower()\n        if file_type == 'gltf':\n            return gltf.export_gltf(self)\n        elif file_type == 'glb':\n            return gltf.export_glb(self)\n\n        export = {'graph': self.graph.to_edgelist(),\n                  'geometry': {},\n                  'scene_cache': {'bounds': self.bounds.tolist(),\n                                  'extents': self.extents.tolist(),\n                                  'centroid': self.centroid.tolist(),\n                                  'scale': self.scale}}\n\n        if file_type is None:\n            file_type = {'Trimesh': 'ply',\n                         'Path2D': 'dxf'}\n\n        # if the mesh has an export method use it\n        # otherwise put the mesh\n        # itself into the export object\n        for geometry_name, geometry in self.geometry.items():\n            if hasattr(geometry, 'export'):\n                if isinstance(file_type, dict):\n                    # case where we have export types that are different\n                    # for different classes of objects.\n                    for query_class, query_format in file_type.items():\n                        if util.is_instance_named(geometry, query_class):\n                            export_type = query_format\n                            break\n                else:\n                    # if file_type is not a dict, try to export everything in the\n                    # the scene as that value like 'ply'\n                    export_type = file_type\n                exported = {'data': geometry.export(file_type=export_type),\n                            'file_type': export_type}\n                export['geometry'][geometry_name] = exported\n            else:\n                # case where mesh object doesn't have exporter\n                # might be that someone replaced the mesh with a URL\n                export['geometry'][geometry_name] = geometry\n        return export", "response": "Exports a snapshot of the current scene."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsaving a PNG image of a scene in a specific resolution.", "response": "def save_image(self, resolution=(1024, 768), **kwargs):\n        \"\"\"\n        Get a PNG image of a scene.\n\n        Parameters\n        -----------\n        resolution: (2,) int, resolution to render image\n        **kwargs:  passed to SceneViewer constructor\n\n        Returns\n        -----------\n        png: bytes, render of scene in PNG form\n        \"\"\"\n        from ..viewer import render_scene\n        png = render_scene(scene=self,\n                           resolution=resolution,\n                           **kwargs)\n        return png"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef units(self):\n        existing = [i.units for i in self.geometry.values()]\n\n        if any(existing[0] != e for e in existing):\n            # if all of our geometry doesn't have the same units already\n            # this function will only do some hot nonsense\n            raise ValueError('models in scene have inconsistent units!')\n\n        return existing[0]", "response": "Returns the units for every model in the scene and raises a ValueError if there are mixed units"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef units(self, value):\n        for m in self.geometry.values():\n            m.units = value", "response": "Set the units for every model in the scene without converting any units just setting the tag."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef convert_units(self, desired, guess=False):\n        # if there is no geometry do nothing\n        if len(self.geometry) == 0:\n            return self.copy()\n\n        current = self.units\n        if current is None:\n            # will raise ValueError if not in metadata\n            # and not allowed to guess\n            current = units.units_from_metadata(self, guess=guess)\n\n        # find the float conversion\n        scale = units.unit_conversion(current=current,\n                                      desired=desired)\n\n        # exit early if our current units are the same as desired units\n        if np.isclose(scale, 1.0):\n            result = self.copy()\n        else:\n            result = self.scaled(scale=scale)\n\n        # apply the units to every geometry of the scaled result\n        result.units = desired\n\n        return result", "response": "Returns a new scene with geometries and transforms scaled to the desired unit system."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef explode(self, vector=None, origin=None):\n        if origin is None:\n            origin = self.centroid\n        if vector is None:\n            vector = self.scale / 25.0\n\n        vector = np.asanyarray(vector, dtype=np.float64)\n        origin = np.asanyarray(origin, dtype=np.float64)\n\n        for node_name in self.graph.nodes_geometry:\n            transform, geometry_name = self.graph[node_name]\n\n            centroid = self.geometry[geometry_name].centroid\n            # transform centroid into nodes location\n            centroid = np.dot(transform,\n                              np.append(centroid, 1))[:3]\n\n            if vector.shape == ():\n                # case where our vector is a single number\n                offset = (centroid - origin) * vector\n            elif np.shape(vector) == (3,):\n                projected = np.dot(vector, (centroid - origin))\n                offset = vector * projected\n            else:\n                raise ValueError('explode vector wrong shape!')\n\n            transform[0:3, 3] += offset\n            self.graph[node_name] = transform", "response": "Explode a scene around a point and vector."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef scaled(self, scale):\n        scale = float(scale)\n        # matrix for 2D scaling\n        scale_2D = np.eye(3) * scale\n        # matrix for 3D scaling\n        scale_3D = np.eye(4) * scale\n\n        # preallocate transforms and geometries\n        nodes = self.graph.nodes_geometry\n        transforms = np.zeros((len(nodes), 4, 4))\n        geometries = [None] * len(nodes)\n\n        # collect list of transforms\n        for i, node in enumerate(nodes):\n            transforms[i], geometries[i] = self.graph[node]\n\n        # result is a copy\n        result = self.copy()\n        # remove all existing transforms\n        result.graph.clear()\n\n        for group in grouping.group(geometries):\n            # hashable reference to self.geometry\n            geometry = geometries[group[0]]\n            # original transform from world to geometry\n            original = transforms[group[0]]\n            # transform for geometry\n            new_geom = np.dot(scale_3D, original)\n\n            if result.geometry[geometry].vertices.shape[1] == 2:\n                # if our scene is 2D only scale in 2D\n                result.geometry[geometry].apply_transform(scale_2D)\n            else:\n                # otherwise apply the full transform\n                result.geometry[geometry].apply_transform(new_geom)\n\n            for node, T in zip(self.graph.nodes_geometry[group],\n                               transforms[group]):\n                # generate the new transforms\n                transform = util.multi_dot(\n                    [scale_3D, T, np.linalg.inv(new_geom)])\n                # apply scale to translation\n                transform[:3, 3] *= scale\n                # update scene with new transforms\n                result.graph.update(frame_to=node,\n                                    matrix=transform,\n                                    geometry=geometry)\n        return result", "response": "Returns a copy of the current scene with meshes and scene transforms scaled to the requested factor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndisplays the current scene.", "response": "def show(self, viewer=None, **kwargs):\n        \"\"\"\n        Display the current scene.\n\n        Parameters\n        -----------\n        viewer: str 'gl':       open a pyglet window\n                str,'notebook': return ipython.display.HTML\n                None: automatically pick based on whether or not\n                          we are in an ipython notebook\n        smooth : bool\n          Turn on or off automatic smooth shading\n        \"\"\"\n\n        if viewer is None:\n            # check to see if we are in a notebook or not\n            from ..viewer import in_notebook\n            viewer = ['gl', 'notebook'][int(in_notebook())]\n\n        if viewer == 'gl':\n            # this imports pyglet, and will raise an ImportError\n            # if pyglet is not available\n            from ..viewer import SceneViewer\n            return SceneViewer(self, **kwargs)\n        elif viewer == 'notebook':\n            from ..viewer import scene_to_notebook\n            return scene_to_notebook(self, **kwargs)\n        else:\n            raise ValueError('viewer must be \"gl\", \"notebook\", or None')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef available_formats():\n    loaders = mesh_formats()\n    loaders.extend(path_formats())\n    loaders.extend(compressed_loaders.keys())\n\n    return loaders", "response": "Returns a list of all available loaders"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load(file_obj,\n         file_type=None,\n         resolver=None,\n         **kwargs):\n    \"\"\"\n    Load a mesh or vectorized path into objects:\n    Trimesh, Path2D, Path3D, Scene\n\n    Parameters\n    ---------\n    file_obj : str, or file- like object\n      The source of the data to be loadeded\n    file_type: str\n      What kind of file type do we have (eg: 'stl')\n    resolver : trimesh.visual.Resolver\n      Object to load referenced assets like materials and textures\n    kwargs : **\n      Passed to geometry __init__\n\n    Returns\n    ---------\n    geometry : Trimesh, Path2D, Path3D, Scene\n      Loaded geometry as trimesh classes\n    \"\"\"\n    # check to see if we're trying to load something\n    # that is already a native trimesh object\n    # do the check by name to avoid circular imports\n    out_types = ('Trimesh', 'Path', 'Scene')\n    if any(util.is_instance_named(file_obj, t)\n           for t in out_types):\n        log.info('Loaded called on %s object, returning input',\n                 file_obj.__class__.__name__)\n        return file_obj\n\n    # parse the file arguments into clean loadable form\n    (file_obj,  # file- like object\n     file_type,  # str, what kind of file\n     metadata,  # dict, any metadata from file name\n     opened,    # bool, did we open the file ourselves\n     resolver   # object to load referenced resources\n     ) = parse_file_args(file_obj=file_obj,\n                         file_type=file_type,\n                         resolver=resolver)\n\n    try:\n        if isinstance(file_obj, dict):\n            # if we've been passed a dict treat it as kwargs\n            kwargs.update(file_obj)\n            loaded = load_kwargs(kwargs)\n        elif file_type in path_formats():\n            # path formats get loaded with path loader\n            loaded = load_path(file_obj,\n                               file_type=file_type,\n                               **kwargs)\n        elif file_type in mesh_loaders:\n            # mesh loaders use mesh loader\n            loaded = load_mesh(file_obj,\n                               file_type=file_type,\n                               resolver=resolver,\n                               **kwargs)\n        elif file_type in compressed_loaders:\n            # for archives, like ZIP files\n            loaded = load_compressed(file_obj,\n                                     file_type=file_type,\n                                     **kwargs)\n        else:\n            if file_type in ['svg', 'dxf']:\n                # call the dummy function to raise the import error\n                # this prevents the exception from being super opaque\n                load_path()\n            else:\n                raise ValueError('File type: %s not supported' %\n                                 file_type)\n    finally:\n        # close any opened files even if we crashed out\n        if opened:\n            file_obj.close()\n\n    # add load metadata ('file_name') to each loaded geometry\n    for i in util.make_sequence(loaded):\n        i.metadata.update(metadata)\n\n    # if we opened the file in this function ourselves from a\n    # file name clean up after ourselves by closing it\n    if opened:\n        file_obj.close()\n\n    return loaded", "response": "Load a trimesh object from a file - like object or a vectorized path into a Trimesh object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_mesh(file_obj,\n              file_type=None,\n              resolver=None,\n              **kwargs):\n    \"\"\"\n    Load a mesh file into a Trimesh object\n\n    Parameters\n    -----------\n    file_obj : str or file object\n      File name or file with mesh data\n    file_type : str or None\n      Which file type, e.g. 'stl'\n    kwargs : dict\n      Passed to Trimesh constructor\n\n    Returns\n    ----------\n    mesh : trimesh.Trimesh or trimesh.Scene\n      Loaded geometry data\n    \"\"\"\n\n    # parse the file arguments into clean loadable form\n    (file_obj,  # file- like object\n     file_type,  # str, what kind of file\n     metadata,  # dict, any metadata from file name\n     opened,    # bool, did we open the file ourselves\n     resolver   # object to load referenced resources\n     ) = parse_file_args(file_obj=file_obj,\n                         file_type=file_type,\n                         resolver=resolver)\n\n    try:\n        # make sure we keep passed kwargs to loader\n        # but also make sure loader keys override passed keys\n        results = mesh_loaders[file_type](file_obj,\n                                          file_type=file_type,\n                                          resolver=resolver,\n                                          **kwargs)\n\n        if util.is_file(file_obj):\n            file_obj.close()\n\n        log.debug('loaded mesh using %s',\n                  mesh_loaders[file_type].__name__)\n\n        if not isinstance(results, list):\n            results = [results]\n\n        loaded = []\n        for result in results:\n            kwargs.update(result)\n            loaded.append(load_kwargs(kwargs))\n            loaded[-1].metadata.update(metadata)\n        if len(loaded) == 1:\n            loaded = loaded[0]\n    finally:\n        # if we failed to load close file\n        if opened:\n            file_obj.close()\n\n    return loaded", "response": "Loads a mesh file into a Trimesh object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_compressed(file_obj,\n                    file_type=None,\n                    resolver=None,\n                    mixed=False,\n                    **kwargs):\n    \"\"\"\n    Given a compressed archive load all the geometry that\n    we can from it.\n\n    Parameters\n    ----------\n    file_obj : open file-like object\n      Containing compressed data\n    file_type : str\n      Type of the archive file\n    mixed : bool\n      If False, for archives containing both 2D and 3D\n      data will only load the 3D data into the Scene.\n\n    Returns\n    ----------\n    scene : trimesh.Scene\n      Geometry loaded in to a Scene object\n    \"\"\"\n\n    # parse the file arguments into clean loadable form\n    (file_obj,  # file- like object\n     file_type,  # str, what kind of file\n     metadata,  # dict, any metadata from file name\n     opened,    # bool, did we open the file ourselves\n     resolver   # object to load referenced resources\n     ) = parse_file_args(file_obj=file_obj,\n                         file_type=file_type,\n                         resolver=resolver)\n\n    try:\n        # a dict of 'name' : file-like object\n        files = util.decompress(file_obj=file_obj,\n                                file_type=file_type)\n        # store loaded geometries as a list\n        geometries = []\n\n        # so loaders can access textures/etc\n        resolver = visual.resolvers.ZipResolver(files)\n\n        # try to save the files with meaningful metadata\n        if 'file_path' in metadata:\n            archive_name = metadata['file_path']\n        else:\n            archive_name = 'archive'\n\n        # populate our available formats\n        if mixed:\n            available = available_formats()\n        else:\n            # all types contained in ZIP archive\n            contains = set(util.split_extension(n).lower()\n                           for n in files.keys())\n            # if there are no mesh formats available\n            if contains.isdisjoint(mesh_formats()):\n                available = path_formats()\n            else:\n                available = mesh_formats()\n\n        for name, data in files.items():\n            # only load formats that we support\n            compressed_type = util.split_extension(name).lower()\n            if compressed_type not in available:\n                # don't raise an exception, just try the next one\n                continue\n            # store the file name relative to the archive\n            metadata['file_name'] = (archive_name + '/' +\n                                     os.path.basename(name))\n            # load the individual geometry\n            loaded = load(file_obj=data,\n                          file_type=compressed_type,\n                          resolver=resolver,\n                          metadata=metadata,\n                          **kwargs)\n\n            # some loaders return multiple geometries\n            if util.is_sequence(loaded):\n                # if the loader has returned a list of meshes\n                geometries.extend(loaded)\n            else:\n                # if the loader has returned a single geometry\n                geometries.append(loaded)\n\n    finally:\n        # if we opened the file in this function\n        # clean up after ourselves\n        if opened:\n            file_obj.close()\n\n    # append meshes or scenes into a single Scene object\n    result = append_scenes(geometries)\n\n    return result", "response": "Load a compressed archive into a Scene object."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_remote(url, **kwargs):\n    # import here to keep requirement soft\n    import requests\n\n    # download the mesh\n    response = requests.get(url)\n    # wrap as file object\n    file_obj = util.wrap_as_stream(response.content)\n\n    # so loaders can access textures/etc\n    resolver = visual.resolvers.WebResolver(url)\n\n    # actually load\n    loaded = load(file_obj=file_obj,\n                  file_type=url,\n                  resolver=resolver,\n                  **kwargs)\n    return loaded", "response": "Load a single mesh at a remote URL into a local object."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nloading a set of kwargs into a Trimesh object.", "response": "def load_kwargs(*args, **kwargs):\n    \"\"\"\n    Load geometry from a properly formatted dict or kwargs\n    \"\"\"\n    def handle_scene():\n        \"\"\"\n        Load a scene from our kwargs:\n\n        class:      Scene\n        geometry:   dict, name: Trimesh kwargs\n        graph:      list of dict, kwargs for scene.graph.update\n        base_frame: str, base frame of graph\n        \"\"\"\n        scene = Scene()\n        scene.geometry.update({k: load_kwargs(v) for\n                               k, v in kwargs['geometry'].items()})\n        for k in kwargs['graph']:\n            if isinstance(k, dict):\n                scene.graph.update(**k)\n            elif util.is_sequence(k) and len(k) == 3:\n                scene.graph.update(k[1], k[0], **k[2])\n        if 'base_frame' in kwargs:\n            scene.graph.base_frame = kwargs['base_frame']\n        if 'metadata' in kwargs:\n            scene.metadata.update(kwargs['metadata'])\n\n        return scene\n\n    def handle_trimesh_kwargs():\n        \"\"\"\n        Load information with vertices and faces into a mesh\n        or PointCloud object.\n        \"\"\"\n        if (isinstance(kwargs['vertices'], dict) or\n                isinstance(kwargs['faces'], dict)):\n            return Trimesh(**misc.load_dict(kwargs))\n        elif kwargs['faces'] is None:\n            # vertices without faces returns a PointCloud\n            return PointCloud(**kwargs)\n        else:\n            return Trimesh(**kwargs)\n\n    def handle_trimesh_export():\n        data, file_type = kwargs['data'], kwargs['file_type']\n        if not isinstance(data, dict):\n            data = util.wrap_as_stream(data)\n        k = mesh_loaders[file_type](data,\n                                    file_type=file_type)\n        return Trimesh(**k)\n\n    # if we've been passed a single dict instead of kwargs\n    # substitute the dict for kwargs\n    if (len(kwargs) == 0 and\n        len(args) == 1 and\n            isinstance(args[0], dict)):\n        kwargs = args[0]\n\n    # function : list of expected keys\n    handlers = {handle_scene: ('graph', 'geometry'),\n                handle_trimesh_kwargs: ('vertices', 'faces'),\n                handle_trimesh_export: ('file_type', 'data')}\n\n    # loop through handler functions and expected key\n    handler = None\n    for func, expected in handlers.items():\n        if all(i in kwargs for i in expected):\n            # all expected kwargs exist\n            handler = func\n            # exit the loop as we found one\n            break\n\n    if handler is None:\n        raise ValueError('unable to determine type!')\n\n    return handler()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the file arguments into a file - like object and a lowercase string of file type.", "response": "def parse_file_args(file_obj,\n                    file_type,\n                    resolver=None,\n                    **kwargs):\n    \"\"\"\n    Given a file_obj and a file_type try to turn them into a file-like\n    object and a lowercase string of file type.\n\n    Parameters\n    -----------\n    file_obj:  str: if string represents a file path, returns\n                    -------------------------------------------\n                    file_obj:   an 'rb' opened file object of the path\n                    file_type:  the extension from the file path\n\n               str: if string is NOT a path, but has JSON-like special characters\n                    -------------------------------------------\n                    file_obj:   the same string passed as file_obj\n                    file_type:  set to 'json'\n\n               str: string is a valid URL\n                    -------------------------------------------\n                    file_obj: an open 'rb' file object with retrieved data\n                    file_type: from the extension\n\n               str: string is not an existing path or a JSON-like object\n                    -------------------------------------------\n                    ValueError will be raised as we can't do anything with input\n\n               file like object: we cannot grab information on file_type automatically\n                    -------------------------------------------\n                    ValueError will be raised if file_type is None\n                    file_obj:  same as input\n                    file_type: same as input\n\n               other object: like a shapely.geometry.Polygon, etc:\n                    -------------------------------------------\n                    file_obj:  same as input\n                    file_type: if None initially, set to the class name\n                               (in lower case), otherwise passed through\n\n    file_type: str, type of file and handled according to above\n\n    Returns\n    -----------\n    file_obj:  loadable object\n    file_type: str, lower case of the type of file (eg 'stl', 'dae', etc)\n    metadata:  dict, any metadata\n    opened:    bool, did we open the file or not\n    \"\"\"\n    metadata = {}\n    opened = False\n    if ('metadata' in kwargs and\n            isinstance(kwargs['metadata'], dict)):\n        metadata.update(kwargs['metadata'])\n\n    if util.is_file(file_obj) and file_type is None:\n        raise ValueError('file_type must be set when passing file objects!')\n    if util.is_string(file_obj):\n        try:\n            # os.path.isfile will return False incorrectly\n            # if we don't give it an absolute path\n            file_path = os.path.expanduser(file_obj)\n            file_path = os.path.abspath(file_path)\n            exists = os.path.isfile(file_path)\n        except BaseException:\n            exists = False\n\n        # file obj is a string which exists on filesystm\n        if exists:\n            # if not passed create a resolver to find other files\n            if resolver is None:\n                resolver = visual.resolvers.FilePathResolver(file_path)\n            # save the file name and path to metadata\n            metadata['file_path'] = file_path\n            metadata['file_name'] = os.path.basename(file_obj)\n            # if file_obj is a path that exists use extension as file_type\n            if file_type is None:\n                file_type = util.split_extension(\n                    file_path,\n                    special=['tar.gz', 'tar.bz2'])\n            # actually open the file\n            file_obj = open(file_path, 'rb')\n            opened = True\n        else:\n            if '{' in file_obj:\n                # if a dict bracket is in the string, its probably a straight\n                # JSON\n                file_type = 'json'\n            elif 'https://' in file_obj or 'http://' in file_obj:\n                # we've been passed a URL, warn to use explicit function\n                # and don't do network calls via magical pipeline\n                raise ValueError(\n                    'use load_remote to load URL: {}'.format(file_obj))\n            elif file_type is None:\n                raise ValueError('string is not a file: {}'.format(file_obj))\n\n    if file_type is None:\n        file_type = file_obj.__class__.__name__\n\n    if util.is_string(file_type) and '.' in file_type:\n        # if someone has passed the whole filename as the file_type\n        # use the file extension as the file_type\n        if 'file_path' not in metadata:\n            metadata['file_path'] = file_type\n        metadata['file_name'] = os.path.basename(file_type)\n        file_type = util.split_extension(file_type)\n        if resolver is None and os.path.exists(file_type):\n            resolver = visual.resolvers.FilePathResolver(file_type)\n\n    # all our stored extensions reference in lower case\n    file_type = file_type.lower()\n\n    # if we still have no resolver try using file_obj name\n    if (resolver is None and\n        hasattr(file_obj, 'name') and\n            len(file_obj.name) > 0):\n        resolver = visual.resolvers.FilePathResolver(file_obj.name)\n\n    return file_obj, file_type, metadata, opened, resolver"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npacks smaller rectangles onto a larger rectangle using a binary tree partition tree.", "response": "def pack_rectangles(rectangles, sheet_size, shuffle=False):\n    \"\"\"\n    Pack smaller rectangles onto a larger rectangle, using a binary\n    space partition tree.\n\n    Parameters\n    ----------\n    rectangles : (n, 2) float\n      An array of (width, height) pairs\n      representing the rectangles to be packed.\n    sheet_size : (2,) float\n      Width, height of rectangular sheet\n    shuffle : bool\n      Whether or not to shuffle the insert order of the\n      smaller rectangles, as the final packing density depends\n      on insertion order.\n\n    Returns\n    ---------\n    density : float\n      Area filled over total sheet area\n    offset :  (m,2) float\n      Offsets to move rectangles to their packed location\n    inserted : (n,) bool\n      Which of the original rectangles were packed\n    consumed_box : (2,) float\n      Bounding box size of packed result\n    \"\"\"\n    offset = np.zeros((len(rectangles), 2))\n    inserted = np.zeros(len(rectangles), dtype=np.bool)\n    box_order = np.argsort(np.sum(rectangles**2, axis=1))[::-1]\n    area = 0.0\n    density = 0.0\n\n    if shuffle:\n        shuffle_len = int(np.random.random() * len(rectangles)) - 1\n        box_order[0:shuffle_len] = np.random.permutation(\n            box_order[0:shuffle_len])\n\n    sheet = RectangleBin(size=sheet_size)\n    for index in box_order:\n        insert_location = sheet.insert(rectangles[index])\n        if insert_location is not None:\n            area += np.prod(rectangles[index])\n            offset[index] += insert_location\n            inserted[index] = True\n\n    consumed_box = np.max((offset + rectangles)[inserted], axis=0)\n    density = area / np.product(consumed_box)\n\n    return density, offset[inserted], inserted, consumed_box"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pack_paths(paths, sheet_size=None):\n    from .util import concatenate\n\n    if sheet_size is not None:\n        sheet_size = np.sort(sheet_size)[::-1]\n\n    quantity = []\n    for path in paths:\n        if 'quantity' in path.metadata:\n            quantity.append(path.metadata['quantity'])\n        else:\n            quantity.append(1)\n\n    # pack using exterior polygon (will OBB)\n    polygons = [i.polygons_closed[i.root[0]] for i in paths]\n\n    # pack the polygons using rectangular bin packing\n    inserted, transforms = multipack(polygons=polygons,\n                                     quantity=quantity,\n                                     sheet_size=sheet_size)\n\n    multi = []\n    for i, T in zip(inserted, transforms):\n        multi.append(paths[i].copy())\n        multi[-1].apply_transform(T)\n\n    # append all packed paths into a single Path object\n    packed = concatenate(multi)\n\n    return packed, inserted", "response": "Packs a list of Path2D objects into a rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef multipack(polygons,\n              sheet_size=None,\n              iterations=50,\n              density_escape=.95,\n              spacing=0.094,\n              quantity=None):\n    \"\"\"\n    Pack polygons into a rectangle by taking each Polygon's OBB\n    and then packing that as a rectangle.\n\n    Parameters\n    ------------\n    polygons : (n,) shapely.geometry.Polygon\n      Source geometry\n    sheet_size : (2,) float\n      Size of rectangular sheet\n    iterations : int\n      Number of times to run the loop\n    density_escape : float\n      When to exit early (0.0 - 1.0)\n    spacing : float\n      How big a gap to leave between polygons\n    quantity : (n,) int, or None\n      Quantity of each Polygon\n\n    Returns\n    -------------\n    overall_inserted : (m,) int\n      Indexes of inserted polygons\n    packed : (m, 3, 3) float\n      Homogeonous transforms from original frame to packed frame\n    \"\"\"\n\n    from .polygons import polygons_obb\n\n    if quantity is None:\n        quantity = np.ones(len(polygons), dtype=np.int64)\n    else:\n        quantity = np.asanyarray(quantity, dtype=np.int64)\n    if len(quantity) != len(polygons):\n        raise ValueError('quantity must match polygons')\n\n    # find the oriented bounding box of the polygons\n    obb, rectangles = polygons_obb(polygons)\n\n    # pad all sides of the rectangle\n    rectangles += 2.0 * spacing\n    # move the OBB transform so the polygon is centered\n    # in the padded rectangle\n    for i, r in enumerate(rectangles):\n        obb[i][0:2, 2] += r * .5\n\n    # for polygons occurring multiple times\n    indexes = np.hstack([np.ones(q, dtype=np.int64) * i\n                         for i, q in enumerate(quantity)])\n    # stack using advanced indexing\n    obb = obb[indexes]\n    rectangles = rectangles[indexes]\n\n    # store timing\n    tic = time.time()\n    overall_density = 0.0\n\n    # if no sheet size specified, make a large one\n    if sheet_size is None:\n        max_dim = np.max(rectangles, axis=0)\n        sum_dim = np.sum(rectangles, axis=0)\n        sheet_size = [sum_dim[0], max_dim[1] * 2]\n\n    log.debug('packing %d polygons', len(polygons))\n    # run packing for a number of iterations, shuffling insertion order\n    for i in range(iterations):\n        (density,\n         offset,\n         inserted,\n         sheet) = pack_rectangles(rectangles,\n                                  sheet_size=sheet_size,\n                                  shuffle=(i != 0))\n        if density > overall_density:\n            overall_density = density\n            overall_offset = offset\n            overall_inserted = inserted\n            if density > density_escape:\n                break\n\n    toc = time.time()\n    log.debug('packing finished %i iterations in %f seconds',\n              i + 1,\n              toc - tic)\n    log.debug('%i/%i parts were packed successfully',\n              np.sum(overall_inserted),\n              quantity.sum())\n    log.debug('final rectangular density is %f.', overall_density)\n\n    # transformations to packed positions\n    packed = obb[overall_inserted]\n    # apply the offset and inter- polygon spacing\n    packed.reshape(-1, 9)[:, [2, 5]] += overall_offset + spacing\n\n    return indexes[overall_inserted], packed", "response": "Packs polygons into a rectangle by taking each Polygon s OBB and then packing them into a rectangle."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef insert(self, rectangle):\n        rectangle = np.asanyarray(rectangle, dtype=np.float64)\n\n        for child in self.child:\n            if child is not None:\n                attempt = child.insert(rectangle)\n                if attempt is not None:\n                    return attempt\n\n        if self.occupied:\n            return None\n\n        # compare the bin size to the insertion candidate size\n        size_test = self.extents - rectangle\n\n        # this means the inserted rectangle is too big for the cell\n        if np.any(size_test < -tol.zero):\n            return None\n\n        # since the cell is big enough for the current rectangle, either it\n        # is going to be inserted here, or the cell is going to be split\n        # either way, the cell is now occupied.\n        self.occupied = True\n\n        # this means the inserted rectangle fits perfectly\n        # since we already checked to see if it was negative, no abs is needed\n        if np.all(size_test < tol.zero):\n            return self.bounds[0:2]\n\n        # since the rectangle fits but the empty space is too big,\n        # we need to create some children to insert into\n        # first, we decide which way to split\n        vertical = size_test[0] > size_test[1]\n        length = rectangle[int(not vertical)]\n        child_bounds = self.split(length, vertical)\n\n        self.child[0] = RectangleBin(bounds=child_bounds[0])\n        self.child[1] = RectangleBin(bounds=child_bounds[1])\n\n        return self.child[0].insert(rectangle)", "response": "Insert a rectangle into the bin."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef split(self, length, vertical=True):\n        # also know as [minx, miny, maxx, maxy]\n        [left, bottom, right, top] = self.bounds\n        if vertical:\n            box = [[left, bottom, left + length, top],\n                   [left + length, bottom, right, top]]\n        else:\n            box = [[left, bottom, right, bottom + length],\n                   [left, bottom + length, right, top]]\n        return box", "response": "Returns two bounding boxes representing the current\n        bounds split into two smaller boxes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef oriented_bounds_2D(points, qhull_options='QbB'):\n    # make sure input is a numpy array\n    points = np.asanyarray(points, dtype=np.float64)\n    # create a convex hull object of our points\n    # 'QbB' is a qhull option which has it scale the input to unit\n    # box to avoid precision issues with very large/small meshes\n    convex = spatial.ConvexHull(\n        points, qhull_options=qhull_options)\n\n    # (n,2,3) line segments\n    hull_edges = convex.points[convex.simplices]\n    # (n,2) points on the convex hull\n    hull_points = convex.points[convex.vertices]\n\n    # direction of the edges of the hull polygon\n    edge_vectors = np.diff(hull_edges, axis=1).reshape((-1, 2))\n\n    # unitize vectors\n    edge_vectors /= np.linalg.norm(edge_vectors, axis=1).reshape((-1, 1))\n    # create a set of perpendicular vectors\n    perp_vectors = np.fliplr(edge_vectors) * [-1.0, 1.0]\n\n    # find the projection of every hull point on every edge vector\n    # this does create a potentially gigantic n^2 array in memory,\n    # and there is the 'rotating calipers' algorithm which avoids this\n    # however, we have reduced n with a convex hull and numpy dot products\n    # are extremely fast so in practice this usually ends up being pretty\n    # reasonable\n    x = np.dot(edge_vectors, hull_points.T)\n    y = np.dot(perp_vectors, hull_points.T)\n\n    # reduce the projections to maximum and minimum per edge vector\n    bounds = np.column_stack((x.min(axis=1),\n                              y.min(axis=1),\n                              x.max(axis=1),\n                              y.max(axis=1)))\n\n    # calculate the extents and area for each edge vector pair\n    extents = np.diff(bounds.reshape((-1, 2, 2)),\n                      axis=1).reshape((-1, 2))\n    area = np.product(extents, axis=1)\n    area_min = area.argmin()\n\n    # (2,) float of smallest rectangle size\n    rectangle = extents[area_min]\n\n    # find the (3,3) homogenous transformation which moves the input\n    # points to have a bounding box centered at the origin\n    offset = -bounds[area_min][:2] - (rectangle * .5)\n    theta = np.arctan2(*edge_vectors[area_min][::-1])\n    transform = transformations.planar_matrix(offset,\n                                              theta)\n\n    # we would like to consistently return an OBB with\n    # the largest dimension along the X axis rather than\n    # the long axis being arbitrarily X or Y.\n    if np.less(*rectangle):\n        # a 90 degree rotation\n        flip = transformations.planar_matrix(theta=np.pi / 2)\n        # apply the rotation\n        transform = np.dot(flip, transform)\n        # switch X and Y in the OBB extents\n        rectangle = np.roll(rectangle, 1)\n\n    return transform, rectangle", "response": "This function returns a 2D array of 2D points."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds the oriented bounding box for a Trimesh Parameters ---------- obj : trimesh.Trimesh, (n, 2) float, or (n, 3) float Mesh object or points in 2D or 3D space angle_digits : int How much angular precision do we want on our result. Even with less precision the returned extents will cover the mesh albeit with larger than minimal volume, and may experience substantial speedups. Returns ---------- to_origin : (4,4) float Transformation matrix which will move the center of the bounding box of the input mesh to the origin. extents: (3,) float The extents of the mesh once transformed with to_origin", "response": "def oriented_bounds(obj, angle_digits=1, ordered=True):\n    \"\"\"\n    Find the oriented bounding box for a Trimesh\n\n    Parameters\n    ----------\n    obj : trimesh.Trimesh, (n, 2) float, or (n, 3) float\n       Mesh object or points in 2D or 3D space\n    angle_digits : int\n       How much angular precision do we want on our result.\n       Even with less precision the returned extents will cover\n       the mesh albeit with larger than minimal volume, and may\n       experience substantial speedups.\n\n    Returns\n    ----------\n    to_origin : (4,4) float\n      Transformation matrix which will move the center of the\n      bounding box of the input mesh to the origin.\n    extents: (3,) float\n      The extents of the mesh once transformed with to_origin\n    \"\"\"\n\n    # extract a set of convex hull vertices and normals from the input\n    # we bother to do this to avoid recomputing the full convex hull if\n    # possible\n    if hasattr(obj, 'convex_hull'):\n        # if we have been passed a mesh, use its existing convex hull to pull from\n        # cache rather than recomputing. This version of the cached convex hull has\n        # normals pointing in arbitrary directions (straight from qhull)\n        # using this avoids having to compute the expensive corrected normals\n        # that mesh.convex_hull uses since normal directions don't matter here\n        vertices = obj.convex_hull.vertices\n        hull_normals = obj.convex_hull.face_normals\n    elif util.is_sequence(obj):\n        # we've been passed a list of points\n        points = np.asanyarray(obj)\n        if util.is_shape(points, (-1, 2)):\n            return oriented_bounds_2D(points)\n        elif util.is_shape(points, (-1, 3)):\n            hull_obj = spatial.ConvexHull(points)\n            vertices = hull_obj.points[hull_obj.vertices]\n            hull_normals, valid = triangles.normals(\n                hull_obj.points[hull_obj.simplices])\n        else:\n            raise ValueError('Points are not (n,3) or (n,2)!')\n    else:\n        raise ValueError(\n            'Oriented bounds must be passed a mesh or a set of points!')\n\n    # convert face normals to spherical coordinates on the upper hemisphere\n    # the vector_hemisphere call effectivly merges negative but otherwise\n    # identical vectors\n    spherical_coords = util.vector_to_spherical(\n        util.vector_hemisphere(hull_normals))\n    # the unique_rows call on merge angles gets unique spherical directions to check\n    # we get a substantial speedup in the transformation matrix creation\n    # inside the loop by converting to angles ahead of time\n    spherical_unique = grouping.unique_rows(spherical_coords,\n                                            digits=angle_digits)[0]\n\n    min_volume = np.inf\n    tic = time.time()\n\n    for spherical in spherical_coords[spherical_unique]:\n        # a matrix which will rotate each hull normal to [0,0,1]\n        to_2D = np.linalg.inv(transformations.spherical_matrix(*spherical))\n        # apply the transform here\n        projected = np.dot(to_2D, np.column_stack(\n            (vertices, np.ones(len(vertices)))).T).T[:, :3]\n\n        height = projected[:, 2].ptp()\n        rotation_2D, box = oriented_bounds_2D(projected[:, :2])\n        volume = np.product(box) * height\n        if volume < min_volume:\n            min_volume = volume\n            min_extents = np.append(box, height)\n            min_2D = to_2D.copy()\n            rotation_2D[:2, 2] = 0.0\n            rotation_Z = transformations.planar_matrix_to_3D(rotation_2D)\n\n    # combine the 2D OBB transformation with the 2D projection transform\n    to_origin = np.dot(rotation_Z, min_2D)\n\n    # transform points using our matrix to find the translation for the\n    # transform\n    transformed = transformations.transform_points(vertices,\n                                                   to_origin)\n    box_center = (transformed.min(axis=0) + transformed.ptp(axis=0) * .5)\n    to_origin[:3, 3] = -box_center\n\n    # return ordered 3D extents\n    if ordered:\n        # sort the three extents\n        order = min_extents.argsort()\n        # generate a matrix which will flip transform\n        # to match the new ordering\n        flip = np.eye(4)\n        flip[:3, :3] = -np.eye(3)[order]\n\n        # make sure transform isn't mangling triangles\n        # by reversing windings on triangles\n        if np.isclose(np.trace(flip[:3, :3]), 0.0):\n            flip[:3, :3] = np.dot(flip[:3, :3], -np.eye(3))\n\n        # apply the flip to the OBB transform\n        to_origin = np.dot(flip, to_origin)\n        # apply the order to the extents\n        min_extents = min_extents[order]\n\n    log.debug('oriented_bounds checked %d vectors in %0.4fs',\n              len(spherical_unique),\n              time.time() - tic)\n\n    return to_origin, min_extents"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef minimum_cylinder(obj, sample_count=6, angle_tol=.001):\n\n    def volume_from_angles(spherical, return_data=False):\n        \"\"\"\n        Takes spherical coordinates and calculates the volume\n        of a cylinder along that vector\n\n        Parameters\n        ---------\n        spherical : (2,) float\n           Theta and phi\n        return_data : bool\n           Flag for returned\n\n        Returns\n        --------\n        if return_data:\n            transform ((4,4) float)\n            radius (float)\n            height (float)\n        else:\n            volume (float)\n        \"\"\"\n        to_2D = transformations.spherical_matrix(*spherical,\n                                                 axes='rxyz')\n        projected = transformations.transform_points(hull,\n                                                     matrix=to_2D)\n        height = projected[:, 2].ptp()\n\n        try:\n            center_2D, radius = nsphere.minimum_nsphere(projected[:, :2])\n        except BaseException:\n            # in degenerate cases return as infinite volume\n            return np.inf\n\n        volume = np.pi * height * (radius ** 2)\n        if return_data:\n            center_3D = np.append(center_2D, projected[\n                                  :, 2].min() + (height * .5))\n            transform = np.dot(np.linalg.inv(to_2D),\n                               transformations.translation_matrix(center_3D))\n            return transform, radius, height\n        return volume\n\n    # we've been passed a mesh with radial symmetry\n    # use center mass and symmetry axis and go home early\n    if hasattr(obj, 'symmetry') and obj.symmetry == 'radial':\n        # find our origin\n        if obj.is_watertight:\n            # set origin to center of mass\n            origin = obj.center_mass\n        else:\n            # convex hull should be watertight\n            origin = obj.convex_hull.center_mass\n        # will align symmetry axis with Z and move origin to zero\n        to_2D = geometry.plane_transform(\n            origin=origin,\n            normal=obj.symmetry_axis)\n        # transform vertices to plane to check\n        on_plane = transformations.transform_points(\n            obj.vertices, to_2D)\n        # cylinder height is overall Z span\n        height = on_plane[:, 2].ptp()\n        # center mass is correct on plane, but position\n        # along symmetry axis may be wrong so slide it\n        slide = transformations.translation_matrix(\n            [0, 0, (height / 2.0) - on_plane[:, 2].max()])\n        to_2D = np.dot(slide, to_2D)\n        # radius is maximum radius\n        radius = (on_plane[:, :2] ** 2).sum(axis=1).max() ** 0.5\n        # save kwargs\n        result = {'height': height,\n                  'radius': radius,\n                  'transform': np.linalg.inv(to_2D)}\n        return result\n\n    # get the points on the convex hull of the result\n    hull = convex.hull_points(obj)\n    if not util.is_shape(hull, (-1, 3)):\n        raise ValueError('Input must be reducable to 3D points!')\n\n    # sample a hemisphere so local hill climbing can do its thing\n    samples = util.grid_linspace([[0, 0], [np.pi, np.pi]], sample_count)\n\n    # if it's rotationally symmetric the bounding cylinder\n    # is almost certainly along one of the PCI vectors\n    if hasattr(obj, 'principal_inertia_vectors'):\n        # add the principal inertia vectors if we have a mesh\n        samples = np.vstack(\n            (samples,\n             util.vector_to_spherical(obj.principal_inertia_vectors)))\n\n    tic = [time.time()]\n    # the projected volume at each sample\n    volumes = np.array([volume_from_angles(i) for i in samples])\n    # the best vector in (2,) spherical coordinates\n    best = samples[volumes.argmin()]\n    tic.append(time.time())\n\n    # since we already explored the global space, set the bounds to be\n    # just around the sample that had the lowest volume\n    step = 2 * np.pi / sample_count\n    bounds = [(best[0] - step, best[0] + step),\n              (best[1] - step, best[1] + step)]\n    # run the local optimization\n    r = optimize.minimize(volume_from_angles,\n                          best,\n                          tol=angle_tol,\n                          method='SLSQP',\n                          bounds=bounds)\n\n    tic.append(time.time())\n    log.info('Performed search in %f and minimize in %f', *np.diff(tic))\n\n    # actually chunk the information about the cylinder\n    transform, radius, height = volume_from_angles(r['x'], return_data=True)\n    result = {'transform': transform,\n              'radius': radius,\n              'height': height}\n    return result", "response": "Returns the approximate minimum volume of a cylinder."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the 8 corners of the cube containing the given bounds.", "response": "def corners(bounds):\n    \"\"\"\n    Given a pair of axis aligned bounds, return all\n    8 corners of the bounding box.\n\n    Parameters\n    ----------\n    bounds : (2,3) or (2,2) float\n      Axis aligned bounds\n\n    Returns\n    ----------\n    corners : (8,3) float\n      Corner vertices of the cube\n    \"\"\"\n\n    bounds = np.asanyarray(bounds, dtype=np.float64)\n\n    if util.is_shape(bounds, (2, 2)):\n        bounds = np.column_stack((bounds, [0, 0]))\n    elif not util.is_shape(bounds, (2, 3)):\n        raise ValueError('bounds must be (2,2) or (2,3)!')\n\n    minx, miny, minz, maxx, maxy, maxz = np.arange(6)\n    corner_index = np.array([minx, miny, minz,\n                             maxx, miny, minz,\n                             maxx, maxy, minz,\n                             minx, maxy, minz,\n                             minx, miny, maxz,\n                             maxx, miny, maxz,\n                             maxx, maxy, maxz,\n                             minx, maxy, maxz]).reshape((-1, 3))\n\n    corners = bounds.reshape(-1)[corner_index]\n    return corners"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef contains(bounds, points):\n    # make sure we have correct input types\n    bounds = np.asanyarray(bounds, dtype=np.float64)\n    points = np.asanyarray(points, dtype=np.float64)\n\n    if len(bounds) != 2:\n        raise ValueError('bounds must be (2,dimension)!')\n    if not util.is_shape(points, (-1, bounds.shape[1])):\n        raise ValueError('bounds shape must match points!')\n\n    # run the simple check\n    points_inside = np.logical_and(\n        (points > bounds[0]).all(axis=1),\n        (points < bounds[1]).all(axis=1))\n\n    return points_inside", "response": "Does an axis aligned bounding box check on a list of points."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the HTML that will render the scene using GLTF encoded to base64 loaded by three. js", "response": "def scene_to_html(scene):\n    \"\"\"\n    Return HTML that will render the scene using\n    GLTF/GLB encoded to base64 loaded by three.js\n\n    Parameters\n    --------------\n    scene : trimesh.Scene\n      Source geometry\n\n    Returns\n    --------------\n    html : str\n      HTML containing embedded geometry\n    \"\"\"\n    # use os.path.join so this works on windows\n    base = get_resource('viewer.html.template')\n\n    # get export as bytes\n    data = scene.export(file_type='glb')\n    # encode as base64 string\n    encoded = base64.b64encode(data).decode('utf-8')\n\n    # replace keyword with our scene data\n    result = base.replace('$B64GLTF', encoded)\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scene_to_notebook(scene, height=500, **kwargs):\n    # keep as soft dependency\n    from IPython import display\n\n    # convert scene to a full HTML page\n    as_html = scene_to_html(scene=scene)\n\n    # escape the quotes in the HTML\n    srcdoc = as_html.replace('\"', '&quot;')\n    # embed this puppy as the srcdoc attr of an IFframe\n    # I tried this a dozen ways and this is the only one that works\n    # display.IFrame/display.Javascript really, really don't work\n    embedded = display.HTML(\n        '<iframe srcdoc=\"{srcdoc}\" '\n        'width=\"100%\" height=\"{height}px\" '\n        'style=\"border:none;\"></iframe>'.format(\n            srcdoc=srcdoc,\n            height=height))\n    return embedded", "response": "Convert a scene to a full HTML page containing embedded geometry\n    and a three. js viewer that will display nicely in\n    an IPython notebook."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef in_notebook():\n    try:\n        # function returns IPython context, but only in IPython\n        ipy = get_ipython()  # NOQA\n        # we only want to render rich output in notebooks\n        # in terminals we definitely do not want to output HTML\n        name = str(ipy.__class__).lower()\n        terminal = 'terminal' in name\n\n        # spyder uses ZMQshell, and can appear to be a notebook\n        spyder = '_' in os.environ and 'spyder' in os.environ['_']\n\n        # assume we are in a notebook if we are not in\n        # a terminal and we haven't been run by spyder\n        notebook = (not terminal) and (not spyder)\n\n        return notebook\n\n    except BaseException:\n        return False", "response": "Check to see if we are in a notebook"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncon is TRUE <= > b [ i is a consonant.", "response": "def _cons(self, word, i):\n        \"\"\"cons(i) is TRUE <=> b[i] is a consonant.\"\"\"\n        if word[i] in self.vowels:\n            return False\n        if word[i] == 'y':\n            if i == 0:\n                return True\n            else:\n                return (not self._cons(word, i - 1))\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _vowelinstem(self, stem):\n        for i in range(len(stem)):\n            if not self._cons(stem, i):\n                return True\n        return False", "response": "vowelinstem is TRUE <= > stem contains a vowel"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nam TRUE <= > word ends with a double consonant", "response": "def _doublec(self, word):\n        \"\"\"doublec(word) is TRUE <=> word ends with a double consonant\"\"\"\n        if len(word) < 2:\n            return False\n        if (word[-1] != word[-2]):\n            return False\n        return self._cons(word, len(word)-1)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cvc(self, word, i):\n        if i == 0: return False  # i == 0 never happens perhaps\n        if i == 1: return (not self._cons(word, 0) and self._cons(word, 1))\n        if not self._cons(word, i) or self._cons(word, i-1) or not self._cons(word, i-2): return False\n\n        ch = word[i]\n        if ch == 'w' or ch == 'x' or ch == 'y':\n            return False\n\n        return True", "response": "Return True if word contains a consonant and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _step1c(self, word):\n        if word[-1] == 'y' and len(word) > 2 and self._cons(word, len(word) - 2):\n            return word[:-1] + 'i'\n        else:\n            return word", "response": "This function turns terminal y to i when there is another vowel in the stem."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the unusual part of the stemmer word.", "response": "def _step2(self, word):\n        \"\"\"step2() maps double suffices to single ones.\n        so -ization ( = -ize plus -ation) maps to -ize etc. note that the\n        string before the suffix must give m() > 0.\n        \"\"\"\n        if len(word) <= 1: # Only possible at this stage given unusual inputs to stem_word like 'oed'\n            return word\n\n        ch = word[-2]\n\n        if ch == 'a':\n            if word.endswith(\"ational\"):\n                return word[:-7] + \"ate\" if self._m(word, len(word)-8) > 0 else word\n            elif word.endswith(\"tional\"):\n                return word[:-2] if self._m(word, len(word)-7) > 0 else word\n            else:\n                return word\n        elif ch == 'c':\n            if word.endswith(\"enci\"):\n                return word[:-4] + \"ence\" if self._m(word, len(word)-5) > 0 else word\n            elif word.endswith(\"anci\"):\n                return word[:-4] + \"ance\" if self._m(word, len(word)-5) > 0 else word\n            else:\n                return word\n        elif ch == 'e':\n            if word.endswith(\"izer\"):\n                return word[:-1] if self._m(word, len(word)-5) > 0 else word\n            else:\n                return word\n        elif ch == 'l':\n            if word.endswith(\"bli\"):\n                return word[:-3] + \"ble\" if self._m(word, len(word)-4) > 0 else word # --DEPARTURE--\n            # To match the published algorithm, replace \"bli\" with \"abli\" and \"ble\" with \"able\"\n            elif word.endswith(\"alli\"):\n                # --NEW--\n                if self._m(word, len(word)-5) > 0:\n                    word = word[:-2]\n                    return self._step2(word)\n                else:\n                    return word\n            elif word.endswith(\"fulli\"):\n                return word[:-2] if self._m(word, len(word)-6) else word # --NEW--\n            elif word.endswith(\"entli\"):\n                return word[:-2] if self._m(word, len(word)-6) else word\n            elif word.endswith(\"eli\"):\n                return word[:-2] if self._m(word, len(word)-4) else word\n            elif word.endswith(\"ousli\"):\n                return word[:-2] if self._m(word, len(word)-6) else word\n            else:\n                return word\n        elif ch == 'o':\n            if word.endswith(\"ization\"):\n                return word[:-7] + \"ize\" if self._m(word, len(word)-8) else word\n            elif word.endswith(\"ation\"):\n                return word[:-5] + \"ate\" if self._m(word, len(word)-6) else word\n            elif word.endswith(\"ator\"):\n                return word[:-4] + \"ate\" if self._m(word, len(word)-5) else word\n            else:\n                return word\n        elif ch == 's':\n            if word.endswith(\"alism\"):\n                return word[:-3] if self._m(word, len(word)-6) else word\n            elif word.endswith(\"ness\"):\n                if word.endswith(\"iveness\"):\n                    return word[:-4] if self._m(word, len(word)-8) else word\n                elif word.endswith(\"fulness\"):\n                    return word[:-4] if self._m(word, len(word)-8) else word\n                elif word.endswith(\"ousness\"):\n                    return word[:-4] if self._m(word, len(word)-8) else word\n                else:\n                    return word\n            else:\n                return word\n        elif ch == 't':\n            if word.endswith(\"aliti\"):\n                return word[:-3] if self._m(word, len(word)-6) else word\n            elif word.endswith(\"iviti\"):\n                return word[:-5] + \"ive\" if self._m(word, len(word)-6) else word\n            elif word.endswith(\"biliti\"):\n                return word[:-6] + \"ble\" if self._m(word, len(word)-7) else word\n            else:\n                return word\n        elif ch == 'g': # --DEPARTURE--\n            if word.endswith(\"logi\"):\n                return word[:-1] if self._m(word, len(word) - 4) else word # --NEW-- (Barry Wilkins)\n            # To match the published algorithm, pass len(word)-5 to _m instead of len(word)-4\n            else:\n                return word\n\n        else:\n            return word"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstep 3 returns the most similar version of the words in the sequence", "response": "def _step3(self, word):\n        \"\"\"step3() deals with -ic-, -full, -ness etc. similar strategy to step2.\"\"\"\n\n        ch = word[-1]\n\n        if ch == 'e':\n            if word.endswith(\"icate\"):\n                return word[:-3] if self._m(word, len(word)-6) else word\n            elif word.endswith(\"ative\"):\n                return word[:-5] if self._m(word, len(word)-6) else word\n            elif word.endswith(\"alize\"):\n                return word[:-3] if self._m(word, len(word)-6) else word\n            else:\n                return word\n        elif ch == 'i':\n            if word.endswith(\"iciti\"):\n                return word[:-3] if self._m(word, len(word)-6) else word\n            else:\n                return word\n        elif ch == 'l':\n            if word.endswith(\"ical\"):\n                return word[:-2] if self._m(word, len(word)-5) else word\n            elif word.endswith(\"ful\"):\n                return word[:-3] if self._m(word, len(word)-4) else word\n            else:\n                return word\n        elif ch == 's':\n            if word.endswith(\"ness\"):\n                return word[:-4] if self._m(word, len(word)-5) else word\n            else:\n                return word\n\n        else:\n            return word"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of unique id for the stemmed word.", "response": "def _step4(self, word):\n        \"\"\"step4() takes off -ant, -ence etc., in context <c>vcvc<v>.\"\"\"\n\n        if len(word) <= 1: # Only possible at this stage given unusual inputs to stem_word like 'oed'\n            return word\n\n        ch = word[-2]\n\n        if ch == 'a':\n            if word.endswith(\"al\"):\n                return word[:-2] if self._m(word, len(word)-3) > 1 else word\n            else:\n                return word\n        elif ch == 'c':\n            if word.endswith(\"ance\"):\n                return word[:-4] if self._m(word, len(word)-5) > 1 else word\n            elif word.endswith(\"ence\"):\n                return word[:-4] if self._m(word, len(word)-5) > 1 else word\n            else:\n                return word\n        elif ch == 'e':\n            if word.endswith(\"er\"):\n                return word[:-2] if self._m(word, len(word)-3) > 1 else word\n            else:\n                return word\n        elif ch == 'i':\n            if word.endswith(\"ic\"):\n                return word[:-2] if self._m(word, len(word)-3) > 1 else word\n            else:\n                return word\n        elif ch == 'l':\n            if word.endswith(\"able\"):\n                return word[:-4] if self._m(word, len(word)-5) > 1 else word\n            elif word.endswith(\"ible\"):\n                return word[:-4] if self._m(word, len(word)-5) > 1 else word\n            else:\n                return word\n        elif ch == 'n':\n            if word.endswith(\"ant\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            elif word.endswith(\"ement\"):\n                return word[:-5] if self._m(word, len(word)-6) > 1 else word\n            elif word.endswith(\"ment\"):\n                return word[:-4] if self._m(word, len(word)-5) > 1 else word\n            elif word.endswith(\"ent\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            else:\n                return word\n        elif ch == 'o':\n            if word.endswith(\"sion\") or word.endswith(\"tion\"): # slightly different logic to all the other cases\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            elif word.endswith(\"ou\"):\n                return word[:-2] if self._m(word, len(word)-3) > 1 else word\n            else:\n                return word\n        elif ch == 's':\n            if word.endswith(\"ism\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            else:\n                return word\n        elif ch == 't':\n            if word.endswith(\"ate\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            elif word.endswith(\"iti\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            else:\n                return word\n        elif ch == 'u':\n            if word.endswith(\"ous\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            else:\n                return word\n        elif ch == 'v':\n            if word.endswith(\"ive\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            else:\n                return word\n        elif ch == 'z':\n            if word.endswith(\"ize\"):\n                return word[:-3] if self._m(word, len(word)-4) > 1 else word\n            else:\n                return word\n        else:\n            return word"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving final - e if m > 1 and changes - ll to - l if m > 1.", "response": "def _step5(self, word):\n        \"\"\"step5() removes a final -e if m() > 1, and changes -ll to -l if\n        m() > 1.\n        \"\"\"\n        if word[-1] == 'e':\n            a = self._m(word, len(word)-1)\n            if a > 1 or (a == 1 and not self._cvc(word, len(word)-2)):\n                word = word[:-1]\n        if word.endswith('ll') and self._m(word, len(word)-1) > 1:\n            word = word[:-1]\n\n        return word"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stem_word(self, p, i=0, j=None):\n        ## --NLTK--\n        if j is None and i == 0:\n            word = p\n        else:\n            if j is None:\n                j = len(p) - 1\n            word = p[i:j+1]\n\n        if word in self.pool:\n            return self.pool[word]\n\n        if len(word) <= 2:\n            return word # --DEPARTURE--\n        # With this line, strings of length 1 or 2 don't go through the\n        # stemming process, although no mention is made of this in the\n        # published algorithm. Remove the line to match the published\n        # algorithm.\n\n        word = self._step1ab(word)\n        word = self._step1c(word)\n        word = self._step2(word)\n        word = self._step3(word)\n        word = self._step4(word)\n        word = self._step5(word)\n        return word", "response": "Returns the stem of the word in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _r1_scandinavian(self, word, vowels):\n        r1 = \"\"\n        for i in range(1, len(word)):\n            if word[i] not in vowels and word[i-1] in vowels:\n                if len(word[:i+1]) < 3 and len(word[:i+1]) > 0:\n                    r1 = word[3:]\n                elif len(word[:i+1]) >= 3:\n                    r1 = word[i+1:]\n                else:\n                    return word\n                break\n\n        return r1", "response": "This method returns the R1 that is used by the Scandinavian stemmers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _r1r2_standard(self, word, vowels):\n        r1 = \"\"\n        r2 = \"\"\n        for i in range(1, len(word)):\n            if word[i] not in vowels and word[i-1] in vowels:\n                r1 = word[i+1:]\n                break\n\n        for i in range(1, len(r1)):\n            if r1[i] not in vowels and r1[i-1] in vowels:\n                r2 = r1[i+1:]\n                break\n\n        return (r1, r2)", "response": "This method returns the standard interpretations of the string regions R1 and R2 for the respective word."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the standard interpretation of the string region RV. If the second letter is a consonant, RV is the region after the next following vowel. If the first two letters are vowels, RV is the region after the next following consonant. Otherwise, RV is the region after the third letter. :param word: The word whose region RV is determined. :type word: str or unicode :param vowels: The vowels of the respective language that are used to determine the region RV. :type vowels: unicode :return: the region RV for the respective word. :rtype: unicode :note: This helper method is invoked by the respective stem method of the subclasses ItalianStemmer, PortugueseStemmer, RomanianStemmer, and SpanishStemmer. It is not to be invoked directly!", "response": "def _rv_standard(self, word, vowels):\n        \"\"\"\n        Return the standard interpretation of the string region RV.\n\n        If the second letter is a consonant, RV is the region after the\n        next following vowel. If the first two letters are vowels, RV is\n        the region after the next following consonant. Otherwise, RV is\n        the region after the third letter.\n\n        :param word: The word whose region RV is determined.\n        :type word: str or unicode\n        :param vowels: The vowels of the respective language that are\n                       used to determine the region RV.\n        :type vowels: unicode\n        :return: the region RV for the respective word.\n        :rtype: unicode\n        :note: This helper method is invoked by the respective stem method of\n               the subclasses ItalianStemmer, PortugueseStemmer,\n               RomanianStemmer, and SpanishStemmer. It is not to be\n               invoked directly!\n\n        \"\"\"\n        rv = \"\"\n        if len(word) >= 2:\n            if word[1] not in vowels:\n                for i in range(2, len(word)):\n                    if word[i] in vowels:\n                        rv = word[i+1:]\n                        break\n\n            elif word[:2] in vowels:\n                for i in range(2, len(word)):\n                    if word[i] not in vowels:\n                        rv = word[i+1:]\n                        break\n            else:\n                rv = word[3:]\n\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stem(self, word):\n        word = word.lower()\n\n        step2_success = False\n\n        # Vowel accents are removed.\n        word = (word.replace(\"\\xE4\", \"a\").replace(\"\\xE1\", \"a\")\n                    .replace(\"\\xEB\", \"e\").replace(\"\\xE9\", \"e\")\n                    .replace(\"\\xED\", \"i\").replace(\"\\xEF\", \"i\")\n                    .replace(\"\\xF6\", \"o\").replace(\"\\xF3\", \"o\")\n                    .replace(\"\\xFC\", \"u\").replace(\"\\xFA\", \"u\"))\n\n        # An initial 'y', a 'y' after a vowel,\n        # and an 'i' between self.__vowels is put into upper case.\n        # As from now these are treated as consonants.\n        if word.startswith(\"y\"):\n            word = \"\".join((\"Y\", word[1:]))\n\n        for i in range(1, len(word)):\n            if word[i-1] in self.__vowels and word[i] == \"y\":\n                word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        for i in range(1, len(word)-1):\n            if (word[i-1] in self.__vowels and word[i] == \"i\" and\n               word[i+1] in self.__vowels):\n                word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n        # R1 is adjusted so that the region before it\n        # contains at least 3 letters.\n        for i in range(1, len(word)):\n            if word[i] not in self.__vowels and word[i-1] in self.__vowels:\n                if len(word[:i+1]) < 3 and len(word[:i+1]) > 0:\n                    r1 = word[3:]\n                elif len(word[:i+1]) == 0:\n                    return word\n                break\n\n        # STEP 1\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"heden\":\n                    word = \"\".join((word[:-5], \"heid\"))\n                    r1 = \"\".join((r1[:-5], \"heid\"))\n                    if r2.endswith(\"heden\"):\n                        r2 = \"\".join((r2[:-5], \"heid\"))\n\n                elif (suffix in (\"ene\", \"en\") and\n                      not word.endswith(\"heden\") and\n                      word[-len(suffix)-1] not in self.__vowels and\n                      word[-len(suffix)-3:-len(suffix)] != \"gem\"):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    if word.endswith((\"kk\", \"dd\", \"tt\")):\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n\n                elif (suffix in (\"se\", \"s\") and\n                      word[-len(suffix)-1] not in self.__vowels and\n                      word[-len(suffix)-1] != \"j\"):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        # STEP 2\n        if r1.endswith(\"e\") and word[-2] not in self.__vowels:\n            step2_success = True\n            word = word[:-1]\n            r1 = r1[:-1]\n            r2 = r2[:-1]\n\n            if word.endswith((\"kk\", \"dd\", \"tt\")):\n                word = word[:-1]\n                r1 = r1[:-1]\n                r2 = r2[:-1]\n\n        # STEP 3a\n        if r2.endswith(\"heid\") and word[-5] != \"c\":\n            word = word[:-4]\n            r1 = r1[:-4]\n            r2 = r2[:-4]\n\n            if (r1.endswith(\"en\") and word[-3] not in self.__vowels and\n                word[-5:-2] != \"gem\"):\n                word = word[:-2]\n                r1 = r1[:-2]\n                r2 = r2[:-2]\n\n                if word.endswith((\"kk\", \"dd\", \"tt\")):\n                    word = word[:-1]\n                    r1 = r1[:-1]\n                    r2 = r2[:-1]\n\n        # STEP 3b: Derivational suffixes\n        for suffix in self.__step3b_suffixes:\n            if r2.endswith(suffix):\n                if suffix in (\"end\", \"ing\"):\n                    word = word[:-3]\n                    r2 = r2[:-3]\n\n                    if r2.endswith(\"ig\") and word[-3] != \"e\":\n                        word = word[:-2]\n                    else:\n                        if word.endswith((\"kk\", \"dd\", \"tt\")):\n                            word = word[:-1]\n\n                elif suffix == \"ig\" and word[-3] != \"e\":\n                    word = word[:-2]\n\n                elif suffix == \"lijk\":\n                    word = word[:-4]\n                    r1 = r1[:-4]\n\n                    if r1.endswith(\"e\") and word[-2] not in self.__vowels:\n                        word = word[:-1]\n                        if word.endswith((\"kk\", \"dd\", \"tt\")):\n                            word = word[:-1]\n\n                elif suffix == \"baar\":\n                    word = word[:-4]\n\n                elif suffix == \"bar\" and step2_success:\n                    word = word[:-3]\n                break\n\n        # STEP 4: Undouble vowel\n        if len(word) >= 4:\n            if word[-1] not in self.__vowels and word[-1] != \"I\":\n                if word[-3:-1] in (\"aa\", \"ee\", \"oo\", \"uu\"):\n                    if word[-4] not in self.__vowels:\n                        word = \"\".join((word[:-3], word[-3], word[-1]))\n\n        # All occurrences of 'I' and 'Y' are put back into lower case.\n        word = word.replace(\"I\", \"i\").replace(\"Y\", \"y\")\n\n\n        return word", "response": "Stem a Dutch word and return the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stem(self, word):\n\n        \"\"\"\n        Stem an English word and return the stemmed form.\n\n        :param word: The word that is stemmed.\n        :type word: str or unicode\n        :return: The stemmed form.\n        :rtype: unicode\n\n        \"\"\"\n        word = word.lower()\n\n        if len(word) <= 2:\n            return word\n\n        elif word in self.__special_words:\n            return self.__special_words[word]\n\n        # Map the different apostrophe characters to a single consistent one\n        word = (word.replace(\"\\u2019\", \"\\x27\")\n                    .replace(\"\\u2018\", \"\\x27\")\n                    .replace(\"\\u201B\", \"\\x27\"))\n\n        if word.startswith(\"\\x27\"):\n            word = word[1:]\n\n        if word.startswith(\"y\"):\n            word = \"\".join((\"Y\", word[1:]))\n\n        for i in range(1, len(word)):\n            if word[i-1] in self.__vowels and word[i] == \"y\":\n                word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        step1a_vowel_found = False\n        step1b_vowel_found = False\n\n        r1 = \"\"\n        r2 = \"\"\n\n        if word.startswith((\"gener\", \"commun\", \"arsen\")):\n            if word.startswith((\"gener\", \"arsen\")):\n                r1 = word[5:]\n            else:\n                r1 = word[6:]\n\n            for i in range(1, len(r1)):\n                if r1[i] not in self.__vowels and r1[i-1] in self.__vowels:\n                    r2 = r1[i+1:]\n                    break\n        else:\n            r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n\n        # STEP 0\n        for suffix in self.__step0_suffixes:\n            if word.endswith(suffix):\n                word = word[:-len(suffix)]\n                r1 = r1[:-len(suffix)]\n                r2 = r2[:-len(suffix)]\n                break\n\n        # STEP 1a\n        for suffix in self.__step1a_suffixes:\n            if word.endswith(suffix):\n\n                if suffix == \"sses\":\n                    word = word[:-2]\n                    r1 = r1[:-2]\n                    r2 = r2[:-2]\n\n                elif suffix in (\"ied\", \"ies\"):\n                    if len(word[:-len(suffix)]) > 1:\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                    else:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n\n                elif suffix == \"s\":\n                    for letter in word[:-2]:\n                        if letter in self.__vowels:\n                            step1a_vowel_found = True\n                            break\n\n                    if step1a_vowel_found:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                break\n\n        # STEP 1b\n        for suffix in self.__step1b_suffixes:\n            if word.endswith(suffix):\n                if suffix in (\"eed\", \"eedly\"):\n\n                    if r1.endswith(suffix):\n                        word = \"\".join((word[:-len(suffix)], \"ee\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ee\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ee\"))\n                        else:\n                            r2 = \"\"\n                else:\n                    for letter in word[:-len(suffix)]:\n                        if letter in self.__vowels:\n                            step1b_vowel_found = True\n                            break\n\n                    if step1b_vowel_found:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n\n                        if word.endswith((\"at\", \"bl\", \"iz\")):\n                            word = \"\".join((word, \"e\"))\n                            r1 = \"\".join((r1, \"e\"))\n\n                            if len(word) > 5 or len(r1) >=3:\n                                r2 = \"\".join((r2, \"e\"))\n\n                        elif word.endswith(self.__double_consonants):\n                            word = word[:-1]\n                            r1 = r1[:-1]\n                            r2 = r2[:-1]\n\n                        elif ((r1 == \"\" and len(word) >= 3 and\n                               word[-1] not in self.__vowels and\n                               word[-1] not in \"wxY\" and\n                               word[-2] in self.__vowels and\n                               word[-3] not in self.__vowels)\n                              or\n                              (r1 == \"\" and len(word) == 2 and\n                               word[0] in self.__vowels and\n                               word[1] not in self.__vowels)):\n\n                            word = \"\".join((word, \"e\"))\n\n                            if len(r1) > 0:\n                                r1 = \"\".join((r1, \"e\"))\n\n                            if len(r2) > 0:\n                                r2 = \"\".join((r2, \"e\"))\n                break\n\n        # STEP 1c\n        if len(word) > 2 and word[-1] in \"yY\" and word[-2] not in self.__vowels:\n            word = \"\".join((word[:-1], \"i\"))\n            if len(r1) >= 1:\n                r1 = \"\".join((r1[:-1], \"i\"))\n            else:\n                r1 = \"\"\n\n            if len(r2) >= 1:\n                r2 = \"\".join((r2[:-1], \"i\"))\n            else:\n                r2 = \"\"\n\n        # STEP 2\n        for suffix in self.__step2_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix == \"tional\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix in (\"enci\", \"anci\", \"abli\"):\n                        word = \"\".join((word[:-1], \"e\"))\n\n                        if len(r1) >= 1:\n                            r1 = \"\".join((r1[:-1], \"e\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= 1:\n                            r2 = \"\".join((r2[:-1], \"e\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"entli\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix in (\"izer\", \"ization\"):\n                        word = \"\".join((word[:-len(suffix)], \"ize\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ize\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ize\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix in (\"ational\", \"ation\", \"ator\"):\n                        word = \"\".join((word[:-len(suffix)], \"ate\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ate\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ate\"))\n                        else:\n                            r2 = \"e\"\n\n                    elif suffix in (\"alism\", \"aliti\", \"alli\"):\n                        word = \"\".join((word[:-len(suffix)], \"al\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"al\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"al\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"fulness\":\n                        word = word[:-4]\n                        r1 = r1[:-4]\n                        r2 = r2[:-4]\n\n                    elif suffix in (\"ousli\", \"ousness\"):\n                        word = \"\".join((word[:-len(suffix)], \"ous\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ous\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ous\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix in (\"iveness\", \"iviti\"):\n                        word = \"\".join((word[:-len(suffix)], \"ive\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ive\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ive\"))\n                        else:\n                            r2 = \"e\"\n\n                    elif suffix in (\"biliti\", \"bli\"):\n                        word = \"\".join((word[:-len(suffix)], \"ble\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ble\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ble\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"ogi\" and word[-4] == \"l\":\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n\n                    elif suffix in (\"fulli\", \"lessli\"):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix == \"li\" and word[-3] in self.__li_ending:\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                break\n\n        # STEP 3\n        for suffix in self.__step3_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix == \"tional\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                    elif suffix == \"ational\":\n                        word = \"\".join((word[:-len(suffix)], \"ate\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ate\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ate\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix == \"alize\":\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n\n                    elif suffix in (\"icate\", \"iciti\", \"ical\"):\n                        word = \"\".join((word[:-len(suffix)], \"ic\"))\n\n                        if len(r1) >= len(suffix):\n                            r1 = \"\".join((r1[:-len(suffix)], \"ic\"))\n                        else:\n                            r1 = \"\"\n\n                        if len(r2) >= len(suffix):\n                            r2 = \"\".join((r2[:-len(suffix)], \"ic\"))\n                        else:\n                            r2 = \"\"\n\n                    elif suffix in (\"ful\", \"ness\"):\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n\n                    elif suffix == \"ative\" and r2.endswith(suffix):\n                        word = word[:-5]\n                        r1 = r1[:-5]\n                        r2 = r2[:-5]\n                break\n\n        # STEP 4\n        for suffix in self.__step4_suffixes:\n            if word.endswith(suffix):\n                if r2.endswith(suffix):\n                    if suffix == \"ion\":\n                        if word[-4] in \"st\":\n                            word = word[:-3]\n                            r1 = r1[:-3]\n                            r2 = r2[:-3]\n                    else:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                break\n\n        # STEP 5\n        if r2.endswith(\"l\") and word[-2] == \"l\":\n            word = word[:-1]\n        elif r2.endswith(\"e\"):\n            word = word[:-1]\n        elif r1.endswith(\"e\"):\n            if len(word) >= 4 and (word[-2] in self.__vowels or\n                                   word[-2] in \"wxY\" or\n                                   word[-3] not in self.__vowels or\n                                   word[-4] in self.__vowels):\n                word = word[:-1]\n\n\n        word = word.replace(\"Y\", \"y\")\n\n\n        return word", "response": "This function takes an English word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stem(self, word):\n        word = word.lower()\n\n        step3_success = False\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n        # STEP 1: Particles etc.\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"sti\":\n                    if suffix in r2:\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                else:\n                    if word[-len(suffix)-1] in \"ntaeiouy\\xE4\\xF6\":\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                break\n\n        # STEP 2: Possessives\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"si\":\n                    if word[-3] != \"k\":\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                elif suffix == \"ni\":\n                    word = word[:-2]\n                    r1 = r1[:-2]\n                    r2 = r2[:-2]\n                    if word.endswith(\"kse\"):\n                        word = \"\".join((word[:-3], \"ksi\"))\n\n                    if r1.endswith(\"kse\"):\n                        r1 = \"\".join((r1[:-3], \"ksi\"))\n\n                    if r2.endswith(\"kse\"):\n                        r2 = \"\".join((r2[:-3], \"ksi\"))\n\n                elif suffix == \"an\":\n                    if (word[-4:-2] in (\"ta\", \"na\") or\n                        word[-5:-2] in (\"ssa\", \"sta\", \"lla\", \"lta\")):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                elif suffix == \"\\xE4n\":\n                    if (word[-4:-2] in (\"t\\xE4\", \"n\\xE4\") or\n                        word[-5:-2] in (\"ss\\xE4\", \"st\\xE4\",\n                                        \"ll\\xE4\", \"lt\\xE4\")):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n\n                elif suffix == \"en\":\n                    if word[-5:-2] in (\"lle\", \"ine\"):\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                else:\n                    word = word[:-3]\n                    r1 = r1[:-3]\n                    r2 = r2[:-3]\n                break\n\n        # STEP 3: Cases\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                if suffix in (\"han\", \"hen\", \"hin\", \"hon\", \"h\\xE4n\",\n                              \"h\\xF6n\"):\n                    if ((suffix == \"han\" and word[-4] == \"a\") or\n                        (suffix == \"hen\" and word[-4] == \"e\") or\n                        (suffix == \"hin\" and word[-4] == \"i\") or\n                        (suffix == \"hon\" and word[-4] == \"o\") or\n                        (suffix == \"h\\xE4n\" and word[-4] == \"\\xE4\") or\n                        (suffix == \"h\\xF6n\" and word[-4] == \"\\xF6\")):\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                        step3_success = True\n\n                elif suffix in (\"siin\", \"den\", \"tten\"):\n                    if (word[-len(suffix)-1] == \"i\" and\n                        word[-len(suffix)-2] in self.__restricted_vowels):\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        step3_success = True\n                    else:\n                        continue\n\n                elif suffix == \"seen\":\n                    if word[-6:-4] in self.__long_vowels:\n                        word = word[:-4]\n                        r1 = r1[:-4]\n                        r2 = r2[:-4]\n                        step3_success = True\n                    else:\n                        continue\n\n                elif suffix in (\"a\", \"\\xE4\"):\n                    if word[-2] in self.__vowels and word[-3] in self.__consonants:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                        step3_success = True\n\n                elif suffix in (\"tta\", \"tt\\xE4\"):\n                    if word[-4] == \"e\":\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                        step3_success = True\n\n                elif suffix == \"n\":\n                    word = word[:-1]\n                    r1 = r1[:-1]\n                    r2 = r2[:-1]\n                    step3_success = True\n\n                    if word[-2:] == \"ie\" or word[-2:] in self.__long_vowels:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    step3_success = True\n                break\n\n        # STEP 4: Other endings\n        for suffix in self.__step4_suffixes:\n            if r2.endswith(suffix):\n                if suffix in (\"mpi\", \"mpa\", \"mp\\xE4\", \"mmi\", \"mma\",\n                              \"mm\\xE4\"):\n                    if word[-5:-3] != \"po\":\n                        word = word[:-3]\n                        r1 = r1[:-3]\n                        r2 = r2[:-3]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        # STEP 5: Plurals\n        if step3_success and len(r1) >= 1 and r1[-1] in \"ij\":\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        elif (not step3_success and len(r1) >= 2 and\n              r1[-1] == \"t\" and r1[-2] in self.__vowels):\n            word = word[:-1]\n            r1 = r1[:-1]\n            r2 = r2[:-1]\n            if r2.endswith(\"imma\"):\n                word = word[:-4]\n                r1 = r1[:-4]\n            elif r2.endswith(\"mma\") and r2[-5:-3] != \"po\":\n                word = word[:-3]\n                r1 = r1[:-3]\n\n        # STEP 6: Tidying up\n        if r1[-2:] in self.__long_vowels:\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        if (len(r1) >= 2 and r1[-2] in self.__consonants and\n            r1[-1] in \"a\\xE4ei\"):\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        if r1.endswith((\"oj\", \"uj\")):\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        if r1.endswith(\"jo\"):\n            word = word[:-1]\n            r1 = r1[:-1]\n\n        # If the word ends with a double consonant\n        # followed by zero or more vowels, the last consonant is removed.\n        for i in range(1, len(word)):\n            if word[-i] in self.__vowels:\n                continue\n            else:\n                if i == 1:\n                    if word[-i-1:] in self.__double_consonants:\n                        word = word[:-1]\n                else:\n                    if word[-i-1:-i+1] in self.__double_consonants:\n                        word = \"\".join((word[:-i], word[-i+1:]))\n                break\n\n\n        return word", "response": "This method stems a Finnish word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stem(self, word):\n        word = word.lower()\n\n        step1_success = False\n        rv_ending_found = False\n        step2a_success = False\n        step2b_success = False\n\n        # Every occurrence of 'u' after 'q' is put into upper case.\n        for i in range(1, len(word)):\n            if word[i-1] == \"q\" and word[i] == \"u\":\n                word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n        # Every occurrence of 'u' and 'i'\n        # between vowels is put into upper case.\n        # Every occurrence of 'y' preceded or\n        # followed by a vowel is also put into upper case.\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word[i] == \"i\":\n                    word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n            if word[i-1] in self.__vowels or word[i+1] in self.__vowels:\n                if word[i] == \"y\":\n                    word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self.__rv_french(word, self.__vowels)\n\n        # STEP 1: Standard suffix removal\n        for suffix in self.__step1_suffixes:\n            if word.endswith(suffix):\n                if suffix == \"eaux\":\n                    word = word[:-1]\n                    step1_success = True\n\n                elif suffix in (\"euse\", \"euses\"):\n                    if suffix in r2:\n                        word = word[:-len(suffix)]\n                        step1_success = True\n\n                    elif suffix in r1:\n                        word = \"\".join((word[:-len(suffix)], \"eux\"))\n                        step1_success = True\n\n                elif suffix in (\"ement\", \"ements\") and suffix in rv:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-2:] == \"iv\" and \"iv\" in r2:\n                        word = word[:-2]\n\n                        if word[-2:] == \"at\" and \"at\" in r2:\n                            word = word[:-2]\n\n                    elif word[-3:] == \"eus\":\n                        if \"eus\" in r2:\n                            word = word[:-3]\n                        elif \"eus\" in r1:\n                            word = \"\".join((word[:-1], \"x\"))\n\n                    elif word[-3:] in (\"abl\", \"iqU\"):\n                        if \"abl\" in r2 or \"iqU\" in r2:\n                            word = word[:-3]\n\n                    elif word[-3:] in (\"i\\xE8r\", \"I\\xE8r\"):\n                        if \"i\\xE8r\" in rv or \"I\\xE8r\" in rv:\n                            word = \"\".join((word[:-3], \"i\"))\n\n                elif suffix == \"amment\" and suffix in rv:\n                    word = \"\".join((word[:-6], \"ant\"))\n                    rv = \"\".join((rv[:-6], \"ant\"))\n                    rv_ending_found = True\n\n                elif suffix == \"emment\" and suffix in rv:\n                    word = \"\".join((word[:-6], \"ent\"))\n                    rv_ending_found = True\n\n                elif (suffix in (\"ment\", \"ments\") and suffix in rv and\n                      not rv.startswith(suffix) and\n                      rv[rv.rindex(suffix)-1] in self.__vowels):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    rv_ending_found = True\n\n                elif suffix == \"aux\" and suffix in r1:\n                    word = \"\".join((word[:-2], \"l\"))\n                    step1_success = True\n\n                elif (suffix in (\"issement\", \"issements\") and suffix in r1\n                      and word[-len(suffix)-1] not in self.__vowels):\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                elif suffix in (\"ance\", \"iqUe\", \"isme\", \"able\", \"iste\",\n                              \"eux\", \"ances\", \"iqUes\", \"ismes\",\n                              \"ables\", \"istes\") and suffix in r2:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                elif suffix in (\"atrice\", \"ateur\", \"ation\", \"atrices\",\n                                \"ateurs\", \"ations\") and suffix in r2:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-2:] == \"ic\":\n                        if \"ic\" in r2:\n                            word = word[:-2]\n                        else:\n                            word = \"\".join((word[:-2], \"iqU\"))\n\n                elif suffix in (\"logie\", \"logies\") and suffix in r2:\n                    word = \"\".join((word[:-len(suffix)], \"log\"))\n                    step1_success = True\n\n                elif (suffix in (\"usion\", \"ution\", \"usions\", \"utions\") and\n                      suffix in r2):\n                    word = \"\".join((word[:-len(suffix)], \"u\"))\n                    step1_success = True\n\n                elif suffix in (\"ence\", \"ences\") and suffix in r2:\n                    word = \"\".join((word[:-len(suffix)], \"ent\"))\n                    step1_success = True\n\n                elif suffix in (\"it\\xE9\", \"it\\xE9s\") and suffix in r2:\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-4:] == \"abil\":\n                        if \"abil\" in r2:\n                            word = word[:-4]\n                        else:\n                            word = \"\".join((word[:-2], \"l\"))\n\n                    elif word[-2:] == \"ic\":\n                        if \"ic\" in r2:\n                            word = word[:-2]\n                        else:\n                            word = \"\".join((word[:-2], \"iqU\"))\n\n                    elif word[-2:] == \"iv\":\n                        if \"iv\" in r2:\n                            word = word[:-2]\n\n                elif (suffix in (\"if\", \"ive\", \"ifs\", \"ives\") and\n                      suffix in r2):\n                    word = word[:-len(suffix)]\n                    step1_success = True\n\n                    if word[-2:] == \"at\" and \"at\" in r2:\n                        word = word[:-2]\n\n                        if word[-2:] == \"ic\":\n                            if \"ic\" in r2:\n                                word = word[:-2]\n                            else:\n                                word = \"\".join((word[:-2], \"iqU\"))\n                break\n\n        # STEP 2a: Verb suffixes beginning 'i'\n        if not step1_success or rv_ending_found:\n            for suffix in self.__step2a_suffixes:\n                if word.endswith(suffix):\n                    if (suffix in rv and len(rv) > len(suffix) and\n                        rv[rv.rindex(suffix)-1] not in self.__vowels):\n                        word = word[:-len(suffix)]\n                        step2a_success = True\n                    break\n\n        # STEP 2b: Other verb suffixes\n            if not step2a_success:\n                for suffix in self.__step2b_suffixes:\n                    if rv.endswith(suffix):\n                        if suffix == \"ions\" and \"ions\" in r2:\n                            word = word[:-4]\n                            step2b_success = True\n\n                        elif suffix in ('eraIent', 'erions', '\\xE8rent',\n                                        'erais', 'erait', 'eriez',\n                                        'erons', 'eront', 'erai', 'eras',\n                                        'erez', '\\xE9es', 'era', 'iez',\n                                        '\\xE9e', '\\xE9s', 'er', 'ez',\n                                        '\\xE9'):\n                            word = word[:-len(suffix)]\n                            step2b_success = True\n\n                        elif suffix in ('assions', 'assent', 'assiez',\n                                        'aIent', 'antes', 'asses',\n                                        '\\xE2mes', '\\xE2tes', 'ante',\n                                        'ants', 'asse', 'ais', 'ait',\n                                        'ant', '\\xE2t', 'ai', 'as',\n                                        'a'):\n                            word = word[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                            step2b_success = True\n                            if rv.endswith(\"e\"):\n                                word = word[:-1]\n                        break\n\n        # STEP 3\n        if step1_success or step2a_success or step2b_success:\n            if word[-1] == \"Y\":\n                word = \"\".join((word[:-1], \"i\"))\n            elif word[-1] == \"\\xE7\":\n                word = \"\".join((word[:-1], \"c\"))\n\n        # STEP 4: Residual suffixes\n        else:\n            if (len(word) >= 2 and word[-1] == \"s\" and\n                word[-2] not in \"aiou\\xE8s\"):\n                word = word[:-1]\n\n            for suffix in self.__step4_suffixes:\n                if word.endswith(suffix):\n                    if suffix in rv:\n                        if (suffix == \"ion\" and suffix in r2 and\n                            rv[-4] in \"st\"):\n                            word = word[:-3]\n\n                        elif suffix in (\"ier\", \"i\\xE8re\", \"Ier\",\n                                        \"I\\xE8re\"):\n                            word = \"\".join((word[:-len(suffix)], \"i\"))\n\n                        elif suffix == \"e\":\n                            word = word[:-1]\n\n                        elif suffix == \"\\xEB\" and word[-3:-1] == \"gu\":\n                            word = word[:-1]\n                        break\n\n        # STEP 5: Undouble\n        if word.endswith((\"enn\", \"onn\", \"ett\", \"ell\", \"eill\")):\n            word = word[:-1]\n\n        # STEP 6: Un-accent\n        for i in range(1, len(word)):\n            if word[-i] not in self.__vowels:\n                i += 1\n            else:\n                if i != 1 and word[-i] in (\"\\xE9\", \"\\xE8\"):\n                    word = \"\".join((word[:-i], \"e\", word[-i+1:]))\n                break\n\n        word = (word.replace(\"I\", \"i\")\n                    .replace(\"U\", \"u\")\n                    .replace(\"Y\", \"y\"))\n\n\n        return word", "response": "This function stems a word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __rv_french(self, word, vowels):\n        rv = \"\"\n        if len(word) >= 2:\n            if (word.startswith((\"par\", \"col\", \"tap\")) or\n                (word[0] in vowels and word[1] in vowels)):\n                rv = word[3:]\n            else:\n                for i in range(1, len(word)):\n                    if word[i] in vowels:\n                        rv = word[i+1:]\n                        break\n\n        return rv", "response": "This method returns the RV that is used by the French stemmer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstem a German word and return the stemmed form.", "response": "def stem(self, word):\n        \"\"\"\n        Stem a German word and return the stemmed form.\n\n        :param word: The word that is stemmed.\n        :type word: str or unicode\n        :return: The stemmed form.\n        :rtype: unicode\n\n        \"\"\"\n        word = word.lower()\n\n        word = word.replace(\"\\xDF\", \"ss\")\n\n        # Every occurrence of 'u' and 'y'\n        # between vowels is put into upper case.\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word[i] == \"y\":\n                    word = \"\".join((word[:i], \"Y\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n\n        # R1 is adjusted so that the region before it\n        # contains at least 3 letters.\n        for i in range(1, len(word)):\n            if word[i] not in self.__vowels and word[i-1] in self.__vowels:\n                if len(word[:i+1]) < 3 and len(word[:i+1]) > 0:\n                    r1 = word[3:]\n                elif len(word[:i+1]) == 0:\n                    return word\n                break\n\n        # STEP 1\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if (suffix in (\"en\", \"es\", \"e\") and\n                    word[-len(suffix)-4:-len(suffix)] == \"niss\"):\n                    word = word[:-len(suffix)-1]\n                    r1 = r1[:-len(suffix)-1]\n                    r2 = r2[:-len(suffix)-1]\n\n                elif suffix == \"s\":\n                    if word[-2] in self.__s_ending:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                        r2 = r2[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        # STEP 2\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"st\":\n                    if word[-3] in self.__st_ending and len(word[:-3]) >= 3:\n                        word = word[:-2]\n                        r1 = r1[:-2]\n                        r2 = r2[:-2]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                break\n\n        # STEP 3: Derivational suffixes\n        for suffix in self.__step3_suffixes:\n            if r2.endswith(suffix):\n                if suffix in (\"end\", \"ung\"):\n                    if (\"ig\" in r2[-len(suffix)-2:-len(suffix)] and\n                        \"e\" not in r2[-len(suffix)-3:-len(suffix)-2]):\n                        word = word[:-len(suffix)-2]\n                    else:\n                        word = word[:-len(suffix)]\n\n                elif (suffix in (\"ig\", \"ik\", \"isch\") and\n                      \"e\" not in r2[-len(suffix)-1:-len(suffix)]):\n                    word = word[:-len(suffix)]\n\n                elif suffix in (\"lich\", \"heit\"):\n                    if (\"er\" in r1[-len(suffix)-2:-len(suffix)] or\n                        \"en\" in r1[-len(suffix)-2:-len(suffix)]):\n                        word = word[:-len(suffix)-2]\n                    else:\n                        word = word[:-len(suffix)]\n\n                elif suffix == \"keit\":\n                    if \"lich\" in r2[-len(suffix)-4:-len(suffix)]:\n                        word = word[:-len(suffix)-4]\n\n                    elif \"ig\" in r2[-len(suffix)-2:-len(suffix)]:\n                        word = word[:-len(suffix)-2]\n                    else:\n                        word = word[:-len(suffix)]\n                break\n\n        # Umlaut accents are removed and\n        # 'u' and 'y' are put back into lower case.\n        word = (word.replace(\"\\xE4\", \"a\").replace(\"\\xF6\", \"o\")\n                    .replace(\"\\xFC\", \"u\").replace(\"U\", \"u\")\n                    .replace(\"Y\", \"y\"))\n\n\n        return word"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstem an Hungarian word and return the stemmed form. :param word: The word that is stemmed. :type word: str or unicode :return: The stemmed form. :rtype: unicode", "response": "def stem(self, word):\n        \"\"\"\n        Stem an Hungarian word and return the stemmed form.\n\n        :param word: The word that is stemmed.\n        :type word: str or unicode\n        :return: The stemmed form.\n        :rtype: unicode\n\n        \"\"\"\n        word = word.lower()\n\n        r1 = self.__r1_hungarian(word, self.__vowels, self.__digraphs)\n\n        # STEP 1: Remove instrumental case\n        if r1.endswith(self.__step1_suffixes):\n            for double_cons in self.__double_consonants:\n                if word[-2-len(double_cons):-2] == double_cons:\n                    word = \"\".join((word[:-4], word[-3]))\n\n                    if r1[-2-len(double_cons):-2] == double_cons:\n                        r1 = \"\".join((r1[:-4], r1[-3]))\n                    break\n\n        # STEP 2: Remove frequent cases\n        for suffix in self.__step2_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n\n                    if r1.endswith(\"\\xE1\"):\n                        word = \"\".join((word[:-1], \"a\"))\n                        r1 = \"\".join((r1[:-1], \"a\"))\n\n                    elif r1.endswith(\"\\xE9\"):\n                        word = \"\".join((word[:-1], \"e\"))\n                        r1 = \"\".join((r1[:-1], \"e\"))\n                break\n\n        # STEP 3: Remove special cases\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"\\xE9n\":\n                    word = \"\".join((word[:-2], \"e\"))\n                    r1 = \"\".join((r1[:-2], \"e\"))\n                else:\n                    word = \"\".join((word[:-len(suffix)], \"a\"))\n                    r1 = \"\".join((r1[:-len(suffix)], \"a\"))\n                break\n\n        # STEP 4: Remove other cases\n        for suffix in self.__step4_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"\\xE1stul\":\n                    word = \"\".join((word[:-5], \"a\"))\n                    r1 = \"\".join((r1[:-5], \"a\"))\n\n                elif suffix == \"\\xE9st\\xFCl\":\n                    word = \"\".join((word[:-5], \"e\"))\n                    r1 = \"\".join((r1[:-5], \"e\"))\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        # STEP 5: Remove factive case\n        for suffix in self.__step5_suffixes:\n            if r1.endswith(suffix):\n                for double_cons in self.__double_consonants:\n                    if word[-1-len(double_cons):-1] == double_cons:\n                        word = \"\".join((word[:-3], word[-2]))\n\n                        if r1[-1-len(double_cons):-1] == double_cons:\n                            r1 = \"\".join((r1[:-3], r1[-2]))\n                        break\n\n        # STEP 6: Remove owned\n        for suffix in self.__step6_suffixes:\n            if r1.endswith(suffix):\n                if suffix in (\"\\xE1k\\xE9\", \"\\xE1\\xE9i\"):\n                    word = \"\".join((word[:-3], \"a\"))\n                    r1 = \"\".join((r1[:-3], \"a\"))\n\n                elif suffix in (\"\\xE9k\\xE9\", \"\\xE9\\xE9i\",\n                                \"\\xE9\\xE9\"):\n                    word = \"\".join((word[:-len(suffix)], \"e\"))\n                    r1 = \"\".join((r1[:-len(suffix)], \"e\"))\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        # STEP 7: Remove singular owner suffixes\n        for suffix in self.__step7_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix in (\"\\xE1nk\", \"\\xE1juk\", \"\\xE1m\",\n                                  \"\\xE1d\", \"\\xE1\"):\n                        word = \"\".join((word[:-len(suffix)], \"a\"))\n                        r1 = \"\".join((r1[:-len(suffix)], \"a\"))\n\n                    elif suffix in (\"\\xE9nk\", \"\\xE9j\\xFCk\",\n                                    \"\\xE9m\", \"\\xE9d\", \"\\xE9\"):\n                        word = \"\".join((word[:-len(suffix)], \"e\"))\n                        r1 = \"\".join((r1[:-len(suffix)], \"e\"))\n                    else:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                break\n\n        # STEP 8: Remove plural owner suffixes\n        for suffix in self.__step8_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix in (\"\\xE1im\", \"\\xE1id\", \"\\xE1i\",\n                                  \"\\xE1ink\", \"\\xE1itok\", \"\\xE1ik\"):\n                        word = \"\".join((word[:-len(suffix)], \"a\"))\n                        r1 = \"\".join((r1[:-len(suffix)], \"a\"))\n\n                    elif suffix in (\"\\xE9im\", \"\\xE9id\", \"\\xE9i\",\n                                    \"\\xE9ink\", \"\\xE9itek\", \"\\xE9ik\"):\n                        word = \"\".join((word[:-len(suffix)], \"e\"))\n                        r1 = \"\".join((r1[:-len(suffix)], \"e\"))\n                    else:\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                break\n\n        # STEP 9: Remove plural suffixes\n        for suffix in self.__step9_suffixes:\n            if word.endswith(suffix):\n                if r1.endswith(suffix):\n                    if suffix == \"\\xE1k\":\n                        word = \"\".join((word[:-2], \"a\"))\n                    elif suffix == \"\\xE9k\":\n                        word = \"\".join((word[:-2], \"e\"))\n                    else:\n                        word = word[:-len(suffix)]\n                break\n\n\n        return word"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the region R1 that is used by the Hungarian stemmer. If the word begins with a vowel, R1 is defined as the region after the first consonant or digraph (= two letters stand for one phoneme) in the word. If the word begins with a consonant, it is defined as the region after the first vowel in the word. If the word does not contain both a vowel and consonant, R1 is the null region at the end of the word. :param word: The Hungarian word whose region R1 is determined. :type word: str or unicode :param vowels: The Hungarian vowels that are used to determine the region R1. :type vowels: unicode :param digraphs: The digraphs that are used to determine the region R1. :type digraphs: tuple :return: the region R1 for the respective word. :rtype: unicode :note: This helper method is invoked by the stem method of the subclass HungarianStemmer. It is not to be invoked directly!", "response": "def __r1_hungarian(self, word, vowels, digraphs):\n        \"\"\"\n        Return the region R1 that is used by the Hungarian stemmer.\n\n        If the word begins with a vowel, R1 is defined as the region\n        after the first consonant or digraph (= two letters stand for\n        one phoneme) in the word. If the word begins with a consonant,\n        it is defined as the region after the first vowel in the word.\n        If the word does not contain both a vowel and consonant, R1\n        is the null region at the end of the word.\n\n        :param word: The Hungarian word whose region R1 is determined.\n        :type word: str or unicode\n        :param vowels: The Hungarian vowels that are used to determine\n                       the region R1.\n        :type vowels: unicode\n        :param digraphs: The digraphs that are used to determine the\n                         region R1.\n        :type digraphs: tuple\n        :return: the region R1 for the respective word.\n        :rtype: unicode\n        :note: This helper method is invoked by the stem method of the subclass\n               HungarianStemmer. It is not to be invoked directly!\n\n        \"\"\"\n        r1 = \"\"\n        if word[0] in vowels:\n            for digraph in digraphs:\n                if digraph in word[1:]:\n                    r1 = word[word.index(digraph[-1])+1:]\n                    return r1\n\n            for i in range(1, len(word)):\n                if word[i] not in vowels:\n                    r1 = word[i+1:]\n                    break\n        else:\n            for i in range(1, len(word)):\n                if word[i] in vowels:\n                    r1 = word[i+1:]\n                    break\n\n        return r1"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stem(self, word):\n        word = word.lower()\n\n        step1_success = False\n\n        # All acute accents are replaced by grave accents.\n        word = (word.replace(\"\\xE1\", \"\\xE0\")\n                    .replace(\"\\xE9\", \"\\xE8\")\n                    .replace(\"\\xED\", \"\\xEC\")\n                    .replace(\"\\xF3\", \"\\xF2\")\n                    .replace(\"\\xFA\", \"\\xF9\"))\n\n        # Every occurrence of 'u' after 'q'\n        # is put into upper case.\n        for i in range(1, len(word)):\n            if word[i-1] == \"q\" and word[i] == \"u\":\n                word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n        # Every occurrence of 'u' and 'i'\n        # between vowels is put into upper case.\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word [i] == \"i\":\n                    word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self._rv_standard(word, self.__vowels)\n\n        # STEP 0: Attached pronoun\n        for suffix in self.__step0_suffixes:\n            if rv.endswith(suffix):\n                if rv[-len(suffix)-4:-len(suffix)] in (\"ando\", \"endo\"):\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n\n                elif (rv[-len(suffix)-2:-len(suffix)] in\n                      (\"ar\", \"er\", \"ir\")):\n                    word = \"\".join((word[:-len(suffix)], \"e\"))\n                    r1 = \"\".join((r1[:-len(suffix)], \"e\"))\n                    r2 = \"\".join((r2[:-len(suffix)], \"e\"))\n                    rv = \"\".join((rv[:-len(suffix)], \"e\"))\n                break\n\n        # STEP 1: Standard suffix removal\n        for suffix in self.__step1_suffixes:\n            if word.endswith(suffix):\n                if suffix == \"amente\" and r1.endswith(suffix):\n                    step1_success = True\n                    word = word[:-6]\n                    r2 = r2[:-6]\n                    rv = rv[:-6]\n\n                    if r2.endswith(\"iv\"):\n                        word = word[:-2]\n                        r2 = r2[:-2]\n                        rv = rv[:-2]\n\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                    elif r2.endswith((\"os\", \"ic\")):\n                        word = word[:-2]\n                        rv = rv[:-2]\n\n                    elif r2 .endswith(\"abil\"):\n                        word = word[:-4]\n                        rv = rv[:-4]\n\n                elif (suffix in (\"amento\", \"amenti\",\n                                 \"imento\", \"imenti\") and\n                      rv.endswith(suffix)):\n                    step1_success = True\n                    word = word[:-6]\n                    rv = rv[:-6]\n\n                elif r2.endswith(suffix):\n                    step1_success = True\n                    if suffix in (\"azione\", \"azioni\", \"atore\", \"atori\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                        if r2.endswith(\"ic\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                    elif suffix in (\"logia\", \"logie\"):\n                        word = word[:-2]\n                        rv = word[:-2]\n\n                    elif suffix in (\"uzione\", \"uzioni\",\n                                    \"usione\", \"usioni\"):\n                        word = word[:-5]\n                        rv = rv[:-5]\n\n                    elif suffix in (\"enza\", \"enze\"):\n                        word = \"\".join((word[:-2], \"te\"))\n                        rv = \"\".join((rv[:-2], \"te\"))\n\n                    elif suffix == \"it\\xE0\":\n                        word = word[:-3]\n                        r2 = r2[:-3]\n                        rv = rv[:-3]\n\n                        if r2.endswith((\"ic\", \"iv\")):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                        elif r2.endswith(\"abil\"):\n                            word = word[:-4]\n                            rv = rv[:-4]\n\n                    elif suffix in (\"ivo\", \"ivi\", \"iva\", \"ive\"):\n                        word = word[:-3]\n                        r2 = r2[:-3]\n                        rv = rv[:-3]\n\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            r2 = r2[:-2]\n                            rv = rv[:-2]\n\n                            if r2.endswith(\"ic\"):\n                                word = word[:-2]\n                                rv = rv[:-2]\n                    else:\n                        word = word[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                break\n\n        # STEP 2: Verb suffixes\n        if not step1_success:\n            for suffix in self.__step2_suffixes:\n                if rv.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n        # STEP 3a\n        if rv.endswith((\"a\", \"e\", \"i\", \"o\", \"\\xE0\", \"\\xE8\",\n                        \"\\xEC\", \"\\xF2\")):\n            word = word[:-1]\n            rv = rv[:-1]\n\n            if rv.endswith(\"i\"):\n                word = word[:-1]\n                rv = rv[:-1]\n\n        # STEP 3b\n        if rv.endswith((\"ch\", \"gh\")):\n            word = word[:-1]\n\n        word = word.replace(\"I\", \"i\").replace(\"U\", \"u\")\n\n\n        return word", "response": "This method stems an Italian word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stem(self, word):\n        word = word.lower()\n\n        step1_success = False\n        step2_success = False\n\n        for i in range(1, len(word)-1):\n            if word[i-1] in self.__vowels and word[i+1] in self.__vowels:\n                if word[i] == \"u\":\n                    word = \"\".join((word[:i], \"U\", word[i+1:]))\n\n                elif word[i] == \"i\":\n                    word = \"\".join((word[:i], \"I\", word[i+1:]))\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self._rv_standard(word, self.__vowels)\n\n        # STEP 0: Removal of plurals and other simplifications\n        for suffix in self.__step0_suffixes:\n            if word.endswith(suffix):\n                if suffix in r1:\n                    if suffix in (\"ul\", \"ului\"):\n                        word = word[:-len(suffix)]\n\n                        if suffix in rv:\n                            rv = rv[:-len(suffix)]\n                        else:\n                            rv = \"\"\n\n                    elif (suffix == \"aua\" or suffix == \"atei\" or\n                          (suffix == \"ile\" and word[-5:-3] != \"ab\")):\n                        word = word[:-2]\n\n                    elif suffix in (\"ea\", \"ele\", \"elor\"):\n                        word = \"\".join((word[:-len(suffix)], \"e\"))\n\n                        if suffix in rv:\n                            rv = \"\".join((rv[:-len(suffix)], \"e\"))\n                        else:\n                            rv = \"\"\n\n                    elif suffix in (\"ii\", \"iua\", \"iei\",\n                                    \"iile\", \"iilor\", \"ilor\"):\n                        word = \"\".join((word[:-len(suffix)], \"i\"))\n\n                        if suffix in rv:\n                            rv = \"\".join((rv[:-len(suffix)], \"i\"))\n                        else:\n                            rv = \"\"\n\n                    elif suffix in (\"a\\u0163ie\", \"a\\u0163ia\"):\n                        word = word[:-1]\n                break\n\n        # STEP 1: Reduction of combining suffixes\n        while True:\n\n            replacement_done = False\n\n            for suffix in self.__step1_suffixes:\n                if word.endswith(suffix):\n                    if suffix in r1:\n                        step1_success = True\n                        replacement_done = True\n\n                        if suffix in (\"abilitate\", \"abilitati\",\n                                      \"abilit\\u0103i\",\n                                      \"abilit\\u0103\\u0163i\"):\n                            word = \"\".join((word[:-len(suffix)], \"abil\"))\n\n                        elif suffix == \"ibilitate\":\n                            word = word[:-5]\n\n                        elif suffix in (\"ivitate\", \"ivitati\",\n                                        \"ivit\\u0103i\",\n                                        \"ivit\\u0103\\u0163i\"):\n                            word = \"\".join((word[:-len(suffix)], \"iv\"))\n\n                        elif suffix in (\"icitate\", \"icitati\", \"icit\\u0103i\",\n                                        \"icit\\u0103\\u0163i\", \"icator\",\n                                        \"icatori\", \"iciv\", \"iciva\",\n                                        \"icive\", \"icivi\", \"iciv\\u0103\",\n                                        \"ical\", \"icala\", \"icale\", \"icali\",\n                                        \"ical\\u0103\"):\n                            word = \"\".join((word[:-len(suffix)], \"ic\"))\n\n                        elif suffix in (\"ativ\", \"ativa\", \"ative\", \"ativi\",\n                                        \"ativ\\u0103\", \"a\\u0163iune\",\n                                        \"atoare\", \"ator\", \"atori\",\n                                        \"\\u0103toare\",\n                                        \"\\u0103tor\", \"\\u0103tori\"):\n                            word = \"\".join((word[:-len(suffix)], \"at\"))\n\n                            if suffix in r2:\n                                r2 = \"\".join((r2[:-len(suffix)], \"at\"))\n\n                        elif suffix in (\"itiv\", \"itiva\", \"itive\", \"itivi\",\n                                        \"itiv\\u0103\", \"i\\u0163iune\",\n                                        \"itoare\", \"itor\", \"itori\"):\n                            word = \"\".join((word[:-len(suffix)], \"it\"))\n\n                            if suffix in r2:\n                                r2 = \"\".join((r2[:-len(suffix)], \"it\"))\n                    else:\n                        step1_success = False\n                    break\n\n            if not replacement_done:\n                break\n\n        # STEP 2: Removal of standard suffixes\n        for suffix in self.__step2_suffixes:\n            if word.endswith(suffix):\n                if suffix in r2:\n                    step2_success = True\n\n                    if suffix in (\"iune\", \"iuni\"):\n                        if word[-5] == \"\\u0163\":\n                            word = \"\".join((word[:-5], \"t\"))\n\n                    elif suffix in (\"ism\", \"isme\", \"ist\", \"ista\", \"iste\",\n                                    \"isti\", \"ist\\u0103\", \"i\\u015Fti\"):\n                        word = \"\".join((word[:-len(suffix)], \"ist\"))\n\n                    else:\n                        word = word[:-len(suffix)]\n                break\n\n        # STEP 3: Removal of verb suffixes\n        if not step1_success and not step2_success:\n            for suffix in self.__step3_suffixes:\n                if word.endswith(suffix):\n                    if suffix in rv:\n                        if suffix in ('seser\\u0103\\u0163i', 'seser\\u0103m',\n                                      'ser\\u0103\\u0163i', 'sese\\u015Fi',\n                                      'seser\\u0103', 'ser\\u0103m', 'sesem',\n                                      'se\\u015Fi', 'ser\\u0103', 'sese',\n                                      'a\\u0163i', 'e\\u0163i', 'i\\u0163i',\n                                      '\\xE2\\u0163i', 'sei', '\\u0103m',\n                                      'em', 'im', '\\xE2m', 'se'):\n                            word = word[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                        else:\n                            if (not rv.startswith(suffix) and\n                                rv[rv.index(suffix)-1] not in\n                                \"aeio\\u0103\\xE2\\xEE\"):\n                                word = word[:-len(suffix)]\n                        break\n\n        # STEP 4: Removal of final vowel\n        for suffix in (\"ie\", \"a\", \"e\", \"i\", \"\\u0103\"):\n            if word.endswith(suffix):\n                if suffix in rv:\n                    word = word[:-len(suffix)]\n                break\n\n        word = word.replace(\"I\", \"i\").replace(\"U\", \"u\")\n\n\n        return word", "response": "This function stems a Romanian word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stem(self, word):\n        chr_exceeded = False\n        for i in range(len(word)):\n            if ord(word[i]) > 255:\n                chr_exceeded = True\n                break\n\n        if chr_exceeded:\n            word = self.__cyrillic_to_roman(word)\n\n        step1_success = False\n        adjectival_removed = False\n        verb_removed = False\n        undouble_success = False\n        superlative_removed = False\n\n        rv, r2 = self.__regions_russian(word)\n\n        # Step 1\n        for suffix in self.__perfective_gerund_suffixes:\n            if rv.endswith(suffix):\n                if suffix in (\"v\", \"vshi\", \"vshis'\"):\n                    if (rv[-len(suffix)-3:-len(suffix)] == \"i^a\" or\n                        rv[-len(suffix)-1:-len(suffix)] == \"a\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                        step1_success = True\n                        break\n                else:\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    step1_success = True\n                    break\n\n        if not step1_success:\n            for suffix in self.__reflexive_suffixes:\n                if rv.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    r2 = r2[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n            for suffix in self.__adjectival_suffixes:\n                if rv.endswith(suffix):\n                    if suffix in ('i^ushchi^ui^u', 'i^ushchi^ai^a',\n                              'i^ushchui^u', 'i^ushchai^a', 'i^ushchoi^u',\n                              'i^ushchei^u', 'i^ushchimi', 'i^ushchymi',\n                              'i^ushchego', 'i^ushchogo', 'i^ushchemu',\n                              'i^ushchomu', 'i^ushchikh', 'i^ushchykh',\n                              'shchi^ui^u', 'shchi^ai^a', 'i^ushchee',\n                              'i^ushchie', 'i^ushchye', 'i^ushchoe',\n                              'i^ushchei`', 'i^ushchii`', 'i^ushchyi`',\n                              'i^ushchoi`', 'i^ushchem', 'i^ushchim',\n                              'i^ushchym', 'i^ushchom', 'vshi^ui^u',\n                              'vshi^ai^a', 'shchui^u', 'shchai^a',\n                              'shchoi^u', 'shchei^u', 'emi^ui^u',\n                              'emi^ai^a', 'nni^ui^u', 'nni^ai^a',\n                              'shchimi', 'shchymi', 'shchego', 'shchogo',\n                              'shchemu', 'shchomu', 'shchikh', 'shchykh',\n                              'vshui^u', 'vshai^a', 'vshoi^u', 'vshei^u',\n                              'shchee', 'shchie', 'shchye', 'shchoe',\n                              'shchei`', 'shchii`', 'shchyi`', 'shchoi`',\n                              'shchem', 'shchim', 'shchym', 'shchom',\n                              'vshimi', 'vshymi', 'vshego', 'vshogo',\n                              'vshemu', 'vshomu', 'vshikh', 'vshykh',\n                              'emui^u', 'emai^a', 'emoi^u', 'emei^u',\n                              'nnui^u', 'nnai^a', 'nnoi^u', 'nnei^u',\n                              'vshee', 'vshie', 'vshye', 'vshoe',\n                              'vshei`', 'vshii`', 'vshyi`', 'vshoi`',\n                              'vshem', 'vshim', 'vshym', 'vshom',\n                              'emimi', 'emymi', 'emego', 'emogo',\n                              'ememu', 'emomu', 'emikh', 'emykh',\n                              'nnimi', 'nnymi', 'nnego', 'nnogo',\n                              'nnemu', 'nnomu', 'nnikh', 'nnykh',\n                              'emee', 'emie', 'emye', 'emoe', 'emei`',\n                              'emii`', 'emyi`', 'emoi`', 'emem', 'emim',\n                              'emym', 'emom', 'nnee', 'nnie', 'nnye',\n                              'nnoe', 'nnei`', 'nnii`', 'nnyi`', 'nnoi`',\n                              'nnem', 'nnim', 'nnym', 'nnom'):\n                        if (rv[-len(suffix)-3:-len(suffix)] == \"i^a\" or\n                            rv[-len(suffix)-1:-len(suffix)] == \"a\"):\n                            word = word[:-len(suffix)]\n                            r2 = r2[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                            adjectival_removed = True\n                            break\n                    else:\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                        adjectival_removed = True\n                        break\n\n            if not adjectival_removed:\n                for suffix in self.__verb_suffixes:\n                    if rv.endswith(suffix):\n                        if suffix in (\"la\", \"na\", \"ete\", \"i`te\", \"li\",\n                                      \"i`\", \"l\", \"em\", \"n\", \"lo\", \"no\",\n                                      \"et\", \"i^ut\", \"ny\", \"t'\", \"esh'\",\n                                      \"nno\"):\n                            if (rv[-len(suffix)-3:-len(suffix)] == \"i^a\" or\n                                rv[-len(suffix)-1:-len(suffix)] == \"a\"):\n                                word = word[:-len(suffix)]\n                                r2 = r2[:-len(suffix)]\n                                rv = rv[:-len(suffix)]\n                                verb_removed = True\n                                break\n                        else:\n                            word = word[:-len(suffix)]\n                            r2 = r2[:-len(suffix)]\n                            rv = rv[:-len(suffix)]\n                            verb_removed = True\n                            break\n\n            if not adjectival_removed and not verb_removed:\n                for suffix in self.__noun_suffixes:\n                    if rv.endswith(suffix):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                        break\n\n        # Step 2\n        if rv.endswith(\"i\"):\n            word = word[:-1]\n            r2 = r2[:-1]\n\n        # Step 3\n        for suffix in self.__derivational_suffixes:\n            if r2.endswith(suffix):\n                word = word[:-len(suffix)]\n                break\n\n        # Step 4\n        if word.endswith(\"nn\"):\n            word = word[:-1]\n            undouble_success = True\n\n        if not undouble_success:\n            for suffix in self.__superlative_suffixes:\n                if word.endswith(suffix):\n                    word = word[:-len(suffix)]\n                    superlative_removed = True\n                    break\n            if word.endswith(\"nn\"):\n                word = word[:-1]\n\n        if not undouble_success and not superlative_removed:\n            if word.endswith(\"'\"):\n                word = word[:-1]\n\n        if chr_exceeded:\n            word = self.__roman_to_cyrillic(word)\n\n\n        return word", "response": "This method stems a word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __regions_russian(self, word):\n        r1 = \"\"\n        r2 = \"\"\n        rv = \"\"\n\n        vowels = (\"A\", \"U\", \"E\", \"a\", \"e\", \"i\", \"o\", \"u\", \"y\")\n        word = (word.replace(\"i^a\", \"A\")\n                    .replace(\"i^u\", \"U\")\n                    .replace(\"e`\", \"E\"))\n\n        for i in range(1, len(word)):\n            if word[i] not in vowels and word[i-1] in vowels:\n                r1 = word[i+1:]\n                break\n\n        for i in range(1, len(r1)):\n            if r1[i] not in vowels and r1[i-1] in vowels:\n                r2 = r1[i+1:]\n                break\n\n        for i in range(len(word)):\n            if word[i] in vowels:\n                rv = word[i+1:]\n                break\n\n        r2 = (r2.replace(\"A\", \"i^a\")\n                .replace(\"U\", \"i^u\")\n                .replace(\"E\", \"e`\"))\n        rv = (rv.replace(\"A\", \"i^a\")\n              .replace(\"U\", \"i^u\")\n              .replace(\"E\", \"e`\"))\n\n\n        return (rv, r2)", "response": "This method returns the RV and R2 which are used by the Russian stemmer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef __cyrillic_to_roman(self, word):\n        word = (word.replace(\"\\u0410\", \"a\").replace(\"\\u0430\", \"a\")\n                    .replace(\"\\u0411\", \"b\").replace(\"\\u0431\", \"b\")\n                    .replace(\"\\u0412\", \"v\").replace(\"\\u0432\", \"v\")\n                    .replace(\"\\u0413\", \"g\").replace(\"\\u0433\", \"g\")\n                    .replace(\"\\u0414\", \"d\").replace(\"\\u0434\", \"d\")\n                    .replace(\"\\u0415\", \"e\").replace(\"\\u0435\", \"e\")\n                    .replace(\"\\u0401\", \"e\").replace(\"\\u0451\", \"e\")\n                    .replace(\"\\u0416\", \"zh\").replace(\"\\u0436\", \"zh\")\n                    .replace(\"\\u0417\", \"z\").replace(\"\\u0437\", \"z\")\n                    .replace(\"\\u0418\", \"i\").replace(\"\\u0438\", \"i\")\n                    .replace(\"\\u0419\", \"i`\").replace(\"\\u0439\", \"i`\")\n                    .replace(\"\\u041A\", \"k\").replace(\"\\u043A\", \"k\")\n                    .replace(\"\\u041B\", \"l\").replace(\"\\u043B\", \"l\")\n                    .replace(\"\\u041C\", \"m\").replace(\"\\u043C\", \"m\")\n                    .replace(\"\\u041D\", \"n\").replace(\"\\u043D\", \"n\")\n                    .replace(\"\\u041E\", \"o\").replace(\"\\u043E\", \"o\")\n                    .replace(\"\\u041F\", \"p\").replace(\"\\u043F\", \"p\")\n                    .replace(\"\\u0420\", \"r\").replace(\"\\u0440\", \"r\")\n                    .replace(\"\\u0421\", \"s\").replace(\"\\u0441\", \"s\")\n                    .replace(\"\\u0422\", \"t\").replace(\"\\u0442\", \"t\")\n                    .replace(\"\\u0423\", \"u\").replace(\"\\u0443\", \"u\")\n                    .replace(\"\\u0424\", \"f\").replace(\"\\u0444\", \"f\")\n                    .replace(\"\\u0425\", \"kh\").replace(\"\\u0445\", \"kh\")\n                    .replace(\"\\u0426\", \"t^s\").replace(\"\\u0446\", \"t^s\")\n                    .replace(\"\\u0427\", \"ch\").replace(\"\\u0447\", \"ch\")\n                    .replace(\"\\u0428\", \"sh\").replace(\"\\u0448\", \"sh\")\n                    .replace(\"\\u0429\", \"shch\").replace(\"\\u0449\", \"shch\")\n                    .replace(\"\\u042A\", \"''\").replace(\"\\u044A\", \"''\")\n                    .replace(\"\\u042B\", \"y\").replace(\"\\u044B\", \"y\")\n                    .replace(\"\\u042C\", \"'\").replace(\"\\u044C\", \"'\")\n                    .replace(\"\\u042D\", \"e`\").replace(\"\\u044D\", \"e`\")\n                    .replace(\"\\u042E\", \"i^u\").replace(\"\\u044E\", \"i^u\")\n                    .replace(\"\\u042F\", \"i^a\").replace(\"\\u044F\", \"i^a\"))\n\n\n        return word", "response": "Transliterate a word into the Cyrillic alphabet and return the word."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef __roman_to_cyrillic(self, word):\n        word = (word.replace(\"i^u\", \"\\u044E\").replace(\"i^a\", \"\\u044F\")\n                    .replace(\"shch\", \"\\u0449\").replace(\"kh\", \"\\u0445\")\n                    .replace(\"t^s\", \"\\u0446\").replace(\"ch\", \"\\u0447\")\n                    .replace(\"e`\", \"\\u044D\").replace(\"i`\", \"\\u0439\")\n                    .replace(\"sh\", \"\\u0448\").replace(\"k\", \"\\u043A\")\n                    .replace(\"e\", \"\\u0435\").replace(\"zh\", \"\\u0436\")\n                    .replace(\"a\", \"\\u0430\").replace(\"b\", \"\\u0431\")\n                    .replace(\"v\", \"\\u0432\").replace(\"g\", \"\\u0433\")\n                    .replace(\"d\", \"\\u0434\").replace(\"e\", \"\\u0435\")\n                    .replace(\"z\", \"\\u0437\").replace(\"i\", \"\\u0438\")\n                    .replace(\"l\", \"\\u043B\").replace(\"m\", \"\\u043C\")\n                    .replace(\"n\", \"\\u043D\").replace(\"o\", \"\\u043E\")\n                    .replace(\"p\", \"\\u043F\").replace(\"r\", \"\\u0440\")\n                    .replace(\"s\", \"\\u0441\").replace(\"t\", \"\\u0442\")\n                    .replace(\"u\", \"\\u0443\").replace(\"f\", \"\\u0444\")\n                    .replace(\"''\", \"\\u044A\").replace(\"y\", \"\\u044B\")\n                    .replace(\"'\", \"\\u044C\"))\n\n\n        return word", "response": "Transliterate a Russian word back into the Cyrillic alphabet."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stem(self, word):\n        word = word.lower()\n\n        step1_success = False\n\n        r1, r2 = self._r1r2_standard(word, self.__vowels)\n        rv = self._rv_standard(word, self.__vowels)\n\n        # STEP 0: Attached pronoun\n        for suffix in self.__step0_suffixes:\n            if word.endswith(suffix):\n                if rv.endswith(suffix):\n                    if rv[:-len(suffix)].endswith((\"i\\xE9ndo\",\n                                                   \"\\xE1ndo\",\n                                                   \"\\xE1r\", \"\\xE9r\",\n                                                   \"\\xEDr\")):\n                        word = (word[:-len(suffix)].replace(\"\\xE1\", \"a\")\n                                                   .replace(\"\\xE9\", \"e\")\n                                                   .replace(\"\\xED\", \"i\"))\n                        r1 = (r1[:-len(suffix)].replace(\"\\xE1\", \"a\")\n                                               .replace(\"\\xE9\", \"e\")\n                                               .replace(\"\\xED\", \"i\"))\n                        r2 = (r2[:-len(suffix)].replace(\"\\xE1\", \"a\")\n                                               .replace(\"\\xE9\", \"e\")\n                                               .replace(\"\\xED\", \"i\"))\n                        rv = (rv[:-len(suffix)].replace(\"\\xE1\", \"a\")\n                                               .replace(\"\\xE9\", \"e\")\n                                               .replace(\"\\xED\", \"i\"))\n\n                    elif rv[:-len(suffix)].endswith((\"ando\", \"iendo\",\n                                                     \"ar\", \"er\", \"ir\")):\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                    elif (rv[:-len(suffix)].endswith(\"yendo\") and\n                          word[:-len(suffix)].endswith(\"uyendo\")):\n                        word = word[:-len(suffix)]\n                        r1 = r1[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                break\n\n        # STEP 1: Standard suffix removal\n        for suffix in self.__step1_suffixes:\n            if word.endswith(suffix):\n                if suffix == \"amente\" and r1.endswith(suffix):\n                    step1_success = True\n                    word = word[:-6]\n                    r2 = r2[:-6]\n                    rv = rv[:-6]\n\n                    if r2.endswith(\"iv\"):\n                        word = word[:-2]\n                        r2 = r2[:-2]\n                        rv = rv[:-2]\n\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                    elif r2.endswith((\"os\", \"ic\", \"ad\")):\n                        word = word[:-2]\n                        rv = rv[:-2]\n\n                elif r2.endswith(suffix):\n                    step1_success = True\n                    if suffix in (\"adora\", \"ador\", \"aci\\xF3n\", \"adoras\",\n                                  \"adores\", \"aciones\", \"ante\", \"antes\",\n                                  \"ancia\", \"ancias\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                        if r2.endswith(\"ic\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n\n                    elif suffix in (\"log\\xEDa\", \"log\\xEDas\"):\n                        word = word.replace(suffix, \"log\")\n                        rv = rv.replace(suffix, \"log\")\n\n                    elif suffix in (\"uci\\xF3n\", \"uciones\"):\n                        word = word.replace(suffix, \"u\")\n                        rv = rv.replace(suffix, \"u\")\n\n                    elif suffix in (\"encia\", \"encias\"):\n                        word = word.replace(suffix, \"ente\")\n                        rv = rv.replace(suffix, \"ente\")\n\n                    elif suffix == \"mente\":\n                        word = word[:-5]\n                        r2 = r2[:-5]\n                        rv = rv[:-5]\n\n                        if r2.endswith((\"ante\", \"able\", \"ible\")):\n                            word = word[:-4]\n                            rv = rv[:-4]\n\n                    elif suffix in (\"idad\", \"idades\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                        for pre_suff in (\"abil\", \"ic\", \"iv\"):\n                            if r2.endswith(pre_suff):\n                                word = word[:-len(pre_suff)]\n                                rv = rv[:-len(pre_suff)]\n\n                    elif suffix in (\"ivo\", \"iva\", \"ivos\", \"ivas\"):\n                        word = word[:-len(suffix)]\n                        r2 = r2[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                        if r2.endswith(\"at\"):\n                            word = word[:-2]\n                            rv = rv[:-2]\n                    else:\n                        word = word[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                break\n\n        # STEP 2a: Verb suffixes beginning 'y'\n        if not step1_success:\n            for suffix in self.__step2a_suffixes:\n                if (rv.endswith(suffix) and\n                    word[-len(suffix)-1:-len(suffix)] == \"u\"):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n                    break\n\n        # STEP 2b: Other verb suffixes\n            for suffix in self.__step2b_suffixes:\n                if rv.endswith(suffix):\n                    if suffix in (\"en\", \"es\", \"\\xE9is\", \"emos\"):\n                        word = word[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n\n                        if word.endswith(\"gu\"):\n                            word = word[:-1]\n\n                        if rv.endswith(\"gu\"):\n                            rv = rv[:-1]\n                    else:\n                        word = word[:-len(suffix)]\n                        rv = rv[:-len(suffix)]\n                    break\n\n        # STEP 3: Residual suffix\n        for suffix in self.__step3_suffixes:\n            if rv.endswith(suffix):\n                if suffix in (\"e\", \"\\xE9\"):\n                    word = word[:-len(suffix)]\n                    rv = rv[:-len(suffix)]\n\n                    if word[-2:] == \"gu\" and rv[-1] == \"u\":\n                        word = word[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                break\n\n        word = (word.replace(\"\\xE1\", \"a\").replace(\"\\xE9\", \"e\")\n                    .replace(\"\\xED\", \"i\").replace(\"\\xF3\", \"o\")\n                    .replace(\"\\xFA\", \"u\"))\n\n\n        return word", "response": "This method stems a Spanish word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstems a Swedish word and return the stemmed form. :param word: The word that is stemmed. :type word: str or unicode :return: The stemmed form. :rtype: unicode", "response": "def stem(self, word):\n        \"\"\"\n        Stem a Swedish word and return the stemmed form.\n\n        :param word: The word that is stemmed.\n        :type word: str or unicode\n        :return: The stemmed form.\n        :rtype: unicode\n\n        \"\"\"\n        word = word.lower()\n\n        r1 = self._r1_scandinavian(word, self.__vowels)\n\n        # STEP 1\n        for suffix in self.__step1_suffixes:\n            if r1.endswith(suffix):\n                if suffix == \"s\":\n                    if word[-2] in self.__s_ending:\n                        word = word[:-1]\n                        r1 = r1[:-1]\n                else:\n                    word = word[:-len(suffix)]\n                    r1 = r1[:-len(suffix)]\n                break\n\n        # STEP 2\n        for suffix in self.__step2_suffixes:\n            if r1.endswith(suffix):\n                word = word[:-1]\n                r1 = r1[:-1]\n                break\n\n        # STEP 3\n        for suffix in self.__step3_suffixes:\n            if r1.endswith(suffix):\n                if suffix in (\"els\", \"lig\", \"ig\"):\n                    word = word[:-len(suffix)]\n                elif suffix in (\"fullt\", \"l\\xF6st\"):\n                    word = word[:-1]\n                break\n\n\n        return word"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnormalize the pre - processing of the token.", "response": "def __normalize_pre(self, token):\n        \"\"\"\n        :param token: string\n        :return: normalized token type string\n        \"\"\"\n        # strip diacritics\n        token = self.__vocalization.sub('', token)\n        # strip kasheeda\n        token = self.__kasheeda.sub('', token)\n        # strip punctuation marks\n        token = self.__arabic_punctuation_marks.sub('', token)\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef stem(self, word):\n        # set initial values\n        self.is_verb = True\n        self.is_noun = True\n        self.is_defined = False\n\n        self.suffix_verb_step2a_success = False\n        self.suffix_verb_step2b_success = False\n        self.suffix_noun_step2c2_success = False\n        self.suffix_noun_step1a_success = False\n        self.suffix_noun_step2a_success = False\n        self.suffix_noun_step2b_success = False\n        self.suffixe_noun_step1b_success = False\n        self.prefix_step2a_success = False\n        self.prefix_step3a_noun_success = False\n        self.prefix_step3b_noun_success = False\n\n        modified_word = word\n        # guess type and properties\n        # checks1\n        self.__checks_1(modified_word)\n        # checks2\n        self.__checks_2(modified_word)\n        # Pre_Normalization\n        modified_word = self.__normalize_pre(modified_word)\n        # Start stemming\n        if self.is_verb:\n            modified_word = self.__Suffix_Verb_Step1(modified_word)\n            if self.suffixes_verb_step1_success:\n                modified_word = self.__Suffix_Verb_Step2a(modified_word)\n                if not self.suffix_verb_step2a_success:\n                    modified_word = self.__Suffix_Verb_Step2c(modified_word)\n                # or next TODO: How to deal with or next instruction\n            else:\n                modified_word = self.__Suffix_Verb_Step2b(modified_word)\n                if not self.suffix_verb_step2b_success:\n                    modified_word = self.__Suffix_Verb_Step2a(modified_word)\n        if self.is_noun:\n            modified_word = self.__Suffix_Noun_Step2c2(modified_word)\n            if not self.suffix_noun_step2c2_success:\n                if not self.is_defined:\n                    modified_word = self.__Suffix_Noun_Step1a(modified_word)\n                    # if self.suffix_noun_step1a_success:\n                    modified_word = self.__Suffix_Noun_Step2a(modified_word)\n                    if not self.suffix_noun_step2a_success:\n                        modified_word = self.__Suffix_Noun_Step2b(modified_word)\n                    if (\n                        not self.suffix_noun_step2b_success\n                        and not self.suffix_noun_step2a_success\n                    ):\n                        modified_word = self.__Suffix_Noun_Step2c1(modified_word)\n                    # or next ? todo : how to deal with or next\n                else:\n                    modified_word = self.__Suffix_Noun_Step1b(modified_word)\n                    if self.suffixe_noun_step1b_success:\n                        modified_word = self.__Suffix_Noun_Step2a(modified_word)\n                        if not self.suffix_noun_step2a_success:\n                            modified_word = self.__Suffix_Noun_Step2b(modified_word)\n                        if (\n                            not self.suffix_noun_step2b_success\n                            and not self.suffix_noun_step2a_success\n                        ):\n                            modified_word = self.__Suffix_Noun_Step2c1(modified_word)\n                    else:\n                        if not self.is_defined:\n                            modified_word = self.__Suffix_Noun_Step2a(modified_word)\n                        modified_word = self.__Suffix_Noun_Step2b(modified_word)\n            modified_word = self.__Suffix_Noun_Step3(modified_word)\n        if not self.is_noun and self.is_verb:\n            modified_word = self.__Suffix_All_alef_maqsura(modified_word)\n\n        # prefixes\n        modified_word = self.__Prefix_Step1(modified_word)\n        modified_word = self.__Prefix_Step2a(modified_word)\n        if not self.prefix_step2a_success:\n            modified_word = self.__Prefix_Step2b(modified_word)\n        modified_word = self.__Prefix_Step3a_Noun(modified_word)\n        if not self.prefix_step3a_noun_success and self.is_noun:\n            modified_word = self.__Prefix_Step3b_Noun(modified_word)\n        else:\n            if not self.prefix_step3b_noun_success and self.is_verb:\n                modified_word = self.__Prefix_Step3_Verb(modified_word)\n                modified_word = self.__Prefix_Step4_Verb(modified_word)\n\n        # post normalization stemming\n        modified_word = self.__normalize_post(modified_word)\n        stemmed_word = modified_word\n        return stemmed_word", "response": "This method stemmed an Arabic word and returns the stemmed form."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove accent from the given string.", "response": "def deaccent(text):\n    \"\"\"\n    Remove accentuation from the given string.\n    \"\"\"\n    norm = unicodedata.normalize(\"NFD\", text)\n    result = \"\".join(ch for ch in norm if unicodedata.category(ch) != 'Mn')\n    return unicodedata.normalize(\"NFC\", result)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef tokenize(text, lowercase=False, deacc=False):\n    if lowercase:\n        text = text.lower()\n    if deacc:\n        text = deaccent(text)\n    for match in PAT_ALPHABETIC.finditer(text):\n        yield match.group()", "response": "Yields tokens as unicode strings optionally also lowercasing them\n    Removes accent marks."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clean_text_by_word(text, language=\"english\", deacc=False, additional_stopwords=None):\n    init_textcleanner(language, additional_stopwords)\n    text_without_acronyms = replace_with_separator(text, \"\", [AB_ACRONYM_LETTERS])\n    original_words = list(tokenize(text_without_acronyms, lowercase=True, deacc=deacc))\n    filtered_words = filter_words(original_words)\n    if HAS_PATTERN:\n        tags = tag(\" \".join(original_words))  # tag needs the context of the words in the text\n    else:\n        tags = None\n    units = merge_syntactic_units(original_words, filtered_words, tags)\n    return { unit.text : unit for unit in units }", "response": "Tokenizes a given text into words applying filters and lemmatizing them."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_sentences_with_word_count(sentences, words):\n    word_count = 0\n    selected_sentences = []\n    # Loops until the word count is reached.\n    for sentence in sentences:\n        words_in_sentence = len(sentence.text.split())\n\n        # Checks if the inclusion of the sentence gives a better approximation\n        # to the word parameter.\n        if abs(words - word_count - words_in_sentence) > abs(words - word_count):\n            return selected_sentences\n\n        selected_sentences.append(sentence)\n        word_count += words_in_sentence\n\n    return selected_sentences", "response": "Given a list of sentences returns a list of sentences with total word count similar to the word count provided."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dictionary of keywords with scores for each lemma in extracted_lemmas.", "response": "def _get_keywords_with_score(extracted_lemmas, lemma_to_word):\n    \"\"\"\n    :param extracted_lemmas:list of tuples\n    :param lemma_to_word: dict of {lemma:list of words}\n    :return: dict of {keyword:score}\n    \"\"\"\n    keywords = {}\n    for score, lemma in extracted_lemmas:\n        keyword_list = lemma_to_word[lemma]\n        for keyword in keyword_list:\n            keywords[keyword] = score\n    return keywords"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the combined keywords from the keywords dictionary.", "response": "def _get_combined_keywords(_keywords, split_text):\n    \"\"\"\n    :param keywords:dict of keywords:scores\n    :param split_text: list of strings\n    :return: combined_keywords:list\n    \"\"\"\n    result = []\n    _keywords = _keywords.copy()\n    len_text = len(split_text)\n    for i in range(len_text):\n        word = _strip_word(split_text[i])\n        if word in _keywords:\n            combined_word = [word]\n            if i + 1 == len_text:\n                result.append(word)   # appends last word if keyword and doesn't iterate\n            for j in range(i + 1, len_text):\n                other_word = _strip_word(split_text[j])\n                if other_word in _keywords and other_word == split_text[j] \\\n                        and other_word not in combined_word:\n                    combined_word.append(other_word)\n                else:\n                    for keyword in combined_word:\n                        _keywords.pop(keyword)\n                    result.append(\" \".join(combined_word))\n                    break\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _format_results(_keywords, combined_keywords, split, scores):\n    combined_keywords.sort(key=lambda w: _get_average_score(w, _keywords), reverse=True)\n    if scores:\n        return [(word, _get_average_score(word, _keywords)) for word in combined_keywords]\n    if split:\n        return combined_keywords\n    return \"\\n\".join(combined_keywords)", "response": "Format the results of the _get_average_score function."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pagerank_weighted(graph, initial_value=None, damping=0.85):\n    if initial_value == None: initial_value = 1.0 / len(graph.nodes())\n    scores = dict.fromkeys(graph.nodes(), initial_value)\n\n    iteration_quantity = 0\n    for iteration_number in range(100):\n        iteration_quantity += 1\n        convergence_achieved = 0\n        for i in graph.nodes():\n            rank = 1 - damping\n            for j in graph.neighbors(i):\n                neighbors_sum = sum(graph.edge_weight((j, k)) for k in graph.neighbors(j))\n                rank += damping * scores[j] * graph.edge_weight((j, i)) / neighbors_sum\n\n            if abs(scores[i] - rank) <= CONVERGENCE_THRESHOLD:\n                convergence_achieved += 1\n\n            scores[i] = rank\n\n        if convergence_achieved == len(graph.nodes()):\n            break\n\n    return scores", "response": "Calculates PageRank for an undirected graph"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the string representation of a key", "response": "def str(self, var, default=NOTSET, multiline=False):\n        \"\"\"\n        :rtype: str\n        \"\"\"\n        value = self.get_value(var, default=default)\n        if multiline:\n            return value.replace('\\\\n', '\\n')\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a bytes object from the cache.", "response": "def bytes(self, var, default=NOTSET, encoding='utf8'):\n        \"\"\"\n        :rtype: bytes\n        \"\"\"\n        return self.get_value(var, cast=str).encode(encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bool(self, var, default=NOTSET):\n        return self.get_value(var, cast=bool, default=default)", "response": "Get a boolean value from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef int(self, var, default=NOTSET):\n        return self.get_value(var, cast=int, default=default)", "response": "Get an integer value from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the named attribute as a float.", "response": "def float(self, var, default=NOTSET):\n        \"\"\"\n        :rtype: float\n        \"\"\"\n        return self.get_value(var, cast=float, default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a Json parsed from the cache.", "response": "def json(self, var, default=NOTSET):\n        \"\"\"\n        :returns: Json parsed\n        \"\"\"\n        return self.get_value(var, cast=json.loads, default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a tuple value from the cache.", "response": "def tuple(self, var, cast=None, default=NOTSET):\n        \"\"\"\n        :rtype: tuple\n        \"\"\"\n        return self.get_value(var, cast=tuple if not cast else (cast,), default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a key from a key - value mapping.", "response": "def dict(self, var, cast=dict, default=NOTSET):\n        \"\"\"\n        :rtype: dict\n        \"\"\"\n        return self.get_value(var, cast=cast, default=default)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef url(self, var, default=NOTSET):\n        return self.get_value(var, cast=urlparse, default=default, parse_default=True)", "response": "Get a parse result from a key."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a config dictionary defaulting to DATABASE_URL.", "response": "def db_url(self, var=DEFAULT_DATABASE_ENV, default=NOTSET, engine=None):\n        \"\"\"Returns a config dictionary, defaulting to DATABASE_URL.\n\n        :rtype: dict\n        \"\"\"\n        return self.db_url_config(self.get_value(var, default=default), engine=engine)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a config dictionary with the values for the cache_url variable.", "response": "def cache_url(self, var=DEFAULT_CACHE_ENV, default=NOTSET, backend=None):\n        \"\"\"Returns a config dictionary, defaulting to CACHE_URL.\n\n        :rtype: dict\n        \"\"\"\n        return self.cache_url_config(self.url(var, default=default), backend=backend)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef email_url(self, var=DEFAULT_EMAIL_ENV, default=NOTSET, backend=None):\n        return self.email_url_config(self.url(var, default=default), backend=backend)", "response": "Returns a config dictionary defaulting to EMAIL_URL."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef search_url(self, var=DEFAULT_SEARCH_ENV, default=NOTSET, engine=None):\n        return self.search_url_config(self.url(var, default=default), engine=engine)", "response": "Returns a config dictionary with the values for the given search environment variable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a Path object from the cache.", "response": "def path(self, var, default=NOTSET, **kwargs):\n        \"\"\"\n        :rtype: Path\n        \"\"\"\n        return Path(self.get_value(var, default=default), **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the value for a given environment variable.", "response": "def get_value(self, var, cast=None, default=NOTSET, parse_default=False):\n        \"\"\"Return value for given environment variable.\n\n        :param var: Name of variable.\n        :param cast: Type to cast return value as.\n        :param default: If var not present in environ, return this instead.\n        :param parse_default: force to parse default..\n\n        :returns: Value from environment or default (if set)\n        \"\"\"\n\n        logger.debug(\"get '{0}' casted as '{1}' with default '{2}'\".format(\n            var, cast, default\n        ))\n\n        if var in self.scheme:\n            var_info = self.scheme[var]\n\n            try:\n                has_default = len(var_info) == 2\n            except TypeError:\n                has_default = False\n\n            if has_default:\n                if not cast:\n                    cast = var_info[0]\n\n                if default is self.NOTSET:\n                    try:\n                        default = var_info[1]\n                    except IndexError:\n                        pass\n            else:\n                if not cast:\n                    cast = var_info\n\n        try:\n            value = self.ENVIRON[var]\n        except KeyError:\n            if default is self.NOTSET:\n                error_msg = \"Set the {0} environment variable\".format(var)\n                raise ImproperlyConfigured(error_msg)\n\n            value = default\n\n        # Resolve any proxied values\n        if hasattr(value, 'startswith') and value.startswith('$'):\n            value = value.lstrip('$')\n            value = self.get_value(value, cast=cast, default=default)\n\n        if cast is None and default is not None and not isinstance(default, NoValue):\n            cast = type(default)\n\n        if value != default or (parse_default and value):\n            value = self.parse_value(value, cast)\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses and cast provided value.", "response": "def parse_value(cls, value, cast):\n        \"\"\"Parse and cast provided value\n\n        :param value: Stringed value.\n        :param cast: Type to cast return value as.\n\n        :returns: Casted value\n        \"\"\"\n        if cast is None:\n            return value\n        elif cast is bool:\n            try:\n                value = int(value) != 0\n            except ValueError:\n                value = value.lower() in cls.BOOLEAN_TRUE_STRINGS\n        elif isinstance(cast, list):\n            value = list(map(cast[0], [x for x in value.split(',') if x]))\n        elif isinstance(cast, tuple):\n            val = value.strip('(').strip(')').split(',')\n            value = tuple(map(cast[0], [x for x in val if x]))\n        elif isinstance(cast, dict):\n            key_cast = cast.get('key', str)\n            value_cast = cast.get('value', str)\n            value_cast_by_key = cast.get('cast', dict())\n            value = dict(map(\n                lambda kv: (\n                    key_cast(kv[0]),\n                    cls.parse_value(kv[1], value_cast_by_key.get(kv[0], value_cast))\n                ),\n                [val.split('=') for val in value.split(';') if val]\n            ))\n        elif cast is dict:\n            value = dict([val.split('=') for val in value.split(',') if val])\n        elif cast is list:\n            value = [x for x in value.split(',') if x]\n        elif cast is tuple:\n            val = value.strip('(').strip(')').split(',')\n            value = tuple([x for x in val if x])\n        elif cast is float:\n            # clean string\n            float_str = re.sub(r'[^\\d,\\.]', '', value)\n            # split for avoid thousand separator and different locale comma/dot symbol\n            parts = re.split(r'[,\\.]', float_str)\n            if len(parts) == 1:\n                float_str = parts[0]\n            else:\n                float_str = \"{0}.{1}\".format(''.join(parts[0:-1]), parts[-1])\n            value = float(float_str)\n        else:\n            value = cast(value)\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef db_url_config(cls, url, engine=None):\n        if not isinstance(url, cls.URL_CLASS):\n            if url == 'sqlite://:memory:':\n                # this is a special case, because if we pass this URL into\n                # urlparse, urlparse will choke trying to interpret \"memory\"\n                # as a port number\n                return {\n                    'ENGINE': cls.DB_SCHEMES['sqlite'],\n                    'NAME': ':memory:'\n                }\n                # note: no other settings are required for sqlite\n            url = urlparse(url)\n\n        config = {}\n\n        # Remove query strings.\n        path = url.path[1:]\n        path = unquote_plus(path.split('?', 2)[0])\n\n        if url.scheme == 'sqlite':\n            if path == '':\n                # if we are using sqlite and we have no path, then assume we\n                # want an in-memory database (this is the behaviour of  sqlalchemy)\n                path = ':memory:'\n            if url.netloc:\n                warnings.warn(\n                    'SQLite URL contains host component %r, it will be ignored' % url.netloc, stacklevel=3)\n        if url.scheme == 'ldap':\n            path = '{scheme}://{hostname}'.format(scheme=url.scheme, hostname=url.hostname)\n            if url.port:\n                path += ':{port}'.format(port=url.port)\n\n        # Update with environment configuration.\n        config.update({\n            'NAME': path or '',\n            'USER': _cast_urlstr(url.username) or '',\n            'PASSWORD': _cast_urlstr(url.password) or '',\n            'HOST': url.hostname or '',\n            'PORT': _cast_int(url.port) or '',\n        })\n\n        if url.scheme == 'postgres' and path.startswith('/'):\n            config['HOST'], config['NAME'] = path.rsplit('/', 1)\n\n        if url.scheme == 'oracle' and path == '':\n            config['NAME'] = config['HOST']\n            config['HOST'] = ''\n\n        if url.scheme == 'oracle':\n            # Django oracle/base.py strips port and fails on non-string value\n            if not config['PORT']:\n                del(config['PORT'])\n            else:\n                config['PORT'] = str(config['PORT'])\n\n        if url.query:\n            config_options = {}\n            for k, v in parse_qs(url.query).items():\n                if k.upper() in cls._DB_BASE_OPTIONS:\n                    config.update({k.upper(): _cast(v[0])})\n                else:\n                    config_options.update({k: _cast_int(v[0])})\n            config['OPTIONS'] = config_options\n\n        if engine:\n            config['ENGINE'] = engine\n        else:\n            config['ENGINE'] = url.scheme\n\n        if config['ENGINE'] in Env.DB_SCHEMES:\n            config['ENGINE'] = Env.DB_SCHEMES[config['ENGINE']]\n\n        if not config.get('ENGINE', False):\n            warnings.warn(\"Engine not recognized from url: {0}\".format(config))\n            return {}\n\n        return config", "response": "Parse an arbitrary DJ - Database - URL and return a dictionary of the configuration parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse an arbitrary Cache URL and returns a dictionary of the appropriate configuration values.", "response": "def cache_url_config(cls, url, backend=None):\n        \"\"\"Pulled from DJ-Cache-URL, parse an arbitrary Cache URL.\n\n        :param url:\n        :param backend:\n        :return:\n        \"\"\"\n        url = urlparse(url) if not isinstance(url, cls.URL_CLASS) else url\n\n        location = url.netloc.split(',')\n        if len(location) == 1:\n            location = location[0]\n\n        config = {\n            'BACKEND': cls.CACHE_SCHEMES[url.scheme],\n            'LOCATION': location,\n        }\n\n        # Add the drive to LOCATION\n        if url.scheme == 'filecache':\n            config.update({\n                'LOCATION': url.netloc + url.path,\n            })\n\n        if url.path and url.scheme in ['memcache', 'pymemcache']:\n            config.update({\n                'LOCATION': 'unix:' + url.path,\n            })\n        elif url.scheme.startswith('redis'):\n            if url.hostname:\n                scheme = url.scheme.replace('cache', '')\n            else:\n                scheme = 'unix'\n            locations = [scheme + '://' + loc + url.path for loc in url.netloc.split(',')]\n            config['LOCATION'] = locations[0] if len(locations) == 1 else locations\n\n        if url.query:\n            config_options = {}\n            for k, v in parse_qs(url.query).items():\n                opt = {k.upper(): _cast(v[0])}\n                if k.upper() in cls._CACHE_BASE_OPTIONS:\n                    config.update(opt)\n                else:\n                    config_options.update(opt)\n            config['OPTIONS'] = config_options\n\n        if backend:\n            config['BACKEND'] = backend\n\n        return config"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef email_url_config(cls, url, backend=None):\n\n        config = {}\n\n        url = urlparse(url) if not isinstance(url, cls.URL_CLASS) else url\n\n        # Remove query strings\n        path = url.path[1:]\n        path = unquote_plus(path.split('?', 2)[0])\n\n        # Update with environment configuration\n        config.update({\n            'EMAIL_FILE_PATH': path,\n            'EMAIL_HOST_USER': _cast_urlstr(url.username),\n            'EMAIL_HOST_PASSWORD': _cast_urlstr(url.password),\n            'EMAIL_HOST': url.hostname,\n            'EMAIL_PORT': _cast_int(url.port),\n        })\n\n        if backend:\n            config['EMAIL_BACKEND'] = backend\n        elif url.scheme not in cls.EMAIL_SCHEMES:\n            raise ImproperlyConfigured('Invalid email schema %s' % url.scheme)\n        elif url.scheme in cls.EMAIL_SCHEMES:\n            config['EMAIL_BACKEND'] = cls.EMAIL_SCHEMES[url.scheme]\n\n        if url.scheme in ('smtps', 'smtp+tls'):\n            config['EMAIL_USE_TLS'] = True\n        elif url.scheme == 'smtp+ssl':\n            config['EMAIL_USE_SSL'] = True\n\n        if url.query:\n            config_options = {}\n            for k, v in parse_qs(url.query).items():\n                opt = {k.upper(): _cast_int(v[0])}\n                if k.upper() in cls._EMAIL_BASE_OPTIONS:\n                    config.update(opt)\n                else:\n                    config_options.update(opt)\n            config['OPTIONS'] = config_options\n\n        return config", "response": "Parses an email URL and returns a dictionary of configuration parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_env(cls, env_file=None, **overrides):\n        if env_file is None:\n            frame = sys._getframe()\n            env_file = os.path.join(os.path.dirname(frame.f_back.f_code.co_filename), '.env')\n            if not os.path.exists(env_file):\n                warnings.warn(\n                    \"%s doesn't exist - if you're not configuring your \"\n                    \"environment separately, create one.\" % env_file)\n                return\n\n        try:\n            with open(env_file) if isinstance(env_file, basestring) else env_file as f:\n                content = f.read()\n        except IOError:\n            warnings.warn(\n                \"Error reading %s - if you're not configuring your \"\n                \"environment separately, check this.\" % env_file)\n            return\n\n        logger.debug('Read environment variables from: {0}'.format(env_file))\n\n        for line in content.splitlines():\n            m1 = re.match(r'\\A(?:export )?([A-Za-z_0-9]+)=(.*)\\Z', line)\n            if m1:\n                key, val = m1.group(1), m1.group(2)\n                m2 = re.match(r\"\\A'(.*)'\\Z\", val)\n                if m2:\n                    val = m2.group(1)\n                m3 = re.match(r'\\A\"(.*)\"\\Z', val)\n                if m3:\n                    val = re.sub(r'\\\\(.)', r'\\1', m3.group(1))\n                cls.ENVIRON.setdefault(key, str(val))\n\n        # set defaults\n        for key, value in overrides.items():\n            cls.ENVIRON.setdefault(key, value)", "response": "Read a. env file into the environment dict."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new Path based on self. root and provided paths.", "response": "def path(self, *paths, **kwargs):\n        \"\"\"Create new Path based on self.root and provided paths.\n\n        :param paths: List of sub paths\n        :param kwargs: required=False\n        :rtype: Path\n        \"\"\"\n        return self.__class__(self.__root__, *paths, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef file(self, name, *args, **kwargs):\n        return open(self(name), *args, **kwargs)", "response": "Open a file in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisplays a deprecation warning in a standard way.", "response": "def warn_deprecated(since, message='', name='', alternative='', pending=False,\n                    obj_type='attribute', addendum=''):\n    \"\"\"Display deprecation warning in a standard way.\n\n    Parameters\n    ----------\n    since : str\n        The release at which this API became deprecated.\n\n    message : str, optional\n        Override the default deprecation message.  The format\n        specifier `%(name)s` may be used for the name of the function,\n        and `%(alternative)s` may be used in the deprecation message\n        to insert the name of an alternative to the deprecated\n        function.  `%(obj_type)s` may be used to insert a friendly name\n        for the type of object being deprecated.\n\n    name : str, optional\n        The name of the deprecated object.\n\n    alternative : str, optional\n        An alternative function that the user may use in place of the\n        deprecated function.  The deprecation warning will tell the user\n        about this alternative if provided.\n\n    pending : bool, optional\n        If True, uses a PendingDeprecationWarning instead of a\n        DeprecationWarning.\n\n    obj_type : str, optional\n        The object type being deprecated.\n\n    addendum : str, optional\n        Additional text appended directly to the final message.\n\n    Examples\n    --------\n        Basic example::\n\n            # To warn of the deprecation of \"metpy.name_of_module\"\n            warn_deprecated('0.6.0', name='metpy.name_of_module',\n                            obj_type='module')\n\n    \"\"\"\n    message = _generate_deprecation_message(since, message, name, alternative,\n                                            pending, obj_type)\n\n    warnings.warn(message, metpyDeprecation, stacklevel=1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_grid_coords(gx, gy):\n    return np.vstack([gx.ravel(), gy.ravel()]).T", "response": "r Generate x y coordinates of each grid cell."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_xy_range(bbox):\n    x_range = bbox['east'] - bbox['west']\n    y_range = bbox['north'] - bbox['south']\n\n    return x_range, y_range", "response": "r Returns x and y ranges in meters based on bounding box."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_xy_steps(bbox, h_dim):\n    x_range, y_range = get_xy_range(bbox)\n\n    x_steps = np.ceil(x_range / h_dim)\n    y_steps = np.ceil(y_range / h_dim)\n\n    return int(x_steps), int(y_steps)", "response": "r Return the number of grids in x and y dimensions based on bounding box."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef natural_neighbor_to_grid(xp, yp, variable, grid_x, grid_y):\n    # Handle grid-to-points conversion, and use function from `interpolation`\n    points_obs = list(zip(xp, yp))\n    points_grid = generate_grid_coords(grid_x, grid_y)\n    img = natural_neighbor_to_points(points_obs, variable, points_grid)\n    return img.reshape(grid_x.shape)", "response": "r Generate a natural neighbor interpolation of the given points to a regular grid."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap natural_neighbor_to_grid for deprecated natural_neighbor function.", "response": "def natural_neighbor(xp, yp, variable, grid_x, grid_y):\n    \"\"\"Wrap natural_neighbor_to_grid for deprecated natural_neighbor function.\"\"\"\n    return natural_neighbor_to_grid(xp, yp, variable, grid_x, grid_y)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping inverse_distance_to_grid for deprecated inverse_distance function.", "response": "def inverse_distance(xp, yp, variable, grid_x, grid_y, r, gamma=None, kappa=None,\n                     min_neighbors=3, kind='cressman'):\n    \"\"\"Wrap inverse_distance_to_grid for deprecated inverse_distance function.\"\"\"\n    return inverse_distance_to_grid(xp, yp, variable, grid_x, grid_y, r, gamma=gamma,\n                                    kappa=kappa, min_neighbors=min_neighbors, kind=kind)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interpolate_to_grid(x, y, z, interp_type='linear', hres=50000,\n                        minimum_neighbors=3, gamma=0.25, kappa_star=5.052,\n                        search_radius=None, rbf_func='linear', rbf_smooth=0,\n                        boundary_coords=None):\n    r\"\"\"Interpolate given (x,y), observation (z) pairs to a grid based on given parameters.\n\n    Parameters\n    ----------\n    x: array_like\n        x coordinate\n    y: array_like\n        y coordinate\n    z: array_like\n        observation value\n    interp_type: str\n        What type of interpolation to use. Available options include:\n        1) \"linear\", \"nearest\", \"cubic\", or \"rbf\" from `scipy.interpolate`.\n        2) \"natural_neighbor\", \"barnes\", or \"cressman\" from `metpy.interpolate`.\n        Default \"linear\".\n    hres: float\n        The horizontal resolution of the generated grid, given in the same units as the\n        x and y parameters. Default 50000.\n    minimum_neighbors: int\n        Minimum number of neighbors needed to perform barnes or cressman interpolation for a\n        point. Default is 3.\n    gamma: float\n        Adjustable smoothing parameter for the barnes interpolation. Default 0.25.\n    kappa_star: float\n        Response parameter for barnes interpolation, specified nondimensionally\n        in terms of the Nyquist. Default 5.052\n    search_radius: float\n        A search radius to use for the barnes and cressman interpolation schemes.\n        If search_radius is not specified, it will default to the average spacing of\n        observations.\n    rbf_func: str\n        Specifies which function to use for Rbf interpolation.\n        Options include: 'multiquadric', 'inverse', 'gaussian', 'linear', 'cubic',\n        'quintic', and 'thin_plate'. Defualt 'linear'. See `scipy.interpolate.Rbf` for more\n        information.\n    rbf_smooth: float\n        Smoothing value applied to rbf interpolation.  Higher values result in more smoothing.\n    boundary_coords: dictionary\n        Optional dictionary containing coordinates of the study area boundary. Dictionary\n        should be in format: {'west': west, 'south': south, 'east': east, 'north': north}\n\n    Returns\n    -------\n    grid_x: (N, 2) ndarray\n        Meshgrid for the resulting interpolation in the x dimension\n    grid_y: (N, 2) ndarray\n        Meshgrid for the resulting interpolation in the y dimension ndarray\n    img: (M, N) ndarray\n        2-dimensional array representing the interpolated values for each grid.\n\n    Notes\n    -----\n    This function acts as a wrapper for `interpolate_points` to allow it to generate a regular\n    grid.\n\n    See Also\n    --------\n    interpolate_to_points\n\n    \"\"\"\n    # Generate the grid\n    if boundary_coords is None:\n        boundary_coords = get_boundary_coords(x, y)\n    grid_x, grid_y = generate_grid(hres, boundary_coords)\n\n    # Handle grid-to-points conversion, and use function from `interpolation`\n    points_obs = np.array(list(zip(x, y)))\n    points_grid = generate_grid_coords(grid_x, grid_y)\n    img = interpolate_to_points(points_obs, z, points_grid, interp_type=interp_type,\n                                minimum_neighbors=minimum_neighbors, gamma=gamma,\n                                kappa_star=kappa_star, search_radius=search_radius,\n                                rbf_func=rbf_func, rbf_smooth=rbf_smooth)\n\n    return grid_x, grid_y, img.reshape(grid_x.shape)", "response": "r Interpolate given x y z pairs to a grid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef interpolate(x, y, z, interp_type='linear', hres=50000,\n                minimum_neighbors=3, gamma=0.25, kappa_star=5.052,\n                search_radius=None, rbf_func='linear', rbf_smooth=0,\n                boundary_coords=None):\n    \"\"\"Wrap interpolate_to_grid for deprecated interpolate function.\"\"\"\n    return interpolate_to_grid(x, y, z, interp_type=interp_type, hres=hres,\n                               minimum_neighbors=minimum_neighbors, gamma=gamma,\n                               kappa_star=kappa_star, search_radius=search_radius,\n                               rbf_func=rbf_func, rbf_smooth=rbf_smooth,\n                               boundary_coords=boundary_coords)", "response": "Wrap interpolate_to_grid for deprecated interpolate function."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interpolate_nans_1d(x, y, kind='linear'):\n    x_sort_args = np.argsort(x)\n    x = x[x_sort_args]\n    y = y[x_sort_args]\n    nans = np.isnan(y)\n    if kind == 'linear':\n        y[nans] = np.interp(x[nans], x[~nans], y[~nans])\n    elif kind == 'log':\n        y[nans] = np.interp(np.log(x[nans]), np.log(x[~nans]), y[~nans])\n    else:\n        raise ValueError('Unknown option for kind: {0}'.format(str(kind)))\n    return y[x_sort_args]", "response": "Interpolate NaN values in x and y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interpolate_1d(x, xp, *args, **kwargs):\n    # Pull out keyword args\n    fill_value = kwargs.pop('fill_value', np.nan)\n    axis = kwargs.pop('axis', 0)\n\n    # Make x an array\n    x = np.asanyarray(x).reshape(-1)\n\n    # Save number of dimensions in xp\n    ndim = xp.ndim\n\n    # Sort input data\n    sort_args = np.argsort(xp, axis=axis)\n    sort_x = np.argsort(x)\n\n    # indices for sorting\n    sorter = broadcast_indices(xp, sort_args, ndim, axis)\n\n    # sort xp\n    xp = xp[sorter]\n    # Ensure pressure in increasing order\n    variables = [arr[sorter] for arr in args]\n\n    # Make x broadcast with xp\n    x_array = x[sort_x]\n    expand = [np.newaxis] * ndim\n    expand[axis] = slice(None)\n    x_array = x_array[tuple(expand)]\n\n    # Calculate value above interpolated value\n    minv = np.apply_along_axis(np.searchsorted, axis, xp, x[sort_x])\n    minv2 = np.copy(minv)\n\n    # If fill_value is none and data is out of bounds, raise value error\n    if ((np.max(minv) == xp.shape[axis]) or (np.min(minv) == 0)) and fill_value is None:\n        raise ValueError('Interpolation point out of data bounds encountered')\n\n    # Warn if interpolated values are outside data bounds, will make these the values\n    # at end of data range.\n    if np.max(minv) == xp.shape[axis]:\n        warnings.warn('Interpolation point out of data bounds encountered')\n        minv2[minv == xp.shape[axis]] = xp.shape[axis] - 1\n    if np.min(minv) == 0:\n        minv2[minv == 0] = 1\n\n    # Get indices for broadcasting arrays\n    above = broadcast_indices(xp, minv2, ndim, axis)\n    below = broadcast_indices(xp, minv2 - 1, ndim, axis)\n\n    if np.any(x_array < xp[below]):\n        warnings.warn('Interpolation point out of data bounds encountered')\n\n    # Create empty output list\n    ret = []\n\n    # Calculate interpolation for each variable\n    for var in variables:\n        # Var needs to be on the *left* of the multiply to ensure that if it's a pint\n        # Quantity, it gets to control the operation--at least until we make sure\n        # masked arrays and pint play together better. See https://github.com/hgrecco/pint#633\n        var_interp = var[below] + (var[above] - var[below]) * ((x_array - xp[below])\n                                                               / (xp[above] - xp[below]))\n\n        # Set points out of bounds to fill value.\n        var_interp[minv == xp.shape[axis]] = fill_value\n        var_interp[x_array < xp[below]] = fill_value\n\n        # Check for input points in decreasing order and return output to match.\n        if x[0] > x[-1]:\n            var_interp = np.swapaxes(np.swapaxes(var_interp, 0, axis)[::-1], 0, axis)\n        # Output to list\n        ret.append(var_interp)\n    if len(ret) == 1:\n        return ret[0]\n    else:\n        return ret", "response": "r Interpolates data with any shape over a specified axis."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef log_interpolate_1d(x, xp, *args, **kwargs):\n    # Pull out kwargs\n    fill_value = kwargs.pop('fill_value', np.nan)\n    axis = kwargs.pop('axis', 0)\n\n    # Log x and xp\n    log_x = np.log(x)\n    log_xp = np.log(xp)\n    return interpolate_1d(log_x, log_xp, *args, axis=axis, fill_value=fill_value)", "response": "r Log - interpolation of data over a specified axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the distances in the x and y directions along a cross - section.", "response": "def distances_from_cross_section(cross):\n    \"\"\"Calculate the distances in the x and y directions along a cross-section.\n\n    Parameters\n    ----------\n    cross : `xarray.DataArray`\n        The input DataArray of a cross-section from which to obtain geometeric distances in\n        the x and y directions.\n\n    Returns\n    -------\n    x, y : tuple of `xarray.DataArray`\n        A tuple of the x and y distances as DataArrays\n\n    \"\"\"\n    if (CFConventionHandler.check_axis(cross.metpy.x, 'lon')\n            and CFConventionHandler.check_axis(cross.metpy.y, 'lat')):\n        # Use pyproj to obtain x and y distances\n        from pyproj import Geod\n\n        g = Geod(cross.metpy.cartopy_crs.proj4_init)\n        lon = cross.metpy.x\n        lat = cross.metpy.y\n\n        forward_az, _, distance = g.inv(lon[0].values * np.ones_like(lon),\n                                        lat[0].values * np.ones_like(lat),\n                                        lon.values,\n                                        lat.values)\n        x = distance * np.sin(np.deg2rad(forward_az))\n        y = distance * np.cos(np.deg2rad(forward_az))\n\n        # Build into DataArrays\n        x = xr.DataArray(x, coords=lon.coords, dims=lon.dims, attrs={'units': 'meters'})\n        y = xr.DataArray(y, coords=lat.coords, dims=lat.dims, attrs={'units': 'meters'})\n\n    elif (CFConventionHandler.check_axis(cross.metpy.x, 'x')\n            and CFConventionHandler.check_axis(cross.metpy.y, 'y')):\n\n        # Simply return what we have\n        x = cross.metpy.x\n        y = cross.metpy.y\n\n    else:\n        raise AttributeError('Sufficient horizontal coordinates not defined.')\n\n    return x, y"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the latitude of points in a cross - section.", "response": "def latitude_from_cross_section(cross):\n    \"\"\"Calculate the latitude of points in a cross-section.\n\n    Parameters\n    ----------\n    cross : `xarray.DataArray`\n        The input DataArray of a cross-section from which to obtain latitudes.\n\n    Returns\n    -------\n    latitude : `xarray.DataArray`\n        Latitude of points\n\n    \"\"\"\n    y = cross.metpy.y\n    if CFConventionHandler.check_axis(y, 'lat'):\n        return y\n    else:\n        import cartopy.crs as ccrs\n        latitude = ccrs.Geodetic().transform_points(cross.metpy.cartopy_crs,\n                                                    cross.metpy.x.values,\n                                                    y.values)[..., 1]\n        latitude = xr.DataArray(latitude, coords=y.coords, dims=y.dims,\n                                attrs={'units': 'degrees_north'})\n        return latitude"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unit_vectors_from_cross_section(cross, index='index'):\n    x, y = distances_from_cross_section(cross)\n    dx_di = first_derivative(x, axis=index).values\n    dy_di = first_derivative(y, axis=index).values\n    tangent_vector_mag = np.hypot(dx_di, dy_di)\n    unit_tangent_vector = np.vstack([dx_di / tangent_vector_mag, dy_di / tangent_vector_mag])\n    unit_normal_vector = np.vstack([-dy_di / tangent_vector_mag, dx_di / tangent_vector_mag])\n    return unit_tangent_vector, unit_normal_vector", "response": "r Calculates the unit tangent vector and unit normal vector from a cross - section."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cross_section_components(data_x, data_y, index='index'):\n    # Get the unit vectors\n    unit_tang, unit_norm = unit_vectors_from_cross_section(data_x, index=index)\n\n    # Take the dot products\n    component_tang = data_x * unit_tang[0] + data_y * unit_tang[1]\n    component_norm = data_x * unit_norm[0] + data_y * unit_norm[1]\n\n    # Reattach units (only reliable attribute after operation)\n    component_tang.attrs = {'units': data_x.attrs['units']}\n    component_norm.attrs = {'units': data_x.attrs['units']}\n\n    return component_tang, component_norm", "response": "r Returns the tangential and normal components of a cross - section of a vector field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef normal_component(data_x, data_y, index='index'):\n    # Get the unit vectors\n    _, unit_norm = unit_vectors_from_cross_section(data_x, index=index)\n\n    # Take the dot products\n    component_norm = data_x * unit_norm[0] + data_y * unit_norm[1]\n\n    # Reattach only reliable attributes after operation\n    for attr in ('units', 'grid_mapping'):\n        if attr in data_x.attrs:\n            component_norm.attrs[attr] = data_x.attrs[attr]\n\n    return component_norm", "response": "r Return the normal component of a cross - section of a vector field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef scan_resource(self, pkg, path):\n        for fname in resource_listdir(pkg, path):\n            if fname.endswith(TABLE_EXT):\n                table_path = posixpath.join(path, fname)\n                with contextlib.closing(resource_stream(pkg, table_path)) as stream:\n                    self.add_colortable(stream,\n                                        posixpath.splitext(posixpath.basename(fname))[0])", "response": "r Scan a resource directory for colortable files and add them to the registry."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_colortable(self, fobj, name):\n        self[name] = read_colortable(fobj)\n        self[name + '_r'] = self[name][::-1]", "response": "r Add a color table from a file to the registry."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_with_steps(self, name, start, step):\n        from numpy import arange\n\n        # Need one more boundary than color\n        num_steps = len(self[name]) + 1\n        boundaries = arange(start, start + step * num_steps, step)\n        return self.get_with_boundaries(name, boundaries)", "response": "r Get a color table from the registry with a corresponding norm."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_with_range(self, name, start, end):\n        from numpy import linspace\n\n        # Need one more boundary than color\n        num_steps = len(self[name]) + 1\n        boundaries = linspace(start, end, num_steps)\n        return self.get_with_boundaries(name, boundaries)", "response": "r Get a color table from the registry with a corresponding norm."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a color table from the registry with a corresponding norm.", "response": "def get_with_boundaries(self, name, boundaries):\n        r\"\"\"Get a color table from the registry with a corresponding norm.\n\n        Builds a `matplotlib.colors.BoundaryNorm` using `boundaries`.\n\n        Parameters\n        ----------\n        name : str\n            The name under which the color table will be stored\n        boundaries : array_like\n            The list of boundaries for the norm\n\n        Returns\n        -------\n        `matplotlib.colors.BoundaryNorm`, `matplotlib.colors.ListedColormap`\n            The boundary norm based on `boundaries`, and the color table itself.\n\n        \"\"\"\n        cmap = self.get_colortable(name)\n        return mcolors.BoundaryNorm(boundaries, cmap.N), cmap"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef barnes_point(sq_dist, values, kappa, gamma=None):\n    if gamma is None:\n        gamma = 1\n    weights = tools.barnes_weights(sq_dist, kappa, gamma)\n    total_weights = np.sum(weights)\n\n    return sum(v * (w / total_weights) for (w, v) in zip(weights, values))", "response": "r Generate a single pass barnes interpolation value for a given set of values."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef natural_neighbor_to_points(points, values, xi):\n    tri = Delaunay(points)\n\n    members, triangle_info = geometry.find_natural_neighbors(tri, xi)\n\n    img = np.empty(shape=(xi.shape[0]), dtype=values.dtype)\n    img.fill(np.nan)\n\n    for ind, (grid, neighbors) in enumerate(members.items()):\n\n        if len(neighbors) > 0:\n\n            points_transposed = np.array(points).transpose()\n            img[ind] = natural_neighbor_point(points_transposed[0], points_transposed[1],\n                                              values, xi[grid], tri, neighbors, triangle_info)\n\n    return img", "response": "r Generate a natural neighbor interpolation to the given points."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _make_datetime(s):\n    s = bytearray(s)  # For Python 2\n    year, month, day, hour, minute, second, cs = s\n    if year < 70:\n        year += 100\n    return datetime(1900 + year, month, day, hour, minute, second, 10000 * cs)", "response": "r Convert 7 bytes from a GINI file to a datetime instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _name_lookup(names):\n    mapper = dict(zip(range(len(names)), names))\n\n    def lookup(val):\n        return mapper.get(val, 'Unknown')\n    return lookup", "response": "rCreate an io helper to convert an integer to a named value."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _add_projection_coords(ds, prod_desc, proj_var, dx, dy):\n    proj = cf_to_proj(proj_var)\n\n    # Get projected location of lower left point\n    x0, y0 = proj(prod_desc.lo1, prod_desc.la1)\n\n    # Coordinate variable for x\n    ds.createDimension('x', prod_desc.nx)\n    x_var = ds.createVariable('x', np.float64, dimensions=('x',))\n    x_var.units = 'm'\n    x_var.long_name = 'x coordinate of projection'\n    x_var.standard_name = 'projection_x_coordinate'\n    x_var[:] = x0 + np.arange(prod_desc.nx) * (1000. * dx)\n\n    # Now y\n    ds.createDimension('y', prod_desc.ny)\n    y_var = ds.createVariable('y', np.float64, dimensions=('y',))\n    y_var.units = 'm'\n    y_var.long_name = 'y coordinate of projection'\n    y_var.standard_name = 'projection_y_coordinate'\n\n    # Need to flip y because we calculated from the lower left corner, but the raster data\n    # is stored with top row first.\n    y_var[::-1] = y0 + np.arange(prod_desc.ny) * (1000. * dy)\n\n    # Get the two-D lon,lat grid as well\n    x, y = np.meshgrid(x_var[:], y_var[:])\n    lon, lat = proj(x, y, inverse=True)\n    lon_var = ds.createVariable('lon', np.float64, dimensions=('y', 'x'), wrap_array=lon)\n    lon_var.long_name = 'longitude'\n    lon_var.units = 'degrees_east'\n\n    lat_var = ds.createVariable('lat', np.float64, dimensions=('y', 'x'), wrap_array=lat)\n    lat_var.long_name = 'latitude'\n    lat_var.units = 'degrees_north'\n\n    ds.img_extent = (x_var[:].min(), x_var[:].max(), y_var[:].min(), y_var[:].max())", "response": "Add coordinate variables ( projection and lon / lat to a dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tke(u, v, w, perturbation=False, axis=-1):\n    if not perturbation:\n        u = get_perturbation(u, axis=axis)\n        v = get_perturbation(v, axis=axis)\n        w = get_perturbation(w, axis=axis)\n\n    u_cont = np.mean(u * u, axis=axis)\n    v_cont = np.mean(v * v, axis=axis)\n    w_cont = np.mean(w * w, axis=axis)\n\n    return 0.5 * np.sqrt(u_cont + v_cont + w_cont)", "response": "r Returns the turbulence kinetic energy of a given set of time series components."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef kinematic_flux(vel, b, perturbation=False, axis=-1):\n    kf = np.mean(vel * b, axis=axis)\n    if not perturbation:\n        kf -= np.mean(vel, axis=axis) * np.mean(b, axis=axis)\n    return np.atleast_1d(kf)", "response": "r Compute the kinematic flux from two time series."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef friction_velocity(u, w, v=None, perturbation=False, axis=-1):\n    uw = kinematic_flux(u, w, perturbation=perturbation, axis=axis)\n    kf = uw * uw\n    if v is not None:\n        vw = kinematic_flux(v, w, perturbation=perturbation, axis=axis)\n        kf += vw * vw\n    # the friction velocity is the 4th root of the kinematic momentum flux\n    # As an optimization, first do inplace square root, then return the\n    # square root of that. This is faster than np.power(..., 0.25)\n    np.sqrt(kf, out=kf)\n    return np.sqrt(kf)", "response": "r Returns the friction velocity from the time series of the x z and w and optionally y velocity components."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a file - object given either a filename or an object.", "response": "def open_as_needed(filename):\n    \"\"\"Return a file-object given either a filename or an object.\n\n    Handles opening with the right class based on the file extension.\n\n    \"\"\"\n    if hasattr(filename, 'read'):\n        return filename\n\n    if filename.endswith('.bz2'):\n        return bz2.BZ2File(filename, 'rb')\n    elif filename.endswith('.gz'):\n        return gzip.GzipFile(filename, 'rb')\n    else:\n        return open(filename, 'rb')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef zlib_decompress_all_frames(data):\n    frames = bytearray()\n    data = bytes(data)\n    while data:\n        decomp = zlib.decompressobj()\n        try:\n            frames.extend(decomp.decompress(data))\n            data = decomp.unused_data\n        except zlib.error:\n            frames.extend(data)\n            break\n    return frames", "response": "Decompress all frames of zlib - compressed bytes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hexdump(buf, num_bytes, offset=0, width=32):\n    ind = offset\n    end = offset + num_bytes\n    lines = []\n    while ind < end:\n        chunk = buf[ind:ind + width]\n        actual_width = len(chunk)\n        hexfmt = '{:02X}'\n        blocksize = 4\n        blocks = [hexfmt * blocksize for _ in range(actual_width // blocksize)]\n\n        # Need to get any partial lines\n        num_left = actual_width % blocksize  # noqa: S001  Fix false alarm\n        if num_left:\n            blocks += [hexfmt * num_left + '--' * (blocksize - num_left)]\n        blocks += ['--' * blocksize] * (width // blocksize - len(blocks))\n\n        hexoutput = ' '.join(blocks)\n        printable = tuple(chunk)\n        lines.append('  '.join((hexoutput.format(*printable), str(ind).ljust(len(str(end))),\n                                str(ind - offset).ljust(len(str(end))),\n                                ''.join(chr(c) if 31 < c < 128 else '.' for c in chunk))))\n        ind += width\n    return '\\n'.join(lines)", "response": "Perform a hexudmp of the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef units(self, val):\n        if isinstance(val, units.Unit):\n            self._unit = val\n        else:\n            self._unit = units(val)", "response": "Override the units on the underlying variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unpack(self, s):\n        return self._create(super(NamedStruct, self).unpack(s))", "response": "Parse bytes and return a namedtuple."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading bytes from a buffer and return as a namedtuple.", "response": "def unpack_from(self, buff, offset=0):\n        \"\"\"Read bytes from a buffer and return as a namedtuple.\"\"\"\n        return self._create(super(NamedStruct, self).unpack_from(buff, offset))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses bytes and return a namedtuple.", "response": "def unpack(self, s):\n        \"\"\"Parse bytes and return a namedtuple.\"\"\"\n        return self._create(super(DictStruct, self).unpack(s))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef unpack_from(self, buff, offset=0):\n        return self._create(super(DictStruct, self).unpack_from(buff, offset))", "response": "Unpack the next bytes from a file object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_mark(self):\n        self._bookmarks.append(self._offset)\n        return len(self._bookmarks) - 1", "response": "Mark the current location and return its id so that the buffer can return later."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef jump_to(self, mark, offset=0):\n        self._offset = self._bookmarks[mark] + offset", "response": "Jump to a previously set mark."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreplace the data after the marked location with the specified data.", "response": "def splice(self, mark, newdata):\n        \"\"\"Replace the data after the marked location with the specified data.\"\"\"\n        self.jump_to(mark)\n        self._data = self._data[:self._offset] + bytearray(newdata)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse and return a structure from the current buffer offset.", "response": "def read_struct(self, struct_class):\n        \"\"\"Parse and return a structure from the current buffer offset.\"\"\"\n        struct = struct_class.unpack_from(bytearray_to_buff(self._data), self._offset)\n        self.skip(struct_class.size)\n        return struct"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses data from the current buffer offset using a function.", "response": "def read_func(self, func, num_bytes=None):\n        \"\"\"Parse data from the current buffer offset using a function.\"\"\"\n        # only advance if func succeeds\n        res = func(self.get_next(num_bytes))\n        self.skip(num_bytes)\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_binary(self, num, item_type='B'):\n        if 'B' in item_type:\n            return self.read(num)\n\n        if item_type[0] in ('@', '=', '<', '>', '!'):\n            order = item_type[0]\n            item_type = item_type[1:]\n        else:\n            order = '@'\n\n        return list(self.read_struct(Struct(order + '{:d}'.format(int(num)) + item_type)))", "response": "Parse the current buffer offset as the specified code."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, num_bytes=None):\n        res = self.get_next(num_bytes)\n        self.skip(len(res))\n        return res", "response": "Read and return the specified bytes from the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the next bytes in the buffer without modifying the offset.", "response": "def get_next(self, num_bytes=None):\n        \"\"\"Get the next bytes in the buffer without modifying the offset.\"\"\"\n        if num_bytes is None:\n            return self._data[self._offset:]\n        else:\n            return self._data[self._offset:self._offset + num_bytes]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef skip(self, num_bytes):\n        if num_bytes is None:\n            self._offset = len(self._data)\n        else:\n            self._offset += num_bytes", "response": "Jump the ahead the specified bytes in the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef draw_polygon_with_info(ax, polygon, off_x=0, off_y=0):\n    pts = np.array(polygon)[ConvexHull(polygon).vertices]\n    for i, pt in enumerate(pts):\n        ax.plot([pt[0], pts[(i + 1) % len(pts)][0]],\n                [pt[1], pts[(i + 1) % len(pts)][1]], 'k-')\n\n    avex, avey = np.mean(pts, axis=0)\n    ax.annotate('area: {:.3f}'.format(geometry.area(pts)), xy=(avex + off_x, avey + off_y),\n                fontsize=12)", "response": "Draw one of the natural neighbor polygons with some information."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_and_flip(arr):\n    if hasattr(arr, 'ndim'):\n        if arr.ndim >= 2:\n            return arr.T\n        else:\n            return arr\n    elif not is_string_like(arr) and iterable(arr):\n        return tuple(_check_and_flip(a) for a in arr)\n    else:\n        return arr", "response": "Transpose array or list of arrays if they are 2D."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_yx_order(func):\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        # Check what order we're given\n        dim_order = kwargs.pop('dim_order', None)\n        x_first = _is_x_first_dim(dim_order)\n\n        # If x is the first dimension, flip (transpose) every array within the function args.\n        if x_first:\n            args = tuple(_check_and_flip(arr) for arr in args)\n            for k, v in kwargs:\n                kwargs[k] = _check_and_flip(v)\n\n        ret = func(*args, **kwargs)\n\n        # If we flipped on the way in, need to flip on the way out so that output array(s)\n        # match the dimension order of the original input.\n        if x_first:\n            return _check_and_flip(ret)\n        else:\n            return ret\n\n    # Inject a docstring for the dim_order argument into the function's docstring.\n    dim_order_doc = \"\"\"\n    dim_order : str or ``None``, optional\n        The ordering of dimensions in passed in arrays. Can be one of ``None``, ``'xy'``,\n        or ``'yx'``. ``'xy'`` indicates that the dimension corresponding to x is the leading\n        dimension, followed by y. ``'yx'`` indicates that x is the last dimension, preceded\n        by y. ``None`` indicates that the default ordering should be assumed,\n        which is 'yx'. Can only be passed as a keyword argument, i.e.\n        func(..., dim_order='xy').\"\"\"\n\n    # Find the first blank line after the start of the parameters section\n    params = wrapper.__doc__.find('Parameters')\n    blank = wrapper.__doc__.find('\\n\\n', params)\n    wrapper.__doc__ = wrapper.__doc__[:blank] + dim_order_doc + wrapper.__doc__[blank:]\n\n    return wrapper", "response": "Wrap a function to ensure all array arguments are y x ordered based on kwarg."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef total_deformation(u, v, dx, dy):\n    dudy, dudx = gradient(u, deltas=(dy, dx), axes=(-2, -1))\n    dvdy, dvdx = gradient(v, deltas=(dy, dx), axes=(-2, -1))\n    return np.sqrt((dvdx + dudy)**2 + (dudx - dvdy)**2)", "response": "r Calculates the horizontal total deformation of the horizontal wind."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ageostrophic_wind(heights, f, dx, dy, u, v, dim_order='yx'):\n    u_geostrophic, v_geostrophic = geostrophic_wind(heights, f, dx, dy, dim_order=dim_order)\n    return u - u_geostrophic, v - v_geostrophic", "response": "r Returns the ageostrophic wind given from the heights or geopotential."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef storm_relative_helicity(u, v, heights, depth, bottom=0 * units.m,\n                            storm_u=0 * units('m/s'), storm_v=0 * units('m/s')):\n    # Partially adapted from similar SharpPy code\n    r\"\"\"Calculate storm relative helicity.\n\n    Calculates storm relatively helicity following [Markowski2010] 230-231.\n\n    .. math:: \\int\\limits_0^d (\\bar v - c) \\cdot \\bar\\omega_{h} \\,dz\n\n    This is applied to the data from a hodograph with the following summation:\n\n    .. math:: \\sum_{n = 1}^{N-1} [(u_{n+1} - c_{x})(v_{n} - c_{y}) -\n                                  (u_{n} - c_{x})(v_{n+1} - c_{y})]\n\n    Parameters\n    ----------\n    u : array-like\n        u component winds\n    v : array-like\n        v component winds\n    heights : array-like\n        atmospheric heights, will be converted to AGL\n    depth : number\n        depth of the layer\n    bottom : number\n        height of layer bottom AGL (default is surface)\n    storm_u : number\n        u component of storm motion (default is 0 m/s)\n    storm_v : number\n        v component of storm motion (default is 0 m/s)\n\n    Returns\n    -------\n    `pint.Quantity, pint.Quantity, pint.Quantity`\n        positive, negative, total storm-relative helicity\n\n    \"\"\"\n    _, u, v = get_layer_heights(heights, depth, u, v, with_agl=True, bottom=bottom)\n\n    storm_relative_u = u - storm_u\n    storm_relative_v = v - storm_v\n\n    int_layers = (storm_relative_u[1:] * storm_relative_v[:-1]\n                  - storm_relative_u[:-1] * storm_relative_v[1:])\n\n    # Need to manually check for masked value because sum() on masked array with non-default\n    # mask will return a masked value rather than 0. See numpy/numpy#11736\n    positive_srh = int_layers[int_layers.magnitude > 0.].sum()\n    if np.ma.is_masked(positive_srh):\n        positive_srh = 0.0 * units('meter**2 / second**2')\n    negative_srh = int_layers[int_layers.magnitude < 0.].sum()\n    if np.ma.is_masked(negative_srh):\n        negative_srh = 0.0 * units('meter**2 / second**2')\n\n    return (positive_srh.to('meter ** 2 / second ** 2'),\n            negative_srh.to('meter ** 2 / second ** 2'),\n            (positive_srh + negative_srh).to('meter ** 2 / second ** 2'))", "response": "Calculates storm relative helicity of a given set of atmospheric heights."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the absolute vorticity of the horizontal wind.", "response": "def absolute_vorticity(u, v, dx, dy, lats, dim_order='yx'):\n    \"\"\"Calculate the absolute vorticity of the horizontal wind.\n\n    Parameters\n    ----------\n    u : (M, N) ndarray\n        x component of the wind\n    v : (M, N) ndarray\n        y component of the wind\n    dx : float or ndarray\n        The grid spacing(s) in the x-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis.\n    dy : float or ndarray\n        The grid spacing(s) in the y-direction. If an array, there should be one item less than\n        the size of `u` along the applicable axis.\n    lats : (M, N) ndarray\n        latitudes of the wind data in radians or with appropriate unit information attached\n\n    Returns\n    -------\n    (M, N) ndarray\n        absolute vorticity\n\n    Notes\n    -----\n    If inputs have more than two dimensions, they are assumed to have either leading dimensions\n    of (x, y) or trailing dimensions of (y, x), depending on the value of ``dim_order``.\n\n    \"\"\"\n    f = coriolis_parameter(lats)\n    relative_vorticity = vorticity(u, v, dx, dy, dim_order=dim_order)\n    return relative_vorticity + f"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef potential_vorticity_baroclinic(potential_temperature, pressure, u, v, dx, dy, lats):\n    if ((np.shape(potential_temperature)[-3] < 3) or (np.shape(pressure)[-3] < 3)\n       or (np.shape(potential_temperature)[-3] != (np.shape(pressure)[-3]))):\n        raise ValueError('Length of potential temperature along the pressure axis '\n                         '{} must be at least 3.'.format(-3))\n\n    avor = absolute_vorticity(u, v, dx, dy, lats, dim_order='yx')\n    dthtadp = first_derivative(potential_temperature, x=pressure, axis=-3)\n\n    if ((np.shape(potential_temperature)[-2] == 1)\n       and (np.shape(potential_temperature)[-1] == 1)):\n        dthtady = 0 * units.K / units.m  # axis=-2 only has one dimension\n        dthtadx = 0 * units.K / units.m  # axis=-1 only has one dimension\n    else:\n        dthtady = first_derivative(potential_temperature, delta=dy, axis=-2)\n        dthtadx = first_derivative(potential_temperature, delta=dx, axis=-1)\n    dudp = first_derivative(u, x=pressure, axis=-3)\n    dvdp = first_derivative(v, x=pressure, axis=-3)\n\n    return (-mpconsts.g * (dudp * dthtady - dvdp * dthtadx\n                           + avor * dthtadp)).to(units.kelvin * units.meter**2\n                                                 / (units.second * units.kilogram))", "response": "r Returns the baroclinic potential vorticity for the given potential temperature pressure u v dx dy lats"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inertial_advective_wind(u, v, u_geostrophic, v_geostrophic, dx, dy, lats):\n    f = coriolis_parameter(lats)\n\n    dugdy, dugdx = gradient(u_geostrophic, deltas=(dy, dx), axes=(-2, -1))\n    dvgdy, dvgdx = gradient(v_geostrophic, deltas=(dy, dx), axes=(-2, -1))\n\n    u_component = -(u * dvgdx + v * dvgdy) / f\n    v_component = (u * dugdx + v * dugdy) / f\n\n    return u_component, v_component", "response": "r Calculates the inertial advective wind."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef q_vector(u, v, temperature, pressure, dx, dy, static_stability=1):\n    dudy, dudx = gradient(u, deltas=(dy, dx), axes=(-2, -1))\n    dvdy, dvdx = gradient(v, deltas=(dy, dx), axes=(-2, -1))\n    dtempdy, dtempdx = gradient(temperature, deltas=(dy, dx), axes=(-2, -1))\n\n    q1 = -mpconsts.Rd / (pressure * static_stability) * (dudx * dtempdx + dvdx * dtempdy)\n    q2 = -mpconsts.Rd / (pressure * static_stability) * (dudy * dtempdx + dvdy * dtempdy)\n\n    return q1.to_base_units(), q2.to_base_units()", "response": "r Returns a sequence of Q - vectors at a given pressure level."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef basic_map(proj):\n    fig = plt.figure(figsize=(15, 10))\n    add_metpy_logo(fig, 0, 80, size='large')\n    view = fig.add_axes([0, 0, 1, 1], projection=proj)\n    view.set_extent([-120, -70, 20, 50])\n    view.add_feature(cfeature.STATES.with_scale('50m'))\n    view.add_feature(cfeature.OCEAN)\n    view.add_feature(cfeature.COASTLINE)\n    view.add_feature(cfeature.BORDERS, linestyle=':')\n    return fig, view", "response": "Make our basic default map for plotting"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_point_count_within_r(center_points, target_points, r):\n    tree = cKDTree(target_points)\n    indices = tree.query_ball_point(center_points, r)\n    return np.array([len(x) for x in indices])", "response": "r Returns the number of target points within a specified radius from center points."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef triangle_area(pt1, pt2, pt3):\n    a = 0.0\n\n    a += pt1[0] * pt2[1] - pt2[0] * pt1[1]\n    a += pt2[0] * pt3[1] - pt3[0] * pt2[1]\n    a += pt3[0] * pt1[1] - pt1[0] * pt3[1]\n\n    return abs(a) / 2", "response": "r Return the area of a triangle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef circumcircle_radius_2(pt0, pt1, pt2):\n    a = distance(pt0, pt1)\n    b = distance(pt1, pt2)\n    c = distance(pt2, pt0)\n\n    t_area = triangle_area(pt0, pt1, pt2)\n    prod2 = a * b * c\n\n    if t_area > 0:\n        radius = prod2 * prod2 / (16 * t_area * t_area)\n    else:\n        radius = np.nan\n\n    return radius", "response": "r Calculates and return the squared radius of a given triangle s circumcircle. This is faster than calculating radius but should only be used with comparable ratios."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef circumcenter(pt0, pt1, pt2):\n    a_x = pt0[0]\n    a_y = pt0[1]\n    b_x = pt1[0]\n    b_y = pt1[1]\n    c_x = pt2[0]\n    c_y = pt2[1]\n\n    bc_y_diff = b_y - c_y\n    ca_y_diff = c_y - a_y\n    ab_y_diff = a_y - b_y\n    cb_x_diff = c_x - b_x\n    ac_x_diff = a_x - c_x\n    ba_x_diff = b_x - a_x\n\n    d_div = (a_x * bc_y_diff + b_x * ca_y_diff + c_x * ab_y_diff)\n\n    if d_div == 0:\n        raise ZeroDivisionError\n\n    d_inv = 0.5 / d_div\n\n    a_mag = a_x * a_x + a_y * a_y\n    b_mag = b_x * b_x + b_y * b_y\n    c_mag = c_x * c_x + c_y * c_y\n\n    cx = (a_mag * bc_y_diff + b_mag * ca_y_diff + c_mag * ab_y_diff) * d_inv\n    cy = (a_mag * cb_x_diff + b_mag * ac_x_diff + c_mag * ba_x_diff) * d_inv\n\n    return cx, cy", "response": "r Calculates and returns the circumcenter of a given triangle."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef order_edges(edges):\n    edge = edges[0]\n    edges = edges[1:]\n\n    ordered_edges = [edge]\n\n    num_max = len(edges)\n    while len(edges) > 0 and num_max > 0:\n\n        match = edge[1]\n\n        for search_edge in edges:\n            vertex = search_edge[0]\n            if match == vertex:\n                edge = search_edge\n                edges.remove(edge)\n                ordered_edges.append(search_edge)\n                break\n        num_max -= 1\n\n    return ordered_edges", "response": "r Return an ordered traversal of the edges of a two - dimensional polygon."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef critical_angle(pressure, u, v, heights, stormu, stormv):\n    # Convert everything to m/s\n    u = u.to('m/s')\n    v = v.to('m/s')\n    stormu = stormu.to('m/s')\n    stormv = stormv.to('m/s')\n\n    sort_inds = np.argsort(pressure[::-1])\n    pressure = pressure[sort_inds]\n    heights = heights[sort_inds]\n    u = u[sort_inds]\n    v = v[sort_inds]\n\n    # Calculate sfc-500m shear vector\n    shr5 = bulk_shear(pressure, u, v, heights=heights, depth=500 * units('meter'))\n\n    # Make everything relative to the sfc wind orientation\n    umn = stormu - u[0]\n    vmn = stormv - v[0]\n\n    vshr = np.asarray([shr5[0].magnitude, shr5[1].magnitude])\n    vsm = np.asarray([umn.magnitude, vmn.magnitude])\n    angle_c = np.dot(vshr, vsm) / (np.linalg.norm(vshr) * np.linalg.norm(vsm))\n    critical_angle = np.arccos(angle_c) * units('radian')\n\n    return critical_angle.to('degrees')", "response": "r Calculates the critical angle between the 10m storm - relative inflow vector and the 10m - 500m shear vector."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef broadcast_indices(x, minv, ndim, axis):\n    ret = []\n    for dim in range(ndim):\n        if dim == axis:\n            ret.append(minv)\n        else:\n            broadcast_slice = [np.newaxis] * ndim\n            broadcast_slice[dim] = slice(None)\n            dim_inds = np.arange(x.shape[dim])\n            ret.append(dim_inds[tuple(broadcast_slice)])\n    return tuple(ret)", "response": "Calculate index values to properly broadcast index array within data array."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register(self, name):\n        def dec(func):\n            self._registry[name] = func\n            return func\n        return dec", "response": "Register a callable with the registry under a particular name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wind_components(speed, wdir):\n    wdir = _check_radians(wdir, max_radians=4 * np.pi)\n    u = -speed * np.sin(wdir)\n    v = -speed * np.cos(wdir)\n    return u, v", "response": "r Calculates the U V wind vector components from the speed and direction."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pressure_to_height_std(pressure):\n    t0 = 288. * units.kelvin\n    gamma = 6.5 * units('K/km')\n    p0 = 1013.25 * units.mbar\n    return (t0 / gamma) * (1 - (pressure / p0).to('dimensionless')**(\n        mpconsts.Rd * gamma / mpconsts.g))", "response": "r Convert pressure data to heights using the U. S. standard atmosphere."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef geopotential_to_height(geopot):\n    # Calculate geopotential\n    height = (((1 / mpconsts.Re) - (geopot / (mpconsts.G * mpconsts.me))) ** -1) - mpconsts.Re\n\n    return height", "response": "r Compute height from a given geopotential."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef coriolis_parameter(latitude):\n    latitude = _check_radians(latitude, max_radians=np.pi / 2)\n    return (2. * mpconsts.omega * np.sin(latitude)).to('1/s')", "response": "r Calculates the coriolis parameter at each point."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef smooth_gaussian(scalar_grid, n):\n    # Compute standard deviation in a manner consistent with GEMPAK\n    n = int(round(n))\n    if n < 2:\n        n = 2\n    sgma = n / (2 * np.pi)\n\n    # Construct sigma sequence so smoothing occurs only in horizontal direction\n    nax = len(scalar_grid.shape)\n    # Assume the last two axes represent the horizontal directions\n    sgma_seq = [sgma if i > nax - 3 else 0 for i in range(nax)]\n\n    # Compute smoothed field and reattach units\n    res = gaussian_filter(scalar_grid, sgma_seq, truncate=2 * np.sqrt(2))\n    if hasattr(scalar_grid, 'units'):\n        res = res * scalar_grid.units\n    return res", "response": "This function is used to smooth a 2D scalar grid with normal distribution of weights."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef smooth_n_point(scalar_grid, n=5, passes=1):\n    if n == 9:\n        p = 0.25\n        q = 0.125\n        r = 0.0625\n    elif n == 5:\n        p = 0.5\n        q = 0.125\n        r = 0.0\n    else:\n        raise ValueError('The number of points to use in the smoothing '\n                         'calculation must be either 5 or 9.')\n\n    smooth_grid = scalar_grid[:].copy()\n    for _i in range(passes):\n        smooth_grid[1:-1, 1:-1] = (p * smooth_grid[1:-1, 1:-1]\n                                   + q * (smooth_grid[2:, 1:-1] + smooth_grid[1:-1, 2:]\n                                          + smooth_grid[:-2, 1:-1] + smooth_grid[1:-1, :-2])\n                                   + r * (smooth_grid[2:, 2:] + smooth_grid[2:, :-2] +\n                                          + smooth_grid[:-2, 2:] + smooth_grid[:-2, :-2]))\n    return smooth_grid", "response": "Smooth a 2D scalar grid by a number of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_radians(value, max_radians=2 * np.pi):\n    try:\n        value = value.to('radians').m\n    except AttributeError:\n        pass\n    if np.greater(np.nanmax(np.abs(value)), max_radians):\n        warnings.warn('Input over {} radians. '\n                      'Ensure proper units are given.'.format(max_radians))\n    return value", "response": "Input validation of values that could be in degrees instead of radians."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_observations_below_value(x, y, z, val=0):\n    x_ = x[z >= val]\n    y_ = y[z >= val]\n    z_ = z[z >= val]\n\n    return x_, y_, z_", "response": "r Removes all observations x y z where z is less than val."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_nan_observations(x, y, z):\n    x_ = x[~np.isnan(z)]\n    y_ = y[~np.isnan(z)]\n    z_ = z[~np.isnan(z)]\n\n    return x_, y_, z_", "response": "r Removes all x y and z where z is nan."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove_repeat_coordinates(x, y, z):\n    coords = []\n    variable = []\n\n    for (x_, y_, t_) in zip(x, y, z):\n        if (x_, y_) not in coords:\n            coords.append((x_, y_))\n            variable.append(t_)\n\n    coords = np.array(coords)\n\n    x_ = coords[:, 0]\n    y_ = coords[:, 1]\n\n    z_ = np.array(variable)\n\n    return x_, y_, z_", "response": "r Removes all x y and z where x y and z are repeated and keep the first occurrence only."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the location of tick in data coords with scalar loc.", "response": "def update_position(self, loc):\n        \"\"\"Set the location of tick in data coords with scalar *loc*.\"\"\"\n        # This ensures that the new value of the location is set before\n        # any other updates take place.\n        self._loc = loc\n        super(SkewXTick, self).update_position(loc)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncontrolling whether the gridline is drawn for this tick.", "response": "def gridOn(self):  # noqa: N802\n        \"\"\"Control whether the gridline is drawn for this tick.\"\"\"\n        return (self._gridOn and (self._has_default_loc()\n                or transforms.interval_contains(self.get_view_interval(), self.get_loc())))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting limits and transforms for the base class.", "response": "def _set_lim_and_transforms(self):\n        \"\"\"Set limits and transforms.\n\n        This is called once when the plot is created to set up all the\n        transforms for the data, text and grids.\n\n        \"\"\"\n        # Get the standard transform setup from the Axes base class\n        Axes._set_lim_and_transforms(self)\n\n        # Need to put the skew in the middle, after the scale and limits,\n        # but before the transAxes. This way, the skew is done in Axes\n        # coordinates thus performing the transform around the proper origin\n        # We keep the pre-transAxes transform around for other users, like the\n        # spines for finding bounds\n        self.transDataToAxes = (self.transScale\n                                + (self.transLimits\n                                   + transforms.Affine2D().skew_deg(self.rot, 0)))\n\n        # Create the full transform from Data to Pixels\n        self.transData = self.transDataToAxes + self.transAxes\n\n        # Blended transforms like this need to have the skewing applied using\n        # both axes, in axes coords like before.\n        self._xaxis_transform = (\n            transforms.blended_transform_factory(self.transScale + self.transLimits,\n                                                 transforms.IdentityTransform())\n            + transforms.Affine2D().skew_deg(self.rot, 0)) + self.transAxes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef bzip_blocks_decompress_all(data):\n    frames = bytearray()\n    offset = 0\n    while offset < len(data):\n        size_bytes = data[offset:offset + 4]\n        offset += 4\n        block_cmp_bytes = abs(Struct('>l').unpack(size_bytes)[0])\n        try:\n            frames.extend(bz2.decompress(data[offset:offset + block_cmp_bytes]))\n            offset += block_cmp_bytes\n        except IOError:\n            # If we've decompressed any frames, this is an error mid-stream, so warn, stop\n            # trying to decompress and let processing proceed\n            if frames:\n                logging.warning('Error decompressing bz2 block stream at offset: %d',\n                                offset - 4)\n                break\n            else:  # Otherwise, this isn't a bzip2 stream, so bail\n                raise ValueError('Not a bz2 stream.')\n    return frames", "response": "Decompress all of the bzip2 - ed blocks.\n    Returns the decompressed data as a bytearray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef nexrad_to_datetime(julian_date, ms_midnight):\n    # Subtracting one from julian_date is because epoch date is 1\n    return datetime.datetime.utcfromtimestamp((julian_date - 1) * day + ms_midnight * milli)", "response": "Convert NEXRAD date time format to python datetime. datetime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert status integer value to appropriate bitmask.", "response": "def remap_status(val):\n    \"\"\"Convert status integer value to appropriate bitmask.\"\"\"\n    status = 0\n    bad = BAD_DATA if val & 0xF0 else 0\n    val &= 0x0F\n    if val == 0:\n        status = START_ELEVATION\n    elif val == 1:\n        status = 0\n    elif val == 2:\n        status = END_ELEVATION\n    elif val == 3:\n        status = START_ELEVATION | START_VOLUME\n    elif val == 4:\n        status = END_ELEVATION | END_VOLUME\n    elif val == 5:\n        status = START_ELEVATION | LAST_ELEVATION\n\n    return status | bad"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reduce_lists(d):\n    for field in d:\n        old_data = d[field]\n        if len(old_data) == 1:\n            d[field] = old_data[0]", "response": "Replace single item lists in a dictionary with the single item."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef float16(val):\n    # Fraction is 10 LSB, Exponent middle 5, and Sign the MSB\n    frac = val & 0x03ff\n    exp = (val >> 10) & 0x1F\n    sign = val >> 15\n\n    if exp:\n        value = 2 ** (exp - 16) * (1 + float(frac) / 2**10)\n    else:\n        value = float(frac) / 2**9\n\n    if sign:\n        value *= -1\n\n    return value", "response": "Convert a 16 - bit floating point value to a standard Python float."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a function to parse a datetime from the product - specific blocks.", "response": "def date_elem(ind_days, ind_minutes):\n    \"\"\"Create a function to parse a datetime from the product-specific blocks.\"\"\"\n    def inner(seq):\n        return nexrad_to_datetime(seq[ind_days], seq[ind_minutes] * 60 * 1000)\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a function to combine two specified product - specific blocks into a single int.", "response": "def combine_elem(ind1, ind2):\n    \"\"\"Create a function to combine two specified product-specific blocks into a single int.\"\"\"\n    def inner(seq):\n        shift = 2**16\n        if seq[ind1] < 0:\n            seq[ind1] += shift\n        if seq[ind2] < 0:\n            seq[ind2] += shift\n        return (seq[ind1] << 16) | seq[ind2]\n    return inner"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef relative_humidity_from_dewpoint(temperature, dewpt):\n    e = saturation_vapor_pressure(dewpt)\n    e_s = saturation_vapor_pressure(temperature)\n    return (e / e_s)", "response": "r Calculates the relative humidity from the temperature and dewpoint in celsius."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef exner_function(pressure, reference_pressure=mpconsts.P0):\n    return (pressure / reference_pressure).to('dimensionless')**mpconsts.kappa", "response": "r Calculates the Exner function at the given pressure and reference pressure."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lcl(pressure, temperature, dewpt, max_iters=50, eps=1e-5):\n    def _lcl_iter(p, p0, w, t):\n        td = dewpoint(vapor_pressure(units.Quantity(p, pressure.units), w))\n        return (p0 * (td / t) ** (1. / mpconsts.kappa)).m\n\n    w = mixing_ratio(saturation_vapor_pressure(dewpt), pressure)\n    fp = so.fixed_point(_lcl_iter, pressure.m, args=(pressure.m, w, temperature),\n                        xtol=eps, maxiter=max_iters)\n    lcl_p = fp * pressure.units\n    return lcl_p, dewpoint(vapor_pressure(lcl_p, w))", "response": "r Calculates the lifted condensation level using from the starting point pressure temperature and dewpt."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parcel_profile(pressure, temperature, dewpt):\n    _, _, _, t_l, _, t_u = _parcel_profile_helper(pressure, temperature, dewpt)\n    return concatenate((t_l, t_u))", "response": "r Calculates the parcel profile for a given pressure and temperature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nhelping calculate parcel profiles.", "response": "def _parcel_profile_helper(pressure, temperature, dewpt):\n    \"\"\"Help calculate parcel profiles.\n\n    Returns the temperature and pressure, above, below, and including the LCL. The\n    other calculation functions decide what to do with the pieces.\n\n    \"\"\"\n    # Find the LCL\n    press_lcl, temp_lcl = lcl(pressure[0], temperature, dewpt)\n    press_lcl = press_lcl.to(pressure.units)\n\n    # Find the dry adiabatic profile, *including* the LCL. We need >= the LCL in case the\n    # LCL is included in the levels. It's slightly redundant in that case, but simplifies\n    # the logic for removing it later.\n    press_lower = concatenate((pressure[pressure >= press_lcl], press_lcl))\n    temp_lower = dry_lapse(press_lower, temperature)\n\n    # If the pressure profile doesn't make it to the lcl, we can stop here\n    if _greater_or_close(np.nanmin(pressure), press_lcl.m):\n        return (press_lower[:-1], press_lcl, np.array([]) * press_lower.units,\n                temp_lower[:-1], temp_lcl, np.array([]) * temp_lower.units)\n\n    # Find moist pseudo-adiabatic profile starting at the LCL\n    press_upper = concatenate((press_lcl, pressure[pressure < press_lcl]))\n    temp_upper = moist_lapse(press_upper, temp_lower[-1]).to(temp_lower.units)\n\n    # Return profile pieces\n    return (press_lower[:-1], press_lcl, press_upper[1:],\n            temp_lower[:-1], temp_lcl, temp_upper[1:])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninsert the LCL pressure into the profile.", "response": "def _insert_lcl_level(pressure, temperature, lcl_pressure):\n    \"\"\"Insert the LCL pressure into the profile.\"\"\"\n    interp_temp = interpolate_1d(lcl_pressure, pressure, temperature)\n\n    # Pressure needs to be increasing for searchsorted, so flip it and then convert\n    # the index back to the original array\n    loc = pressure.size - pressure[::-1].searchsorted(lcl_pressure)\n    return np.insert(temperature.m, loc, interp_temp.m) * temperature.units"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef saturation_equivalent_potential_temperature(pressure, temperature):\n    t = temperature.to('kelvin').magnitude\n    p = pressure.to('hPa').magnitude\n    e = saturation_vapor_pressure(temperature).to('hPa').magnitude\n    r = saturation_mixing_ratio(pressure, temperature).magnitude\n\n    th_l = t * (1000 / (p - e)) ** mpconsts.kappa\n    th_es = th_l * np.exp((3036. / t - 1.78) * r * (1 + 0.448 * r))\n\n    return th_es * units.kelvin", "response": "r Calculates the saturation equivalent potential temperature for a given air parcel s pressure and temperature."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cape_cin(pressure, temperature, dewpt, parcel_profile):\n    # Calculate LFC limit of integration\n    lfc_pressure, _ = lfc(pressure, temperature, dewpt,\n                          parcel_temperature_profile=parcel_profile)\n\n    # If there is no LFC, no need to proceed.\n    if np.isnan(lfc_pressure):\n        return 0 * units('J/kg'), 0 * units('J/kg')\n    else:\n        lfc_pressure = lfc_pressure.magnitude\n\n    # Calculate the EL limit of integration\n    el_pressure, _ = el(pressure, temperature, dewpt,\n                        parcel_temperature_profile=parcel_profile)\n\n    # No EL and we use the top reading of the sounding.\n    if np.isnan(el_pressure):\n        el_pressure = pressure[-1].magnitude\n    else:\n        el_pressure = el_pressure.magnitude\n\n    # Difference between the parcel path and measured temperature profiles\n    y = (parcel_profile - temperature).to(units.degK)\n\n    # Estimate zero crossings\n    x, y = _find_append_zero_crossings(np.copy(pressure), y)\n\n    # CAPE\n    # Only use data between the LFC and EL for calculation\n    p_mask = _less_or_close(x, lfc_pressure) & _greater_or_close(x, el_pressure)\n    x_clipped = x[p_mask]\n    y_clipped = y[p_mask]\n    cape = (mpconsts.Rd\n            * (np.trapz(y_clipped, np.log(x_clipped)) * units.degK)).to(units('J/kg'))\n\n    # CIN\n    # Only use data between the surface and LFC for calculation\n    p_mask = _greater_or_close(x, lfc_pressure)\n    x_clipped = x[p_mask]\n    y_clipped = y[p_mask]\n    cin = (mpconsts.Rd\n           * (np.trapz(y_clipped, np.log(x_clipped)) * units.degK)).to(units('J/kg'))\n\n    return cape, cin", "response": "r Returns a base for the cape and CIN."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _find_append_zero_crossings(x, y):\n    # Find and append crossings to the data\n    crossings = find_intersections(x[1:], y[1:], np.zeros_like(y[1:]) * y.units)\n    x = concatenate((x, crossings[0]))\n    y = concatenate((y, crossings[1]))\n\n    # Resort so that data are in order\n    sort_idx = np.argsort(x)\n    x = x[sort_idx]\n    y = y[sort_idx]\n\n    # Remove duplicate data points if there are any\n    keep_idx = np.ediff1d(x, to_end=[1]) > 0\n    x = x[keep_idx]\n    y = y[keep_idx]\n    return x, y", "response": "r Find and interpolate zero crossings."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines the most unstable parcel in a layer.", "response": "def most_unstable_parcel(pressure, temperature, dewpoint, heights=None,\n                         bottom=None, depth=300 * units.hPa):\n    \"\"\"\n    Determine the most unstable parcel in a layer.\n\n    Determines the most unstable parcel of air by calculating the equivalent\n    potential temperature and finding its maximum in the specified layer.\n\n    Parameters\n    ----------\n    pressure: `pint.Quantity`\n        Atmospheric pressure profile\n    temperature: `pint.Quantity`\n        Atmospheric temperature profile\n    dewpoint: `pint.Quantity`\n        Atmospheric dewpoint profile\n    heights: `pint.Quantity`, optional\n        Atmospheric height profile. Standard atmosphere assumed when None (the default).\n    bottom: `pint.Quantity`, optional\n        Bottom of the layer to consider for the calculation in pressure or height.\n        Defaults to using the bottom pressure or height.\n    depth: `pint.Quantity`, optional\n        Depth of the layer to consider for the calculation in pressure or height. Defaults\n        to 300 hPa.\n\n    Returns\n    -------\n    `pint.Quantity`\n        Pressure, temperature, and dew point of most unstable parcel in the profile.\n    integer\n        Index of the most unstable parcel in the given profile\n\n    See Also\n    --------\n    get_layer\n\n    \"\"\"\n    p_layer, t_layer, td_layer = get_layer(pressure, temperature, dewpoint, bottom=bottom,\n                                           depth=depth, heights=heights, interpolate=False)\n    theta_e = equivalent_potential_temperature(p_layer, t_layer, td_layer)\n    max_idx = np.argmax(theta_e)\n    return p_layer[max_idx], t_layer[max_idx], td_layer[max_idx], max_idx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef isentropic_interpolation(theta_levels, pressure, temperature, *args, **kwargs):\n    # iteration function to be used later\n    # Calculates theta from linearly interpolated temperature and solves for pressure\n    def _isen_iter(iter_log_p, isentlevs_nd, ka, a, b, pok):\n        exner = pok * np.exp(-ka * iter_log_p)\n        t = a * iter_log_p + b\n        # Newton-Raphson iteration\n        f = isentlevs_nd - t * exner\n        fp = exner * (ka * t - a)\n        return iter_log_p - (f / fp)\n\n    # Change when Python 2.7 no longer supported\n    # Pull out keyword arguments\n    tmpk_out = kwargs.pop('tmpk_out', False)\n    max_iters = kwargs.pop('max_iters', 50)\n    eps = kwargs.pop('eps', 1e-6)\n    axis = kwargs.pop('axis', 0)\n    bottom_up_search = kwargs.pop('bottom_up_search', True)\n\n    # Get dimensions in temperature\n    ndim = temperature.ndim\n\n    # Convert units\n    pres = pressure.to('hPa')\n    temperature = temperature.to('kelvin')\n\n    slices = [np.newaxis] * ndim\n    slices[axis] = slice(None)\n    slices = tuple(slices)\n    pres = np.broadcast_to(pres[slices], temperature.shape) * pres.units\n\n    # Sort input data\n    sort_pres = np.argsort(pres.m, axis=axis)\n    sort_pres = np.swapaxes(np.swapaxes(sort_pres, 0, axis)[::-1], 0, axis)\n    sorter = broadcast_indices(pres, sort_pres, ndim, axis)\n    levs = pres[sorter]\n    tmpk = temperature[sorter]\n\n    theta_levels = np.asanyarray(theta_levels.to('kelvin')).reshape(-1)\n    isentlevels = theta_levels[np.argsort(theta_levels)]\n\n    # Make the desired isentropic levels the same shape as temperature\n    shape = list(temperature.shape)\n    shape[axis] = isentlevels.size\n    isentlevs_nd = np.broadcast_to(isentlevels[slices], shape)\n\n    # exponent to Poisson's Equation, which is imported above\n    ka = mpconsts.kappa.m_as('dimensionless')\n\n    # calculate theta for each point\n    pres_theta = potential_temperature(levs, tmpk)\n\n    # Raise error if input theta level is larger than pres_theta max\n    if np.max(pres_theta.m) < np.max(theta_levels):\n        raise ValueError('Input theta level out of data bounds')\n\n    # Find log of pressure to implement assumption of linear temperature dependence on\n    # ln(p)\n    log_p = np.log(levs.m)\n\n    # Calculations for interpolation routine\n    pok = mpconsts.P0 ** ka\n\n    # index values for each point for the pressure level nearest to the desired theta level\n    above, below, good = find_bounding_indices(pres_theta.m, theta_levels, axis,\n                                               from_below=bottom_up_search)\n\n    # calculate constants for the interpolation\n    a = (tmpk.m[above] - tmpk.m[below]) / (log_p[above] - log_p[below])\n    b = tmpk.m[above] - a * log_p[above]\n\n    # calculate first guess for interpolation\n    isentprs = 0.5 * (log_p[above] + log_p[below])\n\n    # Make sure we ignore any nans in the data for solving; checking a is enough since it\n    # combines log_p and tmpk.\n    good &= ~np.isnan(a)\n\n    # iterative interpolation using scipy.optimize.fixed_point and _isen_iter defined above\n    log_p_solved = so.fixed_point(_isen_iter, isentprs[good],\n                                  args=(isentlevs_nd[good], ka, a[good], b[good], pok.m),\n                                  xtol=eps, maxiter=max_iters)\n\n    # get back pressure from log p\n    isentprs[good] = np.exp(log_p_solved)\n\n    # Mask out points we know are bad as well as points that are beyond the max pressure\n    isentprs[~(good & _less_or_close(isentprs, np.max(pres.m)))] = np.nan\n\n    # create list for storing output data\n    ret = [isentprs * units.hPa]\n\n    # if tmpk_out = true, calculate temperature and output as last item in list\n    if tmpk_out:\n        ret.append((isentlevs_nd / ((mpconsts.P0.m / isentprs) ** ka)) * units.kelvin)\n\n    # do an interpolation for each additional argument\n    if args:\n        others = interpolate_1d(isentlevels, pres_theta.m, *(arr[sorter] for arr in args),\n                                axis=axis)\n        if len(args) > 1:\n            ret.extend(others)\n        else:\n            ret.append(others)\n\n    return ret", "response": "r This function is used to interpolate data in isobaric coordinates to isentropic coordinates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef most_unstable_cape_cin(pressure, temperature, dewpoint, **kwargs):\n    _, parcel_temperature, parcel_dewpoint, parcel_idx = most_unstable_parcel(pressure,\n                                                                              temperature,\n                                                                              dewpoint,\n                                                                              **kwargs)\n    mu_profile = parcel_profile(pressure[parcel_idx:], parcel_temperature, parcel_dewpoint)\n    return cape_cin(pressure[parcel_idx:], temperature[parcel_idx:],\n                    dewpoint[parcel_idx:], mu_profile)", "response": "r Returns the most unstable CIN for a given pressure temperature and dewpoint."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mixed_parcel(p, temperature, dewpt, parcel_start_pressure=None,\n                 heights=None, bottom=None, depth=100 * units.hPa, interpolate=True):\n    r\"\"\"Calculate the properties of a parcel mixed from a layer.\n\n    Determines the properties of an air parcel that is the result of complete mixing of a\n    given atmospheric layer.\n\n    Parameters\n    ----------\n    p : `pint.Quantity`\n        Atmospheric pressure profile\n    temperature : `pint.Quantity`\n        Atmospheric temperature profile\n    dewpt : `pint.Quantity`\n        Atmospheric dewpoint profile\n    parcel_start_pressure : `pint.Quantity`, optional\n        Pressure at which the mixed parcel should begin (default None)\n    heights: `pint.Quantity`, optional\n        Atmospheric heights corresponding to the given pressures (default None)\n    bottom : `pint.Quantity`, optional\n        The bottom of the layer as a pressure or height above the surface pressure\n        (default None)\n    depth : `pint.Quantity`, optional\n        The thickness of the layer as a pressure or height above the bottom of the layer\n        (default 100 hPa)\n    interpolate : bool, optional\n        Interpolate the top and bottom points if they are not in the given data\n\n    Returns\n    -------\n    `pint.Quantity, pint.Quantity, pint.Quantity`\n        The pressure, temperature, and dewpoint of the mixed parcel.\n\n    \"\"\"\n    # If a parcel starting pressure is not provided, use the surface\n    if not parcel_start_pressure:\n        parcel_start_pressure = p[0]\n\n    # Calculate the potential temperature and mixing ratio over the layer\n    theta = potential_temperature(p, temperature)\n    mixing_ratio = saturation_mixing_ratio(p, dewpt)\n\n    # Mix the variables over the layer\n    mean_theta, mean_mixing_ratio = mixed_layer(p, theta, mixing_ratio, bottom=bottom,\n                                                heights=heights, depth=depth,\n                                                interpolate=interpolate)\n\n    # Convert back to temperature\n    mean_temperature = (mean_theta / potential_temperature(parcel_start_pressure,\n                                                           1 * units.kelvin)) * units.kelvin\n\n    # Convert back to dewpoint\n    mean_vapor_pressure = vapor_pressure(parcel_start_pressure, mean_mixing_ratio)\n    mean_dewpoint = dewpoint(mean_vapor_pressure)\n\n    return (parcel_start_pressure, mean_temperature.to(temperature.units),\n            mean_dewpoint.to(dewpt.units))", "response": "r Calculates the properties of a mixed parcel from a given atmospheric layer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef thickness_hydrostatic_from_relative_humidity(pressure, temperature, relative_humidity,\n                                                 **kwargs):\n    r\"\"\"Calculate the thickness of a layer given pressure, temperature and relative humidity.\n\n    Similar to ``thickness_hydrostatic``, this thickness calculation uses the pressure,\n    temperature, and relative humidity profiles via the hypsometric equation with virtual\n    temperature adjustment.\n\n    .. math:: Z_2 - Z_1 = -\\frac{R_d}{g} \\int_{p_1}^{p_2} T_v d\\ln p,\n\n    which is based off of Equation 3.24 in [Hobbs2006]_. Virtual temperature is calculated\n    from the profiles of temperature and relative humidity.\n\n    This assumes a hydrostatic atmosphere.\n\n    Layer bottom and depth specified in pressure.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressure profile\n    temperature : `pint.Quantity`\n        Atmospheric temperature profile\n    relative_humidity : `pint.Quantity`\n        Atmospheric relative humidity profile. The relative humidity is expressed as a\n        unitless ratio in the range [0, 1]. Can also pass a percentage if proper units are\n        attached.\n    bottom : `pint.Quantity`, optional\n        The bottom of the layer in pressure. Defaults to the first observation.\n    depth : `pint.Quantity`, optional\n        The depth of the layer in hPa. Defaults to the full profile if bottom is not given,\n        and 100 hPa if bottom is given.\n\n    Returns\n    -------\n    `pint.Quantity`\n        The thickness of the layer in meters.\n\n    See Also\n    --------\n    thickness_hydrostatic, pressure_to_height_std, virtual_temperature,\n    mixing_ratio_from_relative_humidity\n\n    \"\"\"\n    bottom = kwargs.pop('bottom', None)\n    depth = kwargs.pop('depth', None)\n    mixing = mixing_ratio_from_relative_humidity(relative_humidity, temperature, pressure)\n\n    return thickness_hydrostatic(pressure, temperature, mixing=mixing, bottom=bottom,\n                                 depth=depth)", "response": "r Calculates the thickness of a layer given pressure temperature and relative humidity."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef brunt_vaisala_frequency_squared(heights, potential_temperature, axis=0):\n    # Ensure validity of temperature units\n    potential_temperature = potential_temperature.to('K')\n\n    # Calculate and return the square of Brunt-Vaisala frequency\n    return mpconsts.g / potential_temperature * first_derivative(potential_temperature,\n                                                                 x=heights, axis=axis)", "response": "r Calculates the square of the Brunt - Visala frequency."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef brunt_vaisala_frequency(heights, potential_temperature, axis=0):\n    bv_freq_squared = brunt_vaisala_frequency_squared(heights, potential_temperature,\n                                                      axis=axis)\n    bv_freq_squared[bv_freq_squared.magnitude < 0] = np.nan\n\n    return np.sqrt(bv_freq_squared)", "response": "r Calculates the Brunt - Visala frequency for a given set of heights and potential temperature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the wet - bulb temperature using Normand s rule.", "response": "def wet_bulb_temperature(pressure, temperature, dewpoint):\n    \"\"\"Calculate the wet-bulb temperature using Normand's rule.\n\n    This function calculates the wet-bulb temperature using the Normand method. The LCL is\n    computed, and that parcel brought down to the starting pressure along a moist adiabat.\n    The Normand method (and others) are described and compared by [Knox2017]_.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Initial atmospheric pressure\n    temperature : `pint.Quantity`\n        Initial atmospheric temperature\n    dewpoint : `pint.Quantity`\n        Initial atmospheric dewpoint\n\n    Returns\n    -------\n    array-like\n        Wet-bulb temperature\n\n    See Also\n    --------\n    lcl, moist_lapse\n\n    \"\"\"\n    if not hasattr(pressure, 'shape'):\n        pressure = atleast_1d(pressure)\n        temperature = atleast_1d(temperature)\n        dewpoint = atleast_1d(dewpoint)\n\n    it = np.nditer([pressure, temperature, dewpoint, None],\n                   op_dtypes=['float', 'float', 'float', 'float'],\n                   flags=['buffered'])\n\n    for press, temp, dewp, ret in it:\n        press = press * pressure.units\n        temp = temp * temperature.units\n        dewp = dewp * dewpoint.units\n        lcl_pressure, lcl_temperature = lcl(press, temp, dewp)\n        moist_adiabat_temperatures = moist_lapse(concatenate([lcl_pressure, press]),\n                                                 lcl_temperature)\n        ret[...] = moist_adiabat_temperatures[-1]\n\n    # If we started with a scalar, return a scalar\n    if it.operands[3].size == 1:\n        return it.operands[3][0] * moist_adiabat_temperatures.units\n    return it.operands[3] * moist_adiabat_temperatures.units"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef static_stability(pressure, temperature, axis=0):\n    theta = potential_temperature(pressure, temperature)\n\n    return - mpconsts.Rd * temperature / pressure * first_derivative(np.log(theta / units.K),\n                                                                     x=pressure, axis=axis)", "response": "r Calculates the static stability within a vertical profile."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef vertical_velocity(omega, pressure, temperature, mixing=0):\n    rho = density(pressure, temperature, mixing)\n    return (omega / (- mpconsts.g * rho)).to('m/s')", "response": "r Calculates the vertical velocity of a virtual\n    assuming hydrostatic conditions on the virtual sequence."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef geometries(self):\n        # Ensure that the associated files are in the cache\n        fname = '{}_{}'.format(self.name, self.scale)\n        for extension in ['.dbf', '.shx']:\n            get_test_data(fname + extension)\n        path = get_test_data(fname + '.shp', as_file_obj=False)\n        return iter(tuple(shpreader.Reader(path).geometries()))", "response": "Return an iterator of ( shapely ) geometries for this feature."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nplots wind speeds and wind direction.", "response": "def plot_winds(self, ws, wd, wsmax, plot_range=None):\n        \"\"\"\n        Required input:\n            ws: Wind speeds (knots)\n            wd: Wind direction (degrees)\n            wsmax: Wind gust (knots)\n        Optional Input:\n            plot_range: Data range for making figure (list of (min,max,step))\n        \"\"\"\n        # PLOT WIND SPEED AND WIND DIRECTION\n        self.ax1 = fig.add_subplot(4, 1, 1)\n        ln1 = self.ax1.plot(self.dates, ws, label='Wind Speed')\n        self.ax1.fill_between(self.dates, ws, 0)\n        self.ax1.set_xlim(self.start, self.end)\n        if not plot_range:\n            plot_range = [0, 20, 1]\n        self.ax1.set_ylabel('Wind Speed (knots)', multialignment='center')\n        self.ax1.set_ylim(plot_range[0], plot_range[1], plot_range[2])\n        self.ax1.grid(b=True, which='major', axis='y', color='k', linestyle='--',\n                      linewidth=0.5)\n        ln2 = self.ax1.plot(self.dates, wsmax, '.r', label='3-sec Wind Speed Max')\n\n        ax7 = self.ax1.twinx()\n        ln3 = ax7.plot(self.dates, wd, '.k', linewidth=0.5, label='Wind Direction')\n        ax7.set_ylabel('Wind\\nDirection\\n(degrees)', multialignment='center')\n        ax7.set_ylim(0, 360)\n        ax7.set_yticks(np.arange(45, 405, 90), ['NE', 'SE', 'SW', 'NW'])\n        lns = ln1 + ln2 + ln3\n        labs = [l.get_label() for l in lns]\n        ax7.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%H UTC'))\n        ax7.legend(lns, labs, loc='upper center',\n                   bbox_to_anchor=(0.5, 1.2), ncol=3, prop={'size': 12})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef plot_thermo(self, t, td, plot_range=None):\n        # PLOT TEMPERATURE AND DEWPOINT\n        if not plot_range:\n            plot_range = [10, 90, 2]\n        self.ax2 = fig.add_subplot(4, 1, 2, sharex=self.ax1)\n        ln4 = self.ax2.plot(self.dates, t, 'r-', label='Temperature')\n        self.ax2.fill_between(self.dates, t, td, color='r')\n\n        self.ax2.set_ylabel('Temperature\\n(F)', multialignment='center')\n        self.ax2.grid(b=True, which='major', axis='y', color='k', linestyle='--',\n                      linewidth=0.5)\n        self.ax2.set_ylim(plot_range[0], plot_range[1], plot_range[2])\n\n        ln5 = self.ax2.plot(self.dates, td, 'g-', label='Dewpoint')\n        self.ax2.fill_between(self.dates, td, self.ax2.get_ylim()[0], color='g')\n\n        ax_twin = self.ax2.twinx()\n        ax_twin.set_ylim(plot_range[0], plot_range[1], plot_range[2])\n        lns = ln4 + ln5\n        labs = [l.get_label() for l in lns]\n        ax_twin.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%H UTC'))\n\n        self.ax2.legend(lns, labs, loc='upper center',\n                        bbox_to_anchor=(0.5, 1.2), ncol=2, prop={'size': 12})", "response": "Plot a thermo entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_rh(self, rh, plot_range=None):\n        # PLOT RELATIVE HUMIDITY\n        if not plot_range:\n            plot_range = [0, 100, 4]\n        self.ax3 = fig.add_subplot(4, 1, 3, sharex=self.ax1)\n        self.ax3.plot(self.dates, rh, 'g-', label='Relative Humidity')\n        self.ax3.legend(loc='upper center', bbox_to_anchor=(0.5, 1.22), prop={'size': 12})\n        self.ax3.grid(b=True, which='major', axis='y', color='k', linestyle='--',\n                      linewidth=0.5)\n        self.ax3.set_ylim(plot_range[0], plot_range[1], plot_range[2])\n\n        self.ax3.fill_between(self.dates, rh, self.ax3.get_ylim()[0], color='g')\n        self.ax3.set_ylabel('Relative Humidity\\n(%)', multialignment='center')\n        self.ax3.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%H UTC'))\n        axtwin = self.ax3.twinx()\n        axtwin.set_ylim(plot_range[0], plot_range[1], plot_range[2])", "response": "Plot a relative humidity."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_pressure(self, p, plot_range=None):\n        # PLOT PRESSURE\n        if not plot_range:\n            plot_range = [970, 1030, 2]\n        self.ax4 = fig.add_subplot(4, 1, 4, sharex=self.ax1)\n        self.ax4.plot(self.dates, p, 'm', label='Mean Sea Level Pressure')\n        self.ax4.set_ylabel('Mean Sea\\nLevel Pressure\\n(mb)', multialignment='center')\n        self.ax4.set_ylim(plot_range[0], plot_range[1], plot_range[2])\n\n        axtwin = self.ax4.twinx()\n        axtwin.set_ylim(plot_range[0], plot_range[1], plot_range[2])\n        axtwin.fill_between(self.dates, p, axtwin.get_ylim()[0], color='m')\n        axtwin.xaxis.set_major_formatter(mpl.dates.DateFormatter('%d/%H UTC'))\n\n        self.ax4.legend(loc='upper center', bbox_to_anchor=(0.5, 1.2), prop={'size': 12})\n        self.ax4.grid(b=True, which='major', axis='y', color='k', linestyle='--',\n                      linewidth=0.5)", "response": "Plot the Mean Sea Level Pressure for a specific entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a geostationary projection object.", "response": "def make_geo(attrs_dict, globe):\n    \"\"\"Handle geostationary projection.\"\"\"\n    attr_mapping = [('satellite_height', 'perspective_point_height'),\n                    ('sweep_axis', 'sweep_angle_axis')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n\n    # CartoPy can't handle central latitude for Geostationary (nor should it)\n    # Just remove it if it's 0.\n    if not kwargs.get('central_latitude'):\n        kwargs.pop('central_latitude', None)\n\n    # If sweep_angle_axis is not present, we should look for fixed_angle_axis and adjust\n    if 'sweep_axis' not in kwargs:\n        kwargs['sweep_axis'] = 'x' if attrs_dict['fixed_angle_axis'] == 'y' else 'y'\n\n    return ccrs.Geostationary(globe=globe, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a Lambert Conformal projection object.", "response": "def make_lcc(attrs_dict, globe):\n    \"\"\"Handle Lambert conformal conic projection.\"\"\"\n    attr_mapping = [('central_longitude', 'longitude_of_central_meridian'),\n                    ('standard_parallels', 'standard_parallel')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n    if 'standard_parallels' in kwargs:\n        try:\n            len(kwargs['standard_parallels'])\n        except TypeError:\n            kwargs['standard_parallels'] = [kwargs['standard_parallels']]\n    return ccrs.LambertConformal(globe=globe, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_stereo(attrs_dict, globe):\n    attr_mapping = [('scale_factor', 'scale_factor_at_projection_origin')]\n    kwargs = CFProjection.build_projection_kwargs(attrs_dict, attr_mapping)\n\n    return ccrs.Stereographic(globe=globe, **kwargs)", "response": "Handle generic stereographic projection."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles mapping a dictionary of metadata to keyword arguments.", "response": "def build_projection_kwargs(cls, source, mapping):\n        \"\"\"Handle mapping a dictionary of metadata to keyword arguments.\"\"\"\n        return cls._map_arg_names(source, cls._default_attr_mapping + mapping)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _map_arg_names(source, mapping):\n        return {cartopy_name: source[cf_name] for cartopy_name, cf_name in mapping\n                if cf_name in source}", "response": "Map one set of keys to another."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes a cartopy. ccrs. Globe from the metadata.", "response": "def cartopy_globe(self):\n        \"\"\"Initialize a `cartopy.crs.Globe` from the metadata.\"\"\"\n        if 'earth_radius' in self._attrs:\n            kwargs = {'ellipse': 'sphere', 'semimajor_axis': self._attrs['earth_radius'],\n                      'semiminor_axis': self._attrs['earth_radius']}\n        else:\n            attr_mapping = [('semimajor_axis', 'semi_major_axis'),\n                            ('semiminor_axis', 'semi_minor_axis'),\n                            ('inverse_flattening', 'inverse_flattening')]\n            kwargs = self._map_arg_names(self._attrs, attr_mapping)\n\n            # WGS84 with semi_major==semi_minor is NOT the same as spherical Earth\n            # Also need to handle the case where we're not given any spheroid\n            kwargs['ellipse'] = None if kwargs else 'sphere'\n\n        return ccrs.Globe(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_cartopy(self):\n        globe = self.cartopy_globe\n        proj_name = self._attrs['grid_mapping_name']\n        try:\n            proj_handler = self.projection_registry[proj_name]\n        except KeyError:\n            raise ValueError('Unhandled projection: {}'.format(proj_name))\n\n        return proj_handler(self._attrs, globe)", "response": "Convert to a CartoPy projection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_timestamp(ax, time=None, x=0.99, y=-0.04, ha='right', high_contrast=False,\n                  pretext='Created: ', time_format='%Y-%m-%dT%H:%M:%SZ', **kwargs):\n    \"\"\"Add a timestamp to a plot.\n\n    Adds a timestamp to a plot, defaulting to the time of plot creation in ISO format.\n\n    Parameters\n    ----------\n    ax : `matplotlib.axes.Axes`\n        The `Axes` instance used for plotting\n    time : `datetime.datetime`\n        Specific time to be plotted - datetime.utcnow will be use if not specified\n    x : float\n        Relative x position on the axes of the timestamp\n    y : float\n        Relative y position on the axes of the timestamp\n    ha : str\n        Horizontal alignment of the time stamp string\n    high_contrast : bool\n        Outline text for increased contrast\n    pretext : str\n        Text to appear before the timestamp, optional. Defaults to 'Created: '\n    time_format : str\n        Display format of time, optional. Defaults to ISO format.\n\n    Returns\n    -------\n    `matplotlib.text.Text`\n        The `matplotlib.text.Text` instance created\n\n    \"\"\"\n    if high_contrast:\n        text_args = {'color': 'white',\n                     'path_effects':\n                         [mpatheffects.withStroke(linewidth=2, foreground='black')]}\n    else:\n        text_args = {}\n    text_args.update(**kwargs)\n    if not time:\n        time = datetime.utcnow()\n    timestr = pretext + time.strftime(time_format)\n    return ax.text(x, y, timestr, ha=ha, transform=ax.transAxes, **text_args)", "response": "Adds a timestamp to a plot."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_logo(fig, x=10, y=25, zorder=100, which='metpy', size='small', **kwargs):\n    fname_suffix = {'small': '_75x75.png',\n                    'large': '_150x150.png'}\n    fname_prefix = {'unidata': 'unidata',\n                    'metpy': 'metpy'}\n    try:\n        fname = fname_prefix[which] + fname_suffix[size]\n        fpath = posixpath.join('_static', fname)\n    except KeyError:\n        raise ValueError('Unknown logo size or selection')\n\n    logo = imread(pkg_resources.resource_stream('metpy.plots', fpath))\n    return fig.figimage(logo, x, y, zorder=zorder, **kwargs)", "response": "Adds the MetPy or Unidata logo to a figure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_metpy_logo(fig, x=10, y=25, zorder=100, size='small', **kwargs):\n    return _add_logo(fig, x=x, y=y, zorder=zorder, which='metpy', size=size, **kwargs)", "response": "Adds a MetPy logo to a figure."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a multi - colored line from a set of points and a color - mapping.", "response": "def colored_line(x, y, c, **kwargs):\n    \"\"\"Create a multi-colored line.\n\n    Takes a set of points and turns them into a collection of lines colored by another array.\n\n    Parameters\n    ----------\n    x : array-like\n        x-axis coordinates\n    y : array-like\n        y-axis coordinates\n    c : array-like\n        values used for color-mapping\n    kwargs : dict\n        Other keyword arguments passed to :class:`matplotlib.collections.LineCollection`\n\n    Returns\n    -------\n        The created :class:`matplotlib.collections.LineCollection` instance.\n\n    \"\"\"\n    # Mask out any NaN values\n    nan_mask = ~(np.isnan(x) | np.isnan(y) | np.isnan(c))\n    x = x[nan_mask]\n    y = y[nan_mask]\n    c = c[nan_mask]\n\n    # Paste values end to end\n    points = concatenate([x, y])\n\n    # Exploit numpy's strides to present a view of these points without copying.\n    # Dimensions are (segment, start/end, x/y). Since x and y are concatenated back to back,\n    # moving between segments only moves one item; moving start to end is only an item;\n    # The move between x any moves from one half of the array to the other\n    num_pts = points.size // 2\n    final_shape = (num_pts - 1, 2, 2)\n    final_strides = (points.itemsize, points.itemsize, num_pts * points.itemsize)\n    segments = np.lib.stride_tricks.as_strided(points, shape=final_shape,\n                                               strides=final_strides)\n\n    # Create a LineCollection from the segments and set it to colormap based on c\n    lc = LineCollection(segments, **kwargs)\n    lc.set_array(c)\n    return lc"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef convert_gempak_color(c, style='psc'):\n    def normalize(x):\n        \"\"\"Transform input x to an int in range 0 to 31 consistent with GEMPAK color quirks.\"\"\"\n        x = int(x)\n        if x < 0 or x == 101:\n            x = 0\n        else:\n            x = x % 32\n        return x\n\n    # Define GEMPAK colors (Matplotlib doesn't appear to like numbered variants)\n    cols = ['white',       # 0/32\n            'black',       # 1\n            'red',         # 2\n            'green',       # 3\n            'blue',        # 4\n            'yellow',      # 5\n            'cyan',        # 6\n            'magenta',     # 7\n            '#CD6839',     # 8 (sienna3)\n            '#FF8247',     # 9 (sienna1)\n            '#FFA54F',     # 10 (tan1)\n            '#FFAEB9',     # 11 (LightPink1)\n            '#FF6A6A',     # 12 (IndianRed1)\n            '#EE2C2C',     # 13 (firebrick2)\n            '#8B0000',     # 14 (red4)\n            '#CD0000',     # 15 (red3)\n            '#EE4000',     # 16 (OrangeRed2)\n            '#FF7F00',     # 17 (DarkOrange1)\n            '#CD8500',     # 18 (orange3)\n            'gold',        # 19\n            '#EEEE00',     # 20 (yellow2)\n            'chartreuse',  # 21\n            '#00CD00',     # 22 (green3)\n            '#008B00',     # 23 (green4)\n            '#104E8B',     # 24 (DodgerBlue4)\n            'DodgerBlue',  # 25\n            '#00B2EE',     # 26 (DeepSkyBlue2)\n            '#00EEEE',     # 27 (cyan2)\n            '#8968CD',     # 28 (MediumPurple3)\n            '#912CEE',     # 29 (purple2)\n            '#8B008B',     # 30 (magenta4)\n            'bisque']      # 31\n\n    if style != 'psc':\n        if style == 'xw':\n            cols[0] = 'black'\n            cols[1] = 'bisque'\n            cols[31] = 'white'\n        else:\n            raise ValueError('Unknown style parameter')\n\n    try:\n        c_list = list(c)\n        res = [cols[normalize(x)] for x in c_list]\n    except TypeError:\n        res = cols[normalize(c)]\n    return res", "response": "Convert GEMPAK color numbers into corresponding Matplotlib colors."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling information for message type 3.", "response": "def process_msg3(fname):\n    \"\"\"Handle information for message type 3.\"\"\"\n    with open(fname, 'r') as infile:\n        info = []\n        for lineno, line in enumerate(infile):\n            parts = line.split('  ')\n            try:\n                var_name, desc, typ, units = parts[:4]\n                size_hw = parts[-1]\n                if '-' in size_hw:\n                    start, end = map(int, size_hw.split('-'))\n                    size = (end - start + 1) * 2\n                else:\n                    size = 2\n\n                assert size >= 2\n                fmt = fix_type(typ, size)\n\n                var_name = fix_var_name(var_name)\n                full_desc = fix_desc(desc, units)\n\n                info.append({'name': var_name, 'desc': full_desc, 'fmt': fmt})\n\n                if ignored_item(info[-1]) and var_name != 'Spare':\n                    warnings.warn('{} has type {}. Setting as Spare'.format(var_name, typ))\n\n            except (ValueError, AssertionError):\n                warnings.warn('{} > {}'.format(lineno + 1, ':'.join(parts)))\n                raise\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_msg18(fname):\n    with open(fname, 'r') as infile:\n        info = []\n        for lineno, line in enumerate(infile):\n            parts = line.split('  ')\n            try:\n                if len(parts) == 8:\n                    parts = parts[:6] + [parts[6] + parts[7]]\n\n                var_name, desc, typ, units, rng, prec, byte_range = parts\n                start, end = map(int, byte_range.split('-'))\n                size = end - start + 1\n                assert size >= 4\n                fmt = fix_type(typ, size,\n                               additional=[('See Note (5)', ('{size}s', 1172))])\n\n                if ' ' in var_name:\n                    warnings.warn('Space in {}'.format(var_name))\n                if not desc:\n                    warnings.warn('null description for {}'.format(var_name))\n\n                var_name = fix_var_name(var_name)\n                full_desc = fix_desc(desc, units)\n\n                info.append({'name': var_name, 'desc': full_desc, 'fmt': fmt})\n\n                if (ignored_item(info[-1]) and var_name != 'SPARE'\n                        and 'SPARE' not in full_desc):\n                    warnings.warn('{} has type {}. Setting as SPARE'.format(var_name, typ))\n\n            except (ValueError, AssertionError):\n                warnings.warn('{} > {}'.format(lineno + 1, ':'.join(parts)))\n                raise\n        return info", "response": "Handle information for message type 18."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfixing up creating the appropriate struct type based on the information in the column.", "response": "def fix_type(typ, size, additional=None):\n    \"\"\"Fix up creating the appropriate struct type based on the information in the column.\"\"\"\n    if additional is not None:\n        my_types = types + additional\n    else:\n        my_types = types\n\n    for t, info in my_types:\n        if callable(t):\n            matches = t(typ)\n        else:\n            matches = t == typ\n\n        if matches:\n            if callable(info):\n                fmt_str, true_size = info(size)\n            else:\n                fmt_str, true_size = info\n            assert size == true_size, ('{}: Got size {} instead of {}'.format(typ, size,\n                                                                              true_size))\n            return fmt_str.format(size=size)\n\n    raise ValueError('No type match! ({})'.format(typ))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fix_var_name(var_name):\n    name = var_name.strip()\n    for char in '(). /#,':\n        name = name.replace(char, '_')\n    name = name.replace('+', 'pos_')\n    name = name.replace('-', 'neg_')\n    if name.endswith('_'):\n        name = name[:-1]\n    return name", "response": "Clean up and apply standard formatting to variable names."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclean up description column.", "response": "def fix_desc(desc, units=None):\n    \"\"\"Clean up description column.\"\"\"\n    full_desc = desc.strip()\n    if units and units != 'N/A':\n        if full_desc:\n            full_desc += ' (' + units + ')'\n        else:\n            full_desc = units\n    return full_desc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting out the generated Python code.", "response": "def write_file(fname, info):\n    \"\"\"Write out the generated Python code.\"\"\"\n    with open(fname, 'w') as outfile:\n        # File header\n        outfile.write('# Copyright (c) 2018 MetPy Developers.\\n')\n        outfile.write('# Distributed under the terms of the BSD 3-Clause License.\\n')\n        outfile.write('# SPDX-License-Identifier: BSD-3-Clause\\n\\n')\n        outfile.write('# flake8: noqa\\n')\n        outfile.write('# Generated file -- do not modify\\n')\n\n        # Variable descriptions\n        outfile.write('descriptions = {')\n        outdata = ',\\n                '.join('\"{name}\": \"{desc}\"'.format(\n            **i) for i in info if need_desc(i))\n        outfile.write(outdata)\n        outfile.write('}\\n\\n')\n\n        # Now the struct format\n        outfile.write('fields = [')\n        outdata = ',\\n          '.join('({fname}, \"{fmt}\")'.format(\n            fname=field_name(i), **i) for i in info)\n        outfile.write(outdata)\n        outfile.write(']\\n')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pandas_dataframe_to_unit_arrays(df, column_units=None):\n    if not column_units:\n        try:\n            column_units = df.units\n        except AttributeError:\n            raise ValueError('No units attribute attached to pandas '\n                             'dataframe and col_units not given.')\n\n    # Iterate through columns attaching units if we have them, if not, don't touch it\n    res = {}\n    for column in df:\n        if column in column_units and column_units[column]:\n            res[column] = df[column].values * units(column_units[column])\n        else:\n            res[column] = df[column].values\n    return res", "response": "Convert a pandas dataframe to a dictionary of united arrays."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef diff(x, **kwargs):\n    ret = np.diff(x, **kwargs)\n    if hasattr(x, 'units'):\n        # Can't just use units because of how things like temperature work\n        it = x.flat\n        true_units = (next(it) - next(it)).units\n        ret = ret * true_units\n    return ret", "response": "Calculate the n - th discrete difference along given axis."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef masked_array(data, data_units=None, **kwargs):\n    if data_units is None:\n        data_units = data.units\n    return units.Quantity(np.ma.masked_array(data, **kwargs), data_units)", "response": "Create a ~numpy. ma. MaskedArray with units attached to the result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _check_argument_units(args, dimensionality):\n    for arg, val in args.items():\n        # Get the needed dimensionality (for printing) as well as cached, parsed version\n        # for this argument.\n        try:\n            need, parsed = dimensionality[arg]\n        except KeyError:\n            # Argument did not have units specified in decorator\n            continue\n\n        # See if the value passed in is appropriate\n        try:\n            if val.dimensionality != parsed:\n                yield arg, val.units, need\n        # No dimensionality\n        except AttributeError:\n            # If this argument is dimensionless, don't worry\n            if parsed != '':\n                yield arg, 'none', need", "response": "Yield arguments with improper dimensionality."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a decorator to check the units of function arguments.", "response": "def check_units(*units_by_pos, **units_by_name):\n    \"\"\"Create a decorator to check units of function arguments.\"\"\"\n    try:\n        from inspect import signature\n\n        def dec(func):\n            # Match the signature of the function to the arguments given to the decorator\n            sig = signature(func)\n            bound_units = sig.bind_partial(*units_by_pos, **units_by_name)\n\n            # Convert our specified dimensionality (e.g. \"[pressure]\") to one used by\n            # pint directly (e.g. \"[mass] / [length] / [time]**2). This is for both efficiency\n            # reasons and to ensure that problems with the decorator are caught at import,\n            # rather than runtime.\n            dims = {name: (orig, units.get_dimensionality(orig.replace('dimensionless', '')))\n                    for name, orig in bound_units.arguments.items()}\n\n            @functools.wraps(func)\n            def wrapper(*args, **kwargs):\n                # Match all passed in value to their proper arguments so we can check units\n                bound_args = sig.bind(*args, **kwargs)\n                bad = list(_check_argument_units(bound_args.arguments, dims))\n\n                # If there are any bad units, emit a proper error message making it clear\n                # what went wrong.\n                if bad:\n                    msg = '`{0}` given arguments with incorrect units: {1}.'.format(\n                        func.__name__,\n                        ', '.join('`{}` requires \"{}\" but given \"{}\"'.format(arg, req, given)\n                                  for arg, given, req in bad))\n                    if 'none' in msg:\n                        msg += ('\\nAny variable `x` can be assigned a unit as follows:\\n'\n                                '    from metpy.units import units\\n'\n                                '    x = x * units.meter / units.second')\n                    raise ValueError(msg)\n                return func(*args, **kwargs)\n\n            return wrapper\n\n    # signature() only available on Python >= 3.3, so for 2.7 we just do nothing.\n    except ImportError:\n        def dec(func):\n            return func\n\n    return dec"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interpolate_to_slice(data, points, interp_type='linear'):\n    try:\n        x, y = data.metpy.coordinates('x', 'y')\n    except AttributeError:\n        raise ValueError('Required coordinate information not available. Verify that '\n                         'your data has been parsed by MetPy with proper x and y '\n                         'dimension coordinates.')\n\n    data_sliced = data.interp({\n        x.name: xr.DataArray(points[:, 0], dims='index', attrs=x.attrs),\n        y.name: xr.DataArray(points[:, 1], dims='index', attrs=y.attrs)\n    }, method=interp_type)\n    data_sliced.coords['index'] = range(len(points))\n\n    return data_sliced", "response": "r Returns an interpolated slice of data using xarray. DataArray. interp"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef geodesic(crs, start, end, steps):\n    import cartopy.crs as ccrs\n    from pyproj import Geod\n\n    # Geod.npts only gives points *in between* the start and end, and we want to include\n    # the endpoints.\n    g = Geod(crs.proj4_init)\n    geodesic = np.concatenate([\n        np.array(start[::-1])[None],\n        np.array(g.npts(start[1], start[0], end[1], end[0], steps - 2)),\n        np.array(end[::-1])[None]\n    ]).transpose()\n    points = crs.transform_points(ccrs.Geodetic(), *geodesic)[:, :2]\n\n    return points", "response": "r Constructs a geodesic path between two points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cross_section(data, start, end, steps=100, interp_type='linear'):\n    if isinstance(data, xr.Dataset):\n        # Recursively apply to dataset\n        return data.apply(cross_section, True, (start, end), steps=steps,\n                          interp_type=interp_type)\n    elif data.ndim == 0:\n        # This has no dimensions, so it is likely a projection variable. In any case, there\n        # are no data here to take the cross section with. Therefore, do nothing.\n        return data\n    else:\n\n        # Get the projection and coordinates\n        try:\n            crs_data = data.metpy.cartopy_crs\n            x = data.metpy.x\n        except AttributeError:\n            raise ValueError('Data missing required coordinate information. Verify that '\n                             'your data have been parsed by MetPy with proper x and y '\n                             'dimension coordinates and added crs coordinate of the '\n                             'correct projection for each variable.')\n\n        # Get the geodesic\n        points_cross = geodesic(crs_data, start, end, steps)\n\n        # Patch points_cross to match given longitude range, whether [0, 360) or (-180,  180]\n        if CFConventionHandler.check_axis(x, 'lon') and (x > 180).any():\n            points_cross[points_cross[:, 0] < 0, 0] += 360.\n\n        # Return the interpolated data\n        return interpolate_to_slice(data, points_cross, interp_type=interp_type)", "response": "r Returns an interpolated cross - sectional slice through a gridded data array."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndecorating a function to convert all DataArray arguments to pint. Quantities. This uses the metpy xarray accessors to do the actual conversion.", "response": "def preprocess_xarray(func):\n    \"\"\"Decorate a function to convert all DataArray arguments to pint.Quantities.\n\n    This uses the metpy xarray accessors to do the actual conversion.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        args = tuple(a.metpy.unit_array if isinstance(a, xr.DataArray) else a for a in args)\n        kwargs = {name: (v.metpy.unit_array if isinstance(v, xr.DataArray) else v)\n                  for name, v in kwargs.items()}\n        return func(*args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndecorating a function to make sure all given DataArrays have matching coordinates.", "response": "def check_matching_coordinates(func):\n    \"\"\"Decorate a function to make sure all given DataArrays have matching coordinates.\"\"\"\n    @functools.wraps(func)\n    def wrapper(*args, **kwargs):\n        data_arrays = ([a for a in args if isinstance(a, xr.DataArray)]\n                       + [a for a in kwargs.values() if isinstance(a, xr.DataArray)])\n        if len(data_arrays) > 1:\n            first = data_arrays[0]\n            for other in data_arrays[1:]:\n                if not first.metpy.coordinates_identical(other):\n                    raise ValueError('Input DataArray arguments must be on same coordinates.')\n        return func(*args, **kwargs)\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreassign a units. Quantity indexer to units of relevant coordinate.", "response": "def _reassign_quantity_indexer(data, indexers):\n    \"\"\"Reassign a units.Quantity indexer to units of relevant coordinate.\"\"\"\n    def _to_magnitude(val, unit):\n        try:\n            return val.to(unit).m\n        except AttributeError:\n            return val\n\n    for coord_name in indexers:\n        # Handle axis types for DataArrays\n        if (isinstance(data, xr.DataArray) and coord_name not in data.dims\n                and coord_name in readable_to_cf_axes):\n            axis = coord_name\n            coord_name = next(data.metpy.coordinates(axis)).name\n            indexers[coord_name] = indexers[axis]\n            del indexers[axis]\n\n        # Handle slices of quantities\n        if isinstance(indexers[coord_name], slice):\n            start = _to_magnitude(indexers[coord_name].start, data[coord_name].metpy.units)\n            stop = _to_magnitude(indexers[coord_name].stop, data[coord_name].metpy.units)\n            step = _to_magnitude(indexers[coord_name].step, data[coord_name].metpy.units)\n            indexers[coord_name] = slice(start, stop, step)\n\n        # Handle quantities\n        indexers[coord_name] = _to_magnitude(indexers[coord_name],\n                                             data[coord_name].metpy.units)\n\n    return indexers"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef resample_nn_1d(a, centers):\n    ix = []\n    for center in centers:\n        index = (np.abs(a - center)).argmin()\n        if index not in ix:\n            ix.append(index)\n    return ix", "response": "Return one - dimensional nearest - neighbor indexes based on user - specified centers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine the index of the point just before two lines with common x values.", "response": "def nearest_intersection_idx(a, b):\n    \"\"\"Determine the index of the point just before two lines with common x values.\n\n    Parameters\n    ----------\n    a : array-like\n        1-dimensional array of y-values for line 1\n    b : array-like\n        1-dimensional array of y-values for line 2\n\n    Returns\n    -------\n        An array of indexes representing the index of the values\n        just before the intersection(s) of the two lines.\n\n    \"\"\"\n    # Difference in the two y-value sets\n    difference = a - b\n\n    # Determine the point just before the intersection of the lines\n    # Will return multiple points for multiple intersections\n    sign_change_idx, = np.nonzero(np.diff(np.sign(difference)))\n\n    return sign_change_idx"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_intersections(x, a, b, direction='all'):\n    # Find the index of the points just before the intersection(s)\n    nearest_idx = nearest_intersection_idx(a, b)\n    next_idx = nearest_idx + 1\n\n    # Determine the sign of the change\n    sign_change = np.sign(a[next_idx] - b[next_idx])\n\n    # x-values around each intersection\n    _, x0 = _next_non_masked_element(x, nearest_idx)\n    _, x1 = _next_non_masked_element(x, next_idx)\n\n    # y-values around each intersection for the first line\n    _, a0 = _next_non_masked_element(a, nearest_idx)\n    _, a1 = _next_non_masked_element(a, next_idx)\n\n    # y-values around each intersection for the second line\n    _, b0 = _next_non_masked_element(b, nearest_idx)\n    _, b1 = _next_non_masked_element(b, next_idx)\n\n    # Calculate the x-intersection. This comes from finding the equations of the two lines,\n    # one through (x0, a0) and (x1, a1) and the other through (x0, b0) and (x1, b1),\n    # finding their intersection, and reducing with a bunch of algebra.\n    delta_y0 = a0 - b0\n    delta_y1 = a1 - b1\n    intersect_x = (delta_y1 * x0 - delta_y0 * x1) / (delta_y1 - delta_y0)\n\n    # Calculate the y-intersection of the lines. Just plug the x above into the equation\n    # for the line through the a points. One could solve for y like x above, but this\n    # causes weirder unit behavior and seems a little less good numerically.\n    intersect_y = ((intersect_x - x0) / (x1 - x0)) * (a1 - a0) + a0\n\n    # If there's no intersections, return\n    if len(intersect_x) == 0:\n        return intersect_x, intersect_y\n\n    # Check for duplicates\n    duplicate_mask = (np.ediff1d(intersect_x, to_end=1) != 0)\n\n    # Make a mask based on the direction of sign change desired\n    if direction == 'increasing':\n        mask = sign_change > 0\n    elif direction == 'decreasing':\n        mask = sign_change < 0\n    elif direction == 'all':\n        return intersect_x[duplicate_mask], intersect_y[duplicate_mask]\n    else:\n        raise ValueError('Unknown option for direction: {0}'.format(str(direction)))\n\n    return intersect_x[mask & duplicate_mask], intersect_y[mask & duplicate_mask]", "response": "Finds the intersection of two data sets that share a common x - value set."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the next non - masked element of an array.", "response": "def _next_non_masked_element(a, idx):\n    \"\"\"Return the next non masked element of a masked array.\n\n    If an array is masked, return the next non-masked element (if the given index is masked).\n    If no other unmasked points are after the given masked point, returns none.\n\n    Parameters\n    ----------\n    a : array-like\n        1-dimensional array of numeric values\n    idx : integer\n        index of requested element\n\n    Returns\n    -------\n        Index of next non-masked element and next non-masked element\n\n    \"\"\"\n    try:\n        next_idx = idx + a[idx:].mask.argmin()\n        if ma.is_masked(a[next_idx]):\n            return None, None\n        else:\n            return next_idx, a[next_idx]\n    except (AttributeError, TypeError, IndexError):\n        return idx, a[idx]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete masked points from arrays and returns a new array - like object.", "response": "def _delete_masked_points(*arrs):\n    \"\"\"Delete masked points from arrays.\n\n    Takes arrays and removes masked points to help with calculations and plotting.\n\n    Parameters\n    ----------\n    arrs : one or more array-like\n        source arrays\n\n    Returns\n    -------\n    arrs : one or more array-like\n        arrays with masked elements removed\n\n    \"\"\"\n    if any(hasattr(a, 'mask') for a in arrs):\n        keep = ~functools.reduce(np.logical_or, (np.ma.getmaskarray(a) for a in arrs))\n        return tuple(ma.asarray(a[keep]) for a in arrs)\n    else:\n        return arrs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the bounding pressure and height in a node.", "response": "def _get_bound_pressure_height(pressure, bound, heights=None, interpolate=True):\n    \"\"\"Calculate the bounding pressure and height in a layer.\n\n    Given pressure, optional heights, and a bound, return either the closest pressure/height\n    or interpolated pressure/height. If no heights are provided, a standard atmosphere is\n    assumed.\n\n    Parameters\n    ----------\n    pressure : `pint.Quantity`\n        Atmospheric pressures\n    bound : `pint.Quantity`\n        Bound to retrieve (in pressure or height)\n    heights : `pint.Quantity`, optional\n        Atmospheric heights associated with the pressure levels. Defaults to using\n        heights calculated from ``pressure`` assuming a standard atmosphere.\n    interpolate : boolean, optional\n        Interpolate the bound or return the nearest. Defaults to True.\n\n    Returns\n    -------\n    `pint.Quantity`\n        The bound pressure and height.\n\n    \"\"\"\n    # Make sure pressure is monotonically decreasing\n    sort_inds = np.argsort(pressure)[::-1]\n    pressure = pressure[sort_inds]\n    if heights is not None:\n        heights = heights[sort_inds]\n\n    # Bound is given in pressure\n    if bound.dimensionality == {'[length]': -1.0, '[mass]': 1.0, '[time]': -2.0}:\n        # If the bound is in the pressure data, we know the pressure bound exactly\n        if bound in pressure:\n            bound_pressure = bound\n            # If we have heights, we know the exact height value, otherwise return standard\n            # atmosphere height for the pressure\n            if heights is not None:\n                bound_height = heights[pressure == bound_pressure]\n            else:\n                bound_height = pressure_to_height_std(bound_pressure)\n        # If bound is not in the data, return the nearest or interpolated values\n        else:\n            if interpolate:\n                bound_pressure = bound  # Use the user specified bound\n                if heights is not None:  # Interpolate heights from the height data\n                    bound_height = log_interpolate_1d(bound_pressure, pressure, heights)\n                else:  # If not heights given, use the standard atmosphere\n                    bound_height = pressure_to_height_std(bound_pressure)\n            else:  # No interpolation, find the closest values\n                idx = (np.abs(pressure - bound)).argmin()\n                bound_pressure = pressure[idx]\n                if heights is not None:\n                    bound_height = heights[idx]\n                else:\n                    bound_height = pressure_to_height_std(bound_pressure)\n\n    # Bound is given in height\n    elif bound.dimensionality == {'[length]': 1.0}:\n        # If there is height data, see if we have the bound or need to interpolate/find nearest\n        if heights is not None:\n            if bound in heights:  # Bound is in the height data\n                bound_height = bound\n                bound_pressure = pressure[heights == bound]\n            else:  # Bound is not in the data\n                if interpolate:\n                    bound_height = bound\n\n                    # Need to cast back to the input type since interp (up to at least numpy\n                    # 1.13 always returns float64. This can cause upstream users problems,\n                    # resulting in something like np.append() to upcast.\n                    bound_pressure = np.interp(np.atleast_1d(bound), heights,\n                                               pressure).astype(bound.dtype) * pressure.units\n                else:\n                    idx = (np.abs(heights - bound)).argmin()\n                    bound_pressure = pressure[idx]\n                    bound_height = heights[idx]\n        else:  # Don't have heights, so assume a standard atmosphere\n            bound_height = bound\n            bound_pressure = height_to_pressure_std(bound)\n            # If interpolation is on, this is all we need, if not, we need to go back and\n            # find the pressure closest to this and refigure the bounds\n            if not interpolate:\n                idx = (np.abs(pressure - bound_pressure)).argmin()\n                bound_pressure = pressure[idx]\n                bound_height = pressure_to_height_std(bound_pressure)\n\n    # Bound has invalid units\n    else:\n        raise ValueError('Bound must be specified in units of length or pressure.')\n\n    # If the bound is out of the range of the data, we shouldn't extrapolate\n    if not (_greater_or_close(bound_pressure, np.nanmin(pressure) * pressure.units)\n            and _less_or_close(bound_pressure, np.nanmax(pressure) * pressure.units)):\n        raise ValueError('Specified bound is outside pressure range.')\n    if heights is not None:\n        if not (_less_or_close(bound_height, np.nanmax(heights) * heights.units)\n                and _greater_or_close(bound_height, np.nanmin(heights) * heights.units)):\n            raise ValueError('Specified bound is outside height range.')\n\n    return bound_pressure, bound_height"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_layer_heights(heights, depth, *args, **kwargs):\n    bottom = kwargs.pop('bottom', None)\n    interpolate = kwargs.pop('interpolate', True)\n    with_agl = kwargs.pop('with_agl', False)\n\n    # Make sure pressure and datavars are the same length\n    for datavar in args:\n        if len(heights) != len(datavar):\n            raise ValueError('Height and data variables must have the same length.')\n\n    # If we want things in AGL, subtract the minimum height from all height values\n    if with_agl:\n        sfc_height = np.min(heights)\n        heights = heights - sfc_height\n\n    # If the bottom is not specified, make it the surface\n    if bottom is None:\n        bottom = heights[0]\n\n    # Make heights and arguments base units\n    heights = heights.to_base_units()\n    bottom = bottom.to_base_units()\n\n    # Calculate the top of the layer\n    top = bottom + depth\n\n    ret = []  # returned data variables in layer\n\n    # Ensure heights are sorted in ascending order\n    sort_inds = np.argsort(heights)\n    heights = heights[sort_inds]\n\n    # Mask based on top and bottom\n    inds = _greater_or_close(heights, bottom) & _less_or_close(heights, top)\n    heights_interp = heights[inds]\n\n    # Interpolate heights at bounds if necessary and sort\n    if interpolate:\n        # If we don't have the bottom or top requested, append them\n        if top not in heights_interp:\n            heights_interp = np.sort(np.append(heights_interp, top)) * heights.units\n        if bottom not in heights_interp:\n            heights_interp = np.sort(np.append(heights_interp, bottom)) * heights.units\n\n    ret.append(heights_interp)\n\n    for datavar in args:\n        # Ensure that things are sorted in ascending order\n        datavar = datavar[sort_inds]\n\n        if interpolate:\n            # Interpolate for the possibly missing bottom/top values\n            datavar_interp = interpolate_1d(heights_interp, heights, datavar)\n            datavar = datavar_interp\n        else:\n            datavar = datavar[inds]\n\n        ret.append(datavar)\n    return ret", "response": "This function returns an atmospheric layer from upper air dataset with the requested bottom and depth."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_layer(pressure, *args, **kwargs):\n    # Pop off keyword arguments\n    heights = kwargs.pop('heights', None)\n    bottom = kwargs.pop('bottom', None)\n    depth = kwargs.pop('depth', 100 * units.hPa)\n    interpolate = kwargs.pop('interpolate', True)\n\n    # If we get the depth kwarg, but it's None, set it to the default as well\n    if depth is None:\n        depth = 100 * units.hPa\n\n    # Make sure pressure and datavars are the same length\n    for datavar in args:\n        if len(pressure) != len(datavar):\n            raise ValueError('Pressure and data variables must have the same length.')\n\n    # If the bottom is not specified, make it the surface pressure\n    if bottom is None:\n        bottom = np.nanmax(pressure) * pressure.units\n\n    bottom_pressure, bottom_height = _get_bound_pressure_height(pressure, bottom,\n                                                                heights=heights,\n                                                                interpolate=interpolate)\n\n    # Calculate the top if whatever units depth is in\n    if depth.dimensionality == {'[length]': -1.0, '[mass]': 1.0, '[time]': -2.0}:\n        top = bottom_pressure - depth\n    elif depth.dimensionality == {'[length]': 1}:\n        top = bottom_height + depth\n    else:\n        raise ValueError('Depth must be specified in units of length or pressure')\n\n    top_pressure, _ = _get_bound_pressure_height(pressure, top, heights=heights,\n                                                 interpolate=interpolate)\n\n    ret = []  # returned data variables in layer\n\n    # Ensure pressures are sorted in ascending order\n    sort_inds = np.argsort(pressure)\n    pressure = pressure[sort_inds]\n\n    # Mask based on top and bottom pressure\n    inds = (_less_or_close(pressure, bottom_pressure)\n            & _greater_or_close(pressure, top_pressure))\n    p_interp = pressure[inds]\n\n    # Interpolate pressures at bounds if necessary and sort\n    if interpolate:\n        # If we don't have the bottom or top requested, append them\n        if not np.any(np.isclose(top_pressure, p_interp)):\n            p_interp = np.sort(np.append(p_interp, top_pressure)) * pressure.units\n        if not np.any(np.isclose(bottom_pressure, p_interp)):\n            p_interp = np.sort(np.append(p_interp, bottom_pressure)) * pressure.units\n\n    ret.append(p_interp[::-1])\n\n    for datavar in args:\n        # Ensure that things are sorted in ascending order\n        datavar = datavar[sort_inds]\n\n        if interpolate:\n            # Interpolate for the possibly missing bottom/top values\n            datavar_interp = log_interpolate_1d(p_interp, pressure, datavar)\n            datavar = datavar_interp\n        else:\n            datavar = datavar[inds]\n\n        ret.append(datavar[::-1])\n    return ret", "response": "r Returns an atmospheric layer from the upper air dataset with the requested bottom and depth."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interp(x, xp, *args, **kwargs):\n    return interpolate_1d(x, xp, *args, **kwargs)", "response": "Wrap interpolate_1d for deprecated interp."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the indices surrounding the values within arr along the specified axis.", "response": "def find_bounding_indices(arr, values, axis, from_below=True):\n    \"\"\"Find the indices surrounding the values within arr along axis.\n\n    Returns a set of above, below, good. Above and below are lists of arrays of indices.\n    These lists are formulated such that they can be used directly to index into a numpy\n    array and get the expected results (no extra slices or ellipsis necessary). `good` is\n    a boolean array indicating the \"columns\" that actually had values to bound the desired\n    value(s).\n\n    Parameters\n    ----------\n    arr : array-like\n        Array to search for values\n\n    values: array-like\n        One or more values to search for in `arr`\n\n    axis : int\n        The dimension of `arr` along which to search.\n\n    from_below : bool, optional\n        Whether to search from \"below\" (i.e. low indices to high indices). If `False`,\n        the search will instead proceed from high indices to low indices. Defaults to `True`.\n\n    Returns\n    -------\n    above : list of arrays\n        List of broadcasted indices to the location above the desired value\n\n    below : list of arrays\n        List of broadcasted indices to the location below the desired value\n\n    good : array\n        Boolean array indicating where the search found proper bounds for the desired value\n\n    \"\"\"\n    # The shape of generated indices is the same as the input, but with the axis of interest\n    # replaced by the number of values to search for.\n    indices_shape = list(arr.shape)\n    indices_shape[axis] = len(values)\n\n    # Storage for the found indices and the mask for good locations\n    indices = np.empty(indices_shape, dtype=np.int)\n    good = np.empty(indices_shape, dtype=np.bool)\n\n    # Used to put the output in the proper location\n    store_slice = [slice(None)] * arr.ndim\n\n    # Loop over all of the values and for each, see where the value would be found from a\n    # linear search\n    for level_index, value in enumerate(values):\n        # Look for changes in the value of the test for <= value in consecutive points\n        # Taking abs() because we only care if there is a flip, not which direction.\n        switches = np.abs(np.diff((arr <= value).astype(np.int), axis=axis))\n\n        # Good points are those where it's not just 0's along the whole axis\n        good_search = np.any(switches, axis=axis)\n\n        if from_below:\n            # Look for the first switch; need to add 1 to the index since argmax is giving the\n            # index within the difference array, which is one smaller.\n            index = switches.argmax(axis=axis) + 1\n        else:\n            # Generate a list of slices to reverse the axis of interest so that searching from\n            # 0 to N is starting at the \"top\" of the axis.\n            arr_slice = [slice(None)] * arr.ndim\n            arr_slice[axis] = slice(None, None, -1)\n\n            # Same as above, but we use the slice to come from the end; then adjust those\n            # indices to measure from the front.\n            index = arr.shape[axis] - 1 - switches[tuple(arr_slice)].argmax(axis=axis)\n\n        # Set all indices where the results are not good to 0\n        index[~good_search] = 0\n\n        # Put the results in the proper slice\n        store_slice[axis] = level_index\n        indices[tuple(store_slice)] = index\n        good[tuple(store_slice)] = good_search\n\n    # Create index values for broadcasting arrays\n    above = broadcast_indices(arr, indices, arr.ndim, axis)\n    below = broadcast_indices(arr, indices - 1, arr.ndim, axis)\n\n    return above, below, good"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwrapping log_interp_1d for deprecated log_interp.", "response": "def log_interp(x, xp, *args, **kwargs):\n    \"\"\"Wrap log_interpolate_1d for deprecated log_interp.\"\"\"\n    return log_interpolate_1d(x, xp, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _greater_or_close(a, value, **kwargs):\n    return (a > value) | np.isclose(a, value, **kwargs)", "response": "r Compare values for greater or close to boolean masks."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lat_lon_grid_spacing(longitude, latitude, **kwargs):\n    # Use the absolute value of the signed function replacing this\n    dx, dy = lat_lon_grid_deltas(longitude, latitude, **kwargs)\n\n    return np.abs(dx), np.abs(dy)", "response": "r Calculates the distance between grid points that are in a latitude and longitude format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the horizontal deltas between grid points of a DataArray.", "response": "def grid_deltas_from_dataarray(f):\n    \"\"\"Calculate the horizontal deltas between grid points of a DataArray.\n\n    Calculate the signed delta distance between grid points of a DataArray in the horizontal\n    directions, whether the grid is lat/lon or x/y.\n\n    Parameters\n    ----------\n    f : `xarray.DataArray`\n        Parsed DataArray on a latitude/longitude grid, in (..., lat, lon) or (..., y, x)\n        dimension order\n\n    Returns\n    -------\n    dx, dy:\n        arrays of signed deltas between grid points in the x and y directions with dimensions\n        matching those of `f`.\n\n    See Also\n    --------\n    lat_lon_grid_deltas\n\n    \"\"\"\n    if f.metpy.crs['grid_mapping_name'] == 'latitude_longitude':\n        dx, dy = lat_lon_grid_deltas(f.metpy.x, f.metpy.y,\n                                     initstring=f.metpy.cartopy_crs.proj4_init)\n        slc_x = slc_y = tuple([np.newaxis] * (f.ndim - 2) + [slice(None)] * 2)\n    else:\n        dx = np.diff(f.metpy.x.metpy.unit_array.to('m').magnitude) * units('m')\n        dy = np.diff(f.metpy.y.metpy.unit_array.to('m').magnitude) * units('m')\n        slc = [np.newaxis] * (f.ndim - 2)\n        slc_x = tuple(slc + [np.newaxis, slice(None)])\n        slc_y = tuple(slc + [slice(None), np.newaxis])\n    return dx[slc_x], dy[slc_y]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndecorates the derivative functions to make them work nicely with DataArrays.", "response": "def xarray_derivative_wrap(func):\n    \"\"\"Decorate the derivative functions to make them work nicely with DataArrays.\n\n    This will automatically determine if the coordinates can be pulled directly from the\n    DataArray, or if a call to lat_lon_grid_deltas is needed.\n    \"\"\"\n    @functools.wraps(func)\n    def wrapper(f, **kwargs):\n        if 'x' in kwargs or 'delta' in kwargs:\n            # Use the usual DataArray to pint.Quantity preprocessing wrapper\n            return preprocess_xarray(func)(f, **kwargs)\n        elif isinstance(f, xr.DataArray):\n            # Get axis argument, defaulting to first dimension\n            axis = f.metpy.find_axis_name(kwargs.get('axis', 0))\n\n            # Initialize new kwargs with the axis number\n            new_kwargs = {'axis': f.get_axis_num(axis)}\n\n            if f[axis].attrs.get('_metpy_axis') == 'T':\n                # Time coordinate, need to convert to seconds from datetimes\n                new_kwargs['x'] = f[axis].metpy.as_timestamp().metpy.unit_array\n            elif CFConventionHandler.check_axis(f[axis], 'lon'):\n                # Longitude coordinate, need to get grid deltas\n                new_kwargs['delta'], _ = grid_deltas_from_dataarray(f)\n            elif CFConventionHandler.check_axis(f[axis], 'lat'):\n                # Latitude coordinate, need to get grid deltas\n                _, new_kwargs['delta'] = grid_deltas_from_dataarray(f)\n            else:\n                # General coordinate, use as is\n                new_kwargs['x'] = f[axis].metpy.unit_array\n\n            # Calculate and return result as a DataArray\n            result = func(f.metpy.unit_array, **new_kwargs)\n            return xr.DataArray(result.magnitude,\n                                coords=f.coords,\n                                dims=f.dims,\n                                attrs={'units': str(result.units)})\n        else:\n            # Error\n            raise ValueError('Must specify either \"x\" or \"delta\" for value positions when \"f\" '\n                             'is not a DataArray.')\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef first_derivative(f, **kwargs):\n    n, axis, delta = _process_deriv_args(f, kwargs)\n\n    # create slice objects --- initially all are [:, :, ..., :]\n    slice0 = [slice(None)] * n\n    slice1 = [slice(None)] * n\n    slice2 = [slice(None)] * n\n    delta_slice0 = [slice(None)] * n\n    delta_slice1 = [slice(None)] * n\n\n    # First handle centered case\n    slice0[axis] = slice(None, -2)\n    slice1[axis] = slice(1, -1)\n    slice2[axis] = slice(2, None)\n    delta_slice0[axis] = slice(None, -1)\n    delta_slice1[axis] = slice(1, None)\n\n    combined_delta = delta[tuple(delta_slice0)] + delta[tuple(delta_slice1)]\n    delta_diff = delta[tuple(delta_slice1)] - delta[tuple(delta_slice0)]\n    center = (- delta[tuple(delta_slice1)] / (combined_delta * delta[tuple(delta_slice0)])\n              * f[tuple(slice0)]\n              + delta_diff / (delta[tuple(delta_slice0)] * delta[tuple(delta_slice1)])\n              * f[tuple(slice1)]\n              + delta[tuple(delta_slice0)] / (combined_delta * delta[tuple(delta_slice1)])\n              * f[tuple(slice2)])\n\n    # Fill in \"left\" edge with forward difference\n    slice0[axis] = slice(None, 1)\n    slice1[axis] = slice(1, 2)\n    slice2[axis] = slice(2, 3)\n    delta_slice0[axis] = slice(None, 1)\n    delta_slice1[axis] = slice(1, 2)\n\n    combined_delta = delta[tuple(delta_slice0)] + delta[tuple(delta_slice1)]\n    big_delta = combined_delta + delta[tuple(delta_slice0)]\n    left = (- big_delta / (combined_delta * delta[tuple(delta_slice0)])\n            * f[tuple(slice0)]\n            + combined_delta / (delta[tuple(delta_slice0)] * delta[tuple(delta_slice1)])\n            * f[tuple(slice1)]\n            - delta[tuple(delta_slice0)] / (combined_delta * delta[tuple(delta_slice1)])\n            * f[tuple(slice2)])\n\n    # Now the \"right\" edge with backward difference\n    slice0[axis] = slice(-3, -2)\n    slice1[axis] = slice(-2, -1)\n    slice2[axis] = slice(-1, None)\n    delta_slice0[axis] = slice(-2, -1)\n    delta_slice1[axis] = slice(-1, None)\n\n    combined_delta = delta[tuple(delta_slice0)] + delta[tuple(delta_slice1)]\n    big_delta = combined_delta + delta[tuple(delta_slice1)]\n    right = (delta[tuple(delta_slice1)] / (combined_delta * delta[tuple(delta_slice0)])\n             * f[tuple(slice0)]\n             - combined_delta / (delta[tuple(delta_slice0)] * delta[tuple(delta_slice1)])\n             * f[tuple(slice1)]\n             + big_delta / (combined_delta * delta[tuple(delta_slice1)])\n             * f[tuple(slice2)])\n\n    return concatenate((left, center, right), axis=axis)", "response": "Calculate the first derivative of a regularly - spaced data and grids with varying spacing."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the gradient of a regularly - spaced data array.", "response": "def gradient(f, **kwargs):\n    \"\"\"Calculate the gradient of a grid of values.\n\n    Works for both regularly-spaced data, and grids with varying spacing.\n\n    Either `coordinates` or `deltas` must be specified, or `f` must be given as an\n    `xarray.DataArray` with  attached coordinate and projection information. If `f` is an\n    `xarray.DataArray`, and `coordinates` or `deltas` are given, `f` will be converted to a\n    `pint.Quantity` and the gradient returned as a tuple of `pint.Quantity`, otherwise, if\n    neither `coordinates` nor `deltas` are given, the attached coordinate information belonging\n    to `axis` will be used and the gradient will be returned as a tuple of `xarray.DataArray`.\n\n    Parameters\n    ----------\n    f : array-like\n        Array of values of which to calculate the derivative\n    coordinates : array-like, optional\n        Sequence of arrays containing the coordinate values corresponding to the\n        grid points in `f` in axis order.\n    deltas : array-like, optional\n        Sequence of arrays or scalars that specify the spacing between the grid points in `f`\n        in axis order. There should be one item less than the size of `f` along the applicable\n        axis.\n    axes : sequence, optional\n        Sequence of strings (if `f` is a `xarray.DataArray` and implicit conversion to\n        `pint.Quantity` is not used) or integers that specify the array axes along which to\n        take the derivatives. Defaults to all axes of `f`. If given, and used with\n        `coordinates` or `deltas`, its length must be less than or equal to that of the\n        `coordinates` or `deltas` given.\n\n    Returns\n    -------\n    tuple of array-like\n        The first derivative calculated along each specified axis of the original array\n\n    See Also\n    --------\n    laplacian, first_derivative\n\n    Notes\n    -----\n    `gradient` previously accepted `x` as a parameter for coordinate values. This has been\n    deprecated in 0.9 in favor of `coordinates`.\n\n    If this function is used without the `axes` parameter, the length of `coordinates` or\n    `deltas` (as applicable) should match the number of dimensions of `f`.\n\n    \"\"\"\n    pos_kwarg, positions, axes = _process_gradient_args(f, kwargs)\n    return tuple(first_derivative(f, axis=axis, **{pos_kwarg: positions[ind]})\n                 for ind, axis in enumerate(axes))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef laplacian(f, **kwargs):\n    pos_kwarg, positions, axes = _process_gradient_args(f, kwargs)\n    derivs = [second_derivative(f, axis=axis, **{pos_kwarg: positions[ind]})\n              for ind, axis in enumerate(axes)]\n    laplac = sum(derivs)\n    if isinstance(derivs[0], xr.DataArray):\n        # Patch in the units that are dropped\n        laplac.attrs['units'] = derivs[0].attrs['units']\n    return laplac", "response": "Calculates the laplacian of a regularly - spaced data and grids with varying spacing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _broadcast_to_axis(arr, axis, ndim):\n    if arr.ndim == 1 and arr.ndim < ndim:\n        new_shape = [1] * ndim\n        new_shape[axis] = arr.size\n        arr = arr.reshape(*new_shape)\n    return arr", "response": "Handle reshaping coordinate array to have proper dimensionality."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _process_gradient_args(f, kwargs):\n    axes = kwargs.get('axes', range(f.ndim))\n\n    def _check_length(positions):\n        if 'axes' in kwargs and len(positions) < len(axes):\n            raise ValueError('Length of \"coordinates\" or \"deltas\" cannot be less than that '\n                             'of \"axes\".')\n        elif 'axes' not in kwargs and len(positions) != len(axes):\n            raise ValueError('Length of \"coordinates\" or \"deltas\" must match the number of '\n                             'dimensions of \"f\" when \"axes\" is not given.')\n\n    if 'deltas' in kwargs:\n        if 'coordinates' in kwargs or 'x' in kwargs:\n            raise ValueError('Cannot specify both \"coordinates\" and \"deltas\".')\n        _check_length(kwargs['deltas'])\n        return 'delta', kwargs['deltas'], axes\n    elif 'coordinates' in kwargs:\n        _check_length(kwargs['coordinates'])\n        return 'x', kwargs['coordinates'], axes\n    elif 'x' in kwargs:\n        warnings.warn('The use of \"x\" as a parameter for coordinate values has been '\n                      'deprecated. Use \"coordinates\" instead.', metpyDeprecation)\n        _check_length(kwargs['x'])\n        return 'x', kwargs['x'], axes\n    elif isinstance(f, xr.DataArray):\n        return 'pass', axes, axes  # only the axis argument matters\n    else:\n        raise ValueError('Must specify either \"coordinates\" or \"deltas\" for value positions '\n                         'when \"f\" is not a DataArray.')", "response": "Handle common processing of arguments for gradient and gradient - like functions."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nhandle common processing of derivative functions.", "response": "def _process_deriv_args(f, kwargs):\n    \"\"\"Handle common processing of arguments for derivative functions.\"\"\"\n    n = f.ndim\n    axis = normalize_axis_index(kwargs.get('axis', 0), n)\n\n    if f.shape[axis] < 3:\n        raise ValueError('f must have at least 3 point along the desired axis.')\n\n    if 'delta' in kwargs:\n        if 'x' in kwargs:\n            raise ValueError('Cannot specify both \"x\" and \"delta\".')\n\n        delta = atleast_1d(kwargs['delta'])\n        if delta.size == 1:\n            diff_size = list(f.shape)\n            diff_size[axis] -= 1\n            delta_units = getattr(delta, 'units', None)\n            delta = np.broadcast_to(delta, diff_size, subok=True)\n            if delta_units is not None:\n                delta = delta * delta_units\n        else:\n            delta = _broadcast_to_axis(delta, axis, n)\n    elif 'x' in kwargs:\n        x = _broadcast_to_axis(kwargs['x'], axis, n)\n        delta = diff(x, axis=axis)\n    else:\n        raise ValueError('Must specify either \"x\" or \"delta\" for value positions.')\n\n    return n, axis, delta"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the meteorological angle from a directional text.", "response": "def parse_angle(input_dir):\n    \"\"\"Calculate the meteorological angle from directional text.\n\n    Works for abbrieviations or whole words (E -> 90 | South -> 180)\n    and also is able to parse 22.5 degreee angles such as ESE/East South East\n\n    Parameters\n    ----------\n    input_dir : string or array-like strings\n        Directional text such as west, [south-west, ne], etc\n\n    Returns\n    -------\n    angle\n        The angle in degrees\n\n    \"\"\"\n    if isinstance(input_dir, str):\n        # abb_dirs = abbrieviated directions\n        abb_dirs = [_abbrieviate_direction(input_dir)]\n    elif isinstance(input_dir, list):\n        input_dir_str = ','.join(input_dir)\n        abb_dir_str = _abbrieviate_direction(input_dir_str)\n        abb_dirs = abb_dir_str.split(',')\n    return itemgetter(*abb_dirs)(DIR_DICT)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts extended ( non - abbrievated ) directions to abbrieviation.", "response": "def _abbrieviate_direction(ext_dir_str):\n    \"\"\"Convert extended (non-abbrievated) directions to abbrieviation.\"\"\"\n    return (ext_dir_str\n            .upper()\n            .replace('_', '')\n            .replace('-', '')\n            .replace(' ', '')\n            .replace('NORTH', 'N')\n            .replace('EAST', 'E')\n            .replace('SOUTH', 'S')\n            .replace('WEST', 'W')\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the underlying data as a Python dict.", "response": "def to_dict(self):\n        \"\"\"\n        Returns the underlying data as a Python dict.\n        \"\"\"\n        return {\n            \"state_size\": self.state_size,\n            \"chain\": self.chain.to_json(),\n            \"parsed_sentences\": self.parsed_sentences if self.retain_original else None\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_corpus(self, text):\n        if isinstance(text, str):\n            sentences = self.sentence_split(text)\n        else:\n            sentences = []\n            for line in text:\n                sentences += self.sentence_split(line)\n        passing = filter(self.test_sentence_input, sentences)\n        runs = map(self.word_split, passing)\n        return runs", "response": "Given a text string returns a list of lists that is a list of lists that is a list of lists each of which is a list of words each of which is a list of sentences each of which is a list of words."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef make_sentence(self, init_state=None, **kwargs):\n        tries = kwargs.get('tries', DEFAULT_TRIES)\n        mor = kwargs.get('max_overlap_ratio', DEFAULT_MAX_OVERLAP_RATIO)\n        mot = kwargs.get('max_overlap_total', DEFAULT_MAX_OVERLAP_TOTAL)\n        test_output = kwargs.get('test_output', True)\n        max_words = kwargs.get('max_words', None)\n\n        if init_state != None:\n            prefix = list(init_state)\n            for word in prefix:\n                if word == BEGIN:\n                    prefix = prefix[1:]\n                else:\n                    break\n        else:\n            prefix = []\n\n        for _ in range(tries):\n            words = prefix + self.chain.walk(init_state)\n            if max_words != None and len(words) > max_words:\n                continue\n            if test_output and hasattr(self, \"rejoined_text\"):\n                if self.test_sentence_output(words, mor, mot):\n                    return self.word_join(words)\n            else:\n                return self.word_join(words)\n        return None", "response": "Generates a valid sentence from the current state of the model."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef make_short_sentence(self, max_chars, min_chars=0, **kwargs):\n        tries = kwargs.get('tries', DEFAULT_TRIES)\n\n        for _ in range(tries):\n            sentence = self.make_sentence(**kwargs)\n            if sentence and len(sentence) <= max_chars and len(sentence) >= min_chars:\n                return sentence", "response": "Tries to make a short sentence of no more than max_chars characters and optionally with min_chars charcaters passing kwargs to self. make_sentence."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef make_sentence_with_start(self, beginning, strict=True, **kwargs):\n        split = tuple(self.word_split(beginning))\n        word_count = len(split)\n\n        if word_count == self.state_size:\n            init_states = [ split ]\n\n        elif word_count > 0 and word_count < self.state_size:\n            if strict:\n                init_states = [ (BEGIN,) * (self.state_size - word_count) + split ]\n\n            else:\n                init_states = [ key for key in self.chain.model.keys()\n                    # check for starting with begin as well ordered lists\n                    if tuple(filter(lambda x: x != BEGIN, key))[:word_count] == split ]\n\n                random.shuffle(init_states)\n        else:\n            err_msg = \"`make_sentence_with_start` for this model requires a string containing 1 to {0} words. Yours has {1}: {2}\".format(self.state_size, word_count, str(split))\n            raise ParamError(err_msg)\n\n        for init_state in init_states:\n            output = self.make_sentence(init_state, **kwargs)\n            if output is not None:\n                return output\n\n        return None", "response": "Tries to make a sentence that starts with the beginning string."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef build(self, corpus, state_size):\n\n        # Using a DefaultDict here would be a lot more convenient, however the memory\n        # usage is far higher.\n        model = {}\n\n        for run in corpus:\n            items = ([ BEGIN ] * state_size) + run + [ END ]\n            for i in range(len(run) + 1):\n                state = tuple(items[i:i+state_size])\n                follow = items[i+state_size]\n                if state not in model:\n                    model[state] = {}\n\n                if follow not in model[state]:\n                    model[state][follow] = 0\n\n                model[state][follow] += 1\n        return model", "response": "Build a Python representation of the Markov model."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef precompute_begin_state(self):\n        begin_state = tuple([ BEGIN ] * self.state_size)\n        choices, weights = zip(*self.model[begin_state].items())\n        cumdist = list(accumulate(weights))\n        self.begin_cumdist = cumdist\n        self.begin_choices = choices", "response": "Precompute the cumdist and choices for the beginning state."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a state choose the next item at random.", "response": "def move(self, state):\n        \"\"\"\n        Given a state, choose the next item at random.\n        \"\"\"\n        if state == tuple([ BEGIN ] * self.state_size):\n            choices = self.begin_choices\n            cumdist = self.begin_cumdist\n        else:\n            choices, weights = zip(*self.model[state].items())\n            cumdist = list(accumulate(weights))\n        r = random.random() * cumdist[-1]\n        selection = choices[bisect.bisect(cumdist, r)]\n        return selection"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngenerate a generator that yields successive items from the current state.", "response": "def gen(self, init_state=None):\n        \"\"\"\n        Starting either with a naive BEGIN state, or the provided `init_state`\n        (as a tuple), return a generator that will yield successive items\n        until the chain reaches the END state.\n        \"\"\"\n        state = init_state or (BEGIN,) * self.state_size\n        while True:\n            next_word = self.move(state)\n            if next_word == END: break\n            yield next_word\n            state = tuple(state[1:]) + (next_word,)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_json(cls, json_thing):\n\n        if isinstance(json_thing, basestring):\n            obj = json.loads(json_thing)\n        else:\n            obj = json_thing\n\n        if isinstance(obj, list):\n            rehydrated = dict((tuple(item[0]), item[1]) for item in obj)\n        elif isinstance(obj, dict):\n            rehydrated = obj\n        else:\n            raise ValueError(\"Object should be dict or list\")\n\n        state_size = len(list(rehydrated.keys())[0])\n\n        inst = cls(None, state_size, rehydrated)\n        return inst", "response": "Given a JSON object or string that was created by self. to_json returns the corresponding markovify. Chain.\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a new Algorithm for use when creating and verifying tokens.", "response": "def register_algorithm(self, alg_id, alg_obj):\n        \"\"\"\n        Registers a new Algorithm for use when creating and verifying tokens.\n        \"\"\"\n        if alg_id in self._algorithms:\n            raise ValueError('Algorithm already has a handler.')\n\n        if not isinstance(alg_obj, Algorithm):\n            raise TypeError('Object is not of type `Algorithm`')\n\n        self._algorithms[alg_id] = alg_obj\n        self._valid_algs.add(alg_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns back the JWT header parameters as a dict", "response": "def get_unverified_header(self, jwt):\n        \"\"\"Returns back the JWT header parameters as a dict()\n\n        Note: The signature is not verified so the header parameters\n        should not be fully trusted until signature verification is complete\n        \"\"\"\n        headers = self._load(jwt)[2]\n        self._validate_headers(headers)\n\n        return headers"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the default algorithms for the current language.", "response": "def get_default_algorithms():\n    \"\"\"\n    Returns the algorithms that are implemented by the library.\n    \"\"\"\n    default_algorithms = {\n        'none': NoneAlgorithm(),\n        'HS256': HMACAlgorithm(HMACAlgorithm.SHA256),\n        'HS384': HMACAlgorithm(HMACAlgorithm.SHA384),\n        'HS512': HMACAlgorithm(HMACAlgorithm.SHA512)\n    }\n\n    if has_crypto:\n        default_algorithms.update({\n            'RS256': RSAAlgorithm(RSAAlgorithm.SHA256),\n            'RS384': RSAAlgorithm(RSAAlgorithm.SHA384),\n            'RS512': RSAAlgorithm(RSAAlgorithm.SHA512),\n            'ES256': ECAlgorithm(ECAlgorithm.SHA256),\n            'ES384': ECAlgorithm(ECAlgorithm.SHA384),\n            'ES521': ECAlgorithm(ECAlgorithm.SHA512),\n            'ES512': ECAlgorithm(ECAlgorithm.SHA512),  # Backward compat for #219 fix\n            'PS256': RSAPSSAlgorithm(RSAPSSAlgorithm.SHA256),\n            'PS384': RSAPSSAlgorithm(RSAPSSAlgorithm.SHA384),\n            'PS512': RSAPSSAlgorithm(RSAPSSAlgorithm.SHA512)\n        })\n\n    return default_algorithms"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating information for a bug report.", "response": "def info():\n    \"\"\"\n    Generate information for a bug report.\n    Based on the requests package help utility module.\n    \"\"\"\n    try:\n        platform_info = {\"system\": platform.system(), \"release\": platform.release()}\n    except IOError:\n        platform_info = {\"system\": \"Unknown\", \"release\": \"Unknown\"}\n\n    implementation = platform.python_implementation()\n\n    if implementation == \"CPython\":\n        implementation_version = platform.python_version()\n    elif implementation == \"PyPy\":\n        implementation_version = \"%s.%s.%s\" % (\n            sys.pypy_version_info.major,\n            sys.pypy_version_info.minor,\n            sys.pypy_version_info.micro,\n        )\n        if sys.pypy_version_info.releaselevel != \"final\":\n            implementation_version = \"\".join(\n                [implementation_version, sys.pypy_version_info.releaselevel]\n            )\n    else:\n        implementation_version = \"Unknown\"\n\n    return {\n        \"platform\": platform_info,\n        \"implementation\": {\"name\": implementation, \"version\": implementation_version},\n        \"cryptography\": {\"version\": getattr(cryptography, \"__version__\", \"\")},\n        \"pyjwt\": {\"version\": pyjwt_version},\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nauthenticates with netcup server. Must be called first.", "response": "def _authenticate(self):\n        \"\"\"Authenticate with netcup server. Must be called first.\"\"\"\n        login_info = self._apicall('login')\n        self.api_session_id = login_info['apisessionid']\n        if not self.api_session_id:\n            raise Exception('Login failed')\n        # query ttl and verify access to self.domain:\n        zone_info = self._apicall('infoDnsZone', domainname=self.domain)\n        self.zone_ttl = zone_info['ttl']"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate record. If it already exists do nothing.", "response": "def _create_record(self, rtype, name, content):\n        \"\"\"Create record. If it already exists, do nothing.\"\"\"\n        if not self._list_records(rtype, name, content):\n            self._update_records([{}], {\n                'type': rtype,\n                'hostname': self._relative_name(name),\n                'destination': content,\n                'priority': self._get_lexicon_option('priority'),\n            })\n        LOGGER.debug('create_record: %s', True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlisting all records. Return an empty list if no records found. rtype name and content are used to filter records.", "response": "def _list_records(self, rtype=None, name=None, content=None):\n        \"\"\"List all records. Return an empty list if no records found.\n        ``rtype``, ``name`` and ``content`` are used to filter records.\"\"\"\n        records = [\n            {\n                'id': record['id'],\n                'type': record['type'],\n                'name': self._full_name(record['hostname']),\n                'content': record['destination'],\n                'priority': record['priority'],\n                'ttl': self.zone_ttl,\n            }\n            for record in self._raw_records(None, rtype, name, content)\n        ]\n        LOGGER.debug('list_records: %s', records)\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate or update a record.", "response": "def _update_record(self, identifier, rtype=None, name=None, content=None):\n        \"\"\"Create or update a record.\"\"\"\n        records = self._raw_records(identifier, rtype, name, content)\n        self._update_records(records, {\n            'type': rtype,\n            'hostname': self._relative_name(name),\n            'destination': content,\n            'priority': self._get_lexicon_option('priority'),\n        })\n        LOGGER.debug('update_record: %s', True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeleting an existing record.", "response": "def _delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"Delete an existing record. If record does not exist, do nothing.\"\"\"\n        records = self._raw_records(identifier, rtype, name, content)\n        LOGGER.debug('delete_records: %s', [rec['id'] for rec in records])\n        self._update_records(records, {\n            'deleterecord': True,\n            'type': rtype,\n            'hostname': name,\n            'destination': content,\n        })\n        LOGGER.debug('delete_record: %s', True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn list of record dicts in the netcup API convention.", "response": "def _raw_records(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"Return list of record dicts in the netcup API convention.\"\"\"\n        record_fields = {\n            'id': identifier,\n            'type': rtype,\n            'hostname': name and self._relative_name(name),\n            'destination': content,\n        }\n        # type/hostname/destination of the dnsrecord type are mandatory (even\n        # when deleting), and must be queried if not all were specified:\n        if all(record_fields.values()):\n            return [record_fields]\n        data = self._apicall('infoDnsRecords', domainname=self.domain)\n        records = data.get('dnsrecords', [])\n        return [\n            record for record in records\n            if all(record[k] == v for k, v in record_fields.items() if v)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _update_records(self, records, data):\n        data = {k: v for k, v in data.items() if v}\n        records = [dict(record, **data) for record in records]\n        return self._apicall(\n            'updateDnsRecords',\n            domainname=self.domain,\n            dnsrecordset={'dnsrecords': records},\n        ).get('dnsrecords', [])", "response": "Insert or update a list of DNS records specified in the netcup API\n            convention."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _apicall(self, method, **params):\n        LOGGER.debug('%s(%r)', method, params)\n        auth = {\n            'customernumber': self._get_provider_option('auth_customer_id'),\n            'apikey': self._get_provider_option('auth_api_key'),\n        }\n        if method == 'login':\n            auth['apipassword'] = self._get_provider_option('auth_api_password')\n        else:\n            auth['apisessionid'] = self.api_session_id\n        if not all(auth.values()):\n            raise Exception('No valid authentication mechanism found')\n        data = self._request('POST', url='', data={\n            'action': method,\n            'param': dict(params, **auth),\n        })\n        if data['status'] != 'success':\n            raise Exception(\"{} ({})\".format(\n                data['longmessage'], data['statuscode']))\n        return data.get('responsedata', {})", "response": "Call an API method and return response data. For more info see :\n            https://ccp. netcup. net / run / servers / endpoint"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconfigure provider parser for Rackspace", "response": "def provider_parser(subparser):\n    \"\"\"Configure provider parser for Rackspace\"\"\"\n    subparser.add_argument(\n        \"--auth-account\", help=\"specify account number for authentication\")\n    subparser.add_argument(\n        \"--auth-username\",\n        help=\"specify username for authentication. Only used if --auth-token is empty.\")\n    subparser.add_argument(\n        \"--auth-api-key\",\n        help=\"specify api key for authentication. Only used if --auth-token is empty.\")\n    subparser.add_argument(\n        \"--auth-token\",\n        help=(\"specify token for authentication. \"\n              \"If empty, the username and api key will be used to create a token.\"))\n    subparser.add_argument(\"--sleep-time\", type=float, default=1,\n                           help=\"number of seconds to wait between update requests.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nspecify arguments for AWS Route 53 Lexicon Provider.", "response": "def provider_parser(subparser):\n    \"\"\"Specify arguments for AWS Route 53 Lexicon Provider.\"\"\"\n    subparser.add_argument(\"--auth-access-key\",\n                           help=\"specify ACCESS_KEY for authentication\")\n    subparser.add_argument(\"--auth-access-secret\",\n                           help=\"specify ACCESS_SECRET for authentication\")\n    subparser.add_argument(\n        \"--private-zone\",\n        help=(\"indicates what kind of hosted zone to use. If true, use \"\n              \"only private zones. If false, use only public zones\"))\n\n    # TODO: these are only required for testing, we should figure out\n    # a way to remove them & update the integration tests\n    # to dynamically populate the auth credentials that are required.\n    subparser.add_argument(\n        \"--auth-username\", help=\"alternative way to specify the ACCESS_KEY for authentication\")\n    subparser.add_argument(\n        \"--auth-token\", help=\"alternative way to specify the ACCESS_SECRET for authentication\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting base kwargs for API call.", "response": "def get_base_kwargs(self):\n        \"\"\"Get base kwargs for API call.\"\"\"\n        kwargs = {\n            'HostedZoneId': self.hosted_zone_id\n        }\n        if self.max_items is not None:\n            kwargs.update({\n                'MaxItems': str(self.max_items)\n            })\n        return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef all_record_sets(self):\n        is_truncated = True\n        start_record_name = None\n        start_record_type = None\n        kwargs = self.get_base_kwargs()\n        while is_truncated:\n            if start_record_name is not None:\n                kwargs.update({\n                    'StartRecordName': start_record_name,\n                    'StartRecordType': start_record_type\n                })\n            result = self.get_record_sets(**kwargs)\n            for record_set in result.get('ResourceRecordSets', []):\n                yield record_set\n\n            is_truncated = result.get('IsTruncated', False)\n\n            start_record_name = result.get('NextRecordName', None)\n            start_record_type = result.get('NextRecordType', None)", "response": "Generator to loop through current record set."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if a zone is private", "response": "def filter_zone(self, data):\n        \"\"\"Check if a zone is private\"\"\"\n        if self.private_zone is not None:\n            if data['Config']['PrivateZone'] != self.str2bool(self.private_zone):\n                return False\n\n        if data['Name'] != '{0}.'.format(self.domain):\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining the hosted zone id for the domain.", "response": "def _authenticate(self):\n        \"\"\"Determine the hosted zone id for the domain.\"\"\"\n        try:\n            hosted_zones = self.r53_client.list_hosted_zones_by_name()[\n                'HostedZones'\n            ]\n            hosted_zone = next(\n                hz for hz in hosted_zones\n                if self.filter_zone(hz)\n            )\n            self.domain_id = hosted_zone['Id']\n        except StopIteration:\n            raise Exception('No domain found')"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_record(self, rtype, name, content):\n        return self._change_record_sets('CREATE', rtype, name, content)", "response": "Create a record in the hosted zone."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_record(self, identifier=None, rtype=None, name=None, content=None):\n        return self._change_record_sets('UPSERT', rtype, name, content)", "response": "Update a record from the hosted zone."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a record from the hosted zone.", "response": "def _delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"Delete a record from the hosted zone.\"\"\"\n        return self._change_record_sets('DELETE', rtype, name, content)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _list_records(self, rtype=None, name=None, content=None):\n        records = []\n        paginator = RecordSetPaginator(self.r53_client, self.domain_id)\n        for record in paginator.all_record_sets():\n            if rtype is not None and record['Type'] != rtype:\n                continue\n            if name is not None and record['Name'] != self._fqdn_name(name):\n                continue\n            if record.get('AliasTarget', None) is not None:\n                record_content = [record['AliasTarget'].get('DNSName', None)]\n            if record.get('ResourceRecords', None) is not None:\n                record_content = [self._format_content(record['Type'], value['Value']) for value\n                                  in record['ResourceRecords']]\n            if content is not None and content not in record_content:\n                continue\n            LOGGER.debug('record: %s', record)\n            records.append({\n                'type': record['Type'],\n                'name': self._full_name(record['Name']),\n                'ttl': record.get('TTL', None),\n                'content': record_content[0] if len(record_content) == 1 else record_content,\n            })\n        LOGGER.debug('list_records: %s', records)\n        return records", "response": "List all records for the hosted zone."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nauthenticate against Easyname website and find out the domain id.", "response": "def _authenticate(self):\n        \"\"\"\n        Authenticates against Easyname website and try to find out the domain\n        id.\n        Easyname uses a CSRF token in its login form, so two requests are\n        neccessary to actually login.\n\n        Returns:\n          bool: True if domain id was found.\n\n        Raises:\n          AssertionError: When a request returns unexpected or unknown data.\n          ValueError: When login data is wrong or the domain does not exist.\n        \"\"\"\n        csrf_token = self._get_csrf_token()\n        self._login(csrf_token)\n\n        domain_text_element = self._get_domain_text_of_authoritative_zone()\n        self.domain_id = self._get_domain_id(domain_text_element)\n        LOGGER.debug('Easyname domain ID: %s', self.domain_id)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new DNS entry in the domain zone if it does not already exist. Args: rtype (str): The DNS type (e.g. A, TXT, MX, etc) of the new entry. name (str): The name of the new DNS entry, e.g the domain for which a MX entry shall be valid. content (str): The content of the new DNS entry, e.g. the mail server hostname for a MX entry. [identifier] (str): The easyname id of a DNS entry. Use to overwrite an existing entry. Returns: bool: True if the record was created successfully, False otherwise.", "response": "def _create_record_internal(self, rtype, name, content, identifier=None):\n        \"\"\"\n        Create a new DNS entry in the domain zone if it does not already exist.\n\n        Args:\n          rtype (str): The DNS type (e.g. A, TXT, MX, etc) of the new entry.\n          name (str): The name of the new DNS entry, e.g the domain for which a\n                      MX entry shall be valid.\n          content (str): The content of the new DNS entry, e.g. the mail server\n                         hostname for a MX entry.\n          [identifier] (str): The easyname id of a DNS entry. Use to overwrite an\n                    existing entry.\n\n        Returns:\n          bool: True if the record was created successfully, False otherwise.\n        \"\"\"\n        name = self._relative_name(name) if name is not None else name\n        LOGGER.debug('Creating record with name %s', name)\n        if self._is_duplicate_record(rtype, name, content):\n            return True\n\n        data = self._get_post_data_to_create_dns_entry(rtype, name, content, identifier)\n        LOGGER.debug('Create DNS data: %s', data)\n        create_response = self.session.post(\n            self.URLS['dns_create_entry'].format(self.domain_id),\n            data=data\n        )\n        self._invalidate_records_cache()\n        self._log('Create DNS entry', create_response)\n\n        # Pull a list of records and check for ours\n        was_success = len(self._list_records(rtype, name, content)) > 0\n        if was_success:\n            msg = 'Successfully added record %s'\n        else:\n            msg = 'Failed to add record %s'\n\n        LOGGER.info(msg, name)\n        return was_success"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting one or more DNS entries in the domain zone that match the given criteria.", "response": "def _delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"\n        Delete one or more DNS entries in the domain zone that match the given\n        criteria.\n\n        Args:\n          [identifier] (str): An ID to match against DNS entry easyname IDs.\n          [rtype] (str): A DNS rtype (e.g. A, TXT, MX, etc) to match against DNS\n                      entry types.\n          [name] (str): A name to match against DNS entry names.\n          [content] (str): A content to match against a DNS entry contents.\n\n        Returns:\n          bool: True if the record(s) were deleted successfully, False\n                otherwise.\n        \"\"\"\n        success_url = self.URLS['dns'].format(self.domain_id)\n        record_ids = self._get_matching_dns_entry_ids(identifier, rtype,\n                                                      name, content)\n        LOGGER.debug('Record IDs to delete: %s', record_ids)\n\n        success = True\n        for rec_id in record_ids:\n            delete_response = self.session.get(\n                self.URLS['dns_delete_entry'].format(self.domain_id, rec_id))\n            self._invalidate_records_cache()\n            self._log('Delete DNS entry {}'.format(rec_id), delete_response)\n            success = success and delete_response.url == success_url\n\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating a DNS entry identified by identifier or name in the domain zone.", "response": "def _update_record(self, identifier, rtype=None, name=None, content=None):\n        \"\"\"\n        Update a DNS entry identified by identifier or name in the domain zone.\n        Any non given argument will leave the current value of the DNS entry.\n\n        Args:\n          identifier (str): The easyname id of the DNS entry to update.\n          [rtype] (str): The DNS rtype (e.g. A, TXT, MX, etc) of the new entry.\n          [name] (str): The name of the new DNS entry, e.g the domain for which\n                        a MX entry shall be valid.\n          [content] (str): The content of the new DNS entry, e.g. the mail\n                           server hostname for a MX entry.\n\n        Returns:\n          bool: True if the record was updated successfully, False otherwise.\n\n        Raises:\n          AssertionError: When a request returns unexpected or unknown data.\n        \"\"\"\n        if identifier is not None:\n            identifier = int(identifier)\n            records = self._list_records_internal(identifier=identifier)\n        else:\n            records = self._list_records_internal(name=name, rtype=rtype)\n        LOGGER.debug('Records to update (%d): %s', len(records), records)\n        assert records, 'No record found to update'\n        success = True\n\n        for record in records:\n            name = name if name is not None else record['name']\n            rtype = rtype if rtype is not None else record['type']\n            content = content if content is not None \\\n                else record['content']\n            success = success and self._create_record_internal(\n                rtype, name, content, record['id'])\n        return success"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfilter and list DNS entries of domain zone on Easyname. Easyname shows each entry in a HTML table row and each attribute on a table column. Args: [rtype] (str): Filter by DNS rtype (e.g. A, TXT, MX, etc) [name] (str): Filter by the name of the DNS entry, e.g the domain for which a MX entry shall be valid. [content] (str): Filter by the content of the DNS entry, e.g. the mail server hostname for a MX entry. [identifier] (str): Filter by the easyname id of the DNS entry. Returns: list: A list of DNS entries. A DNS entry is an object with DNS attribute names as keys (e.g. name, content, priority, etc) and additionally an id. Raises: AssertionError: When a request returns unexpected or unknown data.", "response": "def _list_records_internal(self, rtype=None, name=None, content=None, identifier=None):\n        \"\"\"\n        Filter and list DNS entries of domain zone on Easyname.\n        Easyname shows each entry in a HTML table row and each attribute on a\n        table column.\n\n        Args:\n          [rtype] (str): Filter by DNS rtype (e.g. A, TXT, MX, etc)\n          [name] (str): Filter by the name of the DNS entry, e.g the domain for\n                      which a MX entry shall be valid.\n          [content] (str): Filter by the content of the DNS entry, e.g. the\n                           mail server hostname for a MX entry.\n          [identifier] (str): Filter by the easyname id of the DNS entry.\n\n        Returns:\n          list: A list of DNS entries. A DNS entry is an object with DNS\n                attribute names as keys (e.g. name, content, priority, etc)\n                and additionally an id.\n\n        Raises:\n          AssertionError: When a request returns unexpected or unknown data.\n        \"\"\"\n        name = self._full_name(name) if name is not None else name\n        if self._records is None:\n            records = []\n            rows = self._get_dns_entry_trs()\n\n            for index, row in enumerate(rows):\n                self._log('DNS list entry', row)\n                try:\n                    rec = {}\n                    if row.has_attr('ondblclick'):\n                        rec['id'] = int(row['ondblclick'].split(\n                            'id=')[1].split(\"'\")[0])\n                    else:\n                        rec['id'] = -index\n\n                    columns = row.find_all('td')\n                    rec['name'] = (columns[0].string or '').strip()\n                    rec['type'] = (columns[1].contents[1] or '').strip()\n                    rec['content'] = (columns[2].string or '').strip()\n                    rec['priority'] = (columns[3].string or '').strip()\n                    rec['ttl'] = (columns[4].string or '').strip()\n\n                    if rec['priority']:\n                        rec['priority'] = int(rec['priority'])\n\n                    if rec['ttl']:\n                        rec['ttl'] = int(rec['ttl'])\n                except Exception as error:\n                    errmsg = 'Cannot parse DNS entry ({}).'.format(error)\n                    LOGGER.warning(errmsg)\n                    raise AssertionError(errmsg)\n                records.append(rec)\n            self._records = records\n\n        records = self._filter_records(self._records, rtype, name, content, identifier)\n        LOGGER.debug('Final records (%d): %s', len(records), records)\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_post_data_to_create_dns_entry(self, rtype, name, content, identifier=None):\n        is_update = identifier is not None\n        if is_update:\n            records = self._list_records_internal(identifier=identifier)\n            assert len(records) == 1, 'ID is not unique or does not exist'\n            record = records[0]\n            LOGGER.debug('Create post data to update record: %s', record)\n\n        data = {\n            'id': str(identifier) if is_update else '',\n            'action': 'save',\n            'name': name,\n            'type': rtype,\n            'content': content,\n            'prio': str(record['priority']) if is_update else '10',\n            'ttl': str(record['ttl']) if is_update else '360',\n            'commit': ''\n        }\n        ttl = self._get_lexicon_option('ttl')\n        if ttl and ttl > 360:\n            data['ttl'] = str(ttl)\n\n        prio = self._get_lexicon_option('priority')\n        if prio and prio > 0:\n            data['prio'] = str(prio)\n\n        return data", "response": "Builds and returns the post date that is needed to create a DNS entry."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if a DNS entry already exists.", "response": "def _is_duplicate_record(self, rtype, name, content):\n        \"\"\"Check if DNS entry already exists.\"\"\"\n        records = self._list_records(rtype, name, content)\n        is_duplicate = len(records) >= 1\n        if is_duplicate:\n            LOGGER.info('Duplicate record %s %s %s, NOOP', rtype, name, content)\n        return is_duplicate"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_matching_dns_entry_ids(self, identifier=None, rtype=None,\n                                    name=None, content=None):\n        \"\"\"Return a list of DNS entries that match the given criteria.\"\"\"\n        record_ids = []\n        if not identifier:\n            records = self._list_records(rtype, name, content)\n            record_ids = [record['id'] for record in records]\n        else:\n            record_ids.append(identifier)\n        return record_ids", "response": "Return a list of DNS entries that match the given criteria."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_dns_entry_trs(self):\n        from bs4 import BeautifulSoup\n        dns_list_response = self.session.get(\n            self.URLS['dns'].format(self.domain_id))\n        self._log('DNS list', dns_list_response)\n        assert dns_list_response.status_code == 200, \\\n            'Could not load DNS entries.'\n\n        html = BeautifulSoup(dns_list_response.content, 'html.parser')\n        self._log('DNS list', html)\n        dns_table = html.find('table', {'id': 'cp_domains_dnseintraege'})\n        assert dns_table is not None, 'Could not find DNS entry table'\n\n        def _is_zone_tr(elm):\n            has_ondblclick = elm.has_attr('ondblclick')\n            has_class = elm.has_attr('class')\n            return elm.name.lower() == 'tr' and (has_class or has_ondblclick)\n\n        rows = dns_table.findAll(_is_zone_tr)\n        assert rows is not None and rows, 'Could not find any DNS entries'\n        return rows", "response": "Get the TR elements holding the DNS entries."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _filter_records(self, records, rtype=None, name=None, content=None, identifier=None):  # pylint: disable=too-many-arguments,no-self-use\n        if not records:\n            return []\n        if identifier is not None:\n            LOGGER.debug('Filtering %d records by id: %s', len(records), identifier)\n            records = [record for record in records if record['id'] == identifier]\n        if rtype is not None:\n            LOGGER.debug('Filtering %d records by type: %s', len(records), rtype)\n            records = [record for record in records if record['type'] == rtype]\n        if name is not None:\n            LOGGER.debug('Filtering %d records by name: %s', len(records), name)\n            if name.endswith('.'):\n                name = name[:-1]\n            records = [record for record in records if name == record['name']]\n        if content is not None:\n            LOGGER.debug('Filtering %d records by content: %s', len(records), content.lower())\n            records = [record for record in records if\n                       record['content'].lower() == content.lower()]\n        return records", "response": "Filter dns entries based on type name content or identifier."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_csrf_token(self):\n        from bs4 import BeautifulSoup\n        home_response = self.session.get(self.URLS['login'])\n        self._log('Home', home_response)\n        assert home_response.status_code == 200, \\\n            'Could not load Easyname login page.'\n\n        html = BeautifulSoup(home_response.content, 'html.parser')\n        self._log('Home', html)\n        csrf_token_field = html.find('input', {'id': 'loginxtoken'})\n        assert csrf_token_field is not None, 'Could not find login token.'\n        return csrf_token_field['value']", "response": "Return the CSRF Token of easyname login form."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nattempting to login on easyname.", "response": "def _login(self, csrf_token):\n        \"\"\"Attempt to login session on easyname.\"\"\"\n        login_response = self.session.post(\n            self.URLS['login'],\n            data={\n                'username': self._get_provider_option('auth_username') or '',\n                'password': self._get_provider_option('auth_password') or '',\n                'submit': '',\n                'loginxtoken': csrf_token,\n            }\n        )\n        self._log('Login', login_response)\n        assert login_response.status_code == 200, \\\n            'Could not login due to a network error.'\n        assert login_response.url == self.URLS['overview'], \\\n            'Easyname login failed, bad EASYNAME_USER or EASYNAME_PASS.'"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_domain_text_of_authoritative_zone(self):\n        # We are logged in, so get the domain list\n        from bs4 import BeautifulSoup\n        zones_response = self.session.get(self.URLS['domain_list'])\n        self._log('Zone', zones_response)\n        assert zones_response.status_code == 200, \\\n            'Could not retrieve domain list due to a network error.'\n\n        html = BeautifulSoup(zones_response.content, 'html.parser')\n        self._log('Zone', html)\n        domain_table = html.find('table', {'id': 'cp_domain_table'})\n        assert domain_table is not None, 'Could not find domain table'\n\n        # (Sub)domains can either be managed in their own zones or by the\n        # zones of their parent (sub)domains. Iterate over all subdomains\n        # (starting with the deepest one) and see if there is an own zone\n        # for it.\n        domain = self.domain or ''\n        domain_text = None\n        subdomains = domain.split('.')\n        while True:\n            domain = '.'.join(subdomains)\n            LOGGER.debug('Check if %s has own zone', domain)\n            domain_text = domain_table.find(string=domain)\n            if domain_text is not None or len(subdomains) < 3:\n                break\n            subdomains.pop(0)\n\n        # Update domain to equal the zone's domain. This is important if we are\n        # handling a subdomain that has no zone of itself. If we do not do\n        # this, self._relative_name will strip also a part of the subdomain\n        # away.\n        self.domain = domain\n        assert domain_text is not None, \\\n            'The domain does not exist on Easyname.'\n        return domain_text", "response": "Get the authoritative name zone."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the easyname id of the domain.", "response": "def _get_domain_id(self, domain_text_element):  # pylint: disable=no-self-use\n        \"\"\"Return the easyname id of the domain.\"\"\"\n        try:\n            # Hierarchy: TR > TD > SPAN > Domain Text\n            tr_anchor = domain_text_element.parent.parent.parent\n            td_anchor = tr_anchor.find('td', {'class': 'td_2'})\n            link = td_anchor.find('a')['href']\n            domain_id = link.rsplit('/', 1)[-1]\n            return domain_id\n        except Exception as error:\n            errmsg = ('Cannot get the domain id even though the domain seems '\n                      'to exist (%s).', error)\n            LOGGER.warning(errmsg)\n            raise AssertionError(errmsg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlog response and tag elements. Do nothing if elements are none of them.", "response": "def _log(self, name, element):  # pylint: disable=no-self-use\n        \"\"\"\n        Log Response and Tag elements. Do nothing if elements is none of them.\n        \"\"\"\n        from bs4 import BeautifulSoup, Tag\n        if isinstance(element, Response):\n            LOGGER.debug('%s response: URL=%s Code=%s', name, element.url, element.status_code)\n        elif isinstance(element, (BeautifulSoup, Tag)):\n            LOGGER.debug('%s HTML:\\n%s', name, element)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_providers():\n    providers_list = sorted({modname for (_, modname, _)\n                             in pkgutil.iter_modules(providers.__path__)\n                             if modname != 'base'})\n\n    try:\n        distribution = pkg_resources.get_distribution('dns-lexicon')\n    except pkg_resources.DistributionNotFound:\n        return {provider: True for provider in providers_list}\n    else:\n        return {provider: _resolve_requirements(provider, distribution)\n                for provider in providers_list}", "response": "Find all providers registered in Lexicon and their availability"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring a provider parser for Hetzner", "response": "def provider_parser(subparser):\n    \"\"\"Configure a provider parser for Hetzner\"\"\"\n    subparser.add_argument('--auth-account',\n                           help='specify type of Hetzner account: by default Hetzner Robot '\n                           '(robot) or Hetzner konsoleH (konsoleh)')\n    subparser.add_argument('--auth-username', help='specify username of Hetzner account')\n    subparser.add_argument('--auth-password', help='specify password of Hetzner account')\n    subparser.add_argument('--linked',\n                           help='if exists, uses linked CNAME as A|AAAA|TXT record name for edit '\n                           'actions: by default (yes); Further restriction: Only enabled if '\n                           'record name or raw FQDN record identifier \\'type/name/content\\' is '\n                           'specified, and additionally for update actions the record name '\n                           'remains the same',\n                           default=str('yes'),\n                           choices=['yes', 'no'])\n    subparser.add_argument('--propagated',\n                           help='waits until record is publicly propagated after succeeded '\n                           'create|update actions: by default (yes)',\n                           default=str('yes'),\n                           choices=['yes', 'no'])\n    subparser.add_argument('--latency',\n                           help='specify latency, used during checks for publicly propagation '\n                           'and additionally for Hetzner Robot after record edits: by default '\n                           '30s (30)',\n                           default=int(30),\n                           type=int)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new record in Hetzner zone and returns a boolean indicating if the record was created or not.", "response": "def _create_record(self, rtype, name, content):\n        \"\"\"\n        Connects to Hetzner account, adds a new record to the zone and returns a\n        boolean, if creation was successful or not. Needed record rtype, name and\n        content for record to create.\n        \"\"\"\n        with self._session(self.domain, self.domain_id) as ddata:\n            # Validate method parameters\n            if not rtype or not name or not content:\n                LOGGER.warning('Hetzner => Record has no rtype|name|content specified')\n                return False\n\n            # Add record to zone\n            name = ddata['cname'] if ddata['cname'] else self._fqdn_name(name)\n            rrset = ddata['zone']['data'].get_rdataset(name, rdtype=rtype, create=True)\n            for rdata in rrset:\n                if self._convert_content(rtype, content) == rdata.to_text():\n                    LOGGER.info('Hetzner => Record with content \\'%s\\' already exists',\n                                content)\n                    return True\n\n            ttl = (rrset.ttl if 0 < rrset.ttl < self._get_lexicon_option('ttl')\n                   else self._get_lexicon_option('ttl'))\n            rdataset = dns.rdataset.from_text(rrset.rdclass, rrset.rdtype,\n                                              ttl, self._convert_content(rtype, content))\n            rrset.update(rdataset)\n            # Post zone to Hetzner\n            synced_change = self._post_zone(ddata['zone'])\n            if synced_change:\n                self._propagated_record(rtype, name, self._convert_content(rtype, content),\n                                        ddata['nameservers'])\n            return synced_change"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _list_records(self, rtype=None, name=None, content=None):\n        with self._session(self.domain, self.domain_id) as ddata:\n            name = self._fqdn_name(name) if name else None\n            return self._list_records_in_zone(ddata['zone']['data'], rtype, name, content)", "response": "List records in a Hetzner account and returns a list of records filtered by rtype name and content."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates an existing record in Hetzner account.", "response": "def _update_record(self, identifier=None, rtype=None, name=None, content=None):  # pylint: disable=too-many-locals,too-many-branches\n        \"\"\"\n        Connects to Hetzner account, changes an existing record and returns a boolean,\n        if update was successful or not. Needed identifier or rtype & name to lookup\n        over all records of the zone for exactly one record to update.\n        \"\"\"\n        with self._session(self.domain, self.domain_id) as ddata:\n            # Validate method parameters\n            if identifier:\n                dtype, dname, dcontent = self._parse_identifier(identifier, ddata['zone']['data'])\n                if dtype and dname and dcontent:\n                    rtype = rtype if rtype else dtype\n                    name = name if name else dname\n                    content = content if content else dcontent\n                else:\n                    LOGGER.warning('Hetzner => Record with identifier \\'%s\\' does not exist',\n                                   identifier)\n                    return False\n\n            elif rtype and name and content:\n                dtype, dname, dcontent = rtype, name, None\n            else:\n                LOGGER.warning('Hetzner => Record has no rtype|name|content specified')\n                return False\n\n            dname = ddata['cname'] if ddata['cname'] else self._fqdn_name(dname)\n            records = self._list_records_in_zone(ddata['zone']['data'], dtype, dname, dcontent)\n            if len(records) == 1:\n                # Remove record from zone\n                rrset = ddata['zone']['data'].get_rdataset(records[0]['name'] + '.',\n                                                           rdtype=records[0]['type'])\n                rdatas = []\n                for rdata in rrset:\n                    if self._convert_content(records[0]['type'],\n                                             records[0]['content']) != rdata.to_text():\n                        rdatas.append(rdata.to_text())\n                if rdatas:\n                    rdataset = dns.rdataset.from_text_list(rrset.rdclass, rrset.rdtype,\n                                                           records[0]['ttl'], rdatas)\n                    ddata['zone']['data'].replace_rdataset(records[0]['name'] + '.', rdataset)\n                else:\n                    ddata['zone']['data'].delete_rdataset(records[0]['name'] + '.',\n                                                          records[0]['type'])\n                # Add record to zone\n                name = ddata['cname'] if ddata['cname'] else self._fqdn_name(name)\n                rrset = ddata['zone']['data'].get_rdataset(name, rdtype=rtype, create=True)\n                synced_change = False\n                for rdata in rrset:\n                    if self._convert_content(rtype, content) == rdata.to_text():\n                        LOGGER.info('Hetzner => Record with content \\'%s\\' already exists',\n                                    content)\n                        synced_change = True\n                        break\n                if not synced_change:\n                    ttl = (rrset.ttl if 0 < rrset.ttl < self._get_lexicon_option('ttl')\n                           else self._get_lexicon_option('ttl'))\n                    rdataset = dns.rdataset.from_text(rrset.rdclass, rrset.rdtype, ttl,\n                                                      self._convert_content(rtype, content))\n                    rrset.update(rdataset)\n                # Post zone to Hetzner\n                synced_change = self._post_zone(ddata['zone'])\n                if synced_change:\n                    self._propagated_record(rtype, name, self._convert_content(rtype, content),\n                                            ddata['nameservers'])\n                return synced_change\n\n            LOGGER.warning('Hetzner => Record lookup has not only one match')\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a record from the Hetzner account.", "response": "def _delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"\n        Connects to Hetzner account, removes an existing record from the zone and returns a\n        boolean, if deletion was successful or not. Uses identifier or rtype, name & content to\n        lookup over all records of the zone for one or more records to delete.\n        \"\"\"\n        with self._session(self.domain, self.domain_id) as ddata:\n            # Validate method parameters\n            if identifier:\n                rtype, name, content = self._parse_identifier(identifier, ddata['zone']['data'])\n                if rtype is None or name is None or content is None:\n                    LOGGER.info('Hetzner => Record with identifier \\'%s\\' does not exist',\n                                identifier)\n                    return True\n\n            name = ddata['cname'] if ddata['cname'] else (self._fqdn_name(name) if name else None)\n            records = self._list_records_in_zone(ddata['zone']['data'], rtype, name, content)\n            if records:\n                # Remove records from zone\n                for record in records:\n                    rrset = ddata['zone']['data'].get_rdataset(record['name'] + '.',\n                                                               rdtype=record['type'])\n                    rdatas = []\n                    for rdata in rrset:\n                        if self._convert_content(record['type'],\n                                                 record['content']) != rdata.to_text():\n                            rdatas.append(rdata.to_text())\n                    if rdatas:\n                        rdataset = dns.rdataset.from_text_list(rrset.rdclass, rrset.rdtype,\n                                                               record['ttl'], rdatas)\n                        ddata['zone']['data'].replace_rdataset(record['name'] + '.', rdataset)\n                    else:\n                        ddata['zone']['data'].delete_rdataset(record['name'] + '.', record['type'])\n                # Post zone to Hetzner\n                synced_change = self._post_zone(ddata['zone'])\n                return synced_change\n\n            LOGGER.info('Hetzner => Record lookup has no matches')\n            return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate hashed identifier based on full qualified record type name & content.", "response": "def _create_identifier(rdtype, name, content):\n        \"\"\"\n        Creates hashed identifier based on full qualified record type, name & content\n        and returns hash.\n        \"\"\"\n        sha256 = hashlib.sha256()\n        sha256.update((rdtype + '/').encode('UTF-8'))\n        sha256.update((name + '/').encode('UTF-8'))\n        sha256.update(content.encode('UTF-8'))\n        return sha256.hexdigest()[0:7]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the record identifier and returns type name and content of the associated record as tuple. The tuple is empty if no record found.", "response": "def _parse_identifier(self, identifier, zone=None):\n        \"\"\"\n        Parses the record identifier and returns type, name & content of the associated record\n        as tuple. The tuple is empty if no associated record found.\n        \"\"\"\n        rdtype, name, content = None, None, None\n        if len(identifier) > 7:\n            parts = identifier.split('/')\n            rdtype, name, content = parts[0], parts[1], '/'.join(parts[2:])\n        else:\n            records = self._list_records_in_zone(zone)\n            for record in records:\n                if record['id'] == identifier:\n                    rdtype, name, content = record['type'], record['name'] + '.', record['content']\n        return rdtype, name, content"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _convert_content(self, rdtype, content):\n        if rdtype == 'TXT':\n            if content[0] != '\"':\n                content = '\"' + content\n            if content[-1] != '\"':\n                content += '\"'\n        if rdtype in ('CNAME', 'MX', 'NS', 'SRV'):\n            if content[-1] != '.':\n                content = self._fqdn_name(content)\n        return content", "response": "Converts type dependent record content into well formed and fully qualified version of the record content for domain zone and returns content."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _list_records_in_zone(self, zone, rdtype=None, name=None, content=None):\n        records = []\n        rrsets = zone.iterate_rdatasets() if zone else []\n        for rname, rdataset in rrsets:\n            rtype = dns.rdatatype.to_text(rdataset.rdtype)\n            if ((not rdtype or rdtype == rtype)\n                    and (not name or name == rname.to_text())):\n                for rdata in rdataset:\n                    rdata = rdata.to_text()\n                    if not content or self._convert_content(rtype, content) == rdata:\n                        raw_rdata = self._clean_TXT_record({'type': rtype,\n                                                            'content': rdata})['content']\n                        data = {\n                            'type': rtype,\n                            'name': rname.to_text(True),\n                            'ttl': int(rdataset.ttl),\n                            'content': raw_rdata,\n                            'id': Provider._create_identifier(rtype, rname.to_text(), raw_rdata)\n                        }\n                        records.append(data)\n        return records", "response": "Returns a list of records in a zone filtered by record type name and content."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _request(self, action='GET', url='/', data=None, query_params=None):\n        if data is None:\n            data = {}\n        if query_params is None:\n            query_params = {}\n        response = self.session.request(action, self.api[self.account]['endpoint'] + url,\n                                        params=query_params, data=data)\n        response.raise_for_status()\n        return response", "response": "Requests to Hetzner by current session and returns the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _dns_lookup(name, rdtype, nameservers=None):\n        rrset = dns.rrset.from_text(name, 0, 1, rdtype)\n        try:\n            resolver = dns.resolver.Resolver()\n            resolver.lifetime = 1\n            if nameservers:\n                resolver.nameservers = nameservers\n            rrset = resolver.query(name, rdtype)\n            for rdata in rrset:\n                LOGGER.debug('DNS Lookup => %s %s %s %s',\n                             rrset.name.to_text(), dns.rdataclass.to_text(rrset.rdclass),\n                             dns.rdatatype.to_text(rrset.rdtype), rdata.to_text())\n        except dns.exception.DNSException as error:\n            LOGGER.debug('DNS Lookup => %s', error)\n        return rrset", "response": "Looks on specified system domain nameservers to resolve record type\n        & name and returns the record set."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlooks for domain nameservers and returns the IPs of the nameservers as a list.", "response": "def _get_nameservers(domain):\n        \"\"\"\n        Looks for domain nameservers and returns the IPs of the nameservers as a list.\n        The list is empty, if no nameservers were found. Needed associated domain zone\n        name for lookup.\n        \"\"\"\n        nameservers = []\n        rdtypes_ns = ['SOA', 'NS']\n        rdtypes_ip = ['A', 'AAAA']\n        for rdtype_ns in rdtypes_ns:\n            for rdata_ns in Provider._dns_lookup(domain, rdtype_ns):\n                for rdtype_ip in rdtypes_ip:\n                    for rdata_ip in Provider._dns_lookup(rdata_ns.to_text().split(' ')[0],\n                                                         rdtype_ip):\n                        if rdata_ip.to_text() not in nameservers:\n                            nameservers.append(rdata_ip.to_text())\n        LOGGER.debug('DNS Lookup => %s IN NS %s', domain, ' '.join(nameservers))\n        return nameservers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_dns_cname(name, link=False):\n        resolver = dns.resolver.Resolver()\n        resolver.lifetime = 1\n        domain = dns.resolver.zone_for_name(name, resolver=resolver).to_text(True)\n        nameservers = Provider._get_nameservers(domain)\n        cname = None\n        links, max_links = 0, 5\n        while link:\n            if links >= max_links:\n                LOGGER.error('Hetzner => Record %s has more than %d linked CNAME '\n                             'records. Reduce the amount of CNAME links!',\n                             name, max_links)\n                raise AssertionError\n            qname = cname if cname else name\n            rrset = Provider._dns_lookup(qname, 'CNAME', nameservers)\n            if rrset:\n                links += 1\n                cname = rrset[0].to_text()\n                qdomain = dns.resolver.zone_for_name(cname, resolver=resolver).to_text(True)\n                if domain != qdomain:\n                    domain = qdomain\n                    nameservers = Provider._get_nameservers(qdomain)\n            else:\n                link = False\n        if cname:\n            LOGGER.info('Hetzner => Record %s has CNAME %s', name, cname)\n        return domain, nameservers, cname", "response": "Returns the associated domain zone nameservers and linked record name for the given fully qualified record name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking restrictions for use of CNAME lookup and returns a tuple of the fully qualified record name to lookup and a boolean if a CNAME lookup should be done or not.", "response": "def _link_record(self):\n        \"\"\"\n        Checks restrictions for use of CNAME lookup and returns a tuple of the\n        fully qualified record name to lookup and a boolean, if a CNAME lookup\n        should be done or not. The fully qualified record name is empty if no\n        record name is specified by this provider.\n        \"\"\"\n        action = self._get_lexicon_option('action')\n        identifier = self._get_lexicon_option('identifier')\n        rdtype = self._get_lexicon_option('type')\n        name = (self._fqdn_name(self._get_lexicon_option('name'))\n                if self._get_lexicon_option('name') else None)\n        link = self._get_provider_option('linked')\n        qname = name\n        if identifier:\n            rdtype, name, _ = self._parse_identifier(identifier)\n        if action != 'list' and rdtype in ('A', 'AAAA', 'TXT') and name and link == 'yes':\n            if action != 'update' or name == qname or not qname:\n                LOGGER.info('Hetzner => Enable CNAME lookup '\n                            '(see --linked parameter)')\n                return name, True\n        LOGGER.info('Hetzner => Disable CNAME lookup '\n                    '(see --linked parameter)')\n        return name, False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if a record is propagated by the nameservers.", "response": "def _propagated_record(self, rdtype, name, content, nameservers=None):\n        \"\"\"\n        If the publicly propagation check should be done, waits until the domain nameservers\n        responses with the propagated record type, name & content and returns a boolean,\n        if the publicly propagation was successful or not.\n        \"\"\"\n        latency = self._get_provider_option('latency')\n        propagated = self._get_provider_option('propagated')\n        if propagated == 'yes':\n            retry, max_retry = 0, 20\n            while retry < max_retry:\n                for rdata in Provider._dns_lookup(name, rdtype, nameservers):\n                    if content == rdata.to_text():\n                        LOGGER.info('Hetzner => Record %s has %s %s', name, rdtype, content)\n                        return True\n                retry += 1\n                retry_log = (', retry ({}/{}) in {}s...'.format((retry + 1), max_retry, latency)\n                             if retry < max_retry else '')\n                LOGGER.info('Hetzner => Record is not propagated%s', retry_log)\n                time.sleep(latency)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _filter_dom(dom, filters, last_find_all=False):\n        if isinstance(dom, string_types):\n            dom = BeautifulSoup(dom, 'html.parser')\n        for idx, find in enumerate(filters, start=1):\n            if not dom:\n                break\n            name, attrs = find.get('name'), find.get('attrs', {})\n            if len(filters) == idx and last_find_all:\n                dom = dom.find_all(name, attrs=attrs) if name else dom.find_all(attrs=attrs)\n            else:\n                dom = dom.find(name, attrs=attrs) if name else dom.find(attrs=attrs)\n        return dom", "response": "Filters the DOM by a list of filters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract hidden input data from DOM and returns the data as dictionary.", "response": "def _extract_hidden_data(dom):\n        \"\"\"\n        Extracts hidden input data from DOM and returns the data as dictionary.\n        \"\"\"\n        input_tags = dom.find_all('input', attrs={'type': 'hidden'})\n        data = {}\n        for input_tag in input_tags:\n            data[input_tag['name']] = input_tag['value']\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _extract_domain_id(string, regex):\n        regex = re.compile(regex)\n        match = regex.search(string)\n        if not match:\n            return False\n        return str(match.group(1))", "response": "Extracts the domain ID from given string and returns the domain ID."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate and authenticates and exits session to Hetzner account and returns a tuple of additional needed domain data.", "response": "def _session(self, domain, domain_id=None, get_zone=True):\n        \"\"\"\n        Generates, authenticates and exits session to Hetzner account, and\n        provides tuple of additional needed domain data (domain nameservers,\n        zone and linked record name) to public methods. The tuple parameters\n        are empty if not existent or specified. Exits session and raises error\n        if provider fails during session.\n        \"\"\"\n        name, link = self._link_record()\n        qdomain, nameservers, cname = Provider._get_dns_cname(\n            (name if name else domain + '.'), link)\n        qdomain_id, zone = domain_id, None\n        self.session = self._auth_session(self.username, self.password)\n        try:\n            if not domain_id or qdomain != domain:\n                qdomain_id = self._get_domain_id(qdomain)\n            if qdomain == domain:\n                self.domain_id = qdomain_id\n            if get_zone:\n                zone = self._get_zone(qdomain, qdomain_id)\n            yield {'nameservers': nameservers, 'zone': zone, 'cname': cname}\n        except Exception as exc:\n            raise exc\n        finally:\n            self._exit_session()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _auth_session(self, username, password):\n        api = self.api[self.account]['auth']\n        endpoint = api.get('endpoint', self.api[self.account]['endpoint'])\n        session = requests.Session()\n        session_retries = Retry(total=10, backoff_factor=0.5)\n        session_adapter = requests.adapters.HTTPAdapter(max_retries=session_retries)\n        session.mount('https://', session_adapter)\n        response = session.request('GET', endpoint + api['GET'].get('url', '/'))\n        dom = Provider._filter_dom(response.text, api['filter'])\n        data = Provider._extract_hidden_data(dom)\n        data[api['user']], data[api['pass']] = username, password\n        response = session.request('POST', endpoint + api['POST']['url'], data=data)\n        if Provider._filter_dom(response.text, api['filter']):\n            LOGGER.error('Hetzner => Unable to authenticate session with %s account \\'%s\\': '\n                         'Invalid credentials',\n                         self.account, username)\n            raise AssertionError\n        LOGGER.info('Hetzner => Authenticate session with %s account \\'%s\\'',\n                    self.account, username)\n        return session", "response": "Authenticates with given credentials and returns the session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _exit_session(self):\n        api = self.api[self.account]\n        response = self._get(api['exit']['GET']['url'])\n        if not Provider._filter_dom(response.text, api['filter']):\n            LOGGER.info('Hetzner => Exit session')\n        else:\n            LOGGER.warning('Hetzner => Unable to exit session')\n        self.session = None\n        return True", "response": "Exit session to Hetzner account and returns."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the ID for the current domain.", "response": "def _get_domain_id(self, domain):\n        \"\"\"\n        Pulls all domains managed by authenticated Hetzner account, extracts their IDs\n        and returns the ID for the current domain, if exists. Otherwise raises error.\n        \"\"\"\n        api = self.api[self.account]['domain_id']\n        qdomain = dns.name.from_text(domain).to_unicode(True)\n        domains, last_count, page = {}, -1, 0\n        while last_count != len(domains):\n            last_count = len(domains)\n            page += 1\n            url = (api['GET'].copy()).get('url', '/').replace('<index>', str(page))\n            params = api['GET'].get('params', {}).copy()\n            for param in params:\n                params[param] = params[param].replace('<index>', str(page))\n            response = self._get(url, query_params=params)\n            domain_tags = Provider._filter_dom(response.text, api['filter'], True)\n            for domain_tag in domain_tags:\n                domain_id = Provider._extract_domain_id(dict(domain_tag.attrs)[api['id']['attr']],\n                                                        api['id']['regex'])\n                domain = (Provider._filter_dom(domain_tag, api['domain'])\n                          .renderContents().decode('UTF-8'))\n                domains[domain] = domain_id\n                if domain == qdomain:\n                    LOGGER.info('Hetzner => Get ID %s for domain %s', domain_id, qdomain)\n                    return domain_id\n        LOGGER.error('Hetzner => ID for domain %s does not exists', qdomain)\n        raise AssertionError"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the zone for the current domain from authenticated Hetzner account and returns it as a zone object.", "response": "def _get_zone(self, domain, domain_id):\n        \"\"\"\n        Pulls the zone for the current domain from authenticated Hetzner account and\n        returns it as an zone object.\n        \"\"\"\n        api = self.api[self.account]\n        for request in api['zone']['GET']:\n            url = (request.copy()).get('url', '/').replace('<id>', domain_id)\n            params = request.get('params', {}).copy()\n            for param in params:\n                params[param] = params[param].replace('<id>', domain_id)\n            response = self._get(url, query_params=params)\n        dom = Provider._filter_dom(response.text, api['filter'])\n        zone_file_filter = [{'name': 'textarea', 'attrs': {'name': api['zone']['file']}}]\n        zone_file = Provider._filter_dom(dom, zone_file_filter).renderContents().decode('UTF-8')\n        hidden = Provider._extract_hidden_data(dom)\n        zone = {'data': dns.zone.from_text(zone_file, origin=domain, relativize=False),\n                'hidden': hidden}\n        LOGGER.info('Hetzner => Get zone for domain %s', domain)\n        return zone"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _post_zone(self, zone):\n        api = self.api[self.account]['zone']\n        data = zone['hidden']\n        data[api['file']] = zone['data'].to_text(relativize=True)\n        response = self._post(api['POST']['url'], data=data)\n        if Provider._filter_dom(response.text, api['filter']):\n            LOGGER.error('Hetzner => Unable to update zone for domain %s: Syntax error\\n\\n%s',\n                         zone['data'].origin.to_unicode(True),\n                         zone['data'].to_text(relativize=True).decode('UTF-8'))\n            return False\n\n        LOGGER.info('Hetzner => Update zone for domain %s',\n                    zone['data'].origin.to_unicode(True))\n        if self.account == 'robot':\n            latency = self._get_provider_option('latency')\n            LOGGER.info('Hetzner => Wait %ds until Hetzner Robot has taken over zone...',\n                        latency)\n            time.sleep(latency)\n        return True", "response": "Pushes updated zone for current domain to authenticated Hetzner account and returns a boolean if update was successful or not."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_response(self, response, message, exclude_code=None):  # pylint: disable=no-self-use\n        if 'code' in response and response['code'] >= 2000:\n            if exclude_code is not None and response['code'] == exclude_code:\n                return\n\n            raise Exception(\"{0}: {1} ({2})\".format(\n                message, response['msg'], response['code']))", "response": "Raises an exception if the response is not valid"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrun any request against the API just to make sure the credentials are valid", "response": "def _authenticate(self):\n        \"\"\"\n        run any request against the API just to make sure the credentials\n        are valid\n\n        :return bool: success status\n        :raises Exception: on error\n        \"\"\"\n        opts = {'domain': self._domain}\n        opts.update(self._auth)\n        response = self._api.domain.info(opts)\n        self._validate_response(\n            response=response, message='Failed to authenticate')\n\n        # set to fake id to pass tests, inwx doesn't work on domain id but\n        # uses domain names for identification\n        self.domain_id = 1\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a record in the nationale", "response": "def _create_record(self, rtype, name, content):\n        \"\"\"\n        create a record\n        does nothing if the record already exists\n\n        :param str rtype: type of record\n        :param str name: name of record\n        :param mixed content: value of record\n        :return bool: success status\n        :raises Exception: on error\n        \"\"\"\n        opts = {'domain': self._domain, 'type': rtype.upper(),\n                'name': self._full_name(name), 'content': content}\n        if self._get_lexicon_option('ttl'):\n            opts['ttl'] = self._get_lexicon_option('ttl')\n        opts.update(self._auth)\n\n        response = self._api.nameserver.createRecord(opts)\n        self._validate_response(\n            response=response, message='Failed to create record',\n            exclude_code=2302)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _list_records(self, rtype=None, name=None, content=None):\n        opts = {'domain': self._domain}\n        if rtype is not None:\n            opts['type'] = rtype.upper()\n        if name is not None:\n            opts['name'] = self._full_name(name)\n        if content is not None:\n            opts['content'] = content\n        opts.update(self._auth)\n\n        response = self._api.nameserver.info(opts)\n        self._validate_response(\n            response=response, message='Failed to get records')\n\n        records = []\n        if 'record' in response['resData']:\n            for record in response['resData']['record']:\n                processed_record = {\n                    'type': record['type'],\n                    'name': record['name'],\n                    'ttl': record['ttl'],\n                    'content': record['content'],\n                    'id': record['id']\n                }\n                records.append(processed_record)\n\n        return records", "response": "List all records in the nationale."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_record(self, identifier, rtype=None, name=None, content=None):\n        record_ids = []\n        if not identifier:\n            records = self._list_records(rtype, name)\n            record_ids = [record['id'] for record in records]\n        else:\n            record_ids.append(identifier)\n\n        for an_identifier in record_ids:\n            opts = {'id': an_identifier}\n            if rtype is not None:\n                opts['type'] = rtype.upper()\n            if name is not None:\n                opts['name'] = self._full_name(name)\n            if content is not None:\n                opts['content'] = content\n            opts.update(self._auth)\n\n            response = self._api.nameserver.updateRecord(opts)\n            self._validate_response(\n                response=response, message='Failed to update record',\n                exclude_code=2302)\n\n        return True", "response": "Update a record in nics."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a record in nics", "response": "def _delete_record(self, identifier=None, rtype=None, name=None,\n                       content=None):\n        \"\"\"\n        delete a record\n        filter selection to delete by identifier or rtype/name/content\n\n        :param int identifier: identifier of record to update\n        :param str rtype: rtype of record\n        :param str name: name of record\n        :param mixed content: value of record\n        :return bool: success status\n        :raises Exception: on error\n        \"\"\"\n        record_ids = []\n        if not identifier:\n            records = self._list_records(rtype, name, content)\n            record_ids = [record['id'] for record in records]\n        else:\n            record_ids.append(identifier)\n\n        for record_id in record_ids:\n            opts = {'id': record_id}\n            opts.update(self._auth)\n            response = self._api.nameserver.deleteRecord(opts)\n            self._validate_response(\n                response=response, message='Failed to update record')\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_record(self, rtype=None, name=None, content=None, **kwargs):\n        if not rtype and kwargs.get('type'):\n            warnings.warn('Parameter \"type\" is deprecated, use \"rtype\" instead.',\n                          DeprecationWarning)\n            rtype = kwargs.get('type')\n\n        return self._create_record(rtype, name, content)", "response": "Create a new record."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all records. Return an empty list if no records found type, name and content are used to filter records. If possible filter during the query, otherwise filter after response is received.", "response": "def list_records(self, rtype=None, name=None, content=None, **kwargs):\n        \"\"\"\n        List all records. Return an empty list if no records found\n        type, name and content are used to filter records.\n        If possible filter during the query, otherwise filter after response is received.\n        \"\"\"\n\n        if not rtype and kwargs.get('type'):\n            warnings.warn('Parameter \"type\" is deprecated, use \"rtype\" instead.',\n                          DeprecationWarning)\n            rtype = kwargs.get('type')\n\n        return self._list_records(rtype=rtype, name=name, content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_record(self, identifier, rtype=None, name=None, content=None, **kwargs):\n        if not rtype and kwargs.get('type'):\n            warnings.warn('Parameter \"type\" is deprecated, use \"rtype\" instead.',\n                          DeprecationWarning)\n            rtype = kwargs.get('type')\n\n        return self._update_record(identifier, rtype=rtype, name=name, content=content)", "response": "Update a record in the record store."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting an existing record.", "response": "def delete_record(self, identifier=None, rtype=None, name=None, content=None, **kwargs):\n        \"\"\"\n        Delete an existing record.\n        If record does not exist, do nothing.\n        If an identifier is specified, use it, otherwise do a lookup using type, name and content.\n        \"\"\"\n        if not rtype and kwargs.get('type'):\n            warnings.warn('Parameter \"type\" is deprecated, use \"rtype\" instead.',\n                          DeprecationWarning)\n            rtype = kwargs.get('type')\n\n        return self._delete_record(identifier=identifier, rtype=rtype, name=name, content=content)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notify_slaves(self):\n        if self.disable_slave_notify is not None:\n            LOGGER.debug('Slave notifications disabled')\n            return False\n\n        if self.zone_data()['kind'] == 'Master':\n            response_code = self._put('/zones/' + self.domain + '/notify').status_code\n            if response_code == 200:\n                LOGGER.debug('Slave(s) notified')\n                return True\n            LOGGER.debug('Slave notification failed with code %i', response_code)\n        else:\n            LOGGER.debug('Zone type should be \\'Master\\' for slave notifications')\n        return False", "response": "Checks to see if slaves should be notified and notifies them if needed"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _list_records(self, rtype=None, name=None, content=None):\n        if self.protocol == 'rpc':\n            return self.rpc_helper.list_records(rtype, name, content)\n\n        try:\n            if name is not None:\n                if rtype is not None:\n                    query_results = [self._get(\n                        '/domains/{0}/records/{1}/{2}'\n                        .format(self.domain_id, self._relative_name(name), rtype))]\n                else:\n                    query_results = self._get('/domains/{0}/records/{1}'\n                                              .format(self.domain_id, self._relative_name(name)))\n            else:\n                query_results = self._get(\n                    '/domains/{0}/records'.format(self.domain_id))\n                if rtype is not None:\n                    query_results = [\n                        item for item in query_results if item['rrset_type'] == rtype]\n        except requests.exceptions.HTTPError as error:\n            if error.response.status_code == 404:\n                query_results = []\n            else:\n                raise\n\n        # convert records with multiple values into single-value records\n        records = []\n        for query_result in query_results:\n            for value in query_result['rrset_values']:\n                record = {\n                    'type': query_result['rrset_type'],\n                    'name': self._full_name(query_result['rrset_name']),\n                    'ttl': query_result['rrset_ttl'],\n                    'content': value,\n                    'id': query_result['rrset_name'],\n                }\n                # cleanup potential quoting if suitable\n                self._clean_TXT_record(record)\n                records.append(record)\n        # filter for content, if requested\n        if content is not None:\n            records = [\n                record for record in records if record['content'] == content]\n        LOGGER.debug('list_records: %s', records)\n        return records", "response": "List all record for the active Gandi zone."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the specified record in a new Gandi zone.", "response": "def _update_record(self, identifier, rtype=None, name=None, content=None):\n        \"\"\"Updates the specified record in a new Gandi zone\n\n        'content' should be a string or a list of strings\n        \"\"\"\n        if self.protocol == 'rpc':\n            return self.rpc_helper.update_record(identifier, rtype, name, content)\n\n        data = {}\n        if rtype:\n            data['rrset_type'] = rtype\n        if name:\n            data['rrset_name'] = self._relative_name(name)\n        if content:\n            if isinstance(content, (list, tuple, set)):\n                data['rrset_values'] = list(content)\n            else:\n                data['rrset_values'] = [content]\n        if rtype is not None:\n            # replace the records of a specific rtype\n            url = '/domains/{0}/records/{1}/{2}'.format(self.domain_id,\n                                                        identifier or self._relative_name(\n                                                            name),\n                                                        rtype)\n            self._put(url, data)\n        else:\n            # replace all records with a matching name\n            url = '/domains/{0}/records/{1}'.format(self.domain_id,\n                                                    identifier or self._relative_name(name))\n            self._put(url, {'items': [data]})\n        LOGGER.debug('update_record: %s', True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining the current domain and zone IDs for the domain.", "response": "def authenticate(self):\n        \"\"\"Determine the current domain and zone IDs for the domain.\"\"\"\n        try:\n            payload = self._api.domain.info(self._api_key, self._domain)\n            self._zone_id = payload['zone_id']\n            return payload['id']\n        except xmlrpclib.Fault as err:\n            raise Exception(\"Failed to authenticate: '{0}'\".format(err))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new record for the domain in a new Gandi zone.", "response": "def create_record(self, rtype, name, content, ttl):\n        \"\"\"Creates a record for the domain in a new Gandi zone.\"\"\"\n        version = None\n        ret = False\n\n        # This isn't quite \"do nothing\" if the record already exists.\n        # In this case, no new record will be created, but a new zone version\n        # will be created and set.\n        try:\n            version = self._api.domain.zone.version.new(\n                self._api_key, self._zone_id)\n            self._api.domain.zone.record.add(self._api_key, self._zone_id, version,\n                                             {'type': rtype.upper(),\n                                              'name': name,\n                                              'value': content,\n                                              'ttl': ttl\n                                              })\n            self._api.domain.zone.version.set(\n                self._api_key, self._zone_id, version)\n            ret = True\n\n        finally:\n            if not ret and version is not None:\n                self._api.domain.zone.version.delete(\n                    self._api_key, self._zone_id, version)\n\n        LOGGER.debug(\"create_record: %s\", ret)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef list_records(self, rtype=None, name=None, content=None):\n        opts = {}\n        if rtype is not None:\n            opts['type'] = rtype.upper()\n        if name is not None:\n            opts['name'] = self._relative_name(name)\n        if content is not None:\n            opts['value'] = self._txt_encode(content) if opts.get(\n                'type', '') == 'TXT' else content\n\n        records = []\n        payload = self._api.domain.zone.record.list(\n            self._api_key, self._zone_id, 0, opts)\n        for record in payload:\n            processed_record = {\n                'type': record['type'],\n                'name': self._full_name(record['name']),\n                'ttl': record['ttl'],\n                'content': record['value'],\n                'id': record['id']\n            }\n\n            # Gandi will add quotes to all TXT record strings\n            if processed_record['type'] == 'TXT':\n                processed_record['content'] = self._txt_decode(\n                    processed_record['content'])\n\n            records.append(processed_record)\n\n        LOGGER.debug(\"list_records: %s\", records)\n        return records", "response": "List all record for the active Gandi zone."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_record(self, identifier, rtype=None, name=None, content=None):  # pylint: disable=too-many-branches\n        if not identifier:\n            records = self.list_records(rtype, name)\n            if len(records) == 1:\n                identifier = records[0]['id']\n            elif len(records) > 1:\n                raise Exception('Several record identifiers match the request')\n            else:\n                raise Exception('Record identifier could not be found')\n\n        identifier = str(identifier)\n        version = None\n\n        # Gandi doesn't allow you to edit records on the active zone file.\n        # Gandi also doesn't persist zone record identifiers when creating\n        # a new zone file. To update by identifier, we lookup the record\n        # by identifier, then use the record fields to find the record in\n        # the newly created zone.\n        records = self._api.domain.zone.record.list(\n            self._api_key, self._zone_id, 0, {'id': identifier})\n\n        if len(records) == 1:\n            rec = records[0]\n            del rec['id']\n\n            try:\n                version = self._api.domain.zone.version.new(\n                    self._api_key, self._zone_id)\n                records = self._api.domain.zone.record.list(\n                    self._api_key, self._zone_id, version, rec)\n                if len(records) != 1:\n                    raise self.GandiInternalError(\"expected one record\")\n\n                if rtype is not None:\n                    rec['type'] = rtype.upper()\n                if name is not None:\n                    rec['name'] = self._relative_name(name)\n                if content is not None:\n                    rec['value'] = self._txt_encode(\n                        content) if rec['type'] == 'TXT' else content\n\n                records = self._api.domain.zone.record.update(\n                    self._api_key, self._zone_id, version, {'id': records[0]['id']}, rec)\n                if len(records) != 1:\n                    raise self.GandiInternalError(\n                        \"Expected one updated record\")\n\n                self._api.domain.zone.version.set(\n                    self._api_key, self._zone_id, version)\n                ret = True\n\n            except self.GandiInternalError:\n                pass\n\n            finally:\n                if not ret and version is not None:\n                    self._api.domain.zone.version.delete(\n                        self._api_key, self._zone_id, version)\n\n        LOGGER.debug(\"update_record: %s\", ret)\n        return ret", "response": "Updates the specified record in a new Gandi zone."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nremove the specified records in a new Gandi zone.", "response": "def delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"Removes the specified records in a new Gandi zone.\"\"\"\n        version = None\n        ret = False\n\n        opts = {}\n        if identifier is not None:\n            opts['id'] = identifier\n        else:\n            if not rtype and not name and not content:\n                raise ValueError(\n                    'Error, at least one parameter from type, name or content must be set')\n            if rtype:\n                opts['type'] = rtype.upper()\n            if name:\n                opts['name'] = self._relative_name(name)\n            if content:\n                opts['value'] = self._txt_encode(\n                    content) if opts['type'] == 'TXT' else content\n\n        records = self._api.domain.zone.record.list(\n            self._api_key, self._zone_id, 0, opts)\n\n        if records:\n            try:\n                version = self._api.domain.zone.version.new(\n                    self._api_key, self._zone_id)\n                for record in records:\n                    del record['id']\n                    self._api.domain.zone.record.delete(\n                        self._api_key, self._zone_id, version, record)\n                self._api.domain.zone.version.set(\n                    self._api_key, self._zone_id, version)\n                ret = True\n            finally:\n                if not ret and version is not None:\n                    self._api.domain.zone.version.delete(\n                        self._api_key, self._zone_id, version)\n\n        LOGGER.debug(\"delete_record: %s\", ret)\n        return ret"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve(self, config_key):\n        for config_source in self._config_sources:\n            value = config_source.resolve(config_key)\n            if value:\n                return value\n\n        return None", "response": "Resolves the value of the given config parameter key."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_config_source(self, config_source, position=None):\n        rank = position if position is not None else len(self._config_sources)\n        self._config_sources.insert(rank, config_source)", "response": "Adds a config source to the current ConfigResolver instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef with_config_dir(self, dir_path):\n        lexicon_provider_config_files = []\n        lexicon_config_files = []\n\n        for path in os.listdir(dir_path):\n            path = os.path.join(dir_path, path)\n            if os.path.isfile(path):\n                basename = os.path.basename(path)\n                search = re.search(r'^lexicon(?:_(\\w+)|)\\.yml$', basename)\n                if search:\n                    provider = search.group(1)\n                    if provider:\n                        lexicon_provider_config_files.append((provider, path))\n                    else:\n                        lexicon_config_files.append(path)\n\n        for lexicon_provider_config_file in lexicon_provider_config_files:\n            self.with_provider_config_file(lexicon_provider_config_file[0],\n                                           lexicon_provider_config_file[1])\n\n        for lexicon_config_file in lexicon_config_files:\n            self.with_config_file(lexicon_config_file)\n\n        return self", "response": "Configure current resolver to use every valid YAML configuration files available in the given directory path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconfiguring a source that consumes the dict that where used on Lexicon 2. x", "response": "def with_legacy_dict(self, legacy_dict_object):\n        \"\"\"Configure a source that consumes the dict that where used on Lexicon 2.x\"\"\"\n        warnings.warn(DeprecationWarning('Legacy configuration object has been used '\n                                         'to load the ConfigResolver.'))\n        return self.with_config_source(LegacyDictConfigSource(legacy_dict_object))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconfigure provider parser for CloudNS", "response": "def provider_parser(subparser):\n    \"\"\"Configure provider parser for CloudNS\"\"\"\n    identity_group = subparser.add_mutually_exclusive_group()\n    identity_group.add_argument(\n        \"--auth-id\", help=\"specify user id for authentication\")\n    identity_group.add_argument(\n        \"--auth-subid\", help=\"specify subuser id for authentication\")\n    identity_group.add_argument(\n        \"--auth-subuser\", help=\"specify subuser name for authentication\")\n    subparser.add_argument(\n        \"--auth-password\", help=\"specify password for authentication\")\n    subparser.add_argument(\"--weight\", help=\"specify the SRV record weight\")\n    subparser.add_argument(\"--port\", help=\"specify the SRV record port\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching for a record on NS1 across zones. returns None if not found.", "response": "def _find_record(self, domain, _type=None):\n        \"\"\"search for a record on NS1 across zones. returns None if not found.\"\"\"\n\n        def _is_matching(record):\n            \"\"\"filter function for records\"\"\"\n\n            if domain and record.get('domain', None) != domain:\n                return False\n            if _type and record.get('type', None) != _type:\n                return False\n            return True\n\n        payload = self._get('/search?q={0}&type=record'.format(domain))\n        for record in payload:\n            if _is_matching(record):\n                match = record\n                break\n        else:\n            # no such domain on ns1\n            return None\n\n        record = self._get(\n            '/zones/{0}/{1}/{2}'.format(match['zone'], match['domain'], match['type']))\n        if record.get('message', None):\n            return None  # {\"message\":\"record not found\"}\n        short_answers = [x['answer'][0] for x in record['answers']]\n\n        # ensure a compatibility level with self._list_records\n        record['short_answers'] = short_answers\n        return record"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts returned data from list actions into a nice table for command line usage", "response": "def generate_list_table_result(lexicon_logger, output=None, without_header=None):\n    \"\"\"Convert returned data from list actions into a nice table for command line usage\"\"\"\n    if not isinstance(output, list):\n        lexicon_logger.debug('Command output is not a list, and then cannot '\n                             'be printed with --quiet parameter not enabled.')\n        return None\n\n    array = [[\n        row.get('id', ''),\n        row.get('type', ''),\n        row.get('name', ''),\n        row.get('content', ''),\n        row.get('ttl', '')] for row in output]\n\n    # Insert header (insert before calculating the max width of each column\n    # to take headers size into account)\n    if not without_header:\n        headers = ['ID', 'TYPE', 'NAME', 'CONTENT', 'TTL']\n        array.insert(0, headers)\n\n    column_widths = [0, 0, 0, 0, 0]\n    # Find max width for each column\n    for row in array:\n        for idx, col in enumerate(row):\n            width = len(str(col))\n            if width > column_widths[idx]:\n                column_widths[idx] = width\n\n    # Add a 'nice' separator\n    if not without_header:\n        array.insert(1, ['-' * column_widths[idx]\n                         for idx in range(len(column_widths))])\n\n    # Construct table to be printed\n    table = []\n    for row in array:\n        row_list = []\n        for idx, col in enumerate(row):\n            row_list.append(str(col).ljust(column_widths[idx]))\n        table.append(' '.join(row_list))\n\n    # Return table\n    return os.linesep.join(table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_table_results(output=None, without_header=None):\n    array = []\n    str_output = str(output)\n\n    if not without_header:\n        array.append('RESULT')\n        array.append('-' * max(6, len(str_output)))\n\n    array.append(str_output)\n    return os.linesep.join(array)", "response": "Convert returned data from non - list actions into a nice table for command line usage"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef handle_output(results, output_type, action):\n    if output_type == 'QUIET':\n        return\n\n    if not output_type == 'JSON':\n        if action == 'list':\n            table = generate_list_table_result(\n                logger, results, output_type == 'TABLE-NO-HEADER')\n        else:\n            table = generate_table_results(results, output_type == 'TABLE-NO-HEADER')\n        if table:\n            print(table)\n    else:\n        try:\n            json_str = json.dumps(results)\n            if json_str:\n                print(json_str)\n        except TypeError:\n            logger.debug('Output is not JSON serializable, and then cannot '\n                         'be printed with --output=JSON parameter.')", "response": "Print the relevant output for given output_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    # Dynamically determine all the providers available and gather command line arguments.\n    parsed_args = generate_cli_main_parser().parse_args()\n\n    log_level = logging.getLevelName(parsed_args.log_level)\n    logging.basicConfig(stream=sys.stdout, level=log_level,\n                        format='%(message)s')\n    logger.debug('Arguments: %s', parsed_args)\n\n    # In the CLI context, will get configuration interactively:\n    #   * from the command line\n    #   * from the environment variables\n    #   * from lexicon configuration files found in given --config-dir (default is current dir)\n    config = ConfigResolver()\n    config.with_args(parsed_args).with_env().with_config_dir(parsed_args.config_dir)\n\n    client = Client(config)\n\n    results = client.execute()\n\n    handle_output(results, parsed_args.output, config.resolve('lexicon:action'))", "response": "Main function of Lexicon."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nexecutes the action statement in class constructor to the DNS records", "response": "def execute(self):\n        \"\"\"Execute provided configuration in class constructor to the DNS records\"\"\"\n        self.provider.authenticate()\n        identifier = self.config.resolve('lexicon:identifier')\n        record_type = self.config.resolve('lexicon:type')\n        name = self.config.resolve('lexicon:name')\n        content = self.config.resolve('lexicon:content')\n\n        if self.action == 'create':\n            return self.provider.create_record(record_type, name, content)\n\n        if self.action == 'list':\n            return self.provider.list_records(record_type, name, content)\n\n        if self.action == 'update':\n            return self.provider.update_record(identifier, record_type, name, content)\n\n        if self.action == 'delete':\n            return self.provider.delete_record(identifier, record_type, name, content)\n\n        raise ValueError('Invalid action statement: {0}'.format(self.action))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging - in the user and checks the domain name", "response": "def _authenticate(self):\n        \"\"\"Logs-in the user and checks the domain name\"\"\"\n        if not self._get_provider_option(\n                'auth_username') or not self._get_provider_option('auth_password'):\n            raise Exception(\n                'No valid authentication data passed, expected: auth-username and auth-password')\n        response = self._request_login(self._get_provider_option('auth_username'),\n                                       self._get_provider_option('auth_password'))\n        if 'ssid' in response:\n            self.ssid = response['ssid']\n            domains = self.domains_list()\n            if any((domain['name'] == self.domain for domain in domains)):\n                self.domain_id = self.domain\n            else:\n                raise Exception(\"Unknown domain {}\".format(self.domain))\n        else:\n            raise Exception(\"No SSID provided by server\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new unique record", "response": "def _create_record(self, rtype, name, content):\n        \"\"\"Creates a new unique record\"\"\"\n        found = self._list_records(rtype=rtype, name=name, content=content)\n        if found:\n            return True\n\n        record = self._create_request_record(None, rtype, name, content,\n                                             self._get_lexicon_option('ttl'),\n                                             self._get_lexicon_option('priority'))\n\n        self._request_add_dns_record(record)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate a record. Name changes are allowed but the record identifier will change.", "response": "def _update_record(self, identifier, rtype=None, name=None, content=None):\n        \"\"\"Updates a record. Name changes are allowed, but the record identifier will change\"\"\"\n        if identifier is not None:\n            if name is not None:\n                records = self._list_records_internal(identifier=identifier)\n                if len(records) == 1 and records[0]['name'] != self._full_name(name):\n                    # API does not allow us to update name directly\n                    self._update_record_with_name(\n                        records[0], rtype, name, content)\n                else:\n                    self._update_record_with_id(identifier, rtype, content)\n            else:\n                self._update_record_with_id(identifier, rtype, content)\n        else:\n            guessed_record = self._guess_record(rtype, name)\n            self._update_record_with_id(guessed_record['id'], rtype, content)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nupdating existing record with the given identifier.", "response": "def _update_record_with_id(self, identifier, rtype, content):\n        \"\"\"Updates existing record with no sub-domain name changes\"\"\"\n        record = self._create_request_record(identifier, rtype, None, content,\n                                             self._get_lexicon_option('ttl'),\n                                             self._get_lexicon_option('priority'))\n\n        self._request_modify_dns_record(record)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate existing record and changes it s sub - domain name", "response": "def _update_record_with_name(self, old_record, rtype, new_name, content):\n        \"\"\"Updates existing record and changes it's sub-domain name\"\"\"\n        new_type = rtype if rtype else old_record['type']\n\n        new_ttl = self._get_lexicon_option('ttl')\n        if new_ttl is None and 'ttl' in old_record:\n            new_ttl = old_record['ttl']\n\n        new_priority = self._get_lexicon_option('priority')\n        if new_priority is None and 'priority' in old_record:\n            new_priority = old_record['priority']\n\n        new_content = content\n        if new_content is None and 'content' in old_record:\n            new_content = old_record['content']\n\n        record = self._create_request_record(None,\n                                             new_type,\n                                             new_name,\n                                             new_content,\n                                             new_ttl,\n                                             new_priority)\n\n        # This will be a different domain name, so no name collision should\n        # happen. First create a new entry and when it succeeds, delete the old\n        # one.\n        self._request_add_dns_record(record)\n        self._request_delete_dns_record_by_id(old_record['id'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        to_delete_ids = list()\n        if identifier:\n            to_delete_ids.append(identifier)\n        else:\n            for record in self._list_records(rtype=rtype, name=name, content=content):\n                to_delete_ids.append(record[\"id\"])\n\n        for to_delete_id in to_delete_ids:\n            self._request_delete_dns_record_by_id(to_delete_id)\n        return True", "response": "Deletes an existing record"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a record for Subreg API calls.", "response": "def _create_request_record(self, identifier, rtype, name, content, ttl, priority):  # pylint: disable=too-many-arguments\n        \"\"\"Creates record for Subreg API calls\"\"\"\n        record = collections.OrderedDict()\n\n        # Mandatory content\n\n        # Just for update - not for creation\n        if identifier is not None:\n            record['id'] = identifier\n\n        record['type'] = rtype\n\n        # Just for creation - not for update\n        if name is not None:\n            record['name'] = self._relative_name(name)\n\n        # Optional content\n        if content is not None:\n            record['content'] = content\n        if ttl is not None:\n            record['ttl'] = ttl\n        if priority is not None:\n            record['prio'] = priority\n        return record"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a record for lexicon API calls", "response": "def _create_response_record(self, response):\n        \"\"\"Creates record for lexicon API calls\"\"\"\n        record = dict()\n        record['id'] = response['id']\n        record['type'] = response['type']\n        record['name'] = self._full_name(response['name'])\n        if 'content' in response:\n            record['content'] = response['content'] or \"\"\n        if 'ttl' in response:\n            record['ttl'] = response['ttl']\n        if 'prio' in response:\n            record['priority'] = response['prio']\n        return record"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _full_name(self, record_name):\n        # Handle None and empty strings\n        if not record_name:\n            return self.domain\n        return super(Provider, self)._full_name(record_name)", "response": "Returns the full domain name of a sub - domain name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _relative_name(self, record_name):\n        # Handle None and empty strings as None\n        if not record_name:\n            return None\n        subdomain = super(Provider, self)._relative_name(record_name)\n        return subdomain if subdomain else None", "response": "Returns the sub - domain of a domain name"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists all records by the specified criteria", "response": "def _list_records_internal(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"Lists all records by the specified criteria\"\"\"\n        response = self._request_get_dns_zone()\n        if 'records' in response:\n            # Interpret empty string as None because zeep does so too\n            content_check = content if content != \"\" else None\n            name_check = self._relative_name(name)\n\n            # Stringize the identifier to prevent any rtype differences\n            identifier_check = str(\n                identifier) if identifier is not None else None\n\n            filtered_records = [\n                record for record in response['records'] if (\n                    identifier is None or str(\n                        record['id']) == identifier_check) and (\n                            rtype is None or record['type'] == rtype) and (\n                                name is None or record['name'] == name_check) and (\n                                    content is None or (\n                                        'content' in record\n                                        and record['content'] == content_check))]\n            records = [self._create_response_record(\n                filtered_record) for filtered_record in filtered_records]\n        else:\n            records = []\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _guess_record(self, rtype, name=None, content=None):\n        records = self._list_records_internal(\n            identifier=None, rtype=rtype, name=name, content=content)\n        if len(records) == 1:\n            return records[0]\n        if len(records) > 1:\n            raise Exception(\n                'Identifier was not provided and several existing '\n                'records match the request for {0}/{1}'.format(rtype, name))\n        raise Exception(\n            'Identifier was not provided and no existing records match '\n            'the request for {0}/{1}'.format(rtype, name))", "response": "Tries to find existing unique record by type name and content"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _request_login(self, login, password):\n        return self._request_internal(\"Login\",\n                                      login=login,\n                                      password=password)", "response": "Sends a Login request"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _request_delete_dns_record_by_id(self, identifier):\n        return self._request_internal(\"Delete_DNS_Record\",\n                                      domain=self.domain,\n                                      record={'id': identifier})", "response": "Sends Delete_DNS_Record request with identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _request_internal(self, command, **kwargs):\n        args = dict(kwargs)\n        if self.ssid:\n            args['ssid'] = self.ssid\n        method = getattr(self.api, command)\n        response = method(**args)\n        if response and 'status' in response:\n            if response['status'] == 'error':\n                raise SubregError(\n                    message=response['error']['errormsg'],\n                    major=response['error']['errorcode']['major'],\n                    minor=response['error']['errorcode']['minor']\n                )\n            if response['status'] == 'ok':\n                return response['data'] if 'data' in response else dict()\n            raise Exception(\"Invalid status found in SOAP response\")\n        raise Exception('Invalid response')", "response": "Make request parse response"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _convert_to_namecheap(self, record):\n\n        name = record['name']\n        if name.endswith('.'):\n            name = name[:-1]\n\n        short_name = name[:name.find(self.domain) - 1]\n        processed_record = {\n            'Type': record['type'],\n            'Name': short_name,\n            'TTL': record['ttl'],\n            'Address': record['content'],\n            'HostId': record['id']\n        }\n\n        return processed_record", "response": "converts from lexicon format record to namecheap format record suitable to sending through the api to namecheap"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting from namecheap raw record format to lexicon format", "response": "def _convert_to_lexicon(self, record):\n        \"\"\" converts from namecheap raw record format to lexicon format record\n        \"\"\"\n\n        name = record['Name']\n        if self.domain not in name:\n            name = \"{}.{}\".format(name, self.domain)\n\n        processed_record = {\n            'type': record['Type'],\n            'name': '{0}.{1}'.format(record['Name'], self.domain),\n            'ttl': record['TTL'],\n            'content': record['Address'],\n            'id': record['HostId']\n        }\n\n        return processed_record"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _authenticate(self):\n        response = self._get(\"/v1/domains/{0}\".format(self.domain))\n\n        self.domain_id = response[\"domain\"][\"id\"]", "response": "An innocent call to check that the credentials are okay."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a record if it does not already exist with same content", "response": "def _create_record(self, rtype, name, content):\n        \"\"\"Create record if doesnt already exist with same content\"\"\"\n        # check if record already exists\n        existing_records = self._list_records(rtype, name, content)\n        if len(existing_records) >= 1:\n            return True\n\n        record = {\n            \"record_type\": rtype,\n            \"name\": self._relative_name(name),\n            \"content\": content,\n        }\n        if self._get_lexicon_option(\"ttl\"):\n            record[\"ttl\"] = self._get_lexicon_option(\"ttl\")\n        if self._get_lexicon_option(\"priority\"):\n            record[\"prio\"] = self._get_lexicon_option(\"priority\")\n\n        payload = self._post(\n            \"/v1/domains/{0}/records\".format(self.domain),\n            {\"record\": record},\n        )\n\n        status = \"id\" in payload.get(\"record\", {})\n        LOGGER.debug(\"create_record: %s\", status)\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlist all records. record_type, name and content are used to filter the records. If possible it filters during the query, otherwise afterwards. An empty list is returned if no records are found.", "response": "def _list_records(self, rtype=None, name=None, content=None):\n        \"\"\"List all records.\n\n        record_type, name and content are used to filter the records.\n        If possible it filters during the query, otherwise afterwards.\n        An empty list is returned if no records are found.\n        \"\"\"\n\n        filter_query = {}\n        if rtype:\n            filter_query[\"record_type\"] = rtype\n        if name:\n            name = self._relative_name(name)\n            filter_query[\"name\"] = name\n        payload = self._get(\n            \"/v1/domains/{0}/records\".format(self.domain),\n            query_params=filter_query,\n        )\n\n        records = []\n        for data in payload:\n            record = data[\"record\"]\n\n            if content and record[\"content\"] != content:\n                continue\n\n            if record[\"name\"] == \"\":\n                rname = self.domain\n            else:\n                rname = \".\".join((record[\"name\"], self.domain))\n\n            processed_record = {\n                \"type\": record[\"record_type\"],\n                \"name\": rname,\n                \"ttl\": record[\"ttl\"],\n                \"content\": record[\"content\"],\n                \"id\": record[\"id\"],\n            }\n            if record[\"prio\"]:\n                processed_record[\"options\"] = {\n                    \"mx\": {\"priority\": record[\"prio\"]}\n                }\n            records.append(processed_record)\n\n        LOGGER.debug(\"list_records: %s\", records)\n        return records"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates or update a record.", "response": "def _update_record(self, identifier, rtype=None, name=None, content=None):\n        \"\"\"Create or update a record.\"\"\"\n        record = {}\n\n        if not identifier:\n            records = self._list_records(rtype, name, content)\n            identifiers = [r[\"id\"] for r in records]\n        else:\n            identifiers = [identifier]\n\n        if name:\n            record[\"name\"] = self._relative_name(name)\n        if content:\n            record[\"content\"] = content\n        if self._get_lexicon_option('ttl'):\n            record[\"ttl\"] = self._get_lexicon_option('ttl')\n        if self._get_lexicon_option('priority'):\n            record[\"prio\"] = self._get_lexicon_option('priority')\n\n        LOGGER.debug(\"update_records: %s\", identifiers)\n\n        for record_id in identifiers:\n            self._put(\n                \"/v1/domains/{0}/records/{1}\".format(\n                    self.domain, identifier\n                ),\n                record,\n            )\n            LOGGER.debug(\"update_record: %s\", record_id)\n\n        LOGGER.debug(\"update_record: %s\", True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete an existing record.", "response": "def _delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"Delete an existing record.\n\n        If the record doesn't exist, does nothing.\n        \"\"\"\n        if not identifier:\n            records = self._list_records(rtype, name, content)\n            identifiers = [record[\"id\"] for record in records]\n        else:\n            identifiers = [identifier]\n\n        LOGGER.debug(\"delete_records: %s\", identifiers)\n\n        for record_id in identifiers:\n            self._delete(\n                \"/v1/domains/{0}/records/{1}\".format(\n                    self.domain, record_id\n                )\n            )\n            LOGGER.debug(\"delete_record: %s\", record_id)\n\n        LOGGER.debug(\"delete_record: %s\", True)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconfigure provider parser for auto provider", "response": "def provider_parser(subparser):\n    \"\"\"Configure provider parser for auto provider\"\"\"\n    subparser.description = '''\n        Provider 'auto' enables the Lexicon provider auto-discovery.\n        Based on the nameservers declared for the given domain,\n        Lexicon will try to find the DNS provider holding the DNS zone if it is supported.\n        Actual DNS zone read/write operations will be delegated to the provider found:\n        every environment variable or command line specific to this provider\n        can be passed to Lexicon and will be processed accordingly.\n        '''\n    subparser.add_argument(\"--mapping-override\", metavar=\"[DOMAIN]:[PROVIDER], ...\",\n                           help=\"comma separated list of elements in the form of \"\n                                \"[DOMAIN]:[PROVIDER] to authoritatively map a \"\n                                \"particular domain to a particular provider\")\n\n    # Explore and load the arguments available for every provider into the 'auto' provider.\n    for provider_name, provider_module in AVAILABLE_PROVIDERS.items():\n        parser = argparse.ArgumentParser(add_help=False)\n        provider_module.provider_parser(parser)\n\n        for action in parser._actions:  # pylint: disable=protected-access\n            action.option_strings = [re.sub(\n                r'^--(.*)$', r'--{0}-\\1'.format(provider_name), option)\n                                     for option in action.option_strings]\n            action.dest = 'auto_{0}_{1}'.format(provider_name, action.dest)\n            subparser._add_action(action)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef authenticate(self):  # pylint: disable=too-many-locals\n        mapping_override = self.config.resolve('lexicon:auto:mapping_override')\n        mapping_override_processed = {}\n        if mapping_override:\n            for one_mapping in mapping_override.split(','):\n                one_mapping_processed = one_mapping.split(':')\n                mapping_override_processed[one_mapping_processed[0]\n                                           ] = one_mapping_processed[1]\n\n        override_provider = mapping_override_processed.get(self.domain)\n        if override_provider:\n            provider = [\n                element for element in AVAILABLE_PROVIDERS.items()\n                if element[0] == override_provider][0]\n            LOGGER.info('Provider authoritatively mapped for domain %s: %s.',\n                        self.domain, provider.__name__)\n            (provider_name, provider_module) = provider\n        else:\n            (provider_name, provider_module) = _relevant_provider_for_domain(self.domain)\n            LOGGER.info('Provider discovered for domain %s: %s.',\n                        self.domain, provider_name)\n\n        new_config = ConfigResolver()\n        new_config.with_dict({'lexicon:provider_name': provider_name})\n\n        target_prefix = 'auto_{0}_'.format(provider_name)\n        for config_source in self.config._config_sources:  # pylint: disable=protected-access\n            if not isinstance(config_source, ArgsConfigSource):\n                new_config.with_config_source(config_source)\n            else:\n                # ArgsConfigSource needs to be reprocessed to rescope the provided\n                # args to the delegate provider\n                new_dict = {}\n                for key, value in config_source._parameters.items():  # pylint: disable=protected-access\n                    if key.startswith(target_prefix):\n                        new_param_name = re.sub(\n                            '^{0}'.format(target_prefix), '', key)\n                        new_dict['lexicon:{0}:{1}'.format(\n                            provider_name, new_param_name)] = value\n                    elif not key.startswith('auto_'):\n                        new_dict['lexicon:{0}'.format(key)] = value\n                new_config.with_dict(new_dict)\n\n        self.proxy_provider = provider_module.Provider(new_config)\n        self.proxy_provider.authenticate()", "response": "Launch the authentication process for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfunction that generates the base provider parser that can be used by all dns providers.", "response": "def generate_base_provider_parser():\n    \"\"\"Function that generates the base provider to be used by all dns providers.\"\"\"\n    parser = argparse.ArgumentParser(add_help=False)\n    parser.add_argument('action', help='specify the action to take', default='list',\n                        choices=['create', 'list', 'update', 'delete'])\n    parser.add_argument(\n        'domain', help='specify the domain, supports subdomains as well')\n    parser.add_argument('type', help='specify the entry type', default='TXT',\n                        choices=['A', 'AAAA', 'CNAME', 'MX', 'NS', 'SOA', 'TXT', 'SRV', 'LOC'])\n\n    parser.add_argument('--name', help='specify the record name')\n    parser.add_argument('--content', help='specify the record content')\n    parser.add_argument('--ttl', type=int,\n                        help='specify the record time-to-live')\n    parser.add_argument('--priority', help='specify the record priority')\n    parser.add_argument(\n        '--identifier', help='specify the record for update or delete actions')\n    parser.add_argument('--log_level', help='specify the log level', default='ERROR',\n                        choices=['CRITICAL', 'ERROR', 'WARNING', 'INFO', 'DEBUG', 'NOTSET'])\n    parser.add_argument('--output',\n                        help=('specify the type of output: by default a formatted table (TABLE), '\n                              'a formatted table without header (TABLE-NO-HEADER), '\n                              'a JSON string (JSON) or no output (QUIET)'),\n                        default='TABLE', choices=['TABLE', 'TABLE-NO-HEADER', 'JSON', 'QUIET'])\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating a parser that will be used by Lexicon CLI", "response": "def generate_cli_main_parser():\n    \"\"\"Using all providers available, generate a parser that will be used by Lexicon CLI\"\"\"\n    parser = argparse.ArgumentParser(\n        description='Create, Update, Delete, List DNS entries')\n\n    parser.add_argument('--version', help='show the current version of lexicon',\n                        action='version', version='%(prog)s {0}'\n                        .format(discovery.lexicon_version()))\n    parser.add_argument('--delegated', help='specify the delegated domain')\n    parser.add_argument('--config-dir', default=os.getcwd(),\n                        help='specify the directory where to search lexicon.yml and '\n                             'lexicon_[provider].yml configuration files '\n                             '(default: current directory).')\n    subparsers = parser.add_subparsers(\n        dest='provider_name', help='specify the DNS provider to use')\n    subparsers.required = True\n\n    for provider, available in discovery.find_providers().items():\n        provider_module = importlib.import_module(\n            'lexicon.providers.' + provider)\n        provider_parser = getattr(provider_module, 'provider_parser')\n\n        subparser = subparsers.add_parser(provider, help='{0} provider'.format(provider),\n                                          parents=[generate_base_provider_parser()])\n        provider_parser(subparser)\n\n        if not available:\n            subparser.epilog = ('WARNING: some required dependencies for this provider are not '\n                                'installed. Please install lexicon[{0}] first before using it.'\n                                .format(provider))\n\n    return parser"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a resource record.", "response": "def _create_record(self, rtype, name, content):\n        \"\"\"\n        Create a resource record. If a record already exists with the same\n        content, do nothing.\n        \"\"\"\n        result = False\n        name = self._relative_name(name)\n        ttl = None\n\n        # TODO: shoud assert that this is an int\n        if self.ttl:\n            ttl = self.ttl\n\n        with localzone.manage(self.filename, self.origin, autosave=True) as zone:\n            if zone.add_record(name, rtype, content, ttl=ttl):  # pylint: disable=no-member\n                result = True\n\n        LOGGER.debug(\"create_record: %s\", result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _list_records(self, rtype=None, name=None, content=None):\n        if name:\n            name = self._relative_name(name)\n        if not rtype:\n            rtype = \"ANY\"\n\n        filter_query = {\"rdtype\": rtype, \"name\": name, \"content\": content}\n\n        with localzone.manage(self.filename, self.origin, autosave=True) as zone:\n            records = zone.find_record(**filter_query)  # pylint: disable=no-member\n\n        result = []\n        for record in records:\n            rdict = {\n                \"type\": record.rdtype,\n                \"name\": self._full_name(record.name),\n                \"ttl\": record.ttl,\n                \"content\": record.content,\n                \"id\": record.hashid,\n            }\n\n            if rdict[\"type\"] == \"TXT\":\n                rdict[\"content\"] = rdict[\"content\"].replace('\"', \"\")\n\n            result.append(rdict)\n\n        LOGGER.debug(\"list_records: %s\", result)\n        return result", "response": "Return a list of records matching the supplied params."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate a record. Returns True if the record was updated False otherwise.", "response": "def _update_record(self, identifier, rtype=None, name=None, content=None):\n        \"\"\"\n        Update a record. Returns `False` if no matching record is found.\n        \"\"\"\n        result = False\n\n        # TODO: some providers allow content-based updates without supplying an\n        # ID, and therefore `identifier` is here optional. If we don't receive\n        # an ID, look it up.\n        if not identifier and rtype and name:\n            records = self._list_records(rtype, name)\n            if len(records) == 1:\n                identifier = records[0][\"id\"]\n\n        if identifier and content:\n            with localzone.manage(self.filename, self.origin, autosave=True) as zone:\n                if zone.update_record(identifier, content):  # pylint: disable=no-member\n                    result = True\n\n        LOGGER.debug(\"update_record: %s\", result)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting record matching the provided params.", "response": "def _delete_record(self, identifier=None, rtype=None, name=None, content=None):\n        \"\"\"\n        Delete record(s) matching the provided params. If there is no match, do\n        nothing.\n        \"\"\"\n        ids = []\n\n        if identifier:\n            ids.append(identifier)\n        elif not identifier and rtype and name:\n            records = self._list_records(rtype, name, content)\n            if records:\n                ids = [record[\"id\"] for record in records]\n\n        if ids:\n            LOGGER.debug(\"delete_records: %s\", ids)\n            with localzone.manage(self.filename, self.origin, autosave=True) as zone:\n                for hashid in ids:\n                    zone.remove_record(hashid)  # pylint: disable=no-member\n                    LOGGER.debug(\"delete_record: %s\", hashid)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_random_state(state):\n    randgen.state_set = True\n    randgen.setstate(state)\n\n    faker.generator.random.setstate(state)", "response": "Force - set the state of factory. fuzzy s random generator."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_model(app, model):\n    if 'get_model' not in _LAZY_LOADS:\n        _lazy_load_get_model()\n\n    _get_model = _LAZY_LOADS['get_model']\n    return _get_model(app, model)", "response": "Wrapper around django s get_model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an instance of the model through objects. get_or_create.", "response": "def _get_or_create(cls, model_class, *args, **kwargs):\n        \"\"\"Create an instance of the model through objects.get_or_create.\"\"\"\n        manager = cls._get_manager(model_class)\n\n        assert 'defaults' not in cls._meta.django_get_or_create, (\n            \"'defaults' is a reserved keyword for get_or_create \"\n            \"(in %s._meta.django_get_or_create=%r)\"\n            % (cls, cls._meta.django_get_or_create))\n\n        key_fields = {}\n        for field in cls._meta.django_get_or_create:\n            if field not in kwargs:\n                raise errors.FactoryError(\n                    \"django_get_or_create - \"\n                    \"Unable to find initialization value for '%s' in factory %s\" %\n                    (field, cls.__name__))\n            key_fields[field] = kwargs.pop(field)\n        key_fields['defaults'] = kwargs\n\n        try:\n            instance, _created = manager.get_or_create(*args, **key_fields)\n        except IntegrityError:\n            try:\n                instance = manager.get(**cls._original_params)\n            except manager.model.DoesNotExist:\n                raise ValueError(\n                    \"django_get_or_create - Unable to create a new object \"\n                    \"due an IntegrityError raised based on \"\n                    \"your model's uniqueness constraints. \"\n                    \"DoesNotExist: Unable to find an existing object based on \"\n                    \"the fields specified in your factory instance.\")\n\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an instance of the model and save it to the database.", "response": "def _create(cls, model_class, *args, **kwargs):\n        \"\"\"Create an instance of the model, and save it to the database.\"\"\"\n        if cls._meta.django_get_or_create:\n            return cls._get_or_create(model_class, *args, **kwargs)\n\n        manager = cls._get_manager(model_class)\n        return manager.create(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate(self, step, params):\n        # Recurse into a DictFactory: allows users to have some params depending\n        # on others.\n        params = step.recurse(base.DictFactory, params, force_sequence=step.sequence)\n        filename, content = self._make_content(params)\n        return django_files.File(content.file, filename)", "response": "Generate the file for the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports an object from its absolute path.", "response": "def import_object(module_name, attribute_name):\n    \"\"\"Import an object from its absolute path.\n\n    Example:\n        >>> import_object('datetime', 'datetime')\n        <type 'datetime.datetime'>\n    \"\"\"\n    # Py2 compatibility: force str (i.e bytes) when importing.\n    module = __import__(str(module_name), {}, {}, [str(attribute_name)], 0)\n    return getattr(module, str(attribute_name))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsorting an iterable of OrderedBase instances.", "response": "def sort_ordered_objects(items, getter=lambda x: x):\n    \"\"\"Sort an iterable of OrderedBase instances.\n\n    Args:\n        items (iterable): the objects to sort\n        getter (callable or None): a function to extract the OrderedBase instance from an object.\n\n    Examples:\n        >>> sort_ordered_objects([x, y, z])\n        >>> sort_ordered_objects(v.items(), getter=lambda e: e[1])\n    \"\"\"\n    return sorted(items, key=lambda x: getattr(getter(x), OrderedBase.CREATION_COUNTER_FIELD, -1))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding the first definition of an attribute according to MRO order.", "response": "def resolve_attribute(name, bases, default=None):\n    \"\"\"Find the first definition of an attribute according to MRO order.\"\"\"\n    for base in bases:\n        if hasattr(base, name):\n            return getattr(base, name)\n    return default"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nforces the use of a different strategy.", "response": "def use_strategy(new_strategy):\n    \"\"\"Force the use of a different strategy.\n\n    This is an alternative to setting default_strategy in the class definition.\n    \"\"\"\n    def wrapped_class(klass):\n        klass._meta.strategy = new_strategy\n        return klass\n    return wrapped_class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nproviding the default value for all allowed fields.", "response": "def _build_default_options(self):\n        \"\"\"\"Provide the default value for all allowed fields.\n\n        Custom FactoryOptions classes should override this method\n        to update() its return value.\n        \"\"\"\n        return [\n            OptionDefault('model', None, inherit=True),\n            OptionDefault('abstract', False, inherit=False),\n            OptionDefault('strategy', enums.CREATE_STRATEGY, inherit=True),\n            OptionDefault('inline_args', (), inherit=True),\n            OptionDefault('exclude', (), inherit=True),\n            OptionDefault('rename', {}, inherit=True),\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nidentify which factory should be used for a shared counter.", "response": "def _get_counter_reference(self):\n        \"\"\"Identify which factory should be used for a shared counter.\"\"\"\n\n        if (self.model is not None\n                and self.base_factory is not None\n                and self.base_factory._meta.model is not None\n                and issubclass(self.model, self.base_factory._meta.model)):\n            return self.base_factory._meta.counter_reference\n        else:\n            return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _initialize_counter(self):\n        if self._counter is not None:\n            return\n\n        if self.counter_reference is self:\n            self._counter = _Counter(seq=self.factory._setup_next_sequence())\n        else:\n            self.counter_reference._initialize_counter()\n            self._counter = self.counter_reference._counter", "response": "Initialize our counter pointer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting an attributes dict to a tuple of args and kwargs.", "response": "def prepare_arguments(self, attributes):\n        \"\"\"Convert an attributes dict to a (args, kwargs) tuple.\"\"\"\n        kwargs = dict(attributes)\n        # 1. Extension points\n        kwargs = self.factory._adjust_kwargs(**kwargs)\n\n        # 2. Remove hidden objects\n        kwargs = {\n            k: v for k, v in kwargs.items()\n            if k not in self.exclude and k not in self.parameters and v is not declarations.SKIP\n        }\n\n        # 3. Rename fields\n        for old_name, new_name in self.rename.items():\n            if old_name in kwargs:\n                kwargs[new_name] = kwargs.pop(old_name)\n\n        # 4. Extract inline args\n        args = tuple(\n            kwargs.pop(arg_name)\n            for arg_name in self.inline_args\n        )\n\n        return args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines if a class attribute is a field value declaration.", "response": "def _is_declaration(self, name, value):\n        \"\"\"Determines if a class attribute is a field value declaration.\n\n        Based on the name and value of the class attribute, return ``True`` if\n        it looks like a declaration of a default field value, ``False`` if it\n        is private (name starts with '_') or a classmethod or staticmethod.\n\n        \"\"\"\n        if isinstance(value, (classmethod, staticmethod)):\n            return False\n        elif enums.get_builder_phase(value):\n            # All objects with a defined 'builder phase' are declarations.\n            return True\n        return not name.startswith(\"_\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _check_parameter_dependencies(self, parameters):\n        # Warning: parameters only provide reverse dependencies; we reverse them into standard dependencies.\n        # deep_revdeps: set of fields a field depend indirectly upon\n        deep_revdeps = collections.defaultdict(set)\n        # Actual, direct dependencies\n        deps = collections.defaultdict(set)\n\n        for name, parameter in parameters.items():\n            if isinstance(parameter, declarations.Parameter):\n                field_revdeps = parameter.get_revdeps(parameters)\n                if not field_revdeps:\n                    continue\n                deep_revdeps[name] = set.union(*(deep_revdeps[dep] for dep in field_revdeps))\n                deep_revdeps[name] |= set(field_revdeps)\n                for dep in field_revdeps:\n                    deps[dep].add(name)\n\n        # Check for cyclical dependencies\n        cyclic = [name for name, field_deps in deep_revdeps.items() if name in field_deps]\n        if cyclic:\n            raise errors.CyclicDefinitionError(\n                \"Cyclic definition detected on %r; Params around %s\"\n                % (self.factory, ', '.join(cyclic)))\n        return deps", "response": "Find out in what order parameters should be called."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreset the sequence counter.", "response": "def reset_sequence(cls, value=None, force=False):\n        \"\"\"Reset the sequence counter.\n\n        Args:\n            value (int or None): the new 'next' sequence value; if None,\n                recompute the next value from _setup_next_sequence().\n            force (bool): whether to force-reset parent sequence counters\n                in a factory inheritance chain.\n        \"\"\"\n        cls._meta.reset_sequence(value, force=force)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nbuilds a dictionary of attribute values for the given class.", "response": "def attributes(cls, create=False, extra=None):\n        \"\"\"Build a dict of attribute values, respecting declaration order.\n\n        The process is:\n        - Handle 'orderless' attributes, overriding defaults with provided\n            kwargs when applicable\n        - Handle ordered attributes, overriding them with provided kwargs when\n            applicable; the current list of computed attributes is available\n            to the currently processed object.\n        \"\"\"\n        warnings.warn(\n            \"Usage of Factory.attributes() is deprecated.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        declarations = cls._meta.pre_declarations.as_dict()\n        declarations.update(extra or {})\n        from . import helpers\n        return helpers.make_factory(dict, **declarations)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef declarations(cls, extra_defs=None):\n        warnings.warn(\n            \"Factory.declarations is deprecated; use Factory._meta.pre_declarations instead.\",\n            DeprecationWarning,\n            stacklevel=2,\n        )\n        decls = cls._meta.pre_declarations.as_dict()\n        decls.update(extra_defs or {})\n        return decls", "response": "Retrieve a copy of the declared attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the object. Args: params (dict): attributes to use for generating the object strategy: the strategy to use", "response": "def _generate(cls, strategy, params):\n        \"\"\"generate the object.\n\n        Args:\n            params (dict): attributes to use for generating the object\n            strategy: the strategy to use\n        \"\"\"\n        if cls._meta.abstract:\n            raise errors.FactoryError(\n                \"Cannot generate instances of abstract factory %(f)s; \"\n                \"Ensure %(f)s.Meta.model is set and %(f)s.Meta.abstract \"\n                \"is either not set or False.\" % dict(f=cls.__name__))\n\n        step = builder.StepBuilder(cls._meta, params, strategy)\n        return step.build()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_batch(cls, size, **kwargs):\n        return [cls.build(**kwargs) for _ in range(size)]", "response": "Build a batch of instances of the given class with overriden attrs."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_batch(cls, size, **kwargs):\n        return [cls.create(**kwargs) for _ in range(size)]", "response": "Create a batch of instances of the given class with overriden attrs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef stub_batch(cls, size, **kwargs):\n        return [cls.stub(**kwargs) for _ in range(size)]", "response": "Stub a batch of instances of the given class with overriden attrs.\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a new instance of the class with the given strategy.", "response": "def generate(cls, strategy, **kwargs):\n        \"\"\"Generate a new instance.\n\n        The instance will be created with the given strategy (one of\n        BUILD_STRATEGY, CREATE_STRATEGY, STUB_STRATEGY).\n\n        Args:\n            strategy (str): the strategy to use for generating the instance.\n\n        Returns:\n            object: the generated instance\n        \"\"\"\n        assert strategy in (enums.STUB_STRATEGY, enums.BUILD_STRATEGY, enums.CREATE_STRATEGY)\n        action = getattr(cls, strategy)\n        return action(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_batch(cls, strategy, size, **kwargs):\n        assert strategy in (enums.STUB_STRATEGY, enums.BUILD_STRATEGY, enums.CREATE_STRATEGY)\n        batch_action = getattr(cls, '%s_batch' % strategy)\n        return batch_action(size, **kwargs)", "response": "Generate a batch of instances."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simple_generate(cls, create, **kwargs):\n        strategy = enums.CREATE_STRATEGY if create else enums.BUILD_STRATEGY\n        return cls.generate(strategy, **kwargs)", "response": "Generate a new instance of the class with the specified parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a batch of instances.", "response": "def simple_generate_batch(cls, create, size, **kwargs):\n        \"\"\"Generate a batch of instances.\n\n        These instances will be either 'built' or 'created'.\n\n        Args:\n            size (int): the number of instances to generate\n            create (bool): whether to 'build' or 'create' the instances.\n\n        Returns:\n            object list: the generated instances\n        \"\"\"\n        strategy = enums.CREATE_STRATEGY if create else enums.BUILD_STRATEGY\n        return cls.generate_batch(strategy, size, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_factory(klass, **kwargs):\n    factory_name = '%sFactory' % klass.__name__\n\n    class Meta:\n        model = klass\n\n    kwargs['Meta'] = Meta\n    base_class = kwargs.pop('FACTORY_CLASS', base.Factory)\n\n    factory_class = type(base.Factory).__new__(type(base.Factory), factory_name, (base_class,), kwargs)\n    factory_class.__name__ = '%sFactory' % klass.__name__\n    factory_class.__doc__ = 'Auto-generated factory for class %s' % klass\n    return factory_class", "response": "Create a new simple factory for the given class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_batch(klass, strategy, size, **kwargs):\n    return make_factory(klass, **kwargs).generate_batch(strategy, size)", "response": "Create a factory for the given class and generate instances."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simple_generate_batch(klass, create, size, **kwargs):\n    return make_factory(klass, **kwargs).simple_generate_batch(create, size)", "response": "Create a factory for the given class and simple_generate instances."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating an instance of the model and save it to the database.", "response": "def _create(cls, model_class, *args, **kwargs):\n        \"\"\"Create an instance of the model, and save it to the database.\"\"\"\n        session = cls._meta.sqlalchemy_session\n        session_persistence = cls._meta.sqlalchemy_session_persistence\n        if cls._meta.force_flush:\n            session_persistence = SESSION_PERSISTENCE_FLUSH\n\n        obj = model_class(*args, **kwargs)\n        if session is None:\n            raise RuntimeError(\"No session provided.\")\n        session.add(obj)\n        if session_persistence == SESSION_PERSISTENCE_FLUSH:\n            session.flush()\n        elif session_persistence == SESSION_PERSISTENCE_COMMIT:\n            session.commit()\n        return obj"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deepgetattr(obj, name, default=_UNSPECIFIED):\n    try:\n        if '.' in name:\n            attr, subname = name.split('.', 1)\n            return deepgetattr(getattr(obj, attr), subname, default)\n        else:\n            return getattr(obj, name)\n    except AttributeError:\n        if default is _UNSPECIFIED:\n            raise\n        else:\n            return default", "response": "Try to retrieve the given attribute of an object recursively splitting on."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate(self, instance, step, extra):\n        # Strip the current instance from the chain\n        chain = step.chain[1:]\n        if self.strict and not chain:\n            raise TypeError(\n                \"A ContainerAttribute in 'strict' mode can only be used \"\n                \"within a SubFactory.\")\n\n        return self.function(instance, chain)", "response": "Evaluate the current ContainerAttribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef evaluate(self, instance, step, extra):\n        defaults = dict(self.defaults)\n        if extra:\n            defaults.update(extra)\n\n        return self.generate(step, defaults)", "response": "Evaluate the current definition and fill its attributes with the values defined when instantiating the containing factory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate(self, step, params):\n        subfactory = self.get_factory()\n        logger.debug(\n            \"SubFactory: Instantiating %s.%s(%s), create=%r\",\n            subfactory.__module__, subfactory.__name__,\n            utils.log_pprint(kwargs=params),\n            step,\n        )\n        force_sequence = step.sequence if self.FORCE_SEQUENCE else None\n        return step.recurse(subfactory, params, force_sequence=force_sequence)", "response": "Evaluate the current definition and fill its attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef split(cls, entry):\n        if enums.SPLITTER in entry:\n            return entry.split(enums.SPLITTER, 1)\n        else:\n            return (entry, None)", "response": "Split a declaration name into a tuple."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef join(cls, root, subkey):\n        if subkey is None:\n            return root\n        return enums.SPLITTER.join((root, subkey))", "response": "Rebuild a full declaration name from its components."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, values):\n        for k, v in values.items():\n            root, sub = self.split(k)\n            if sub is None:\n                self.declarations[root] = v\n            else:\n                self.contexts[root][sub] = v\n\n        extra_context_keys = set(self.contexts) - set(self.declarations)\n        if extra_context_keys:\n            raise errors.InvalidDeclarationError(\n                \"Received deep context for unknown fields: %r (known=%r)\" % (\n                    {\n                        self.join(root, sub): v\n                        for root in extra_context_keys\n                        for sub, v in self.contexts[root].items()\n                    },\n                    sorted(self.declarations),\n                )\n            )", "response": "Add new declarations to this set and update the deep context set with the new ones."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter a set of declarations that are related to this object", "response": "def filter(self, entries):\n        \"\"\"Filter a set of declarations: keep only those related to this object.\n\n        This will keep:\n        - Declarations that 'override' the current ones\n        - Declarations that are parameters to current ones\n        \"\"\"\n        return [\n            entry for entry in entries\n            if self.split(entry)[0] in self.declarations\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract a list of ( key value pairs suitable for our __init__.", "response": "def _items(self):\n        \"\"\"Extract a list of (key, value) pairs, suitable for our __init__.\"\"\"\n        for name in self.declarations:\n            yield name, self.declarations[name]\n            for subkey, value in self.contexts[name].items():\n                yield self.join(name, subkey), value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild a new instance of the factory class.", "response": "def build(self, parent_step=None, force_sequence=None):\n        \"\"\"Build a factory instance.\"\"\"\n        # TODO: Handle \"batch build\" natively\n        pre, post = parse_declarations(\n            self.extras,\n            base_pre=self.factory_meta.pre_declarations,\n            base_post=self.factory_meta.post_declarations,\n        )\n\n        if force_sequence is not None:\n            sequence = force_sequence\n        elif self.force_init_sequence is not None:\n            sequence = self.force_init_sequence\n        else:\n            sequence = self.factory_meta.next_sequence()\n\n        step = BuildStep(\n            builder=self,\n            sequence=sequence,\n            parent_step=parent_step,\n        )\n        step.resolve(pre)\n\n        args, kwargs = self.factory_meta.prepare_arguments(step.attributes)\n\n        instance = self.factory_meta.instantiate(\n            step=step,\n            args=args,\n            kwargs=kwargs,\n        )\n\n        postgen_results = {}\n        for declaration_name in post.sorted():\n            declaration = post[declaration_name]\n            unrolled_context = declaration.declaration.unroll_context(\n                instance=instance,\n                step=step,\n                context=declaration.context,\n            )\n\n            postgen_context = PostGenerationContext(\n                value_provided='' in unrolled_context,\n                value=unrolled_context.get(''),\n                extra={k: v for k, v in unrolled_context.items() if k != ''},\n            )\n            postgen_results[declaration_name] = declaration.declaration.call(\n                instance=instance,\n                step=step,\n                context=postgen_context,\n            )\n        self.factory_meta.use_postgeneration_results(\n            instance=instance,\n            step=step,\n            results=postgen_results,\n        )\n        return instance"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef recurse(self, factory_meta, extras):\n        return self.__class__(factory_meta, extras, strategy=self.strategy)", "response": "Recurse into a sub - factory call."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse an expression into an AST", "response": "def parse(code, mode='exec', **exception_kwargs):\n    \"\"\"Parse an expression into AST\"\"\"\n\n    try:\n        return _ast_util.parse(code, '<unknown>', mode)\n    except Exception:\n        raise exceptions.SyntaxException(\n                    \"(%s) %s (%r)\" % (\n                        compat.exception_as().__class__.__name__,\n                        compat.exception_as(),\n                        code[0:50]\n                    ), **exception_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef tokenize(expr):\n    tokens = []\n    escape = False\n    cur_token = ''\n\n    for c in expr:\n        if escape == True:\n            cur_token += c\n            escape = False\n        else:\n            if c == '\\\\':\n                # Next char will be escaped\n                escape = True\n                continue\n            elif c == '[':\n                # Next token is of type index (list)\n                if len(cur_token) > 0:\n                    tokens.append(cur_token)\n                    cur_token = ''\n            elif c == ']':\n                # End of index token. Next token defaults to a key (dict)\n                if len(cur_token) > 0:\n                    tokens.append(int(cur_token))\n                    cur_token = ''\n            elif c == '.':\n                # End of key token. Next token defaults to a key (dict)\n                if len(cur_token) > 0:\n                    tokens.append(cur_token)\n                    cur_token = ''\n            else:\n                # Append char to token name\n                cur_token += c\n    if len(cur_token) > 0:\n        tokens.append(cur_token)\n\n    return tokens", "response": "Parse a string expression into a list of tokens that can be used as a path\n    into a Python datastructure."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the value of a key in a JSON structure.", "response": "def jsonxs(data, expr, action=ACTION_GET, value=None, default=None):\n    \"\"\"\n    Get, set, delete values in a JSON structure. `expr` is a JSONpath-like\n    expression pointing to the desired value. `action` determines the action to\n    perform. See the module-level `ACTION_*` constants. `value` should be given\n    if action is `ACTION_SET`. If `default` is set and `expr` isn't found,\n    return `default` instead. This will override all exceptions.\n    \"\"\"\n    tokens = tokenize(expr)\n\n    # Walk through the list of tokens to reach the correct path in the data\n    # structure.\n    try:\n        prev_path = None\n        cur_path = data\n        for token in tokens:\n            prev_path = cur_path\n            if not token in cur_path and action in [ACTION_SET, ACTION_MKDICT, ACTION_MKLIST]:\n                # When setting values or creating dicts/lists, the key can be\n                # missing from the data struture\n               continue\n            cur_path = cur_path[token]\n    except Exception:\n        if default is not None:\n            return default\n        else:\n            raise\n\n    # Perform action the user requested.\n    if action == ACTION_GET:\n        return cur_path\n    elif action == ACTION_DEL:\n        del prev_path[token]\n    elif action == ACTION_SET:\n        prev_path[token] = value\n    elif action == ACTION_APPEND:\n        prev_path[token].append(value)\n    elif action == ACTION_INSERT:\n        prev_path.insert(token, value)\n    elif action == ACTION_MKDICT:\n        prev_path[token] = {}\n    elif action == ACTION_MKLIST:\n        prev_path[token] = []\n    else:\n        raise ValueError(\"Invalid action: {}\".format(action))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef compile(node,\n                uri,\n                filename=None,\n                default_filters=None,\n                buffer_filters=None,\n                imports=None,\n                future_imports=None,\n                source_encoding=None,\n                generate_magic_comment=True,\n                disable_unicode=False,\n                strict_undefined=False,\n                enable_loop=True,\n                reserved_names=frozenset()):\n\n    \"\"\"Generate module source code given a parsetree node,\n      uri, and optional source filename\"\"\"\n\n    # if on Py2K, push the \"source_encoding\" string to be\n    # a bytestring itself, as we will be embedding it into\n    # the generated source and we don't want to coerce the\n    # result into a unicode object, in \"disable_unicode\" mode\n    if not compat.py3k and isinstance(source_encoding, compat.text_type):\n        source_encoding = source_encoding.encode(source_encoding)\n\n\n    buf = util.FastEncodingBuffer()\n\n    printer = PythonPrinter(buf)\n    _GenerateRenderMethod(printer,\n                            _CompileContext(uri,\n                                            filename,\n                                            default_filters,\n                                            buffer_filters,\n                                            imports,\n                                            future_imports,\n                                            source_encoding,\n                                            generate_magic_comment,\n                                            disable_unicode,\n                                            strict_undefined,\n                                            enable_loop,\n                                            reserved_names),\n                                node)\n    return buf.getvalue()", "response": "Generate a module source code given a parsetree node uri and optional source filename."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef mangle_mako_loop(node, printer):\n    loop_variable = LoopVariable()\n    node.accept_visitor(loop_variable)\n    if loop_variable.detected:\n        node.nodes[-1].has_loop_context = True\n        match = _FOR_LOOP.match(node.text)\n        if match:\n            printer.writelines(\n                    'loop = __M_loop._enter(%s)' % match.group(2),\n                    'try:'\n                    #'with __M_loop(%s) as loop:' % match.group(2)\n            )\n            text = 'for %s in loop:' % match.group(1)\n        else:\n            raise SyntaxError(\"Couldn't apply loop context: %s\" % node.text)\n    else:\n        text = node.text\n    return text", "response": "converts a for loop into a context manager wrapped around a for loop\n    when access to the loop variable has been detected in the for loop body\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntraverse a template structure for module - level directives and generate the start of the module - level code.", "response": "def write_toplevel(self):\n        \"\"\"Traverse a template structure for module-level directives and\n        generate the start of module-level code.\n\n        \"\"\"\n        inherit = []\n        namespaces = {}\n        module_code = []\n\n        self.compiler.pagetag = None\n\n        class FindTopLevel(object):\n            def visitInheritTag(s, node):\n                inherit.append(node)\n            def visitNamespaceTag(s, node):\n                namespaces[node.name] = node\n            def visitPageTag(s, node):\n                self.compiler.pagetag = node\n            def visitCode(s, node):\n                if node.ismodule:\n                    module_code.append(node)\n\n        f = FindTopLevel()\n        for n in self.node.nodes:\n            n.accept_visitor(f)\n\n        self.compiler.namespaces = namespaces\n\n        module_ident = set()\n        for n in module_code:\n            module_ident = module_ident.union(n.declared_identifiers())\n\n        module_identifiers = _Identifiers(self.compiler)\n        module_identifiers.declared = module_ident\n\n        # module-level names, python code\n        if self.compiler.generate_magic_comment and \\\n                self.compiler.source_encoding:\n            self.printer.writeline(\"# -*- coding:%s -*-\" %\n                                    self.compiler.source_encoding)\n\n        if self.compiler.future_imports:\n            self.printer.writeline(\"from __future__ import %s\" %\n                                   (\", \".join(self.compiler.future_imports),))\n        self.printer.writeline(\"from mako import runtime, filters, cache\")\n        self.printer.writeline(\"UNDEFINED = runtime.UNDEFINED\")\n        self.printer.writeline(\"__M_dict_builtin = dict\")\n        self.printer.writeline(\"__M_locals_builtin = locals\")\n        self.printer.writeline(\"_magic_number = %r\" % MAGIC_NUMBER)\n        self.printer.writeline(\"_modified_time = %r\" % time.time())\n        self.printer.writeline(\"_enable_loop = %r\" % self.compiler.enable_loop)\n        self.printer.writeline(\n                            \"_template_filename = %r\" % self.compiler.filename)\n        self.printer.writeline(\"_template_uri = %r\" % self.compiler.uri)\n        self.printer.writeline(\n                    \"_source_encoding = %r\" % self.compiler.source_encoding)\n        if self.compiler.imports:\n            buf = ''\n            for imp in self.compiler.imports:\n                buf += imp + \"\\n\"\n                self.printer.writeline(imp)\n            impcode = ast.PythonCode(\n                            buf,\n                            source='', lineno=0,\n                            pos=0,\n                            filename='template defined imports')\n        else:\n            impcode = None\n\n        main_identifiers = module_identifiers.branch(self.node)\n        module_identifiers.topleveldefs = \\\n            module_identifiers.topleveldefs.\\\n                union(main_identifiers.topleveldefs)\n        module_identifiers.declared.add(\"UNDEFINED\")\n        if impcode:\n            module_identifiers.declared.update(impcode.declared_identifiers)\n\n        self.compiler.identifiers = module_identifiers\n        self.printer.writeline(\"_exports = %r\" %\n                            [n.name for n in\n                            main_identifiers.topleveldefs.values()]\n                        )\n        self.printer.write_blanks(2)\n\n        if len(module_code):\n            self.write_module_code(module_code)\n\n        if len(inherit):\n            self.write_namespaces(namespaces)\n            self.write_inherit(inherit[-1])\n        elif len(namespaces):\n            self.write_namespaces(namespaces)\n\n        return list(main_identifiers.topleveldefs.values())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write_render_callable(self, node, name, args, buffered, filtered,\n            cached):\n        \"\"\"write a top-level render callable.\n\n        this could be the main render() method or that of a top-level def.\"\"\"\n\n        if self.in_def:\n            decorator = node.decorator\n            if decorator:\n                self.printer.writeline(\n                                \"@runtime._decorate_toplevel(%s)\" % decorator)\n\n        self.printer.start_source(node.lineno)\n        self.printer.writelines(\n            \"def %s(%s):\" % (name, ','.join(args)),\n                # push new frame, assign current frame to __M_caller\n                \"__M_caller = context.caller_stack._push_frame()\",\n                \"try:\"\n        )\n        if buffered or filtered or cached:\n            self.printer.writeline(\"context._push_buffer()\")\n\n        self.identifier_stack.append(\n                                self.compiler.identifiers.branch(self.node))\n        if (not self.in_def or self.node.is_block) and '**pageargs' in args:\n            self.identifier_stack[-1].argument_declared.add('pageargs')\n\n        if not self.in_def and (\n                                len(self.identifiers.locally_assigned) > 0 or\n                                len(self.identifiers.argument_declared) > 0\n                                ):\n            self.printer.writeline(\"__M_locals = __M_dict_builtin(%s)\" %\n                                    ','.join([\n                                            \"%s=%s\" % (x, x) for x in\n                                            self.identifiers.argument_declared\n                                            ]))\n\n        self.write_variable_declares(self.identifiers, toplevel=True)\n\n        for n in self.node.nodes:\n            n.accept_visitor(self)\n\n        self.write_def_finish(self.node, buffered, filtered, cached)\n        self.printer.writeline(None)\n        self.printer.write_blanks(2)\n        if cached:\n            self.write_cache_decorator(\n                                node, name,\n                                args, buffered,\n                                self.identifiers, toplevel=True)", "response": "write a top - level render callable."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nwriting the module - level template code.", "response": "def write_module_code(self, module_code):\n        \"\"\"write module-level template code, i.e. that which\n        is enclosed in <%! %> tags in the template.\"\"\"\n        for n in module_code:\n            self.printer.start_source(n.lineno)\n            self.printer.write_indented_block(n.text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef write_namespaces(self, namespaces):\n        self.printer.writelines(\n            \"def _mako_get_namespace(context, name):\",\n                \"try:\",\n                    \"return context.namespaces[(__name__, name)]\",\n                \"except KeyError:\",\n                    \"_mako_generate_namespaces(context)\",\n                \"return context.namespaces[(__name__, name)]\",\n            None, None\n        )\n        self.printer.writeline(\"def _mako_generate_namespaces(context):\")\n\n\n        for node in namespaces.values():\n            if 'import' in node.attributes:\n                self.compiler.has_ns_imports = True\n            self.printer.start_source(node.lineno)\n            if len(node.nodes):\n                self.printer.writeline(\"def make_namespace():\")\n                export = []\n                identifiers = self.compiler.identifiers.branch(node)\n                self.in_def = True\n                class NSDefVisitor(object):\n                    def visitDefTag(s, node):\n                        s.visitDefOrBase(node)\n\n                    def visitBlockTag(s, node):\n                        s.visitDefOrBase(node)\n\n                    def visitDefOrBase(s, node):\n                        if node.is_anonymous:\n                            raise exceptions.CompileException(\n                                \"Can't put anonymous blocks inside \"\n                                \"<%namespace>\",\n                                **node.exception_kwargs\n                            )\n                        self.write_inline_def(node, identifiers, nested=False)\n                        export.append(node.funcname)\n                vis = NSDefVisitor()\n                for n in node.nodes:\n                    n.accept_visitor(vis)\n                self.printer.writeline(\"return [%s]\" % (','.join(export)))\n                self.printer.writeline(None)\n                self.in_def = False\n                callable_name = \"make_namespace()\"\n            else:\n                callable_name = \"None\"\n\n            if 'file' in node.parsed_attributes:\n                self.printer.writeline(\n                                \"ns = runtime.TemplateNamespace(%r,\"\n                                \" context._clean_inheritance_tokens(),\"\n                                \" templateuri=%s, callables=%s, \"\n                                \" calling_uri=_template_uri)\" %\n                                (\n                                    node.name,\n                                    node.parsed_attributes.get('file', 'None'),\n                                    callable_name,\n                                )\n                            )\n            elif 'module' in node.parsed_attributes:\n                self.printer.writeline(\n                                \"ns = runtime.ModuleNamespace(%r,\"\n                                \" context._clean_inheritance_tokens(),\"\n                                \" callables=%s, calling_uri=_template_uri,\"\n                                \" module=%s)\" %\n                                (\n                                    node.name,\n                                    callable_name,\n                                    node.parsed_attributes.get(\n                                                'module', 'None')\n                                )\n                            )\n            else:\n                self.printer.writeline(\n                                \"ns = runtime.Namespace(%r,\"\n                                \" context._clean_inheritance_tokens(),\"\n                                \" callables=%s, calling_uri=_template_uri)\" %\n                                (\n                                    node.name,\n                                    callable_name,\n                                )\n                            )\n            if eval(node.attributes.get('inheritable', \"False\")):\n                self.printer.writeline(\"context['self'].%s = ns\" % (node.name))\n\n            self.printer.writeline(\n                \"context.namespaces[(__name__, %s)] = ns\" % repr(node.name))\n            self.printer.write_blanks(1)\n        if not len(namespaces):\n            self.printer.writeline(\"pass\")\n        self.printer.writeline(None)", "response": "write the module - level namespace - generating callable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_variable_declares(self, identifiers, toplevel=False, limit=None):\n\n        # collection of all defs available to us in this scope\n        comp_idents = dict([(c.funcname, c) for c in identifiers.defs])\n        to_write = set()\n\n        # write \"context.get()\" for all variables we are going to\n        # need that arent in the namespace yet\n        to_write = to_write.union(identifiers.undeclared)\n\n        # write closure functions for closures that we define\n        # right here\n        to_write = to_write.union(\n                        [c.funcname for c in identifiers.closuredefs.values()])\n\n        # remove identifiers that are declared in the argument\n        # signature of the callable\n        to_write = to_write.difference(identifiers.argument_declared)\n\n        # remove identifiers that we are going to assign to.\n        # in this way we mimic Python's behavior,\n        # i.e. assignment to a variable within a block\n        # means that variable is now a \"locally declared\" var,\n        # which cannot be referenced beforehand.\n        to_write = to_write.difference(identifiers.locally_declared)\n\n        if self.compiler.enable_loop:\n            has_loop = \"loop\" in to_write\n            to_write.discard(\"loop\")\n        else:\n            has_loop = False\n\n        # if a limiting set was sent, constraint to those items in that list\n        # (this is used for the caching decorator)\n        if limit is not None:\n            to_write = to_write.intersection(limit)\n\n        if toplevel and getattr(self.compiler, 'has_ns_imports', False):\n            self.printer.writeline(\"_import_ns = {}\")\n            self.compiler.has_imports = True\n            for ident, ns in self.compiler.namespaces.items():\n                if 'import' in ns.attributes:\n                    self.printer.writeline(\n                            \"_mako_get_namespace(context, %r).\"\n                                    \"_populate(_import_ns, %r)\" %\n                            (\n                                ident,\n                                re.split(r'\\s*,\\s*', ns.attributes['import'])\n                            ))\n\n        if has_loop:\n            self.printer.writeline(\n                'loop = __M_loop = runtime.LoopStack()'\n            )\n\n        for ident in to_write:\n            if ident in comp_idents:\n                comp = comp_idents[ident]\n                if comp.is_block:\n                    if not comp.is_anonymous:\n                        self.write_def_decl(comp, identifiers)\n                    else:\n                        self.write_inline_def(comp, identifiers, nested=True)\n                else:\n                    if comp.is_root():\n                        self.write_def_decl(comp, identifiers)\n                    else:\n                        self.write_inline_def(comp, identifiers, nested=True)\n\n            elif ident in self.compiler.namespaces:\n                self.printer.writeline(\n                            \"%s = _mako_get_namespace(context, %r)\" %\n                                (ident, ident)\n                            )\n            else:\n                if getattr(self.compiler, 'has_ns_imports', False):\n                    if self.compiler.strict_undefined:\n                        self.printer.writelines(\n                        \"%s = _import_ns.get(%r, UNDEFINED)\" %\n                        (ident, ident),\n                        \"if %s is UNDEFINED:\" % ident,\n                            \"try:\",\n                                \"%s = context[%r]\" % (ident, ident),\n                            \"except KeyError:\",\n                                \"raise NameError(\\\"'%s' is not defined\\\")\" %\n                                    ident,\n                            None, None\n                        )\n                    else:\n                        self.printer.writeline(\n                        \"%s = _import_ns.get(%r, context.get(%r, UNDEFINED))\" %\n                        (ident, ident, ident))\n                else:\n                    if self.compiler.strict_undefined:\n                        self.printer.writelines(\n                            \"try:\",\n                                \"%s = context[%r]\" % (ident, ident),\n                            \"except KeyError:\",\n                                \"raise NameError(\\\"'%s' is not defined\\\")\" %\n                                    ident,\n                            None\n                        )\n                    else:\n                        self.printer.writeline(\n                            \"%s = context.get(%r, UNDEFINED)\" % (ident, ident)\n                        )\n\n        self.printer.writeline(\"__M_writer = context.writer()\")", "response": "Writes the variable declarations at the top of a function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite a locally - available callable referencing a top - level def", "response": "def write_def_decl(self, node, identifiers):\n        \"\"\"write a locally-available callable referencing a top-level def\"\"\"\n        funcname = node.funcname\n        namedecls = node.get_argument_expressions()\n        nameargs = node.get_argument_expressions(as_call=True)\n\n        if not self.in_def and (\n                                len(self.identifiers.locally_assigned) > 0 or\n                                len(self.identifiers.argument_declared) > 0):\n            nameargs.insert(0, 'context._locals(__M_locals)')\n        else:\n            nameargs.insert(0, 'context')\n        self.printer.writeline(\"def %s(%s):\" % (funcname, \",\".join(namedecls)))\n        self.printer.writeline(\n                    \"return render_%s(%s)\" % (funcname, \",\".join(nameargs)))\n        self.printer.writeline(None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef write_inline_def(self, node, identifiers, nested):\n\n        namedecls = node.get_argument_expressions()\n\n        decorator = node.decorator\n        if decorator:\n            self.printer.writeline(\n                        \"@runtime._decorate_inline(context, %s)\" % decorator)\n        self.printer.writeline(\n                        \"def %s(%s):\" % (node.funcname, \",\".join(namedecls)))\n        filtered = len(node.filter_args.args) > 0\n        buffered = eval(node.attributes.get('buffered', 'False'))\n        cached = eval(node.attributes.get('cached', 'False'))\n        self.printer.writelines(\n            # push new frame, assign current frame to __M_caller\n            \"__M_caller = context.caller_stack._push_frame()\",\n            \"try:\"\n        )\n        if buffered or filtered or cached:\n            self.printer.writelines(\n                \"context._push_buffer()\",\n            )\n\n        identifiers = identifiers.branch(node, nested=nested)\n\n        self.write_variable_declares(identifiers)\n\n        self.identifier_stack.append(identifiers)\n        for n in node.nodes:\n            n.accept_visitor(self)\n        self.identifier_stack.pop()\n\n        self.write_def_finish(node, buffered, filtered, cached)\n        self.printer.writeline(None)\n        if cached:\n            self.write_cache_decorator(node, node.funcname,\n                                        namedecls, False, identifiers,\n                                        inline=True, toplevel=False)", "response": "write a locally - available def callable inside an enclosing def."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write_def_finish(self, node, buffered, filtered, cached,\n            callstack=True):\n        \"\"\"write the end section of a rendering function, either outermost or\n        inline.\n\n        this takes into account if the rendering function was filtered,\n        buffered, etc.  and closes the corresponding try: block if any, and\n        writes code to retrieve captured content, apply filters, send proper\n        return value.\"\"\"\n\n        if not buffered and not cached and not filtered:\n            self.printer.writeline(\"return ''\")\n            if callstack:\n                self.printer.writelines(\n                    \"finally:\",\n                        \"context.caller_stack._pop_frame()\",\n                    None\n                )\n\n        if buffered or filtered or cached:\n            if buffered or cached:\n                # in a caching scenario, don't try to get a writer\n                # from the context after popping; assume the caching\n                # implemenation might be using a context with no\n                # extra buffers\n                self.printer.writelines(\n                    \"finally:\",\n                        \"__M_buf = context._pop_buffer()\"\n                )\n            else:\n                self.printer.writelines(\n                    \"finally:\",\n                    \"__M_buf, __M_writer = context._pop_buffer_and_writer()\"\n                )\n\n            if callstack:\n                self.printer.writeline(\"context.caller_stack._pop_frame()\")\n\n            s = \"__M_buf.getvalue()\"\n            if filtered:\n                s = self.create_filter_callable(node.filter_args.args, s,\n                                                False)\n            self.printer.writeline(None)\n            if buffered and not cached:\n                s = self.create_filter_callable(self.compiler.buffer_filters,\n                                                s, False)\n            if buffered or cached:\n                self.printer.writeline(\"return %s\" % s)\n            else:\n                self.printer.writelines(\n                    \"__M_writer(%s)\" % s,\n                    \"return ''\"\n                )", "response": "write the end section of a rendering function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwrites a post - function decorator to replace a rendering callable with a cached version of itself.", "response": "def write_cache_decorator(self, node_or_pagetag, name,\n                                    args, buffered, identifiers,\n                                    inline=False, toplevel=False):\n        \"\"\"write a post-function decorator to replace a rendering\n            callable with a cached version of itself.\"\"\"\n\n        self.printer.writeline(\"__M_%s = %s\" % (name, name))\n        cachekey = node_or_pagetag.parsed_attributes.get('cache_key',\n                                                         repr(name))\n\n        cache_args = {}\n        if self.compiler.pagetag is not None:\n            cache_args.update(\n                (\n                    pa[6:],\n                    self.compiler.pagetag.parsed_attributes[pa]\n                )\n                for pa in self.compiler.pagetag.parsed_attributes\n                if pa.startswith('cache_') and pa != 'cache_key'\n            )\n        cache_args.update(\n            (\n                pa[6:],\n                node_or_pagetag.parsed_attributes[pa]\n            ) for pa in node_or_pagetag.parsed_attributes\n            if pa.startswith('cache_') and pa != 'cache_key'\n        )\n        if 'timeout' in cache_args:\n            cache_args['timeout'] = int(eval(cache_args['timeout']))\n\n        self.printer.writeline(\"def %s(%s):\" % (name, ','.join(args)))\n\n        # form \"arg1, arg2, arg3=arg3, arg4=arg4\", etc.\n        pass_args = [\n                        \"%s=%s\" % ((a.split('=')[0],) * 2) if '=' in a else a\n                        for a in args\n                    ]\n\n        self.write_variable_declares(\n                            identifiers,\n                            toplevel=toplevel,\n                            limit=node_or_pagetag.undeclared_identifiers()\n                        )\n        if buffered:\n            s = \"context.get('local').\"\\\n                \"cache._ctx_get_or_create(\"\\\n                \"%s, lambda:__M_%s(%s),  context, %s__M_defname=%r)\" % (\n                                cachekey, name, ','.join(pass_args),\n                                ''.join([\"%s=%s, \" % (k, v)\n                                for k, v in cache_args.items()]),\n                                name\n                            )\n            # apply buffer_filters\n            s = self.create_filter_callable(self.compiler.buffer_filters, s,\n                                            False)\n            self.printer.writelines(\"return \" + s, None)\n        else:\n            self.printer.writelines(\n                    \"__M_writer(context.get('local').\"\n                    \"cache._ctx_get_or_create(\"\n                    \"%s, lambda:__M_%s(%s), context, %s__M_defname=%r))\" %\n                    (\n                        cachekey, name, ','.join(pass_args),\n                        ''.join([\"%s=%s, \" % (k, v)\n                        for k, v in cache_args.items()]),\n                        name,\n                    ),\n                    \"return ''\",\n                None\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_filter_callable(self, args, target, is_expression):\n\n        def locate_encode(name):\n            if re.match(r'decode\\..+', name):\n                return \"filters.\" + name\n            elif self.compiler.disable_unicode:\n                return filters.NON_UNICODE_ESCAPES.get(name, name)\n            else:\n                return filters.DEFAULT_ESCAPES.get(name, name)\n\n        if 'n' not in args:\n            if is_expression:\n                if self.compiler.pagetag:\n                    args = self.compiler.pagetag.filter_args.args + args\n                if self.compiler.default_filters:\n                    args = self.compiler.default_filters + args\n        for e in args:\n            # if filter given as a function, get just the identifier portion\n            if e == 'n':\n                continue\n            m = re.match(r'(.+?)(\\(.*\\))', e)\n            if m:\n                ident, fargs = m.group(1, 2)\n                f = locate_encode(ident)\n                e = f + fargs\n            else:\n                e = locate_encode(e)\n                assert e is not None\n            target = \"%s(%s)\" % (e, target)\n        return target", "response": "write a filter - applying expression based on the filters\n            present in the given filter names adjusting for the global\n        default filter aliases as needed."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new Identifiers for a new Node with this Identifiers as the parent", "response": "def branch(self, node, **kwargs):\n        \"\"\"create a new Identifiers for a new Node, with\n          this Identifiers as the parent.\"\"\"\n\n        return _Identifiers(self.compiler, node, self, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_declared(self, node):\n\n        for ident in node.undeclared_identifiers():\n            if ident != 'context' and\\\n                    ident not in self.declared.union(self.locally_declared):\n                self.undeclared.add(ident)\n        for ident in node.declared_identifiers():\n            self.locally_declared.add(ident)", "response": "update the state of this Identifiers with the undeclared and declared identifiers of the given node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_hosts_contents(self, hosts_contents):\n        sections = []\n        cur_section = {\n            'type': 'hosts',\n            'name': None,\n            'entries': []\n        }\n\n        for line in hosts_contents:\n            line = line.strip()\n            if line.startswith('#') or not line:\n                continue\n            elif line.startswith('['):\n                sections.append(cur_section)\n                section_type, name = self._parse_line_section(line)\n                cur_section = {\n                    'type': section_type,\n                    'name': name,\n                    'entries': []\n                }\n            else:\n                name, vars = self._parse_line_entry(line, cur_section['type'])\n                entry = {\n                    'name': name,\n                    'hostvars': vars\n                }\n                cur_section['entries'].append(entry)\n        sections.append(cur_section)\n        return sections", "response": "Parse the inventory contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a line containing a group definition. Returns a tuple of group_type group_name where group_type is in the set of hosts children vars.", "response": "def _parse_line_section(self, line):\n        \"\"\"\n        Parse a line containing a group definition. Returns a tuple:\n        (group_type, group_name), where group_type is in the set ('hosts',\n        'children', 'vars').\n\n        For example:\n            [prod]\n        Returns:\n            ('hosts', 'prod')\n\n        For example:\n            [prod:children]\n        Returns:\n            ('children', 'prod')\n        \"\"\"\n        m = re.match(\"\\[(.*)\\]\", line)\n        group_def = m.groups()[0]\n        if ':' in group_def:\n            group_name, group_type = group_def.split(':')\n        else:\n            group_name = group_def\n            group_type = 'hosts'\n\n        return (group_type, group_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_line_entry(self, line, type):\n\n        name = None\n        key_values = {}\n\n        if type == 'vars':\n            key_values = self._parse_line_vars(line)\n        else:\n            tokens = shlex.split(line.strip())\n            name = tokens.pop(0)\n            try:\n                key_values = self._parse_vars(tokens)\n            except ValueError:\n                self.log.warning(\"Unsupported vars syntax. Skipping line: {0}\".format(line))\n                return (name, {})\n        return (name, key_values)", "response": "Parse a line of a section entry into its components."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_line_vars(self, line):\n        key_values = {}\n\n        # Undocumented feature allows json in vars sections like so:\n        #   [prod:vars]\n        #   json_like_vars=[{'name': 'htpasswd_auth'}]\n        # We'll try this first. If it fails, we'll fall back to normal var\n        # lines. Since it's undocumented, we just assume some things.\n        k, v = line.strip().split('=', 1)\n        if v.startswith('['):\n            try:\n                list_res = ihateyaml.safe_load(v)\n                if isinstance(list_res[0], dict):\n                    key_values = list_res[0]\n                    return key_values\n            except ValueError:\n                pass\n\n        # Guess it's not YAML. Parse as normal host variables\n        tokens = shlex.split(line.strip())\n        key_values = self._parse_vars(tokens)\n        return key_values", "response": "Parse a line in a [ XXXXX : vars section."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_vars(self, tokens):\n        key_values = {}\n        for token in tokens:\n            if token.startswith('#'):\n                # End parsing if we encounter a comment, which lasts\n                # until the end of the line.\n                break\n            else:\n                k, v = token.split('=', 1)\n                key = k.strip()\n                key_values[key] = v.strip()\n        return key_values", "response": "Given an iterable of tokens returns variables and their values as a\n        dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_distinct_hostnames(self):\n        hostnames = []\n        for section in self.sections:\n            hostnames.extend(self._group_get_hostnames(section['name']))\n        return set(hostnames)", "response": "Return a set of distinct hostnames found in the entire inventory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\napplying the variables for each entry in a section to the hosts dictionary.", "response": "def _apply_section_hosts(self, section, hosts):\n        \"\"\"\n        Add the variables for each entry in a 'hosts' section to the hosts\n        belonging to that entry.\n        \"\"\"\n        for entry in section['entries']:\n            for hostname in self.expand_hostdef(entry['name']):\n                if hostname not in hosts:\n                    # Expanded host or child host or something else refers to a\n                    # host that isn't actually defined. Ansible skips this, so\n                    # we will too.\n                    continue\n                host = hosts[hostname]\n                for var_key, var_val in entry['hostvars'].items():\n                    host['hostvars'][var_key] = var_val"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply the variables for each entry in a children section to the hosts dictionary.", "response": "def _apply_section_children(self, section, hosts):\n        \"\"\"\n        Add the variables for each entry in a 'children' section to the hosts\n        belonging to that entry.\n        \"\"\"\n        for entry in section['entries']:\n            for hostname in self._group_get_hostnames(entry['name']):\n                host = hosts[hostname]\n                for var_key, var_val in entry['hostvars'].items():\n                    host['hostvars'][var_key] = var_val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_section(self, name, type):\n        for section in self.sections:\n            if section['name'] == name and section['type'] == type:\n                return section\n        return None", "response": "Find and return a section with name and type"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexpands a host definition into seperate hosts.", "response": "def expand_hostdef(self, hostdef):\n        \"\"\"\n        Expand a host definition (e.g. \"foo[001:010].bar.com\") into seperate\n        hostnames. Supports zero-padding, numbered ranges and alphabetical\n        ranges. Multiple patterns in a host defnition are also supported.\n        Returns a list of the fully expanded hostnames. Ports are also removed\n        from hostnames as a bonus (e.g. \"foo.bar.com:8022\" -> \"foo.bar.com\")\n        \"\"\"\n        try:\n            hosts_todo = [hostdef]\n            hosts_done = []\n\n            # Keep going through the todo list of hosts until they no longer have a\n            # pattern in them. We only handle the first pattern found in the host for\n            # each iteration of the while loop. If more patterns are present, the\n            # partially expanded host(s) gets added back to the todo list.\n            while hosts_todo:\n                host = hosts_todo.pop(0)\n                if '[' not in host:\n                    hosts_done.append(host)\n                    continue\n\n                # Extract the head, first pattern and tail. E.g. foo[0:3].bar.com ->\n                # head=\"foo\", pattern=\"0:3\", tail=\".bar.com\"\n                head, rest = host.split('[', 1)\n                pattern, tail = rest.split(']', 1)\n                start, end = pattern.split(':')\n                fill = False\n                if start.startswith('0') and len(start) > 0:\n                    fill = len(start)\n\n                try:\n                    for i in range(int(start), int(end) + 1):\n                        if fill:\n                            range_nr = str(i).zfill(fill)\n                        else:\n                            range_nr = i\n                        new_host = '{0}{1}{2}'.format(head, range_nr, tail)\n                        if '[' in new_host:\n                            hosts_todo.append(new_host)\n                        else:\n                            hosts_done.append(new_host)\n                except ValueError:\n                    for i in range(ord(start), ord(end) + 1):\n                        new_host = '{0}{1}{2}'.format(head, chr(i), tail)\n                        if '[' in new_host:\n                            hosts_todo.append(new_host)\n                        else:\n                            hosts_done.append(new_host)\n\n            # Strip port numbers off and return\n            return [host_name.split(':')[0] for host_name in hosts_done]\n        except Exception as e:\n            self.log.warning(\"Couldn't parse host definition '{0}': {1}\".format(hostdef, e))\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_host(self, hostname):\n        if hostname not in self.hosts:\n            self.hosts[hostname] = {\n                'groups': set(),\n                'hostvars': {}\n            }\n        return self.hosts[hostname]", "response": "Get an existing host or initialize a new one."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a group definition from a dynamic inventory output.", "response": "def _parse_group(self, group_name, group):\n        \"\"\"\n        Parse a group definition from a dynamic inventory. These are top-level\n        elements which are not '_meta(data)'.\n        \"\"\"\n        if type(group) == dict:\n            # Example:\n            #     {\n            #       \"mgmt\": {\n            #         \"hosts\": [ \"mgmt01\", \"mgmt02\" ],\n            #         \"vars\": {\n            #           \"eth0\": {\n            #             \"onboot\": \"yes\",\n            #             \"nm_controlled\": \"no\"\n            #           }\n            #         }\n            #       }\n            #     }\n            #\n            hostnames_in_group = set()\n\n            # Group member with hosts and variable definitions.\n            for hostname in group.get('hosts', []):\n                self._get_host(hostname)['groups'].add(group_name)\n                hostnames_in_group.add(hostname)\n            # Apply variables to all hosts in group\n            for var_key, var_val in group.get('vars', {}).items():\n                for hostname in hostnames_in_group:\n                    self._get_host(hostname)['hostvars'][var_key] = var_val\n        elif type(group) == list:\n            # List of hostnames for this group\n            for hostname in group:\n                self._get_host(hostname)['groups'].add(group_name)\n        else:\n            self.log.warning(\"Invalid element found in dynamic inventory output: {0}\".format(type(group)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the _meta element from a dynamic host inventory output.", "response": "def _parse_meta(self, meta):\n        \"\"\"\n        Parse the _meta element from a dynamic host inventory output.\n        \"\"\"\n        for hostname, hostvars in meta.get('hostvars', {}).items():\n            for var_key, var_val in hostvars.items():\n                self._get_host(hostname)['hostvars'][var_key] = var_val"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates and or verify a filesystem directory.", "response": "def verify_directory(dir):\n    \"\"\"create and/or verify a filesystem directory.\"\"\"\n\n    tries = 0\n\n    while not os.path.exists(dir):\n        try:\n            tries += 1\n            os.makedirs(dir, compat.octal(\"0775\"))\n        except:\n            if tries > 5:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse the encoding of a Python source file.", "response": "def parse_encoding(fp):\n    \"\"\"Deduce the encoding of a Python source file (binary mode) from magic\n    comment.\n\n    It does this in the same way as the `Python interpreter`__\n\n    .. __: http://docs.python.org/ref/encodings.html\n\n    The ``fp`` argument should be a seekable file object in binary mode.\n    \"\"\"\n    pos = fp.tell()\n    fp.seek(0)\n    try:\n        line1 = fp.readline()\n        has_bom = line1.startswith(codecs.BOM_UTF8)\n        if has_bom:\n            line1 = line1[len(codecs.BOM_UTF8):]\n\n        m = _PYTHON_MAGIC_COMMENT_re.match(line1.decode('ascii', 'ignore'))\n        if not m:\n            try:\n                import parser\n                parser.suite(line1.decode('ascii', 'ignore'))\n            except (ImportError, SyntaxError):\n                # Either it's a real syntax error, in which case the source\n                # is not valid python source, or line2 is a continuation of\n                # line1, in which case we don't want to scan line2 for a magic\n                # comment.\n                pass\n            else:\n                line2 = fp.readline()\n                m = _PYTHON_MAGIC_COMMENT_re.match(\n                                               line2.decode('ascii', 'ignore'))\n\n        if has_bom:\n            if m:\n                raise SyntaxError(\"python refuses to compile code with both a UTF8\" \\\n                      \" byte-order-mark and a magic encoding comment\")\n            return 'utf_8'\n        elif m:\n            return m.group(1)\n        else:\n            return None\n    finally:\n        fp.seek(pos)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef restore__ast(_ast):\n    if hasattr(_ast, 'AST'):\n        return\n    _ast.PyCF_ONLY_AST = 2 << 9\n    m = compile(\"\"\"\\\ndef foo(): pass\nclass Bar(object): pass\nif False: pass\nbaz = 'mako'\n1 + 2 - 3 * 4 / 5\n6 // 7 % 8 << 9 >> 10\n11 & 12 ^ 13 | 14\n15 and 16 or 17\n-baz + (not +18) - ~17\nbaz and 'foo' or 'bar'\n(mako is baz == baz) is not baz != mako\nmako > baz < mako >= baz <= mako\nmako in baz not in mako\"\"\", '<unknown>', 'exec', _ast.PyCF_ONLY_AST)\n    _ast.Module = type(m)\n\n    for cls in _ast.Module.__mro__:\n        if cls.__name__ == 'mod':\n            _ast.mod = cls\n        elif cls.__name__ == 'AST':\n            _ast.AST = cls\n\n    _ast.FunctionDef = type(m.body[0])\n    _ast.ClassDef = type(m.body[1])\n    _ast.If = type(m.body[2])\n\n    _ast.Name = type(m.body[3].targets[0])\n    _ast.Store = type(m.body[3].targets[0].ctx)\n    _ast.Str = type(m.body[3].value)\n\n    _ast.Sub = type(m.body[4].value.op)\n    _ast.Add = type(m.body[4].value.left.op)\n    _ast.Div = type(m.body[4].value.right.op)\n    _ast.Mult = type(m.body[4].value.right.left.op)\n\n    _ast.RShift = type(m.body[5].value.op)\n    _ast.LShift = type(m.body[5].value.left.op)\n    _ast.Mod = type(m.body[5].value.left.left.op)\n    _ast.FloorDiv = type(m.body[5].value.left.left.left.op)\n\n    _ast.BitOr = type(m.body[6].value.op)\n    _ast.BitXor = type(m.body[6].value.left.op)\n    _ast.BitAnd = type(m.body[6].value.left.left.op)\n\n    _ast.Or = type(m.body[7].value.op)\n    _ast.And = type(m.body[7].value.values[0].op)\n\n    _ast.Invert = type(m.body[8].value.right.op)\n    _ast.Not = type(m.body[8].value.left.right.op)\n    _ast.UAdd = type(m.body[8].value.left.right.operand.op)\n    _ast.USub = type(m.body[8].value.left.left.op)\n\n    _ast.Or = type(m.body[9].value.op)\n    _ast.And = type(m.body[9].value.values[0].op)\n\n    _ast.IsNot = type(m.body[10].value.ops[0])\n    _ast.NotEq = type(m.body[10].value.ops[1])\n    _ast.Is = type(m.body[10].value.left.ops[0])\n    _ast.Eq = type(m.body[10].value.left.ops[1])\n\n    _ast.Gt = type(m.body[11].value.ops[0])\n    _ast.Lt = type(m.body[11].value.ops[1])\n    _ast.GtE = type(m.body[11].value.ops[2])\n    _ast.LtE = type(m.body[11].value.ops[3])\n\n    _ast.In = type(m.body[12].value.ops[0])\n    _ast.NotIn = type(m.body[12].value.ops[1])", "response": "Attempt to restore the required classes to the _ast module if it appears to be missing them\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef union(self, other):\n        x = SetLikeDict(**self)\n        x.update(other)\n        return x", "response": "produce a union of this dict and another dict"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_source(node, indent_with=' ' * 4):\n    generator = SourceGenerator(indent_with)\n    generator.visit(node)\n    return ''.join(generator.result)", "response": "Convert a node tree into a python sourcecode string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncopies the source location hint from the old node to the new node if possible and return the new one.", "response": "def copy_location(new_node, old_node):\n    \"\"\"\n    Copy the source location hint (`lineno` and `col_offset`) from the\n    old to the new node if possible and return the new one.\n    \"\"\"\n    for attr in 'lineno', 'col_offset':\n        if attr in old_node._attributes and attr in new_node._attributes \\\n           and hasattr(old_node, attr):\n            setattr(new_node, attr, getattr(old_node, attr))\n    return new_node"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fix_missing_locations(node):\n    def _fix(node, lineno, col_offset):\n        if 'lineno' in node._attributes:\n            if not hasattr(node, 'lineno'):\n                node.lineno = lineno\n            else:\n                lineno = node.lineno\n        if 'col_offset' in node._attributes:\n            if not hasattr(node, 'col_offset'):\n                node.col_offset = col_offset\n            else:\n                col_offset = node.col_offset\n        for child in iter_child_nodes(node):\n            _fix(child, lineno, col_offset)\n    _fix(node, 1, 0)\n    return node", "response": "This function fixes missing locations in the node."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef increment_lineno(node, n=1):\n    for node in zip((node,), walk(node)):\n        if 'lineno' in node._attributes:\n            node.lineno = getattr(node, 'lineno', 0) + n", "response": "Increment the line number of all nodes by n."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\niterate over all fields of a node only yielding existing fields.", "response": "def iter_fields(node):\n    \"\"\"Iterate over all fields of a node, only yielding existing fields.\"\"\"\n    # CPython 2.5 compat\n    if not hasattr(node, '_fields') or not node._fields:\n        return\n    for field in node._fields:\n        try:\n            yield field, getattr(node, field)\n        except AttributeError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\niterates over all child nodes or a node.", "response": "def iter_child_nodes(node):\n    \"\"\"Iterate over all child nodes or a node.\"\"\"\n    for name, field in iter_fields(node):\n        if isinstance(field, AST):\n            yield field\n        elif isinstance(field, list):\n            for item in field:\n                if isinstance(item, AST):\n                    yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_compile_mode(node):\n    if not isinstance(node, mod):\n        raise TypeError('expected mod node, got %r' % node.__class__.__name__)\n    return {\n        Expression: 'eval',\n        Interactive: 'single'\n    }.get(node.__class__, 'expr')", "response": "Get the compile mode for a given node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the docstring for the given node or None if no docstring can be found.", "response": "def get_docstring(node):\n    \"\"\"\n    Return the docstring for the given node or `None` if no docstring can be\n    found.  If the node provided does not accept docstrings a `TypeError`\n    will be raised.\n    \"\"\"\n    if not isinstance(node, (FunctionDef, ClassDef, Module)):\n        raise TypeError(\"%r can't have docstrings\" % node.__class__.__name__)\n    if node.body and isinstance(node.body[0], Str):\n        return node.body[0].s"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef walk(node):\n    from collections import deque\n    todo = deque([node])\n    while todo:\n        node = todo.popleft()\n        todo.extend(iter_child_nodes(node))\n        yield node", "response": "Iterate over all nodes in a node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef match(self, regexp, flags=None):\n\n        try:\n            reg = _regexp_cache[(regexp, flags)]\n        except KeyError:\n            if flags:\n                reg = re.compile(regexp, flags)\n            else:\n                reg = re.compile(regexp)\n            _regexp_cache[(regexp, flags)] = reg\n\n        return self.match_reg(reg)", "response": "compile the given regexp cache the reg and call match_reg()."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match_reg(self, reg):\n\n        mp = self.match_position\n\n        match = reg.match(self.text, self.match_position)\n        if match:\n            (start, end) = match.span()\n            if end == start:\n                self.match_position = end + 1\n            else:\n                self.match_position = end\n            self.matched_lineno = self.lineno\n            lines = re.findall(r\"\\n\", self.text[mp:self.match_position])\n            cp = mp - 1\n            while (cp >= 0 and cp < self.textlength and self.text[cp] != '\\n'):\n                cp -= 1\n            self.matched_charpos = mp - cp\n            self.lineno += len(lines)\n            #print \"MATCHED:\", match.group(0), \"LINE START:\",\n            # self.matched_lineno, \"LINE END:\", self.lineno\n        #print \"MATCH:\", regexp, \"\\n\", self.text[mp : mp + 15], \\\n        #          (match and \"TRUE\" or \"FALSE\")\n        return match", "response": "match the given regular expression object to the current text and line position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive string or bytes or string determine encoding from magic encoding comment return body as unicode or raw if decode_raw = False return body as raw if decode_raw = True return body as unicode", "response": "def decode_raw_stream(self, text, decode_raw, known_encoding, filename):\n        \"\"\"given string/unicode or bytes/string, determine encoding\n           from magic encoding comment, return body as unicode\n           or raw if decode_raw=False\n\n        \"\"\"\n        if isinstance(text, compat.text_type):\n            m = self._coding_re.match(text)\n            encoding = m and m.group(1) or known_encoding or 'ascii'\n            return encoding, text\n\n        if text.startswith(codecs.BOM_UTF8):\n            text = text[len(codecs.BOM_UTF8):]\n            parsed_encoding = 'utf-8'\n            m = self._coding_re.match(text.decode('utf-8', 'ignore'))\n            if m is not None and m.group(1) != 'utf-8':\n                raise exceptions.CompileException(\n                                \"Found utf-8 BOM in file, with conflicting \"\n                                \"magic encoding comment of '%s'\" % m.group(1),\n                                text.decode('utf-8', 'ignore'),\n                                0, 0, filename)\n        else:\n            m = self._coding_re.match(text.decode('utf-8', 'ignore'))\n            if m:\n                parsed_encoding = m.group(1)\n            else:\n                parsed_encoding = known_encoding or 'ascii'\n\n        if decode_raw:\n            try:\n                text = text.decode(parsed_encoding)\n            except UnicodeDecodeError:\n                raise exceptions.CompileException(\n                        \"Unicode decode operation of encoding '%s' failed\" %\n                        parsed_encoding,\n                        text.decode('utf-8', 'ignore'),\n                        0, 0, filename)\n\n        return parsed_encoding, text"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef match_comment(self):\n        match = self.match(r\"<%doc>(.*?)</%doc>\", re.S)\n        if match:\n            self.append_node(parsetree.Comment, match.group(1))\n            return True\n        else:\n            return False", "response": "matches the multiline version of a comment"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract messages from a Mako file - like object.", "response": "def extract(fileobj, keywords, comment_tags, options):\n    \"\"\"Extract messages from Mako templates.\n\n    :param fileobj: the file-like object the messages should be extracted from\n    :param keywords: a list of keywords (i.e. function names) that should be\n                     recognized as translation functions\n    :param comment_tags: a list of translator tags to search for and include\n                         in the results\n    :param options: a dictionary of additional options (optional)\n    :return: an iterator over ``(lineno, funcname, message, comments)`` tuples\n    :rtype: ``iterator``\n    \"\"\"\n    extractor = BabelMakoExtractor(keywords, comment_tags, options)\n    for message in extractor(fileobj):\n        yield message"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_ternary(self, keyword):\n\n        return keyword in {\n            'if':set(['else', 'elif']),\n            'try':set(['except', 'finally']),\n            'for':set(['else'])\n        }.get(self.keyword, [])", "response": "return true if the given keyword is a ternary keyword for this ControlLine"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_or_create(self, key, creation_function, **kw):\n\n        return self._ctx_get_or_create(key, creation_function, None, **kw)", "response": "Retrieve a value from the cache using the given creation function to generate a new value."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ctx_get_or_create(self, key, creation_function, context, **kw):\n\n        if not self.template.cache_enabled:\n            return creation_function()\n\n        return self.impl.get_or_create(\n            key,\n            creation_function,\n            **self._get_cache_kw(kw, context))", "response": "Retrieve a value from the cache using the given creation function\n            to generate a new value."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nplaces a value in the cache.", "response": "def set(self, key, value, **kw):\n        \"\"\"Place a value in the cache.\n\n        :param key: the value's key.\n        :param value: the value.\n        :param \\**kw: cache configuration arguments.\n\n        \"\"\"\n\n        self.impl.set(key, value, **self._get_cache_kw(kw, None))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves a value from the cache.", "response": "def get(self, key, **kw):\n        \"\"\"Retrieve a value from the cache.\n\n        :param key: the value's key.\n        :param \\**kw: cache configuration arguments.  The\n         backend is configured using these arguments upon first request.\n         Subsequent requests that use the same series of configuration\n         values will use that same backend.\n\n        \"\"\"\n        return self.impl.get(key, **self._get_cache_kw(kw, None))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef invalidate(self, key, **kw):\n        self.impl.invalidate(key, **self._get_cache_kw(kw, None))", "response": "Invalidate a value in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef supports_caller(func):\n\n    def wrap_stackframe(context, *args, **kwargs):\n        context.caller_stack._push_frame()\n        try:\n            return func(context, *args, **kwargs)\n        finally:\n            context.caller_stack._pop_frame()\n    return wrap_stackframe", "response": "Decorator to apply a caller_stack compatibility decorator to a plain\n    Python function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nexecute the given callable and return the output of the buffer.", "response": "def capture(context, callable_, *args, **kwargs):\n    \"\"\"Execute the given template def, capturing the output into\n    a buffer.\n\n    See the example in :ref:`namespaces_python_modules`.\n\n    \"\"\"\n\n    if not compat.callable(callable_):\n        raise exceptions.RuntimeException(\n                        \"capture() function expects a callable as \"\n                        \"its argument (i.e. capture(func, *args, **kwargs))\"\n                        )\n    context._push_buffer()\n    try:\n        callable_(*args, **kwargs)\n    finally:\n        buf = context._pop_buffer()\n    return buf.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _include_file(context, uri, calling_uri, **kwargs):\n\n    template = _lookup_template(context, uri, calling_uri)\n    (callable_, ctx) = _populate_self_namespace(\n                                context._clean_inheritance_tokens(),\n                                template)\n    callable_(ctx, **_kwargs_for_include(callable_, context._data, **kwargs))", "response": "locate the template from the given uri and include it in\n    the current output."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling by the _inherit method in template modules to set up the inheritance chain at the start of a template s execution.", "response": "def _inherit_from(context, uri, calling_uri):\n    \"\"\"called by the _inherit method in template modules to set\n    up the inheritance chain at the start of a template's\n    execution.\"\"\"\n\n    if uri is None:\n        return None\n    template = _lookup_template(context, uri, calling_uri)\n    self_ns = context['self']\n    ih = self_ns\n    while ih.inherits is not None:\n        ih = ih.inherits\n    lclcontext = context._locals({'next': ih})\n    ih.inherits = TemplateNamespace(\"self:%s\" % template.uri,\n                                lclcontext,\n                                template=template,\n                                populate_self=False)\n    context._data['parent'] = lclcontext._data['local'] = ih.inherits\n    callable_ = getattr(template.module, '_mako_inherit', None)\n    if callable_ is not None:\n        ret = callable_(template, lclcontext)\n        if ret:\n            return ret\n\n    gen_ns = getattr(template.module, '_mako_generate_namespaces', None)\n    if gen_ns is not None:\n        gen_ns(context)\n    return (template.callable_, lclcontext)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a Context and return the string output of the given template and template callable.", "response": "def _render(template, callable_, args, data, as_unicode=False):\n    \"\"\"create a Context and return the string\n    output of the given template and template callable.\"\"\"\n\n    if as_unicode:\n        buf = util.FastEncodingBuffer(as_unicode=True)\n    elif template.bytestring_passthrough:\n        buf = compat.StringIO()\n    else:\n        buf = util.FastEncodingBuffer(\n                        as_unicode=as_unicode,\n                        encoding=template.output_encoding,\n                        errors=template.encoding_errors)\n    context = Context(buf, **data)\n    context._outputting_as_unicode = as_unicode\n    context._set_with_template(template)\n\n    _render_context(template, callable_, context, *args,\n                            **_kwargs_for_callable(callable_, data))\n    return context._pop_buffer().getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _exec_template(callable_, context, args=None, kwargs=None):\n    template = context._with_template\n    if template is not None and \\\n            (template.format_exceptions or template.error_handler):\n        try:\n            callable_(context, *args, **kwargs)\n        except Exception:\n            _render_error(template, context, compat.exception_as())\n        except:\n            e = sys.exc_info()[0]\n            _render_error(template, context, e)\n    else:\n        callable_(context, *args, **kwargs)", "response": "execute a rendering callable given the callable context and optional explicit arguments\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _push_writer(self):\n\n        buf = util.FastEncodingBuffer()\n        self._buffer_stack.append(buf)\n        return buf.write", "response": "push a capturing buffer onto this Context and return\n        the new writer function."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npops the most recent capturing buffer from this Context and return the current writer after the pop.", "response": "def _pop_buffer_and_writer(self):\n        \"\"\"pop the most recent capturing buffer from this Context\n        and return the current writer after the pop.\n\n        \"\"\"\n\n        buf = self._buffer_stack.pop()\n        return buf, self._buffer_stack[-1].write"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, key, default=None):\n\n        return self._data.get(key, compat_builtins.__dict__.get(key, default))", "response": "Return a value from this : class :. Context."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new Context with a copy of this one with the given dictionary d.", "response": "def _locals(self, d):\n        \"\"\"Create a new :class:`.Context` with a copy of this\n        :class:`.Context`'s current state,\n        updated with the given dictionary.\n\n        The :attr:`.Context.kwargs` collection remains\n        unaffected.\n\n\n        \"\"\"\n\n        if not d:\n            return self\n        c = self._copy()\n        c._data.update(d)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new copy of this : class :. Context with no tokens related to inheritance state removed.", "response": "def _clean_inheritance_tokens(self):\n        \"\"\"create a new copy of this :class:`.Context`. with\n        tokens related to inheritance state removed.\"\"\"\n\n        c = self._copy()\n        x = c._data\n        x.pop('self', None)\n        x.pop('parent', None)\n        x.pop('next', None)\n        return c"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncycles through the values in the sequence.", "response": "def cycle(self, *values):\n        \"\"\"Cycle through values as the loop progresses.\n        \"\"\"\n        if not values:\n            raise ValueError(\"You must provide values to cycle through\")\n        return values[self.index % len(values)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a : class :. Namespace corresponding to the given uri.", "response": "def get_namespace(self, uri):\n        \"\"\"Return a :class:`.Namespace` corresponding to the given ``uri``.\n\n        If the given ``uri`` is a relative URI (i.e. it does not\n        contain a leading slash ``/``), the ``uri`` is adjusted to\n        be relative to the ``uri`` of the namespace itself. This\n        method is therefore mostly useful off of the built-in\n        ``local`` namespace, described in :ref:`namespace_local`.\n\n        In\n        most cases, a template wouldn't need this function, and\n        should instead use the ``<%namespace>`` tag to load\n        namespaces. However, since all ``<%namespace>`` tags are\n        evaluated before the body of a template ever runs,\n        this method can be used to locate namespaces using\n        expressions that were generated within the body code of\n        the template, or to conditionally use a particular\n        namespace.\n\n        \"\"\"\n        key = (self, uri)\n        if key in self.context.namespaces:\n            return self.context.namespaces[key]\n        else:\n            ns = TemplateNamespace(uri, self.context._copy(),\n                                templateuri=uri,\n                                calling_uri=self._templateuri)\n            self.context.namespaces[key] = ns\n            return ns"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef include_file(self, uri, **kwargs):\n\n        _include_file(self.context, uri, self._templateuri, **kwargs)", "response": "Include a file at the given uri."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_path(dirs, path_to_find):\n    for dir in dirs:\n        if os.path.exists(os.path.join(dir, path_to_find)):\n            return dir\n    return None", "response": "Find the first dir that matches path_to_find."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef to_bool(s):\n    if isinstance(s, bool):\n        return s\n    elif s.lower() in ['true', '1']:\n        return True\n    elif s.lower() in ['false', '0']:\n        return False\n    else:\n        raise ValueError(\"Can't cast '%s' to bool\" % (s))", "response": "Convert string s into a boolean."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender the output of this template as a string.", "response": "def render(self, *args, **data):\n        \"\"\"Render the output of this template as a string.\n\n        If the template specifies an output encoding, the string\n        will be encoded accordingly, else the output is raw (raw\n        output uses `cStringIO` and can't handle multibyte\n        characters). A :class:`.Context` object is created corresponding\n        to the given data. Arguments that are explicitly declared\n        by this template's internal rendering method are also\n        pulled from the given ``*args``, ``**data`` members.\n\n        \"\"\"\n        return runtime._render(self, self.callable_, args, data)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrendering the output of this template as a unicode object.", "response": "def render_unicode(self, *args, **data):\n        \"\"\"Render the output of this template as a unicode object.\"\"\"\n\n        return runtime._render(self,\n                                self.callable_,\n                                args,\n                                data,\n                                as_unicode=True)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender this : class :. Template with the given context.", "response": "def render_context(self, context, *args, **kwargs):\n        \"\"\"Render this :class:`.Template` with the given context.\n\n        The data is written to the context's buffer.\n\n        \"\"\"\n        if getattr(context, '_with_template', None) is None:\n            context._set_with_template(self)\n        runtime._render_context(self,\n                                self.callable_,\n                                context,\n                                *args,\n                                **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a string and an interable of extensions strip the extenion off the string.", "response": "def strip_exts(s, exts):\n    \"\"\"\n    Given a string and an interable of extensions, strip the extenion off the\n    string if the string ends with one of the extensions.\n    \"\"\"\n    f_split = os.path.splitext(s)\n    if f_split[1] in exts:\n        return f_split[0]\n    else:\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _parse_limit(self, limit):\n        if limit is None:\n            return None\n\n        limit_parsed = {\n            \"include\": [],\n            \"exclude\": []\n        }\n        elems = limit.split(\":\")\n        for elem in elems:\n            if elem.startswith('!'):\n                limit_parsed['exclude'].append(elem[1:])\n            else:\n                limit_parsed['include'].append(elem)\n\n        return limit_parsed", "response": "Parse a host or group limit into a dict of things to be included and things to be excluded."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _handle_inventory(self, inventory_path):\n        self.log.debug(\"Determining type of inventory_path {}\".format(inventory_path))\n        if os.path.isfile(inventory_path) and \\\n           util.is_executable(inventory_path):\n            # It's a file and it's executable. Handle as dynamic inventory script\n            self.log.debug(\"{} is a executable. Handle as dynamic inventory script\".format(inventory_path))\n            self._parse_dyn_inventory(inventory_path)\n        elif os.path.isfile(inventory_path):\n            # Static inventory hosts file\n            self.log.debug(\"{} is a file. Handle as static inventory file\".format(inventory_path))\n            self._parse_hosts_inventory(inventory_path)\n        elif os.path.isdir(inventory_path):\n            # Directory\n            self.log.debug(\"{} is a dir. Just try most files to see if they happen to be inventory files\".format(inventory_path))\n\n            # Don't parse folder as inventory if it is a .git or group/host_vars\n            if any(os.path.basename(inventory_path) == name for name in ['.git', 'group_vars', 'host_vars']):\n                return\n\n            # Scan directory\n            for fname in os.listdir(inventory_path):\n                # Skip files that end with certain extensions or characters\n                if any(fname.endswith(ext) for ext in [\"~\", \".orig\", \".bak\", \".ini\", \".cfg\", \".retry\", \".pyc\", \".pyo\", \".gitignore\"]):\n                    continue\n\n                self._handle_inventory(os.path.join(inventory_path, fname))\n        else:\n            raise IOError(\"Invalid inventory file / dir: '{0}'\".format(inventory_path))", "response": "Scan the inventory and handle any files that are found in the inventory_path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_hosts_inventory(self, inventory_path):\n        hosts_contents = []\n        if os.path.isdir(inventory_path):\n            self.log.debug(\"Inventory path {} is a dir. Looking for inventory files in that dir.\".format(inventory_path))\n            for fname in os.listdir(inventory_path):\n                # Skip .git folder\n                if fname == '.git':\n                    continue\n                path = os.path.join(inventory_path, fname)\n                if os.path.isdir(path):\n                    continue\n                with codecs.open(path, 'r', encoding='utf8') as f:\n                    hosts_contents += f.readlines()\n        else:\n            self.log.debug(\"Inventory path {} is a file. Reading as inventory.\".format(inventory_path))\n            with codecs.open(inventory_path, 'r', encoding='utf8') as f:\n                hosts_contents = f.readlines()\n\n        # Parse inventory and apply it to the hosts\n        hosts_parser = parser.HostsParser(hosts_contents)\n        for hostname, key_values in hosts_parser.hosts.items():\n            self.update_host(hostname, key_values)", "response": "Read all the available hosts inventory into one big list\n        and parse it."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the host_vars directory and return a tuple of the hostname and the value of the first entry in the file.", "response": "def _parse_hostvar_dir(self, inventory_path):\n        \"\"\"\n        Parse host_vars dir, if it exists.\n        \"\"\"\n        # inventory_path could point to a `hosts` file, or to a dir. So we\n        # construct the location to the `host_vars` differently.\n        if os.path.isdir(inventory_path):\n            path = os.path.join(inventory_path, 'host_vars')\n        else:\n            path = os.path.join(os.path.dirname(inventory_path), 'host_vars')\n\n        self.log.debug(\"Parsing host vars (dir): {0}\".format(path))\n        if not os.path.exists(path):\n            self.log.info(\"No such dir {0}\".format(path))\n            return\n\n        for entry in os.listdir(path):\n            # Skip .git folder\n            if entry == '.git':\n                continue\n            full_path = os.path.join(path, entry)\n\n            # file or dir name is the hostname\n            hostname = strip_exts(entry, ('.yml', '.yaml', '.json'))\n\n            if os.path.isfile(full_path):\n                # Parse contents of file as host vars.\n                self._parse_hostvar_file(hostname, full_path)\n            elif os.path.isdir(full_path):\n                # Parse each file in the directory as a file containing\n                # variables for the host.\n                for file_entry in os.listdir(full_path):\n                    p = os.path.join(full_path, file_entry)\n                    if not os.path.isdir(p):\n                        self._parse_hostvar_file(hostname, p)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_hostvar_file(self, hostname, path):\n        # Check for ansible-vault files, because they're valid yaml for\n        # some reason... (psst, the reason is that yaml sucks)\n        first_line = open(path, 'r').readline()\n        if first_line.startswith('$ANSIBLE_VAULT'):\n            self.log.warning(\"Skipping encrypted vault file {0}\".format(path))\n            return\n\n        try:\n            self.log.debug(\"Reading host vars from {}\".format(path))\n            f = codecs.open(path, 'r', encoding='utf8')\n            invars = ihateyaml.safe_load(f)\n            f.close()\n        except Exception as err:\n            # Just catch everything because yaml...\n            self.log.warning(\"Yaml couldn't load '{0}'. Skipping. Error was: {1}\".format(path, err))\n            return\n\n        if invars is None:\n            # Empty file or whatever. This is *probably* a side-effect of our\n            # own yaml.SafeLoader implementation ('ihateyaml'), because this\n            # problem didn't exist before.\n            return\n\n        if hostname == \"all\":\n            # Hostname 'all' is special and applies to all hosts\n            for hostname in self.hosts_all():\n                self.update_host(hostname, {'hostvars': invars}, overwrite=False)\n        else:\n            self.update_host(hostname, {'hostvars': invars}, overwrite=True)", "response": "Parse a host var file and apply it to the hostname."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse group_vars dir and update hosts and group_vars dict.", "response": "def _parse_groupvar_dir(self, inventory_path):\n        \"\"\"\n        Parse group_vars dir, if it exists. Encrypted vault files are skipped.\n        \"\"\"\n        # inventory_path could point to a `hosts` file, or to a dir. So we\n        # construct the location to the `group_vars` differently.\n        if os.path.isdir(inventory_path):\n            path = os.path.join(inventory_path, 'group_vars')\n        else:\n            path = os.path.join(os.path.dirname(inventory_path), 'group_vars')\n\n        self.log.debug(\"Parsing group vars (dir): {0}\".format(path))\n        if not os.path.exists(path):\n            self.log.info(\"No such dir {0}\".format(path))\n            return\n\n        for (dirpath, dirnames, filenames) in os.walk(path):\n            for filename in filenames:\n                full_path = os.path.join(dirpath, filename)\n\n                # filename is the group name\n                groupname = strip_exts(filename, ('.yml', '.yaml', '.json'))\n\n                try:\n                    self.log.debug(\"Reading group vars from {}\".format(full_path))\n                    f = codecs.open(full_path, 'r', encoding='utf8')\n                    invars = ihateyaml.safe_load(f)\n                    f.close()\n                except Exception as err:\n                    # Just catch everything because yaml...\n                    self.log.warning(\"Yaml couldn't load '{0}' because '{1}'. Skipping\".format(full_path, err))\n                    continue  # Go to next file\n\n                if groupname == 'all':\n                    # groupname 'all' is special and applies to all hosts.\n                    for hostname in self.hosts_all():\n                        self.update_host(hostname, {'hostvars': invars}, overwrite=False)\n                else:\n                    for hostname in self.hosts_in_group(groupname):\n                        self.update_host(hostname, {'hostvars': invars}, overwrite=False)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_fact_dir(self, fact_dir, fact_cache=False):\n        self.log.debug(\"Parsing fact dir: {0}\".format(fact_dir))\n        if not os.path.isdir(fact_dir):\n            raise IOError(\"Not a directory: '{0}'\".format(fact_dir))\n\n        flist = []\n        for (dirpath, dirnames, filenames) in os.walk(fact_dir):\n            flist.extend(filenames)\n            break\n\n        for fname in flist:\n            if fname.startswith('.'):\n                continue\n            self.log.debug(\"Reading host facts from {0}\".format(os.path.join(fact_dir, fname)))\n            hostname = fname\n\n            fd = codecs.open(os.path.join(fact_dir, fname), 'r', encoding='utf8')\n            s = fd.readlines()\n            fd.close()\n            try:\n                x = json.loads(''.join(s))\n                # for compatibility with fact_caching=jsonfile\n                # which omits the \"ansible_facts\" parent key added by the setup module\n                if fact_cache:\n                    x = json.loads('{ \"ansible_facts\": ' + ''.join(s) + ' }')\n                self.update_host(hostname, x)\n                self.update_host(hostname, {'name': hostname})\n            except ValueError as e:\n                # Ignore non-JSON files (and bonus errors)\n                self.log.warning(\"Error parsing: %s: %s\" % (fname, e))", "response": "Parse a directory of fact files and extract information from them."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute a dynamic inventory script and parse the results.", "response": "def _parse_dyn_inventory(self, script):\n        \"\"\"\n        Execute a dynamic inventory script and parse the results.\n        \"\"\"\n        self.log.debug(\"Reading dynamic inventory {0}\".format(script))\n        try:\n            proc = subprocess.Popen([script, '--list'],\n                                    stdout=subprocess.PIPE,\n                                    stderr=subprocess.PIPE,\n                                    close_fds=True)\n            stdout, stderr = proc.communicate(input)\n            if proc.returncode != 0:\n                sys.stderr.write(\"Dynamic inventory script '{0}' returned \"\n                                 \"exitcode {1}\\n\".format(script,\n                                                         proc.returncode))\n                for line in stderr:\n                    sys.stderr.write(line)\n\n            dyninv_parser = parser.DynInvParser(stdout.decode('utf8'))\n            for hostname, key_values in dyninv_parser.hosts.items():\n                self.update_host(hostname, key_values)\n        except OSError as err:\n            sys.stderr.write(\"Exception while executing dynamic inventory script '{0}':\\n\\n\".format(script))\n            sys.stderr.write(str(err) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate a host s information.", "response": "def update_host(self, hostname, key_values, overwrite=True):\n        \"\"\"\n        Update a hosts information. This is called by various collectors such\n        as the ansible setup module output and the hosts parser to add\n        informatio to a host. It does some deep inspection to make sure nested\n        information can be updated.\n        \"\"\"\n        default_empty_host = {\n            'name': hostname,\n            'hostvars': {},\n        }\n        host_info = self.hosts.get(hostname, default_empty_host)\n        util.deepupdate(host_info, key_values, overwrite=overwrite)\n        self.hosts[hostname] = host_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of hostnames that are in a group.", "response": "def hosts_in_group(self, groupname):\n        \"\"\"\n        Return a list of hostnames that are in a group.\n        \"\"\"\n        result = []\n        for hostname, hostinfo in self.hosts.items():\n            if groupname == 'all':\n                result.append(hostname)\n            elif 'groups' in hostinfo:\n                if groupname in hostinfo['groups']:\n                    result.append(hostname)\n            else:\n                hostinfo['groups'] = [groupname]\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of parsed hosts info with the limit applied if required.", "response": "def get_hosts(self):\n        \"\"\"\n        Return a list of parsed hosts info, with the limit applied if required.\n        \"\"\"\n        limited_hosts = {}\n        if self.limit is not None:\n            # Find hosts and groups of hosts to include\n            for include in self.limit['include']:\n                # Include whole group\n                for hostname in self.hosts_in_group(include):\n                    limited_hosts[hostname] = self.hosts[hostname]\n                # Include individual host\n                if include in self.hosts:\n                    limited_hosts[include] = self.hosts[include]\n            # Find hosts and groups of hosts to exclude\n            for exclude in self.limit[\"exclude\"]:\n                # Exclude whole group\n                for hostname in self.hosts_in_group(exclude):\n                    if hostname in limited_hosts:\n                        limited_hosts.pop(hostname)\n                # Exclude individual host\n                if exclude in limited_hosts:\n                    limited_hosts.pop(exclude)\n\n            return limited_hosts\n        else:\n            # Return all hosts\n            return self.hosts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_data_dir():\n    data_dir_paths = [\n        os.path.join(os.path.dirname(ansiblecmdb.__file__), 'data'),\n        os.path.join(os.path.dirname(sys.argv[0]), '..', 'lib', 'ansiblecmdb', 'data'),\n        '/usr/local/lib/ansiblecmdb/data',\n        '/usr/lib/ansiblecmdb/data',\n    ]\n\n    data_dir = util.find_path(data_dir_paths, 'tpl/html_fancy.tpl')\n    if not data_dir:\n        sys.stdout.write(\"Couldn't find the data dir for the templates. I tried: {0}\\n\".format(\", \".join(data_dir_paths)))\n        sys.exit(1)\n\n    return data_dir", "response": "Find out what data directory is used for the current ansible - cmdb page."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind out the location of the hosts file.", "response": "def get_hosts_files(option):\n    \"\"\"\n    Find out the location of the `hosts` file. This looks in multiple places\n    such as the `-i` option, current dir and ansible configuration files. The\n    first match is returned as a list.\n    \"\"\"\n    if option is not None:\n        return option.split(',')\n\n    # Use hosts file from the current dir if it exists\n    if os.path.isfile('hosts'):\n        return ['hosts']\n\n    # Perhaps it's configured in a configuration file. Try to find a\n    # configuration file and see if it contains a `hostsfile` entry.\n    config_locations = [\n        '.',\n        '/etc/ansible/'\n    ]\n    config_dir = util.find_path(config_locations, 'ansible.cfg')\n    log.debug('config_dir = {0}'.format(config_dir))\n    if config_dir:\n        with open(os.path.join(config_dir, 'ansible.cfg'), 'r') as cf:\n            for line in cf:\n                if line.startswith('hostfile'):\n                    return [line.split('=', 1)[1].strip()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload custom column definitions.", "response": "def get_cust_cols(path):\n    \"\"\"\n    Load custom column definitions.\n    \"\"\"\n    required_keys = [\"title\", \"id\", \"sType\", \"visible\"]\n\n    with open(path, 'r') as f:\n        try:\n            cust_cols = ast.literal_eval(f.read())\n        except Exception as err:\n            sys.stderr.write(\"Invalid custom columns file: {}\\n\".format(path))\n            sys.stderr.write(\"{}\\n\".format(err))\n            sys.exit(1)\n\n    # Validate\n    for col in cust_cols:\n        for required_key in required_keys:\n            if required_key not in col:\n                sys.stderr.write(\"Missing required key '{}' in custom \"\n                                 \"column {}\\n\".format(required_key, col))\n                sys.exit(1)\n            if \"jsonxs\" not in col and \"tpl\" not in col:\n                sys.stderr.write(\"You need to specify 'jsonxs' or 'tpl' \"\n                                 \"for custom column {}\\n\".format(col))\n                sys.exit(1)\n\n    return cust_cols"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_user_params(user_params):\n    if user_params:\n        params = {}\n        try:\n            for param in options.params.split(','):\n                param_key, param_value = param.split('=', 1)\n                params[param_key] = param_value\n        except ValueError as e:\n            sys.stdout.write(\"Invalid params specified. Should be in format: <key=value>[,<key=value>..]\\n\")\n            sys.exit(1)\n        return params\n    else:\n        return {}", "response": "Parse the user params and return a dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _tpl_possibilities(self):\n        tpl_possibilities = [\n            os.path.realpath(self.tpl)\n        ]\n        for tpl_dir in self.tpl_dirs:\n            tpl_possibilities.append(os.path.realpath(os.path.join(tpl_dir, \"{0}.tpl\".format(self.tpl))))\n            tpl_possibilities.append(os.path.realpath(os.path.join(tpl_dir, \"{0}.py\".format(self.tpl))))\n\n        return tpl_possibilities", "response": "Construct a list of possible paths to templates."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _find_tpl(self):\n        for tpl_possibility in self.tpl_possibilities:\n            if os.path.isfile(tpl_possibility):\n                return tpl_possibility\n\n        return None", "response": "Find a template in the list of possible paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render(self, hosts, vars={}):\n        if self.tpl_file.endswith(\".tpl\"):\n            return self._render_mako(hosts, vars)\n        elif self.tpl_file.endswith(\".py\"):\n            return self._render_py(hosts, vars)\n        else:\n            raise ValueError(\"Don't know how to handle '{0}'\".format(self.tpl_file))", "response": "Render a mako or. py file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef sourcehook(self, newfile, encoding='utf-8'):\r\n        \"Hook called on a filename to be sourced.\"\r\n        from codecs import open\r\n        if newfile[0] == '\"':\r\n            newfile = newfile[1:-1]\r\n        # This implements cpp-like semantics for relative-path inclusion.\r\n        if isinstance(self.infile, basestring) and not os.path.isabs(newfile):\r\n            newfile = os.path.join(os.path.dirname(self.infile), newfile)\r\n        return (newfile, open(newfile, \"r\", encoding))", "response": "Hook called on a filename to be sourced."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a template from a file or a string", "response": "def load_template(self, templatename, template_string=None):\n        \"\"\"Loads a template from a file or a string\"\"\"\n        if template_string is not None:\n            return Template(template_string, **self.tmpl_options)\n        # Translate TG dot notation to normal / template path\n        if '/' not in templatename:\n            templatename = '/' + templatename.replace('.', '/') + '.' +\\\n                    self.extension\n\n        # Lookup template\n        return self.lookup.get_template(templatename)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_argument_expressions(self, as_call=False):\n\n        namedecls = []\n\n        # Build in reverse order, since defaults and slurpy args come last\n        argnames = self.argnames[::-1]\n        kwargnames = self.kwargnames[::-1]\n        defaults = self.defaults[::-1]\n        kwdefaults = self.kwdefaults[::-1]\n\n        # Named arguments\n        if self.kwargs:\n            namedecls.append(\"**\" + kwargnames.pop(0))\n\n        for name in kwargnames:\n            # Keyword-only arguments must always be used by name, so even if\n            # this is a call, print out `foo=foo`\n            if as_call:\n                namedecls.append(\"%s=%s\" % (name, name))\n            elif kwdefaults:\n                default = kwdefaults.pop(0)\n                if default is None:\n                    # The AST always gives kwargs a default, since you can do\n                    # `def foo(*, a=1, b, c=3)`\n                    namedecls.append(name)\n                else:\n                    namedecls.append(\"%s=%s\" % (\n                        name, pyparser.ExpressionGenerator(default).value()))\n            else:\n                namedecls.append(name)\n\n        # Positional arguments\n        if self.varargs:\n            namedecls.append(\"*\" + argnames.pop(0))\n\n        for name in argnames:\n            if as_call or not defaults:\n                namedecls.append(name)\n            else:\n                default = defaults.pop(0)\n                namedecls.append(\"%s=%s\" % (\n                    name, pyparser.ExpressionGenerator(default).value()))\n\n        namedecls.reverse()\n        return namedecls", "response": "Return the argument declarations of this FunctionDecl as a printable\n        list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef legacy_html_escape(s):\n    s = s.replace(\"&\", \"&amp;\")\n    s = s.replace(\">\", \"&gt;\")\n    s = s.replace(\"<\", \"&lt;\")\n    s = s.replace('\"', \"&#34;\")\n    s = s.replace(\"'\", \"&#39;\")\n    return s", "response": "legacy HTML escape for non - unicode mode."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef htmlentityreplace_errors(ex):\n    if isinstance(ex, UnicodeEncodeError):\n        # Handle encoding errors\n        bad_text = ex.object[ex.start:ex.end]\n        text = _html_entities_escaper.escape(bad_text)\n        return (compat.text_type(text), ex.end)\n    raise ex", "response": "An encoding error handler that replaces unencodable unicode characters with HTML entities."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef escape(self, text):\n        return self.__escapable.sub(self.__escape, compat.text_type(text)\n                                    ).encode('ascii')", "response": "Escape the given text."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the multiline comment at lineno split into a list of of comment line numbers and the accompanying comment line numbers", "response": "def _split_comment(lineno, comment):\n        \"\"\"Return the multiline comment at lineno split into a list of\n        comment line numbers and the accompanying comment line\"\"\"\n        return [(lineno + index, line) for index, line in\n                enumerate(comment.splitlines())]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef adjust_whitespace(text):\n\n    state = [False, False]\n    (backslashed, triplequoted) = (0, 1)\n\n    def in_multi_line(line):\n        start_state = (state[backslashed] or state[triplequoted])\n\n        if re.search(r\"\\\\$\", line):\n            state[backslashed] = True\n        else:\n            state[backslashed] = False\n\n        def match(reg, t):\n            m = re.match(reg, t)\n            if m:\n                return m, t[len(m.group(0)):]\n            else:\n                return None, t\n\n        while line:\n            if state[triplequoted]:\n                m, line = match(r\"%s\" % state[triplequoted], line)\n                if m:\n                    state[triplequoted] = False\n                else:\n                    m, line = match(r\".*?(?=%s|$)\" % state[triplequoted], line)\n            else:\n                m, line = match(r'#', line)\n                if m:\n                    return start_state\n\n                m, line = match(r\"\\\"\\\"\\\"|\\'\\'\\'\", line)\n                if m:\n                    state[triplequoted] = m.group(0)\n                    continue\n\n                m, line = match(r\".*?(?=\\\"\\\"\\\"|\\'\\'\\'|#|$)\", line)\n\n        return start_state\n\n    def _indent_line(line, stripspace=''):\n        return re.sub(r\"^%s\" % stripspace, '', line)\n\n    lines = []\n    stripspace = None\n\n    for line in re.split(r'\\r?\\n', text):\n        if in_multi_line(line):\n            lines.append(line)\n        else:\n            line = line.expandtabs()\n            if stripspace is None and re.search(r\"^[ \\t]*[^# \\t]\", line):\n                stripspace = re.match(r\"^([ \\t]*)\", line).group(1)\n            lines.append(_indent_line(line, stripspace))\n    return \"\\n\".join(lines)", "response": "remove the left - whitespace margin of a block of Python code."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints a line or lines of python which already contain indentation.", "response": "def write_indented_block(self, block):\n        \"\"\"print a line or lines of python which already contain indentation.\n\n        The indentation of the total block of lines will be adjusted to that of\n        the current indent level.\"\"\"\n        self.in_indent_lines = False\n        for l in re.split(r'\\r?\\n', block):\n            self.line_buffer.append(l)\n            self._update_lineno(1)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprint a line of python", "response": "def writeline(self, line):\n        \"\"\"print a line of python, indenting it according to the current\n        indent level.\n\n        this also adjusts the indentation counter according to the\n        content of the line.\n\n        \"\"\"\n\n        if not self.in_indent_lines:\n            self._flush_adjusted_lines()\n            self.in_indent_lines = True\n\n        if (line is None or\n            re.match(r\"^\\s*#\",line) or\n            re.match(r\"^\\s*$\", line)\n            ):\n            hastext = False\n        else:\n            hastext = True\n\n        is_comment = line and len(line) and line[0] == '#'\n\n        # see if this line should decrease the indentation level\n        if (not is_comment and\n            (not hastext or self._is_unindentor(line))\n            ):\n\n            if self.indent > 0:\n                self.indent -= 1\n                # if the indent_detail stack is empty, the user\n                # probably put extra closures - the resulting\n                # module wont compile.\n                if len(self.indent_detail) == 0:\n                    raise exceptions.SyntaxException(\n                                    \"Too many whitespace closures\")\n                self.indent_detail.pop()\n\n        if line is None:\n            return\n\n        # write the line\n        self.stream.write(self._indent_line(line) + \"\\n\")\n        self._update_lineno(len(line.split(\"\\n\")))\n\n        # see if this line should increase the indentation level.\n        # note that a line can both decrase (before printing) and\n        # then increase (after printing) the indentation level.\n\n        if re.search(r\":[ \\t]*(?:#.*)?$\", line):\n            # increment indentation count, and also\n            # keep track of what the keyword was that indented us,\n            # if it is a python compound statement keyword\n            # where we might have to look for an \"unindent\" keyword\n            match = re.match(r\"^\\s*(if|try|elif|while|for|with)\", line)\n            if match:\n                # its a \"compound\" keyword, so we will check for \"unindentors\"\n                indentor = match.group(1)\n                self.indent += 1\n                self.indent_detail.append(indentor)\n            else:\n                indentor = None\n                # its not a \"compound\" keyword.  but lets also\n                # test for valid Python keywords that might be indenting us,\n                # else assume its a non-indenting line\n                m2 = re.match(r\"^\\s*(def|class|else|elif|except|finally)\",\n                              line)\n                if m2:\n                    self.indent += 1\n                    self.indent_detail.append(indentor)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_unindentor(self, line):\n\n        # no indentation detail has been pushed on; return False\n        if len(self.indent_detail) == 0:\n            return False\n\n        indentor = self.indent_detail[-1]\n\n        # the last indent keyword we grabbed is not a\n        # compound statement keyword; return False\n        if indentor is None:\n            return False\n\n        # if the current line doesnt have one of the \"unindentor\" keywords,\n        # return False\n        match = re.match(r\"^\\s*(else|elif|except|finally).*\\:\", line)\n        if not match:\n            return False\n\n        # whitespace matches up, we have a compound indentor,\n        # and this line has an unindentor, this\n        # is probably good enough\n        return True", "response": "return true if the given line is an unindentor"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _indent_line(self, line, stripspace=''):\n\n        return re.sub(r\"^%s\" % stripspace, self.indentstring\n                      * self.indent, line)", "response": "indent the given line according to the current indent level."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _in_multi_line(self, line):\n\n        # we are only looking for explicitly joined lines here, not\n        # implicit ones (i.e. brackets, braces etc.).  this is just to\n        # guard against the possibility of modifying the space inside of\n        # a literal multiline string with unfortunately placed\n        # whitespace\n\n        current_state = (self.backslashed or self.triplequoted)\n\n        if re.search(r\"\\\\$\", line):\n            self.backslashed = True\n        else:\n            self.backslashed = False\n\n        triples = len(re.findall(r\"\\\"\\\"\\\"|\\'\\'\\'\", line))\n        if triples == 1 or triples % 2 != 0:\n            self.triplequoted = not self.triplequoted\n\n        return current_state", "response": "return true if the given line is part of a multi - line block"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprovide a template that renders a stack trace in an HTML format.", "response": "def html_error_template():\n    \"\"\"Provides a template that renders a stack trace in an HTML format,\n    providing an excerpt of code as well as substituting source template\n    filenames, line numbers and code for that of the originating source\n    template, as applicable.\n\n    The template's default ``encoding_errors`` value is\n    ``'htmlentityreplace'``. The template has two options. With the\n    ``full`` option disabled, only a section of an HTML document is\n    returned. With the ``css`` option disabled, the default stylesheet\n    won't be included.\n\n    \"\"\"\n    import mako.template\n    return mako.template.Template(r\"\"\"\n<%!\n    from mako.exceptions import RichTraceback, syntax_highlight,\\\n            pygments_html_formatter\n%>\n<%page args=\"full=True, css=True, error=None, traceback=None\"/>\n% if full:\n<html>\n<head>\n    <title>Mako Runtime Error</title>\n% endif\n% if css:\n    <style>\n        body { font-family:verdana; margin:10px 30px 10px 30px;}\n        .stacktrace { margin:5px 5px 5px 5px; }\n        .highlight { padding:0px 10px 0px 10px; background-color:#9F9FDF; }\n        .nonhighlight { padding:0px; background-color:#DFDFDF; }\n        .sample { padding:10px; margin:10px 10px 10px 10px;\n                  font-family:monospace; }\n        .sampleline { padding:0px 10px 0px 10px; }\n        .sourceline { margin:5px 5px 10px 5px; font-family:monospace;}\n        .location { font-size:80%; }\n        .highlight { white-space:pre; }\n        .sampleline { white-space:pre; }\n\n    % if pygments_html_formatter:\n        ${pygments_html_formatter.get_style_defs()}\n        .linenos { min-width: 2.5em; text-align: right; }\n        pre { margin: 0; }\n        .syntax-highlighted { padding: 0 10px; }\n        .syntax-highlightedtable { border-spacing: 1px; }\n        .nonhighlight { border-top: 1px solid #DFDFDF;\n                        border-bottom: 1px solid #DFDFDF; }\n        .stacktrace .nonhighlight { margin: 5px 15px 10px; }\n        .sourceline { margin: 0 0; font-family:monospace; }\n        .code { background-color: #F8F8F8; width: 100%; }\n        .error .code { background-color: #FFBDBD; }\n        .error .syntax-highlighted { background-color: #FFBDBD; }\n    % endif\n\n    </style>\n% endif\n% if full:\n</head>\n<body>\n% endif\n\n<h2>Error !</h2>\n<%\n    tback = RichTraceback(error=error, traceback=traceback)\n    src = tback.source\n    line = tback.lineno\n    if src:\n        lines = src.split('\\n')\n    else:\n        lines = None\n%>\n<h3>${tback.errorname}: ${tback.message|h}</h3>\n\n% if lines:\n    <div class=\"sample\">\n    <div class=\"nonhighlight\">\n% for index in range(max(0, line-4),min(len(lines), line+5)):\n    <%\n       if pygments_html_formatter:\n           pygments_html_formatter.linenostart = index + 1\n    %>\n    % if index + 1 == line:\n    <%\n       if pygments_html_formatter:\n           old_cssclass = pygments_html_formatter.cssclass\n           pygments_html_formatter.cssclass = 'error ' + old_cssclass\n    %>\n        ${lines[index] | syntax_highlight(language='mako')}\n    <%\n       if pygments_html_formatter:\n           pygments_html_formatter.cssclass = old_cssclass\n    %>\n    % else:\n        ${lines[index] | syntax_highlight(language='mako')}\n    % endif\n% endfor\n    </div>\n    </div>\n% endif\n\n<div class=\"stacktrace\">\n% for (filename, lineno, function, line) in tback.reverse_traceback:\n    <div class=\"location\">${filename}, line ${lineno}:</div>\n    <div class=\"nonhighlight\">\n    <%\n       if pygments_html_formatter:\n           pygments_html_formatter.linenostart = lineno\n    %>\n      <div class=\"sourceline\">${line | syntax_highlight(filename)}</div>\n    </div>\n% endfor\n</div>\n\n% if full:\n</body>\n</html>\n% endif\n\"\"\", output_encoding=sys.getdefaultencoding(),\n        encoding_errors='htmlentityreplace')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a unicode representation of self. error and initialize self. message.", "response": "def _init_message(self):\n        \"\"\"Find a unicode representation of self.error\"\"\"\n        try:\n            self.message = compat.text_type(self.error)\n        except UnicodeError:\n            try:\n                self.message = str(self.error)\n            except UnicodeEncodeError:\n                # Fallback to args as neither unicode nor\n                # str(Exception(u'\\xe6')) work in Python < 2.6\n                self.message = self.error.args[0]\n        if not isinstance(self.message, compat.text_type):\n            self.message = compat.text_type(self.message, 'ascii', 'replace')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _init(self, trcback):\n\n        import mako.template\n        mods = {}\n        rawrecords = traceback.extract_tb(trcback)\n        new_trcback = []\n        for filename, lineno, function, line in rawrecords:\n            if not line:\n                line = ''\n            try:\n                (line_map, template_lines) = mods[filename]\n            except KeyError:\n                try:\n                    info = mako.template._get_module_info(filename)\n                    module_source = info.code\n                    template_source = info.source\n                    template_filename = info.template_filename or filename\n                except KeyError:\n                    # A normal .py file (not a Template)\n                    if not compat.py3k:\n                        try:\n                            fp = open(filename, 'rb')\n                            encoding = util.parse_encoding(fp)\n                            fp.close()\n                        except IOError:\n                            encoding = None\n                        if encoding:\n                            line = line.decode(encoding)\n                        else:\n                            line = line.decode('ascii', 'replace')\n                    new_trcback.append((filename, lineno, function, line,\n                                            None, None, None, None))\n                    continue\n\n                template_ln = 1\n\n                source_map = mako.template.ModuleInfo.\\\n                                get_module_source_metadata(\n                                    module_source, full_line_map=True)\n                line_map = source_map['full_line_map']\n\n                template_lines = [line for line in\n                                    template_source.split(\"\\n\")]\n                mods[filename] = (line_map, template_lines)\n\n            template_ln = line_map[lineno - 1]\n\n            if template_ln <= len(template_lines):\n                template_line = template_lines[template_ln - 1]\n            else:\n                template_line = None\n            new_trcback.append((filename, lineno, function,\n                                line, template_filename, template_ln,\n                                template_line, template_source))\n        if not self.source:\n            for l in range(len(new_trcback) - 1, 0, -1):\n                if new_trcback[l][5]:\n                    self.source = new_trcback[l][7]\n                    self.lineno = new_trcback[l][5]\n                    break\n            else:\n                if new_trcback:\n                    try:\n                        # A normal .py file (not a Template)\n                        fp = open(new_trcback[-1][0], 'rb')\n                        encoding = util.parse_encoding(fp)\n                        fp.seek(0)\n                        self.source = fp.read()\n                        fp.close()\n                        if encoding:\n                            self.source = self.source.decode(encoding)\n                    except IOError:\n                        self.source = ''\n                    self.lineno = new_trcback[-1][1]\n        return new_trcback", "response": "format a traceback from sys. exc_info() into 7 - item tuples plus the original\n        template filename line number adjusted relative to the template\n        source line number of the template."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef has_template(self, uri):\n        try:\n            self.get_template(uri)\n            return True\n        except exceptions.TemplateLookupException:\n            return False", "response": "Return True if this : class :. TemplateLookup is capable of returning a : class :. Template object for the given uri."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a : class :. Template object corresponding to the given uri.", "response": "def get_template(self, uri):\n        \"\"\"Return a :class:`.Template` object corresponding to the given\n        ``uri``.\n\n        .. note:: The ``relativeto`` argument is not supported here at the moment.\n\n        \"\"\"\n\n        try:\n            if self.filesystem_checks:\n                return self._check(uri, self._collection[uri])\n            else:\n                return self._collection[uri]\n        except KeyError:\n            u = re.sub(r'^\\/+', '', uri)\n            for dir in self.directories:\n                srcfile = posixpath.normpath(posixpath.join(dir, u))\n                if os.path.isfile(srcfile):\n                    return self._load(srcfile, uri)\n            else:\n                raise exceptions.TopLevelLookupException(\n                                    \"Cant locate template for uri %r\" % uri)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef adjust_uri(self, uri, relativeto):\n\n        key = (uri, relativeto)\n        if key in self._uri_cache:\n            return self._uri_cache[key]\n\n        if uri[0] != '/':\n            if relativeto is not None:\n                v = self._uri_cache[key] = posixpath.join(\n                                            posixpath.dirname(relativeto), uri)\n            else:\n                v = self._uri_cache[key] = '/' + uri\n        else:\n            v = self._uri_cache[key] = uri\n        return v", "response": "Adjust the given uri based on the given relative URI."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting the given filename to a URI relative to the base to this : class :. TemplateCollection.", "response": "def filename_to_uri(self, filename):\n        \"\"\"Convert the given ``filename`` to a URI relative to\n           this :class:`.TemplateCollection`.\"\"\"\n\n        try:\n            return self._uri_cache[filename]\n        except KeyError:\n            value = self._relativeize(filename)\n            self._uri_cache[filename] = value\n            return value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the portion of a filename that is relative to the directories in this lookup.", "response": "def _relativeize(self, filename):\n        \"\"\"Return the portion of a filename that is 'relative'\n           to the directories in this lookup.\n\n        \"\"\"\n\n        filename = posixpath.normpath(filename)\n        for dir in self.directories:\n            if filename[0:len(dir)] == dir:\n                return filename[len(dir):]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef put_string(self, uri, text):\n        self._collection[uri] = Template(\n                                    text,\n                                    lookup=self,\n                                    uri=uri,\n                                    **self.template_args)", "response": "Place a new Template object into this\n        based on the given string of\n        text."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the specified resource and parses all style URLs and their respective asset URLs.", "response": "def _get_style_urls(self, asset_url_path):\n        \"\"\"\n        Gets the specified resource and parses all style URLs and their\n        assets in the form of the specified patterns.\n        \"\"\"\n        # Check cache\n        if self.cache_path:\n            cached = self._get_cached_style_urls(asset_url_path)\n            # Skip fetching styles if there's any already cached\n            if cached:\n                return cached\n\n        # Find style URLs\n        r = requests.get(STYLE_URLS_SOURCE)\n        if not 200 <= r.status_code < 300:\n            print('Warning: retrieving styles gave status code',\n                  r.status_code, file=sys.stderr)\n        urls = []\n        for style_urls_re in STYLE_URLS_RES:\n            urls.extend(re.findall(style_urls_re, r.text))\n        if not urls:\n            print('Warning: no styles found - see https://github.com/joeyespo/'\n                  'grip/issues/265', file=sys.stderr)\n\n        # Cache the styles and their assets\n        if self.cache_path:\n            is_cached = self._cache_contents(urls, asset_url_path)\n            if is_cached:\n                urls = self._get_cached_style_urls(asset_url_path)\n\n        return urls"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the URLs of the cached styles.", "response": "def _get_cached_style_urls(self, asset_url_path):\n        \"\"\"\n        Gets the URLs of the cached styles.\n        \"\"\"\n        try:\n            cached_styles = os.listdir(self.cache_path)\n        except IOError as ex:\n            if ex.errno != errno.ENOENT and ex.errno != errno.ESRCH:\n                raise\n            return []\n        except OSError:\n            return []\n        return [posixpath.join(asset_url_path, style)\n                for style in cached_styles\n                if style.endswith('.css')]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cache_contents(self, style_urls, asset_url_path):\n        files = {}\n\n        asset_urls = []\n        for style_url in style_urls:\n            if not self.quiet:\n                print(' * Downloading style', style_url, file=sys.stderr)\n            r = requests.get(style_url)\n            if not 200 <= r.status_code < 300:\n                print(' -> Warning: Style request responded with',\n                      r.status_code, file=sys.stderr)\n                files = None\n                continue\n            asset_content = r.text\n            # Find assets and replace their base URLs with the cache directory\n            for url in re.findall(STYLE_ASSET_URLS_RE, asset_content):\n                asset_urls.append(urljoin(style_url, url))\n            contents = re.sub(\n                STYLE_ASSET_URLS_RE,\n                STYLE_ASSET_URLS_SUB_FORMAT.format(asset_url_path.rstrip('/')),\n                asset_content)\n            # Prepare cache\n            if files is not None:\n                filename = self.cache_filename(style_url)\n                files[filename] = contents.encode('utf-8')\n\n        for asset_url in asset_urls:\n            if not self.quiet:\n                print(' * Downloading asset', asset_url, file=sys.stderr)\n            # Retrieve binary file and show message\n            r = requests.get(asset_url, stream=True)\n            if not 200 <= r.status_code < 300:\n                print(' -> Warning: Asset request responded with',\n                      r.status_code, file=sys.stderr)\n                files = None\n                continue\n            # Prepare cache\n            if files is not None:\n                filename = self.cache_filename(asset_url)\n                files[filename] = r.raw.read(decode_content=True)\n\n        # Skip caching if something went wrong to try again next time\n        if not files:\n            return False\n\n        # Cache files if all downloads were successful\n        cache = {}\n        for relname in files:\n            cache[safe_join(self.cache_path, relname)] = files[relname]\n        if not os.path.exists(self.cache_path):\n            os.makedirs(self.cache_path)\n        for filename in cache:\n            with open(filename, 'wb') as f:\n                f.write(cache[filename])\n        if not self.quiet:\n            print(\n                ' * Cached all downloads in', self.cache_path, file=sys.stderr)\n        return True", "response": "Fetches the given URLs and caches their contents\n            and their assets in the given directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef retrieve_styles(self, asset_url_path):\n        if not asset_url_path.endswith('/'):\n            asset_url_path += '/'\n        self.style_urls.extend(self._get_style_urls(asset_url_path))", "response": "Retrieve the style URLs from the source HTML page and specified asset base URL."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_server_running(host, port):\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    try:\n        return s.connect_ex((host, port)) == 0\n    finally:\n        s.close()", "response": "Checks whether a server is currently listening on the specified host and port."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait_for_server(host, port, cancel_event=None):\n    while not is_server_running(host, port):\n        # Stop waiting if shutting down\n        if cancel_event and cancel_event.is_set():\n            return False\n        time.sleep(0.1)\n    return True", "response": "Waits until a local server is listening on the specified host and port."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef wait_and_start_browser(host, port=None, cancel_event=None):\n    if host == '0.0.0.0':\n        host = 'localhost'\n    if port is None:\n        port = 80\n\n    if wait_for_server(host, port, cancel_event):\n        start_browser('http://{0}:{1}/'.format(host, port))", "response": "Waits for the server to run and then opens the specified address in\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start_browser_when_ready(host, port=None, cancel_event=None):\n    browser_thread = Thread(\n        target=wait_and_start_browser, args=(host, port, cancel_event))\n    browser_thread.daemon = True\n    browser_thread.start()\n    return browser_thread", "response": "Starts a thread that waits for the server then opens the specified\n    address in the browser."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrender the specified markdown content and embedded styles.", "response": "def render(self, text, auth=None):\n        \"\"\"\n        Renders the specified markdown content and embedded styles.\n\n        Raises TypeError if text is not a Unicode string.\n        Raises requests.HTTPError if the request fails.\n        \"\"\"\n        # Ensure text is Unicode\n        expected = str if sys.version_info[0] >= 3 else unicode  # noqa\n        if not isinstance(text, expected):\n            raise TypeError(\n                'Expected a Unicode string, got {!r}.'.format(text))\n\n        if self.user_content:\n            url = '{0}/markdown'.format(self.api_url)\n            data = {'text': text, 'mode': 'gfm'}\n            if self.context:\n                data['context'] = self.context\n            data = json.dumps(data, ensure_ascii=False).encode('utf-8')\n            headers = {'content-type': 'application/json; charset=UTF-8'}\n        else:\n            url = '{0}/markdown/raw'.format(self.api_url)\n            data = text.encode('utf-8')\n            headers = {'content-type': 'text/x-markdown; charset=UTF-8'}\n\n        r = requests.post(url, headers=headers, data=data, auth=auth)\n        r.raise_for_status()\n\n        # FUTURE: Remove this once GitHub API properly handles Unicode markdown\n        r.encoding = 'utf-8'\n\n        return r.text if self.raw else patch(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render(self, text, auth=None):\n        if markdown is None:\n            import markdown\n        if UrlizeExtension is None:\n            from .mdx_urlize import UrlizeExtension\n        return markdown.markdown(text, extensions=[\n            'fenced_code',\n            'codehilite(css_class=highlight)',\n            'toc',\n            'tables',\n            'sane_lists',\n            UrlizeExtension(),\n        ])", "response": "Renders the specified text using the specified markdown content and embedded styles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrender the specified cache file.", "response": "def _render_asset(self, subpath):\n        \"\"\"\n        Renders the specified cache file.\n        \"\"\"\n        return send_from_directory(\n            self.assets.cache_path, self.assets.cache_filename(subpath))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrendering the rate limit page.", "response": "def _render_rate_limit_page(self, exception=None):\n        \"\"\"\n        Renders the rate limit page.\n        \"\"\"\n        auth = request.args.get('auth')\n        is_auth = auth == '1' if auth else bool(self.auth)\n        return render_template('limit.html', is_authenticated=is_auth), 403"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_styles(self, style_urls, asset_url_path):\n        styles = []\n        for style_url in style_urls:\n            urls_inline = STYLE_ASSET_URLS_INLINE_FORMAT.format(\n                asset_url_path.rstrip('/'))\n            asset_content = self._download(style_url)\n            content = re.sub(urls_inline, self._match_asset, asset_content)\n            styles.append(content)\n\n        return styles", "response": "Gets the content of the given list of style URLs and inlines assets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _inline_styles(self):\n        styles = self._get_styles(self.assets.style_urls, url_for('asset'))\n        self.assets.styles.extend(styles)\n        self.assets.style_urls[:] = []", "response": "Adds the inline styles to the literal style list."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the style URLs from the source and caches them.", "response": "def _retrieve_styles(self):\n        \"\"\"\n        Retrieves the style URLs from the source and caches them. This\n        is called before the first request is dispatched.\n        \"\"\"\n        if self._styles_retrieved:\n            return\n        self._styles_retrieved = True\n\n        try:\n            self.assets.retrieve_styles(url_for('asset'))\n        except Exception as ex:\n            if self.debug:\n                print(format_exc(), file=sys.stderr)\n            else:\n                print(' * Error: could not retrieve styles:', ex,\n                      file=sys.stderr)\n        if self.render_inline:\n            self._inline_styles()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef default_asset_manager(self):\n        cache_path = None\n        cache_directory = self.config['CACHE_DIRECTORY']\n        if cache_directory:\n            cache_directory = cache_directory.format(version=__version__)\n            cache_path = os.path.join(self.instance_path, cache_directory)\n        return GitHubAssetManager(\n            cache_path, self.config['STYLE_URLS'], self.quiet)", "response": "Returns the default asset manager based on the current config."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the application and returns the HTML unicode that would normally appear when visiting in the browser.", "response": "def render(self, route=None):\n        \"\"\"\n        Renders the application and returns the HTML unicode that would\n        normally appear when visiting in the browser.\n        \"\"\"\n        if route is None:\n            route = '/'\n        with self.test_client() as c:\n            response = c.get(route, follow_redirects=True)\n            encoding = response.charset\n            return response.data.decode(encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nstart a local server and runs it.", "response": "def run(self, host=None, port=None, debug=None, use_reloader=None,\n            open_browser=False):\n        \"\"\"\n        Starts a server to render the README.\n        \"\"\"\n        if host is None:\n            host = self.config['HOST']\n        if port is None:\n            port = self.config['PORT']\n        if debug is None:\n            debug = self.debug\n        if use_reloader is None:\n            use_reloader = self.config['DEBUG_GRIP']\n\n        # Verify the server is not already running and start\n        with self._run_mutex:\n            if self._shutdown_event:\n                raise AlreadyRunningError()\n            self._shutdown_event = threading.Event()\n\n        # Authentication message\n        if self.auth and not self.quiet:\n            if isinstance(self.auth, tuple):\n                username, password = self.auth\n                auth_method = ('credentials: {0}'.format(username)\n                               if username\n                               else 'personal access token')\n            else:\n                auth_method = type(self.auth).__name__\n            print(' * Using', auth_method, file=sys.stderr)\n\n        # Get random port manually when needed ahead of time\n        if port == 0 and open_browser:\n            sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n            sock.bind(('localhost', 0))\n            port = sock.getsockname()[1]\n            sock.close()\n\n        # Open browser\n        browser_thread = (\n            start_browser_when_ready(host, port, self._shutdown_event)\n            if open_browser else None)\n\n        # Run local server\n        super(Grip, self).run(host, port, debug=debug,\n                              use_reloader=use_reloader,\n                              threaded=True)\n\n        # Signal to the polling and browser threads that they should exit\n        if not self.quiet:\n            print(' * Shutting down...')\n        self._shutdown_event.set()\n\n        # Wait for browser thread to finish\n        if browser_thread:\n            browser_thread.join()\n\n        # Cleanup\n        self._shutdown_event = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a Grip application with the specified overrides.", "response": "def create_app(path=None, user_content=False, context=None, username=None,\n               password=None, render_offline=False, render_wide=False,\n               render_inline=False, api_url=None, title=None, text=None,\n               autorefresh=None, quiet=None, grip_class=None):\n    \"\"\"\n    Creates a Grip application with the specified overrides.\n    \"\"\"\n    # Customize the app\n    if grip_class is None:\n        grip_class = Grip\n\n    # Customize the reader\n    if text is not None:\n        display_filename = DirectoryReader(path, True).filename_for(None)\n        source = TextReader(text, display_filename)\n    elif path == '-':\n        source = StdinReader()\n    else:\n        source = DirectoryReader(path)\n\n    # Customize the renderer\n    if render_offline:\n        renderer = OfflineRenderer(user_content, context)\n    elif user_content or context or api_url:\n        renderer = GitHubRenderer(user_content, context, api_url)\n    else:\n        renderer = None\n\n    # Optional basic auth\n    auth = (username, password) if username or password else None\n\n    # Create the customized app with default asset manager\n    return grip_class(source, auth, renderer, None, render_wide,\n                      render_inline, title, autorefresh, quiet)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts a server to render the specified file or directory containing a README.", "response": "def serve(path=None, host=None, port=None, user_content=False, context=None,\n          username=None, password=None, render_offline=False,\n          render_wide=False, render_inline=False, api_url=None, title=None,\n          autorefresh=True, browser=False, quiet=None, grip_class=None):\n    \"\"\"\n    Starts a server to render the specified file or directory containing\n    a README.\n    \"\"\"\n    app = create_app(path, user_content, context, username, password,\n                     render_offline, render_wide, render_inline, api_url,\n                     title, None, autorefresh, quiet, grip_class)\n    app.run(host, port, open_browser=browser)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_page(path=None, user_content=False, context=None,\n                username=None, password=None,\n                render_offline=False, render_wide=False, render_inline=False,\n                api_url=None, title=None, text=None, quiet=None,\n                grip_class=None):\n    \"\"\"\n    Renders the specified markup text to an HTML page and returns it.\n    \"\"\"\n    return create_app(path, user_content, context, username, password,\n                      render_offline, render_wide, render_inline, api_url,\n                      title, text, False, quiet, grip_class).render()", "response": "Renders the specified markup text to an HTML page and returns it."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_content(text, user_content=False, context=None, username=None,\n                   password=None, render_offline=False, api_url=None):\n    \"\"\"\n    Renders the specified markup and returns the result.\n    \"\"\"\n    renderer = (GitHubRenderer(user_content, context, api_url)\n                if not render_offline else\n                OfflineRenderer(user_content, context))\n    auth = (username, password) if username or password else None\n    return renderer.render(text, auth)", "response": "Renders the specified text using the specified context and username and password."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexporting the rendered HTML to a file.", "response": "def export(path=None, user_content=False, context=None,\n           username=None, password=None, render_offline=False,\n           render_wide=False, render_inline=True, out_filename=None,\n           api_url=None, title=None, quiet=False, grip_class=None):\n    \"\"\"\n    Exports the rendered HTML to a file.\n    \"\"\"\n    export_to_stdout = out_filename == '-'\n    if out_filename is None:\n        if path == '-':\n            export_to_stdout = True\n        else:\n            filetitle, _ = os.path.splitext(\n                os.path.relpath(DirectoryReader(path).root_filename))\n            out_filename = '{0}.html'.format(filetitle)\n\n    if not export_to_stdout and not quiet:\n        print('Exporting to', out_filename, file=sys.stderr)\n\n    page = render_page(path, user_content, context, username, password,\n                       render_offline, render_wide, render_inline, api_url,\n                       title, None, quiet, grip_class)\n\n    if export_to_stdout:\n        try:\n            print(page)\n        except IOError as ex:\n            if ex.errno != 0 and ex.errno != errno.EPIPE:\n                raise\n    else:\n        with io.open(out_filename, 'w', encoding='utf-8') as f:\n            f.write(page)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the HTML rendered by the GitHub API patching any inconsistencies from the main site.", "response": "def patch(html, user_content=False):\n    \"\"\"\n    Processes the HTML rendered by the GitHub API, patching\n    any inconsistencies from the main site.\n    \"\"\"\n    # FUTURE: Remove this once GitHub API renders task lists\n    # https://github.com/isaacs/github/issues/309\n    if not user_content:\n        html = INCOMPLETE_TASK_RE.sub(INCOMPLETE_TASK_SUB, html)\n        html = COMPLETE_TASK_RE.sub(COMPLETE_TASK_SUB, html)\n\n    # FUTURE: Remove this once GitHub API fixes the header bug\n    # https://github.com/joeyespo/grip/issues/244\n    html = HEADER_PATCH_RE.sub(HEADER_PATCH_SUB, html)\n\n    return html"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _find_file(self, path, silent=False):\n        for filename in DEFAULT_FILENAMES:\n            full_path = os.path.join(path, filename) if path else filename\n            if os.path.exists(full_path):\n                return full_path\n\n        # Return default filename if silent\n        if silent:\n            return os.path.join(path, DEFAULT_FILENAME)\n\n        raise ReadmeNotFoundError(path)", "response": "Find the README file at the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _resolve_readme(self, path=None, silent=False):\n        # Default to current working directory\n        if path is None:\n            path = '.'\n\n        # Normalize the path\n        path = os.path.normpath(path)\n\n        # Resolve README file if path is a directory\n        if os.path.isdir(path):\n            return self._find_file(path, silent)\n\n        # Return path if file exists or if silent\n        if silent or os.path.exists(path):\n            return path\n\n        raise ReadmeNotFoundError(path, 'File not found: ' + path)", "response": "Resolves a README file in the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize_subpath(self, subpath):\n        if subpath is None:\n            return None\n\n        # Normalize the subpath\n        subpath = posixpath.normpath(subpath)\n\n        # Add or remove trailing slash to properly support relative links\n        filename = os.path.normpath(safe_join(self.root_directory, subpath))\n        if os.path.isdir(filename):\n            subpath += '/'\n\n        return subpath", "response": "Normalizes the specified subpath."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the full path for the README file for the specified subpath.", "response": "def readme_for(self, subpath):\n        \"\"\"\n        Returns the full path for the README file for the specified\n        subpath, or the root filename if subpath is None.\n\n        Raises ReadmeNotFoundError if a README for the specified subpath\n        does not exist.\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        if subpath is None:\n            return self.root_filename\n\n        # Join for safety and to convert subpath to normalized OS-specific path\n        filename = os.path.normpath(safe_join(self.root_directory, subpath))\n\n        # Check for existence\n        if not os.path.exists(filename):\n            raise ReadmeNotFoundError(filename)\n\n        # Resolve README file if path is a directory\n        if os.path.isdir(filename):\n            return self._find_file(filename)\n\n        return filename"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns the relative filename for the specified subpath or None if subpath is None.", "response": "def filename_for(self, subpath):\n        \"\"\"\n        Returns the relative filename for the specified subpath, or the\n        root filename if subpath is None.\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        try:\n            filename = self.readme_for(subpath)\n            return os.path.relpath(filename, self.root_directory)\n        except ReadmeNotFoundError:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_binary(self, subpath=None):\n        mimetype = self.mimetype_for(subpath)\n        return mimetype is not None and mimetype.startswith('image/')", "response": "Determines whether the specified subpath is a supported binary file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the time of the last modification of the Readme or specified subpath.", "response": "def last_updated(self, subpath=None):\n        \"\"\"\n        Returns the time of the last modification of the Readme or\n        specified subpath, or None if the file does not exist.\n\n        The return value is a number giving the number of seconds since\n        the epoch (see the time module).\n\n        Raises werkzeug.exceptions.NotFound if the resulting path\n        would fall out of the root directory.\n        \"\"\"\n        try:\n            return os.path.getmtime(self.readme_for(subpath))\n        except ReadmeNotFoundError:\n            return None\n        # OSError for Python 3 base class, EnvironmentError for Python 2\n        except (OSError, EnvironmentError) as ex:\n            if ex.errno == errno.ENOENT:\n                return None\n            raise"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef read(self, subpath=None):\n        is_binary = self.is_binary(subpath)\n        filename = self.readme_for(subpath)\n        try:\n            if is_binary:\n                return self._read_binary(filename)\n            return self._read_text(filename)\n        # OSError for Python 3 base class, EnvironmentError for Python 2\n        except (OSError, EnvironmentError) as ex:\n            if ex.errno == errno.ENOENT:\n                raise ReadmeNotFoundError(filename)\n            raise", "response": "Reads the UTF - 8 content of the specified subpath."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads the contents of the file into the internal text.", "response": "def read(self, subpath=None):\n        \"\"\"\n        Returns the UTF-8 Readme content.\n\n        Raises ReadmeNotFoundError if subpath is specified since\n        subpaths are not supported for text readers.\n        \"\"\"\n        # Lazily read STDIN\n        if self.text is None and subpath is None:\n            self.text = self.read_stdin()\n\n        return super(StdinReader, self).read(subpath)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_stdin(self):\n        text = sys.stdin.read()\n\n        # Decode the bytes returned from earlier Python STDIN implementations\n        if sys.version_info[0] < 3 and text is not None:\n            text = text.decode(sys.stdin.encoding or 'utf-8')\n\n        return text", "response": "Reads STDIN until the end of input and returns a unicode string."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fit(\n        self,\n        frequency,\n        recency,\n        T,\n        weights=None,\n        iterative_fitting=1,\n        initial_params=None,\n        verbose=False,\n        tol=1e-4,\n        index=None,\n        fit_method=\"Nelder-Mead\",\n        maxiter=2000,\n        **kwargs\n    ):\n        \"\"\"\n        Pareto/NBD model fitter.\n\n        Parameters\n        ----------\n        frequency: array_like\n            the frequency vector of customers' purchases\n            (denoted x in literature).\n        recency: array_like\n            the recency vector of customers' purchases\n            (denoted t_x in literature).\n        T: array_like\n            customers' age (time units since first purchase)\n        weights: None or array_like\n            Number of customers with given frequency/recency/T,\n            defaults to 1 if not specified. Fader and\n            Hardie condense the individual RFM matrix into all\n            observed combinations of frequency/recency/T. This\n            parameter represents the count of customers with a given\n            purchase pattern. Instead of calculating individual\n            log-likelihood, the log-likelihood is calculated for each\n            pattern and multiplied by the number of customers with\n            that pattern.\n        iterative_fitting: int, optional\n            perform iterative_fitting fits over random/warm-started initial params\n        initial_params: array_like, optional\n            set the initial parameters for the fitter.\n        verbose : bool, optional\n            set to true to print out convergence diagnostics.\n        tol : float, optional\n            tolerance for termination of the function minimization process.\n        index: array_like, optional\n            index for resulted DataFrame which is accessible via self.data\n        fit_method : string, optional\n            fit_method to passing to scipy.optimize.minimize\n        maxiter : int, optional\n            max iterations for optimizer in scipy.optimize.minimize will be\n            overwritten if set in kwargs.\n        kwargs:\n            key word arguments to pass to the scipy.optimize.minimize\n            function as options dict\n\n        Returns\n        -------\n        ParetoNBDFitter\n            with additional properties like ``params_`` and methods like ``predict``\n\n        \"\"\"\n        frequency = asarray(frequency).astype(int)\n        recency = asarray(recency)\n        T = asarray(T)\n\n        if weights is None:\n            weights = np.ones(recency.shape[0], dtype=np.int64)\n        else:\n            weights = asarray(weights)\n\n        _check_inputs(frequency, recency, T)\n\n        self._scale = _scale_time(T)\n        scaled_recency = recency * self._scale\n        scaled_T = T * self._scale\n\n        params, self._negative_log_likelihood_ = self._fit(\n            (frequency, scaled_recency, scaled_T, weights, self.penalizer_coef),\n            iterative_fitting,\n            initial_params,\n            4,\n            verbose,\n            tol,\n            fit_method,\n            maxiter,\n            **kwargs\n        )\n        self._hessian_ = None\n        self.params_ = pd.Series(*(params, [\"r\", \"alpha\", \"s\", \"beta\"]))\n        self.params_[\"alpha\"] /= self._scale\n        self.params_[\"beta\"] /= self._scale\n\n        self.data = DataFrame({\"frequency\": frequency, \"recency\": recency, \"T\": T, \"weights\": weights}, index=index)\n        self.generate_new_data = lambda size=1: pareto_nbd_model(\n            T, *self._unload_params(\"r\", \"alpha\", \"s\", \"beta\"), size=size\n        )\n\n        self.predict = self.conditional_expected_number_of_purchases_up_to_time\n        return self", "response": "Fit the Pareto NBD model to the all - key words in the current language."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef conditional_expected_number_of_purchases_up_to_time(self, t, frequency, recency, T):\n        x, t_x = frequency, recency\n        params = self._unload_params(\"r\", \"alpha\", \"s\", \"beta\")\n        r, alpha, s, beta = params\n\n        likelihood = self._conditional_log_likelihood(params, x, t_x, T)\n        first_term = (\n            gammaln(r + x) - gammaln(r) + r * log(alpha) + s * log(beta) - (r + x) * log(alpha + T) - s * log(beta + T)\n        )\n        second_term = log(r + x) + log(beta + T) - log(alpha + T)\n        third_term = log((1 - ((beta + T) / (beta + T + t)) ** (s - 1)) / (s - 1))\n        return exp(first_term + second_term + third_term - likelihood)", "response": "Conditional expected number of purchases up to time t."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the conditional probability alive of a specific history entry.", "response": "def conditional_probability_alive(self, frequency, recency, T):\n        \"\"\"\n        Conditional probability alive.\n\n        Compute the probability that a customer with history\n        (frequency, recency, T) is currently alive.\n        From paper:\n        http://brucehardie.com/notes/009/pareto_nbd_derivations_2005-11-05.pdf\n\n        Parameters\n        ----------\n        frequency: float\n            historical frequency of customer.\n        recency: float\n            historical recency of customer.\n        T: float\n            age of the customer.\n\n        Returns\n        -------\n        float\n            value representing a probability\n\n        \"\"\"\n        x, t_x = frequency, recency\n        r, alpha, s, beta = self._unload_params(\"r\", \"alpha\", \"s\", \"beta\")\n        A_0 = self._log_A_0([r, alpha, s, beta], x, t_x, T)\n        return 1.0 / (1.0 + exp(log(s) - log(r + s + x) + (r + x) * log(alpha + T) + s * log(beta + T) + A_0))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the probability alive matrix for the current record set.", "response": "def conditional_probability_alive_matrix(self, max_frequency=None, max_recency=None):\n        \"\"\"\n        Compute the probability alive matrix.\n\n        Parameters\n        ----------\n        max_frequency: float, optional\n            the maximum frequency to plot. Default is max observed frequency.\n        max_recency: float, optional\n            the maximum recency to plot. This also determines the age of the\n            customer. Default to max observed age.\n\n        Returns\n        -------\n        matrix:\n            A matrix of the form [t_x: historical recency, x: historical frequency]\n\n        \"\"\"\n        max_frequency = max_frequency or int(self.data[\"frequency\"].max())\n        max_recency = max_recency or int(self.data[\"T\"].max())\n\n        Z = np.zeros((max_recency + 1, max_frequency + 1))\n        for i, recency in enumerate(np.arange(max_recency + 1)):\n            for j, frequency in enumerate(np.arange(max_frequency + 1)):\n                Z[i, j] = self.conditional_probability_alive(frequency, recency, max_recency)\n\n        return Z"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expected_number_of_purchases_up_to_time(self, t):\n        r, alpha, s, beta = self._unload_params(\"r\", \"alpha\", \"s\", \"beta\")\n        first_term = r * beta / alpha / (s - 1)\n        second_term = 1 - (beta / (beta + t)) ** (s - 1)\n        return first_term * second_term", "response": "Return the expected number of repeat purchases up to time t."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the conditional probability of n purchases up to time t.", "response": "def conditional_probability_of_n_purchases_up_to_time(self, n, t, frequency, recency, T):\n        \"\"\"\n        Return conditional probability of n purchases up to time t.\n\n        Calculate the probability of n purchases up to time t for an individual\n        with history frequency, recency and T (age).\n\n        From paper:\n        http://www.brucehardie.com/notes/028/pareto_nbd_conditional_pmf.pdf\n\n        Parameters\n        ----------\n        n: int\n            number of purchases.\n        t: a scalar\n            time up to which probability should be calculated.\n        frequency: float\n            historical frequency of customer.\n        recency: float\n            historical recency of customer.\n        T: float\n            age of the customer.\n\n        Returns\n        -------\n        array_like\n\n        \"\"\"\n        if t <= 0:\n            return 0\n\n        x, t_x = frequency, recency\n        params = self._unload_params(\"r\", \"alpha\", \"s\", \"beta\")\n        r, alpha, s, beta = params\n\n        if alpha < beta:\n            min_of_alpha_beta, max_of_alpha_beta, p, _, _ = (alpha, beta, r + x + n, r + x, r + x + 1)\n        else:\n            min_of_alpha_beta, max_of_alpha_beta, p, _, _ = (beta, alpha, s + 1, s + 1, s)\n        abs_alpha_beta = max_of_alpha_beta - min_of_alpha_beta\n\n        log_l = self._conditional_log_likelihood(params, x, t_x, T)\n        log_p_zero = (\n            gammaln(r + x)\n            + r * log(alpha)\n            + s * log(beta)\n            - (gammaln(r) + (r + x) * log(alpha + T) + s * log(beta + T) + log_l)\n        )\n        log_B_one = (\n            gammaln(r + x + n)\n            + r * log(alpha)\n            + s * log(beta)\n            - (gammaln(r) + (r + x + n) * log(alpha + T + t) + s * log(beta + T + t))\n        )\n        log_B_two = (\n            r * log(alpha)\n            + s * log(beta)\n            + gammaln(r + s + x)\n            + betaln(r + x + n, s + 1)\n            + log(hyp2f1(r + s + x, p, r + s + x + n + 1, abs_alpha_beta / (max_of_alpha_beta + T)))\n            - (gammaln(r) + gammaln(s) + (r + s + x) * log(max_of_alpha_beta + T))\n        )\n\n        def _log_B_three(i):\n            return (\n                r * log(alpha)\n                + s * log(beta)\n                + gammaln(r + s + x + i)\n                + betaln(r + x + n, s + 1)\n                + log(hyp2f1(r + s + x + i, p, r + s + x + n + 1, abs_alpha_beta / (max_of_alpha_beta + T + t)))\n                - (gammaln(r) + gammaln(s) + (r + s + x + i) * log(max_of_alpha_beta + T + t))\n            )\n\n        zeroth_term = (n == 0) * (1 - exp(log_p_zero))\n        first_term = n * log(t) - gammaln(n + 1) + log_B_one - log_l\n        second_term = log_B_two - log_l\n        third_term = logsumexp([i * log(t) - gammaln(i + 1) + _log_B_three(i) - log_l for i in range(n + 1)], axis=0)\n\n        try:\n            size = len(x)\n            sign = np.ones(size)\n        except TypeError:\n            sign = 1\n\n        # In some scenarios (e.g. large n) tiny numerical errors in the calculation of second_term and third_term\n        # cause sumexp to be ever so slightly negative and logsumexp throws an error. Hence we ignore the sign here.\n        return zeroth_term + exp(\n            logsumexp([first_term, second_term, third_term], b=[sign, sign, -sign], axis=0, return_sign=True)[0]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfitting the function for the given set of lifetimes.", "response": "def _fit(\n        self,\n        minimizing_function_args,\n        iterative_fitting,\n        initial_params,\n        params_size,\n        disp,\n        tol=1e-6,\n        fit_method=\"Nelder-Mead\",\n        maxiter=2000,\n        **kwargs\n    ):\n        \"\"\"Fit function for fitters.\"\"\"\n        ll = []\n        sols = []\n\n        if iterative_fitting <= 0:\n            raise ValueError(\"iterative_fitting parameter should be greater than 0 as of lifetimes v0.2.1\")\n\n        if iterative_fitting > 1 and initial_params is not None:\n            raise ValueError(\n                \"iterative_fitting and initial_params should not be both set, as no improvement could be made.\"\n            )\n\n        # set options for minimize, if specified in kwargs will be overwritten\n        minimize_options = {}\n        minimize_options[\"disp\"] = disp\n        minimize_options[\"maxiter\"] = maxiter\n        minimize_options.update(kwargs)\n\n        total_count = 0\n        while total_count < iterative_fitting:\n            current_init_params = (\n                np.random.normal(1.0, scale=0.05, size=params_size) if initial_params is None else initial_params\n            )\n            if minimize_options[\"disp\"]:\n                print(\"Optimize function with {}\".format(fit_method))\n\n            output = minimize(\n                self._negative_log_likelihood,\n                method=fit_method,\n                tol=tol,\n                x0=current_init_params,\n                args=minimizing_function_args,\n                options=minimize_options,\n            )\n            sols.append(output.x)\n            ll.append(output.fun)\n\n            total_count += 1\n        argmin_ll, min_ll = min(enumerate(ll), key=lambda x: x[1])\n        minimizing_params = sols[argmin_ll]\n        return minimizing_params, min_ll"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving model with dill package.", "response": "def save_model(self, path, save_data=True, save_generate_data_method=True, values_to_save=None):\n        \"\"\"\n        Save model with dill package.\n\n        Parameters\n        ----------\n        path: str\n            Path where to save model.\n        save_data: bool, optional\n            Whether to save data from fitter.data to pickle object\n        save_generate_data_method: bool, optional\n            Whether to save generate_new_data method (if it exists) from\n            fitter.generate_new_data to pickle object.\n        values_to_save: list, optional\n            Placeholders for original attributes for saving object. If None\n            will be extended to attr_list length like [None] * len(attr_list)\n\n        \"\"\"\n        attr_list = [\"data\" * (not save_data), \"generate_new_data\" * (not save_generate_data_method)]\n        _save_obj_without_attr(self, attr_list, path, values_to_save=values_to_save)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload the object from a file.", "response": "def load_model(self, path):\n        \"\"\"\n        Load model with dill package.\n\n        Parameters\n        ----------\n        path: str\n            From what path load model.\n\n        \"\"\"\n        with open(path, \"rb\") as in_file:\n            self.__dict__.update(dill.load(in_file).__dict__)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(\n        self, frequency, recency, T, weights=None, initial_params=None, verbose=False, tol=1e-7, index=None, **kwargs\n    ):\n        \"\"\"\n        Fit the data to the MBG/NBD model.\n\n        Parameters\n        ----------\n        frequency: array_like\n            the frequency vector of customers' purchases\n            (denoted x in literature).\n        recency: array_like\n            the recency vector of customers' purchases\n            (denoted t_x in literature).\n        T: array_like\n            customers' age (time units since first purchase)\n        weights: None or array_like\n            Number of customers with given frequency/recency/T,\n            defaults to 1 if not specified. Fader and\n            Hardie condense the individual RFM matrix into all\n            observed combinations of frequency/recency/T. This\n            parameter represents the count of customers with a given\n            purchase pattern. Instead of calculating individual\n            log-likelihood, the log-likelihood is calculated for each\n            pattern and multiplied by the number of customers with\n            that pattern.\n        verbose : bool, optional\n            set to true to print out convergence diagnostics.\n        tol : float, optional\n            tolerance for termination of the function minimization process.\n        index: array_like, optional\n            index for resulted DataFrame which is accessible via self.data\n        kwargs:\n            key word arguments to pass to the scipy.optimize.minimize\n            function as options dict\n\n        Returns\n        -------\n        ModifiedBetaGeoFitter:\n            With additional properties and methods like ``params_`` and ``predict``\n\n        \"\"\"\n        # although the parent method is called, this class's\n        # _negative_log_likelihood is referenced\n        super(ModifiedBetaGeoFitter, self).fit(\n            frequency, recency, T, weights, initial_params, verbose, tol, index=index, **kwargs\n        )\n        # this needs to be reassigned from the parent method\n        self.generate_new_data = lambda size=1: modified_beta_geometric_nbd_model(\n            T, *self._unload_params(\"r\", \"alpha\", \"a\", \"b\"), size=size\n        )\n\n        self.variance_matrix_ = self._compute_variance_matrix()\n        self.standard_errors_ = self._compute_standard_errors()\n        self.confidence_intervals_ = self._compute_confidence_intervals()\n        return self", "response": "Fit the data to the MBG NBD model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the expected number of repeat purchases up to time t.", "response": "def expected_number_of_purchases_up_to_time(self, t):\n        \"\"\"\n        Return expected number of repeat purchases up to time t.\n\n        Calculate the expected number of repeat purchases up to time t for a\n        randomly choose individual from the population.\n\n        Parameters\n        ----------\n        t: array_like\n            times to calculate the expectation for\n\n        Returns\n        -------\n        array_like\n\n        \"\"\"\n        r, alpha, a, b = self._unload_params(\"r\", \"alpha\", \"a\", \"b\")\n        hyp = hyp2f1(r, b + 1, a + b, t / (alpha + t))\n        return b / (a - 1) * (1 - hyp * (alpha / (alpha + t)) ** r)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncomputes the conditional probability alive for a given set of conditions.", "response": "def conditional_probability_alive(self, frequency, recency, T):\n        \"\"\"\n        Conditional probability alive.\n\n        Compute the probability that a customer with history (frequency,\n        recency, T) is currently alive.\n        From https://www.researchgate.net/publication/247219660_Empirical_validation_and_comparison_of_models_for_customer_base_analysis\n        Appendix A, eq. (5)\n\n        Parameters\n        ----------\n        frequency: array or float\n            historical frequency of customer.\n        recency: array or float\n            historical recency of customer.\n        T: array or float\n            age of the customer.\n\n        Returns\n        -------\n        array:\n            value representing probability of being alive\n\n        \"\"\"\n        r, alpha, a, b = self._unload_params(\"r\", \"alpha\", \"a\", \"b\")\n        return np.atleast_1d(1.0 / (1 + (a / (b + frequency)) * ((alpha + T) / (alpha + recency)) ** (r + frequency)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(\n        self, frequency, recency, T, weights=None, initial_params=None, verbose=False, tol=1e-7, index=None, **kwargs\n    ):\n        \"\"\"\n        Fit a dataset to the BG/NBD model.\n\n        Parameters\n        ----------\n        frequency: array_like\n            the frequency vector of customers' purchases\n            (denoted x in literature).\n        recency: array_like\n            the recency vector of customers' purchases\n            (denoted t_x in literature).\n        T: array_like\n            customers' age (time units since first purchase)\n        weights: None or array_like\n            Number of customers with given frequency/recency/T,\n            defaults to 1 if not specified. Fader and\n            Hardie condense the individual RFM matrix into all\n            observed combinations of frequency/recency/T. This\n            parameter represents the count of customers with a given\n            purchase pattern. Instead of calculating individual\n            loglikelihood, the loglikelihood is calculated for each\n            pattern and multiplied by the number of customers with\n            that pattern.\n        initial_params: array_like, optional\n            set the initial parameters for the fitter.\n        verbose : bool, optional\n            set to true to print out convergence diagnostics.\n        tol : float, optional\n            tolerance for termination of the function minimization process.\n        index: array_like, optional\n            index for resulted DataFrame which is accessible via self.data\n        kwargs:\n            key word arguments to pass to the scipy.optimize.minimize\n            function as options dict\n\n\n        Returns\n        -------\n        BetaGeoFitter\n            with additional properties like ``params_`` and methods like ``predict``\n\n        \"\"\"\n        frequency = np.asarray(frequency).astype(int)\n        recency = np.asarray(recency)\n        T = np.asarray(T)\n        _check_inputs(frequency, recency, T)\n\n        if weights is None:\n            weights = np.ones_like(recency, dtype=int)\n        else:\n            weights = np.asarray(weights)\n\n        self._scale = _scale_time(T)\n        scaled_recency = recency * self._scale\n        scaled_T = T * self._scale\n\n        log_params_, self._negative_log_likelihood_, self._hessian_ = self._fit(\n            (frequency, scaled_recency, scaled_T, weights, self.penalizer_coef),\n            initial_params,\n            4,\n            verbose,\n            tol,\n            **kwargs\n        )\n\n        self.params_ = pd.Series(np.exp(log_params_), index=[\"r\", \"alpha\", \"a\", \"b\"])\n        self.params_[\"alpha\"] /= self._scale\n\n        self.data = pd.DataFrame({\"frequency\": frequency, \"recency\": recency, \"T\": T, \"weights\": weights}, index=index)\n\n        self.generate_new_data = lambda size=1: beta_geometric_nbd_model(\n            T, *self._unload_params(\"r\", \"alpha\", \"a\", \"b\"), size=size\n        )\n\n        self.predict = self.conditional_expected_number_of_purchases_up_to_time\n\n        self.variance_matrix_ = self._compute_variance_matrix()\n        self.standard_errors_ = self._compute_standard_errors()\n        self.confidence_intervals_ = self._compute_confidence_intervals()\n        return self", "response": "Fit a BetaGeoFitter to the NBD model."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes the conditional probability alive for the given frequency and recency.", "response": "def conditional_probability_alive(self, frequency, recency, T):\n        \"\"\"\n        Compute conditional probability alive.\n\n        Compute the probability that a customer with history\n        (frequency, recency, T) is currently alive.\n\n        From http://www.brucehardie.com/notes/021/palive_for_BGNBD.pdf\n\n        Parameters\n        ----------\n        frequency: array or scalar\n            historical frequency of customer.\n        recency: array or scalar\n            historical recency of customer.\n        T: array or scalar\n            age of the customer.\n\n        Returns\n        -------\n        array\n            value representing a probability\n\n        \"\"\"\n        r, alpha, a, b = self._unload_params(\"r\", \"alpha\", \"a\", \"b\")\n\n        log_div = (r + frequency) * np.log((alpha + T) / (alpha + recency)) + np.log(\n            a / (b + np.maximum(frequency, 1) - 1)\n        )\n        return np.atleast_1d(np.where(frequency == 0, 1.0, expit(-log_div)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the probability alive matrix for the current entry.", "response": "def conditional_probability_alive_matrix(self, max_frequency=None, max_recency=None):\n        \"\"\"\n        Compute the probability alive matrix.\n\n        Parameters\n        ----------\n        max_frequency: float, optional\n            the maximum frequency to plot. Default is max observed frequency.\n        max_recency: float, optional\n            the maximum recency to plot. This also determines the age of the\n            customer. Default to max observed age.\n\n        Returns\n        -------\n        matrix:\n            A matrix of the form [t_x: historical recency, x: historical frequency]\n\n        \"\"\"\n        max_frequency = max_frequency or int(self.data[\"frequency\"].max())\n        max_recency = max_recency or int(self.data[\"T\"].max())\n\n        return np.fromfunction(\n            self.conditional_probability_alive, (max_frequency + 1, max_recency + 1), T=max_recency\n        ).T"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef calibration_and_holdout_data(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    calibration_period_end,\n    observation_period_end=None,\n    freq=\"D\",\n    datetime_format=None,\n    monetary_value_col=None,\n):\n    \"\"\"\n    Create a summary of each customer over a calibration and holdout period.\n\n    This function creates a summary of each customer over a calibration and\n    holdout period (training and testing, respectively).\n    It accepts transaction data, and returns a DataFrame of sufficient statistics.\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    calibration_period_end: :obj: datetime\n        a period to limit the calibration to, inclusive.\n    observation_period_end: :obj: datetime, optional\n         a string or datetime to denote the final date of the study.\n         Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    freq: string, optional\n        Default 'D' for days. Other examples: 'W' for weekly.\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    monetary_value_col: string, optional\n        the column in transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n\n    Returns\n    -------\n    :obj: DataFrame\n        A dataframe with columns frequency_cal, recency_cal, T_cal, frequency_holdout, duration_holdout\n        If monetary_value_col isn't None, the dataframe will also have the columns monetary_value_cal and\n        monetary_value_holdout.\n\n    \"\"\"\n\n    def to_period(d):\n        return d.to_period(freq)\n\n    if observation_period_end is None:\n        observation_period_end = transactions[datetime_col].max()\n\n    transaction_cols = [customer_id_col, datetime_col]\n    if monetary_value_col:\n        transaction_cols.append(monetary_value_col)\n    transactions = transactions[transaction_cols].copy()\n\n    transactions[datetime_col] = pd.to_datetime(transactions[datetime_col], format=datetime_format)\n    observation_period_end = pd.to_datetime(observation_period_end, format=datetime_format)\n    calibration_period_end = pd.to_datetime(calibration_period_end, format=datetime_format)\n\n    # create calibration dataset\n    calibration_transactions = transactions.loc[transactions[datetime_col] <= calibration_period_end]\n    calibration_summary_data = summary_data_from_transaction_data(\n        calibration_transactions,\n        customer_id_col,\n        datetime_col,\n        datetime_format=datetime_format,\n        observation_period_end=calibration_period_end,\n        freq=freq,\n        monetary_value_col=monetary_value_col,\n    )\n    calibration_summary_data.columns = [c + \"_cal\" for c in calibration_summary_data.columns]\n\n    # create holdout dataset\n    holdout_transactions = transactions.loc[\n        (observation_period_end >= transactions[datetime_col]) & (transactions[datetime_col] > calibration_period_end)\n    ]\n\n    if holdout_transactions.empty:\n        raise ValueError(\n            \"There is no data available. Check the `observation_period_end` and  `calibration_period_end` and confirm that values in `transactions` occur prior to those dates.\"\n        )\n\n    holdout_transactions[datetime_col] = holdout_transactions[datetime_col].map(to_period)\n    holdout_summary_data = (\n        holdout_transactions.groupby([customer_id_col, datetime_col], sort=False)\n        .agg(lambda r: 1)\n        .groupby(level=customer_id_col)\n        .agg([\"count\"])\n    )\n    holdout_summary_data.columns = [\"frequency_holdout\"]\n    if monetary_value_col:\n        holdout_summary_data[\"monetary_value_holdout\"] = holdout_transactions.groupby(customer_id_col)[\n            monetary_value_col\n        ].mean()\n\n    combined_data = calibration_summary_data.join(holdout_summary_data, how=\"left\")\n    combined_data.fillna(0, inplace=True)\n\n    delta_time = (to_period(observation_period_end) - to_period(calibration_period_end)).n\n    combined_data[\"duration_holdout\"] = delta_time\n\n    return combined_data", "response": "This function creates a summary of each customer over a calibration and holdout period."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_first_transactions(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    monetary_value_col=None,\n    datetime_format=None,\n    observation_period_end=None,\n    freq=\"D\",\n):\n    \"\"\"\n    Return dataframe with first transactions.\n\n    This takes a DataFrame of transaction data of the form:\n        customer_id, datetime [, monetary_value]\n    and appends a column named 'repeated' to the transaction log which indicates which rows\n    are repeated transactions for that customer_id.\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    monetary_value_col: string, optional\n        the column in transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n    observation_period_end: :obj: datetime\n        a string or datetime to denote the final date of the study.\n        Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    freq: string, optional\n        Default 'D' for days, 'W' for weeks, 'M' for months... etc. Full list here:\n        http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects\n\n    \"\"\"\n    if observation_period_end is None:\n        observation_period_end = transactions[datetime_col].max()\n\n    if type(observation_period_end) == pd.Period:\n        observation_period_end = observation_period_end.to_timestamp()\n\n    select_columns = [customer_id_col, datetime_col]\n\n    if monetary_value_col:\n        select_columns.append(monetary_value_col)\n\n    transactions = transactions[select_columns].sort_values(select_columns).copy()\n\n    # make sure the date column uses datetime objects, and use Pandas' DateTimeIndex.to_period()\n    # to convert the column to a PeriodIndex which is useful for time-wise grouping and truncating\n    transactions[datetime_col] = pd.to_datetime(transactions[datetime_col], format=datetime_format)\n    transactions = transactions.set_index(datetime_col).to_period(freq).to_timestamp()\n\n    transactions = transactions.loc[(transactions.index <= observation_period_end)].reset_index()\n\n    period_groupby = transactions.groupby([datetime_col, customer_id_col], sort=False, as_index=False)\n\n    if monetary_value_col:\n        # when we have a monetary column, make sure to sum together any values in the same period\n        period_transactions = period_groupby.sum()\n    else:\n        # by calling head() on the groupby object, the datetime_col and customer_id_col columns\n        # will be reduced\n        period_transactions = period_groupby.head(1)\n\n    # initialize a new column where we will indicate which are the first transactions\n    period_transactions[\"first\"] = False\n    # find all of the initial transactions and store as an index\n    first_transactions = period_transactions.groupby(customer_id_col, sort=True, as_index=False).head(1).index\n    # mark the initial transactions as True\n    period_transactions.loc[first_transactions, \"first\"] = True\n    select_columns.append(\"first\")\n    # reset datetime_col to period\n    period_transactions[datetime_col] = pd.Index(period_transactions[datetime_col]).to_period(freq)\n\n    return period_transactions[select_columns]", "response": "Returns a dataframe with first transactions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef summary_data_from_transaction_data(\n    transactions,\n    customer_id_col,\n    datetime_col,\n    monetary_value_col=None,\n    datetime_format=None,\n    observation_period_end=None,\n    freq=\"D\",\n    freq_multiplier=1,\n):\n    \"\"\"\n    Return summary data from transactions.\n\n    This transforms a DataFrame of transaction data of the form:\n        customer_id, datetime [, monetary_value]\n    to a DataFrame of the form:\n        customer_id, frequency, recency, T [, monetary_value]\n\n    Parameters\n    ----------\n    transactions: :obj: DataFrame\n        a Pandas DataFrame that contains the customer_id col and the datetime col.\n    customer_id_col: string\n        the column in transactions DataFrame that denotes the customer_id\n    datetime_col:  string\n        the column in transactions that denotes the datetime the purchase was made.\n    monetary_value_col: string, optional\n        the columns in the transactions that denotes the monetary value of the transaction.\n        Optional, only needed for customer lifetime value estimation models.\n    observation_period_end: datetime, optional\n         a string or datetime to denote the final date of the study.\n         Events after this date are truncated. If not given, defaults to the max 'datetime_col'.\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't understand\n        the provided format.\n    freq: string, optional\n        Default 'D' for days, 'W' for weeks, 'M' for months... etc. Full list here:\n        http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects\n    freq_multiplier: int, optional\n        Default 1, could be use to get exact recency and T, i.e. with freq='W'\n        row for user id_sample=1 will be recency=30 and T=39 while data in\n        CDNOW summary are different. Exact values could be obtained with\n        freq='D' and freq_multiplier=7 which will lead to recency=30.43\n        and T=38.86\n\n    Returns\n    -------\n    :obj: DataFrame:\n        customer_id, frequency, recency, T [, monetary_value]\n\n    \"\"\"\n    if observation_period_end is None:\n        observation_period_end = (\n            pd.to_datetime(transactions[datetime_col].max(), format=datetime_format).to_period(freq).to_timestamp()\n        )\n    else:\n        observation_period_end = (\n            pd.to_datetime(observation_period_end, format=datetime_format).to_period(freq).to_timestamp()\n        )\n\n    # label all of the repeated transactions\n    repeated_transactions = _find_first_transactions(\n        transactions, customer_id_col, datetime_col, monetary_value_col, datetime_format, observation_period_end, freq\n    )\n    # reset datetime_col to timestamp\n    repeated_transactions[datetime_col] = pd.Index(repeated_transactions[datetime_col]).to_timestamp()\n\n    # count all orders by customer.\n    customers = repeated_transactions.groupby(customer_id_col, sort=False)[datetime_col].agg([\"min\", \"max\", \"count\"])\n\n    # subtract 1 from count, as we ignore their first order.\n    customers[\"frequency\"] = customers[\"count\"] - 1\n\n    customers[\"T\"] = (observation_period_end - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n    customers[\"recency\"] = (customers[\"max\"] - customers[\"min\"]) / np.timedelta64(1, freq) / freq_multiplier\n\n    summary_columns = [\"frequency\", \"recency\", \"T\"]\n\n    if monetary_value_col:\n        # create an index of all the first purchases\n        first_purchases = repeated_transactions[repeated_transactions[\"first\"]].index\n        # by setting the monetary_value cells of all the first purchases to NaN,\n        # those values will be excluded from the mean value calculation\n        repeated_transactions.loc[first_purchases, monetary_value_col] = np.nan\n        customers[\"monetary_value\"] = (\n            repeated_transactions.groupby(customer_id_col)[monetary_value_col].mean().fillna(0)\n        )\n        summary_columns.append(\"monetary_value\")\n\n    return customers[summary_columns].astype(float)", "response": "Transforms a DataFrame of transaction data into a summary data."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the alive path for plotting alive history of the user.", "response": "def calculate_alive_path(model, transactions, datetime_col, t, freq=\"D\"):\n    \"\"\"\n    Calculate alive path for plotting alive history of user.\n\n    Parameters\n    ----------\n    model:\n        A fitted lifetimes model\n    transactions: DataFrame\n        a Pandas DataFrame containing the transactions history of the customer_id\n    datetime_col: string\n        the column in the transactions that denotes the datetime the purchase was made\n    t: array_like\n        the number of time units since the birth for which we want to draw the p_alive\n    freq: string\n        Default 'D' for days. Other examples= 'W' for weekly\n\n    Returns\n    -------\n    :obj: Series\n        A pandas Series containing the p_alive as a function of T (age of the customer)\n\n    \"\"\"\n    customer_history = transactions[[datetime_col]].copy()\n    customer_history[datetime_col] = pd.to_datetime(customer_history[datetime_col])\n    customer_history = customer_history.set_index(datetime_col)\n    # Add transactions column\n    customer_history[\"transactions\"] = 1\n\n    # for some reason fillna(0) not working for resample in pandas with python 3.x,\n    # changed to replace\n    purchase_history = customer_history.resample(freq).sum().replace(np.nan, 0)[\"transactions\"].values\n\n    extra_columns = t + 1 - len(purchase_history)\n    customer_history = pd.DataFrame(np.append(purchase_history, [0] * extra_columns), columns=[\"transactions\"])\n    # add T column\n    customer_history[\"T\"] = np.arange(customer_history.shape[0])\n    # add cumulative transactions column\n    customer_history[\"transactions\"] = customer_history[\"transactions\"].apply(lambda t: int(t > 0))\n    customer_history[\"frequency\"] = customer_history[\"transactions\"].cumsum() - 1  # first purchase is ignored\n    # Add t_x column\n    customer_history[\"recency\"] = customer_history.apply(\n        lambda row: row[\"T\"] if row[\"transactions\"] != 0 else np.nan, axis=1\n    )\n    customer_history[\"recency\"] = customer_history[\"recency\"].fillna(method=\"ffill\").fillna(0)\n\n    return customer_history.apply(\n        lambda row: model.conditional_probability_alive(row[\"frequency\"], row[\"recency\"], row[\"T\"]), axis=1\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_inputs(frequency, recency=None, T=None, monetary_value=None):\n    if recency is not None:\n        if T is not None and np.any(recency > T):\n            raise ValueError(\"Some values in recency vector are larger than T vector.\")\n        if np.any(recency[frequency == 0] != 0):\n            raise ValueError(\"There exist non-zero recency values when frequency is zero.\")\n        if np.any(recency < 0):\n            raise ValueError(\"There exist negative recency (ex: last order set before first order)\")\n        if any(x.shape[0] == 0 for x in [recency, frequency, T]):\n            raise ValueError(\"There exists a zero length vector in one of frequency, recency or T.\")\n    if np.sum((frequency - frequency.astype(int)) ** 2) != 0:\n        raise ValueError(\"There exist non-integer values in the frequency vector.\")\n    if monetary_value is not None and np.any(monetary_value <= 0):\n        raise ValueError(\"There exist non-positive values in the monetary_value vector.\")", "response": "Checks validity of inputs."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncompute the average lifetime value for a group of customers.", "response": "def _customer_lifetime_value(\n    transaction_prediction_model, frequency, recency, T, monetary_value, time=12, discount_rate=0.01, freq=\"D\"\n):\n    \"\"\"\n    Compute the average lifetime value for a group of one or more customers.\n\n    This method computes the average lifetime value for a group of one or more customers.\n\n    Parameters\n    ----------\n    transaction_prediction_model:\n        the model to predict future transactions\n    frequency: array_like\n        the frequency vector of customers' purchases (denoted x in literature).\n    recency: array_like\n        the recency vector of customers' purchases (denoted t_x in literature).\n    T: array_like\n        the vector of customers' age (time since first purchase)\n    monetary_value: array_like\n        the monetary value vector of customer's purchases (denoted m in literature).\n    time: int, optional\n        the lifetime expected for the user in months. Default: 12\n    discount_rate: float, optional\n        the monthly adjusted discount rate. Default: 1\n\n    Returns\n    -------\n    :obj: Series\n        series with customer ids as index and the estimated customer lifetime values as values\n\n    \"\"\"\n    df = pd.DataFrame(index=frequency.index)\n    df[\"clv\"] = 0  # initialize the clv column to zeros\n\n    steps = np.arange(1, time + 1)\n    factor = {\"W\": 4.345, \"M\": 1.0, \"D\": 30, \"H\": 30 * 24}[freq]\n\n    for i in steps * factor:\n        # since the prediction of number of transactions is cumulative, we have to subtract off the previous periods\n        expected_number_of_transactions = transaction_prediction_model.predict(\n            i, frequency, recency, T\n        ) - transaction_prediction_model.predict(i - factor, frequency, recency, T)\n        # sum up the CLV estimates of all of the periods\n        df[\"clv\"] += (monetary_value * expected_number_of_transactions) / (1 + discount_rate) ** (i / factor)\n\n    return df[\"clv\"]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef expected_cumulative_transactions(\n    model,\n    transactions,\n    datetime_col,\n    customer_id_col,\n    t,\n    datetime_format=None,\n    freq=\"D\",\n    set_index_date=False,\n    freq_multiplier=1,\n):\n    \"\"\"\n    Get expected and actual repeated cumulative transactions.\n\n    Parameters\n    ----------\n    model:\n        A fitted lifetimes model\n    transactions: :obj: DataFrame\n        a Pandas DataFrame containing the transactions history of the customer_id\n    datetime_col: string\n        the column in transactions that denotes the datetime the purchase was made.\n    customer_id_col: string\n        the column in transactions that denotes the customer_id\n    t: int\n        the number of time units since the begining of\n        data for which we want to calculate cumulative transactions\n    datetime_format: string, optional\n        a string that represents the timestamp format. Useful if Pandas can't\n        understand the provided format.\n    freq: string, optional\n        Default 'D' for days, 'W' for weeks, 'M' for months... etc. Full list here:\n        http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects\n    set_index_date: bool, optional\n        when True set date as Pandas DataFrame index, default False - number of time units\n    freq_multiplier: int, optional\n        Default 1, could be use to get exact cumulative transactions predicted\n        by model, i.e. model trained with freq='W', passed freq to\n        expected_cumulative_transactions is freq='D', and freq_multiplier=7.\n\n    Returns\n    -------\n    :obj: DataFrame\n        A dataframe with columns actual, predicted\n\n    \"\"\"\n    start_date = pd.to_datetime(transactions[datetime_col], format=datetime_format).min()\n    start_period = start_date.to_period(freq)\n    observation_period_end = start_period + t\n\n    repeated_and_first_transactions = _find_first_transactions(\n        transactions,\n        customer_id_col,\n        datetime_col,\n        datetime_format=datetime_format,\n        observation_period_end=observation_period_end,\n        freq=freq,\n    )\n\n    first_trans_mask = repeated_and_first_transactions[\"first\"]\n    repeated_transactions = repeated_and_first_transactions[~first_trans_mask]\n    first_transactions = repeated_and_first_transactions[first_trans_mask]\n\n    date_range = pd.date_range(start_date, periods=t + 1, freq=freq)\n    date_periods = date_range.to_period(freq)\n\n    pred_cum_transactions = []\n    first_trans_size = first_transactions.groupby(datetime_col).size()\n    for i, period in enumerate(date_periods):\n        if i % freq_multiplier == 0 and i > 0:\n            times = np.array([d.n for d in period - first_trans_size.index])\n            times = times[times > 0].astype(float) / freq_multiplier\n            expected_trans_agg = model.expected_number_of_purchases_up_to_time(times)\n\n            mask = first_trans_size.index < period\n            expected_trans = sum(expected_trans_agg * first_trans_size[mask])\n            pred_cum_transactions.append(expected_trans)\n\n    act_trans = repeated_transactions.groupby(datetime_col).size()\n    act_tracking_transactions = act_trans.reindex(date_periods, fill_value=0)\n\n    act_cum_transactions = []\n    for j in range(1, t // freq_multiplier + 1):\n        sum_trans = sum(act_tracking_transactions.iloc[: j * freq_multiplier])\n        act_cum_transactions.append(sum_trans)\n\n    if set_index_date:\n        index = date_periods[freq_multiplier - 1 : -1 : freq_multiplier]\n    else:\n        index = range(0, t // freq_multiplier)\n\n    df_cum_transactions = pd.DataFrame(\n        {\"actual\": act_cum_transactions, \"predicted\": pred_cum_transactions}, index=index\n    )\n\n    return df_cum_transactions", "response": "Returns a DataFrame containing the expected and actual cumulative transactions for the given customer_id_col."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves object with attributes from attr_list to path.", "response": "def _save_obj_without_attr(obj, attr_list, path, values_to_save=None):\n    \"\"\"\n    Save object with attributes from attr_list.\n\n    Parameters\n    ----------\n    obj: obj\n        Object of class with __dict__ attribute.\n    attr_list: list\n        List with attributes to exclude from saving to dill object. If empty\n        list all attributes will be saved.\n    path: str\n        Where to save dill object.\n    values_to_save: list, optional\n        Placeholders for original attributes for saving object. If None will be\n        extended to attr_list length like [None] * len(attr_list)\n\n    \"\"\"\n    if values_to_save is None:\n        values_to_save = [None] * len(attr_list)\n\n    saved_attr_dict = {}\n    for attr, val_save in zip(attr_list, values_to_save):\n        if attr in obj.__dict__:\n            item = obj.__dict__.pop(attr)\n            saved_attr_dict[attr] = item\n            setattr(obj, attr, val_save)\n\n    with open(path, \"wb\") as out_file:\n        dill.dump(obj, out_file)\n\n    for attr, item in saved_attr_dict.items():\n        setattr(obj, attr, item)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlogs likelihood for optimizer.", "response": "def _loglikelihood(params, x, tx, T):\n        warnings.simplefilter(action=\"ignore\", category=FutureWarning)\n\n        \"\"\"Log likelihood for optimizer.\"\"\"\n        alpha, beta, gamma, delta = params\n\n        betaln_ab = betaln(alpha, beta)\n        betaln_gd = betaln(gamma, delta)\n\n        A = betaln(alpha + x, beta + T - x) - betaln_ab + betaln(gamma, delta + T) - betaln_gd\n\n        B = 1e-15 * np.ones_like(T)\n        recency_T = T - tx - 1\n\n        for j in np.arange(recency_T.max() + 1):\n            ix = recency_T >= j\n            B = B + ix * betaf(alpha + x, beta + tx - x + j) * betaf(gamma + 1, delta + tx + j)\n\n        B = log(B) - betaln_gd - betaln_ab\n        return logaddexp(A, B)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfitting the Beta - BetaBinomFitter model to the all - recent - related items of the given frequency recency n_periods and weights and returns the fitted and estimated values.", "response": "def fit(\n        self,\n        frequency,\n        recency,\n        n_periods,\n        weights=None,\n        initial_params=None,\n        verbose=False,\n        tol=1e-7,\n        index=None,\n        **kwargs\n    ):\n        \"\"\"\n        Fit the BG/BB model.\n\n        Parameters\n        ----------\n        frequency: array_like\n            Total periods with observed transactions\n        recency: array_like\n            Period of most recent transaction\n        n_periods: array_like\n            Number of transaction opportunities. Previously called `n`.\n        weights: None or array_like\n            Number of customers with given frequency/recency/T,\n            defaults to 1 if not specified. Fader and\n            Hardie condense the individual RFM matrix into all\n            observed combinations of frequency/recency/T. This\n            parameter represents the count of customers with a given\n            purchase pattern. Instead of calculating individual\n            log-likelihood, the log-likelihood is calculated for each\n            pattern and multiplied by the number of customers with\n            that pattern.  Previously called `n_custs`.\n        verbose: boolean, optional\n            Set to true to print out convergence diagnostics.\n        tol: float, optional\n            Tolerance for termination of the function minimization process.\n        index: array_like, optional\n            Index for resulted DataFrame which is accessible via self.data\n        kwargs:\n            Key word arguments to pass to the scipy.optimize.minimize\n            function as options dict\n\n        Returns\n        -------\n        BetaGeoBetaBinomFitter\n            fitted and with parameters estimated\n\n        \"\"\"\n        frequency = np.asarray(frequency).astype(int)\n        recency = np.asarray(recency).astype(int)\n        n_periods = np.asarray(n_periods).astype(int)\n\n        if weights is None:\n            weights = np.ones_like(recency)\n        else:\n            weights = np.asarray(weights)\n\n        _check_inputs(frequency, recency, n_periods)\n\n        log_params_, self._negative_log_likelihood_, self._hessian_ = self._fit(\n            (frequency, recency, n_periods, weights, self.penalizer_coef), initial_params, 4, verbose, tol, **kwargs\n        )\n        self.params_ = pd.Series(np.exp(log_params_), index=[\"alpha\", \"beta\", \"gamma\", \"delta\"])\n\n        self.data = DataFrame(\n            {\"frequency\": frequency, \"recency\": recency, \"n_periods\": n_periods, \"weights\": weights}, index=index\n        )\n\n        self.generate_new_data = lambda size=1: beta_geometric_beta_binom_model(\n            # Making a large array replicating n by n_custs having n.\n            np.array(sum([n_] * n_cust for (n_, n_cust) in zip(n_periods, weights))),\n            *self._unload_params(\"alpha\", \"beta\", \"gamma\", \"delta\"),\n            size=size\n        )\n\n        self.variance_matrix_ = self._compute_variance_matrix()\n        self.standard_errors_ = self._compute_standard_errors()\n        self.confidence_intervals_ = self._compute_confidence_intervals()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef conditional_probability_alive(self, m_periods_in_future, frequency, recency, n_periods):\n        params = self._unload_params(\"alpha\", \"beta\", \"gamma\", \"delta\")\n        alpha, beta, gamma, delta = params\n\n        p1 = betaln(alpha + frequency, beta + n_periods - frequency) - betaln(alpha, beta)\n        p2 = betaln(gamma, delta + n_periods + m_periods_in_future) - betaln(gamma, delta)\n        p3 = self._loglikelihood(params, frequency, recency, n_periods)\n\n        return exp(p1 + p2) / exp(p3)", "response": "Conditional probability alive at transaction opportunity."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_period_transactions(\n    model,\n    max_frequency=7,\n    title=\"Frequency of Repeat Transactions\",\n    xlabel=\"Number of Calibration Period Transactions\",\n    ylabel=\"Customers\",\n    **kwargs\n):\n    \"\"\"\n    Plot a figure with period actual and predicted transactions.\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model.\n    max_frequency: int, optional\n        The maximum frequency to plot.\n    title: str, optional\n        Figure title\n    xlabel: str, optional\n        Figure xlabel\n    ylabel: str, optional\n        Figure ylabel\n    kwargs\n        Passed into the matplotlib.pyplot.plot command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    labels = kwargs.pop(\"label\", [\"Actual\", \"Model\"])\n\n    n = model.data.shape[0]\n    simulated_data = model.generate_new_data(size=n)\n\n    model_counts = pd.DataFrame(model.data[\"frequency\"].value_counts().sort_index().iloc[:max_frequency])\n    simulated_counts = pd.DataFrame(simulated_data[\"frequency\"].value_counts().sort_index().iloc[:max_frequency])\n    combined_counts = model_counts.merge(simulated_counts, how=\"outer\", left_index=True, right_index=True).fillna(0)\n    combined_counts.columns = labels\n\n    ax = combined_counts.plot(kind=\"bar\", **kwargs)\n\n    plt.legend()\n    plt.title(title)\n    plt.ylabel(ylabel)\n    plt.xlabel(xlabel)\n    return ax", "response": "Plots actual and predicted transactions for a single resource model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_calibration_purchases_vs_holdout_purchases(\n    model, calibration_holdout_matrix, kind=\"frequency_cal\", n=7, **kwargs\n):\n    \"\"\"\n    Plot calibration purchases vs holdout.\n\n    This currently relies too much on the lifetimes.util calibration_and_holdout_data function.\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model.\n    calibration_holdout_matrix: pandas DataFrame\n        DataFrame from calibration_and_holdout_data function.\n    kind: str, optional\n        x-axis :\"frequency_cal\". Purchases in calibration period,\n                 \"recency_cal\". Age of customer at last purchase,\n                 \"T_cal\". Age of customer at the end of calibration period,\n                 \"time_since_last_purchase\". Time since user made last purchase\n    n: int, optional\n        Number of ticks on the x axis\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    x_labels = {\n        \"frequency_cal\": \"Purchases in calibration period\",\n        \"recency_cal\": \"Age of customer at last purchase\",\n        \"T_cal\": \"Age of customer at the end of calibration period\",\n        \"time_since_last_purchase\": \"Time since user made last purchase\",\n    }\n    summary = calibration_holdout_matrix.copy()\n    duration_holdout = summary.iloc[0][\"duration_holdout\"]\n\n    summary[\"model_predictions\"] = model.conditional_expected_number_of_purchases_up_to_time(\n            duration_holdout, summary[\"frequency_cal\"], summary[\"recency_cal\"], summary[\"T_cal\"])\n\n    if kind == \"time_since_last_purchase\":\n        summary[\"time_since_last_purchase\"] = summary[\"T_cal\"] - summary[\"recency_cal\"]\n        ax = (\n            summary.groupby([\"time_since_last_purchase\"])[[\"frequency_holdout\", \"model_predictions\"]]\n            .mean()\n            .iloc[:n]\n            .plot(**kwargs)\n        )\n    else:\n        ax = summary.groupby(kind)[[\"frequency_holdout\", \"model_predictions\"]].mean().iloc[:n].plot(**kwargs)\n\n    plt.title(\"Actual Purchases in Holdout Period vs Predicted Purchases\")\n    plt.xlabel(x_labels[kind])\n    plt.ylabel(\"Average of Purchases in Holdout Period\")\n    plt.legend()\n\n    return ax", "response": "Plots calibration purchases vs holdout."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_frequency_recency_matrix(\n    model,\n    T=1,\n    max_frequency=None,\n    max_recency=None,\n    title=None,\n    xlabel=\"Customer's Historical Frequency\",\n    ylabel=\"Customer's Recency\",\n    **kwargs\n):\n    \"\"\"\n    Plot recency frequecy matrix as heatmap.\n\n    Plot a figure of expected transactions in T next units of time by a customer's frequency and recency.\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model.\n    T: fload, optional\n        Next units of time to make predictions for\n    max_frequency: int, optional\n        The maximum frequency to plot. Default is max observed frequency.\n    max_recency: int, optional\n        The maximum recency to plot. This also determines the age of the customer.\n        Default to max observed age.\n    title: str, optional\n        Figure title\n    xlabel: str, optional\n        Figure xlabel\n    ylabel: str, optional\n        Figure ylabel\n    kwargs\n        Passed into the matplotlib.imshow command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    if max_frequency is None:\n        max_frequency = int(model.data[\"frequency\"].max())\n\n    if max_recency is None:\n        max_recency = int(model.data[\"T\"].max())\n\n    Z = np.zeros((max_recency + 1, max_frequency + 1))\n    for i, recency in enumerate(np.arange(max_recency + 1)):\n        for j, frequency in enumerate(np.arange(max_frequency + 1)):\n            Z[i, j] = model.conditional_expected_number_of_purchases_up_to_time(T, frequency, recency, max_recency)\n\n    interpolation = kwargs.pop(\"interpolation\", \"none\")\n\n    ax = plt.subplot(111)\n    pcm = ax.imshow(Z, interpolation=interpolation, **kwargs)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    if title is None:\n        title = (\n            \"Expected Number of Future Purchases for {} Unit{} of Time,\".format(T, \"s\"[T == 1 :])\n            + \"\\nby Frequency and Recency of a Customer\"\n        )\n    plt.title(title)\n\n    # turn matrix into square\n    forceAspect(ax)\n\n    # plot colorbar beside matrix\n    plt.colorbar(pcm, ax=ax)\n\n    return ax", "response": "Plots the frequency and recency frequecy matrix as heatmap."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the probability alive matrix as heatmap.", "response": "def plot_probability_alive_matrix(\n    model,\n    max_frequency=None,\n    max_recency=None,\n    title=\"Probability Customer is Alive,\\nby Frequency and Recency of a Customer\",\n    xlabel=\"Customer's Historical Frequency\",\n    ylabel=\"Customer's Recency\",\n    **kwargs\n):\n    \"\"\"\n    Plot probability alive matrix as heatmap.\n\n    Plot a figure of the probability a customer is alive based on their\n    frequency and recency.\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model.\n    max_frequency: int, optional\n        The maximum frequency to plot. Default is max observed frequency.\n    max_recency: int, optional\n        The maximum recency to plot. This also determines the age of the customer.\n        Default to max observed age.\n    title: str, optional\n        Figure title\n    xlabel: str, optional\n        Figure xlabel\n    ylabel: str, optional\n        Figure ylabel\n    kwargs\n        Passed into the matplotlib.imshow command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    z = model.conditional_probability_alive_matrix(max_frequency, max_recency)\n\n    interpolation = kwargs.pop(\"interpolation\", \"none\")\n\n    ax = plt.subplot(111)\n    pcm = ax.imshow(z, interpolation=interpolation, **kwargs)\n    plt.xlabel(xlabel)\n    plt.ylabel(ylabel)\n    plt.title(title)\n\n    # turn matrix into square\n    forceAspect(ax)\n\n    # plot colorbar beside matrix\n    plt.colorbar(pcm, ax=ax)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots the expected repeat purchases on the calibration period.", "response": "def plot_expected_repeat_purchases(\n    model,\n    title=\"Expected Number of Repeat Purchases per Customer\",\n    xlabel=\"Time Since First Purchase\",\n    ax=None,\n    label=None,\n    **kwargs\n):\n    \"\"\"\n    Plot expected repeat purchases on calibration period .\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model.\n    max_frequency: int, optional\n        The maximum frequency to plot.\n    title: str, optional\n        Figure title\n    xlabel: str, optional\n        Figure xlabel\n    ax: matplotlib.AxesSubplot, optional\n        Using user axes\n    label: str, optional\n        Label for plot.\n    kwargs\n        Passed into the matplotlib.pyplot.plot command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    if ax is None:\n        ax = plt.subplot(111)\n\n    if plt.matplotlib.__version__ >= \"1.5\":\n        color_cycle = ax._get_lines.prop_cycler\n        color = coalesce(kwargs.pop(\"c\", None), kwargs.pop(\"color\", None), next(color_cycle)[\"color\"])\n    else:\n        color_cycle = ax._get_lines.color_cycle\n        color = coalesce(kwargs.pop(\"c\", None), kwargs.pop(\"color\", None), next(color_cycle))\n\n    max_T = model.data[\"T\"].max()\n\n    times = np.linspace(0, max_T, 100)\n    ax.plot(times, model.expected_number_of_purchases_up_to_time(times), color=color, label=label, **kwargs)\n\n    times = np.linspace(max_T, 1.5 * max_T, 100)\n    ax.plot(times, model.expected_number_of_purchases_up_to_time(times), color=color, ls=\"--\", **kwargs)\n\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.legend(loc=\"lower right\")\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_history_alive(model, t, transactions, datetime_col, freq=\"D\", start_date=None, ax=None, **kwargs):\n    from matplotlib import pyplot as plt\n\n    if start_date is None:\n        start_date = min(transactions[datetime_col])\n\n    if ax is None:\n        ax = plt.subplot(111)\n\n    # Get purchasing history of user\n    customer_history = transactions[[datetime_col]].copy()\n    customer_history.index = pd.DatetimeIndex(customer_history[datetime_col])\n\n    # Add transactions column\n    customer_history[\"transactions\"] = 1\n    customer_history = customer_history.resample(freq).sum()\n\n    # plot alive_path\n    path = calculate_alive_path(model, transactions, datetime_col, t, freq)\n    path_dates = pd.date_range(start=min(transactions[datetime_col]), periods=len(path), freq=freq)\n    plt.plot(path_dates, path, \"-\", label=\"P_alive\")\n\n    # plot buying dates\n    payment_dates = customer_history[customer_history[\"transactions\"] >= 1].index\n    plt.vlines(payment_dates.values, ymin=0, ymax=1, colors=\"r\", linestyles=\"dashed\", label=\"purchases\")\n\n    plt.ylim(0, 1.0)\n    plt.yticks(np.arange(0, 1.1, 0.1))\n    plt.xlim(start_date, path_dates[-1])\n    plt.legend(loc=3)\n    plt.ylabel(\"P_alive\")\n    plt.title(\"History of P_alive\")\n\n    return ax", "response": "Plots the alive probability of being alive for a customer in time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef plot_incremental_transactions(\n    model,\n    transactions,\n    datetime_col,\n    customer_id_col,\n    t,\n    t_cal,\n    datetime_format=None,\n    freq=\"D\",\n    set_index_date=False,\n    title=\"Tracking Daily Transactions\",\n    xlabel=\"day\",\n    ylabel=\"Transactions\",\n    ax=None,\n    **kwargs\n):\n    \"\"\"\n    Plot a figure of the predicted and actual cumulative transactions of users.\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model\n    transactions: pandas DataFrame\n        DataFrame containing the transactions history of the customer_id\n    datetime_col: str\n        The column in transactions that denotes the datetime the purchase was made.\n    customer_id_col: str\n        The column in transactions that denotes the customer_id\n    t: float\n        The number of time units since the begining of\n        data for which we want to calculate cumulative transactions\n    t_cal: float\n        A marker used to indicate where the vertical line for plotting should be.\n    datetime_format: str, optional\n        A string that represents the timestamp format. Useful if Pandas\n        can't understand the provided format.\n    freq: str, optional\n        Default 'D' for days, 'W' for weeks, 'M' for months... etc.\n        Full list here:\n        http://pandas.pydata.org/pandas-docs/stable/timeseries.html#dateoffset-objects\n    set_index_date: bool, optional\n        When True set date as Pandas DataFrame index, default False - number of time units\n    title: str, optional\n        Figure title\n    xlabel: str, optional\n        Figure xlabel\n    ylabel: str, optional\n        Figure ylabel\n    ax: matplotlib.AxesSubplot, optional\n        Using user axes\n    kwargs\n        Passed into the pandas.DataFrame.plot command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    if ax is None:\n        ax = plt.subplot(111)\n\n    df_cum_transactions = expected_cumulative_transactions(\n        model,\n        transactions,\n        datetime_col,\n        customer_id_col,\n        t,\n        datetime_format=datetime_format,\n        freq=freq,\n        set_index_date=set_index_date,\n    )\n\n    # get incremental from cumulative transactions\n    df_cum_transactions = df_cum_transactions.apply(lambda x: x - x.shift(1))\n    ax = df_cum_transactions.plot(ax=ax, title=title, **kwargs)\n\n    if set_index_date:\n        x_vline = df_cum_transactions.index[int(t_cal)]\n        xlabel = \"date\"\n    else:\n        x_vline = t_cal\n    ax.axvline(x=x_vline, color=\"r\", linestyle=\"--\")\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n    return ax", "response": "Plots the predicted and actual cumulative transactions of the users in the order they were made."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_transaction_rate_heterogeneity(\n    model,\n    suptitle=\"Heterogeneity in Transaction Rate\",\n    xlabel=\"Transaction Rate\",\n    ylabel=\"Density\",\n    suptitle_fontsize=14,\n    **kwargs\n):\n    \"\"\"\n    Plot the estimated gamma distribution of lambda (customers' propensities to purchase).\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model, for now only for BG/NBD\n    suptitle: str, optional\n        Figure suptitle\n    xlabel: str, optional\n        Figure xlabel\n    ylabel: str, optional\n        Figure ylabel\n    kwargs\n        Passed into the matplotlib.pyplot.plot command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    r, alpha = model._unload_params(\"r\", \"alpha\")\n    rate_mean = r / alpha\n    rate_var = r / alpha ** 2\n\n    rv = stats.gamma(r, scale=1 / alpha)\n    lim = rv.ppf(0.99)\n    x = np.linspace(0, lim, 100)\n\n    fig, ax = plt.subplots(1)\n    fig.suptitle(\"Heterogeneity in Transaction Rate\", fontsize=suptitle_fontsize, fontweight=\"bold\")\n\n    ax.set_title(\"mean: {:.3f}, var: {:.3f}\".format(rate_mean, rate_var))\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.plot(x, rv.pdf(x), **kwargs)\n    return ax", "response": "Plots the estimated gamma distribution of lambda propensities to purchase."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the estimated gamma distribution of p.", "response": "def plot_dropout_rate_heterogeneity(\n    model,\n    suptitle=\"Heterogeneity in Dropout Probability\",\n    xlabel=\"Dropout Probability p\",\n    ylabel=\"Density\",\n    suptitle_fontsize=14,\n    **kwargs\n):\n    \"\"\"\n    Plot the estimated gamma distribution of p.\n\n    p - (customers' probability of dropping out immediately after a transaction).\n\n    Parameters\n    ----------\n    model: lifetimes model\n        A fitted lifetimes model, for now only for BG/NBD\n    suptitle: str, optional\n        Figure suptitle\n    xlabel: str, optional\n        Figure xlabel\n    ylabel: str, optional\n        Figure ylabel\n    kwargs\n        Passed into the matplotlib.pyplot.plot command.\n\n    Returns\n    -------\n    axes: matplotlib.AxesSubplot\n\n    \"\"\"\n    from matplotlib import pyplot as plt\n\n    a, b = model._unload_params(\"a\", \"b\")\n    beta_mean = a / (a + b)\n    beta_var = a * b / ((a + b) ** 2) / (a + b + 1)\n\n    rv = stats.beta(a, b)\n    lim = rv.ppf(0.99)\n    x = np.linspace(0, lim, 100)\n\n    fig, ax = plt.subplots(1)\n    fig.suptitle(suptitle, fontsize=suptitle_fontsize, fontweight=\"bold\")\n\n    ax.set_title(\"mean: {:.3f}, var: {:.3f}\".format(beta_mean, beta_var))\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel)\n\n    fig.tight_layout(rect=[0, 0.03, 1, 0.95])\n    plt.plot(x, rv.pdf(x), **kwargs)\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate artificial data according to the BG NBD model.", "response": "def beta_geometric_nbd_model(T, r, alpha, a, b, size=1):\n    \"\"\"\n    Generate artificial data according to the BG/NBD model.\n\n    See [1] for model details\n\n    Parameters\n    ----------\n    T: array_like\n        The length of time observing new customers.\n    r, alpha, a, b: float\n        Parameters in the model. See [1]_\n    size: int, optional\n        The number of customers to generate\n\n    Returns\n    -------\n    DataFrame\n        With index as customer_ids and the following columns:\n        'frequency', 'recency', 'T', 'lambda', 'p', 'alive', 'customer_id'\n\n    References\n    ----------\n    .. [1]: '\"Counting Your Customers\" the Easy Way: An Alternative to the Pareto/NBD Model'\n       (http://brucehardie.com/papers/bgnbd_2004-04-20.pdf)\n\n    \"\"\"\n    if type(T) in [float, int]:\n        T = T * np.ones(size)\n    else:\n        T = np.asarray(T)\n\n    probability_of_post_purchase_death = random.beta(a, b, size=size)\n    lambda_ = random.gamma(r, scale=1.0 / alpha, size=size)\n\n    columns = [\"frequency\", \"recency\", \"T\", \"lambda\", \"p\", \"alive\", \"customer_id\"]\n    df = pd.DataFrame(np.zeros((size, len(columns))), columns=columns)\n\n    for i in range(size):\n        p = probability_of_post_purchase_death[i]\n        l = lambda_[i]\n\n        # hacky until I can find something better\n        times = []\n        next_purchase_in = random.exponential(scale=1.0 / l)\n        alive = True\n        while (np.sum(times) + next_purchase_in < T[i]) and alive:\n            times.append(next_purchase_in)\n            next_purchase_in = random.exponential(scale=1.0 / l)\n            alive = random.random() > p\n\n        times = np.array(times).cumsum()\n        df.iloc[i] = (\n            np.unique(np.array(times).astype(int)).shape[0],\n            np.max(times if times.shape[0] > 0 else 0),\n            T[i],\n            l,\n            p,\n            alive,\n            i,\n        )\n\n    return df.set_index(\"customer_id\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate artificial transactional data according to the BG - NBD model.", "response": "def beta_geometric_nbd_model_transactional_data(T, r, alpha, a, b, observation_period_end=\"2019-1-1\", freq=\"D\", size=1):\n    \"\"\"\n    Generate artificial transactional data according to the BG/NBD model.\n\n    See [1] for model details\n\n    Parameters\n    ----------\n    T: int, float or array_like\n        The length of time observing new customers.\n    r, alpha, a, b: float\n        Parameters in the model. See [1]_\n    observation_period_end: date_like\n        The date observation ends\n    freq: string, optional\n        Default 'D' for days, 'W' for weeks, 'h' for hours\n    size: int, optional\n        The number of customers to generate\n\n    Returns\n    -------\n    DataFrame\n        The following columns:\n        'customer_id', 'date'\n\n    References\n    ----------\n    .. [1]: '\"Counting Your Customers\" the Easy Way: An Alternative to the Pareto/NBD Model'\n       (http://brucehardie.com/papers/bgnbd_2004-04-20.pdf)\n\n    \"\"\"\n    observation_period_end = pd.to_datetime(observation_period_end)\n\n    if type(T) in [float, int]:\n        start_date = [observation_period_end - pd.Timedelta(T - 1, unit=freq)] * size\n        T = T * np.ones(size)\n    else:\n        start_date = [observation_period_end - pd.Timedelta(T[i] - 1, unit=freq) for i in range(size)]\n        T = np.asarray(T)\n\n    probability_of_post_purchase_death = random.beta(a, b, size=size)\n    lambda_ = random.gamma(r, scale=1.0 / alpha, size=size)\n\n    columns = [\"customer_id\", \"date\"]\n    df = pd.DataFrame(columns=columns)\n\n    for i in range(size):\n        s = start_date[i]\n        p = probability_of_post_purchase_death[i]\n        l = lambda_[i]\n        age = T[i]\n\n        purchases = [[i, s - pd.Timedelta(1, unit=freq)]]\n        next_purchase_in = random.exponential(scale=1.0 / l)\n        alive = True\n\n        while next_purchase_in < age and alive:\n            purchases.append([i, s + pd.Timedelta(next_purchase_in, unit=freq)])\n            next_purchase_in += random.exponential(scale=1.0 / l)\n            alive = random.random() > p\n\n        df = df.append(pd.DataFrame(purchases, columns=columns))\n\n    return df.reset_index(drop=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pareto_nbd_model(T, r, alpha, s, beta, size=1):\n    if type(T) in [float, int]:\n        T = T * np.ones(size)\n    else:\n        T = np.asarray(T)\n\n    lambda_ = random.gamma(r, scale=1.0 / alpha, size=size)\n    mus = random.gamma(s, scale=1.0 / beta, size=size)\n\n    columns = [\"frequency\", \"recency\", \"T\", \"lambda\", \"mu\", \"alive\", \"customer_id\"]\n    df = pd.DataFrame(np.zeros((size, len(columns))), columns=columns)\n\n    for i in range(size):\n        l = lambda_[i]\n        mu = mus[i]\n        time_of_death = random.exponential(scale=1.0 / mu)\n\n        # hacky until I can find something better\n        times = []\n        next_purchase_in = random.exponential(scale=1.0 / l)\n        while np.sum(times) + next_purchase_in < min(time_of_death, T[i]):\n            times.append(next_purchase_in)\n            next_purchase_in = random.exponential(scale=1.0 / l)\n\n        times = np.array(times).cumsum()\n        df.iloc[i] = (\n            np.unique(np.array(times).astype(int)).shape[0],\n            np.max(times if times.shape[0] > 0 else 0),\n            T[i],\n            l,\n            mu,\n            time_of_death > T[i],\n            i,\n        )\n\n    return df.set_index(\"customer_id\")", "response": "Generate artificial data according to the Pareto NBD model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngenerating artificial data according to the Beta - Geometric and Beta - Binomial Distribution.", "response": "def beta_geometric_beta_binom_model(N, alpha, beta, gamma, delta, size=1):\n    \"\"\"\n    Generate artificial data according to the Beta-Geometric/Beta-Binomial\n    Model.\n\n    You may wonder why we can have frequency = n_periods, when frequency excludes their\n    first order. When a customer purchases something, they are born, _and in the next\n    period_ we start asking questions about their alive-ness. So really they customer has\n    bought frequency + 1, and been observed for n_periods + 1\n\n    Parameters\n    ----------\n    N: array_like\n        Number of transaction opportunities for new customers.\n    alpha, beta, gamma, delta: float\n        Parameters in the model. See [1]_\n    size: int, optional\n        The number of customers to generate\n\n    Returns\n    -------\n    DataFrame\n        with index as customer_ids and the following columns:\n        'frequency', 'recency', 'n_periods', 'lambda', 'p', 'alive', 'customer_id'\n\n    References\n    ----------\n    .. [1] Fader, Peter S., Bruce G.S. Hardie, and Jen Shang (2010),\n       \"Customer-Base Analysis in a Discrete-Time Noncontractual Setting,\"\n       Marketing Science, 29 (6), 1086-1108.\n\n    \"\"\"\n\n    if type(N) in [float, int, np.int64]:\n        N = N * np.ones(size)\n    else:\n        N = np.asarray(N)\n\n    probability_of_post_purchase_death = random.beta(a=alpha, b=beta, size=size)\n    thetas = random.beta(a=gamma, b=delta, size=size)\n\n    columns = [\"frequency\", \"recency\", \"n_periods\", \"p\", \"theta\", \"alive\", \"customer_id\"]\n    df = pd.DataFrame(np.zeros((size, len(columns))), columns=columns)\n    for i in range(size):\n        p = probability_of_post_purchase_death[i]\n        theta = thetas[i]\n\n        # hacky until I can find something better\n        current_t = 0\n        alive = True\n        times = []\n        while current_t < N[i] and alive:\n            alive = random.binomial(1, theta) == 0\n            if alive and random.binomial(1, p) == 1:\n                times.append(current_t)\n            current_t += 1\n        # adding in final death opportunity to agree with [1]\n        if alive:\n            alive = random.binomial(1, theta) == 0\n        df.iloc[i] = len(times), times[-1] + 1 if len(times) != 0 else 0, N[i], p, theta, alive, i\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_cdnow_summary_data_with_monetary_value(**kwargs):\n    df = load_dataset(\"cdnow_customers_summary_with_transactions.csv\", **kwargs)\n    df.columns = [\"customer_id\", \"frequency\", \"recency\", \"T\", \"monetary_value\"]\n    df = df.set_index(\"customer_id\")\n    return df", "response": "Load cdnow customers summary with monetary value as pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef conditional_expected_average_profit(self, frequency=None, monetary_value=None):\n        if monetary_value is None:\n            monetary_value = self.data[\"monetary_value\"]\n        if frequency is None:\n            frequency = self.data[\"frequency\"]\n        p, q, v = self._unload_params(\"p\", \"q\", \"v\")\n\n        # The expected average profit is a weighted average of individual\n        # monetary value and the population mean.\n        individual_weight = p * frequency / (p * frequency + q - 1)\n        population_mean = v * p / (q - 1)\n        return (1 - individual_weight) * population_mean + individual_weight * monetary_value", "response": "This method computes the conditional expectation of the average profit per transaction."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfits the data to the Gamma - GF model.", "response": "def fit(\n        self,\n        frequency,\n        monetary_value,\n        weights=None,\n        initial_params=None,\n        verbose=False,\n        tol=1e-7,\n        index=None,\n        q_constraint=False,\n        **kwargs\n    ):\n        \"\"\"\n        Fit the data to the Gamma/Gamma model.\n\n        Parameters\n        ----------\n        frequency: array_like\n            the frequency vector of customers' purchases\n            (denoted x in literature).\n        monetary_value: array_like\n            the monetary value vector of customer's purchases\n            (denoted m in literature).\n        weights: None or array_like\n            Number of customers with given frequency/monetary_value,\n            defaults to 1 if not specified. Fader and\n            Hardie condense the individual RFM matrix into all\n            observed combinations of frequency/monetary_value. This\n            parameter represents the count of customers with a given\n            purchase pattern. Instead of calculating individual\n            loglikelihood, the loglikelihood is calculated for each\n            pattern and multiplied by the number of customers with\n            that pattern.\n        initial_params: array_like, optional\n            set the initial parameters for the fitter.\n        verbose : bool, optional\n            set to true to print out convergence diagnostics.\n        tol : float, optional\n            tolerance for termination of the function minimization process.\n        index: array_like, optional\n            index for resulted DataFrame which is accessible via self.data\n        q_constraint: bool, optional\n            when q < 1, population mean will result in a negative value\n            leading to negative CLV outputs. If True, we penalize negative values of q to avoid this issue.\n        kwargs:\n            key word arguments to pass to the scipy.optimize.minimize\n            function as options dict\n\n        Returns\n        -------\n        GammaGammaFitter\n            fitted and with parameters estimated\n\n        \"\"\"\n        _check_inputs(frequency, monetary_value=monetary_value)\n\n        frequency = np.asarray(frequency).astype(float)\n        monetary_value = np.asarray(monetary_value).astype(float)\n\n        if weights is None:\n            weights = np.ones_like(frequency, dtype=int)\n        else:\n            weights = np.asarray(weights)\n\n        log_params, self._negative_log_likelihood_, self._hessian_ = self._fit(\n            (frequency, monetary_value, weights, self.penalizer_coef),\n            initial_params,\n            3,\n            verbose,\n            tol=tol,\n            bounds=((None, None), (0, None), (None, None)) if q_constraint else None,\n            **kwargs\n        )\n\n        self.data = DataFrame(\n            {\"monetary_value\": monetary_value, \"frequency\": frequency, \"weights\": weights}, index=index\n        )\n\n        self.params_ = pd.Series(np.exp(log_params), index=[\"p\", \"q\", \"v\"])\n\n        self.variance_matrix_ = self._compute_variance_matrix()\n        self.standard_errors_ = self._compute_standard_errors()\n        self.confidence_intervals_ = self._compute_confidence_intervals()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef customer_lifetime_value(\n        self, transaction_prediction_model, frequency, recency, T, monetary_value, time=12, discount_rate=0.01, freq=\"D\"\n    ):\n        \"\"\"\n        Return customer lifetime value.\n\n        This method computes the average lifetime value for a group of one\n        or more customers.\n\n        Parameters\n        ----------\n        transaction_prediction_model: model\n            the model to predict future transactions, literature uses\n            pareto/ndb models but we can also use a different model like beta-geo models\n        frequency: array_like\n            the frequency vector of customers' purchases\n            (denoted x in literature).\n        recency: the recency vector of customers' purchases\n                 (denoted t_x in literature).\n        T: array_like\n            customers' age (time units since first purchase)\n        monetary_value: array_like\n            the monetary value vector of customer's purchases\n            (denoted m in literature).\n        time: float, optional\n            the lifetime expected for the user in months. Default: 12\n        discount_rate: float, optional\n            the monthly adjusted discount rate. Default: 0.01\n        freq: string, optional\n            {\"D\", \"H\", \"M\", \"W\"} for day, hour, month, week. This represents what unit of time your T is measure in.\n\n        Returns\n        -------\n        Series:\n            Series object with customer ids as index and the estimated customer\n            lifetime values as values\n\n        \"\"\"\n        # use the Gamma-Gamma estimates for the monetary_values\n        adjusted_monetary_value = self.conditional_expected_average_profit(frequency, monetary_value)\n        return _customer_lifetime_value(\n            transaction_prediction_model, frequency, recency, T, adjusted_monetary_value, time, discount_rate, freq=freq\n        )", "response": "This method computes the average lifetime value for a customer in a group of customers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extract_from_sans(self):\n        self.logger.info(\"{} Trying to find Subdomains in SANs list\".format(COLORED_COMBOS.NOTIFY))\n        if self.host.naked:\n            domain = self.host.naked\n            tld_less = domain.split(\".\")[0]\n        else:\n            domain = self.host.target.split(\".\")\n            tld_less = domain[1]\n            domain = \".\".join(domain[1:])\n\n        for san in self.sans:\n            if (tld_less in san or domain in san) and self.target != san and not san.startswith(\"*\"):\n                self.logger.info(\"{} Subdomain detected: {}\".format(COLORED_COMBOS.GOOD, san))", "response": "Extracts the TLDs and subdomains from the SAN list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _detect_cms(self, tries=0):\n        # WhatCMS is under CloudFlare which detects and blocks proxied/Tor traffic, hence normal request.\n        page = requests.get(url=\"https://whatcms.org/?s={}\".format(self.host.target))\n        soup = BeautifulSoup(page.text, \"lxml\")\n        found = soup.select(\".panel.panel-success\")\n        if found:\n            try:\n                cms = [a for a in soup.select(\"a\") if \"/c/\" in a.get(\"href\")][0]\n                self.logger.info(\"{} CMS detected: target is using {}{}{}\".format(\n                    COLORED_COMBOS.GOOD, COLOR.GREEN, cms.get(\"title\"), COLOR.RESET))\n            except IndexError:\n                if tries >= 4:\n                    return\n                else:\n                    self._detect_cms(tries=tries + 1)\n        else:\n            if tries >= 4:\n                return\n            else:\n                self._detect_cms(tries=tries + 1)", "response": "Detect CMS using whatcms. org."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_instance_proxies(self):\n        proxies = {}\n        if self.tor_routing:\n            proxies = {\n                \"http\": \"socks5://127.0.0.1:9050\",\n                \"https\": \"socks5://127.0.0.1:9050\"\n            }\n        elif self.proxy_list:\n            try:\n                with open(self.proxy_list, \"r\") as file:\n                    file = file.readlines()\n                    proxies = [x.replace(\"\\n\", \"\") for x in file]\n            except FileNotFoundError:\n                raise RequestHandlerException(\"Cannot read proxies from {}\".format(self.proxy_list))\n        elif self.single_proxy:\n            proxies = {\n                \"http\": self.single_proxy,\n                \"https\": self.single_proxy\n            }\n        return proxies", "response": "Sets the proxies to use for the current instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send(self, method=\"GET\", *args, **kwargs):\n        proxies = self._get_request_proxies()\n\n        try:\n            if method.upper() in self.allowed_methods:\n                kwargs['timeout'] = kwargs['timeout'] if 'timeout' in kwargs else 5\n                return request(method, proxies=proxies, headers=self.headers, cookies=self.cookies, *args, **kwargs)\n            else:\n                raise RequestHandlerException(\"Unsupported method: {}\".format(method))\n        except ProxyError:\n            # TODO: Apply fail over for bad proxies or drop them\n            raise RequestHandlerException(\"Error connecting to proxy\")\n        except (ConnectTimeout, ReadTimeout):\n            raise RequestHandlerException(\"Connection with server timed out\")\n        except NewConnectionError:\n            raise RequestHandlerException(\"Address cannot be resolved\")\n            # New connection error == Can't resolve address\n        except ConnectionError:\n            # TODO: Increase delay\n            raise RequestHandlerException(\"Error connecting to host\")\n        except TooManyRedirects:\n            raise RequestHandlerException(\"Infinite redirects detected - too many redirects error\")\n        except UnicodeDecodeError:\n            # Following issue #19, apparently some sites do not use utf-8 in their uris :<>\n            pass", "response": "Send a request to the object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a new session using the object s proxies and headers", "response": "def get_new_session(self):\n        \"\"\"Returns a new session using the object's proxies and headers\"\"\"\n        session = Session()\n        session.headers = self.headers\n        session.proxies = self._get_request_proxies()\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the target string into a list of attributes.", "response": "def parse(self):\n        \"\"\"\n        Try to extract domain (full, naked, sub-domain), IP and port.\n        \"\"\"\n        if self.target.endswith(\"/\"):\n            self.target = self.target[:-1]\n\n        if self._is_proto(self.target):\n            try:\n                self.protocol, self.target = self.target.split(\"://\")\n                self.logger.info(\"{} Protocol detected: {}\".format(COLORED_COMBOS.NOTIFY, self.protocol))\n                if self.protocol.lower() == \"https\" and self.port == 80:\n                    self.port = 443\n            except ValueError:\n                raise HostHandlerException(\"Could not make domain and protocol from host\")\n\n        if \":\" in self.target:\n            self._extract_port(self.target)\n\n        if self.validate_ip(self.target):\n            self.logger.info(\"{} Detected {} as an IP address.\".format(COLORED_COMBOS.NOTIFY, self.target))\n            self.is_ip = True\n        else:\n            domains = []\n            if self.target.startswith(\"www.\"):\n                # Obviously an FQDN\n                domains.extend((self.target, self.target.split(\"www.\")[1]))\n                self.fqdn = self.target\n                self.naked = \".\".join(self.fqdn.split('.')[1:])\n            else:\n                domains.append(self.target)\n                domain_levels = self.target.split(\".\")\n                if len(domain_levels) == 2 or (len(domain_levels) == 3 and domain_levels[1] == \"co\"):\n                    self.logger.info(\"{} Found {} to be a naked domain\".format(COLORED_COMBOS.NOTIFY, self.target))\n                    self.naked = self.target\n\n            try:\n                self.dns_results = DNSHandler.query_dns(domains, self.dns_records)\n            except Timeout:\n                raise HostHandlerException(\"DNS Query timed out. Maybe target has DNS protection ?\")\n\n            if self.dns_results.get(\"CNAME\"):\n                # Naked domains shouldn't hold CNAME records according to RFC regulations\n                self.logger.info(\"{} Found {} to be an FQDN by CNAME presence in DNS records\".format(\n                    COLORED_COMBOS.NOTIFY, self.target))\n\n                self.fqdn = self.target\n                self.naked = \".\".join(self.fqdn.split('.')[1:])\n        self.create_host_dir_and_set_file_logger()\n        self.write_up()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef query_dns(cls, domains, records):\n        results = {k: set() for k in records}\n        for record in records:\n            for domain in domains:\n                try:\n                    answers = cls.resolver.query(domain, record)\n                    for answer in answers:\n                        # Add value to record type\n                        results.get(record).add(answer)\n                except (resolver.NoAnswer, resolver.NXDOMAIN, resolver.NoNameservers):\n                    # Type of record doesn't fit domain or no answer from ns\n                    continue\n\n        return {k: v for k, v in results.items() if v}", "response": "Query DNS records for a given host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _are_certificates_identical(self):\n        sni_cert = self.sni_data.get(\"Certificate_details\")\n        non_sni_cert = self.non_sni_data.get(\"Certificate_details\")\n        if all(cert for cert in (sni_cert, non_sni_cert) if cert) and sni_cert == non_sni_cert:\n            return True\n        return", "response": "Validate that both certificates exist and have the same SNI certificate and non - SNI certificate."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nexecute the SSL data extraction command and return the result as a dict.", "response": "async def _execute_ssl_data_extraction(self, sni=False):\n        \"\"\"\n        Test for version support (SNI/non-SNI), get all SANs, get certificate details\n        :param sni: True will call cause _exec_openssl to call openssl with -servername flag\n        \"\"\"\n        # Do for all responses\n        responses = await self._run_openssl_sclient_cmd(self._base_script, sni)\n        tls_dict = self._parse_openssl_sclient_output(responses)\n        # Do for one successful SSL response\n        for res in responses:\n            if self._is_certificate_exists(res):\n                tls_dict[\"SANs\"] = await self._get_sans_from_openssl_cmd(res)\n                tls_dict[\"Certificate_details\"] = await self._extract_certificate_details(res)\n                break\n\n        return tls_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_to_found_storage(self, storage_url):\n        storage_url = self._normalize_url(storage_url)\n        bucket = S3Bucket(storage_url)\n        if bucket.url not in self.storage_urls_found:\n            try:\n                res = self.request_handler.send(\"GET\", url=storage_url)\n                if self._is_amazon_s3_bucket(res):\n                    self.storage_urls_found.add(bucket.url)\n                    self.s3_buckets.add(bucket)\n            except RequestHandlerException:\n                # Cannot connect to storage, move on\n                pass", "response": "Add to found storage if not already in found"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _fetch(self, uri, sub_domain=False):\n        url = self._build_request_url(uri, sub_domain=sub_domain)\n\n        try:\n            res = self.request_handler.send(\"HEAD\", url=url, allow_redirects=self.follow_redirects)\n            if res.status_code not in self.ignored_error_codes:\n                self._log_response(res.status_code, url, res.headers)\n        except (AttributeError, RequestHandlerException):\n            # res is None or another error occurred\n            pass", "response": "Send a HEAD request to the URL and print response code if it s not in ignored_error_codes\n           "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfuzz all the words in the list.", "response": "async def fuzz_all(self, sub_domain=False, log_file_path=None):\n        \"\"\"\n        Create a pool of threads and exhaust self.wordlist on self._fetch\n        Should be run in an event loop.\n        :param sub_domain: Indicate if this is subdomain enumeration or URL busting\n        :param log_file_path: Log subdomain enum results to this path.\n        \"\"\"\n        self.logger = self.get_log_file_path(log_file_path)\n        try:\n            # Rule out wildcard subdomain support/all resources redirect to a 200 page\n            response_codes = self._generate_fake_requests(sub_domain)\n            self._rule_out_false_positives(response_codes, sub_domain)\n\n            if not sub_domain:\n                self.logger.info(\"{} Fuzzing URLs\".format(COLORED_COMBOS.INFO))\n            self.logger.info(\"{} Reading from list: {}\".format(COLORED_COMBOS.INFO, self.path_to_wordlist))\n            pool = ThreadPool(self.num_threads)\n            pool.map(partial(self._fetch, sub_domain=sub_domain), self.wordlist)\n            pool.close()\n            pool.join()\n            if not sub_domain:\n                self.logger.info(\"{} Done fuzzing URLs\".format(COLORED_COMBOS.INFO))\n        except FuzzerException as e:\n            self.logger.info(\"{} {}\".format(COLORED_COMBOS.BAD, e))\n        except ConnectionError as e:\n            if \"Remote end closed connection without response\" in str(e):\n                self.logger.info(\"{} {}. Target is actively closing connections - will not \"\n                                 \"bruteforce URLs\".format(COLORED_COMBOS.BAD, str(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_port_range(cls, port_range):\n        ports = port_range.split(\"-\")\n        if all(ports) and int(ports[-1]) <= 65535 and not len(ports) != 2:\n            return True\n        raise ScannerException(\"Invalid port range {}\".format(port_range))", "response": "Validate the port range for Nmap scan"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the proxy list or tor_routing arguments are supplied.", "response": "def validate_proxy_args(cls, *args):\n        \"\"\"No more than 1 of the following can be specified: tor_routing, proxy, proxy_list\"\"\"\n        supplied_proxies = Counter((not arg for arg in (*args,))).get(False)\n        if not supplied_proxies:\n            return\n        elif supplied_proxies > 1:\n            raise RaccoonException(\"Must specify only one of the following:\\n\"\n                                   \"--tor-routing, --proxy-list, --proxy\")"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef create_output_directory(cls, outdir):\n        cls.PATH = outdir\n        try:\n            os.mkdir(outdir)\n        except FileExistsError:\n            pass", "response": "Tries to create base output directory"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_raw(self, length_tag, value_tag):\n\n        self.raw_len_tags.append(length_tag)\n        self.raw_data_tags.append(value_tag)\n        return", "response": "Define the tags used for a private raw data field."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves the tags for a data type field.", "response": "def remove_raw(self, length_tag, value_tag):\n        \"\"\"Remove the tags for a data type field.\n\n        :param length_tag: tag number of the length field.\n        :param value_tag: tag number of the value field.\n\n        You can remove either private or standard data field definitions in\n        case a particular application uses them for a field of a different\n        type. \"\"\"\n\n        self.raw_len_tags.remove(length_tag)\n        self.raw_data_tags.remove(value_tag)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing the accumulated buffer and return the first complete message.", "response": "def get_message(self):\n        \"\"\"Process the accumulated buffer and return the first message.\n\n        If the buffer starts with FIX fields other than BeginString\n        (8), these are discarded until the start of a message is\n        found.\n\n        If no BeginString (8) field is found, this function returns\n        None.  Similarly, if (after a BeginString) no Checksum (10)\n        field is found, the function returns None.\n\n        Otherwise, it returns a simplefix.FixMessage instance\n        initialised with the fields from the first complete message\n        found in the buffer.\"\"\"\n\n        # Break buffer into tag=value pairs.\n        start = 0\n        point = 0\n        in_tag = True\n        tag = 0\n\n        while point < len(self.buf):\n            if in_tag and self.buf[point] == EQUALS_BYTE:\n                tag_string = self.buf[start:point]\n                point += 1\n\n                tag = int(tag_string)\n                if tag in self.raw_data_tags and self.raw_len > 0:\n                    if self.raw_len > len(self.buf) - point:\n                        break\n\n                    value = self.buf[point:point + self.raw_len]\n                    self.pairs.append((tag, value))\n                    self.buf = self.buf[point + self.raw_len + 1:]\n                    point = 0\n                    self.raw_len = 0\n                    start = point\n\n                else:\n                    in_tag = False\n                    start = point\n\n            elif self.buf[point] == SOH_BYTE:\n                value = self.buf[start:point]\n                self.pairs.append((tag, value))\n                self.buf = self.buf[point + 1:]\n                point = 0\n                start = point\n                in_tag = True\n\n                if tag in self.raw_len_tags:\n                    self.raw_len = int(value)\n\n            point += 1\n\n        if len(self.pairs) == 0:\n            return None\n\n        # Check first pair is FIX BeginString.\n        while self.pairs and self.pairs[0][0] != 8:\n            # Discard pairs until we find the beginning of a message.\n            self.pairs.pop(0)\n\n        if len(self.pairs) == 0:\n            return None\n\n        # Look for checksum.\n        index = 0\n        while index < len(self.pairs) and self.pairs[index][0] != 10:\n            index += 1\n\n        if index == len(self.pairs):\n            return None\n\n        # Found checksum, so we have a complete message.\n        m = FixMessage()\n        pairs = self.pairs[:index + 1]\n        for tag, value in pairs:\n            m.append_pair(tag, value)\n        self.pairs = self.pairs[index + 1:]\n\n        return m"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nmaking a FIX value from a string bytes or number.", "response": "def fix_val(value):\n    \"\"\"Make a FIX value from a string, bytes, or number.\"\"\"\n    if type(value) == bytes:\n        return value\n\n    if sys.version_info[0] == 2:\n        return bytes(value)\n    elif type(value) == str:\n        return bytes(value, 'UTF-8')\n    else:\n        return bytes(str(value), 'ASCII')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a FIX tag value from string bytes or integer.", "response": "def fix_tag(value):\n    \"\"\"Make a FIX tag value from string, bytes, or integer.\"\"\"\n    if sys.version_info[0] == 2:\n        return bytes(value)\n    else:\n        if type(value) == bytes:\n            return value\n        elif type(value) == str:\n            return value.encode('ASCII')\n        return str(value).encode('ASCII')"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nappend a tag = value pair to this message.", "response": "def append_pair(self, tag, value, header=False):\n        \"\"\"Append a tag=value pair to this message.\n\n        :param tag: Integer or string FIX tag number.\n        :param value: FIX tag value.\n        :param header: Append to header if True; default to body.\n\n        Both parameters are explicitly converted to strings before\n        storage, so it's ok to pass integers if that's easier for\n        your program logic.\n\n        Note: a Python 'None' value will be silently ignored, and\n        no field is appended.\"\"\"\n\n        if tag is None or value is None:\n            return\n\n        if int(tag) == 8:\n            self.begin_string = fix_val(value)\n\n        if int(tag) == 35:\n            self.message_type = fix_val(value)\n\n        if header:\n            self.pairs.insert(self.header_index,\n                              (fix_tag(tag),\n                               fix_val(value)))\n            self.header_index += 1\n        else:\n            self.pairs.append((fix_tag(tag), fix_val(value)))\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef append_time(self, tag, timestamp=None, precision=3, utc=True,\n                    header=False):\n        \"\"\"Append a time field to this message.\n\n        :param tag: Integer or string FIX tag number.\n        :param timestamp: Time (see below) value to append, or None for now.\n        :param precision: Number of decimal digits.  Zero for seconds only,\n        three for milliseconds, 6 for microseconds.  Defaults to milliseconds.\n        :param utc: Use UTC if True, local time if False.\n        :param header: Append to header if True; default to body.\n\n        THIS METHOD IS DEPRECATED!\n        USE append_utc_timestamp() OR append_tz_timestamp() INSTEAD.\n\n        Append a timestamp in FIX format from a Python time.time or\n        datetime.datetime value.\n\n        Note that prior to FIX 5.0, precision must be zero or three to be\n        compliant with the standard.\"\"\"\n\n        warnings.warn(\"simplefix.FixMessage.append_time() is deprecated. \"\n                      \"Use append_utc_timestamp() or append_tz_timestamp() \"\n                      \"instead.\", DeprecationWarning)\n        if not timestamp:\n            t = datetime.datetime.utcnow()\n\n        elif type(timestamp) is float:\n            if utc:\n                t = datetime.datetime.utcfromtimestamp(timestamp)\n            else:\n                t = datetime.datetime.fromtimestamp(timestamp)\n\n        else:\n            t = timestamp\n\n        s = t.strftime(\"%Y%m%d-%H:%M:%S\")\n        if precision == 3:\n            s += \".%03d\" % (t.microsecond / 1000)\n        elif precision == 6:\n            s += \".%06d\" % t.microsecond\n        elif precision != 0:\n            raise ValueError(\"Precision should be one of 0, 3 or 6 digits\")\n\n        return self.append_pair(tag, s, header=header)", "response": "Append a time field to this message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _append_utc_datetime(self, tag, format, ts, precision, header):\n\n        if ts is None:\n            t = datetime.datetime.utcnow()\n        elif type(ts) is float:\n            t = datetime.datetime.utcfromtimestamp(ts)\n        else:\n            t = ts\n\n        s = t.strftime(format)\n        if precision == 3:\n            s += \".%03d\" % (t.microsecond / 1000)\n        elif precision == 6:\n            s += \".%06d\" % t.microsecond\n        elif precision != 0:\n            raise ValueError(\"Precision should be one of 0, 3 or 6 digits\")\n\n        return self.append_pair(tag, s, header=header)", "response": "Internal ) Append formatted datetime."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends a UTCTimestamp field to the current object.", "response": "def append_utc_timestamp(self, tag, timestamp=None, precision=3,\n                             header=False):\n        \"\"\"Append a field with a UTCTimestamp value.\n\n        :param tag: Integer or string FIX tag number.\n        :param timestamp: Time value, see below.\n        :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).\n        :param header: Append to FIX header if True; default to body.\n\n        The `timestamp` value should be a datetime, such as created by\n        datetime.datetime.utcnow(); a float, being the number of seconds\n        since midnight 1 Jan 1970 UTC, such as returned by time.time();\n        or, None, in which case datetime.datetime.utcnow() is used to\n        get the current UTC time.\n\n        Precision values other than zero (seconds), 3 (milliseconds),\n        or 6 (microseconds) will raise an exception.  Note that prior\n        to FIX 5.0, only values of 0 or 3 comply with the standard.\"\"\"\n\n        return self._append_utc_datetime(tag,\n                                         \"%Y%m%d-%H:%M:%S\",\n                                         timestamp,\n                                         precision,\n                                         header)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append_utc_time_only(self, tag, timestamp=None, precision=3,\n                             header=False):\n        \"\"\"Append a field with a UTCTimeOnly value.\n\n        :param tag: Integer or string FIX tag number.\n        :param timestamp: Time value, see below.\n        :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).\n        :param header: Append to FIX header if True; default to body.\n\n        The `timestamp` value should be a datetime, such as created by\n        datetime.datetime.utcnow(); a float, being the number of seconds\n        since midnight 1 Jan 1970 UTC, such as returned by time.time();\n        or, None, in which case datetime.datetime.utcnow() is used to\n        get the current UTC time.\n\n        Precision values other than zero (seconds), 3 (milliseconds),\n        or 6 (microseconds) will raise an exception.  Note that prior\n        to FIX 5.0, only values of 0 or 3 comply with the standard.\"\"\"\n\n        return self._append_utc_datetime(tag,\n                                         \"%H:%M:%S\",\n                                         timestamp,\n                                         precision,\n                                         header)", "response": "Append a field with a UTCTimeOnly value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nappending a TZTimestamp value to the end of the message.", "response": "def append_tz_timestamp(self, tag, timestamp=None, precision=3,\n                            header=False):\n        \"\"\"Append a field with a TZTimestamp value, derived from local time.\n\n        :param tag: Integer or string FIX tag number.\n        :param timestamp: Time value, see below.\n        :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).\n        :param header: Append to FIX header if True; default to body.\n\n        The `timestamp` value should be a local datetime, such as created\n        by datetime.datetime.now(); a float, being the number of seconds\n        since midnight 1 Jan 1970 UTC, such as returned by time.time();\n        or, None, in which case datetime.datetime.now() is used to get\n        the current local time.\n\n        Precision values other than zero (seconds), 3 (milliseconds),\n        or 6 (microseconds) will raise an exception.  Note that prior\n        to FIX 5.0, only values of 0 or 3 comply with the standard.\"\"\"\n\n        # Get float offset from Unix epoch.\n        if timestamp is None:\n            now = time.time()\n        elif type(timestamp) is float:\n            now = timestamp\n        else:\n            now = time.mktime(timestamp.timetuple()) + \\\n                  (timestamp.microsecond * 1e-6)\n\n        # Get offset of local timezone east of UTC.\n        utc = datetime.datetime.utcfromtimestamp(now)\n        local = datetime.datetime.fromtimestamp(now)\n        td = local - utc\n        offset = int(((td.days * 86400) + td.seconds) / 60)\n\n        s = local.strftime(\"%Y%m%d-%H:%M:%S\")\n        if precision == 3:\n            s += \".%03u\" % (local.microsecond / 1000)\n        elif precision == 6:\n            s += \".%06u\" % local.microsecond\n        elif precision != 0:\n            raise ValueError(\"Precision (%u) should be one of \"\n                             \"0, 3 or 6 digits\" % precision)\n\n        s += self._tz_offset_string(offset)\n        return self.append_pair(tag, s, header=header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef append_tz_time_only(self, tag, timestamp=None, precision=3,\n                            header=False):\n        \"\"\"Append a field with a TZTimeOnly value.\n\n        :param tag: Integer or string FIX tag number.\n        :param timestamp: Time value, see below.\n        :param precision: Number of decimal places: 0, 3 (ms) or 6 (us).\n        :param header: Append to FIX header if True; default to body.\n\n        The `timestamp` value should be a local datetime, such as created\n        by datetime.datetime.now(); a float, being the number of seconds\n        since midnight 1 Jan 1970 UTC, such as returned by time.time();\n        or, None, in which case datetime.datetime.now() is used to\n        get the current UTC time.\n\n        Precision values other than None (minutes), zero (seconds),\n        3 (milliseconds), or 6 (microseconds) will raise an exception.\n        Note that prior to FIX 5.0, only values of 0 or 3 comply with the\n        standard.\"\"\"\n\n        if timestamp is None:\n            t = datetime.datetime.now()\n        elif type(timestamp) is float:\n            t = datetime.datetime.fromtimestamp(timestamp)\n        else:\n            t = timestamp\n\n        now = time.mktime(t.timetuple()) + (t.microsecond * 1e-6)\n        utc = datetime.datetime.utcfromtimestamp(now)\n        td = t - utc\n        offset = int(((td.days * 86400) + td.seconds) / 60)\n\n        s = t.strftime(\"%H:%M\")\n        if precision == 0:\n            s += t.strftime(\":%S\")\n        elif precision == 3:\n            s += t.strftime(\":%S\")\n            s += \".%03u\" % (t.microsecond / 1000)\n        elif precision == 6:\n            s += t.strftime(\":%S\")\n            s += \".%06u\" % t.microsecond\n        elif precision is not None:\n            raise ValueError(\"Precision should be one of \"\n                             \"None, 0, 3 or 6 digits\")\n\n        s += self._tz_offset_string(offset)\n        return self.append_pair(tag, s, header=header)", "response": "Append a TZTimeOnly value to the internal representation of the record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nappend a TZTimeOnly value to the internal representation of the internal structure.", "response": "def append_tz_time_only_parts(self, tag, h, m, s=None, ms=None, us=None,\n                                  offset=0, header=False):\n        \"\"\"Append a field with a TZTimeOnly value from components.\n\n        :param tag: Integer or string FIX tag number.\n        :param h: Hours, in range 0 to 23.\n        :param m: Minutes, in range 0 to 59.\n        :param s: Optional seconds, in range 0 to 59 (60 for leap second).\n        :param ms: Optional milliseconds, in range 0 to 999.\n        :param us: Optional microseconds, in range 0 to 999.\n        :param offset: Minutes east of UTC, in range -1439 to +1439.\n        :param header: Append to FIX header if True; default to body.\n\n        Formats the TZTimeOnly value from its components.\n\n        If `s`, `ms` or `us` are None, the precision is truncated at\n        that point.\"\"\"\n\n        ih = int(h)\n        if ih < 0 or ih > 23:\n            raise ValueError(\"Hour value `h` (%u) out of range \"\n                             \"0 to 23\" % ih)\n\n        im = int(m)\n        if im < 0 or im > 59:\n            raise ValueError(\"Minute value `m` (%u) out of range \"\n                             \"0 to 59\" % im)\n\n        v = \"%02u:%02u\" % (ih, im)\n\n        if s is not None:\n            isec = int(s)\n            if isec < 0 or isec > 60:\n                raise ValueError(\"Seconds value `s` (%u) out of range \"\n                                 \"0 to 60\" % isec)\n            v += \":%02u\" % isec\n\n            if ms is not None:\n                ims = int(ms)\n                if ims < 0 or ims > 999:\n                    raise ValueError(\"Milliseconds value `ms` (%u) \"\n                                     \"out of range 0 to 999\" % ims)\n                v += \".%03u\" % ims\n\n                if us is not None:\n                    ius = int(us)\n                    if ius < 0 or ius > 999:\n                        raise ValueError(\"Microseconds value `us` (%u) \"\n                                         \"out of range 0 to 999\" % ius)\n                    v += \"%03u\" % ius\n\n        v += self._tz_offset_string(offset)\n        return self.append_pair(tag, v, header=header)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nappends a tag = value pair to the message.", "response": "def append_string(self, field, header=False):\n        \"\"\"Append a tag=value pair in string format.\n\n        :param field: String \"tag=value\" to be appended to this message.\n        :param header: Append to header if True; default to body.\n\n        The string is split at the first '=' character, and the resulting\n        tag and value strings are appended to the message.\"\"\"\n\n        # Split into tag and value.\n        bits = field.split('=', 1)\n        if len(bits) != 2:\n            raise ValueError(\"Field missing '=' separator.\")\n\n        # Check tag is an integer.\n        try:\n            tag_int = int(bits[0])\n        except ValueError:\n            raise ValueError(\"Tag value must be an integer\")\n\n        # Save.\n        self.append_pair(tag_int, bits[1], header=header)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending a list of strings to the message.", "response": "def append_strings(self, string_list, header=False):\n        \"\"\"Append tag=pairs for each supplied string.\n\n        :param string_list: List of \"tag=value\" strings.\n        :param header: Append to header if True; default to body.\n\n        Each string is split, and the resulting tag and value strings\n        are appended to the message.\"\"\"\n\n        for s in string_list:\n            self.append_string(s, header=header)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nappending raw data possibly including an embedded SOH.", "response": "def append_data(self, len_tag, val_tag, data, header=False):\n        \"\"\"Append raw data, possibly including a embedded SOH.\n\n        :param len_tag: Tag number for length field.\n        :param val_tag: Tag number for value field.\n        :param data: Raw data byte string.\n        :param header: Append to header if True; default to body.\n\n        Appends two pairs: a length pair, followed by a data pair,\n        containing the raw data supplied.  Example fields that should\n        use this method include: 95/96, 212/213, 354/355, etc.\"\"\"\n\n        self.append_pair(len_tag, len(data), header=header)\n        self.append_pair(val_tag, data, header=header)\n        return"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns n - th value matching tag.", "response": "def get(self, tag, nth=1):\n        \"\"\"Return n-th value for tag.\n\n        :param tag: FIX field tag number.\n        :param nth: Index of tag if repeating, first is 1.\n        :return: None if nothing found, otherwise value matching tag.\n\n        Defaults to returning the first matching value of 'tag', but if\n        the 'nth' parameter is overridden, can get repeated fields.\"\"\"\n\n        tag = fix_tag(tag)\n        nth = int(nth)\n\n        for t, v in self.pairs:\n            if t == tag:\n                nth -= 1\n                if nth == 0:\n                    return v\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef remove(self, tag, nth=1):\n\n        tag = fix_tag(tag)\n        nth = int(nth)\n\n        for i in range(len(self.pairs)):\n            t, v = self.pairs[i]\n            if t == tag:\n                nth -= 1\n                if nth == 0:\n                    self.pairs.pop(i)\n                    return v\n\n        return None", "response": "Removes the n - th occurrence of tag in this message."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts the message to on - the - wire FIX format.", "response": "def encode(self, raw=False):\n        \"\"\"Convert message to on-the-wire FIX format.\n\n        :param raw: If True, encode pairs exactly as provided.\n\n        Unless 'raw' is set, this function will calculate and\n        correctly set the BodyLength (9) and Checksum (10) fields, and\n        ensure that the BeginString (8), Body Length (9), Message Type\n        (35) and Checksum (10) fields are in the right positions.\n\n        This function does no further validation of the message content.\"\"\"\n\n        buf = b\"\"\n        if raw:\n            # Walk pairs, creating string.\n            for tag, value in self.pairs:\n                buf += tag + b'=' + value + SOH_STR\n\n            return buf\n\n        # Cooked.\n        for tag, value in self.pairs:\n            if int(tag) in (8, 9, 35, 10):\n                continue\n            buf += tag + b'=' + value + SOH_STR\n\n        # Prepend the message type.\n        if self.message_type is None:\n            raise ValueError(\"No message type set\")\n\n        buf = b\"35=\" + self.message_type + SOH_STR + buf\n\n        # Calculate body length.\n        #\n        # From first byte after body length field, to the delimiter\n        # before the checksum (which shouldn't be there yet).\n        body_length = len(buf)\n\n        # Prepend begin-string and body-length.\n        if not self.begin_string:\n            raise ValueError(\"No begin string set\")\n\n        buf = b\"8=\" + self.begin_string + SOH_STR + \\\n              b\"9=\" + fix_val(\"%u\" % body_length) + SOH_STR + \\\n              buf\n\n        # Calculate and append the checksum.\n        checksum = 0\n        for c in buf:\n            checksum += ord(c) if sys.version_info[0] == 2 else c\n        buf += b\"10=\" + fix_val(\"%03u\" % (checksum % 256,)) + SOH_STR\n\n        return buf"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _tz_offset_string(offset):\n\n        s = \"\"\n        io = int(offset)\n        if io == 0:\n            s += \"Z\"\n        else:\n            if -1440 < io < 1440:\n                ho = abs(io) / 60\n                mo = abs(io) % 60\n\n                s += \"%c%02u\" % (\"+\" if io > 0 else \"-\", ho)\n                if mo != 0:\n                    s += \":%02u\" % mo\n\n            else:\n                raise ValueError(\"Timezone `offset` (%u) out of range \"\n                                 \"-1439 to +1439 minutes\" % io)\n        return s", "response": "Convert TZ offset in minutes east to string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_suspicious( pe ):\n\n    relocations_overlap_entry_point = False\n    sequential_relocs = 0\n\n    # If relocation data is found and the entries go over the entry point, and also are very\n    # continuous or point outside section's boundaries => it might imply that an obfuscation\n    # trick is being used or the relocations are corrupt (maybe intentionally)\n    #\n    if hasattr(pe, 'DIRECTORY_ENTRY_BASERELOC'):\n        for base_reloc in pe.DIRECTORY_ENTRY_BASERELOC:\n            last_reloc_rva = None\n            for reloc in base_reloc.entries:\n                if reloc.rva <= pe.OPTIONAL_HEADER.AddressOfEntryPoint <= reloc.rva + 4:\n                    relocations_overlap_entry_point = True\n\n                if last_reloc_rva is not None and last_reloc_rva <= reloc.rva <= last_reloc_rva + 4:\n                    sequential_relocs += 1\n\n                last_reloc_rva = reloc.rva\n\n\n\n    # If import tables or strings exist (are pointed to) to within the header or in the area\n    # between the PE header and the first section that's supicious\n    #\n    # IMPLEMENT\n\n\n    warnings_while_parsing = False\n    # If we have warnings, that's suspicious, some of those will be because of out-of-ordinary\n    # values are found in the PE header fields\n    # Things that are reported in warnings:\n    # (parsing problems, special section characteristics i.e. W & X, uncommon values of fields,\n    # unusual entrypoint, suspicious imports)\n    #\n    warnings = pe.get_warnings()\n    if warnings:\n        warnings_while_parsing\n\n    # If there are few or none (should come with a standard \"density\" of strings/kilobytes of data) longer (>8)\n    # ascii sequences that might indicate packed data, (this is similar to the entropy test in some ways but\n    # might help to discard cases of legitimate installer or compressed data)\n\n    # If compressed data (high entropy) and is_driver => uuuuhhh, nasty\n\n    pass", "response": "Checks if a PE is suspicious."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_probably_packed( pe ):\n\n    # Calculate the lenth of the data up to the end of the last section in the\n    # file. Overlay data won't be taken into account\n    #\n    total_pe_data_length = len( pe.trim() )\n    # Assume that the file is packed when no data is available\n    if not total_pe_data_length:\n        return True\n    has_significant_amount_of_compressed_data = False\n\n    # If some of the sections have high entropy and they make for more than 20% of the file's size\n    # it's assumed that it could be an installer or a packed file\n\n    total_compressed_data = 0\n    for section in pe.sections:\n        s_entropy = section.get_entropy()\n        s_length = len( section.get_data() )\n        # The value of 7.4 is empircal, based on looking at a few files packed\n        # by different packers\n        if s_entropy > 7.4:\n            total_compressed_data += s_length\n\n    if ((1.0 * total_compressed_data)/total_pe_data_length) > .2:\n        has_significant_amount_of_compressed_data = True\n\n    return has_significant_amount_of_compressed_data", "response": "Returns True if the PE file is likely packed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the signature for all the sections in a PE file.", "response": "def generate_section_signatures(self, pe, name, sig_length=512):\n        \"\"\"Generates signatures for all the sections in a PE file.\n\n        If the section contains any data a signature will be created\n        for it. The signature name will be a combination of the\n        parameter 'name' and the section number and its name.\n        \"\"\"\n\n        section_signatures = list()\n\n        for idx, section in enumerate(pe.sections):\n\n            if section.SizeOfRawData < sig_length:\n                continue\n\n            #offset = pe.get_offset_from_rva(section.VirtualAddress)\n            offset = section.PointerToRawData\n\n            sig_name = '%s Section(%d/%d,%s)' % (\n                name, idx + 1, len(pe.sections),\n                ''.join([c for c in section.Name if c in string.printable]))\n\n            section_signatures.append(\n                self.__generate_signature(\n                    pe, offset, sig_name, ep_only=False,\n                    section_start_only=True,\n                    sig_length=sig_length) )\n\n        return '\\n'.join(section_signatures)+'\\n'"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates a signature for the entry point of a PE file.", "response": "def generate_ep_signature(self, pe, name, sig_length=512):\n        \"\"\"Generate signatures for the entry point of a PE file.\n\n        Creates a signature whose name will be the parameter 'name'\n        and the section number and its name.\n        \"\"\"\n\n        offset = pe.get_offset_from_rva(pe.OPTIONAL_HEADER.AddressOfEntryPoint)\n\n        return self.__generate_signature(\n            pe, offset, name, ep_only=True, sig_length=sig_length)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef match(self, pe, ep_only=True, section_start_only=False):\n\n        matches = self.__match(pe, ep_only, section_start_only)\n\n        # The last match (the most precise) from the\n        # list of matches (if any) is returned\n        #\n        if matches:\n            if ep_only == False:\n                # Get the most exact match for each list of matches\n                # at a given offset\n                #\n                return [(match[0], match[1][-1]) for match in matches]\n\n            return matches[1][-1]\n\n        return None", "response": "Matches and returns the exact match ( es."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef __match_signature_tree(self, signature_tree, data, depth = 0):\n\n\n        matched_names = list ()\n        match = signature_tree\n\n        # Walk the bytes in the data and match them\n        # against the signature\n        #\n        for idx, byte in enumerate ( [b if isinstance(b, int) else ord(b) for b in data] ):\n\n            # If the tree is exhausted...\n            #\n            if match is None :\n                break\n\n            # Get the next byte in the tree\n            #\n            match_next = match.get(byte, None)\n\n\n            # If None is among the values for the key\n            # it means that a signature in the database\n            # ends here and that there's an exact match.\n            #\n            if None in list(match.values()):\n                # idx represent how deep we are in the tree\n                #\n                #names = [idx+depth]\n                names = list()\n\n                # For each of the item pairs we check\n                # if it has an element other than None,\n                # if not then we have an exact signature\n                #\n                for item in list(match.items()):\n                    if item[1] is None :\n                        names.append (item[0])\n                matched_names.append(names)\n\n            # If a wildcard is found keep scanning the signature\n            # ignoring the byte.\n            #\n            if '??' in match :\n                match_tree_alternate = match.get ('??', None)\n                data_remaining = data[idx + 1 :]\n                if data_remaining:\n                    matched_names.extend(\n                        self.__match_signature_tree(\n                            match_tree_alternate, data_remaining, idx+depth+1))\n\n            match = match_next\n\n        # If we have any more packer name in the end of the signature tree\n        # add them to the matches\n        #\n        if match is not None and None in list(match.values()):\n            #names = [idx + depth + 1]\n            names = list()\n            for item in list(match.items()) :\n                if item[1] is None:\n                    names.append(item[0])\n            matched_names.append(names)\n\n        return matched_names", "response": "Recursive function to find matches along the signature tree."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a PEiD signature file.", "response": "def load(self , filename=None, data=None):\n        \"\"\"Load a PEiD signature file.\n\n        Invoking this method on different files combines the signatures.\n        \"\"\"\n\n        self.__load(filename=filename, data=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_doc():\n    if sys.version_info.major == 2:\n        with open('pefile.py', 'r') as f:\n            tree = ast.parse(f.read())\n    else:\n        with open('pefile.py', 'r', encoding='utf-8') as f:\n            tree = ast.parse(f.read())\n    return ast.get_docstring(tree)", "response": "Parse docstring from file pefile. py and avoid importing\n    this module directly."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_attr(attr_name):\n    regex = attr_name + r\"\\s+=\\s+'(.+)'\"\n    if sys.version_info.major == 2:\n        with open('pefile.py', 'r') as f:\n            match = re.search(regex, f.read())\n    else:\n        with open('pefile.py', 'r', encoding='utf-8') as f:\n            match = re.search(regex, f.read())\n    # Second item in the group is the value of attribute.\n    return match.group(1)", "response": "Parse attribute from file pefile. py and avoid importing\n    this module directly."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ordLookup(libname, ord_val, make_name=False):\n    '''\n    Lookup a name for the given ordinal if it's in our\n    database.\n    '''\n    names = ords.get(libname.lower())\n    if names is None:\n        if make_name is True:\n            return formatOrdString(ord_val)\n        return None\n    name = names.get(ord_val)\n    if name is None:\n        return formatOrdString(ord_val)\n    return name", "response": "Lookup a name for the given ordinal if it s in our\n    database."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread the flags from a dictionary and return them in a usable form.", "response": "def retrieve_flags(flag_dict, flag_filter):\n    \"\"\"Read the flags from a dictionary and return them in a usable form.\n\n    Will return a list of (flag, value) for all flags in \"flag_dict\"\n    matching the filter \"flag_filter\".\n    \"\"\"\n\n    return [(flag, value) for flag, value in list(flag_dict.items()) if\n            isinstance(flag, (str, bytes)) and flag.startswith(flag_filter)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning True if the next RVA is the one immediately following this one.", "response": "def ask_unicode_16(self, next_rva_ptr):\n        \"\"\"The next RVA is taken to be the one immediately following this one.\n\n        Such RVA could indicate the natural end of the string and will be checked\n        to see if there's a Unicode NULL character there.\n        \"\"\"\n        if self.__get_word_value_at_rva(next_rva_ptr-2) == 0:\n            self.length = next_rva_ptr - self.rva_ptr\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_lines(self, txt, indent=0):\n        for line in txt:\n            self.add_line(line, indent)", "response": "Adds a list of lines."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add(self, txt, indent=0):\n        self.text.append(u'{0}{1}'.format(' '*indent, txt))", "response": "Adds some text to the log."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_text(self):\n        return u''.join(u'{0}'.format(b) for b in self.text)", "response": "Get the text in its current state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump(self, indentation=0):\n\n        dump = []\n\n        dump.append('[{0}]'.format(self.name))\n\n        printable_bytes = [ord(i) for i in string.printable if i not in string.whitespace]\n\n        # Refer to the __set_format__ method for an explanation\n        # of the following construct.\n        for keys in self.__keys__:\n            for key in keys:\n\n                val = getattr(self, key)\n                if isinstance(val, (int, long)):\n                    if key.startswith('Signature_'):\n                        val_str = '%-8X' % (val)\n                    else:\n                        val_str = '0x%-8X' % (val)\n                    if key == 'TimeDateStamp' or key == 'dwTimeStamp':\n                        try:\n                            val_str += ' [%s UTC]' % time.asctime(time.gmtime(val))\n                        except ValueError as e:\n                            val_str += ' [INVALID TIME]'\n                else:\n                    val_str = bytearray(val)\n                    if key.startswith('Signature'):\n                        val_str = ''.join(\n                            ['{:02X}'.format(i) for i in val_str.rstrip(b'\\x00')])\n                    else:\n                        val_str = ''.join(\n                            [chr(i) if (i in printable_bytes) else\n                             '\\\\x{0:02x}'.format(i) for i in val_str.rstrip(b'\\x00')])\n\n                dump.append('0x%-8X 0x%-3X %-30s %s' % (\n                    self.__field_offsets__[key] + self.__file_offset__,\n                    self.__field_offsets__[key], key+':', val_str))\n\n        return dump", "response": "Returns a string representation of the structure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dump_dict(self):\n\n        dump_dict = dict()\n\n        dump_dict['Structure'] = self.name\n\n        # Refer to the __set_format__ method for an explanation\n        # of the following construct.\n        for keys in self.__keys__:\n            for key in keys:\n\n                val = getattr(self, key)\n                if isinstance(val, (int, long)):\n                    if key == 'TimeDateStamp' or key == 'dwTimeStamp':\n                        try:\n                            val = '0x%-8X [%s UTC]' % (val, time.asctime(time.gmtime(val)))\n                        except ValueError as e:\n                            val = '0x%-8X [INVALID TIME]' % val\n                else:\n                    val = ''.join(chr(d) if chr(d) in string.printable\n                                  else \"\\\\x%02x\" % d for d in\n                                    [ord(c) if not isinstance(c, int) else c for c in val])\n\n                dump_dict[key] = {'FileOffset': self.__field_offsets__[key] + self.__file_offset__,\n                                  'Offset': self.__field_offsets__[key],\n                                  'Value': val}\n\n        return dump_dict", "response": "Returns a dictionary representation of the structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the data chunk from a section.", "response": "def get_data(self, start=None, length=None):\n        \"\"\"Get data chunk from a section.\n\n        Allows to query data from the section by passing the\n        addresses where the PE file would be loaded by default.\n        It is then possible to retrieve code and data by their real\n        addresses as they would be if loaded.\n\n        Returns bytes() under Python 3.x and set() under Python 2.7\n        \"\"\"\n\n        PointerToRawData_adj = self.pe.adjust_FileAlignment( self.PointerToRawData,\n            self.pe.OPTIONAL_HEADER.FileAlignment )\n        VirtualAddress_adj = self.pe.adjust_SectionAlignment( self.VirtualAddress,\n            self.pe.OPTIONAL_HEADER.SectionAlignment, self.pe.OPTIONAL_HEADER.FileAlignment )\n\n        if start is None:\n            offset = PointerToRawData_adj\n        else:\n            offset = ( start - VirtualAddress_adj ) + PointerToRawData_adj\n\n        if length is not None:\n            end = offset + length\n        else:\n            end = offset + self.SizeOfRawData\n        # PointerToRawData is not adjusted here as we might want to read any possible extra bytes\n        # that might get cut off by aligning the start (and hence cutting something off the end)\n        #\n        if end > self.PointerToRawData + self.SizeOfRawData:\n            end = self.PointerToRawData + self.SizeOfRawData\n        return self.pe.__data__[offset:end]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef contains_rva(self, rva):\n\n        # Check if the SizeOfRawData is realistic. If it's bigger than the size of\n        # the whole PE file minus the start address of the section it could be\n        # either truncated or the SizeOfRawData contains a misleading value.\n        # In either of those cases we take the VirtualSize\n        #\n        if len(self.pe.__data__) - self.pe.adjust_FileAlignment( self.PointerToRawData,\n            self.pe.OPTIONAL_HEADER.FileAlignment ) < self.SizeOfRawData:\n            # PECOFF documentation v8 says:\n            # VirtualSize: The total size of the section when loaded into memory.\n            # If this value is greater than SizeOfRawData, the section is zero-padded.\n            # This field is valid only for executable images and should be set to zero\n            # for object files.\n            #\n            size = self.Misc_VirtualSize\n        else:\n            size = max(self.SizeOfRawData, self.Misc_VirtualSize)\n\n        VirtualAddress_adj = self.pe.adjust_SectionAlignment( self.VirtualAddress,\n            self.pe.OPTIONAL_HEADER.SectionAlignment, self.pe.OPTIONAL_HEADER.FileAlignment )\n\n        # Check whether there's any section after the current one that starts before the\n        # calculated end for the current one. If so, cut the current section's size\n        # to fit in the range up to where the next section starts.\n        if (self.next_section_virtual_address is not None and\n            self.next_section_virtual_address > self.VirtualAddress and\n            VirtualAddress_adj + size > self.next_section_virtual_address):\n                size = self.next_section_virtual_address - VirtualAddress_adj\n\n        return VirtualAddress_adj <= rva < VirtualAddress_adj + size", "response": "Check whether the section contains the provided RVA."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the entropy of a chunk of data.", "response": "def entropy_H(self, data):\n        \"\"\"Calculate the entropy of a chunk of data.\"\"\"\n\n        if not data:\n            return 0.0\n\n        occurences = Counter(bytearray(data))\n\n        entropy = 0\n        for x in occurences.values():\n            p_x = float(x) / len(data)\n            entropy -= p_x*math.log(p_x, 2)\n\n        return entropy"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_rich_header(self):\n\n        # Rich Header constants\n        #\n        DANS = 0x536E6144 # 'DanS' as dword\n        RICH = 0x68636952 # 'Rich' as dword\n\n        rich_index = self.__data__.find(\n            b'Rich', 0x80, self.OPTIONAL_HEADER.get_file_offset())\n        if rich_index == -1:\n            return None\n\n        # Read a block of data\n        try:\n            # The end of the structure is 8 bytes after the start of the Rich\n            # string.\n            rich_data = self.get_data(0x80, rich_index + 8)\n            # Make the data have length a multiple of 4, otherwise the\n            # subsequent parsing will fail. It's not impossible that we retrieve\n            # truncated data that it's not a multiple.\n            rich_data = rich_data[:4*int(len(rich_data)/4)]\n            data = list(struct.unpack(\n                    '<{0}I'.format(int(len(rich_data)/4)), rich_data))\n            if RICH not in data:\n                return None\n        except PEFormatError:\n            return None\n\n        # get key, raw_data and clear_data\n        key = struct.pack('<L', data[data.index(RICH)+1])\n        result = {\"key\": key}\n\n        raw_data = rich_data[:rich_data.find(b'Rich')]\n        result[\"raw_data\"] = raw_data\n\n        ord_ = lambda c : ord(c) if not isinstance(c, int) else c\n\n        clear_data = bytearray()\n        for i in range(len(raw_data)):\n            clear_data.append((ord_(raw_data[i]) ^ ord_(key[i % len(key)])))\n        result[\"clear_data\"] = bytes(clear_data)\n\n        # the checksum should be present 3 times after the DanS signature\n        #\n        checksum = data[1]\n        if (data[0] ^ checksum != DANS\n            or data[2] != checksum\n            or data[3] != checksum):\n            return None\n\n        result[\"checksum\"] = checksum\n        headervalues = []\n        result[\"values\"] = headervalues\n\n        data = data[4:]\n        for i in range(int(len(data) / 2)):\n\n            # Stop until the Rich footer signature is found\n            #\n            if data[2 * i] == RICH:\n\n                # it should be followed by the checksum\n                #\n                if data[2 * i + 1] != checksum:\n                    self.__warnings.append('Rich Header is malformed')\n                break\n\n            # header values come by pairs\n            #\n            headervalues += [data[2 * i] ^ checksum, data[2 * i + 1] ^ checksum]\n        return result", "response": "Parses the Rich Header and returns a dict containing the key raw_data and clear_data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the data directories. This method will load the data directories which might not have been loaded if the \"fast_load\" option was used.", "response": "def full_load(self):\n        \"\"\"Process the data directories.\n\n        This method will load the data directories which might not have\n        been loaded if the \"fast_load\" option was used.\n        \"\"\"\n\n        self.parse_data_directories()\n\n        class RichHeader(object):\n            pass\n        rich_header = self.parse_rich_header()\n        if rich_header:\n            self.RICH_HEADER = RichHeader()\n            self.RICH_HEADER.checksum = rich_header.get('checksum', None)\n            self.RICH_HEADER.values = rich_header.get('values', None)\n            self.RICH_HEADER.key = rich_header.get('key', None)\n            self.RICH_HEADER.raw_data = rich_header.get('raw_data', None)\n            self.RICH_HEADER.clear_data = rich_header.get('clear_data', None)\n        else:\n            self.RICH_HEADER = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwriting the PE file. This function will process all headers and components of the PE file and include all changes made (by just assigning to attributes in the PE objects) and write the changes back to a file whose name is provided as an argument. The filename is optional, if not provided the data will be returned as a 'str' object.", "response": "def write(self, filename=None):\n        \"\"\"Write the PE file.\n\n        This function will process all headers and components\n        of the PE file and include all changes made (by just\n        assigning to attributes in the PE objects) and write\n        the changes back to a file whose name is provided as\n        an argument. The filename is optional, if not\n        provided the data will be returned as a 'str' object.\n        \"\"\"\n\n        file_data = bytearray(self.__data__)\n\n        for structure in self.__structures__:\n            struct_data = bytearray(structure.__pack__())\n            offset = structure.get_file_offset()\n            file_data[offset:offset+len(struct_data)] = struct_data\n\n        if hasattr(self, 'VS_VERSIONINFO'):\n            if hasattr(self, 'FileInfo'):\n                for finfo in self.FileInfo:\n                    for entry in finfo:\n                        if hasattr(entry, 'StringTable'):\n                            for st_entry in entry.StringTable:\n                                for key, entry in list(st_entry.entries.items()):\n\n                                    # Offsets and lengths of the keys and values.\n                                    # Each value in the dictionary is a tuple:\n                                    #  (key length, value length)\n                                    # The lengths are in characters, not in bytes.\n                                    offsets = st_entry.entries_offsets[key]\n                                    lengths = st_entry.entries_lengths[key]\n\n                                    if len( entry ) > lengths[1]:\n                                        l = entry.decode('utf-8').encode('utf-16le')\n                                        file_data[offsets[1]:offsets[1]+lengths[1]*2 ] = l[:lengths[1]*2]\n                                    else:\n                                        encoded_data = entry.decode('utf-8').encode('utf-16le')\n                                        file_data[offsets[1]:offsets[1]+len(encoded_data)] = encoded_data\n\n        new_file_data = file_data\n        if not filename:\n            return new_file_data\n\n        f = open(filename, 'wb+')\n        f.write(new_file_data)\n        f.close()\n        return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing the PE file sections.", "response": "def parse_sections(self, offset):\n        \"\"\"Fetch the PE file sections.\n\n        The sections will be readily available in the \"sections\" attribute.\n        Its attributes will contain all the section information plus \"data\"\n        a buffer containing the section's data.\n\n        The \"Characteristics\" member will be processed and attributes\n        representing the section characteristics (with the 'IMAGE_SCN_'\n        string trimmed from the constant's names) will be added to the\n        section instance.\n\n        Refer to the SectionStructure class for additional info.\n        \"\"\"\n\n        self.sections = []\n        MAX_SIMULTANEOUS_ERRORS = 3\n        for i in range(self.FILE_HEADER.NumberOfSections):\n            if i >= MAX_SECTIONS:\n                self.__warnings.append(\"Too many sections {0} (>={1})\".format(\n                    self.FILE_HEADER.NumberOfSections, MAX_SECTIONS))\n                break\n            simultaneous_errors = 0\n            section = SectionStructure( self.__IMAGE_SECTION_HEADER_format__, pe=self )\n            if not section:\n                break\n            section_offset = offset + section.sizeof() * i\n            section.set_file_offset(section_offset)\n            section_data = self.__data__[section_offset : section_offset + section.sizeof()]\n            # Check if the section is all nulls and stop if so.\n            if count_zeroes(section_data) == section.sizeof():\n                self.__warnings.append(\n                    'Invalid section {0}. Contents are null-bytes.'.format(i))\n                break\n            if not section_data:\n                self.__warnings.append(\n                    'Invalid section {0}. No data in the file (is this corkami\\'s virtsectblXP?).'.format(i))\n                break\n            section.__unpack__(section_data)\n            self.__structures__.append(section)\n\n            if section.SizeOfRawData+section.PointerToRawData > len(self.__data__):\n                simultaneous_errors += 1\n                self.__warnings.append(\n                    'Error parsing section {0}. SizeOfRawData is larger than file.'.format(i))\n\n            if self.adjust_FileAlignment( section.PointerToRawData,\n                self.OPTIONAL_HEADER.FileAlignment ) > len(self.__data__):\n                simultaneous_errors += 1\n                self.__warnings.append(\n                    'Error parsing section {0}. PointerToRawData points beyond the end of the file.'.format(i))\n\n            if section.Misc_VirtualSize > 0x10000000:\n                simultaneous_errors += 1\n                self.__warnings.append(\n                    'Suspicious value found parsing section {0}. VirtualSize is extremely large > 256MiB.'.format(i))\n\n            if self.adjust_SectionAlignment( section.VirtualAddress,\n                self.OPTIONAL_HEADER.SectionAlignment, self.OPTIONAL_HEADER.FileAlignment ) > 0x10000000:\n                simultaneous_errors += 1\n                self.__warnings.append(\n                    'Suspicious value found parsing section {0}. VirtualAddress is beyond 0x10000000.'.format(i))\n\n            if ( self.OPTIONAL_HEADER.FileAlignment != 0 and\n                ( section.PointerToRawData % self.OPTIONAL_HEADER.FileAlignment) != 0):\n                simultaneous_errors += 1\n                self.__warnings.append(\n                    ('Error parsing section {0}. '\n                    'PointerToRawData should normally be '\n                    'a multiple of FileAlignment, this might imply the file '\n                    'is trying to confuse tools which parse this incorrectly.').format(i))\n\n            if simultaneous_errors >= MAX_SIMULTANEOUS_ERRORS:\n                self.__warnings.append('Too many warnings parsing section. Aborting.')\n                break\n\n\n            section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')\n\n            # Set the section's flags according the the Characteristics member\n            set_flags(section, section.Characteristics, section_flags)\n\n            if ( section.__dict__.get('IMAGE_SCN_MEM_WRITE', False)  and\n                section.__dict__.get('IMAGE_SCN_MEM_EXECUTE', False) ):\n\n                if section.Name.rstrip(b'\\x00') == b'PAGE' and self.is_driver():\n                    # Drivers can have a PAGE section with those flags set without\n                    # implying that it is malicious\n                    pass\n                else:\n                    self.__warnings.append(\n                        ('Suspicious flags set for section %d. ' % i) +\n                        'Both IMAGE_SCN_MEM_WRITE and IMAGE_SCN_MEM_EXECUTE are set. '\n                        'This might indicate a packed executable.')\n\n\n            self.sections.append(section)\n\n        # Sort the sections by their VirtualAddress and add a field to each of them\n        # with the VirtualAddress of the next section. This will allow to check\n        # for potentially overlapping sections in badly constructed PEs.\n        self.sections.sort(key=lambda a: a.VirtualAddress)\n        for idx, section in enumerate(self.sections):\n            if idx == len(self.sections)-1:\n                section.next_section_virtual_address = None\n            else:\n                section.next_section_virtual_address = self.sections[idx+1].VirtualAddress\n\n        if self.FILE_HEADER.NumberOfSections > 0 and self.sections:\n            return offset + self.sections[0].sizeof()*self.FILE_HEADER.NumberOfSections\n        else:\n            return offset"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing and process the PE file s data directories.", "response": "def parse_data_directories(self, directories=None,\n                               forwarded_exports_only=False,\n                               import_dllnames_only=False):\n        \"\"\"Parse and process the PE file's data directories.\n\n        If the optional argument 'directories' is given, only\n        the directories at the specified indexes will be parsed.\n        Such functionality allows parsing of areas of interest\n        without the burden of having to parse all others.\n        The directories can then be specified as:\n\n        For export / import only:\n\n          directories = [ 0, 1 ]\n\n        or (more verbosely):\n\n          directories = [ DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT'],\n            DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_EXPORT'] ]\n\n        If 'directories' is a list, the ones that are processed will be removed,\n        leaving only the ones that are not present in the image.\n\n        If `forwarded_exports_only` is True, the IMAGE_DIRECTORY_ENTRY_EXPORT\n        attribute will only contain exports that are forwarded to another DLL.\n\n        If `import_dllnames_only` is True, symbols will not be parsed from\n        the import table and the entries in the IMAGE_DIRECTORY_ENTRY_IMPORT\n        attribute will not have a `symbols` attribute.\n        \"\"\"\n\n        directory_parsing = (\n            ('IMAGE_DIRECTORY_ENTRY_IMPORT', self.parse_import_directory),\n            ('IMAGE_DIRECTORY_ENTRY_EXPORT', self.parse_export_directory),\n            ('IMAGE_DIRECTORY_ENTRY_RESOURCE', self.parse_resources_directory),\n            ('IMAGE_DIRECTORY_ENTRY_DEBUG', self.parse_debug_directory),\n            ('IMAGE_DIRECTORY_ENTRY_BASERELOC', self.parse_relocations_directory),\n            ('IMAGE_DIRECTORY_ENTRY_TLS', self.parse_directory_tls),\n            ('IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG', self.parse_directory_load_config),\n            ('IMAGE_DIRECTORY_ENTRY_DELAY_IMPORT', self.parse_delay_import_directory),\n            ('IMAGE_DIRECTORY_ENTRY_BOUND_IMPORT', self.parse_directory_bound_imports) )\n\n        if directories is not None:\n            if not isinstance(directories, (tuple, list)):\n                directories = [directories]\n\n        for entry in directory_parsing:\n            # OC Patch:\n            #\n            try:\n                directory_index = DIRECTORY_ENTRY[entry[0]]\n                dir_entry = self.OPTIONAL_HEADER.DATA_DIRECTORY[directory_index]\n            except IndexError:\n                break\n\n            # Only process all the directories if no individual ones have\n            # been chosen\n            #\n            if directories is None or directory_index in directories:\n\n                if dir_entry.VirtualAddress:\n                    if forwarded_exports_only and entry[0] == 'IMAGE_DIRECTORY_ENTRY_EXPORT':\n                        value = entry[1](dir_entry.VirtualAddress, dir_entry.Size, forwarded_only=True)\n                    elif import_dllnames_only and entry[0] == 'IMAGE_DIRECTORY_ENTRY_IMPORT':\n                        value = entry[1](dir_entry.VirtualAddress, dir_entry.Size, dllnames_only=True)\n\n                    else:\n                        value = entry[1](dir_entry.VirtualAddress, dir_entry.Size)\n                    if value:\n                        setattr(self, entry[0][6:], value)\n\n            if (directories is not None) and isinstance(directories, list) and (entry[0] in directories):\n                directories.remove(directory_index)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_resource_data_entry(self, rva):\n\n        try:\n            # If the RVA is invalid all would blow up. Some EXEs seem to be\n            # specially nasty and have an invalid RVA.\n            data = self.get_data(rva, Structure(self.__IMAGE_RESOURCE_DATA_ENTRY_format__).sizeof() )\n        except PEFormatError as excp:\n            self.__warnings.append(\n                'Error parsing a resource directory data entry, '\n                'the RVA is invalid: 0x%x' % ( rva ) )\n            return None\n\n        data_entry = self.__unpack_data__(\n            self.__IMAGE_RESOURCE_DATA_ENTRY_format__, data,\n            file_offset = self.get_offset_from_rva(rva) )\n\n        return data_entry", "response": "Parse a data entry from the resources directory."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the version information structure.", "response": "def parse_version_information(self, version_struct):\n        \"\"\"Parse version information structure.\n\n        The date will be made available in three attributes of the PE object.\n\n        VS_VERSIONINFO     will contain the first three fields of the main structure:\n            'Length', 'ValueLength', and 'Type'\n\n        VS_FIXEDFILEINFO    will hold the rest of the fields, accessible as sub-attributes:\n            'Signature', 'StrucVersion', 'FileVersionMS', 'FileVersionLS',\n            'ProductVersionMS', 'ProductVersionLS', 'FileFlagsMask', 'FileFlags',\n            'FileOS', 'FileType', 'FileSubtype', 'FileDateMS', 'FileDateLS'\n\n        FileInfo    is a list of all StringFileInfo and VarFileInfo structures.\n\n        StringFileInfo structures will have a list as an attribute named 'StringTable'\n        containing all the StringTable structures. Each of those structures contains a\n        dictionary 'entries' with all the key / value version information string pairs.\n\n        VarFileInfo structures will have a list as an attribute named 'Var' containing\n        all Var structures. Each Var structure will have a dictionary as an attribute\n        named 'entry' which will contain the name and value of the Var.\n        \"\"\"\n\n\n        # Retrieve the data for the version info resource\n        #\n        try:\n            start_offset = self.get_offset_from_rva(version_struct.OffsetToData)\n        except PEFormatError as excp:\n            self.__warnings.append(\n                'Error parsing the version information, '\n                'attempting to read OffsetToData with RVA: 0x{:x}'.format(\n                    version_struct.OffsetToData))\n            return\n        raw_data = self.__data__[start_offset:start_offset+version_struct.Size]\n\n\n        # Map the main structure and the subsequent string\n        #\n        versioninfo_struct = self.__unpack_data__(\n            self.__VS_VERSIONINFO_format__, raw_data,\n            file_offset = start_offset )\n\n        if versioninfo_struct is None:\n            return\n\n        ustr_offset = version_struct.OffsetToData + versioninfo_struct.sizeof()\n        section = self.get_section_by_rva(ustr_offset)\n        section_end = None\n        if section:\n            section_end = section.VirtualAddress + max(\n                section.SizeOfRawData, section.Misc_VirtualSize)\n\n        versioninfo_string = None\n        # These should return 'ascii' decoded data. For the case when it's\n        # garbled data the ascii string will retain the byte values while\n        # encoding it to something else may yield values that don't match the\n        # file's contents.\n        try:\n            if section_end is None:\n                versioninfo_string = self.get_string_u_at_rva(\n                    ustr_offset, encoding='ascii')\n            else:\n                versioninfo_string = self.get_string_u_at_rva(\n                    ustr_offset, (section_end - ustr_offset) >> 1,\n                    encoding='ascii')\n        except PEFormatError as excp:\n            self.__warnings.append(\n                'Error parsing the version information, '\n                'attempting to read VS_VERSION_INFO string. Can\\'t '\n                'read unicode string at offset 0x%x' % (\n                ustr_offset))\n\n        if versioninfo_string == None:\n            self.__warnings.append('Invalid VS_VERSION_INFO block: {0}'.format(\n                versioninfo_string))\n            return\n\n        # If the structure does not contain the expected name, it's assumed to\n        # be invalid\n        if (versioninfo_string is not None and\n            versioninfo_string != b'VS_VERSION_INFO'):\n            if len(versioninfo_string) > 128:\n                excerpt = versioninfo_string[:128].decode('ascii')\n                # Don't leave any half-escaped characters\n                excerpt = excerpt[:excerpt.rfind('\\\\u')]\n                versioninfo_string = \\\n                    b('{0} ... ({1} bytes, too long to display)'.format(\n                        excerpt,\n                        len(versioninfo_string)))\n            self.__warnings.append('Invalid VS_VERSION_INFO block: {0}'.format(\n                versioninfo_string.decode('ascii').replace('\\00', '\\\\00')))\n            return\n\n        if not hasattr(self, 'VS_VERSIONINFO'):\n            self.VS_VERSIONINFO = list()\n\n        # Set the PE object's VS_VERSIONINFO to this one\n        vinfo = versioninfo_struct\n\n        # Set the Key attribute to point to the unicode string identifying the structure\n        vinfo.Key = versioninfo_string\n\n        self.VS_VERSIONINFO.append(vinfo)\n\n        if versioninfo_string is None:\n            versioninfo_string = ''\n        # Process the fixed version information, get the offset and structure\n        fixedfileinfo_offset = self.dword_align(\n            versioninfo_struct.sizeof() + 2 * (len(versioninfo_string) + 1),\n            version_struct.OffsetToData)\n        fixedfileinfo_struct = self.__unpack_data__(\n            self.__VS_FIXEDFILEINFO_format__,\n            raw_data[fixedfileinfo_offset:],\n            file_offset = start_offset+fixedfileinfo_offset )\n\n        if not fixedfileinfo_struct:\n            return\n\n        if not hasattr(self, 'VS_FIXEDFILEINFO'):\n            self.VS_FIXEDFILEINFO = list()\n\n        # Set the PE object's VS_FIXEDFILEINFO to this one\n        self.VS_FIXEDFILEINFO.append(fixedfileinfo_struct)\n\n        # Start parsing all the StringFileInfo and VarFileInfo structures\n\n        # Get the first one\n        stringfileinfo_offset = self.dword_align(\n            fixedfileinfo_offset + fixedfileinfo_struct.sizeof(),\n            version_struct.OffsetToData)\n        original_stringfileinfo_offset = stringfileinfo_offset\n\n        # Set the PE object's attribute that will contain them all.\n        if not hasattr(self, 'FileInfo'):\n            self.FileInfo = list()\n\n        finfo = list()\n        while True:\n\n            # Process the StringFileInfo/VarFileInfo structure\n            stringfileinfo_struct = self.__unpack_data__(\n                self.__StringFileInfo_format__,\n                raw_data[stringfileinfo_offset:],\n                file_offset = start_offset+stringfileinfo_offset )\n\n            if stringfileinfo_struct is None:\n                self.__warnings.append(\n                    'Error parsing StringFileInfo/VarFileInfo struct' )\n                return None\n\n            # Get the subsequent string defining the structure.\n            ustr_offset = ( version_struct.OffsetToData +\n                stringfileinfo_offset + versioninfo_struct.sizeof() )\n            try:\n                stringfileinfo_string = self.get_string_u_at_rva( ustr_offset )\n            except PEFormatError as excp:\n                self.__warnings.append(\n                    'Error parsing the version information, '\n                    'attempting to read StringFileInfo string. Can\\'t '\n                    'read unicode string at offset 0x{0:x}'.format(ustr_offset))\n                break\n\n            # Set such string as the Key attribute\n            stringfileinfo_struct.Key = stringfileinfo_string\n\n\n            # Append the structure to the PE object's list\n            finfo.append(stringfileinfo_struct)\n\n\n            # Parse a StringFileInfo entry\n            if stringfileinfo_string and stringfileinfo_string.startswith(b'StringFileInfo'):\n\n                if stringfileinfo_struct.Type in (0,1) and stringfileinfo_struct.ValueLength == 0:\n\n                    stringtable_offset = self.dword_align(\n                        stringfileinfo_offset + stringfileinfo_struct.sizeof() +\n                            2*(len(stringfileinfo_string)+1),\n                        version_struct.OffsetToData)\n\n                    stringfileinfo_struct.StringTable = list()\n\n                    # Process the String Table entries\n                    while True:\n\n                        stringtable_struct = self.__unpack_data__(\n                            self.__StringTable_format__,\n                            raw_data[stringtable_offset:],\n                            file_offset = start_offset+stringtable_offset )\n\n                        if not stringtable_struct:\n                            break\n\n                        ustr_offset = ( version_struct.OffsetToData + stringtable_offset +\n                            stringtable_struct.sizeof() )\n                        try:\n                            stringtable_string = self.get_string_u_at_rva(ustr_offset)\n                        except PEFormatError as excp:\n                            self.__warnings.append(\n                                'Error parsing the version information, '\n                                'attempting to read StringTable string. Can\\'t '\n                                'read unicode string at offset 0x{0:x}'.format(ustr_offset) )\n                            break\n\n                        stringtable_struct.LangID = stringtable_string\n                        stringtable_struct.entries = dict()\n                        stringtable_struct.entries_offsets = dict()\n                        stringtable_struct.entries_lengths = dict()\n                        stringfileinfo_struct.StringTable.append(stringtable_struct)\n\n                        entry_offset = self.dword_align(\n                            stringtable_offset + stringtable_struct.sizeof() +\n                                2*(len(stringtable_string)+1),\n                            version_struct.OffsetToData)\n\n                        # Process all entries in the string table\n\n                        while entry_offset < stringtable_offset + stringtable_struct.Length:\n\n                            string_struct = self.__unpack_data__(\n                                self.__String_format__, raw_data[entry_offset:],\n                                file_offset = start_offset+entry_offset )\n\n                            if not string_struct:\n                                break\n\n                            ustr_offset = ( version_struct.OffsetToData + entry_offset +\n                                string_struct.sizeof() )\n                            try:\n                                key = self.get_string_u_at_rva( ustr_offset )\n                                key_offset = self.get_offset_from_rva( ustr_offset )\n                            except PEFormatError as excp:\n                                self.__warnings.append(\n                                    'Error parsing the version information, '\n                                    'attempting to read StringTable Key string. Can\\'t '\n                                    'read unicode string at offset 0x{0:x}'.format(ustr_offset))\n                                break\n\n                            value_offset = self.dword_align(\n                                2*(len(key)+1) + entry_offset + string_struct.sizeof(),\n                                version_struct.OffsetToData)\n\n                            ustr_offset = version_struct.OffsetToData + value_offset\n                            try:\n                                value = self.get_string_u_at_rva( ustr_offset,\n                                    max_length = string_struct.ValueLength )\n                                value_offset = self.get_offset_from_rva( ustr_offset )\n                            except PEFormatError as excp:\n                                self.__warnings.append(\n                                    'Error parsing the version information, '\n                                    'attempting to read StringTable Value string. '\n                                    'Can\\'t read unicode string at offset 0x{0:x}'.format(\n                                        ustr_offset))\n                                break\n\n                            if string_struct.Length == 0:\n                                entry_offset = stringtable_offset + stringtable_struct.Length\n                            else:\n                                entry_offset = self.dword_align(\n                                    string_struct.Length+entry_offset, version_struct.OffsetToData)\n\n                            stringtable_struct.entries[key] = value\n                            stringtable_struct.entries_offsets[key] = (key_offset, value_offset)\n                            stringtable_struct.entries_lengths[key] = (len(key), len(value))\n\n\n                        new_stringtable_offset = self.dword_align(\n                            stringtable_struct.Length + stringtable_offset,\n                            version_struct.OffsetToData)\n\n                        # Check if the entry is crafted in a way that would lead\n                        # to an infinite loop and break if so.\n                        if new_stringtable_offset == stringtable_offset:\n                            break\n                        stringtable_offset = new_stringtable_offset\n\n                        if stringtable_offset >= stringfileinfo_struct.Length:\n                            break\n\n            # Parse a VarFileInfo entry\n            elif stringfileinfo_string and stringfileinfo_string.startswith( b'VarFileInfo' ):\n\n                varfileinfo_struct = stringfileinfo_struct\n                varfileinfo_struct.name = 'VarFileInfo'\n\n                if varfileinfo_struct.Type in (0, 1) and varfileinfo_struct.ValueLength == 0:\n\n                    var_offset = self.dword_align(\n                        stringfileinfo_offset + varfileinfo_struct.sizeof() +\n                            2*(len(stringfileinfo_string)+1),\n                        version_struct.OffsetToData)\n\n                    varfileinfo_struct.Var = list()\n\n                    # Process all entries\n\n                    while True:\n                        var_struct = self.__unpack_data__(\n                            self.__Var_format__,\n                            raw_data[var_offset:],\n                            file_offset = start_offset+var_offset )\n\n                        if not var_struct:\n                            break\n\n                        ustr_offset = ( version_struct.OffsetToData + var_offset +\n                            var_struct.sizeof() )\n                        try:\n                            var_string = self.get_string_u_at_rva( ustr_offset )\n                        except PEFormatError as excp:\n                            self.__warnings.append(\n                                'Error parsing the version information, '\n                                'attempting to read VarFileInfo Var string. '\n                                'Can\\'t read unicode string at offset 0x{0:x}'.format(ustr_offset))\n                            break\n\n                        if var_string is None:\n                            break\n\n                        varfileinfo_struct.Var.append(var_struct)\n\n                        varword_offset = self.dword_align(\n                            2*(len(var_string)+1) + var_offset + var_struct.sizeof(),\n                            version_struct.OffsetToData)\n                        orig_varword_offset = varword_offset\n\n                        while varword_offset < orig_varword_offset + var_struct.ValueLength:\n                            word1 = self.get_word_from_data(\n                                raw_data[varword_offset:varword_offset+2], 0)\n                            word2 = self.get_word_from_data(\n                                raw_data[varword_offset+2:varword_offset+4], 0)\n                            varword_offset += 4\n\n                            if isinstance(word1, int) and isinstance(word2, int):\n                                var_struct.entry = {var_string: '0x%04x 0x%04x' % (word1, word2)}\n\n                        var_offset = self.dword_align(\n                            var_offset+var_struct.Length, version_struct.OffsetToData)\n\n                        if var_offset <= var_offset+var_struct.Length:\n                            break\n\n\n            # Increment and align the offset\n            stringfileinfo_offset = self.dword_align(\n                stringfileinfo_struct.Length+stringfileinfo_offset,\n                version_struct.OffsetToData)\n\n            # Check if all the StringFileInfo and VarFileInfo items have been processed\n            if stringfileinfo_struct.Length == 0 or stringfileinfo_offset >= versioninfo_struct.Length:\n                break\n\n        self.FileInfo.append(finfo)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the export directory.", "response": "def parse_export_directory(self, rva, size, forwarded_only=False):\n        \"\"\"Parse the export directory.\n\n        Given the RVA of the export directory, it will process all\n        its entries.\n\n        The exports will be made available as a list of ExportData\n        instances in the 'IMAGE_DIRECTORY_ENTRY_EXPORT' PE attribute.\n        \"\"\"\n\n        try:\n            export_dir =  self.__unpack_data__(\n                self.__IMAGE_EXPORT_DIRECTORY_format__,\n                self.get_data( rva, Structure(self.__IMAGE_EXPORT_DIRECTORY_format__).sizeof() ),\n                file_offset = self.get_offset_from_rva(rva) )\n        except PEFormatError:\n            self.__warnings.append(\n                'Error parsing export directory at RVA: 0x%x' % ( rva ) )\n            return\n\n        if not export_dir:\n            return\n\n        # We keep track of the bytes left in the file and use it to set a upper\n        # bound in the number of items that can be read from the different\n        # arrays.\n        def length_until_eof(rva):\n            return len(self.__data__) - self.get_offset_from_rva(rva)\n\n        try:\n            address_of_names = self.get_data(\n                export_dir.AddressOfNames,\n                min(length_until_eof(export_dir.AddressOfNames),\n                    export_dir.NumberOfNames*4))\n            address_of_name_ordinals = self.get_data(\n                export_dir.AddressOfNameOrdinals,\n                min(length_until_eof(export_dir.AddressOfNameOrdinals),\n                    export_dir.NumberOfNames*4))\n            address_of_functions = self.get_data(\n                export_dir.AddressOfFunctions,\n                min(length_until_eof(export_dir.AddressOfFunctions),\n                    export_dir.NumberOfFunctions*4))\n        except PEFormatError:\n            self.__warnings.append(\n                'Error parsing export directory at RVA: 0x%x' % ( rva ) )\n            return\n\n        exports = []\n\n        max_failed_entries_before_giving_up = 10\n\n        section = self.get_section_by_rva(export_dir.AddressOfNames)\n        # Overly generous upper bound\n        safety_boundary = len(self.__data__)\n        if section:\n            safety_boundary = (\n                section.VirtualAddress + len(section.get_data()) -\n                export_dir.AddressOfNames)\n\n        symbol_counts = collections.defaultdict(int)\n        export_parsing_loop_completed_normally = True\n        for i in range(min(export_dir.NumberOfNames, int(safety_boundary / 4))):\n            symbol_ordinal = self.get_word_from_data(\n                address_of_name_ordinals, i)\n\n            if (symbol_ordinal is not None and\n                symbol_ordinal*4 < len(address_of_functions)):\n                symbol_address = self.get_dword_from_data(\n                    address_of_functions, symbol_ordinal)\n            else:\n                # Corrupt? a bad pointer... we assume it's all\n                # useless, no exports\n                return None\n            if symbol_address is None or symbol_address == 0:\n                continue\n\n            # If the function's RVA points within the export directory\n            # it will point to a string with the forwarded symbol's string\n            # instead of pointing the the function start address.\n            if symbol_address >= rva and symbol_address < rva+size:\n                forwarder_str = self.get_string_at_rva(symbol_address)\n                try:\n                    forwarder_offset = self.get_offset_from_rva( symbol_address )\n                except PEFormatError:\n                    continue\n            else:\n                if forwarded_only:\n                    continue\n                forwarder_str = None\n                forwarder_offset = None\n\n            symbol_name_address = self.get_dword_from_data(address_of_names, i)\n            if symbol_name_address is None:\n                max_failed_entries_before_giving_up -= 1\n                if max_failed_entries_before_giving_up <= 0:\n                    export_parsing_loop_completed_normally = False\n                    break\n\n            symbol_name = self.get_string_at_rva(symbol_name_address, MAX_SYMBOL_NAME_LENGTH)\n            if not is_valid_function_name(symbol_name):\n                export_parsing_loop_completed_normally = False\n                break\n            try:\n                symbol_name_offset = self.get_offset_from_rva(symbol_name_address)\n            except PEFormatError:\n                max_failed_entries_before_giving_up -= 1\n                if max_failed_entries_before_giving_up <= 0:\n                    export_parsing_loop_completed_normally = False\n                    break\n                try:\n                    symbol_name_offset = self.get_offset_from_rva( symbol_name_address )\n                except PEFormatError:\n                    max_failed_entries_before_giving_up -= 1\n                    if max_failed_entries_before_giving_up <= 0:\n                        export_parsing_loop_completed_normally = False\n                        break\n                    continue\n\n            # File 0b1d3d3664915577ab9a32188d29bbf3542b86c7b9ce333e245496c3018819f1\n            # was being parsed as potentially containing millions of exports.\n            # Checking for duplicates addresses the issue.\n            symbol_counts[(symbol_name, symbol_address)] += 1\n            if symbol_counts[(symbol_name, symbol_address)] > 10:\n                self.__warnings.append(\n                    'Export directory contains more than 10 repeated entries '\n                    '({:s}, 0x{:x}). Assuming corrupt.'.format(\n                        symbol_name, symbol_address))\n                break\n            elif len(symbol_counts) > MAX_SYMBOL_EXPORT_COUNT:\n                self.__warnings.append(\n                    'Export directory contains more than {} symbol entries. '\n                    'Assuming corrupt.'.format(\n                        MAX_SYMBOL_EXPORT_COUNT))\n                break\n\n            exports.append(\n                ExportData(\n                    pe = self,\n                    ordinal = export_dir.Base+symbol_ordinal,\n                    ordinal_offset = self.get_offset_from_rva( export_dir.AddressOfNameOrdinals + 2*i ),\n                    address = symbol_address,\n                    address_offset = self.get_offset_from_rva( export_dir.AddressOfFunctions + 4*symbol_ordinal ),\n                    name = symbol_name,\n                    name_offset = symbol_name_offset,\n                    forwarder = forwarder_str,\n                    forwarder_offset = forwarder_offset ))\n\n        if not export_parsing_loop_completed_normally:\n            self.__warnings.append(\n                'RVA AddressOfNames in the export directory points to an invalid address: %x' %\n                export_dir.AddressOfNames)\n\n        ordinals = {exp.ordinal for exp in exports}\n\n        max_failed_entries_before_giving_up = 10\n\n        section = self.get_section_by_rva(export_dir.AddressOfFunctions)\n        # Overly generous upper bound\n        safety_boundary = len(self.__data__)\n        if section:\n            safety_boundary = (\n                section.VirtualAddress + len(section.get_data()) -\n                export_dir.AddressOfFunctions)\n\n        symbol_counts = collections.defaultdict(int)\n        export_parsing_loop_completed_normally = True\n        for idx in range(min(\n                export_dir.NumberOfFunctions,\n                int(safety_boundary / 4))):\n\n            if not idx+export_dir.Base in ordinals:\n                try:\n                    symbol_address = self.get_dword_from_data(\n                        address_of_functions, idx)\n                except PEFormatError:\n                    symbol_address = None\n\n                if symbol_address is None:\n                    max_failed_entries_before_giving_up -= 1\n                    if max_failed_entries_before_giving_up <= 0:\n                        export_parsing_loop_completed_normally = False\n                        break\n\n                if symbol_address == 0:\n                    continue\n\n                # Checking for forwarder again.\n                if symbol_address is not None and symbol_address >= rva and symbol_address < rva+size:\n                    forwarder_str = self.get_string_at_rva(symbol_address)\n                else:\n                    forwarder_str = None\n\n                # File 0b1d3d3664915577ab9a32188d29bbf3542b86c7b9ce333e245496c3018819f1\n                # was being parsed as potentially containing millions of exports.\n                # Checking for duplicates addresses the issue.\n                symbol_counts[symbol_address] += 1\n                if symbol_counts[symbol_address] > 10:\n                # if most_common and most_common[0][1] > 10:\n                    self.__warnings.append(\n                        'Export directory contains more than 10 repeated '\n                        'ordinal entries (0x{:x}). Assuming corrupt.'.format(\n                            symbol_address))\n                    break\n                elif len(symbol_counts) > MAX_SYMBOL_EXPORT_COUNT:\n                    self.__warnings.append(\n                        'Export directory contains more than {} ordinal entries. Assuming corrupt.'.format(\n                            MAX_SYMBOL_EXPORT_COUNT))\n                    break\n\n                exports.append(\n                    ExportData(\n                        ordinal = export_dir.Base+idx,\n                        address = symbol_address,\n                        name = None,\n                        forwarder = forwarder_str))\n\n        if not export_parsing_loop_completed_normally:\n            self.__warnings.append(\n                'RVA AddressOfFunctions in the export directory points to an invalid address: %x' %\n                export_dir.AddressOfFunctions)\n            return\n\n        if not exports and export_dir.all_zeroes():\n            return None\n        return ExportDirData(struct=export_dir, symbols=exports,\n                             name=self.get_string_at_rva(export_dir.Name))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwalks and parse the delay import directory.", "response": "def parse_delay_import_directory(self, rva, size):\n        \"\"\"Walk and parse the delay import directory.\"\"\"\n\n        import_descs =  []\n        error_count = 0\n        while True:\n            try:\n                # If the RVA is invalid all would blow up. Some PEs seem to be\n                # specially nasty and have an invalid RVA.\n                data = self.get_data( rva, Structure(self.__IMAGE_DELAY_IMPORT_DESCRIPTOR_format__).sizeof() )\n            except PEFormatError as e:\n                self.__warnings.append(\n                    'Error parsing the Delay import directory at RVA: 0x%x' % ( rva ) )\n                break\n\n            file_offset = self.get_offset_from_rva(rva)\n            import_desc =  self.__unpack_data__(\n                self.__IMAGE_DELAY_IMPORT_DESCRIPTOR_format__,\n                data, file_offset = file_offset )\n\n\n            # If the structure is all zeros, we reached the end of the list\n            if not import_desc or import_desc.all_zeroes():\n                break\n\n\n            rva += import_desc.sizeof()\n\n            # If the array of thunks is somewhere earlier than the import\n            # descriptor we can set a maximum length for the array. Otherwise\n            # just set a maximum length of the size of the file\n            max_len = len(self.__data__) - file_offset\n            if rva > import_desc.pINT or rva > import_desc.pIAT:\n                max_len = max(rva-import_desc.pINT, rva-import_desc.pIAT)\n\n            import_data = []\n            try:\n                import_data =  self.parse_imports(\n                    import_desc.pINT,\n                    import_desc.pIAT,\n                    None,\n                    max_length = max_len)\n            except PEFormatError as e:\n                self.__warnings.append(\n                    'Error parsing the Delay import directory. '\n                    'Invalid import data at RVA: 0x{0:x} ({1})'.format(\n                        rva, e.value))\n\n            if error_count > 5:\n                self.__warnings.append(\n                    'Too may errors parsing the Delay import directory. '\n                    'Invalid import data at RVA: 0x{0:x}'.format(rva) )\n                break\n\n            if not import_data:\n                error_count += 1\n                continue\n\n            dll = self.get_string_at_rva(import_desc.szName, MAX_DLL_LENGTH)\n            if not is_valid_dos_filename(dll):\n                dll = b('*invalid*')\n\n            if dll:\n                for symbol in import_data:\n                    if symbol.name is None:\n                        funcname = ordlookup.ordLookup(dll.lower(), symbol.ordinal)\n                        if funcname:\n                            symbol.name = funcname\n                import_descs.append(\n                    ImportDescData(\n                        struct = import_desc,\n                        imports = import_data,\n                        dll = dll))\n\n        return import_descs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwalks and parse the import directory.", "response": "def parse_import_directory(self, rva, size, dllnames_only=False):\n        \"\"\"Walk and parse the import directory.\"\"\"\n\n        import_descs =  []\n        error_count = 0\n        while True:\n            try:\n                # If the RVA is invalid all would blow up. Some EXEs seem to be\n                # specially nasty and have an invalid RVA.\n                data = self.get_data(rva, Structure(\n                        self.__IMAGE_IMPORT_DESCRIPTOR_format__).sizeof() )\n            except PEFormatError as e:\n                self.__warnings.append(\n                    'Error parsing the import directory at RVA: 0x%x' % ( rva ) )\n                break\n\n            file_offset = self.get_offset_from_rva(rva)\n            import_desc =  self.__unpack_data__(\n                self.__IMAGE_IMPORT_DESCRIPTOR_format__,\n                data, file_offset = file_offset )\n\n            # If the structure is all zeros, we reached the end of the list\n            if not import_desc or import_desc.all_zeroes():\n                break\n\n            rva += import_desc.sizeof()\n\n            # If the array of thunks is somewhere earlier than the import\n            # descriptor we can set a maximum length for the array. Otherwise\n            # just set a maximum length of the size of the file\n            max_len = len(self.__data__) - file_offset\n            if rva > import_desc.OriginalFirstThunk or rva > import_desc.FirstThunk:\n                max_len = max(rva-import_desc.OriginalFirstThunk, rva-import_desc.FirstThunk)\n\n            import_data = []\n            if not dllnames_only:\n                try:\n                    import_data = self.parse_imports(\n                        import_desc.OriginalFirstThunk,\n                        import_desc.FirstThunk,\n                        import_desc.ForwarderChain,\n                        max_length = max_len)\n                except PEFormatError as e:\n                    self.__warnings.append(\n                        'Error parsing the import directory. '\n                        'Invalid Import data at RVA: 0x{0:x} ({1})'.format(\n                            rva, e.value))\n\n                if error_count > 5:\n                    self.__warnings.append(\n                        'Too may errors parsing the import directory. '\n                        'Invalid import data at RVA: 0x{0:x}'.format(rva) )\n                    break\n\n                if not import_data:\n                    error_count += 1\n                    # TODO: do not continue here\n                    continue\n\n            dll = self.get_string_at_rva(import_desc.Name, MAX_DLL_LENGTH)\n            if not is_valid_dos_filename(dll):\n                dll = b('*invalid*')\n\n            if dll:\n                for symbol in import_data:\n                    if symbol.name is None:\n                        funcname = ordlookup.ordLookup(dll.lower(), symbol.ordinal)\n                        if funcname:\n                            symbol.name = funcname\n                import_descs.append(\n                    ImportDescData(\n                        struct = import_desc,\n                        imports = import_data,\n                        dll = dll))\n\n        if not dllnames_only:\n            suspicious_imports = set([ u'LoadLibrary', u'GetProcAddress' ])\n            suspicious_imports_count = 0\n            total_symbols = 0\n            for imp_dll in import_descs:\n                for symbol in imp_dll.imports:\n                    for suspicious_symbol in suspicious_imports:\n                        if not symbol or not symbol.name:\n                            continue\n                        name = symbol.name\n                        if type(symbol.name) == bytes:\n                            name = symbol.name.decode('utf-8')\n                        if name.startswith(suspicious_symbol):\n                            suspicious_imports_count += 1\n                            break\n                    total_symbols += 1\n            if suspicious_imports_count == len(suspicious_imports) and total_symbols < 20:\n                self.__warnings.append(\n                    'Imported symbols contain entries typical of packed executables.' )\n\n        return import_descs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_memory_mapped_image(self, max_virtual_address=0x10000000, ImageBase=None):\n\n        # Rebase if requested\n        #\n        if ImageBase is not None:\n\n            # Keep a copy of the image's data before modifying it by rebasing it\n            #\n            original_data = self.__data__\n\n            self.relocate_image(ImageBase)\n\n        # Collect all sections in one code block\n        mapped_data = self.__data__[:]\n        for section in self.sections:\n\n            # Miscellaneous integrity tests.\n            # Some packer will set these to bogus values to make tools go nuts.\n            if section.Misc_VirtualSize == 0 and section.SizeOfRawData == 0:\n                continue\n\n            srd = section.SizeOfRawData\n            prd = self.adjust_FileAlignment(\n                section.PointerToRawData, self.OPTIONAL_HEADER.FileAlignment)\n            VirtualAddress_adj = self.adjust_SectionAlignment(\n                section.VirtualAddress,\n                self.OPTIONAL_HEADER.SectionAlignment,\n                self.OPTIONAL_HEADER.FileAlignment )\n\n            if (srd > len(self.__data__) or\n                prd > len(self.__data__) or\n                srd + prd > len(self.__data__) or\n                VirtualAddress_adj >= max_virtual_address):\n                continue\n\n            padding_length = VirtualAddress_adj - len(mapped_data)\n\n            if padding_length>0:\n                mapped_data += b'\\0'*padding_length\n            elif padding_length<0:\n                mapped_data = mapped_data[:padding_length]\n\n            mapped_data += section.get_data()\n\n        # If the image was rebased, restore it to its original form\n        #\n        if ImageBase is not None:\n            self.__data__ = original_data\n\n        return mapped_data", "response": "Returns the data corresponding to the memory layout of the PE file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the bytes from the data at the specified offset.", "response": "def get_bytes_from_data(self, offset, data):\n        \"\"\".\"\"\"\n        if offset > len(data):\n            return b''\n        d = data[offset:]\n        if isinstance(d, bytearray):\n            return bytes(d)\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget an ASCII string from data.", "response": "def get_string_from_data(self, offset, data):\n        \"\"\"Get an ASCII string from data.\"\"\"\n        s = self.get_bytes_from_data(offset, data)\n        end = s.find(b'\\0')\n        if end >= 0:\n            s = s[:end]\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_string_u_at_rva(self, rva, max_length = 2**16, encoding=None):\n\n        # If the RVA is invalid let the exception reach the callers. All\n        # call-sites of get_string_u_at_rva() will handle it.\n        data = self.get_data(rva, 2)\n        # max_length is the maximum count of 16bit characters needs to be\n        # doubled to get size in bytes\n        max_length <<= 1\n\n        requested = min(max_length, 256)\n        data = self.get_data(rva, requested)\n        # try to find null-termination\n        null_index = -1\n        while True:\n            null_index = data.find(b'\\x00\\x00', null_index + 1)\n            if null_index == -1:\n                data_length = len(data)\n                if data_length < requested or data_length == max_length:\n                    null_index = len(data) >> 1\n                    break\n                else:\n                    # Request remaining part of data limited by max_length\n                    data += self.get_data(rva + data_length, max_length - data_length)\n                    null_index = requested - 1\n                    requested = max_length\n\n            elif null_index % 2 == 0:\n                null_index >>= 1\n                break\n\n        # convert selected part of the string to unicode\n        uchrs = struct.unpack('<{:d}H'.format(null_index), data[:null_index * 2])\n        s = u''.join(map(chr, uchrs))\n\n        if encoding:\n            return b(s.encode(encoding, 'backslashreplace_'))\n\n        return b(s.encode('utf-8', 'backslashreplace_'))", "response": "Get an Unicode string located at the given RVA."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the section containing the given file offset.", "response": "def get_section_by_offset(self, offset):\n        \"\"\"Get the section containing the given file offset.\"\"\"\n\n        sections = [s for s in self.sections if s.contains_offset(offset)]\n\n        if sections:\n            return sections[0]\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_section_by_rva(self, rva):\n\n        for section in self.sections:\n            if section.contains_rva(rva):\n                return section\n\n        return None", "response": "Get the section containing the given RVA."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef dump_info(self, dump=None, encoding='ascii'):\n\n        if dump is None:\n            dump = Dump()\n\n        warnings = self.get_warnings()\n        if warnings:\n            dump.add_header('Parsing Warnings')\n            for warning in warnings:\n                dump.add_line(warning)\n                dump.add_newline()\n\n\n        dump.add_header('DOS_HEADER')\n        dump.add_lines(self.DOS_HEADER.dump())\n        dump.add_newline()\n\n        dump.add_header('NT_HEADERS')\n        dump.add_lines(self.NT_HEADERS.dump())\n        dump.add_newline()\n\n        dump.add_header('FILE_HEADER')\n        dump.add_lines(self.FILE_HEADER.dump())\n\n        image_flags = retrieve_flags(IMAGE_CHARACTERISTICS, 'IMAGE_FILE_')\n\n        dump.add('Flags: ')\n        flags = []\n        for flag in sorted(image_flags):\n            if getattr(self.FILE_HEADER, flag[0]):\n                flags.append(flag[0])\n        dump.add_line(', '.join(flags))\n        dump.add_newline()\n\n        if hasattr(self, 'OPTIONAL_HEADER') and self.OPTIONAL_HEADER is not None:\n            dump.add_header('OPTIONAL_HEADER')\n            dump.add_lines(self.OPTIONAL_HEADER.dump())\n\n        dll_characteristics_flags = retrieve_flags(DLL_CHARACTERISTICS, 'IMAGE_DLLCHARACTERISTICS_')\n\n        dump.add('DllCharacteristics: ')\n        flags = []\n        for flag in sorted(dll_characteristics_flags):\n            if getattr(self.OPTIONAL_HEADER, flag[0]):\n                flags.append(flag[0])\n        dump.add_line(', '.join(flags))\n        dump.add_newline()\n\n\n        dump.add_header('PE Sections')\n\n        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')\n\n        for section in self.sections:\n            dump.add_lines(section.dump())\n            dump.add('Flags: ')\n            flags = []\n            for flag in sorted(section_flags):\n                if getattr(section, flag[0]):\n                    flags.append(flag[0])\n            dump.add_line(', '.join(flags))\n            dump.add_line('Entropy: {0:f} (Min=0.0, Max=8.0)'.format(\n                section.get_entropy()))\n            if md5 is not None:\n                dump.add_line('MD5     hash: {0}'.format(\n                    section.get_hash_md5()))\n            if sha1 is not None:\n                dump.add_line('SHA-1   hash: %s' % section.get_hash_sha1() )\n            if sha256 is not None:\n                dump.add_line('SHA-256 hash: %s' % section.get_hash_sha256() )\n            if sha512 is not None:\n                dump.add_line('SHA-512 hash: %s' % section.get_hash_sha512() )\n            dump.add_newline()\n\n\n\n        if (hasattr(self, 'OPTIONAL_HEADER') and\n            hasattr(self.OPTIONAL_HEADER, 'DATA_DIRECTORY') ):\n\n            dump.add_header('Directories')\n            for idx in range(len(self.OPTIONAL_HEADER.DATA_DIRECTORY)):\n                directory = self.OPTIONAL_HEADER.DATA_DIRECTORY[idx]\n                dump.add_lines(directory.dump())\n            dump.add_newline()\n\n        if hasattr(self, 'VS_VERSIONINFO'):\n            for idx in range(len(self.VS_VERSIONINFO)):\n                if len(self.VS_VERSIONINFO) > 1:\n                    dump.add_header('Version Information {:d}'.format(idx + 1))\n                else:\n                    dump.add_header('Version Information')\n                dump.add_lines(self.VS_VERSIONINFO[idx].dump())\n                dump.add_newline()\n\n                if hasattr(self, 'VS_FIXEDFILEINFO'):\n                    dump.add_lines(self.VS_FIXEDFILEINFO[idx].dump())\n                    dump.add_newline()\n\n                if hasattr(self, 'FileInfo') and len(self.FileInfo) > idx:\n                    for entry in self.FileInfo[idx]:\n                        dump.add_lines(entry.dump())\n                        dump.add_newline()\n\n                        if hasattr(entry, 'StringTable'):\n                            for st_entry in entry.StringTable:\n                                [dump.add_line(u'  '+line) for line in st_entry.dump()]\n                                dump.add_line(u'  LangID: {0}'.format(\n                                    st_entry.LangID.decode(encoding, 'backslashreplace_')))\n                                dump.add_newline()\n                                for str_entry in sorted(list(st_entry.entries.items())):\n                                    # try:\n                                    dump.add_line( u'    {0}: {1}'.format(\n                                        str_entry[0].decode(encoding, 'backslashreplace_'),\n                                        str_entry[1].decode(encoding, 'backslashreplace_')))\n\n                            dump.add_newline()\n\n                        elif hasattr(entry, 'Var'):\n                            for var_entry in entry.Var:\n                                if hasattr(var_entry, 'entry'):\n                                    [dump.add_line('  '+line) for line in var_entry.dump()]\n                                    dump.add_line(\n                                        u'    {0}: {1}'.format(\n                                            list(var_entry.entry.keys())[0].decode(\n                                                'utf-8', 'backslashreplace_'),\n                                            list(var_entry.entry.values())[0]))\n\n                            dump.add_newline()\n\n        if hasattr(self, 'DIRECTORY_ENTRY_EXPORT'):\n            dump.add_header('Exported symbols')\n            dump.add_lines(self.DIRECTORY_ENTRY_EXPORT.struct.dump())\n            dump.add_newline()\n            dump.add_line(u'%-10s   %-10s  %s' % ('Ordinal', 'RVA', 'Name'))\n            for export in self.DIRECTORY_ENTRY_EXPORT.symbols:\n                if export.address is not None:\n                    name = b('None')\n                    if export.name:\n                        name = export.name\n                    dump.add(u'%-10d 0x%08X    %s' % (\n                        export.ordinal, export.address, name.decode(encoding)))\n                    if export.forwarder:\n                        dump.add_line(u' forwarder: {0}'.format(\n                            export.forwarder.decode(encoding, 'backslashreplace_')))\n                    else:\n                        dump.add_newline()\n\n            dump.add_newline()\n\n        if hasattr(self, 'DIRECTORY_ENTRY_IMPORT'):\n            dump.add_header('Imported symbols')\n            for module in self.DIRECTORY_ENTRY_IMPORT:\n                dump.add_lines(module.struct.dump())\n                # Print the name of the DLL if there are no imports.\n                if not module.imports:\n                    dump.add('  Name -> {0}'.format(\n                        self.get_string_at_rva(module.struct.Name).decode(\n                            encoding, 'backslashreplace_')))\n                    dump.add_newline()\n                dump.add_newline()\n                for symbol in module.imports:\n                    if symbol.import_by_ordinal is True:\n                        if symbol.name is not None:\n                            dump.add('{0}.{1} Ordinal[{2}] (Imported by Ordinal)'.format(\n                                     module.dll.decode('utf-8'),\n                                     symbol.name.decode('utf-8'),\n                                     symbol.ordinal))\n                        else:\n                            dump.add('{0} Ordinal[{1}] (Imported by Ordinal)'.format(\n                                module.dll.decode('utf-8'), symbol.ordinal))\n                    else:\n                        dump.add('{0}.{1} Hint[{2:d}]'.format(\n                            module.dll.decode(encoding, 'backslashreplace_'),\n                            symbol.name.decode(encoding, 'backslashreplace_'),\n                            symbol.hint))\n\n                    if symbol.bound:\n                        dump.add_line(' Bound: 0x{0:08X}'.format(symbol.bound))\n                    else:\n                        dump.add_newline()\n                dump.add_newline()\n\n\n        if hasattr(self, 'DIRECTORY_ENTRY_BOUND_IMPORT'):\n            dump.add_header('Bound imports')\n            for bound_imp_desc in self.DIRECTORY_ENTRY_BOUND_IMPORT:\n\n                dump.add_lines(bound_imp_desc.struct.dump())\n                dump.add_line('DLL: {0}'.format(\n                    bound_imp_desc.name.decode(encoding, 'backslashreplace_')))\n                dump.add_newline()\n\n                for bound_imp_ref in bound_imp_desc.entries:\n                    dump.add_lines(bound_imp_ref.struct.dump(), 4)\n                    dump.add_line('DLL: {0}'.format(\n                        bound_imp_ref.name.decode(encoding, 'backslashreplace_')), 4)\n                    dump.add_newline()\n\n\n        if hasattr(self, 'DIRECTORY_ENTRY_DELAY_IMPORT'):\n            dump.add_header('Delay Imported symbols')\n            for module in self.DIRECTORY_ENTRY_DELAY_IMPORT:\n\n                dump.add_lines(module.struct.dump())\n                dump.add_newline()\n\n                for symbol in module.imports:\n                    if symbol.import_by_ordinal is True:\n                        dump.add('{0} Ordinal[{1:d}] (Imported by Ordinal)'.format(\n                            module.dll.decode(encoding, 'backslashreplace_'),\n                            symbol.ordinal))\n                    else:\n                        dump.add('{0}.{1} Hint[{2}]'.format(\n                            module.dll.decode(encoding, 'backslashreplace_'),\n                            symbol.name.decode(encoding, 'backslashreplace_'), symbol.hint))\n\n                    if symbol.bound:\n                        dump.add_line(' Bound: 0x{0:08X}'.format(symbol.bound))\n                    else:\n                        dump.add_newline()\n                dump.add_newline()\n\n\n        if hasattr(self, 'DIRECTORY_ENTRY_RESOURCE'):\n            dump.add_header('Resource directory')\n\n            dump.add_lines(self.DIRECTORY_ENTRY_RESOURCE.struct.dump())\n\n            for resource_type in self.DIRECTORY_ENTRY_RESOURCE.entries:\n\n                if resource_type.name is not None:\n                    # name = str(resource_type.name) #.string if resource_type.name.string else ''\n                    dump.add_line(u'Name: [{0}]'.format(\n                        resource_type.name.decode(encoding, 'backslashreplace_')\n                        ), 2)\n                else:\n                    dump.add_line(u'Id: [0x{0:X}] ({1})'.format(\n                        resource_type.struct.Id, RESOURCE_TYPE.get(\n                            resource_type.struct.Id, '-')),\n                        2)\n\n                dump.add_lines(resource_type.struct.dump(), 2)\n\n                if hasattr(resource_type, 'directory'):\n\n                    dump.add_lines(resource_type.directory.struct.dump(), 4)\n\n                    for resource_id in resource_type.directory.entries:\n\n                        if resource_id.name is not None:\n                            dump.add_line(u'Name: [{0}]'.format(\n                                resource_id.name.decode(\n                                    'utf-8', 'backslashreplace_')), 6)\n                        else:\n                            dump.add_line('Id: [0x{0:X}]'.format(resource_id.struct.Id), 6)\n\n                        dump.add_lines(resource_id.struct.dump(), 6)\n\n                        if hasattr(resource_id, 'directory'):\n                            dump.add_lines(resource_id.directory.struct.dump(), 8)\n\n                            for resource_lang in resource_id.directory.entries:\n                                if hasattr(resource_lang, 'data'):\n                                    dump.add_line(u'\\\\--- LANG [%d,%d][%s,%s]' % (\n                                        resource_lang.data.lang,\n                                        resource_lang.data.sublang,\n                                        LANG.get(resource_lang.data.lang, '*unknown*'),\n                                        get_sublang_name_for_lang( resource_lang.data.lang, resource_lang.data.sublang ) ), 8)\n                                    dump.add_lines(resource_lang.struct.dump(), 10)\n                                    dump.add_lines(resource_lang.data.struct.dump(), 12)\n                            if hasattr(resource_id.directory, 'strings') and resource_id.directory.strings:\n                                dump.add_line(u'[STRINGS]' , 10 )\n                                for idx, res_string in list(sorted(resource_id.directory.strings.items())):\n                                    dump.add_line( '{0:6d}: {1}'.format(idx,\n                                        res_string.encode(\n                                            'unicode-escape',\n                                            'backslashreplace').decode(\n                                                'ascii')),\n                                        12)\n\n                dump.add_newline()\n\n            dump.add_newline()\n\n\n        if ( hasattr(self, 'DIRECTORY_ENTRY_TLS') and\n             self.DIRECTORY_ENTRY_TLS and\n             self.DIRECTORY_ENTRY_TLS.struct ):\n\n            dump.add_header('TLS')\n            dump.add_lines(self.DIRECTORY_ENTRY_TLS.struct.dump())\n            dump.add_newline()\n\n\n        if ( hasattr(self, 'DIRECTORY_ENTRY_LOAD_CONFIG') and\n             self.DIRECTORY_ENTRY_LOAD_CONFIG and\n             self.DIRECTORY_ENTRY_LOAD_CONFIG.struct ):\n\n            dump.add_header('LOAD_CONFIG')\n            dump.add_lines(self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.dump())\n            dump.add_newline()\n\n        if hasattr(self, 'DIRECTORY_ENTRY_DEBUG'):\n            dump.add_header('Debug information')\n            for dbg in self.DIRECTORY_ENTRY_DEBUG:\n                dump.add_lines(dbg.struct.dump())\n                try:\n                    dump.add_line('Type: '+DEBUG_TYPE[dbg.struct.Type])\n                except KeyError:\n                    dump.add_line(\n                        'Type: 0x{0:x}(Unknown)'.format(dbg.struct.Type))\n                dump.add_newline()\n                if dbg.entry:\n                    dump.add_lines(dbg.entry.dump(), 4)\n                    dump.add_newline()\n\n        if self.has_relocs():\n            dump.add_header('Base relocations')\n            for base_reloc in self.DIRECTORY_ENTRY_BASERELOC:\n                dump.add_lines(base_reloc.struct.dump())\n                for reloc in base_reloc.entries:\n                    try:\n                        dump.add_line('%08Xh %s' % (\n                            reloc.rva, RELOCATION_TYPE[reloc.type][16:]), 4)\n                    except KeyError:\n                        dump.add_line('0x%08X 0x%x(Unknown)' % (\n                            reloc.rva, reloc.type), 4)\n                dump.add_newline()\n\n        return dump.get_text()", "response": "Dump all the PE header information into human readable string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndumps all the PE header information into a dictionary.", "response": "def dump_dict(self, dump=None):\n        \"\"\"Dump all the PE header information into a dictionary.\"\"\"\n\n        dump_dict = dict()\n\n        warnings = self.get_warnings()\n        if warnings:\n            dump_dict['Parsing Warnings'] = warnings\n\n        dump_dict['DOS_HEADER'] = self.DOS_HEADER.dump_dict()\n        dump_dict['NT_HEADERS'] = self.NT_HEADERS.dump_dict()\n        dump_dict['FILE_HEADER'] = self.FILE_HEADER.dump_dict()\n\n        image_flags = retrieve_flags(IMAGE_CHARACTERISTICS, 'IMAGE_FILE_')\n\n        dump_dict['Flags'] = list()\n        for flag in image_flags:\n            if getattr(self.FILE_HEADER, flag[0]):\n                dump_dict['Flags'].append(flag[0])\n\n        if hasattr(self, 'OPTIONAL_HEADER') and self.OPTIONAL_HEADER is not None:\n            dump_dict['OPTIONAL_HEADER'] = self.OPTIONAL_HEADER.dump_dict()\n\n        dll_characteristics_flags = retrieve_flags(DLL_CHARACTERISTICS, 'IMAGE_DLLCHARACTERISTICS_')\n\n        dump_dict['DllCharacteristics'] = list()\n        for flag in dll_characteristics_flags:\n            if getattr(self.OPTIONAL_HEADER, flag[0]):\n                dump_dict['DllCharacteristics'].append(flag[0])\n\n        dump_dict['PE Sections'] = list()\n\n        section_flags = retrieve_flags(SECTION_CHARACTERISTICS, 'IMAGE_SCN_')\n        for section in self.sections:\n            section_dict = section.dump_dict()\n            dump_dict['PE Sections'].append(section_dict)\n            section_dict['Flags'] = list()\n            for flag in section_flags:\n                if getattr(section, flag[0]):\n                    section_dict['Flags'].append(flag[0])\n\n            section_dict['Entropy'] = section.get_entropy()\n            if md5 is not None:\n                section_dict['MD5'] = section.get_hash_md5()\n            if sha1 is not None:\n                section_dict['SHA1'] = section.get_hash_sha1()\n            if sha256 is not None:\n                section_dict['SHA256'] = section.get_hash_sha256()\n            if sha512 is not None:\n                section_dict['SHA512'] = section.get_hash_sha512()\n\n\n\n        if (hasattr(self, 'OPTIONAL_HEADER') and\n            hasattr(self.OPTIONAL_HEADER, 'DATA_DIRECTORY') ):\n\n            dump_dict['Directories'] = list()\n\n            for idx in range(len(self.OPTIONAL_HEADER.DATA_DIRECTORY)):\n                directory = self.OPTIONAL_HEADER.DATA_DIRECTORY[idx]\n                dump_dict['Directories'].append(directory.dump_dict())\n\n        if hasattr(self, 'VS_VERSIONINFO'):\n            dump_dict['Version Information'] = list()\n            for idx in range(len(self.VS_VERSIONINFO)):\n                version_info_list = list()\n                version_info_list.append(self.VS_VERSIONINFO[idx].dump_dict())\n\n                if hasattr(self, 'VS_FIXEDFILEINFO'):\n                    version_info_list.append(self.VS_FIXEDFILEINFO[idx].dump_dict())\n\n                if hasattr(self, 'FileInfo'):\n                    fileinfo_list = list()\n                    for entry in self.FileInfo[idx]:\n                        fileinfo_list.append(entry.dump_dict())\n\n                        if hasattr(entry, 'StringTable'):\n                            stringtable_dict = dict()\n                            for st_entry in entry.StringTable:\n                                [fileinfo_list.append(line) for line in st_entry.dump_dict()]\n                                stringtable_dict['LangID'] = st_entry.LangID\n                                for str_entry in list(st_entry.entries.items()):\n                                    stringtable_dict[str_entry[0]] = str_entry[1]\n                            fileinfo_list.append(stringtable_dict)\n\n\n                        elif hasattr(entry, 'Var'):\n                            for var_entry in entry.Var:\n                                var_dict = dict()\n                                if hasattr(var_entry, 'entry'):\n                                    [fileinfo_list.append(line) for line in var_entry.dump_dict()]\n                                    var_dict[list(var_entry.entry.keys())[0]] = list(\n                                        var_entry.entry.values())[0]\n                                    fileinfo_list.append(var_dict)\n\n                dump_dict['Version Information'].append(version_info_list)\n\n        if hasattr(self, 'DIRECTORY_ENTRY_EXPORT'):\n            dump_dict['Exported symbols'] = list()\n            dump_dict['Exported symbols'].append(self.DIRECTORY_ENTRY_EXPORT.struct.dump_dict())\n            for export in self.DIRECTORY_ENTRY_EXPORT.symbols:\n                export_dict = dict()\n                if export.address is not None:\n                    export_dict.update({'Ordinal': export.ordinal, 'RVA': export.address, 'Name': export.name})\n                    if export.forwarder:\n                        export_dict['forwarder'] = export.forwarder\n                dump_dict['Exported symbols'].append(export_dict)\n\n        if hasattr(self, 'DIRECTORY_ENTRY_IMPORT'):\n            dump_dict['Imported symbols'] = list()\n            for module in self.DIRECTORY_ENTRY_IMPORT:\n                import_list = list()\n                dump_dict['Imported symbols'].append(import_list)\n                import_list.append(module.struct.dump_dict())\n                for symbol in module.imports:\n                    symbol_dict = dict()\n                    if symbol.import_by_ordinal is True:\n                        symbol_dict['DLL'] = module.dll\n                        symbol_dict['Ordinal'] = symbol.ordinal\n                    else:\n                        symbol_dict['DLL'] = module.dll\n                        symbol_dict['Name'] = symbol.name\n                        symbol_dict['Hint'] = symbol.hint\n\n                    if symbol.bound:\n                        symbol_dict['Bound'] = symbol.bound\n                    import_list.append(symbol_dict)\n\n\n        if hasattr(self, 'DIRECTORY_ENTRY_BOUND_IMPORT'):\n            dump_dict['Bound imports'] = list()\n            for bound_imp_desc in self.DIRECTORY_ENTRY_BOUND_IMPORT:\n                bound_imp_desc_dict = dict()\n                dump_dict['Bound imports'].append(bound_imp_desc_dict)\n\n                bound_imp_desc_dict.update(bound_imp_desc.struct.dump_dict())\n                bound_imp_desc_dict['DLL'] = bound_imp_desc.name\n\n                for bound_imp_ref in bound_imp_desc.entries:\n                    bound_imp_ref_dict = dict()\n                    bound_imp_ref_dict.update(bound_imp_ref.struct.dump_dict())\n                    bound_imp_ref_dict['DLL'] = bound_imp_ref.name\n\n\n        if hasattr(self, 'DIRECTORY_ENTRY_DELAY_IMPORT'):\n            dump_dict['Delay Imported symbols'] = list()\n            for module in self.DIRECTORY_ENTRY_DELAY_IMPORT:\n                module_list = list()\n                dump_dict['Delay Imported symbols'].append(module_list)\n                module_list.append(module.struct.dump_dict())\n\n                for symbol in module.imports:\n                    symbol_dict = dict()\n                    if symbol.import_by_ordinal is True:\n                        symbol_dict['DLL'] = module.dll\n                        symbol_dict['Ordinal'] = symbol.ordinal\n                    else:\n                        symbol_dict['DLL'] = module.dll\n                        symbol_dict['Name'] = symbol.name\n                        symbol_dict['Hint'] = symbol.hint\n\n                    if symbol.bound:\n                        symbol_dict['Bound'] = symbol.bound\n                    module_list.append(symbol_dict)\n\n\n        if hasattr(self, 'DIRECTORY_ENTRY_RESOURCE'):\n            dump_dict['Resource directory'] = list()\n            dump_dict['Resource directory'].append(self.DIRECTORY_ENTRY_RESOURCE.struct.dump_dict())\n\n            for resource_type in self.DIRECTORY_ENTRY_RESOURCE.entries:\n                resource_type_dict = dict()\n\n                if resource_type.name is not None:\n                    resource_type_dict['Name'] = resource_type.name\n                else:\n                    resource_type_dict['Id'] = (\n                        resource_type.struct.Id, RESOURCE_TYPE.get(resource_type.struct.Id, '-'))\n\n                resource_type_dict.update(resource_type.struct.dump_dict())\n                dump_dict['Resource directory'].append(resource_type_dict)\n\n                if hasattr(resource_type, 'directory'):\n                    directory_list = list()\n                    directory_list.append(resource_type.directory.struct.dump_dict())\n                    dump_dict['Resource directory'].append(directory_list)\n\n                    for resource_id in resource_type.directory.entries:\n                        resource_id_dict = dict()\n\n                        if resource_id.name is not None:\n                            resource_id_dict['Name'] = resource_id.name\n                        else:\n                            resource_id_dict['Id'] = resource_id.struct.Id\n\n                        resource_id_dict.update(resource_id.struct.dump_dict())\n                        directory_list.append(resource_id_dict)\n\n                        if hasattr(resource_id, 'directory'):\n                            resource_id_list = list()\n                            resource_id_list.append(resource_id.directory.struct.dump_dict())\n                            directory_list.append(resource_id_list)\n\n                            for resource_lang in resource_id.directory.entries:\n                                if hasattr(resource_lang, 'data'):\n                                    resource_lang_dict = dict()\n                                    resource_lang_dict['LANG'] = resource_lang.data.lang\n                                    resource_lang_dict['SUBLANG'] = resource_lang.data.sublang\n                                    resource_lang_dict['LANG_NAME'] = LANG.get(resource_lang.data.lang, '*unknown*')\n                                    resource_lang_dict['SUBLANG_NAME'] = get_sublang_name_for_lang(resource_lang.data.lang, resource_lang.data.sublang)\n                                    resource_lang_dict.update(resource_lang.struct.dump_dict())\n                                    resource_lang_dict.update(resource_lang.data.struct.dump_dict())\n                                    resource_id_list.append(resource_lang_dict)\n                            if hasattr(resource_id.directory, 'strings') and resource_id.directory.strings:\n                                for idx, res_string in list(resource_id.directory.strings.items()):\n                                    resource_id_list.append(res_string.encode(\n                                            'unicode-escape',\n                                            'backslashreplace').decode(\n                                                'ascii'))\n\n\n        if ( hasattr(self, 'DIRECTORY_ENTRY_TLS') and\n             self.DIRECTORY_ENTRY_TLS and\n             self.DIRECTORY_ENTRY_TLS.struct ):\n            dump_dict['TLS'] = self.DIRECTORY_ENTRY_TLS.struct.dump_dict()\n\n\n        if ( hasattr(self, 'DIRECTORY_ENTRY_LOAD_CONFIG') and\n             self.DIRECTORY_ENTRY_LOAD_CONFIG and\n             self.DIRECTORY_ENTRY_LOAD_CONFIG.struct ):\n            dump_dict['LOAD_CONFIG'] = self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.dump_dict()\n\n\n        if hasattr(self, 'DIRECTORY_ENTRY_DEBUG'):\n            dump_dict['Debug information'] = list()\n            for dbg in self.DIRECTORY_ENTRY_DEBUG:\n                dbg_dict = dict()\n                dump_dict['Debug information'].append(dbg_dict)\n                dbg_dict.update(dbg.struct.dump_dict())\n                dbg_dict['Type'] = DEBUG_TYPE.get(dbg.struct.Type, dbg.struct.Type)\n\n\n        if self.has_relocs():\n            dump_dict['Base relocations'] = list()\n            for base_reloc in self.DIRECTORY_ENTRY_BASERELOC:\n                base_reloc_list = list()\n                dump_dict['Base relocations'].append(base_reloc_list)\n                base_reloc_list.append(base_reloc.struct.dump_dict())\n                for reloc in base_reloc.entries:\n                    reloc_dict = dict()\n                    base_reloc_list.append(reloc_dict)\n                    reloc_dict['RVA'] = reloc.rva\n                    try:\n                        reloc_dict['Type'] = RELOCATION_TYPE[reloc.type][16:]\n                    except KeyError:\n                        reloc_dict['Type'] = reloc.type\n\n        return dump_dict"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert four bytes of data to a double word.", "response": "def get_dword_from_data(self, data, offset):\n        \"\"\"Convert four bytes of data to a double word (little endian)\n\n        'offset' is assumed to index into a dword array. So setting it to\n        N will return a dword out of the data starting at offset N*4.\n\n        Returns None if the data can't be turned into a double word.\n        \"\"\"\n\n        if (offset+1)*4 > len(data):\n            return None\n\n        return struct.unpack('<I', data[offset*4:(offset+1)*4])[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the double word value at the given offset.", "response": "def get_dword_from_offset(self, offset):\n        \"\"\"Return the double word value at the given file offset. (little endian)\"\"\"\n\n        if offset+4 > len(self.__data__):\n            return None\n\n        return self.get_dword_from_data(self.__data__[offset:offset+4], 0)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_dword_at_rva(self, rva, dword):\n        return self.set_bytes_at_rva(rva, self.get_data_from_dword(dword))", "response": "Set the double word value at the given RVA."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_dword_at_offset(self, offset, dword):\n        return self.set_bytes_at_offset(offset, self.get_data_from_dword(dword))", "response": "Set the double word value at the given file offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_word_from_data(self, data, offset):\n\n        if (offset+1)*2 > len(data):\n            return None\n\n        return struct.unpack('<H', data[offset*2:(offset+1)*2])[0]", "response": "Convert two bytes of data into a word."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the word value at the given offset.", "response": "def get_word_from_offset(self, offset):\n        \"\"\"Return the word value at the given file offset. (little endian)\"\"\"\n\n        if offset+2 > len(self.__data__):\n            return None\n\n        return self.get_word_from_data(self.__data__[offset:offset+2], 0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_word_at_rva(self, rva, word):\n        return self.set_bytes_at_rva(rva, self.get_data_from_word(word))", "response": "Set the word value at the file offset corresponding to the given RVA."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the word value at the given file offset.", "response": "def set_word_at_offset(self, offset, word):\n        \"\"\"Set the word value at the given file offset.\"\"\"\n        return self.set_bytes_at_offset(offset, self.get_data_from_word(word))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_qword_from_data(self, data, offset):\n\n        if (offset+1)*8 > len(data):\n            return None\n\n        return struct.unpack('<Q', data[offset*8:(offset+1)*8])[0]", "response": "Convert eight bytes of data to a quad word."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_qword_from_offset(self, offset):\n\n        if offset+8 > len(self.__data__):\n            return None\n\n        return self.get_qword_from_data(self.__data__[offset:offset+8], 0)", "response": "Return the quad - word value at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the quad - word value at the file offset corresponding to the given RVA.", "response": "def set_qword_at_rva(self, rva, qword):\n        \"\"\"Set the quad-word value at the file offset corresponding to the given RVA.\"\"\"\n        return self.set_bytes_at_rva(rva, self.get_data_from_qword(qword))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the quad - word value at the given file offset.", "response": "def set_qword_at_offset(self, offset, qword):\n        \"\"\"Set the quad-word value at the given file offset.\"\"\"\n        return self.set_bytes_at_offset(offset, self.get_data_from_qword(qword))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_bytes_at_rva(self, rva, data):\n\n        if not isinstance(data, bytes):\n            raise TypeError('data should be of type: bytes')\n\n        offset = self.get_physical_by_rva(rva)\n        if not offset:\n            return False\n\n        return self.set_bytes_at_offset(offset, data)", "response": "Overwrite the given string with the bytes at the given RVA."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_bytes_at_offset(self, offset, data):\n\n        if not isinstance(data, bytes):\n            raise TypeError('data should be of type: bytes')\n\n        if 0 <= offset < len(self.__data__):\n            self.__data__ = ( self.__data__[:offset] + data + self.__data__[offset+len(data):] )\n        else:\n            return False\n\n        return True", "response": "Overwrite the bytes at the given offset."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef relocate_image(self, new_ImageBase):\n\n        relocation_difference = new_ImageBase - self.OPTIONAL_HEADER.ImageBase\n\n        if self.OPTIONAL_HEADER.DATA_DIRECTORY[5].Size:\n            if not hasattr(self, 'DIRECTORY_ENTRY_BASERELOC'):\n                self.parse_data_directories(\n                    directories=[DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_BASERELOC']])\n            for reloc in self.DIRECTORY_ENTRY_BASERELOC:\n\n                virtual_address = reloc.struct.VirtualAddress\n                size_of_block = reloc.struct.SizeOfBlock\n\n                # We iterate with an index because if the relocation is of type\n                # IMAGE_REL_BASED_HIGHADJ we need to also process the next entry\n                # at once and skip it for the next iteration\n                #\n                entry_idx = 0\n                while entry_idx<len(reloc.entries):\n\n                    entry = reloc.entries[entry_idx]\n                    entry_idx += 1\n\n                    if entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_ABSOLUTE']:\n                        # Nothing to do for this type of relocation\n                        pass\n\n                    elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGH']:\n                        # Fix the high 16-bits of a relocation\n                        #\n                        # Add high 16-bits of relocation_difference to the\n                        # 16-bit value at RVA=entry.rva\n\n                        self.set_word_at_rva(\n                            entry.rva,\n                            ( self.get_word_at_rva(entry.rva) + relocation_difference>>16)&0xffff )\n\n                    elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_LOW']:\n                        # Fix the low 16-bits of a relocation\n                        #\n                        # Add low 16 bits of relocation_difference to the 16-bit value\n                        # at RVA=entry.rva\n\n                        self.set_word_at_rva(\n                            entry.rva,\n                            ( self.get_word_at_rva(entry.rva) + relocation_difference)&0xffff)\n\n                    elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGHLOW']:\n                        # Handle all high and low parts of a 32-bit relocation\n                        #\n                        # Add relocation_difference to the value at RVA=entry.rva\n\n                        self.set_dword_at_rva(\n                            entry.rva,\n                            self.get_dword_at_rva(entry.rva)+relocation_difference)\n\n                    elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_HIGHADJ']:\n                        # Fix the high 16-bits of a relocation and adjust\n                        #\n                        # Add high 16-bits of relocation_difference to the 32-bit value\n                        # composed from the (16-bit value at RVA=entry.rva)<<16 plus\n                        # the 16-bit value at the next relocation entry.\n                        #\n\n                        # If the next entry is beyond the array's limits,\n                        # abort... the table is corrupt\n                        #\n                        if entry_idx == len(reloc.entries):\n                            break\n\n                        next_entry = reloc.entries[entry_idx]\n                        entry_idx += 1\n                        self.set_word_at_rva( entry.rva,\n                            ((self.get_word_at_rva(entry.rva)<<16) + next_entry.rva +\n                            relocation_difference & 0xffff0000) >> 16 )\n\n                    elif entry.type == RELOCATION_TYPE['IMAGE_REL_BASED_DIR64']:\n                        # Apply the difference to the 64-bit value at the offset\n                        # RVA=entry.rva\n\n                        self.set_qword_at_rva(\n                            entry.rva,\n                            self.get_qword_at_rva(entry.rva) + relocation_difference)\n\n            self.OPTIONAL_HEADER.ImageBase = new_ImageBase\n\n            #correct VAs(virtual addresses) occurrences in directory information\n            if hasattr(self, 'IMAGE_DIRECTORY_ENTRY_IMPORT'):\n                for dll in self.DIRECTORY_ENTRY_IMPORT:\n                    for func in dll.imports:\n                        func.address += relocation_difference\n            if hasattr(self, 'IMAGE_DIRECTORY_ENTRY_TLS'):\n                self.DIRECTORY_ENTRY_TLS.struct.StartAddressOfRawData += relocation_difference\n                self.DIRECTORY_ENTRY_TLS.struct.EndAddressOfRawData   += relocation_difference\n                self.DIRECTORY_ENTRY_TLS.struct.AddressOfIndex        += relocation_difference\n                self.DIRECTORY_ENTRY_TLS.struct.AddressOfCallBacks    += relocation_difference\n            if hasattr(self, 'IMAGE_DIRECTORY_ENTRY_LOAD_CONFIG'):\n                if self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.LockPrefixTable:\n                    self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.LockPrefixTable += relocation_difference\n                if self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.EditList:\n                    self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.EditList += relocation_difference\n                if self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.SecurityCookie:\n                    self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.SecurityCookie += relocation_difference\n                if self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.SEHandlerTable:\n                    self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.SEHandlerTable += relocation_difference\n                if self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.GuardCFCheckFunctionPointer:\n                    self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.GuardCFCheckFunctionPointer += relocation_difference\n                if self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.GuardCFFunctionTable:\n                    self.DIRECTORY_ENTRY_LOAD_CONFIG.struct.GuardCFFunctionTable += relocation_difference", "response": "This method relocate the image using the provided new image base."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking whether the file is a standard executable.", "response": "def is_exe(self):\n        \"\"\"Check whether the file is a standard executable.\n\n        This will return true only if the file has the IMAGE_FILE_EXECUTABLE_IMAGE flag set\n        and the IMAGE_FILE_DLL not set and the file does not appear to be a driver either.\n        \"\"\"\n\n        EXE_flag = IMAGE_CHARACTERISTICS['IMAGE_FILE_EXECUTABLE_IMAGE']\n\n        if (not self.is_dll()) and (not self.is_driver()) and (\n                EXE_flag & self.FILE_HEADER.Characteristics) == EXE_flag:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks whether the image is a standard DLL.", "response": "def is_dll(self):\n        \"\"\"Check whether the file is a standard DLL.\n\n        This will return true only if the image has the IMAGE_FILE_DLL flag set.\n        \"\"\"\n\n        DLL_flag = IMAGE_CHARACTERISTICS['IMAGE_FILE_DLL']\n\n        if ( DLL_flag & self.FILE_HEADER.Characteristics) == DLL_flag:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck whether the image is a Windows driver.", "response": "def is_driver(self):\n        \"\"\"Check whether the file is a Windows driver.\n\n        This will return true only if there are reliable indicators of the image\n        being a driver.\n        \"\"\"\n\n        # Checking that the ImageBase field of the OptionalHeader is above or\n        # equal to 0x80000000 (that is, whether it lies in the upper 2GB of\n        # the address space, normally belonging to the kernel) is not a\n        # reliable enough indicator.  For instance, PEs that play the invalid\n        # ImageBase trick to get relocated could be incorrectly assumed to be\n        # drivers.\n\n        # This is not reliable either...\n        #\n        # if any((section.Characteristics &\n        #           SECTION_CHARACTERISTICS['IMAGE_SCN_MEM_NOT_PAGED']) for\n        #        section in self.sections ):\n        #    return True\n\n        # If the import directory was not parsed (fast_load = True); do it now.\n        if not hasattr(self, 'DIRECTORY_ENTRY_IMPORT'):\n            self.parse_data_directories(directories=[\n                DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_IMPORT']])\n\n        # If there's still no import directory (the PE doesn't have one or it's\n        # malformed), give up.\n        if not hasattr(self, 'DIRECTORY_ENTRY_IMPORT'):\n            return False\n\n        # self.DIRECTORY_ENTRY_IMPORT will now exist, although it may be empty.\n        # If it imports from \"ntoskrnl.exe\" or other kernel components it should\n        # be a driver\n        #\n        system_DLLs = set((b'ntoskrnl.exe', b'hal.dll', b'ndis.sys',\n                           b'bootvid.dll', b'kdcom.dll'))\n        if system_DLLs.intersection(\n                [imp.dll.lower() for imp in self.DIRECTORY_ENTRY_IMPORT]):\n            return True\n\n        driver_like_section_names = set(\n            ('page', 'paged'))\n        if driver_like_section_names.intersection(\n                [section.Name.lower().rstrip(b'\\x00') for section in self.sections]) and (\n            self.OPTIONAL_HEADER.Subsystem in (\n                SUBSYSTEM_TYPE['IMAGE_SUBSYSTEM_NATIVE'],\n                SUBSYSTEM_TYPE['IMAGE_SUBSYSTEM_NATIVE_WINDOWS'])):\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_overlay_data_start_offset(self):\n\n        largest_offset_and_size = (0, 0)\n\n        def update_if_sum_is_larger_and_within_file(offset_and_size, file_size=len(self.__data__)):\n            if sum(offset_and_size) <= file_size and sum(offset_and_size) > sum(largest_offset_and_size):\n                return offset_and_size\n            return largest_offset_and_size\n\n        if hasattr(self, 'OPTIONAL_HEADER'):\n            largest_offset_and_size = update_if_sum_is_larger_and_within_file(\n                (self.OPTIONAL_HEADER.get_file_offset(), self.FILE_HEADER.SizeOfOptionalHeader))\n\n        for section in self.sections:\n            largest_offset_and_size = update_if_sum_is_larger_and_within_file(\n                (section.PointerToRawData, section.SizeOfRawData))\n\n        skip_directories = [DIRECTORY_ENTRY['IMAGE_DIRECTORY_ENTRY_SECURITY']]\n\n        for idx, directory in enumerate(self.OPTIONAL_HEADER.DATA_DIRECTORY):\n            if idx in skip_directories:\n                continue\n            try:\n                largest_offset_and_size = update_if_sum_is_larger_and_within_file(\n                    (self.get_offset_from_rva(directory.VirtualAddress), directory.Size))\n            # Ignore directories with RVA out of file\n            except PEFormatError:\n                continue\n\n        if len(self.__data__) > sum(largest_offset_and_size):\n            return sum(largest_offset_and_size)\n\n        return None", "response": "Get the offset of data appended to the file and not contained within the headers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the just data defined by the PE headers removing any overlayed data.", "response": "def trim(self):\n        \"\"\"Return the just data defined by the PE headers, removing any overlayed data.\"\"\"\n\n        overlay_data_offset = self.get_overlay_data_start_offset()\n\n        if overlay_data_offset is not None:\n            return self.__data__[ : overlay_data_offset ]\n\n        return self.__data__[:]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef getUrl(self):\n        url_obj = urlparse(self.url)\n        scheme = self.getScheme(url_obj)\n        hostname = self.getHostname(url_obj)", "response": "\\ Returns the url of the current cache entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_large_images(self, node, parent_depth_level, sibling_depth_level):\n        good_images = self.get_image_candidates(node)\n\n        if good_images:\n            scored_images = self.fetch_images(good_images, parent_depth_level)\n            if scored_images:\n                highscore_image = sorted(scored_images.items(),\n                                        key=lambda x: x[1], reverse=True)[0][0]\n                main_image = Image()\n                main_image.src = highscore_image.src\n                main_image.width = highscore_image.width\n                main_image.height = highscore_image.height\n                main_image.extraction_type = \"bigimage\"\n                main_image.confidence_score = 100 / len(scored_images) \\\n                                    if len(scored_images) > 0 else 0\n                return main_image\n\n        depth_obj = self.get_depth_level(node, parent_depth_level, sibling_depth_level)\n        if depth_obj:\n            return self.check_large_images(depth_obj.node,\n                            depth_obj.parent_depth, depth_obj.sibling_depth)\n\n        return None", "response": "This method checks if the node has any images that are too large and returns the main image that is too large."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef check_known_elements(self):\n        domain = self.get_clean_domain()\n        if domain in self.custom_site_mapping.keys():\n            classes = self.custom_site_mapping.get(domain).split('|')\n            for classname in classes:\n                KNOWN_IMG_DOM_NAMES.append(classname)\n\n        image = None\n        doc = self.article.raw_doc\n\n        def _check_elements(elements):\n            image = None\n            for element in elements:\n                tag = self.parser.getTag(element)\n                if tag == 'img':\n                    image = element\n                    return image\n                else:\n                    images = self.parser.getElementsByTag(element, tag='img')\n                    if images:\n                        image = images[0]\n                        return image\n            return image\n\n        # check for elements with known id\n        for css in KNOWN_IMG_DOM_NAMES:\n            elements = self.parser.getElementsByTag(doc, attr=\"id\", value=css)\n            image = _check_elements(elements)\n            if image is not None:\n                src = self.parser.getAttribute(image, attr='src')\n                if src:\n                    return self.get_image(image, src, score=90, extraction_type='known')\n\n        # check for elements with known classes\n        for css in KNOWN_IMG_DOM_NAMES:\n            elements = self.parser.getElementsByTag(doc, attr='class', value=css)\n            image = _check_elements(elements)\n            if image is not None:\n                src = self.parser.getAttribute(image, attr='src')\n                if src:\n                    return self.get_image(image, src, score=90, extraction_type='known')\n\n        return None", "response": "check for known images in the article"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a unicode object representing s.", "response": "def smart_unicode(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\"\"\n    Returns a unicode object representing 's'. Treats bytestrings using the\n    'encoding' codec.\n\n    If strings_only is True, don't convert (some) non-string-like objects.\n    \"\"\"\n    # if isinstance(s, Promise):\n    #     # The input is the result of a gettext_lazy() call.\n    #     return s\n    return force_unicode(s, encoding, strings_only, errors)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines if the object is of a protected type.", "response": "def is_protected_type(obj):\n    \"\"\"Determine if the object instance is of a protected type.\n\n    Objects of protected types are preserved as-is when passed to\n    force_unicode(strings_only=True).\n    \"\"\"\n    return isinstance(obj, (\n        types.NoneType,\n        int, long,\n        datetime.datetime, datetime.date, datetime.time,\n        float, Decimal)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef force_unicode(s, encoding='utf-8', strings_only=False, errors='strict'):\n    # Handle the common case first, saves 30-40% in performance when s\n    # is an instance of unicode. This function gets called often in that\n    # setting.\n    if isinstance(s, unicode):\n        return s\n    if strings_only and is_protected_type(s):\n        return s\n    try:\n        if not isinstance(s, basestring,):\n            if hasattr(s, '__unicode__'):\n                s = unicode(s)\n            else:\n                try:\n                    s = unicode(str(s), encoding, errors)\n                except UnicodeEncodeError:\n                    if not isinstance(s, Exception):\n                        raise\n                    # If we get to here, the caller has passed in an Exception\n                    # subclass populated with non-ASCII data without special\n                    # handling to display as a string. We need to handle this\n                    # without raising a further exception. We do an\n                    # approximation to what the Exception's standard str()\n                    # output should be.\n                    s = u' '.join([force_unicode(arg, encoding, strings_only,\n                            errors) for arg in s])\n        elif not isinstance(s, unicode):\n            # Note: We use .decode() here, instead of unicode(s, encoding,\n            # errors), so that if s is a SafeString, it ends up being a\n            # SafeUnicode at the end.\n            s = s.decode(encoding, errors)\n    except UnicodeDecodeError, e:\n        if not isinstance(s, Exception):\n            raise DjangoUnicodeDecodeError(s, *e.args)\n        else:\n            # If we get to here, the caller has passed in an Exception\n            # subclass populated with non-ASCII bytestring data without a\n            # working unicode method. Try to handle this without raising a\n            # further exception by individually forcing the exception args\n            # to unicode.\n            s = u' '.join([force_unicode(arg, encoding, strings_only,\n                    errors) for arg in s])\n    return s", "response": "Force unicode to Unicode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a bytestring version of s encoded as specified in encoding.", "response": "def smart_str(s, encoding='utf-8', strings_only=False, errors='strict'):\n    \"\"\"\n    Returns a bytestring version of 's', encoded as specified in 'encoding'.\n\n    If strings_only is True, don't convert (some) non-string-like objects.\n    \"\"\"\n    if strings_only and isinstance(s, (types.NoneType, int)):\n        return s\n    # if isinstance(s, Promise):\n    #     return unicode(s).encode(encoding, errors)\n    if not isinstance(s, basestring):\n        try:\n            return str(s)\n        except UnicodeEncodeError:\n            if isinstance(s, Exception):\n                # An Exception subclass containing non-ASCII data that doesn't\n                # know how to print itself properly. We shouldn't raise a\n                # further exception.\n                return ' '.join([smart_str(arg, encoding, strings_only,\n                        errors) for arg in s])\n            return unicode(s).encode(encoding, errors)\n    elif isinstance(s, unicode):\n        return s.encode(encoding, errors)\n    elif s and encoding != 'utf-8':\n        return s.decode('utf-8', errors).encode(encoding, errors)\n    else:\n        return s"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef post_cleanup(self):\n        targetNode = self.article.top_node\n        node = self.add_siblings(targetNode)\n        for e in self.parser.getChildren(node):\n            e_tag = self.parser.getTag(e)\n            if e_tag != 'p':\n                if self.is_highlink_density(e) \\\n                    or self.is_table_and_no_para_exist(e) \\\n                    or not self.is_nodescore_threshold_met(node, e):\n                    self.parser.remove(e)\n        return node", "response": "\\ This method is called after the article is cleaned up."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a video object from a video embed node", "response": "def get_video(self, node):\n        \"\"\"\n        Create a video object from a video embed\n        \"\"\"\n        video = Video()\n        video.embed_code = self.get_embed_code(node)\n        video.embed_type = self.get_embed_type(node)\n        video.width = self.get_width(node)\n        video.height = self.get_height(node)\n        video.src = self.get_src(node)\n        video.provider = self.get_provider(video.src)\n        return video"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef store_image(self, http_client, link_hash, src, config):\n        # check for a cache hit already on disk\n        image = self.read_localfile(link_hash, src, config)\n        if image:\n            return image\n\n        # no cache found download the image\n        data = self.fetch(http_client, src)\n        if data:\n            image = self.write_localfile(data, link_hash, src, config)\n            if image:\n                return image\n\n        return None", "response": "\\ Writes an image src http string to disk as a temporary file\n        and returns the LocallyStoredImage object that has the info you need on the image."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves negative gravity score nodes from the top node.", "response": "def remove_negativescores_nodes(self):\n        \"\"\"\\\n        if there are elements inside our top node\n        that have a negative gravity score,\n        let's give em the boot\n        \"\"\"\n        gravity_items = self.parser.css_select(self.top_node, \"*[gravityScore]\")\n        for item in gravity_items:\n            score = self.parser.getAttribute(item, 'gravityScore')\n            score = int(score, 0)\n            if score < 1:\n                item.getparent().remove(item)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clean_title(self, title):\n        # check if we have the site name in opengraph data\n        if \"site_name\" in self.article.opengraph.keys():\n            site_name = self.article.opengraph['site_name']\n            # remove the site name from title\n            title = title.replace(site_name, '').strip()\n\n        # try to remove the domain from url\n        if self.article.domain:\n            pattern = re.compile(self.article.domain, re.IGNORECASE)\n            title = pattern.sub(\"\", title).strip()\n\n        # split the title in words\n        # TechCrunch | my wonderfull article\n        # my wonderfull article | TechCrunch\n        title_words = title.split()\n\n        # check for an empty title\n        # so that we don't get an IndexError below\n        if len(title_words) == 0:\n            return u\"\"\n\n        # check if first letter is in TITLE_SPLITTERS\n        # if so remove it\n        if title_words[0] in TITLE_SPLITTERS:\n            title_words.pop(0)\n\n        # check if last letter is in TITLE_SPLITTERS\n        # if so remove it\n        if title_words[-1] in TITLE_SPLITTERS:\n            title_words.pop(-1)\n\n        # rebuild the title\n        title = u\" \".join(title_words).strip()\n\n        return title", "response": "Clean the title with the use of og : site_name\n        and use TITLE_SPLITTERS to reformat it"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_meta_content(self, metaName):\n        meta = self.parser.css_select(self.article.doc, metaName)\n        content = None\n\n        if meta is not None and len(meta) > 0:\n            content = self.parser.getAttribute(meta[0], 'content')\n\n        if content:\n            return content.strip()\n\n        return ''", "response": "\\ Extract a given meta content form document\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _verbose_reporter(self, run_details=None):\n        if run_details is None:\n            print('    |{:^25}|{:^42}|'.format('Population Average',\n                                               'Best Individual'))\n            print('-' * 4 + ' ' + '-' * 25 + ' ' + '-' * 42 + ' ' + '-' * 10)\n            line_format = '{:>4} {:>8} {:>16} {:>8} {:>16} {:>16} {:>10}'\n            print(line_format.format('Gen', 'Length', 'Fitness', 'Length',\n                                     'Fitness', 'OOB Fitness', 'Time Left'))\n\n        else:\n            # Estimate remaining time for run\n            gen = run_details['generation'][-1]\n            generation_time = run_details['generation_time'][-1]\n            remaining_time = (self.generations - gen - 1) * generation_time\n            if remaining_time > 60:\n                remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)\n            else:\n                remaining_time = '{0:.2f}s'.format(remaining_time)\n\n            oob_fitness = 'N/A'\n            line_format = '{:4d} {:8.2f} {:16g} {:8d} {:16g} {:>16} {:>10}'\n            if self.max_samples < 1.0:\n                oob_fitness = run_details['best_oob_fitness'][-1]\n                line_format = '{:4d} {:8.2f} {:16g} {:8d} {:16g} {:16g} {:>10}'\n\n            print(line_format.format(run_details['generation'][-1],\n                                     run_details['average_length'][-1],\n                                     run_details['average_fitness'][-1],\n                                     run_details['best_length'][-1],\n                                     run_details['best_fitness'][-1],\n                                     oob_fitness,\n                                     remaining_time))", "response": "A verbose report of the progress of the evolution process."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(self, X, y, sample_weight=None):\n        random_state = check_random_state(self.random_state)\n\n        # Check arrays\n        if isinstance(self, ClassifierMixin):\n            X, y = check_X_y(X, y, y_numeric=False)\n            check_classification_targets(y)\n            self.classes_, y = np.unique(y, return_inverse=True)\n            n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))\n            if n_trim_classes != 2:\n                raise ValueError(\"y contains %d class after sample_weight \"\n                                 \"trimmed classes with zero weights, while 2 \"\n                                 \"classes are required.\"\n                                 % n_trim_classes)\n            self.n_classes_ = len(self.classes_)\n        else:\n            X, y = check_X_y(X, y, y_numeric=True)\n        if sample_weight is not None:\n            sample_weight = check_array(sample_weight, ensure_2d=False)\n        _, self.n_features_ = X.shape\n\n        hall_of_fame = self.hall_of_fame\n        if hall_of_fame is None:\n            hall_of_fame = self.population_size\n        if hall_of_fame > self.population_size or hall_of_fame < 1:\n            raise ValueError('hall_of_fame (%d) must be less than or equal to '\n                             'population_size (%d).' % (self.hall_of_fame,\n                                                        self.population_size))\n        n_components = self.n_components\n        if n_components is None:\n            n_components = hall_of_fame\n        if n_components > hall_of_fame or n_components < 1:\n            raise ValueError('n_components (%d) must be less than or equal to '\n                             'hall_of_fame (%d).' % (self.n_components,\n                                                     self.hall_of_fame))\n\n        self._function_set = []\n        for function in self.function_set:\n            if isinstance(function, str):\n                if function not in _function_map:\n                    raise ValueError('invalid function name %s found in '\n                                     '`function_set`.' % function)\n                self._function_set.append(_function_map[function])\n            elif isinstance(function, _Function):\n                self._function_set.append(function)\n            else:\n                raise ValueError('invalid type %s found in `function_set`.'\n                                 % type(function))\n        if not self._function_set:\n            raise ValueError('No valid functions found in `function_set`.')\n\n        # For point-mutation to find a compatible replacement node\n        self._arities = {}\n        for function in self._function_set:\n            arity = function.arity\n            self._arities[arity] = self._arities.get(arity, [])\n            self._arities[arity].append(function)\n\n        if isinstance(self.metric, _Fitness):\n            self._metric = self.metric\n        elif isinstance(self, RegressorMixin):\n            if self.metric not in ('mean absolute error', 'mse', 'rmse',\n                                   'pearson', 'spearman'):\n                raise ValueError('Unsupported metric: %s' % self.metric)\n            self._metric = _fitness_map[self.metric]\n        elif isinstance(self, ClassifierMixin):\n            if self.metric != 'log loss':\n                raise ValueError('Unsupported metric: %s' % self.metric)\n            self._metric = _fitness_map[self.metric]\n        elif isinstance(self, TransformerMixin):\n            if self.metric not in ('pearson', 'spearman'):\n                raise ValueError('Unsupported metric: %s' % self.metric)\n            self._metric = _fitness_map[self.metric]\n\n        self._method_probs = np.array([self.p_crossover,\n                                       self.p_subtree_mutation,\n                                       self.p_hoist_mutation,\n                                       self.p_point_mutation])\n        self._method_probs = np.cumsum(self._method_probs)\n\n        if self._method_probs[-1] > 1:\n            raise ValueError('The sum of p_crossover, p_subtree_mutation, '\n                             'p_hoist_mutation and p_point_mutation should '\n                             'total to 1.0 or less.')\n\n        if self.init_method not in ('half and half', 'grow', 'full'):\n            raise ValueError('Valid program initializations methods include '\n                             '\"grow\", \"full\" and \"half and half\". Given %s.'\n                             % self.init_method)\n\n        if not((isinstance(self.const_range, tuple) and\n                len(self.const_range) == 2) or self.const_range is None):\n            raise ValueError('const_range should be a tuple with length two, '\n                             'or None.')\n\n        if (not isinstance(self.init_depth, tuple) or\n                len(self.init_depth) != 2):\n            raise ValueError('init_depth should be a tuple with length two.')\n        if self.init_depth[0] > self.init_depth[1]:\n            raise ValueError('init_depth should be in increasing numerical '\n                             'order: (min_depth, max_depth).')\n\n        if self.feature_names is not None:\n            if self.n_features_ != len(self.feature_names):\n                raise ValueError('The supplied `feature_names` has different '\n                                 'length to n_features. Expected %d, got %d.'\n                                 % (self.n_features_, len(self.feature_names)))\n            for feature_name in self.feature_names:\n                if not isinstance(feature_name, str):\n                    raise ValueError('invalid type %s found in '\n                                     '`feature_names`.' % type(feature_name))\n\n        if self.transformer is not None:\n            if isinstance(self.transformer, _Function):\n                self._transformer = self.transformer\n            elif self.transformer == 'sigmoid':\n                self._transformer = sigmoid\n            else:\n                raise ValueError('Invalid `transformer`. Expected either '\n                                 '\"sigmoid\" or _Function object, got %s' %\n                                 type(self.transformer))\n            if self._transformer.arity != 1:\n                raise ValueError('Invalid arity for `transformer`. Expected 1, '\n                                 'got %d.' % (self._transformer.arity))\n\n        params = self.get_params()\n        params['_metric'] = self._metric\n        if hasattr(self, '_transformer'):\n            params['_transformer'] = self._transformer\n        else:\n            params['_transformer'] = None\n        params['function_set'] = self._function_set\n        params['arities'] = self._arities\n        params['method_probs'] = self._method_probs\n\n        if not self.warm_start or not hasattr(self, '_programs'):\n            # Free allocated memory, if any\n            self._programs = []\n            self.run_details_ = {'generation': [],\n                                 'average_length': [],\n                                 'average_fitness': [],\n                                 'best_length': [],\n                                 'best_fitness': [],\n                                 'best_oob_fitness': [],\n                                 'generation_time': []}\n\n        prior_generations = len(self._programs)\n        n_more_generations = self.generations - prior_generations\n\n        if n_more_generations < 0:\n            raise ValueError('generations=%d must be larger or equal to '\n                             'len(_programs)=%d when warm_start==True'\n                             % (self.generations, len(self._programs)))\n        elif n_more_generations == 0:\n            fitness = [program.raw_fitness_ for program in self._programs[-1]]\n            warn('Warm-start fitting without increasing n_estimators does not '\n                 'fit new programs.')\n\n        if self.warm_start:\n            # Generate and discard seeds that would have been produced on the\n            # initial fit call.\n            for i in range(len(self._programs)):\n                _ = random_state.randint(MAX_INT, size=self.population_size)\n\n        if self.verbose:\n            # Print header fields\n            self._verbose_reporter()\n\n        for gen in range(prior_generations, self.generations):\n\n            start_time = time()\n\n            if gen == 0:\n                parents = None\n            else:\n                parents = self._programs[gen - 1]\n\n            # Parallel loop\n            n_jobs, n_programs, starts = _partition_estimators(\n                self.population_size, self.n_jobs)\n            seeds = random_state.randint(MAX_INT, size=self.population_size)\n\n            population = Parallel(n_jobs=n_jobs,\n                                  verbose=int(self.verbose > 1))(\n                delayed(_parallel_evolve)(n_programs[i],\n                                          parents,\n                                          X,\n                                          y,\n                                          sample_weight,\n                                          seeds[starts[i]:starts[i + 1]],\n                                          params)\n                for i in range(n_jobs))\n\n            # Reduce, maintaining order across different n_jobs\n            population = list(itertools.chain.from_iterable(population))\n\n            fitness = [program.raw_fitness_ for program in population]\n            length = [program.length_ for program in population]\n\n            parsimony_coefficient = None\n            if self.parsimony_coefficient == 'auto':\n                parsimony_coefficient = (np.cov(length, fitness)[1, 0] /\n                                         np.var(length))\n            for program in population:\n                program.fitness_ = program.fitness(parsimony_coefficient)\n\n            self._programs.append(population)\n\n            # Remove old programs that didn't make it into the new population.\n            if not self.low_memory:\n                for old_gen in np.arange(gen, 0, -1):\n                    indices = []\n                    for program in self._programs[old_gen]:\n                        if program is not None:\n                            for idx in program.parents:\n                                if 'idx' in idx:\n                                    indices.append(program.parents[idx])\n                    indices = set(indices)\n                    for idx in range(self.population_size):\n                        if idx not in indices:\n                            self._programs[old_gen - 1][idx] = None\n            elif gen > 0:\n                # Remove old generations\n                self._programs[gen - 1] = None\n\n            # Record run details\n            if self._metric.greater_is_better:\n                best_program = population[np.argmax(fitness)]\n            else:\n                best_program = population[np.argmin(fitness)]\n\n            self.run_details_['generation'].append(gen)\n            self.run_details_['average_length'].append(np.mean(length))\n            self.run_details_['average_fitness'].append(np.mean(fitness))\n            self.run_details_['best_length'].append(best_program.length_)\n            self.run_details_['best_fitness'].append(best_program.raw_fitness_)\n            oob_fitness = np.nan\n            if self.max_samples < 1.0:\n                oob_fitness = best_program.oob_fitness_\n            self.run_details_['best_oob_fitness'].append(oob_fitness)\n            generation_time = time() - start_time\n            self.run_details_['generation_time'].append(generation_time)\n\n            if self.verbose:\n                self._verbose_reporter(self.run_details_)\n\n            # Check for early stopping\n            if self._metric.greater_is_better:\n                best_fitness = fitness[np.argmax(fitness)]\n                if best_fitness >= self.stopping_criteria:\n                    break\n            else:\n                best_fitness = fitness[np.argmin(fitness)]\n                if best_fitness <= self.stopping_criteria:\n                    break\n\n        if isinstance(self, TransformerMixin):\n            # Find the best individuals in the final generation\n            fitness = np.array(fitness)\n            if self._metric.greater_is_better:\n                hall_of_fame = fitness.argsort()[::-1][:self.hall_of_fame]\n            else:\n                hall_of_fame = fitness.argsort()[:self.hall_of_fame]\n            evaluation = np.array([gp.execute(X) for gp in\n                                   [self._programs[-1][i] for\n                                    i in hall_of_fame]])\n            if self.metric == 'spearman':\n                evaluation = np.apply_along_axis(rankdata, 1, evaluation)\n\n            with np.errstate(divide='ignore', invalid='ignore'):\n                correlations = np.abs(np.corrcoef(evaluation))\n            np.fill_diagonal(correlations, 0.)\n            components = list(range(self.hall_of_fame))\n            indices = list(range(self.hall_of_fame))\n            # Iteratively remove least fit individual of most correlated pair\n            while len(components) > self.n_components:\n                most_correlated = np.unravel_index(np.argmax(correlations),\n                                                   correlations.shape)\n                # The correlation matrix is sorted by fitness, so identifying\n                # the least fit of the pair is simply getting the higher index\n                worst = max(most_correlated)\n                components.pop(worst)\n                indices.remove(worst)\n                correlations = correlations[:, indices][indices, :]\n                indices = list(range(len(components)))\n            self._best_programs = [self._programs[-1][i] for i in\n                                   hall_of_fame[components]]\n\n        else:\n            # Find the best individual in the final generation\n            if self._metric.greater_is_better:\n                self._program = self._programs[-1][np.argmax(fitness)]\n            else:\n                self._program = self._programs[-1][np.argmin(fitness)]\n\n        return self", "response": "Fit the Genetic Program according to X y."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform regression on test vectors X.", "response": "def predict(self, X):\n        \"\"\"Perform regression on test vectors X.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        y : array, shape = [n_samples]\n            Predicted values for X.\n\n        \"\"\"\n        if not hasattr(self, '_program'):\n            raise NotFittedError('SymbolicRegressor not fitted.')\n\n        X = check_array(X)\n        _, n_features = X.shape\n        if self.n_features_ != n_features:\n            raise ValueError('Number of features of the model must match the '\n                             'input. Model n_features is %s and input '\n                             'n_features is %s.'\n                             % (self.n_features_, n_features))\n\n        y = self._program.execute(X)\n\n        return y"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict_proba(self, X):\n        if not hasattr(self, '_program'):\n            raise NotFittedError('SymbolicClassifier not fitted.')\n\n        X = check_array(X)\n        _, n_features = X.shape\n        if self.n_features_ != n_features:\n            raise ValueError('Number of features of the model must match the '\n                             'input. Model n_features is %s and input '\n                             'n_features is %s.'\n                             % (self.n_features_, n_features))\n\n        scores = self._program.execute(X)\n        proba = self._transformer(scores)\n        proba = np.vstack([1 - proba, proba]).T\n        return proba", "response": "Predict probabilities on test vectors X."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\npredicts classes on test vectors X.", "response": "def predict(self, X):\n        \"\"\"Predict classes on test vectors X.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            Input vectors, where n_samples is the number of samples\n            and n_features is the number of features.\n\n        Returns\n        -------\n        y : array, shape = [n_samples,]\n            The predicted classes of the input samples.\n\n        \"\"\"\n        proba = self.predict_proba(X)\n        return self.classes_.take(np.argmax(proba, axis=1), axis=0)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform(self, X):\n        if not hasattr(self, '_best_programs'):\n            raise NotFittedError('SymbolicTransformer not fitted.')\n\n        X = check_array(X)\n        _, n_features = X.shape\n        if self.n_features_ != n_features:\n            raise ValueError('Number of features of the model must match the '\n                             'input. Model n_features is %s and input '\n                             'n_features is %s.'\n                             % (self.n_features_, n_features))\n\n        X_new = np.array([gp.execute(X) for gp in self._best_programs]).T\n\n        return X_new", "response": "Transform X according to the fitted transformer."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a fitness measure a metric scoring the quality of a program s fit.", "response": "def make_fitness(function, greater_is_better):\n    \"\"\"Make a fitness measure, a metric scoring the quality of a program's fit.\n\n    This factory function creates a fitness measure object which measures the\n    quality of a program's fit and thus its likelihood to undergo genetic\n    operations into the next generation. The resulting object is able to be\n    called with NumPy vectorized arguments and return a resulting floating\n    point score quantifying the quality of the program's representation of the\n    true relationship.\n\n    Parameters\n    ----------\n    function : callable\n        A function with signature function(y, y_pred, sample_weight) that\n        returns a floating point number. Where `y` is the input target y\n        vector, `y_pred` is the predicted values from the genetic program, and\n        sample_weight is the sample_weight vector.\n\n    greater_is_better : bool\n        Whether a higher value from `function` indicates a better fit. In\n        general this would be False for metrics indicating the magnitude of\n        the error, and True for metrics indicating the quality of fit.\n\n    \"\"\"\n    if not isinstance(greater_is_better, bool):\n        raise ValueError('greater_is_better must be bool, got %s'\n                         % type(greater_is_better))\n    if function.__code__.co_argcount != 3:\n        raise ValueError('function requires 3 arguments (y, y_pred, w),'\n                         ' got %d.' % function.__code__.co_argcount)\n    if not isinstance(function(np.array([1, 1]),\n                      np.array([2, 2]),\n                      np.array([1, 1])), numbers.Number):\n        raise ValueError('function must return a numeric.')\n\n    return _Fitness(function, greater_is_better)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the weighted Pearson correlation coefficient.", "response": "def _weighted_pearson(y, y_pred, w):\n    \"\"\"Calculate the weighted Pearson correlation coefficient.\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        y_pred_demean = y_pred - np.average(y_pred, weights=w)\n        y_demean = y - np.average(y, weights=w)\n        corr = ((np.sum(w * y_pred_demean * y_demean) / np.sum(w)) /\n                np.sqrt((np.sum(w * y_pred_demean ** 2) *\n                         np.sum(w * y_demean ** 2)) /\n                        (np.sum(w) ** 2)))\n    if np.isfinite(corr):\n        return np.abs(corr)\n    return 0."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _weighted_spearman(y, y_pred, w):\n    y_pred_ranked = np.apply_along_axis(rankdata, 0, y_pred)\n    y_ranked = np.apply_along_axis(rankdata, 0, y)\n    return _weighted_pearson(y_pred_ranked, y_ranked, w)", "response": "Calculate the weighted Spearman correlation coefficient."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the mean absolute error.", "response": "def _mean_absolute_error(y, y_pred, w):\n    \"\"\"Calculate the mean absolute error.\"\"\"\n    return np.average(np.abs(y_pred - y), weights=w)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the mean square error.", "response": "def _mean_square_error(y, y_pred, w):\n    \"\"\"Calculate the mean square error.\"\"\"\n    return np.average(((y_pred - y) ** 2), weights=w)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the root mean square error.", "response": "def _root_mean_square_error(y, y_pred, w):\n    \"\"\"Calculate the root mean square error.\"\"\"\n    return np.sqrt(np.average(((y_pred - y) ** 2), weights=w))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _log_loss(y, y_pred, w):\n    eps = 1e-15\n    inv_y_pred = np.clip(1 - y_pred, eps, 1 - eps)\n    y_pred = np.clip(y_pred, eps, 1 - eps)\n    score = y * np.log(y_pred) + (1 - y) * np.log(inv_y_pred)\n    return np.average(-score, weights=w)", "response": "Calculate the log loss."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build_program(self, random_state):\n        if self.init_method == 'half and half':\n            method = ('full' if random_state.randint(2) else 'grow')\n        else:\n            method = self.init_method\n        max_depth = random_state.randint(*self.init_depth)\n\n        # Start a program with a function to avoid degenerative programs\n        function = random_state.randint(len(self.function_set))\n        function = self.function_set[function]\n        program = [function]\n        terminal_stack = [function.arity]\n\n        while terminal_stack:\n            depth = len(terminal_stack)\n            choice = self.n_features + len(self.function_set)\n            choice = random_state.randint(choice)\n            # Determine if we are adding a function or terminal\n            if (depth < max_depth) and (method == 'full' or\n                                        choice <= len(self.function_set)):\n                function = random_state.randint(len(self.function_set))\n                function = self.function_set[function]\n                program.append(function)\n                terminal_stack.append(function.arity)\n            else:\n                # We need a terminal, add a variable or constant\n                if self.const_range is not None:\n                    terminal = random_state.randint(self.n_features + 1)\n                else:\n                    terminal = random_state.randint(self.n_features)\n                if terminal == self.n_features:\n                    terminal = random_state.uniform(*self.const_range)\n                    if self.const_range is None:\n                        # We should never get here\n                        raise ValueError('A constant was produced with '\n                                         'const_range=None.')\n                program.append(terminal)\n                terminal_stack[-1] -= 1\n                while terminal_stack[-1] == 0:\n                    terminal_stack.pop()\n                    if not terminal_stack:\n                        return program\n                    terminal_stack[-1] -= 1\n\n        # We should never get here\n        return None", "response": "Builds a naive random program."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_program(self):\n        terminals = [0]\n        for node in self.program:\n            if isinstance(node, _Function):\n                terminals.append(node.arity)\n            else:\n                terminals[-1] -= 1\n                while terminals[-1] == 0:\n                    terminals.pop()\n                    terminals[-1] -= 1\n        return terminals == [-1]", "response": "Rough check that the embedded program in the object is valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a string Graphviz script for visualizing the program.", "response": "def export_graphviz(self, fade_nodes=None):\n        \"\"\"Returns a string, Graphviz script for visualizing the program.\n\n        Parameters\n        ----------\n        fade_nodes : list, optional\n            A list of node indices to fade out for showing which were removed\n            during evolution.\n\n        Returns\n        -------\n        output : string\n            The Graphviz script to plot the tree representation of the program.\n\n        \"\"\"\n        terminals = []\n        if fade_nodes is None:\n            fade_nodes = []\n        output = 'digraph program {\\nnode [style=filled]\\n'\n        for i, node in enumerate(self.program):\n            fill = '#cecece'\n            if isinstance(node, _Function):\n                if i not in fade_nodes:\n                    fill = '#136ed4'\n                terminals.append([node.arity, i])\n                output += ('%d [label=\"%s\", fillcolor=\"%s\"] ;\\n'\n                           % (i, node.name, fill))\n            else:\n                if i not in fade_nodes:\n                    fill = '#60a6f6'\n                if isinstance(node, int):\n                    if self.feature_names is None:\n                        feature_name = 'X%s' % node\n                    else:\n                        feature_name = self.feature_names[node]\n                    output += ('%d [label=\"%s\", fillcolor=\"%s\"] ;\\n'\n                               % (i, feature_name, fill))\n                else:\n                    output += ('%d [label=\"%.3f\", fillcolor=\"%s\"] ;\\n'\n                               % (i, node, fill))\n                if i == 0:\n                    # A degenerative program of only one node\n                    return output + '}'\n                terminals[-1][0] -= 1\n                terminals[-1].append(i)\n                while terminals[-1][0] == 0:\n                    output += '%d -> %d ;\\n' % (terminals[-1][1],\n                                                terminals[-1][-1])\n                    terminals[-1].pop()\n                    if len(terminals[-1]) == 2:\n                        parent = terminals[-1][-1]\n                        terminals.pop()\n                        if not terminals:\n                            return output + '}'\n                        terminals[-1].append(parent)\n                        terminals[-1][0] -= 1\n\n        # We should never get here\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _depth(self):\n        terminals = [0]\n        depth = 1\n        for node in self.program:\n            if isinstance(node, _Function):\n                terminals.append(node.arity)\n                depth = max(len(terminals), depth)\n            else:\n                terminals[-1] -= 1\n                while terminals[-1] == 0:\n                    terminals.pop()\n                    terminals[-1] -= 1\n        return depth - 1", "response": "Calculates the maximum depth of the program tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nexecute the program according to X.", "response": "def execute(self, X):\n        \"\"\"Execute the program according to X.\n\n        Parameters\n        ----------\n        X : {array-like}, shape = [n_samples, n_features]\n            Training vectors, where n_samples is the number of samples and\n            n_features is the number of features.\n\n        Returns\n        -------\n        y_hats : array-like, shape = [n_samples]\n            The result of executing the program on X.\n\n        \"\"\"\n        # Check for single-node programs\n        node = self.program[0]\n        if isinstance(node, float):\n            return np.repeat(node, X.shape[0])\n        if isinstance(node, int):\n            return X[:, node]\n\n        apply_stack = []\n\n        for node in self.program:\n\n            if isinstance(node, _Function):\n                apply_stack.append([node])\n            else:\n                # Lazily evaluate later\n                apply_stack[-1].append(node)\n\n            while len(apply_stack[-1]) == apply_stack[-1][0].arity + 1:\n                # Apply functions that have sufficient arguments\n                function = apply_stack[-1][0]\n                terminals = [np.repeat(t, X.shape[0]) if isinstance(t, float)\n                             else X[:, t] if isinstance(t, int)\n                             else t for t in apply_stack[-1][1:]]\n                intermediate_result = function(*terminals)\n                if len(apply_stack) != 1:\n                    apply_stack.pop()\n                    apply_stack[-1].append(intermediate_result)\n                else:\n                    return intermediate_result\n\n        # We should never get here\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_indices(self, n_samples=None, max_samples=None,\n                        random_state=None):\n        \"\"\"Get the indices on which to evaluate the fitness of a program.\n\n        Parameters\n        ----------\n        n_samples : int\n            The number of samples.\n\n        max_samples : int\n            The maximum number of samples to use.\n\n        random_state : RandomState instance\n            The random number generator.\n\n        Returns\n        -------\n        indices : array-like, shape = [n_samples]\n            The in-sample indices.\n\n        not_indices : array-like, shape = [n_samples]\n            The out-of-sample indices.\n\n        \"\"\"\n        if self._indices_state is None and random_state is None:\n            raise ValueError('The program has not been evaluated for fitness '\n                             'yet, indices not available.')\n\n        if n_samples is not None and self._n_samples is None:\n            self._n_samples = n_samples\n        if max_samples is not None and self._max_samples is None:\n            self._max_samples = max_samples\n        if random_state is not None and self._indices_state is None:\n            self._indices_state = random_state.get_state()\n\n        indices_state = check_random_state(None)\n        indices_state.set_state(self._indices_state)\n\n        not_indices = sample_without_replacement(\n            self._n_samples,\n            self._n_samples - self._max_samples,\n            random_state=indices_state)\n        sample_counts = np.bincount(not_indices, minlength=self._n_samples)\n        indices = np.where(sample_counts == 0)[0]\n\n        return indices, not_indices", "response": "Get the indices on which to evaluate the fitness of a program."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raw_fitness(self, X, y, sample_weight):\n        y_pred = self.execute(X)\n        if self.transformer:\n            y_pred = self.transformer(y_pred)\n        raw_fitness = self.metric(y, y_pred, sample_weight)\n\n        return raw_fitness", "response": "Evaluate the raw fitness of the program according to X y and sample_weight."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef fitness(self, parsimony_coefficient=None):\n        if parsimony_coefficient is None:\n            parsimony_coefficient = self.parsimony_coefficient\n        penalty = parsimony_coefficient * len(self.program) * self.metric.sign\n        return self.raw_fitness_ - penalty", "response": "Evaluate the penalized fitness of the program according to X y."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_subtree(self, random_state, program=None):\n        if program is None:\n            program = self.program\n        # Choice of crossover points follows Koza's (1992) widely used approach\n        # of choosing functions 90% of the time and leaves 10% of the time.\n        probs = np.array([0.9 if isinstance(node, _Function) else 0.1\n                          for node in program])\n        probs = np.cumsum(probs / probs.sum())\n        start = np.searchsorted(probs, random_state.uniform())\n\n        stack = 1\n        end = start\n        while stack > end - start:\n            node = program[end]\n            if isinstance(node, _Function):\n                stack += node.arity\n            end += 1\n\n        return start, end", "response": "Get a random subtree from the program."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming the crossover genetic operation on the program.", "response": "def crossover(self, donor, random_state):\n        \"\"\"Perform the crossover genetic operation on the program.\n\n        Crossover selects a random subtree from the embedded program to be\n        replaced. A donor also has a subtree selected at random and this is\n        inserted into the original parent to form an offspring.\n\n        Parameters\n        ----------\n        donor : list\n            The flattened tree representation of the donor program.\n\n        random_state : RandomState instance\n            The random number generator.\n\n        Returns\n        -------\n        program : list\n            The flattened tree representation of the program.\n\n        \"\"\"\n        # Get a subtree to replace\n        start, end = self.get_subtree(random_state)\n        removed = range(start, end)\n        # Get a subtree to donate\n        donor_start, donor_end = self.get_subtree(random_state, donor)\n        donor_removed = list(set(range(len(donor))) -\n                             set(range(donor_start, donor_end)))\n        # Insert genetic material from donor\n        return (self.program[:start] +\n                donor[donor_start:donor_end] +\n                self.program[end:]), removed, donor_removed"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nperforms the subtree mutation operation on the embedded program.", "response": "def subtree_mutation(self, random_state):\n        \"\"\"Perform the subtree mutation operation on the program.\n\n        Subtree mutation selects a random subtree from the embedded program to\n        be replaced. A donor subtree is generated at random and this is\n        inserted into the original parent to form an offspring. This\n        implementation uses the \"headless chicken\" method where the donor\n        subtree is grown using the initialization methods and a subtree of it\n        is selected to be donated to the parent.\n\n        Parameters\n        ----------\n        random_state : RandomState instance\n            The random number generator.\n\n        Returns\n        -------\n        program : list\n            The flattened tree representation of the program.\n\n        \"\"\"\n        # Build a new naive program\n        chicken = self.build_program(random_state)\n        # Do subtree mutation via the headless chicken method!\n        return self.crossover(chicken, random_state)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nperforming the hoist mutation on the embedded program.", "response": "def hoist_mutation(self, random_state):\n        \"\"\"Perform the hoist mutation operation on the program.\n\n        Hoist mutation selects a random subtree from the embedded program to\n        be replaced. A random subtree of that subtree is then selected and this\n        is 'hoisted' into the original subtrees location to form an offspring.\n        This method helps to control bloat.\n\n        Parameters\n        ----------\n        random_state : RandomState instance\n            The random number generator.\n\n        Returns\n        -------\n        program : list\n            The flattened tree representation of the program.\n\n        \"\"\"\n        # Get a subtree to replace\n        start, end = self.get_subtree(random_state)\n        subtree = self.program[start:end]\n        # Get a subtree of the subtree to hoist\n        sub_start, sub_end = self.get_subtree(random_state, subtree)\n        hoist = subtree[sub_start:sub_end]\n        # Determine which nodes were removed for plotting\n        removed = list(set(range(start, end)) -\n                       set(range(start + sub_start, start + sub_end)))\n        return self.program[:start] + hoist + self.program[end:], removed"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef point_mutation(self, random_state):\n        program = copy(self.program)\n\n        # Get the nodes to modify\n        mutate = np.where(random_state.uniform(size=len(program)) <\n                          self.p_point_replace)[0]\n\n        for node in mutate:\n            if isinstance(program[node], _Function):\n                arity = program[node].arity\n                # Find a valid replacement with same arity\n                replacement = len(self.arities[arity])\n                replacement = random_state.randint(replacement)\n                replacement = self.arities[arity][replacement]\n                program[node] = replacement\n            else:\n                # We've got a terminal, add a const or variable\n                if self.const_range is not None:\n                    terminal = random_state.randint(self.n_features + 1)\n                else:\n                    terminal = random_state.randint(self.n_features)\n                if terminal == self.n_features:\n                    terminal = random_state.uniform(*self.const_range)\n                    if self.const_range is None:\n                        # We should never get here\n                        raise ValueError('A constant was produced with '\n                                         'const_range=None.')\n                program[node] = terminal\n\n        return program, list(mutate)", "response": "Perform the point mutation operation on the embedded program."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef make_function(function, name, arity):\n    if not isinstance(arity, int):\n        raise ValueError('arity must be an int, got %s' % type(arity))\n    if not isinstance(function, np.ufunc):\n        if function.__code__.co_argcount != arity:\n            raise ValueError('arity %d does not match required number of '\n                             'function arguments of %d.'\n                             % (arity, function.__code__.co_argcount))\n    if not isinstance(name, str):\n        raise ValueError('name must be a string, got %s' % type(name))\n\n    # Check output shape\n    args = [np.ones(10) for _ in range(arity)]\n    try:\n        function(*args)\n    except ValueError:\n        raise ValueError('supplied function %s does not support arity of %d.'\n                         % (name, arity))\n    if not hasattr(function(*args), 'shape'):\n        raise ValueError('supplied function %s does not return a numpy array.'\n                         % name)\n    if function(*args).shape != (10,):\n        raise ValueError('supplied function %s does not return same shape as '\n                         'input vectors.' % name)\n\n    # Check closure for zero & negative input arguments\n    args = [np.zeros(10) for _ in range(arity)]\n    if not np.all(np.isfinite(function(*args))):\n        raise ValueError('supplied function %s does not have closure against '\n                         'zeros in argument vectors.' % name)\n    args = [-1 * np.ones(10) for _ in range(arity)]\n    if not np.all(np.isfinite(function(*args))):\n        raise ValueError('supplied function %s does not have closure against '\n                         'negatives in argument vectors.' % name)\n\n    return _Function(function, name, arity)", "response": "This factory function creates a function node that is able to be called with NumPy vectorized\n    arguments and returns a Numpy vectorized\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nclosuring of division for zero denominator.", "response": "def _protected_division(x1, x2):\n    \"\"\"Closure of division (x1/x2) for zero denominator.\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        return np.where(np.abs(x2) > 0.001, np.divide(x1, x2), 1.)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclosuring of log for zero arguments.", "response": "def _protected_log(x1):\n    \"\"\"Closure of log for zero arguments.\"\"\"\n    with np.errstate(divide='ignore', invalid='ignore'):\n        return np.where(np.abs(x1) > 0.001, np.log(np.abs(x1)), 0.)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _protected_inverse(x1):\n    with np.errstate(divide='ignore', invalid='ignore'):\n        return np.where(np.abs(x1) > 0.001, 1. / x1, 0.)", "response": "Closure of log for zero arguments."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the smallest regions of the image.", "response": "def _generate_segments(im_orig, scale, sigma, min_size):\n    \"\"\"\n        segment smallest regions by the algorithm of Felzenswalb and\n        Huttenlocher\n    \"\"\"\n\n    # open the Image\n    im_mask = skimage.segmentation.felzenszwalb(\n        skimage.util.img_as_float(im_orig), scale=scale, sigma=sigma,\n        min_size=min_size)\n\n    # merge mask channel to the image as a 4th channel\n    im_orig = numpy.append(\n        im_orig, numpy.zeros(im_orig.shape[:2])[:, :, numpy.newaxis], axis=2)\n    im_orig[:, :, 3] = im_mask\n\n    return im_orig"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sim_colour(r1, r2):\n    return sum([min(a, b) for a, b in zip(r1[\"hist_c\"], r2[\"hist_c\"])])", "response": "calculate the sum of histogram intersection of colour\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _sim_texture(r1, r2):\n    return sum([min(a, b) for a, b in zip(r1[\"hist_t\"], r2[\"hist_t\"])])", "response": "calculate the sum of histogram intersection of texture\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the fill similarity over the image", "response": "def _sim_fill(r1, r2, imsize):\n    \"\"\"\n        calculate the fill similarity over the image\n    \"\"\"\n    bbsize = (\n        (max(r1[\"max_x\"], r2[\"max_x\"]) - min(r1[\"min_x\"], r2[\"min_x\"]))\n        * (max(r1[\"max_y\"], r2[\"max_y\"]) - min(r1[\"min_y\"], r2[\"min_y\"]))\n    )\n    return 1.0 - (bbsize - r1[\"size\"] - r2[\"size\"]) / imsize"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calc_colour_hist(img):\n\n    BINS = 25\n    hist = numpy.array([])\n\n    for colour_channel in (0, 1, 2):\n\n        # extracting one colour channel\n        c = img[:, colour_channel]\n\n        # calculate histogram for each colour and join to the result\n        hist = numpy.concatenate(\n            [hist] + [numpy.histogram(c, BINS, (0.0, 255.0))[0]])\n\n    # L1 normalize\n    hist = hist / len(img)\n\n    return hist", "response": "calculate colour histogram for each region of the image"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calc_texture_gradient(img):\n    ret = numpy.zeros((img.shape[0], img.shape[1], img.shape[2]))\n\n    for colour_channel in (0, 1, 2):\n        ret[:, :, colour_channel] = skimage.feature.local_binary_pattern(\n            img[:, :, colour_channel], 8, 1.0)\n\n    return ret", "response": "Calculate the gradient of the image using SelectiveSearch."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calc_texture_hist(img):\n    BINS = 10\n\n    hist = numpy.array([])\n\n    for colour_channel in (0, 1, 2):\n\n        # mask by the colour channel\n        fd = img[:, colour_channel]\n\n        # calculate histogram for each orientation and concatenate them all\n        # and join to the result\n        hist = numpy.concatenate(\n            [hist] + [numpy.histogram(fd, BINS, (0.0, 1.0))[0]])\n\n    # L1 Normalize\n    hist = hist / len(img)\n\n    return hist", "response": "calculate texture histogram for each region and colours"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef selective_search(\n        im_orig, scale=1.0, sigma=0.8, min_size=50):\n    '''Selective Search\n\n    Parameters\n    ----------\n        im_orig : ndarray\n            Input image\n        scale : int\n            Free parameter. Higher means larger clusters in felzenszwalb segmentation.\n        sigma : float\n            Width of Gaussian kernel for felzenszwalb segmentation.\n        min_size : int\n            Minimum component size for felzenszwalb segmentation.\n    Returns\n    -------\n        img : ndarray\n            image with region label\n            region label is stored in the 4th value of each pixel [r,g,b,(region)]\n        regions : array of dict\n            [\n                {\n                    'rect': (left, top, width, height),\n                    'labels': [...],\n                    'size': component_size\n                },\n                ...\n            ]\n    '''\n    assert im_orig.shape[2] == 3, \"3ch image is expected\"\n\n    # load image and get smallest regions\n    # region label is stored in the 4th value of each pixel [r,g,b,(region)]\n    img = _generate_segments(im_orig, scale, sigma, min_size)\n\n    if img is None:\n        return None, {}\n\n    imsize = img.shape[0] * img.shape[1]\n    R = _extract_regions(img)\n\n    # extract neighbouring information\n    neighbours = _extract_neighbours(R)\n\n    # calculate initial similarities\n    S = {}\n    for (ai, ar), (bi, br) in neighbours:\n        S[(ai, bi)] = _calc_sim(ar, br, imsize)\n\n    # hierarchal search\n    while S != {}:\n\n        # get highest similarity\n        i, j = sorted(S.items(), key=lambda i: i[1])[-1][0]\n\n        # merge corresponding regions\n        t = max(R.keys()) + 1.0\n        R[t] = _merge_regions(R[i], R[j])\n\n        # mark similarities for regions to be removed\n        key_to_delete = []\n        for k, v in list(S.items()):\n            if (i in k) or (j in k):\n                key_to_delete.append(k)\n\n        # remove old similarities of related regions\n        for k in key_to_delete:\n            del S[k]\n\n        # calculate similarity set with the new region\n        for k in [a for a in key_to_delete if a != (i, j)]:\n            n = k[1] if k[0] in (i, j) else k[0]\n            S[(t, n)] = _calc_sim(R[t], R[n], imsize)\n\n    regions = []\n    for k, r in list(R.items()):\n        regions.append({\n            'rect': (\n                r['min_x'], r['min_y'],\n                r['max_x'] - r['min_x'], r['max_y'] - r['min_y']),\n            'size': r['size'],\n            'labels': r['labels']\n        })\n\n    return img, regions", "response": "Selective Search of the image with the given image and returns a list of dicts containing the image and the set of unique image."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresets the current state of the object.", "response": "def reset(self, old_scene=None, screen=None):\n        \"\"\"\n        Reset the scene ready for playing.\n\n        :param old_scene: The previous version of this Scene that was running before the\n            application reset - e.g. due to a screen resize.\n        :param screen: New screen to use if old_scene is not None.\n        \"\"\"\n        # Always reset all the effects.\n        for effect in self._effects:\n            effect.reset()\n\n        # If we have an old Scene to recreate, get the data out of that and\n        # apply it where possible by cloning objects where appropriate.\n        if old_scene:\n            for old_effect in old_scene.effects:\n                # Using the \"easier to ask forgiveness...\" mantra, just try\n                # cloning everything and ignore any AttributeErrors.\n                try:\n                    old_effect.clone(screen, self)\n                except AttributeError:\n                    pass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_effect(self, effect, reset=True):\n        # Reset the effect in case this is in the middle of a Scene.\n        if reset:\n            effect.reset()\n        effect.register_scene(self)\n        self._effects.append(effect)", "response": "Add an effect to the Scene."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef process_event(self, event):\n        for effect in reversed(self._effects):\n            event = effect.process_event(event)\n            if event is None:\n                break\n        return event", "response": "Process a new input event."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _spline(t, p0, p1, p2, p3):\n    return (\n        t * ((2 - t) * t - 1) * p0 +\n        (t * t * (3 * t - 5) + 2) * p1 +\n        t * ((4 - 3 * t) * t + 1) * p2 +\n        (t - 1) * t * t * p3) / 2", "response": "Cubic spline interpolation for 4 given points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef next_pos(self):\n        result = None\n        if self._index <= len(self._steps):\n            result = self._steps[self._index]\n            self._index += 1\n        return result", "response": "Returns the next position tuple for the Sprite on this path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_step(self, pos):\n        self._steps.append(pos)\n        self._rec_x = pos[0]\n        self._rec_y = pos[1]", "response": "Add a step to the end of the current recorded path."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nwait at the current location for the specified number of iterations.", "response": "def wait(self, delay):\n        \"\"\"\n        Wait at the current location for the specified number of iterations.\n\n        :param delay: The time to wait (in animation frames).\n        \"\"\"\n        for _ in range(0, delay):\n            self._add_step((self._rec_x, self._rec_y))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef move_straight_to(self, x, y, steps):\n        start_x = self._rec_x\n        start_y = self._rec_y\n        for i in range(1, steps + 1):\n            self._add_step((\n                int(start_x + (x - start_x) / float(steps) * i),\n                int(start_y + (y - start_y) / float(steps) * i)))", "response": "Move the line path from the current location to the specified point."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef move_round_to(self, points, steps):\n        # Spline interpolation needs a before and after point for the curve.\n        # Duplicate the first and last points to handle this.  We also need\n        # to move from the current position to the first specified point.\n        points.insert(0, (self._rec_x, self._rec_y))\n        points.insert(0, (self._rec_x, self._rec_y))\n        points.append(points[-1])\n\n        # Convert the points into an interpolated set of more detailed points.\n        steps_per_spline = steps // (len(points) - 3)\n        for j in range(1, len(points) - 2):\n            for t in range(1, steps_per_spline + 1):\n                y = _spline(float(t) / steps_per_spline,\n                            float(points[j - 1][1]),\n                            float(points[j][1]),\n                            float(points[j + 1][1]),\n                            float(points[j + 2][1]))\n                x = int(points[j][0] + ((points[j + 1][0] - points[j][0]) *\n                                        float(t) / steps_per_spline))\n                self._add_step((x, int(y)))", "response": "This method will move the path to the next point in the path."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef reset(self):\n        self._x = self._start_x\n        self._y = self._start_y", "response": "Reset the x and y values for the current path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _enforce_width(text, width, unicode_aware=True):\n    # Double-width strings cannot be more than twice the string length, so no need to try\n    # expensive truncation if this upper bound isn't an issue.\n    if 2 * len(text) < width:\n        return text\n\n    # Can still optimize performance if we are not handling unicode characters.\n    if unicode_aware:\n        size = 0\n        for i, c in enumerate(text):\n            w = wcwidth(c) if ord(c) >= 256 else 1\n            if size + w > width:\n                return text[0:i]\n            size += w\n    elif len(text) + 1 > width:\n        return text[0:width]\n    return text", "response": "Enforces a displayed piece of text to be a certain number of cells wide."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the starting point in the string that will reduce it to the required length.", "response": "def _find_min_start(text, max_width, unicode_aware=True, at_end=False):\n    \"\"\"\n    Find the starting point in the string that will reduce it to be less than or equal to the\n    specified width when displayed on screen.\n\n    :param text: The text to analyze.\n    :param max_width: The required maximum width\n    :param at_end: At the end of the editable line, so allow spaced for cursor.\n\n    :return: The offset within `text` to start at to reduce it to the required length.\n    \"\"\"\n    # Is the solution trivial?  Worth optimizing for text heavy UIs...\n    if 2 * len(text) < max_width:\n        return 0\n\n    # OK - do it the hard way...\n    result = 0\n    string_len = wcswidth if unicode_aware else len\n    char_len = wcwidth if unicode_aware else lambda x: 1\n    display_end = string_len(text)\n    while display_end > max_width:\n        result += 1\n        display_end -= char_len(text[0])\n        text = text[1:]\n    if at_end and display_end == max_width:\n        result += 1\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_offset(text, visible_width, unicode_aware=True):\n    result = 0\n    width = 0\n    if unicode_aware:\n        for c in text:\n            if visible_width - width <= 0:\n                break\n            result += 1\n            width += wcwidth(c)\n        if visible_width - width < 0:\n            result -= 1\n    else:\n        result = min(len(text), visible_width)\n    return result", "response": "Find the character offset within some text within a given visible width."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsplitting the given text into a list of strings.", "response": "def _split_text(text, width, height, unicode_aware=True):\n    \"\"\"\n    Split text to required dimensions.\n\n    This will first try to split the text into multiple lines, then put a \"...\" on the last\n    3 characters of the last line if this still doesn't fit.\n\n    :param text: The text to split.\n    :param width: The maximum width for any line.\n    :param height: The maximum height for the resulting text.\n    :return: A list of strings of the broken up text.\n    \"\"\"\n    # At a high level, just try to split on whitespace for the best results.\n    tokens = text.split(\" \")\n    result = []\n    current_line = \"\"\n    string_len = wcswidth if unicode_aware else len\n    for token in tokens:\n        for i, line_token in enumerate(token.split(\"\\n\")):\n            if string_len(current_line + line_token) > width or i > 0:\n                # Don't bother inserting completely blank lines - which should only happen on the very first\n                # line (as the rest will inject whitespace/newlines)\n                if len(current_line) > 0:\n                    result.append(current_line.rstrip())\n                current_line = line_token + \" \"\n            else:\n                current_line += line_token + \" \"\n\n    # At this point we've either split nicely or have a hugely long unbroken string (e.g. because the\n    # language doesn't use whitespace.  Either way, break this last line up as best we can.\n    current_line = current_line.rstrip()\n    while string_len(current_line) > 0:\n        new_line = _enforce_width(current_line, width, unicode_aware)\n        result.append(new_line)\n        current_line = current_line[len(new_line):]\n\n    # Check for a height overrun and truncate.\n    if len(result) > height:\n        result = result[:height]\n        result[height - 1] = result[height - 1][:width - 3] + \"...\"\n\n    # Very small columns could be shorter than individual words - truncate\n    # each line if necessary.\n    for i, line in enumerate(result):\n        if len(line) > width:\n            result[i] = line[:width - 3] + \"...\"\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_pos(self):\n        if self._canvas.height >= self._max_height:\n            return 0\n        else:\n            return self._canvas.start_line / (self._max_height - self._canvas.height + 1)", "response": "Get current position for scroll bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_pos(self, pos):\n        if self._canvas.height < self._max_height:\n            pos *= self._max_height - self._canvas.height + 1\n            pos = int(round(max(0, pos), 0))\n            self._canvas.scroll_to(pos)", "response": "Set current position for scroll bar."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_layout(self, layout):\n        layout.register_frame(self)\n        self._layouts.append(layout)", "response": "Add a Layout to the Frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an Effect to the Frame.", "response": "def add_effect(self, effect):\n        \"\"\"\n        Add an Effect to the Frame.\n\n        :param effect: The Effect to be added.\n        \"\"\"\n        effect.register_scene(self._scene)\n        self._effects.append(effect)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fix(self):\n        # Do up to 2 passes in case we have a variable height Layout.\n        fill_layout = None\n        fill_height = y = 0\n        for _ in range(2):\n            # Pick starting point/height - varies for borders.\n            if self._has_border:\n                x = y = start_y = 1\n                height = self._canvas.height - 2\n                width = self._canvas.width - 2\n            else:\n                x = y = start_y = 0\n                height = self._canvas.height\n                width = self._canvas.width\n\n            # Process each Layout in the Frame - getting required height for\n            # each.\n            for layout in self._layouts:\n                if layout.fill_frame:\n                    if fill_layout is None:\n                        # First pass - remember it for now.\n                        fill_layout = layout\n                    elif fill_layout == layout:\n                        # Second pass - pass in max height\n                        y = layout.fix(x, y, width, fill_height)\n                    else:\n                        # A second filler - this is a bug in the application.\n                        raise Highlander(\"Too many Layouts filling Frame\")\n                else:\n                    y = layout.fix(x, y, width, height)\n\n            # If we hit a variable height Layout - figure out the available\n            # space and reset everything to the new values.\n            if fill_layout is None:\n                break\n            else:\n                fill_height = max(1, start_y + height - y)\n\n        # Remember the resulting height of the underlying Layouts.\n        self._max_height = y\n\n        # Reset text\n        while self._focus < len(self._layouts):\n            try:\n                self._layouts[self._focus].focus(force_first=True)\n                break\n            except IndexError:\n                self._focus += 1\n        self._clear()", "response": "Fix the layouts and calculate the locations of all the widgets."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nclearing the current canvas.", "response": "def _clear(self):\n        \"\"\"\n        Clear the current canvas.\n        \"\"\"\n        # It's orders of magnitude faster to reset with a print like this\n        # instead of recreating the screen buffers.\n        (colour, attr, bg) = self.palette[\"background\"]\n        self._canvas.clear_buffer(colour, attr, bg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_theme(self, theme):\n        if theme in THEMES:\n            self.palette = THEMES[theme]\n            if self._scroll_bar:\n                # TODO: fix protected access.\n                self._scroll_bar._palette = self.palette", "response": "Pick a palette from the list of supported THEMES."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef focussed_widget(self):\n        # If the frame has no focus, it can't have a focussed widget.\n        if not self._has_focus:\n            return None\n\n        try:\n            layout = self._layouts[self._focus]\n            return layout._columns[layout._live_col][layout._live_widget]\n        except IndexError:\n            # If the current indexing is invalid it's because no widget is selected.\n            return None", "response": "Returns the widget that currently has the focus within this Frame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef frame_update_count(self):\n        result = 1000000\n        for layout in self._layouts:\n            if layout.frame_update_count > 0:\n                result = min(result, layout.frame_update_count)\n        for effect in self._effects:\n            if effect.frame_update_count > 0:\n                result = min(result, effect.frame_update_count)\n        return result", "response": "Returns the number of frames before this Effect should be updated."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef find_widget(self, name):\n        result = None\n        for layout in self._layouts:\n            result = layout.find_widget(name)\n            if result:\n                break\n        return result", "response": "Look for a widget with a specified name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef clone(self, _, scene):\n        # Assume that the application creates a new set of Frames and so we need to match up the\n        # data from the old object to the new (using the name).\n        if self._name is not None:\n            for effect in scene.effects:\n                if isinstance(effect, Frame):\n                    if effect._name == self._name:\n                        effect.data = self.data\n                        for layout in self._layouts:\n                            layout.update_widgets(new_frame=effect)", "response": "Create a clone of this Frame into a new Screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the current values in all the widgets to the persistent data storage.", "response": "def save(self, validate=False):\n        \"\"\"\n        Save the current values in all the widgets back to the persistent data storage.\n\n        :param validate: Whether to validate the data before saving.\n\n        Calling this while setting the `data` field (e.g. in a widget callback) will have no\n        effect.\n\n        When validating data, it can throw an Exception for any\n        \"\"\"\n        # Don't allow this function to be called if we are already updating the\n        # data for the form.\n        if self._in_call:\n            return\n\n        # We're clear - pass on to all layouts/widgets.\n        invalid = []\n        for layout in self._layouts:\n            try:\n                layout.save(validate=validate)\n            except InvalidFields as exc:\n                invalid.extend(exc.fields)\n\n        # Check for any bad data and raise exception if needed.\n        if len(invalid) > 0:\n            raise InvalidFields(invalid)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nswitching focus to the specified widget.", "response": "def switch_focus(self, layout, column, widget):\n        \"\"\"\n        Switch focus to the specified widget.\n\n        :param layout: The layout that owns the widget.\n        :param column: The column the widget is in.\n        :param widget: The index of the widget to take the focus.\n        \"\"\"\n        # Find the layout to own the focus.\n        for i, l in enumerate(self._layouts):\n            if l is layout:\n                break\n        else:\n            # No matching layout - give up now\n            return\n\n        self._layouts[self._focus].blur()\n        self._focus = i\n        self._layouts[self._focus].focus(force_column=column,\n                                         force_widget=widget)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef move_to(self, x, y, h):\n        if self._has_border:\n            start_x = 1\n            width = self.canvas.width - 2\n            start_y = self.canvas.start_line + 1\n            height = self.canvas.height - 2\n        else:\n            start_x = 0\n            width = self.canvas.width\n            start_y = self.canvas.start_line\n            height = self.canvas.height\n\n        if ((x >= start_x) and (x < start_x + width) and\n                (y >= start_y) and (y + h < start_y + height)):\n            # Already OK - quit now.\n            return\n\n        if y < start_y:\n            self.canvas.scroll_to(y - 1 if self._has_border else y)\n        else:\n            line = y + h - self.canvas.height + (1 if self._has_border else 0)\n            self.canvas.scroll_to(max(0, line))", "response": "Moves the specified location visible."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the number of frames before this Layout should be updated.", "response": "def frame_update_count(self):\n        \"\"\"\n        The number of frames before this Layout should be updated.\n        \"\"\"\n        result = 1000000\n        for column in self._columns:\n            for widget in column:\n                if widget.frame_update_count > 0:\n                    result = min(result, widget.frame_update_count)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef register_frame(self, frame):\n        self._frame = frame\n        for column in self._columns:\n            for widget in column:\n                widget.register_frame(self._frame)", "response": "Register the Frame that owns this Widget."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nadd a widget to this Layout.", "response": "def add_widget(self, widget, column=0):\n        \"\"\"\n        Add a widget to this Layout.\n\n        If you are adding this Widget to the Layout dynamically after starting to play the Scene,\n        don't forget to ensure that the value is explicitly set before the next update.\n\n        :param widget: The widget to be added.\n        :param column: The column within the widget for this widget.  Defaults to zero.\n        \"\"\"\n        # Make sure that the Layout is fully initialised before we try to add any widgets.\n        if self._frame is None:\n            raise RuntimeError(\"You must add the Layout to the Frame before you can add a Widget.\")\n\n        # Now process the widget.\n        self._columns[column].append(widget)\n        widget.register_frame(self._frame)\n\n        if widget.name in self._frame.data:\n            widget.value = self._frame.data[widget.name]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef focus(self, force_first=False, force_last=False, force_column=None,\n              force_widget=None):\n        \"\"\"\n        Call this to give this Layout the input focus.\n\n        :param force_first: Optional parameter to force focus to first widget.\n        :param force_last: Optional parameter to force focus to last widget.\n        :param force_column: Optional parameter to mandate the new column index.\n        :param force_widget: Optional parameter to mandate the new widget index.\n\n        The force_column and force_widget parameters must both be set together or they will\n        otherwise be ignored.\n\n        :raises IndexError: if a force option specifies a bad column or widget, or if the whole\n            Layout is readonly.\n        \"\"\"\n        self._has_focus = True\n        if force_widget is not None and force_column is not None:\n            self._live_col = force_column\n            self._live_widget = force_widget\n        elif force_first:\n            self._live_col = 0\n            self._live_widget = -1\n            self._find_next_widget(1)\n        elif force_last:\n            self._live_col = len(self._columns) - 1\n            self._live_widget = len(self._columns[self._live_col])\n            self._find_next_widget(-1)\n        self._columns[self._live_col][self._live_widget].focus()", "response": "Call this to give this Layout the input focus."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nblur the input focus from this Layout.", "response": "def blur(self):\n        \"\"\"\n        Call this to take the input focus from this Layout.\n        \"\"\"\n        self._has_focus = False\n        try:\n            self._columns[self._live_col][self._live_widget].blur()\n        except IndexError:\n            # don't worry if there are no active widgets in the Layout\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfixes the location and size of all the Widgets in this Layout.", "response": "def fix(self, start_x, start_y, max_width, max_height):\n        \"\"\"\n        Fix the location and size of all the Widgets in this Layout.\n\n        :param start_x: The start column for the Layout.\n        :param start_y: The start line for the Layout.\n        :param max_width: Max width to allow this layout.\n        :param max_height: Max height to allow this layout.\n        :returns: The next line to be used for any further Layouts.\n        \"\"\"\n        x = start_x\n        width = max_width\n        y = w = 0\n        max_y = start_y\n        string_len = wcswidth if self._frame.canvas.unicode_aware else len\n        dimensions = []\n        for i, column in enumerate(self._columns):\n            # For each column determine if we need a tab offset for labels.\n            # Only allow labels to take up 1/3 of the column.\n            if len(column) > 0:\n                offset = max([0 if c.label is None else string_len(c.label) + 1 for c in column])\n            else:\n                offset = 0\n            offset = int(min(offset,\n                         width * self._column_sizes[i] // 3))\n\n            # Start tracking new column\n            dimensions.append(_DotDict())\n            dimensions[i].parameters = []\n            dimensions[i].offset = offset\n\n            # Do first pass to figure out the gaps for widgets that want to fill remaining space.\n            fill_layout = None\n            fill_column = None\n            y = start_y\n            w = int(width * self._column_sizes[i])\n            for widget in column:\n                h = widget.required_height(offset, w)\n                if h == Widget.FILL_FRAME:\n                    if fill_layout is None and fill_column is None:\n                        dimensions[i].parameters.append([widget, x, w, h])\n                        fill_layout = widget\n                    else:\n                        # Two filling widgets in one column - this is a bug.\n                        raise Highlander(\"Too many Widgets filling Layout\")\n                elif h == Widget.FILL_COLUMN:\n                    if fill_layout is None and fill_column is None:\n                        dimensions[i].parameters.append([widget, x, w, h])\n                        fill_column = widget\n                    else:\n                        # Two filling widgets in one column - this is a bug.\n                        raise Highlander(\"Too many Widgets filling Layout\")\n                else:\n                    dimensions[i].parameters.append([widget, x, w, h])\n                    y += h\n\n            # Note space used by this column.\n            dimensions[i].height = y\n\n            # Update tracking variables fpr the next column.\n            max_y = max(max_y, y)\n            x += w\n\n        # Finally check whether the Layout is allowed to expand.\n        if self.fill_frame:\n            max_y = max(max_y, start_y + max_height)\n\n        # Now apply calculated sizes, updating any widgets that need to fill space.\n        for column in dimensions:\n            y = start_y\n            for widget, x, w, h in column.parameters:\n                if h == Widget.FILL_FRAME:\n                    h = max(1, start_y + max_height - column.height)\n                elif h == Widget.FILL_COLUMN:\n                    h = max_y - column.height\n                widget.set_layout(x, y, column.offset, w, h)\n                y += h\n\n        return max_y"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfind the next widget in the list.", "response": "def _find_next_widget(self, direction, stay_in_col=False, start_at=None,\n                          wrap=False):\n        \"\"\"\n        Find the next widget to get the focus, stopping at the start/end of the list if hit.\n\n        :param direction: The direction to move through the widgets.\n        :param stay_in_col: Whether to limit search to current column.\n        :param start_at: Optional starting point in current column.\n        :param wrap: Whether to wrap around columns when at the end.\n        \"\"\"\n        current_widget = self._live_widget\n        current_col = self._live_col\n        if start_at is not None:\n            self._live_widget = start_at\n        still_looking = True\n        while still_looking:\n            while 0 <= self._live_col < len(self._columns):\n                self._live_widget += direction\n                while 0 <= self._live_widget < len(\n                        self._columns[self._live_col]):\n                    widget = self._columns[self._live_col][self._live_widget]\n                    if widget.is_tab_stop and not widget.disabled:\n                        return\n                    self._live_widget += direction\n                if stay_in_col:\n                    # Don't move to another column - just stay where we are.\n                    self._live_widget = current_widget\n                    break\n                else:\n                    self._live_col += direction\n                    self._live_widget = -1 if direction > 0 else \\\n                        len(self._columns[self._live_col])\n                    if self._live_col == current_col:\n                        # We've wrapped all the way back to the same column -\n                        # give up now and stay where we were.\n                        self._live_widget = current_widget\n                        return\n\n            # If we got here we hit the end of the columns - only keep on\n            # looking if we're allowed to wrap.\n            still_looking = wrap\n            if still_looking:\n                if self._live_col < 0:\n                    self._live_col = len(self._columns) - 1\n                else:\n                    self._live_col = 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_event(self, event, hover_focus):\n        # Check whether this Layout is read-only - i.e. has no active focus.\n        if self._live_col < 0 or self._live_widget < 0:\n            # Might just be that we've unset the focus - so check we can't find a focus.\n            self._find_next_widget(1)\n            if self._live_col < 0 or self._live_widget < 0:\n                return event\n\n        # Give the active widget the first refusal for this event.\n        event = self._columns[\n            self._live_col][self._live_widget].process_event(event)\n\n        # Check for any movement keys if the widget refused them.\n        if event is not None:\n            if isinstance(event, KeyboardEvent):\n                if event.key_code == Screen.KEY_TAB:\n                    # Move on to next widget, unless it is the last in the\n                    # Layout.\n                    self._columns[self._live_col][self._live_widget].blur()\n                    self._find_next_widget(1)\n                    if self._live_col >= len(self._columns):\n                        self._live_col = 0\n                        self._live_widget = -1\n                        self._find_next_widget(1)\n                        return event\n\n                    # If we got here, we still should have the focus.\n                    self._columns[self._live_col][self._live_widget].focus()\n                    event = None\n                elif event.key_code == Screen.KEY_BACK_TAB:\n                    # Move on to previous widget, unless it is the first in the\n                    # Layout.\n                    self._columns[self._live_col][self._live_widget].blur()\n                    self._find_next_widget(-1)\n                    if self._live_col < 0:\n                        self._live_col = len(self._columns) - 1\n                        self._live_widget = len(self._columns[self._live_col])\n                        self._find_next_widget(-1)\n                        return event\n\n                    # If we got here, we still should have the focus.\n                    self._columns[self._live_col][self._live_widget].focus()\n                    event = None\n                elif event.key_code == Screen.KEY_DOWN:\n                    # Move on to next widget in this column\n                    wid = self._live_widget\n                    self._columns[self._live_col][self._live_widget].blur()\n                    self._find_next_widget(1, stay_in_col=True)\n                    self._columns[self._live_col][self._live_widget].focus()\n                    # Don't swallow the event if it had no effect.\n                    event = event if wid == self._live_widget else None\n                elif event.key_code == Screen.KEY_UP:\n                    # Move on to previous widget, unless it is the first in the\n                    # Layout.\n                    wid = self._live_widget\n                    self._columns[self._live_col][self._live_widget].blur()\n                    self._find_next_widget(-1, stay_in_col=True)\n                    self._columns[self._live_col][self._live_widget].focus()\n                    # Don't swallow the event if it had no effect.\n                    event = event if wid == self._live_widget else None\n                elif event.key_code == Screen.KEY_LEFT:\n                    # Move on to last widget in the previous column\n                    self._columns[self._live_col][self._live_widget].blur()\n                    self._find_next_widget(-1, start_at=0, wrap=True)\n                    self._columns[self._live_col][self._live_widget].focus()\n                    event = None\n                elif event.key_code == Screen.KEY_RIGHT:\n                    # Move on to first widget in the next column.\n                    self._columns[self._live_col][self._live_widget].blur()\n                    self._find_next_widget(\n                        1,\n                        start_at=len(self._columns[self._live_col]),\n                        wrap=True)\n                    self._columns[self._live_col][self._live_widget].focus()\n                    event = None\n            elif isinstance(event, MouseEvent):\n                logger.debug(\"Check layout: %d, %d\", event.x, event.y)\n                if ((hover_focus and event.buttons >= 0) or\n                        event.buttons > 0):\n                    # Mouse click - look to move focus.\n                    for i, column in enumerate(self._columns):\n                        for j, widget in enumerate(column):\n                            if widget.is_mouse_over(event):\n                                self._frame.switch_focus(self, i, j)\n                                widget.process_event(event)\n                                return None\n        return event", "response": "Process any input event and return the effect."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update(self, frame_no):\n        for column in self._columns:\n            for widget in column:\n                widget.update(frame_no)", "response": "Redraw the widgets inside this Layout."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save(self, validate):\n        invalid = []\n        for column in self._columns:\n            for widget in column:\n                if widget.is_valid or not validate:\n                    if widget.name is not None:\n                        # This relies on the fact that we are passed the actual\n                        # dict and so can edit it directly.  In this case, that\n                        # is all we want - no need to update the widgets.\n                        self._frame._data[widget.name] = widget.value\n                else:\n                    invalid.append(widget.name)\n        if len(invalid) > 0:\n            raise InvalidFields(invalid)", "response": "Save the current values in all the widgets in the current column to the persistent data storage."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsearching for a widget with a specified name.", "response": "def find_widget(self, name):\n        \"\"\"\n        Look for a widget with a specified name.\n\n        :param name: The name to search for.\n\n        :returns: The widget that matches or None if one couldn't be found.\n        \"\"\"\n        result = None\n        for column in self._columns:\n            for widget in column:\n                if widget.name is not None and name == widget.name:\n                    result = widget\n                    break\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the values for any Widgets in this Layout based on the current Frame data store.", "response": "def update_widgets(self, new_frame=None):\n        \"\"\"\n        Reset the values for any Widgets in this Layout based on the current Frame data store.\n\n        :param new_frame: optional old Frame - used when cloning scenes.\n        \"\"\"\n        for column in self._columns:\n            for widget in column:\n                # First handle the normal case - pull the default data from the current frame.\n                if widget.name in self._frame.data:\n                    widget.value = self._frame.data[widget.name]\n                elif widget.is_tab_stop:\n                    # Make sure every active widget is properly initialised, by calling the setter.\n                    # This will fix up any dodgy NoneType values, but preserve any values overridden\n                    # by other code.\n                    widget.value = widget.value\n\n                # If an old frame was present, give the widget a chance to clone internal state\n                # from the previous view.  If there is no clone function, ignore the error.\n                if new_frame:\n                    try:\n                        widget.clone(new_frame.find_widget(widget.name))\n                    except AttributeError:\n                        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresets this Layout and the Widgets it contains.", "response": "def reset(self):\n        \"\"\"\n        Reset this Layout and the Widgets it contains.\n        \"\"\"\n        # Ensure that the widgets are using the right values.\n        self.update_widgets()\n\n        # Reset all the widgets.\n        for column in self._columns:\n            for widget in column:\n                widget.reset()\n                widget.blur()\n\n        # Find the focus for the first widget\n        self._live_widget = -1\n        self._find_next_widget(1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters the Frame that owns this Widget.", "response": "def register_frame(self, frame):\n        \"\"\"\n        Register the Frame that owns this Widget.\n\n        :param frame: The owning Frame.\n        \"\"\"\n        self._frame = frame\n        self.string_len = wcswidth if self._frame.canvas.unicode_aware else len"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set_layout(self, x, y, offset, w, h):\n        self._x = x\n        self._y = y\n        self._offset = offset\n        self._w = w\n        self._h = h", "response": "Set the size and position of the Widget."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the absolute location of this widget on the Screen taking into account the current state of the Frame that is displaying it and any label offsets of the Widget.", "response": "def get_location(self):\n        \"\"\"\n        Return the absolute location of this widget on the Screen, taking into account the\n        current state of the Frame that is displaying it and any label offsets of the Widget.\n\n        :returns: A tuple of the form (<X coordinate>, <Y coordinate>).\n        \"\"\"\n        origin = self._frame.canvas.origin\n        return (self._x + origin[0] + self._offset,\n                self._y + origin[1] - self._frame.canvas.start_line)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef focus(self):\n        self._has_focus = True\n        self._frame.move_to(self._x, self._y, self._h)\n        if self._on_focus is not None:\n            self._on_focus()", "response": "Move the current frame to the input focus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_mouse_over(self, event, include_label=True, width_modifier=0):\n        # Disabled widgets should not react to the mouse.\n        logger.debug(\"Widget: %s (%d, %d) (%d, %d)\", self, self._x, self._y, self._w, self._h)\n        if self._is_disabled:\n            return False\n\n        # Check for any overlap\n        if self._y <= event.y < self._y + self._h:\n            if ((include_label and self._x <= event.x < self._x + self._w - width_modifier) or\n                    (self._x + self._offset <= event.x < self._x + self._w - width_modifier)):\n                return True\n\n        return False", "response": "Check if the mouse event is over this widget."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndraws the label for this widget if needed.", "response": "def _draw_label(self):\n        \"\"\"\n        Draw the label for this widget if needed.\n        \"\"\"\n        if self._label is not None:\n            # Break the label up as required.\n            if self._display_label is None:\n                # noinspection PyTypeChecker\n                self._display_label = _split_text(\n                    self._label, self._offset, self._h, self._frame.canvas.unicode_aware)\n\n            # Draw the  display label.\n            (colour, attr, bg) = self._frame.palette[\"label\"]\n            for i, text in enumerate(self._display_label):\n                self._frame.canvas.paint(\n                    text, self._x, self._y + i, colour, attr, bg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _draw_cursor(self, char, frame_no, x, y):\n        (colour, attr, bg) = self._pick_colours(\"edit_text\")\n        if frame_no % 10 < 5 or self._frame.reduce_cpu:\n            attr |= Screen.A_REVERSE\n        self._frame.canvas.print_at(char, x, y, colour, attr, bg)", "response": "Draw a flashing cursor for this widget."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _pick_palette_key(self, palette_name, selected=False, allow_input_state=True):\n        key = palette_name\n        if self._custom_colour:\n            key = self._custom_colour\n        elif self.disabled:\n            key = \"disabled\"\n        elif not self._is_valid:\n            key = \"invalid\"\n        elif allow_input_state:\n            if self._has_focus:\n                key = \"focus_\" + palette_name\n            if selected:\n                key = \"selected_\" + key\n        return key", "response": "Pick the colour for a widget based on the current state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npicking the rendering colour for a widget based on the current state.", "response": "def _pick_colours(self, palette_name, selected=False):\n        \"\"\"\n        Pick the rendering colour for a widget based on the current state.\n\n        :param palette_name: The stem name for the widget - e.g. \"button\".\n        :param selected: Whether this item is selected or not.\n        :returns: A colour tuple (fg, attr, bg) to be used.\n        \"\"\"\n        return self._frame.palette[self._pick_palette_key(palette_name, selected)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _change_line(self, delta):\n        # Ensure new line is within limits\n        self._line = min(max(0, self._line + delta), len(self._value) - 1)\n\n        # Fix up column if the new line is shorter than before.\n        if self._column >= len(self._value[self._line]):\n            self._column = len(self._value[self._line])", "response": "Move the cursor up or down the specified number of lines."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _reflowed_text(self):\n        if self._reflowed_text_cache is None:\n            if self._line_wrap:\n                self._reflowed_text_cache = []\n                limit = self._w - self._offset\n                for i, line in enumerate(self._value):\n                    column = 0\n                    while self.string_len(line) >= limit:\n                        sub_string = _enforce_width(\n                            line, limit, self._frame.canvas.unicode_aware)\n                        self._reflowed_text_cache.append((sub_string, i, column))\n                        line = line[len(sub_string):]\n                        column += len(sub_string)\n                    self._reflowed_text_cache.append((line, i, column))\n            else:\n                self._reflowed_text_cache = [(x, i, 0) for i, x in enumerate(self._value)]\n\n        return self._reflowed_text_cache", "response": "Return the text as should be formatted on the screen."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_or_remove_scrollbar(self, width, height, dy):\n        if self._scroll_bar is None and len(self._options) > height:\n            self._scroll_bar = _ScrollBar(\n                self._frame.canvas, self._frame.palette, self._x + width - 1, self._y + dy,\n                height, self._get_pos, self._set_pos)\n        elif self._scroll_bar is not None and len(self._options) <= height:\n            self._scroll_bar = None", "response": "Add or remove a scrollbar from this listbox based on width and available options."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_pos(self):\n        if self._h >= len(self._options):\n            return 0\n        else:\n            return self._start_line / (len(self._options) - self._h)", "response": "Get current position for scroll bar."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _set_pos(self, pos):\n        if self._h < len(self._options):\n            pos *= len(self._options) - self._h\n            pos = int(round(max(0, pos), 0))\n            self._start_line = pos", "response": "Set current position for scroll bar."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npopulate the current multi - column list with the contents of the selected directory.", "response": "def _populate_list(self, value):\n        \"\"\"\n        Populate the current multi-column list with the contents of the selected directory.\n\n        :param value: The new value to use.\n        \"\"\"\n        # Nothing to do if the value is rubbish.\n        if value is None:\n            return\n\n        # Stop any recursion - no more returns from here to end of fn please!\n        if self._in_update:\n            return\n        self._in_update = True\n\n        # We need to update the tree view.\n        self._root = os.path.abspath(value if os.path.isdir(value) else os.path.dirname(value))\n\n        # The absolute expansion of \"/\" or \"\\\" is the root of the disk, so is a cross-platform\n        # way of spotting when to insert \"..\" or not.\n        tree_view = []\n        if len(self._root) > len(os.path.abspath(os.sep)):\n            tree_view.append(([\"|-+ ..\"], os.path.abspath(os.path.join(self._root, \"..\"))))\n\n        tree_dirs = []\n        tree_files = []\n        try:\n            files = os.listdir(self._root)\n        except OSError:\n            # Can fail on Windows due to access permissions\n            files = []\n        for my_file in files:\n            full_path = os.path.join(self._root, my_file)\n            try:\n                details = os.lstat(full_path)\n            except OSError:\n                # Can happen on Windows due to access permissions\n                details = namedtuple(\"stat_type\", \"st_size st_mtime\")\n                details.st_size = 0\n                details.st_mtime = 0\n            name = \"|-- {}\".format(my_file)\n            tree = tree_files\n            if os.path.isdir(full_path):\n                tree = tree_dirs\n                if os.path.islink(full_path):\n                    # Show links separately for directories\n                    real_path = os.path.realpath(full_path)\n                    name = \"|-+ {} -> {}\".format(my_file, real_path)\n                else:\n                    name = \"|-+ {}\".format(my_file)\n            elif self._file_filter and not self._file_filter.match(my_file):\n                # Skip files that don't match the filter (if present)\n                continue\n            elif os.path.islink(full_path):\n                # Check if link target exists and if it does, show statistics of the\n                # linked file, otherwise just display the link\n                try:\n                    real_path = os.path.realpath(full_path)\n                except OSError:\n                    # Can fail on Linux prof file system.\n                    real_path = None\n                if real_path and os.path.exists(real_path):\n                    details = os.stat(real_path)\n                    name = \"|-- {} -> {}\".format(my_file, real_path)\n                else:\n                    # Both broken directory and file links fall to this case.\n                    # Actually using the files will cause a FileNotFound exception\n                    name = \"|-- {} -> {}\".format(my_file, real_path)\n\n            # Normalize names for MacOS and then add to the list.\n            tree.append(([unicodedata.normalize(\"NFC\", name),\n                          readable_mem(details.st_size),\n                          readable_timestamp(details.st_mtime)], full_path))\n\n        tree_view.extend(sorted(tree_dirs))\n        tree_view.extend(sorted(tree_files))\n\n        self.options = tree_view\n        self._titles[0] = self._root\n\n        # We're out of the function - unset recursion flag.\n        self._in_update = False"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clone(self, screen, scene):\n        # Only clone the object if the function is safe to do so.\n        if self._on_close is None or isfunction(self._on_close):\n            scene.add_effect(PopUpDialog(screen, self._text, self._buttons, self._on_close))", "response": "Create a clone of this Dialog into a new Screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nclose this temporary pop - up.", "response": "def close(self, cancelled=False):\n        \"\"\"\n        Close this temporary pop-up.\n\n        :param cancelled: Whether the pop-up was cancelled (e.g. by pressing Esc).\n        \"\"\"\n        self._on_close(cancelled)\n        self._scene.remove_effect(self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self):\n        # Sort out chars\n        cursor = u\"\u2588\" if self._canvas.unicode_aware else \"O\"\n        back = u\"\u2591\" if self._canvas.unicode_aware else \"|\"\n\n        # Now draw...\n        try:\n            sb_pos = self._get_pos()\n            sb_pos = min(1, max(0, sb_pos))\n            sb_pos = max(int(self._height * sb_pos) - 1, 0)\n        except ZeroDivisionError:\n            sb_pos = 0\n        (colour, attr, bg) = self._palette[\"scroll\"]\n        y = self._canvas.start_line if self._absolute else 0\n        for dy in range(self._height):\n            self._canvas.print_at(cursor if dy == sb_pos else back,\n                                  self._x, y + self._y + dy,\n                                  colour, attr, bg)", "response": "Draw the scroll bar."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck whether a mouse event is over thus scroll bar.", "response": "def is_mouse_over(self, event):\n        \"\"\"\n        Check whether a MouseEvent is over thus scroll bar.\n\n        :param event: The MouseEvent to check.\n\n        :returns: True if the mouse event is over the scroll bar.\n        \"\"\"\n        return event.x == self._x and self._y <= event.y < self._y + self._height"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprocess the event and update the position if needed.", "response": "def process_event(self, event):\n        \"\"\"\n        Handle input on the scroll bar.\n\n        :param event: the event to be processed.\n\n        :returns: True if the scroll bar handled the event.\n        \"\"\"\n        # Convert into absolute coordinates if needed.\n        new_event = event\n        if self._absolute:\n            new_event.y -= self._canvas.start_line\n\n        # Process event if needed.\n        if self.is_mouse_over(new_event) and event.buttons != 0:\n            self._set_pos((new_event.y - self._y) / (self._height - 1))\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert memory to human - readable form.", "response": "def readable_mem(mem):\n    \"\"\"\n    :param mem: An integer number of bytes to convert to human-readable form.\n    :return: A human-readable string representation of the number.\n    \"\"\"\n    for suffix in [\"\", \"K\", \"M\", \"G\", \"T\"]:\n        if mem < 10000:\n            return \"{}{}\".format(int(mem), suffix)\n        mem /= 1024\n    return \"{}P\".format(int(mem))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a short - readable string representation of the timestamp.", "response": "def readable_timestamp(stamp):\n    \"\"\"\n    :param stamp: A floating point number representing the POSIX file timestamp.\n    :return: A short human-readable string representation of the timestamp.\n    \"\"\"\n    if date.fromtimestamp(stamp) == date.today():\n        return str(datetime.fromtimestamp(stamp).strftime(\"%I:%M:%S%p\"))\n    else:\n        return str(date.fromtimestamp(stamp))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _convert_images(self):\n        self._plain_images = []\n        self._colour_map = []\n        for image in self._images:\n            colour_map = []\n            new_image = []\n            for line in image.split(\"\\n\"):\n                new_line = \"\"\n                attributes = (None, None, None)\n                colours = []\n                while len(line) > 0:\n                    match = self._colour_sequence.match(line)\n                    if match is None:\n                        new_line += line[0]\n                        colours.append(attributes)\n                        line = line[1:]\n                    else:\n                        # The regexp either matches:\n                        # - 2,3,4 for ${c,a,b}\n                        # - 5,6 for ${c,a}\n                        # - 7 for ${c}.\n                        if match.group(2) is not None:\n                            attributes = (int(match.group(2)),\n                                          ATTRIBUTES[match.group(3)],\n                                          int(match.group(4)))\n                        elif match.group(5) is not None:\n                            attributes = (int(match.group(5)),\n                                          ATTRIBUTES[match.group(6)],\n                                          None)\n                        else:\n                            attributes = (int(match.group(7)), 0, None)\n                        line = match.group(8)\n                new_image.append(new_line)\n                colour_map.append(colours)\n            self._plain_images.append(new_image)\n            self._colour_map.append(colour_map)", "response": "Convert any images into a more Screen - friendly format."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef images(self):\n        if len(self._plain_images) <= 0:\n            self._convert_images()\n\n        return iter(self._plain_images)", "response": "An iterator of all the images in the Renderer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the next image and colour map in the sequence as a tuple.", "response": "def rendered_text(self):\n        \"\"\"\n        :return: The next image and colour map in the sequence as a tuple.\n        \"\"\"\n        if len(self._plain_images) <= 0:\n            self._convert_images()\n\n        if self._animation is None:\n            index = self._index\n            self._index += 1\n            if self._index >= len(self._plain_images):\n                self._index = 0\n        else:\n            index = self._animation()\n        return (self._plain_images[index],\n                self._colour_map[index])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the maximum height of the rendered text.", "response": "def max_height(self):\n        \"\"\"\n        :return: The max height of the rendered text (across all images if an\n            animated renderer).\n        \"\"\"\n        if len(self._plain_images) <= 0:\n            self._convert_images()\n\n        if self._max_height == 0:\n            for image in self._plain_images:\n                self._max_height = max(len(image), self._max_height)\n        return self._max_height"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the maximum width of the rendered text.", "response": "def max_width(self):\n        \"\"\"\n        :return: The max width of the rendered text (across all images if an\n            animated renderer).\n        \"\"\"\n        if len(self._plain_images) <= 0:\n            self._convert_images()\n\n        if self._max_width == 0:\n            for image in self._plain_images:\n                new_max = max([wcswidth(x) for x in image])\n                self._max_width = max(new_max, self._max_width)\n        return self._max_width"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclear the current image.", "response": "def _clear(self):\n        \"\"\"\n        Clear the current image.\n        \"\"\"\n        self._plain_image = [\" \" * self._width for _ in range(self._height)]\n        self._colour_map = [[(None, 0, 0) for _ in range(self._width)]\n                            for _ in range(self._height)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites some text to the specified location in the current image.", "response": "def _write(self, text, x, y, colour=Screen.COLOUR_WHITE,\n               attr=Screen.A_NORMAL, bg=Screen.COLOUR_BLACK):\n        \"\"\"\n        Write some text to the specified location in the current image.\n\n        :param text: The text to be added.\n        :param x: The X coordinate in the image.\n        :param y: The Y coordinate in the image.\n        :param colour: The colour of the text to add.\n        :param attr: The attribute of the image.\n        :param bg: The background colour of the text to add.\n        \"\"\"\n        # Limit checks to ensure that we don't try to draw off the end of the arrays\n        if y >= self._height or x >= self._width:\n            return\n\n        # Limit text to draw to visible line\n        if len(text) + x > self._width:\n            text = text[:self._width - x]\n\n        # Now draw it!\n        self._plain_image[y] = text.join(\n            [self._plain_image[y][:x], self._plain_image[y][x + len(text):]])\n        for i, _ in enumerate(text):\n            self._colour_map[y][x + i] = (colour, attr, bg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprocessing the animation effect for the specified frame number.", "response": "def update(self, frame_no):\n        \"\"\"\n        Process the animation effect for the specified frame number.\n\n        :param frame_no: The index of the frame being generated.\n        \"\"\"\n        if (frame_no >= self._start_frame and\n                (self._stop_frame == 0 or frame_no < self._stop_frame)):\n            self._update(frame_no)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npick a random location for the star making sure it does not overwrite an existing piece of text.", "response": "def _respawn(self):\n        \"\"\"\n        Pick a random location for the star making sure it does\n        not overwrite an existing piece of text.\n        \"\"\"\n        self._cycle = randint(0, len(self._star_chars))\n        (height, width) = self._screen.dimensions\n        while True:\n            self._x = randint(0, width - 1)\n            self._y = self._screen.start_line + randint(0, height - 1)\n            if self._screen.get_from(self._x, self._y)[0] == 32:\n                break\n        self._old_char = \" \""}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, reseed):\n        if self._clear:\n            for i in range(0, 3):\n                self._screen.print_at(\" \",\n                                      self._x,\n                                      self._screen.start_line + self._y + i)\n            self._maybe_reseed(reseed)\n        else:\n            for i in range(0, 3):\n                self._screen.print_at(chr(randint(32, 126)),\n                                      self._x,\n                                      self._screen.start_line + self._y + i,\n                                      Screen.COLOUR_GREEN)\n            for i in range(4, 6):\n                self._screen.print_at(chr(randint(32, 126)),\n                                      self._x,\n                                      self._screen.start_line + self._y + i,\n                                      Screen.COLOUR_GREEN,\n                                      Screen.A_BOLD)\n            self._maybe_reseed(reseed)", "response": "Update that trail!\n\n        :param reseed: Whether we are in the normal reseed cycle or not."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the last position of this Sprite in a tuple.", "response": "def last_position(self):\n        \"\"\"\n        Returns the last position of this Sprite as a tuple\n        (x, y, width, height).\n        \"\"\"\n        return self._old_x, self._old_y, self._old_width, self._old_height"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef overlaps(self, other, use_new_pos=False):\n        (x, y) = self._path.next_pos() if use_new_pos else (self._old_x,\n                                                            self._old_y)\n        w = self._old_width\n        h = self._old_height\n\n        x2, y2, w2, h2 = other.last_position()\n\n        if ((x > x2 + w2 - 1) or (x2 > x + w - 1) or\n                (y > y2 + h2 - 1) or (y2 > y + h - 1)):\n            return False\n        else:\n            return True", "response": "Check whether this Sprite overlaps another Sprite."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _reseed(self):\n        self._char = choice(self._snow_chars)\n        self._rate = randint(1, 3)\n        self._x = randint(0, self._screen.width - 1)\n        self._y = self._screen.start_line + randint(0, self._rate)", "response": "Randomly create a new snowflake."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate that snowflake! :param reseed: Whether we are in the normal reseed cycle or not.", "response": "def update(self, reseed):\n        \"\"\"\n        Update that snowflake!\n\n        :param reseed: Whether we are in the normal reseed cycle or not.\n        \"\"\"\n        self._screen.print_at(\" \", self._x, self._y)\n        cell = None\n        for _ in range(self._rate):\n            self._y += 1\n            cell = self._screen.get_from(self._x, self._y)\n            if cell is None or cell[0] != 32:\n                break\n\n        if ((cell is not None and cell[0] in [ord(x) for x in self._snow_chars + \" \"]) and\n                (self._y < self._screen.start_line + self._screen.height)):\n            self._screen.print_at(self._char,\n                                  self._x,\n                                  self._y)\n        else:\n            if self._y > self._screen.start_line + self._screen.height:\n                self._y = self._screen.start_line + self._screen.height\n\n            drift_index = -1\n            if cell:\n                drift_index = self._drift_chars.find(chr(cell[0]))\n            if 0 <= drift_index < len(self._drift_chars) - 1:\n                drift_char = self._drift_chars[drift_index + 1]\n                self._screen.print_at(drift_char, self._x, self._y)\n            else:\n                self._screen.print_at(\",\", self._x, self._y - 1)\n            if reseed:\n                self._reseed()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears the double - buffer.", "response": "def clear(self, fg, attr, bg):\n        \"\"\"\n        Clear the double-buffer.\n\n        This does not clear the screen buffer and so the next call to deltas will still show all changes.\n\n        :param fg: The foreground colour to use for the new buffer.\n        :param attr: The attribute value to use for the new buffer.\n        :param bg: The background colour to use for the new buffer.\n        \"\"\"\n        line = [(ord(u\" \"), fg, attr, bg, 1) for _ in range(self._width)]\n        self._double_buffer = [line[:] for _ in range(self._height)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set(self, x, y, value):\n        self._double_buffer[y][x] = value", "response": "Set the value of the cell at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deltas(self, start, height):\n        for y in range(start, min(start + height, self._height)):\n            for x in range(self._width):\n                old_cell = self._screen_buffer[y][x]\n                new_cell = self._double_buffer[y][x]\n                if old_cell != new_cell:\n                    yield y, x", "response": "Yields the tuples of the screen buffer and double buffer cells that differ."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scroll(self, lines):\n        line = [(ord(u\" \"), Screen.COLOUR_WHITE, 0, 0, 1) for _ in range(self._width)]\n        if lines > 0:\n            # Limit to buffer size - this will just invalidate all the data\n            lines = min(lines, self._height)\n            for y in range(0, self._height - lines):\n                self._double_buffer[y] = self._double_buffer[y + lines]\n                self._screen_buffer[y] = self._screen_buffer[y + lines]\n            for y in range(self._height - lines, self._height):\n                self._double_buffer[y] = line[:]\n                self._screen_buffer[y] = line[:]\n        else:\n            # Limit to buffer size - this will just invalidate all the data\n            lines = max(lines, -self._height)\n            for y in range(0, -lines):\n                self._double_buffer[self._height + lines + y] = line[:]\n                self._screen_buffer[y] = line[:]\n            for y in range(self._height - 1, -lines, -1):\n                self._double_buffer[y] = self._double_buffer[y + lines]\n                self._screen_buffer[y] = self._screen_buffer[y + lines]", "response": "Scroll the window up or down."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef block_transfer(self, buffer, x, y):\n        # Just copy the double-buffer cells - the real screen will sync on refresh.\n        block_min_x = max(0, x)\n        block_max_x = min(x + buffer.width, self._width)\n\n        # Check for trivial non-overlap\n        if block_min_x > block_max_x:\n            return\n\n        # Copy the available section\n        for by in range(0, self._height):\n            if y <= by < y + buffer.height:\n                self._double_buffer[by][block_min_x:block_max_x] = buffer.slice(\n                    block_min_x - x, by - y, block_max_x - block_min_x)", "response": "Copy a buffer entirely to this screen."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprovides a slice of data from the current double - buffer at the specified location", "response": "def slice(self, x, y, width):\n        \"\"\"\n        Provide a slice of data from the buffer at the specified location\n\n        :param x: The X origin\n        :param y: The Y origin\n        :param width: The width of slice required\n        :return: The slice of tuples from the current double-buffer\n        \"\"\"\n        return self._double_buffer[y][x:x + width]"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nclears the current double - buffer used by this object.", "response": "def clear_buffer(self, fg, attr, bg):\n        \"\"\"\n        Clear the current double-buffer used by this object.\n\n        :param fg: The foreground colour to use for the new buffer.\n        :param attr: The attribute value to use for the new buffer.\n        :param bg: The background colour to use for the new buffer.\n        \"\"\"\n        self._buffer.clear(fg, attr, bg)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresetting the internal buffers and screen buffers for the abstract canvas.", "response": "def reset(self):\n        \"\"\"\n        Reset the internal buffers for the abstract canvas.\n        \"\"\"\n        # Reset our screen buffer\n        self._start_line = 0\n        self._x = self._y = None\n        self._buffer = _DoubleBuffer(self._buffer_height, self.width)\n        self._reset()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef scroll_to(self, line):\n        self._buffer.scroll(line - self._start_line)\n        self._start_line = line", "response": "Scroll the abstract canvas to make a specific line."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_from(self, x, y):\n        # Convert to buffer coordinates\n        y -= self._start_line\n        if y < 0 or y >= self._buffer_height or x < 0 or x >= self.width:\n            return None\n        cell = self._buffer.get(x, y)\n        return cell[0], cell[1], cell[2], cell[3]", "response": "Get the character at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_at(self, text, x, y, colour=7, attr=0, bg=0, transparent=False):\n        # Convert to the logically visible window that our double-buffer provides\n        y -= self._start_line\n\n        # Trim text to the buffer vertically.  Don't trim horizontally as we don't know whether any\n        # of these characters are dual-width yet.  Handle it on the fly below...\n        if y < 0 or y >= self._buffer_height or x > self.width:\n            return\n\n        if len(text) > 0:\n            j = 0\n            for i, c in enumerate(text):\n                # Handle under-run and overrun of double-width glyphs now.\n                #\n                # Note that wcwidth uses significant resources, so only call when we have a\n                # unicode aware application.  The rest of the time assume ASCII.\n                width = wcwidth(c) if self._unicode_aware and ord(c) >= 256 else 1\n                if x + i + j < 0:\n                    x += (width - 1)\n                    continue\n                if x + i + j + width > self.width:\n                    return\n\n                # Now handle the update.\n                if c != \" \" or not transparent:\n                    # Fix up orphaned double-width glyphs that we've just bisected.\n                    if x + i + j - 1 >= 0 and self._buffer.get(x + i + j - 1, y)[4] == 2:\n                        self._buffer.set(x + i + j - 1, y,\n                                         (ord(\"x\"), 0, 0, 0, 1))\n\n                    self._buffer.set(\n                        x + i + j, y, (ord(c), colour, attr, bg, width))\n                    if width == 2:\n                        j += 1\n                        if x + i + j < self.width:\n                            self._buffer.set(\n                                x + i + j, y, (ord(c), colour, attr, bg, 0))\n\n                    # Now fix up any glyphs we may have bisected the other way.\n                    if x + i + j + 1 < self.width and self._buffer.get(x + i + j + 1, y)[4] == 0:\n                        self._buffer.set(x + i + j + 1, y,\n                                         (ord(\"x\"), 0, 0, 0, 1))", "response": "Print the text at the specified location using the specified colour and attributes."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncopying a buffer to the screen double buffer at a specified location.", "response": "def block_transfer(self, buffer, x, y):\n        \"\"\"\n        Copy a buffer to the screen double buffer at a specified location.\n\n        :param buffer: The double buffer to copy\n        :param x: The X origin for where to place it in the Screen\n        :param y: The Y origin for where to place it in the Screen\n        \"\"\"\n        self._buffer.block_transfer(buffer, x, y)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncentring the text on the specified line y using the optional colour and attributes.", "response": "def centre(self, text, y, colour=7, attr=0, colour_map=None):\n        \"\"\"\n        Centre the text on the specified line (y) using the optional colour and attributes.\n\n        :param text: The (single line) text to be printed.\n        :param y: The line (y coord) for the start of the text.\n        :param colour: The colour of the text to be displayed.\n        :param attr: The cell attribute of the text to be displayed.\n        :param colour_map: Colour/attribute list for multi-colour text.\n\n        The colours and attributes are the COLOUR_xxx and A_yyy constants\n        defined in the Screen class.\n        \"\"\"\n        if self._unicode_aware:\n            x = (self.width - wcswidth(text)) // 2\n        else:\n            x = (self.width - len(text)) // 2\n        self.paint(text, x, y, colour, attr, colour_map=colour_map)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npaints the text at the defined location.", "response": "def paint(self, text, x, y, colour=7, attr=0, bg=0, transparent=False,\n              colour_map=None):\n        \"\"\"\n        Paint multi-colour text at the defined location.\n\n        :param text: The (single line) text to be printed.\n        :param x: The column (x coord) for the start of the text.\n        :param y: The line (y coord) for the start of the text.\n        :param colour: The default colour of the text to be displayed.\n        :param attr: The default cell attribute of the text to be displayed.\n        :param bg: The default background colour of the text to be displayed.\n        :param transparent: Whether to print spaces or not, thus giving a\n            transparent effect.\n        :param colour_map: Colour/attribute list for multi-colour text.\n\n        The colours and attributes are the COLOUR_xxx and A_yyy constants\n        defined in the Screen class.\n        colour_map is a list of tuples (foreground, attribute, background) that\n        must be the same length as the passed in text (or None if no mapping is\n        required).\n        \"\"\"\n        if colour_map is None:\n            self.print_at(text, x, y, colour, attr, bg, transparent)\n        else:\n            offset = 0\n            for c, m in zip_longest(text, colour_map):\n                if m:\n                    if len(m) > 0 and m[0] is not None:\n                        colour = m[0]\n                    if len(m) > 1 and m[1] is not None:\n                        attr = m[1]\n                    if len(m) > 2 and m[2] is not None:\n                        bg = m[2]\n                if c:\n                    self.print_at(c, x + offset, y, colour, attr, bg, transparent)\n                    offset += wcwidth(c) if ord(c) >= 256 else 1"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nblend the new colour with the old according to the ratio.", "response": "def _blend(self, new, old, ratio):\n        \"\"\"\n        Blend the new colour with the old according to the ratio.\n\n        :param new: The new colour (or None if not required).\n        :param old: The old colour.\n        :param ratio: The ratio to blend new and old\n        :returns: the new colour index to use for the required blend.\n        \"\"\"\n        # Don't bother blending if none is required.\n        if new is None:\n            return old\n\n        # Check colour blend cache for a quick answer.\n        key = (min(new, old), max(new, old))\n        if key in self._blends:\n            return self._blends[key]\n\n        # No quick answer - do it the long way...  First lookup the RGB values\n        # for both colours and blend.\n        (r1, g1, b1) = self.palette[new * 3:new * 3 + 3]\n        (r2, g2, b2) = self.palette[old * 3:old * 3 + 3]\n\n        # Helper function to blend RGB values.\n        def f(c1, c2):\n            return ((c1 * ratio) + (c2 * (100 - ratio))) // 100\n\n        r = f(r1, r2)\n        g = f(g1, g2)\n        b = f(b1, b2)\n\n        # Now do the reverse lookup...\n        nearest = (256 ** 2) * 3\n        match = 0\n        for c in range(self.colours):\n            (rc, gc, bc) = self.palette[c * 3:c * 3 + 3]\n            diff = sqrt(((rc - r) * 0.3) ** 2 + ((gc - g) * 0.59) ** 2 +\n                        ((bc - b) * 0.11) ** 2)\n            if diff < nearest:\n                nearest = diff\n                match = c\n\n        # Save off the answer and return it\n        self._blends[key] = match\n        return match"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef highlight(self, x, y, w, h, fg=None, bg=None, blend=100):\n        # Convert to buffer coordinates\n        y -= self._start_line\n        for i in range(w):\n            if x + i >= self.width or x + i < 0:\n                continue\n\n            for j in range(h):\n                if y + j >= self._buffer_height or y + j < 0:\n                    continue\n\n                old = self._buffer.get(x + i, y + j)\n                new_bg = self._blend(bg, old[3], blend)\n                new_fg = self._blend(fg, old[1], blend)\n                self._buffer.set(x + i, y + j, (old[0], new_fg, old[2], new_bg, old[4]))", "response": "Highlights a specified section of the screen."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_visible(self, x, y):\n        return ((x >= 0) and\n                (x <= self.width) and\n                (y >= self._start_line) and\n                (y < self._start_line + self.height))", "response": "Return whether the specified location is on the visible screen."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef move(self, x, y):\n        self._x = int(round(x * 2, 0))\n        self._y = int(round(y * 2, 0))", "response": "Move the drawing cursor to the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef draw(self, x, y, char=None, colour=7, bg=0, thin=False):\n        # Decide what type of line drawing to use.\n        line_chars = (self._uni_line_chars if self._unicode_aware else\n                      self._line_chars)\n\n        # Define line end points.\n        x0 = self._x\n        y0 = self._y\n        x1 = int(round(x * 2, 0))\n        y1 = int(round(y * 2, 0))\n\n        # Remember last point for next line.\n        self._x = x1\n        self._y = y1\n\n        # Don't bother drawing anything if we're guaranteed to be off-screen\n        if ((x0 < 0 and x1 < 0) or (x0 >= self.width * 2 and x1 >= self.width * 2) or\n                (y0 < 0 and y1 < 0) or (y0 >= self.height * 2 and y1 >= self.height * 2)):\n            return\n\n        dx = abs(x1 - x0)\n        dy = abs(y1 - y0)\n\n        sx = -1 if x0 > x1 else 1\n        sy = -1 if y0 > y1 else 1\n\n        def _get_start_char(cx, cy):\n            needle = self.get_from(cx, cy)\n            if needle is not None:\n                letter, cfg, _, cbg = needle\n                if colour == cfg and bg == cbg and chr(letter) in line_chars:\n                    return line_chars.find(chr(letter))\n            return 0\n\n        def _fast_fill(start_x, end_x, iy):\n            next_char = -1\n            for ix in range(start_x, end_x):\n                if ix % 2 == 0 or next_char == -1:\n                    next_char = _get_start_char(ix // 2, iy // 2)\n                next_char |= 2 ** abs(ix % 2) * 4 ** (iy % 2)\n                if ix % 2 == 1:\n                    self.print_at(line_chars[next_char], ix // 2, iy // 2, colour, bg=bg)\n            if end_x % 2 == 1:\n                self.print_at(line_chars[next_char], end_x // 2, iy // 2, colour, bg=bg)\n\n        def _draw_on_x(ix, iy):\n            err = dx\n            px = ix - 2\n            py = iy - 2\n            next_char = 0\n            while ix != x1:\n                if ix < px or ix - px >= 2 or iy < py or iy - py >= 2:\n                    px = ix & ~1\n                    py = iy & ~1\n                    next_char = _get_start_char(px // 2, py // 2)\n                next_char |= 2 ** abs(ix % 2) * 4 ** (iy % 2)\n                err -= 2 * dy\n                if err < 0:\n                    iy += sy\n                    err += 2 * dx\n                ix += sx\n\n                if char is None:\n                    self.print_at(line_chars[next_char],\n                                  px // 2, py // 2, colour, bg=bg)\n                else:\n                    self.print_at(char, px // 2, py // 2, colour, bg=bg)\n\n        def _draw_on_y(ix, iy):\n            err = dy\n            px = ix - 2\n            py = iy - 2\n            next_char = 0\n            while iy != y1:\n                if ix < px or ix - px >= 2 or iy < py or iy - py >= 2:\n                    px = ix & ~1\n                    py = iy & ~1\n                    next_char = _get_start_char(px // 2, py // 2)\n                next_char |= 2 ** abs(ix % 2) * 4 ** (iy % 2)\n                err -= 2 * dx\n                if err < 0:\n                    ix += sx\n                    err += 2 * dy\n                iy += sy\n\n                if char is None:\n                    self.print_at(line_chars[next_char],\n                                  px // 2, py // 2, colour, bg=bg)\n                else:\n                    self.print_at(char, px // 2, py // 2, colour, bg=bg)\n\n        if dy == 0 and thin and char is None:\n            # Fast-path for polygon filling\n            _fast_fill(min(x0, x1), max(x0, x1), y0)\n        elif dx > dy:\n            _draw_on_x(x0, y0)\n            if not thin:\n                _draw_on_x(x0, y0 + 1)\n        else:\n            _draw_on_y(x0, y0)\n            if not thin:\n                _draw_on_y(x0 + 1, y0)", "response": "Draw a line from drawing cursor to the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fill_polygon(self, polygons, colour=7, bg=0):\n        def _add_edge(a, b):\n            # Ignore horizontal lines - they are redundant\n            if a[1] == b[1]:\n                return\n\n            # Ignore any edges that do not intersect the visible raster lines at all.\n            if (a[1] < 0 and b[1] < 0) or (a[1] >= self.height and b[1] >= self.height):\n                return\n\n            # Save off the edge, always starting at the lowest value of y.\n            new_edge = _DotDict()\n            if a[1] < b[1]:\n                new_edge.min_y = a[1]\n                new_edge.max_y = b[1]\n                new_edge.x = a[0]\n                new_edge.dx = (b[0] - a[0]) / (b[1] - a[1]) / 2\n            else:\n                new_edge.min_y = b[1]\n                new_edge.max_y = a[1]\n                new_edge.x = b[0]\n                new_edge.dx = (a[0] - b[0]) / (a[1] - b[1]) / 2\n            edges.append(new_edge)\n\n        # Create a table of all the edges in the polygon, sorted on smallest x.\n        logger.debug(\"Processing polygon: %s\", polygons)\n        min_y = self.height\n        max_y = -1\n        edges = []\n        last = None\n        for polygon in polygons:\n            # Ignore lines and polygons.\n            if len(polygon) <= 2:\n                continue\n\n            # Ignore any polygons completely off the screen\n            x, y = zip(*polygon)\n            p_min_x = min(x)\n            p_max_x = max(x)\n            p_min_y = min(y)\n            p_max_y = max(y)\n            if p_max_x < 0 or p_min_x >= self.width or p_max_y < 0 or p_min_y > self.height:\n                continue\n\n            # Build up the edge list, maintaining bounding coordinates on the Y axis.\n            min_y = min(p_min_y, min_y)\n            max_y = max(p_max_y, max_y)\n            for i, point in enumerate(polygon):\n                if i != 0:\n                    _add_edge(last, point)\n                last = point\n            _add_edge(polygon[0], polygon[-1])\n            edges = sorted(edges, key=lambda e: e.x)\n\n        # Check we still have something to do:\n        if len(edges) == 0:\n            return\n\n        # Re-base all edges to visible Y coordinates of the screen.\n        for edge in edges:\n            if edge.min_y < 0:\n                edge.x -= int(edge.min_y * 2) * edge.dx\n                edge.min_y = 0\n        min_y = max(0, min_y)\n        max_y = min(max_y - min_y, self.height)\n\n        logger.debug(\"Resulting edges: %s\", edges)\n\n        # Render each line in the bounding rectangle.\n        for y in [min_y + (i / 2) for i in range(0, int(max_y) * 2)]:\n            # Create a list of live edges (for drawing this raster line) and edges for next\n            # iteration of the raster.\n            live_edges = []\n            new_edges = []\n            for edge in edges:\n                if edge.min_y <= y <= edge.max_y:\n                    live_edges.append(edge)\n                if y < edge.max_y:\n                    new_edges.append(edge)\n\n            # Draw the portions of the line that are inside the polygon.\n            count = 0\n            last_x = 0\n            for edge in live_edges:\n                # Draw the next segment\n                if 0 <= y < self.height:\n                    if edge.max_y != y:\n                        count += 1\n                        if count % 2 == 1:\n                            last_x = edge.x\n                        else:\n                            # Don't bother drawing lines entirely off the screen.\n                            if not ((last_x < 0 and edge.x < 0) or\n                                    (last_x >= self.width and edge.x >= self.width)):\n                                # Clip raster to screen width.\n                                self.move(max(0, last_x), y)\n                                self.draw(\n                                    min(edge.x, self.width), y, colour=colour, bg=bg, thin=True)\n\n                # Update the x location for this active edge.\n                edge.x += edge.dx\n\n            # Rely on the fact that we have the same dicts in both live_edges and new_edges, so\n            # we just need to resort new_edges for the next iteration.\n            edges = sorted(new_edges, key=lambda e: e.x)", "response": "Draw a filled polygon."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef refresh(self):\n        self._screen.block_transfer(self._buffer, self._dx, self._dy)", "response": "Flush the canvas content to the underlying screen."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new Screen object for any platform.", "response": "def open(cls, height=None, catch_interrupt=False, unicode_aware=None):\n        \"\"\"\n        Construct a new Screen for any platform.  This will just create the\n        correct Screen object for your environment.  See :py:meth:`.wrapper` for\n        a function to create and tidy up once you've finished with the Screen.\n\n        :param height: The buffer height for this window (for testing only).\n        :param catch_interrupt: Whether to catch and prevent keyboard\n            interrupts.  Defaults to False to maintain backwards compatibility.\n        :param unicode_aware: Whether the application can use unicode or not.\n            If None, try to detect from the environment if UTF-8 is enabled.\n        \"\"\"\n        if sys.platform == \"win32\":\n            # Clone the standard output buffer so that we can do whatever we\n            # need for the application, but restore the buffer at the end.\n            # Note that we need to resize the clone to ensure that it is the\n            # same size as the original in some versions of Windows.\n            old_out = win32console.PyConsoleScreenBufferType(\n                win32file.CreateFile(\"CONOUT$\",\n                                     win32file.GENERIC_READ | win32file.GENERIC_WRITE,\n                                     win32file.FILE_SHARE_WRITE,\n                                     None,\n                                     win32file.OPEN_ALWAYS,\n                                     0,\n                                     None))\n            try:\n                info = old_out.GetConsoleScreenBufferInfo()\n            except pywintypes.error:\n                info = None\n            win_out = win32console.CreateConsoleScreenBuffer()\n            if info:\n                win_out.SetConsoleScreenBufferSize(info['Size'])\n            else:\n                win_out.SetStdHandle(win32console.STD_OUTPUT_HANDLE)\n            win_out.SetConsoleActiveScreenBuffer()\n\n            # Get the standard input buffer.\n            win_in = win32console.PyConsoleScreenBufferType(\n                win32file.CreateFile(\"CONIN$\",\n                                     win32file.GENERIC_READ | win32file.GENERIC_WRITE,\n                                     win32file.FILE_SHARE_READ,\n                                     None,\n                                     win32file.OPEN_ALWAYS,\n                                     0,\n                                     None))\n            win_in.SetStdHandle(win32console.STD_INPUT_HANDLE)\n\n            # Hide the cursor.\n            win_out.SetConsoleCursorInfo(1, 0)\n\n            # Disable scrolling\n            out_mode = win_out.GetConsoleMode()\n            win_out.SetConsoleMode(\n                out_mode & ~ win32console.ENABLE_WRAP_AT_EOL_OUTPUT)\n\n            # Enable mouse input, disable quick-edit mode and disable ctrl-c\n            # if needed.\n            in_mode = win_in.GetConsoleMode()\n            new_mode = (in_mode | win32console.ENABLE_MOUSE_INPUT |\n                        ENABLE_EXTENDED_FLAGS)\n            new_mode &= ~ENABLE_QUICK_EDIT_MODE\n            if catch_interrupt:\n                # Ignore ctrl-c handlers if specified.\n                new_mode &= ~win32console.ENABLE_PROCESSED_INPUT\n            win_in.SetConsoleMode(new_mode)\n\n            screen = _WindowsScreen(win_out, win_in, height, old_out, in_mode,\n                                    unicode_aware=unicode_aware)\n        else:\n            # Reproduce curses.wrapper()\n            stdscr = curses.initscr()\n            curses.noecho()\n            curses.cbreak()\n            stdscr.keypad(1)\n\n            # Fed up with linters complaining about original curses code - trying to be a bit better...\n            # noinspection PyBroadException\n            # pylint: disable=broad-except\n            try:\n                curses.start_color()\n            except Exception as e:\n                logger.debug(e)\n            screen = _CursesScreen(stdscr, height,\n                                   catch_interrupt=catch_interrupt,\n                                   unicode_aware=unicode_aware)\n\n        return screen"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps for Screen. open and Screen. close.", "response": "def wrapper(cls, func, height=None, catch_interrupt=False, arguments=None,\n                unicode_aware=None):\n        \"\"\"\n        Construct a new Screen for any platform.  This will initialize the\n        Screen, call the specified function and then tidy up the system as\n        required when the function exits.\n\n        :param func: The function to call once the Screen has been created.\n        :param height: The buffer height for this Screen (only for test purposes).\n        :param catch_interrupt: Whether to catch and prevent keyboard\n            interrupts.  Defaults to False to maintain backwards compatibility.\n        :param arguments: Optional arguments list to pass to func (after the\n            Screen object).\n        :param unicode_aware: Whether the application can use unicode or not.\n            If None, try to detect from the environment if UTF-8 is enabled.\n        \"\"\"\n        screen = Screen.open(height,\n                             catch_interrupt=catch_interrupt,\n                             unicode_aware=unicode_aware)\n        restore = True\n        try:\n            try:\n                if arguments:\n                    return func(screen, *arguments)\n                else:\n                    return func(screen)\n            except ResizeScreenError:\n                restore = False\n                raise\n        finally:\n            screen.close(restore)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nclear the screen of all content.", "response": "def clear(self):\n        \"\"\"\n        Clear the Screen of all content.\n\n        Note that this will instantly clear the Screen and reset all buffers to the default state,\n        without waiting for you to call :py:meth:`~.Screen.refresh`.\n        \"\"\"\n        # Clear the actual terminal\n        self.reset()\n        self._change_colours(Screen.COLOUR_WHITE, 0, 0)\n        self._clear()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_key(self):\n        event = self.get_event()\n        if event and isinstance(event, KeyboardEvent):\n            return event.key_code\n        return None", "response": "Check for a key without waiting."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ctrl(char):\n        # Convert string to int... assuming any non-integer is a string.\n        # TODO: Consider asserting a more rigorous test without falling back to past basestring.\n        if not isinstance(char, int):\n            char = ord(char.upper())\n\n        # Only deal with the characters between '@' and '_'\n        return char & 0x1f if 64 <= char <= 95 else None", "response": "Converts a given character to a control code."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef putch(self, text, x, y, colour=7, attr=0, bg=0, transparent=False):\n        self.print_at(text, x, y, colour, attr, bg, transparent)", "response": "Print text at the specified location."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _unhandled_event_default(event):\n        if isinstance(event, KeyboardEvent):\n            c = event.key_code\n            if c in (ord(\"X\"), ord(\"x\"), ord(\"Q\"), ord(\"q\")):\n                raise StopApplication(\"User terminated app\")\n            if c in (ord(\" \"), ord(\"\\n\"), ord(\"\\r\")):\n                raise NextScene()", "response": "Default unhandled event handler for handling simple scene navigation."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nplay a set of scenes and return a Screen object.", "response": "def play(self, scenes, stop_on_resize=False, unhandled_input=None,\n             start_scene=None, repeat=True, allow_int=False):\n        \"\"\"\n        Play a set of scenes.\n\n        This is effectively a helper function to wrap :py:meth:`.set_scenes` and\n        :py:meth:`.draw_next_frame` to simplify animation for most applications.\n\n        :param scenes: a list of :py:obj:`.Scene` objects to play.\n        :param stop_on_resize: Whether to stop when the screen is resized.\n            Default is to carry on regardless - which will typically result\n            in an error. This is largely done for back-compatibility.\n        :param unhandled_input: Function to call for any input not handled\n            by the Scenes/Effects being played.  Defaults to a function that\n            closes the application on \"Q\" or \"X\" being pressed.\n        :param start_scene: The old Scene to start from.  This must have name\n            that matches the name of one of the Scenes passed in.\n        :param repeat: Whether to repeat the Scenes once it has reached the end.\n            Defaults to True.\n        :param allow_int: Allow input to interrupt frame rate delay.\n\n        :raises ResizeScreenError: if the screen is resized (and allowed by\n            stop_on_resize).\n\n        The unhandled input function just takes one parameter - the input\n        event that was not handled.\n        \"\"\"\n        # Initialise the Screen for animation.\n        self.set_scenes(\n            scenes, unhandled_input=unhandled_input, start_scene=start_scene)\n\n        # Mainline loop for animations\n        try:\n            while True:\n                a = time.time()\n                self.draw_next_frame(repeat=repeat)\n                if self.has_resized():\n                    if stop_on_resize:\n                        self._scenes[self._scene_index].exit()\n                        raise ResizeScreenError(\"Screen resized\",\n                                                self._scenes[self._scene_index])\n                b = time.time()\n                if b - a < 0.05:\n                    # Just in case time has jumped (e.g. time change), ensure we only delay for 0.05s\n                    pause = min(0.05, a + 0.05 - b)\n                    if allow_int:\n                        self.wait_for_input(pause)\n                    else:\n                        time.sleep(pause)\n        except StopApplication:\n            # Time to stop  - just exit the function.\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_scenes(self, scenes, unhandled_input=None, start_scene=None):\n        # Save off the scenes now.\n        self._scenes = scenes\n\n        # Set up default unhandled input handler if needed.\n        if unhandled_input is None:\n            # Check that none of the Effects is incompatible with the default\n            # handler.\n            safe = True\n            for scene in self._scenes:\n                for effect in scene.effects:\n                    safe &= effect.safe_to_default_unhandled_input\n            if safe:\n                unhandled_input = self._unhandled_event_default\n        self._unhandled_input = unhandled_input\n\n        # Find the starting scene.  Default to first if no match.\n        self._scene_index = 0\n        if start_scene is not None:\n            for i, scene in enumerate(scenes):\n                if scene.name == start_scene.name:\n                    self._scene_index = i\n                    break\n\n        # Reset the Scene - this allows the original scene to pick up old\n        # values on resizing.\n        self._scenes[self._scene_index].reset(\n            old_scene=start_scene, screen=self)\n\n        # Reset other internal state for the animation\n        self._frame = 0\n        self._idle_frame_count = 0\n        self._forced_update = False\n        self.clear()", "response": "Sets the scenes and effects to play."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw the next frame in the currently configured Scenes.", "response": "def draw_next_frame(self, repeat=True):\n        \"\"\"\n        Draw the next frame in the currently configured Scenes. You must call\n        :py:meth:`.set_scenes` before using this for the first time.\n\n        :param repeat: Whether to repeat the Scenes once it has reached the end.\n            Defaults to True.\n\n        :raises StopApplication: if the application should be terminated.\n        \"\"\"\n        scene = self._scenes[self._scene_index]\n        try:\n            # Check for an event now and remember for refresh reasons.\n            event = self.get_event()\n            got_event = event is not None\n\n            # Now process all the input events\n            while event is not None:\n                event = scene.process_event(event)\n                if event is not None and self._unhandled_input is not None:\n                    self._unhandled_input(event)\n                event = self.get_event()\n\n            # Only bother with a refresh if there was an event to process or\n            # we have to refresh due to the refresh limit required for an\n            # Effect.\n            self._frame += 1\n            self._idle_frame_count -= 1\n            if got_event or self._idle_frame_count <= 0 or self._forced_update:\n                self._forced_update = False\n                self._idle_frame_count = 1000000\n                for effect in scene.effects:\n                    # Update the effect and delete if needed.\n                    effect.update(self._frame)\n                    if effect.delete_count is not None:\n                        effect.delete_count -= 1\n                        if effect.delete_count <= 0:\n                            scene.remove_effect(effect)\n\n                    # Sort out when we next _need_ to do a refresh.\n                    if effect.frame_update_count > 0:\n                        self._idle_frame_count = min(self._idle_frame_count,\n                                                     effect.frame_update_count)\n                self.refresh()\n\n            if 0 < scene.duration <= self._frame:\n                raise NextScene()\n        except NextScene as e:\n            # Tidy up the current scene.\n            scene.exit()\n\n            # Find the specified next Scene\n            if e.name is None:\n                # Just allow next iteration of loop\n                self._scene_index += 1\n                if self._scene_index >= len(self._scenes):\n                    if repeat:\n                        self._scene_index = 0\n                    else:\n                        raise StopApplication(\"Repeat disabled\")\n            else:\n                # Find the required scene.\n                for i, scene in enumerate(self._scenes):\n                    if scene.name == e.name:\n                        self._scene_index = i\n                        break\n                else:\n                    raise RuntimeError(\n                        \"Could not find Scene: '{}'\".format(e.name))\n\n            # Reset the screen if needed.\n            scene = self._scenes[self._scene_index]\n            scene.reset()\n            self._frame = 0\n            self._idle_frame_count = 0\n            if scene.clear:\n                self.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _default_next_char(particle):\n        return particle.chars[\n            (len(particle.chars) - 1) * particle.time // particle.life_time]", "response": "Default next character implementation - linear progression through\n            each character."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _default_next_colour(particle):\n        return particle.colours[\n            (len(particle.colours) - 1) * particle.time // particle.life_time]", "response": "Default next colour implementation - linear progression through\n            each colour tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next(self):\n        # Get next particle details\n        x, y = self._move(self)\n        colour = self._next_colour(self)\n        char = self._next_char(self)\n        self._last = char, x, y, colour[0], colour[1], colour[2]\n        self.time += 1\n\n        # Trigger any configured events\n        if self.time == 1 and self._on_create is not None:\n            self._on_create(self)\n        elif self.life_time == self.time and self._on_destroy is not None:\n            self._on_destroy(self)\n        elif self._on_each is not None:\n            self._on_each(self)\n\n        return self._last", "response": "Returns the next particle s attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _scale_coords(self, x, y, extent, xo, yo):\n        return xo + (x * self._size * 2 / extent), yo + ((extent - y) * self._size / extent)", "response": "Convert from tile coordinates to pixels - i. e. text characters"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _convert_latitude(self, latitude):\n        return int((180 - (180 / pi * log(tan(\n            pi / 4 + latitude * pi / 360)))) * (2 ** self._zoom) * self._size / 360)", "response": "Convert from latitude to the y position in overall map."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nshifting the latitude by the required number of pixels.", "response": "def _inc_lat(self, latitude, delta):\n        \"\"\"Shift the latitude by the required number of pixels (i.e. text lines).\"\"\"\n        y = self._convert_latitude(latitude)\n        y += delta\n        return 360 / pi * atan(\n            exp((180 - y * 360 / (2 ** self._zoom) / self._size) * pi / 180)) - 90"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading up a single satellite image tile.", "response": "def _get_satellite_tile(self, x_tile, y_tile, z_tile):\n        \"\"\"Load up a single satellite image tile.\"\"\"\n        cache_file = \"mapscache/{}.{}.{}.jpg\".format(z_tile, x_tile, y_tile)\n        if cache_file not in self._tiles:\n            if not os.path.isfile(cache_file):\n                url = _IMAGE_URL.format(z_tile, x_tile, y_tile, _KEY)\n                data = requests.get(url).content\n                with open(cache_file, 'wb') as f:\n                    f.write(data)\n            self._tiles[cache_file] = [\n                x_tile, y_tile, z_tile,\n                ColourImageFile(self._screen, cache_file, height=_START_SIZE, dither=True,\n                                uni=self._screen.unicode_aware),\n                True]\n            if len(self._tiles) > _CACHE_SIZE:\n                self._tiles.popitem(False)\n            self._screen.force_update()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading up a single vector tile.", "response": "def _get_vector_tile(self, x_tile, y_tile, z_tile):\n        \"\"\"Load up a single vector tile.\"\"\"\n        cache_file = \"mapscache/{}.{}.{}.json\".format(z_tile, x_tile, y_tile)\n        if cache_file not in self._tiles:\n            if os.path.isfile(cache_file):\n                with open(cache_file, 'rb') as f:\n                    tile = json.loads(f.read().decode('utf-8'))\n            else:\n                url = _VECTOR_URL.format(z_tile, x_tile, y_tile, _KEY)\n                data = requests.get(url).content\n                try:\n                    tile = mapbox_vector_tile.decode(data)\n                    with open(cache_file, mode='w') as f:\n                        json.dump(literal_eval(repr(tile)), f)\n                except DecodeError:\n                    tile = None\n            if tile:\n                self._tiles[cache_file] = [x_tile, y_tile, z_tile, tile, False]\n                if len(self._tiles) > _CACHE_SIZE:\n                    self._tiles.popitem(False)\n                self._screen.force_update()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the tiles from the map.", "response": "def _get_tiles(self):\n        \"\"\"Background thread to download map tiles as required.\"\"\"\n        while self._running:\n            self._updated.wait()\n            self._updated.clear()\n\n            # Save off current view and find the nearest tile.\n            satellite = self._satellite\n            zoom = self._zoom\n            size = self._size\n            n = 2 ** zoom\n            x_offset = self._convert_longitude(self._longitude)\n            y_offset = self._convert_latitude(self._latitude)\n\n            # Get the visible tiles around that location - getting most relevant first\n            for x, y, z in [(0, 0, 0), (1, 0, 0), (0, 1, 0), (-1, 0, 0), (0, -1, 0),\n                            (0, 0, -1), (0, 0, 1),\n                            (1, 1, 0), (1, -1, 0), (-1, -1, 0), (-1, 1, 0)]:\n                # Restart if we've already zoomed to another level\n                if self._zoom != zoom:\n                    break\n\n                # Don't get tile if it falls off the grid\n                x_tile = int(x_offset // size) + x\n                y_tile = int(y_offset // size) + y\n                z_tile = zoom + z\n                if (x_tile < 0 or x_tile >= n or y_tile < 0 or y_tile >= n or\n                        z_tile < 0 or z_tile > 20):\n                    continue\n                # noinspection PyBroadException\n                try:\n\n                    # Don't bother rendering if the tile is not visible\n                    top = y_tile * size - y_offset + self._screen.height // 2\n                    left = (x_tile * size - x_offset + self._screen.width // 4) * 2\n                    if z == 0 and (left > self._screen.width or left + self._size * 2 < 0 or\n                                   top > self._screen.height or top + self._size < 0):\n                        continue\n\n                    if satellite:\n                        self._get_satellite_tile(x_tile, y_tile, z_tile)\n                    else:\n                        self._get_vector_tile(x_tile, y_tile, z_tile)\n                # pylint: disable=broad-except\n                except Exception:\n                    self._oops = \"{} - tile loc: {} {} {}\".format(\n                        traceback.format_exc(), x_tile, y_tile, z_tile)\n\n                # Generally refresh screen after we've downloaded everything\n                self._screen.force_update()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of features that can be used to render the current zoom level and view type.", "response": "def _get_features(self):\n        \"\"\"Decide which layers to render based on current zoom level and view type.\"\"\"\n        if self._satellite:\n            return [(\"water\", [], [])]\n        elif self._zoom <= 2:\n            return [\n                (\"water\", [], []),\n                (\"marine_label\", [], [1]),\n            ]\n        elif self._zoom <= 7:\n            return [\n                (\"admin\", [], []),\n                (\"water\", [], []),\n                (\"road\", [\"motorway\"], []),\n                (\"country_label\", [], []),\n                (\"marine_label\", [], [1]),\n                (\"state_label\", [], []),\n                (\"place_label\", [], [\"city\", \"town\"]),\n            ]\n        elif self._zoom <= 10:\n            return [\n                (\"admin\", [], []),\n                (\"water\", [], []),\n                (\"road\", [\"motorway\", \"motorway_link\", \"trunk\"], []),\n                (\"country_label\", [], []),\n                (\"marine_label\", [], [1]),\n                (\"state_label\", [], []),\n                (\"place_label\", [], [\"city\", \"town\"]),\n            ]\n        else:\n            return [\n                (\"landuse\", [\"agriculture\", \"grass\", \"park\"], []),\n                (\"water\", [], []),\n                (\"waterway\", [\"river\", \"canal\"], []),\n                (\"building\", [], []),\n                (\"road\",\n                 [\"motorway\", \"motorway_link\", \"trunk\", \"primary\", \"secondary\"]\n                 if self._zoom <= 14 else\n                 [\"motorway\", \"motorway_link\", \"trunk\", \"primary\", \"secondary\", \"tertiary\",\n                  \"link\", \"street\", \"tunnel\"],\n                 []),\n                (\"poi_label\", [], []),\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw a set of polygons from a vector tile.", "response": "def _draw_polygons(self, feature, bg, colour, extent, polygons, xo, yo):\n        \"\"\"Draw a set of polygons from a vector tile.\"\"\"\n        coords = []\n        for polygon in polygons:\n            coords.append([self._scale_coords(x, y, extent, xo, yo) for x, y in polygon])\n        # Polygons are expensive to draw and the buildings layer is huge - so we convert to\n        # lines in order to process updates fast enough to animate.\n        if \"type\" in feature[\"properties\"] and \"building\" in feature[\"properties\"][\"type\"]:\n            for line in coords:\n                self._draw_lines_internal(line, colour, bg)\n        else:\n            self._screen.fill_polygon(coords, colour=colour, bg=bg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _draw_lines(self, bg, colour, extent, line, xo, yo):\n        coords = [self._scale_coords(x, y, extent, xo, yo) for x, y in line]\n        self._draw_lines_internal(coords, colour, bg)", "response": "Draw a set of lines from a vector tile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _draw_feature(self, feature, extent, colour, bg, xo, yo):\n        geometry = feature[\"geometry\"]\n        if geometry[\"type\"] == \"Polygon\":\n            self._draw_polygons(feature, bg, colour, extent, geometry[\"coordinates\"], xo, yo)\n        elif feature[\"geometry\"][\"type\"] == \"MultiPolygon\":\n            for multi_polygon in geometry[\"coordinates\"]:\n                self._draw_polygons(feature, bg, colour, extent, multi_polygon, xo, yo)\n        elif feature[\"geometry\"][\"type\"] == \"LineString\":\n            self._draw_lines(bg, colour, extent, geometry[\"coordinates\"], xo, yo)\n        elif feature[\"geometry\"][\"type\"] == \"MultiLineString\":\n            for line in geometry[\"coordinates\"]:\n                self._draw_lines(bg, colour, extent, line, xo, yo)\n        elif feature[\"geometry\"][\"type\"] == \"Point\":\n            x, y = self._scale_coords(\n                geometry[\"coordinates\"][0], geometry[\"coordinates\"][1], extent, xo, yo)\n            text = u\" {} \".format(feature[\"properties\"][\"name_en\"])\n            self._screen.print_at(text, int(x - len(text) / 2), int(y), colour=colour, bg=bg)", "response": "Draw a single feature from a vector tile."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _draw_tile_layer(self, tile, layer_name, c_filters, colour, t_filters, x, y, bg):\n        # Don't bother rendering if the tile is not visible\n        left = (x + self._screen.width // 4) * 2\n        top = y + self._screen.height // 2\n        if (left > self._screen.width or left + self._size * 2 < 0 or\n                top > self._screen.height or top + self._size < 0):\n            return 0\n\n        # Not all layers are available in every tile.\n        try:\n            _layer = tile[layer_name]\n            _extent = float(_layer[\"extent\"])\n        except KeyError:\n            return 0\n\n        for _feature in _layer[\"features\"]:\n            try:\n                if c_filters and _feature[\"properties\"][\"class\"] not in c_filters:\n                    continue\n                if (t_filters and _feature[\"type\"] not in t_filters and\n                        _feature[\"properties\"][\"type\"] not in t_filters):\n                    continue\n                self._draw_feature(\n                    _feature, _extent, colour, bg,\n                    (x + self._screen.width // 4) * 2, y + self._screen.height // 2)\n            except KeyError:\n                pass\n        return 1", "response": "Draw the visible geometry in the specified map tile."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _draw_satellite_tile(self, tile, x, y):\n        image, colours = tile.rendered_text\n        for (i, line) in enumerate(image):\n            self._screen.paint(line, x, y + i, colour_map=colours[i])\n        return 1", "response": "Draw a satellite image tile to screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrenders all visible tiles a layer at a time.", "response": "def _draw_tiles(self, x_offset, y_offset, bg):\n        \"\"\"Render all visible tiles a layer at a time.\"\"\"\n        count = 0\n        for layer_name, c_filters, t_filters in self._get_features():\n            colour = (self._256_PALETTE[layer_name]\n                      if self._screen.colours >= 256 else self._16_PALETTE[layer_name])\n            for x, y, z, tile, satellite in sorted(self._tiles.values(), key=lambda k: k[0]):\n                # Don't draw the wrong type or zoom of tile.\n                if satellite != self._satellite or z != self._zoom:\n                    continue\n\n                # Convert tile location into pixels and draw the tile.\n                x *= self._size\n                y *= self._size\n                if satellite:\n                    count += self._draw_satellite_tile(\n                        tile,\n                        int((x-x_offset + self._screen.width // 4) * 2),\n                        int(y-y_offset + self._screen.height // 2))\n                else:\n                    count += self._draw_tile_layer(tile, layer_name, c_filters, colour, t_filters,\n                                                   x - x_offset, y - y_offset, bg)\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _zoom_map(self, zoom_out=True):\n        size_step = 1 / _ZOOM_STEP if zoom_out else _ZOOM_STEP\n        self._next_update = 1\n        if self._satellite:\n            size_step **= _ZOOM_ANIMATION_STEPS\n        self._size *= size_step\n        if self._size <= _ZOOM_OUT_SIZE:\n            if self._zoom > 0:\n                self._zoom -= 1\n                self._size = _START_SIZE\n            else:\n                self._size = _ZOOM_OUT_SIZE\n        elif self._size >= _ZOOM_IN_SIZE:\n            if self._zoom < 20:\n                self._zoom += 1\n                self._size = _START_SIZE\n            else:\n                self._size = _ZOOM_IN_SIZE", "response": "Animate the zoom in or out as appropriate for the displayed map tile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _move_to_desired_location(self):\n        self._next_update = 100000\n        x_start = self._convert_longitude(self._longitude)\n        y_start = self._convert_latitude(self._latitude)\n        x_end = self._convert_longitude(self._desired_longitude)\n        y_end = self._convert_latitude(self._desired_latitude)\n        if sqrt((x_end - x_start) ** 2 + (y_end - y_start) ** 2) > _START_SIZE // 4:\n            self._zoom_map(True)\n        elif self._zoom != self._desired_zoom:\n            self._zoom_map(self._desired_zoom < self._zoom)\n        if self._longitude != self._desired_longitude:\n            self._next_update = 1\n            if self._desired_longitude < self._longitude:\n                self._longitude = max(self._longitude - 360 / 2 ** self._zoom / self._size * 2,\n                                      self._desired_longitude)\n            else:\n                self._longitude = min(self._longitude + 360 / 2 ** self._zoom / self._size * 2,\n                                      self._desired_longitude)\n        if self._latitude != self._desired_latitude:\n            self._next_update = 1\n            if self._desired_latitude < self._latitude:\n                self._latitude = max(self._inc_lat(self._latitude, 2), self._desired_latitude)\n            else:\n                self._latitude = min(self._inc_lat(self._latitude, -2), self._desired_latitude)\n        if self._next_update == 1:\n            self._updated.set()", "response": "Animate movement to desired location on map."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw the latest set of tiles to the Screen.", "response": "def _update(self, frame_no):\n        \"\"\"Draw the latest set of tiles to the Screen.\"\"\"\n        # Check for any fatal errors from the background thread and quit if we hit anything.\n        if self._oops:\n            raise RuntimeError(self._oops)\n\n        # Calculate new positions for animated movement.\n        self._move_to_desired_location()\n\n        # Re-draw the tiles - if we have any suitable ones downloaded.\n        count = 0\n        x_offset = self._convert_longitude(self._longitude)\n        y_offset = self._convert_latitude(self._latitude)\n        if self._tiles:\n            # Clear the area first.\n            bg = 253 if self._screen.unicode_aware and self._screen.colours >= 256 else 0\n            for y in range(self._screen.height):\n                self._screen.print_at(\".\" * self._screen.width, 0, y, colour=bg, bg=bg)\n\n            # Now draw all the available tiles.\n            count = self._draw_tiles(x_offset, y_offset, bg)\n\n        # Just a few pointers on what the user should do...\n        if count == 0:\n            self._screen.centre(\" Loading - please wait... \", self._screen.height // 2, 1)\n\n        self._screen.centre(\"Press '?' for help.\", 0, 1)\n        if _KEY == \"\":\n            footer = \"Using local cached data - go to https://www.mapbox.com/ and get a free key.\"\n        else:\n            footer = u\"Zoom: {} Location: {:.6}, {:.6} Maps: \u00a9 Mapbox, \u00a9 OpenStreetMap\".format(\n                self._zoom, self._longitude, self._latitude)\n        self._screen.centre(footer, self._screen.height - 1, 1)\n\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the event and update the internal state of the object.", "response": "def process_event(self, event):\n        \"\"\"User input for the main map view.\"\"\"\n        if isinstance(event, KeyboardEvent):\n            if event.key_code in [Screen.ctrl(\"m\"), Screen.ctrl(\"j\")]:\n                self._scene.add_effect(\n                    EnterLocation(\n                        self._screen, self._longitude, self._latitude, self._on_new_location))\n            elif event.key_code in [ord('q'), ord('Q'), Screen.ctrl(\"c\")]:\n                raise StopApplication(\"User quit\")\n            elif event.key_code in [ord('t'), ord('T')]:\n                self._satellite = not self._satellite\n                if self._satellite:\n                    self._size = _START_SIZE\n            elif event.key_code == ord(\"?\"):\n                self._scene.add_effect(PopUpDialog(self._screen, _HELP, [\"OK\"]))\n            elif event.key_code == ord(\"+\") and self._zoom <= 20:\n                if self._desired_zoom < 20:\n                    self._desired_zoom += 1\n            elif event.key_code == ord(\"-\") and self._zoom >= 0:\n                if self._desired_zoom > 0:\n                    self._desired_zoom -= 1\n            elif event.key_code == ord(\"0\"):\n                self._desired_zoom = 0\n            elif event.key_code == ord(\"9\"):\n                self._desired_zoom = 20\n            elif event.key_code == Screen.KEY_LEFT:\n                self._desired_longitude -= 360 / 2 ** self._zoom / self._size * 10\n            elif event.key_code == Screen.KEY_RIGHT:\n                self._desired_longitude += 360 / 2 ** self._zoom / self._size * 10\n            elif event.key_code == Screen.KEY_UP:\n                self._desired_latitude = self._inc_lat(self._desired_latitude, -self._size / 10)\n            elif event.key_code == Screen.KEY_DOWN:\n                self._desired_latitude = self._inc_lat(self._desired_latitude, self._size / 10)\n            else:\n                return\n\n            # Trigger a reload of the tiles and redraw map\n            self._updated.set()\n            self._screen.force_update()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _on_new_location(self, form):\n        self._desired_longitude = float(form.data[\"long\"])\n        self._desired_latitude = float(form.data[\"lat\"])\n        self._desired_zoom = 13\n        self._screen.force_update()", "response": "Set a new desired location entered in the pop - up form."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _sign_url(self, base_url=None, params=None, client_secret=None):\n        import hashlib\n        import hmac\n        import base64\n        if six.PY3:\n            from urllib.parse import urlparse, urlencode\n        else:\n            from urllib import urlencode\n            from urlparse import urlparse\n\n        # Return if any parameters aren't given\n        if not base_url or not self.client_secret or not self.client:\n            return None\n\n        # assuming parameters will be submitted to Requests in identical order!\n        url = urlparse(base_url + \"?\" + urlencode(params))\n\n        # We only need to sign the path+query part of the string\n        url_to_sign = (url.path + \"?\" + url.query).encode('utf-8')\n\n        # Decode the private key into its binary format\n        # We need to decode the URL-encoded private key\n        decoded_key = base64.urlsafe_b64decode(client_secret)\n\n        # Create a signature using the private key and the URL-encoded\n        # string using HMAC SHA1. This signature will be binary.\n        signature = hmac.new(decoded_key, url_to_sign, hashlib.sha1)\n\n        # Encode the binary signature into base64 for use within a URL\n        encoded_signature = base64.urlsafe_b64encode(signature.digest())\n\n        # Return signature (to be appended as a 'signature' in params)\n        return encoded_signature", "response": "Sign a request URL with a Crypto Key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cli(location, **kwargs):\n\n    locations = []\n\n    # Read Standard Input\n    # $ cat foo.txt | geocode\n    try:\n        for line in fileinput.input():\n            locations.append(line.strip())\n    except:\n        pass\n\n    # Read multiple files & user input location\n    for item in location:\n        if os.path.exists(item):\n            with open(item, 'rb') as f:\n                locations += f.read().splitlines()\n        else:\n            locations.append(item)\n\n    # Distance calculation\n    if kwargs['distance']:\n        d = geocoder.distance(locations, **kwargs)\n        click.echo(d)\n        return\n\n    # Geocode results from user input\n    for location in locations:\n        g = geocoder.get(location.strip(), **kwargs)\n        try:\n            click.echo(json.dumps(getattr(g, kwargs['output'])))\n        except IOError:\n            # When invalid command is entered a broken pipe error occurs\n            return", "response": "Geocode an arbitrary number of strings from Command Line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _build_params(self, location, provider_key, **kwargs):\n        base_kwargs = {\n            'q': location,\n            'fuzzy': kwargs.get('fuzzy', 1.0),\n            'username': provider_key,\n            'maxRows': kwargs.get('maxRows', 1),\n        }\n        # check out for bbox in kwargs\n        bbox = kwargs.pop('proximity', None)\n        if bbox is not None:\n            bbox = BBox.factory(bbox)\n            base_kwargs.update(\n                {'east': bbox.east, 'west': bbox.west,\n                 'north': bbox.north, 'south': bbox.south})\n\n        # look out for valid extra kwargs\n        supported_kwargs = set((\n            'name', 'name_equals', 'name_startsWith', 'startRow',\n            'country', 'countryBias', 'continentCode',\n            'adminCode1', 'adminCode2', 'adminCode3', 'cities',\n            'featureClass', 'featureCode',\n            'lang', 'type', 'style',\n            'isNameRequired', 'tag', 'operator', 'charset',\n            'east', 'west', 'north', 'south',\n            'orderby', 'inclBbox',\n        ))\n        found_kwargs = supported_kwargs & set(kwargs.keys())\n        LOGGER.debug(\"Adding extra kwargs %s\", found_kwargs)\n\n        # update base kwargs with extra ones\n        base_kwargs.update(dict(\n            [(extra, kwargs[extra]) for extra in found_kwargs]\n        ))\n        return base_kwargs", "response": "Build the base keyword arguments for the base class."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncatches errors from the API and return the error message", "response": "def _catch_errors(self, json_response):\n        \"\"\" Changed: removed check on number of elements:\n            - totalResultsCount not sytematically returned (e.g in hierarchy)\n            - done in base.py\n        \"\"\"\n        status = json_response.get('status')\n        if status:\n            message = status.get('message')\n            value = status.get('value')\n            custom_messages = {\n                10: 'Invalid credentials',\n                18: 'Do not use the demo account for your application',\n            }\n            self.error = custom_messages.get(value, message)\n            LOGGER.error(\"Error %s from JSON %s\", self.error, json_response)\n\n        return self.error"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsigning a request url with a security key.", "response": "def _sign_url(self, base_url, params, security_key):\n        \"\"\"\n        Signs a request url with a security key.\n        \"\"\"\n        import hashlib\n\n        if six.PY3:\n            from urllib.parse import urlencode, quote, quote_plus\n        else:\n            from urllib import urlencode, quote, quote_plus\n\n        if not base_url or not self.security_key:\n            return None\n\n        params = params.copy()\n        address = params.pop('address')\n\n        url = base_url + '?address=' + address + '&' + urlencode(params)\n        encoded_url = quote(url, safe=\"/:=&?#+!$,;'@()*[]\")\n\n        signature = quote_plus(encoded_url + self.security_key).encode('utf-8')\n        encoded_signature = hashlib.md5(signature).hexdigest()\n\n        return encoded_signature"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _parse_json_with_fieldnames(self):\n        for key in dir(self):\n            if not key.startswith('_') and key not in self._TO_EXCLUDE:\n                self.fieldnames.append(key)\n                value = getattr(self, key)\n                if value:\n                    self.json[key] = value\n        # Add OK attribute even if value is \"False\"\n        self.json['ok'] = self.ok", "response": "Parse the raw JSON with all attributes and methods defined in the class."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _is_valid_url(url):\n        try:\n            parsed = urlparse(url)\n            mandatory_parts = [parsed.scheme, parsed.netloc]\n            return all(mandatory_parts)\n        except:\n            return False", "response": "Helper function to validate that a URL is well formed i. e. that it contains a valid\n            protocol and a valid domain."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconnecting to the Asterisk API and return the response as a JSON object.", "response": "def _connect(self):\n        \"\"\" - Query self.url (validated cls._URL)\n            - Analyse reponse and set status, errors accordingly\n            - On success:\n\n                 returns the content of the response as a JSON object\n                 This object will be passed to self._parse_json_response\n        \"\"\"\n        self.status_code = 'Unknown'\n\n        try:\n            # make request and get response\n            self.response = response = self.rate_limited_get(\n                self.url,\n                params=self.params,\n                headers=self.headers,\n                timeout=self.timeout,\n                proxies=self.proxies\n            )\n\n            # check that response is ok\n            self.status_code = response.status_code\n            response.raise_for_status()\n\n            # rely on json method to get non-empty well formatted JSON\n            json_response = response.json()\n            self.url = response.url\n            LOGGER.info(\"Requested %s\", self.url)\n\n        except requests.exceptions.RequestException as err:\n            # store real status code and error\n            self.error = u'ERROR - {}'.format(str(err))\n            LOGGER.error(\"Status code %s from %s: %s\",\n                         self.status_code, self.url, self.error)\n\n            # return False\n            return False\n\n        # return response within its JSON format\n        return json_response"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if specific URL has not been provided otherwise use cls. _URL", "response": "def _before_initialize(self, location, **kwargs):\n        \"\"\" Check if specific URL has not been provided, otherwise, use cls._URL\"\"\"\n        url = kwargs.get('url', '')\n        if url.lower() == 'localhost':\n            self.url = 'http://localhost/nominatim/search'\n        elif url:\n            self.url = url"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef haversine(point1, point2, **kwargs):\n\n    lookup_units = {\n        'miles': 'miles',\n        'mile': 'miles',\n        'mi': 'miles',\n        'ml': 'miles',\n        'kilometers': 'kilometers',\n        'kilometres': 'kilometers',\n        'kilometer': 'kilometers',\n        'kilometre': 'kilometers',\n        'km': 'kilometers',\n        'meters': 'meters',\n        'metres': 'meters',\n        'meter': 'meters',\n        'metre': 'meters',\n        'm': 'meters',\n        'feet': 'feet',\n        'f': 'feet',\n        'ft': 'feet',\n    }\n\n    if point1.ok and point2.ok:\n        # convert all latitudes/longitudes from decimal degrees to radians\n        lat1, lng1, lat2, lng2 = list(map(radians, point1.latlng + point2.latlng))\n\n        # calculate haversine\n        lat = lat2 - lat1\n        lng = lng2 - lng1\n        d = sin(lat / 2) ** 2 + cos(lat1) * cos(lat2) * sin(lng / 2) ** 2\n        h = 2 * AVG_EARTH_RADIUS * asin(sqrt(d))\n\n        # Measurements\n        units = kwargs.get('units', 'kilometers').lower()\n        units_calculation = {\n            'miles': h * 0.621371,\n            'feet': h * 0.621371 * 5280,\n            'meters': h * 1000,\n            'kilometers': h,\n        }\n\n        if units in lookup_units:\n            return units_calculation[lookup_units[units]]\n        else:\n            raise ValueError(\"Unknown units of measurement\")\n\n    else:\n        print(u'[WARNING] Error calculating the following two locations.\\n'\n              'Points: {0} to {1}'.format(point1.location, point2.location))", "response": "Calculate the great - circle distance bewteen two points on the Earth surface."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a single object from the geocoding engine.", "response": "def get(location, **kwargs):\n    \"\"\"Get Geocode\n\n    :param ``location``: Your search location you want geocoded.\n    :param ``provider``: The geocoding engine you want to use.\n\n    :param ``method``: Define the method (geocode, method).\n    \"\"\"\n    provider = kwargs.get('provider', 'bing').lower().strip()\n    method = kwargs.get('method', 'geocode').lower().strip()\n    if isinstance(location, (list, dict)) and method == 'geocode':\n        raise ValueError(\"Location should be a string\")\n\n    if provider not in options:\n        raise ValueError(\"Invalid provider\")\n\n    else:\n        if method not in options[provider]:\n            raise ValueError(\"Invalid method\")\n    return options[provider][method](location, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreverse Geocoding :param ``location``: Your search location you want to reverse geocode. :param ``key``: (optional) use your own API Key from Bing. :param ``provider``: (default=google) Use the following: > google > bing", "response": "def reverse(location, provider=\"google\", **kwargs):\n    \"\"\"Reverse Geocoding\n\n    :param ``location``: Your search location you want to reverse geocode.\n    :param ``key``: (optional) use your own API Key from Bing.\n    :param ``provider``: (default=google) Use the following:\n        > google\n        > bing\n    \"\"\"\n    return get(location, method='reverse', provider=provider, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef area(self):\n        area = self.parse['attributes'].get('Shape_Area')\n        if area:\n            return round(float(area) * 10.76391)", "response": "Square Foot Area (sqft)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the frontage of the user s company.", "response": "def frontage(self):\n        \"\"\"Length in Feet (f)\"\"\"\n        if self.length and self.area:\n            return round(self.area / self.length)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_gif(api_key, gif_id):\n    '''Returns dict with gif informations from the API.'''\n    url = 'http://api.giphy.com/v1/gifs/{}?api_key={}'.format(gif_id, api_key)\n    r = urlopen(url)\n\n    return json.loads(r.read().decode('utf-8'))", "response": "Returns dict with gif informations from the API."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn complete html tag string.", "response": "def create_html(api_key, attrs):\n    '''Returns complete html tag string.'''\n    gif = get_gif(api_key, attrs['gif_id'])\n\n    if 'alt' not in attrs.keys():\n        attrs['alt'] = 'source: {}'.format(gif['data']['source'])\n\n    html_out = '<a href=\"{}\">'.format(gif['data']['url'])\n    html_out += '<img src=\"{}\" alt=\"{}\">'.format(\n        gif['data']['images']['original']['url'],\n        attrs['alt'])\n    html_out += '</a>'\n\n    return html_out"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndoes the regex parsing and running the create_html function.", "response": "def main(api_key, markup):\n    '''Doing the regex parsing and running the create_html function.'''\n    match = GIPHY.search(markup)\n\n    attrs = None\n\n    if match:\n        attrs = dict(\n            [(key, value.strip())\n             for (key, value) in match.groupdict().items() if value])\n\n    else:\n        raise ValueError('Error processing input. '\n                         'Expected syntax: {}'.format(SYNTAX))\n\n    return create_html(api_key, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate modification and creation times from git content", "response": "def filetime_from_git(content, git_content):\n    '''\n    Update modification and creation times from git\n    '''\n    if not content.settings['GIT_FILETIME_FROM_GIT']:\n        # Disabled for everything\n        return\n\n    if not string_to_bool(content.metadata.get('gittime', 'yes')):\n        # Disable for this content\n        return\n\n    path = content.source_path\n    fs_creation_time = datetime_from_timestamp(os.stat(path).st_ctime, content)\n    fs_modified_time = datetime_from_timestamp(os.stat(path).st_mtime, content)\n\n    # 1. file is not managed by git\n    #    date: fs time\n    # 2. file is staged, but has no commits\n    #    date: fs time\n    # 3. file is managed, and clean\n    #    date: first commit time, update: last commit time or None\n    # 4. file is managed, but dirty\n    #    date: first commit time, update: fs time\n    if git_content.is_managed_by_git():\n        if git_content.is_committed():\n            content.date = git_content.get_oldest_commit_date()\n\n            if git_content.is_modified():\n                content.modified = fs_modified_time\n            else:\n                content.modified = git_content.get_newest_commit_date()\n        else:\n            # File isn't committed\n            content.date = fs_creation_time\n    else:\n        # file is not managed by git\n        content.date = fs_creation_time\n\n    # Clean up content attributes\n    if not hasattr(content, 'modified'):\n        content.modified = content.date\n\n    if hasattr(content, 'date'):\n        content.locale_date = strftime(content.date, content.date_format)\n\n    if hasattr(content, 'modified'):\n        content.locale_modified = strftime(\n            content.modified, content.date_format)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd sha metadata to content", "response": "def git_sha_metadata(content, git_content):\n    '''\n    Add sha metadata to content\n    '''\n    if not content.settings['GIT_SHA_METADATA']:\n        return\n\n    if not git_content.is_committed():\n        return\n\n    content.metadata['gitsha_newest'] = str(git_content.get_newest_commit())\n    content.metadata['gitsha_oldest'] = str(git_content.get_oldest_commit())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update_hash_from_str(hsh, str_input):\n    byte_input = str(str_input).encode(\"UTF-8\")\n    hsh.update(byte_input)", "response": "Convert a str to object supporting buffer API and update a hash with it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef git_permalink(content, git_content):\n    '''\n    Add git based permalink id to content metadata\n    '''\n    if not content.settings['GIT_GENERATE_PERMALINK']:\n        return\n\n    if not string_to_bool(content.metadata.get('git_permalink', 'yes')):\n        # Disable for this content\n        return\n\n    if not git_content.is_committed():\n        return\n\n    permalink_hash = hashlib.sha1()\n    update_hash_from_str(permalink_hash, git_content.get_oldest_commit())\n    update_hash_from_str(permalink_hash, git_content.get_oldest_filename())\n    git_permalink_id_raw = base64.urlsafe_b64encode(permalink_hash.digest())\n    git_permalink_id = git_permalink_id_raw.decode(\"UTF-8\")\n    permalink_id_metadata_key = content.settings['PERMALINK_ID_METADATA_KEY']\n\n    if permalink_id_metadata_key in content.metadata:\n        content.metadata[permalink_id_metadata_key] = (\n            ','.join((\n                content.metadata[permalink_id_metadata_key], git_permalink_id))\n        )\n    else:\n        content.metadata[permalink_id_metadata_key] = git_permalink_id", "response": "Add git based permalink id to content metadata\n   "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset user specified MathJax settings in the current project.", "response": "def process_settings(pelicanobj):\n    \"\"\"Sets user specified MathJax settings (see README for more details)\"\"\"\n\n    mathjax_settings = {}\n\n    # NOTE TO FUTURE DEVELOPERS: Look at the README and what is happening in\n    # this function if any additional changes to the mathjax settings need to\n    # be incorporated. Also, please inline comment what the variables\n    # will be used for\n\n    # Default settings\n    mathjax_settings['auto_insert'] = True  # if set to true, it will insert mathjax script automatically into content without needing to alter the template.\n    mathjax_settings['align'] = 'center'  # controls alignment of of displayed equations (values can be: left, right, center)\n    mathjax_settings['indent'] = '0em'  # if above is not set to 'center', then this setting acts as an indent\n    mathjax_settings['show_menu'] = 'true'  # controls whether to attach mathjax contextual menu\n    mathjax_settings['process_escapes'] = 'true'  # controls whether escapes are processed\n    mathjax_settings['latex_preview'] = 'TeX'  # controls what user sees while waiting for LaTex to render\n    mathjax_settings['color'] = 'inherit'  # controls color math is rendered in\n    mathjax_settings['linebreak_automatic'] = 'false'  # Set to false by default for performance reasons (see http://docs.mathjax.org/en/latest/output.html#automatic-line-breaking)\n    mathjax_settings['tex_extensions'] = ''  # latex extensions that can be embedded inside mathjax (see http://docs.mathjax.org/en/latest/tex.html#tex-and-latex-extensions)\n    mathjax_settings['responsive'] = 'false'  # Tries to make displayed math responsive\n    mathjax_settings['responsive_break'] = '768'  # The break point at which it math is responsively aligned (in pixels)\n    mathjax_settings['mathjax_font'] = 'default'  # forces mathjax to use the specified font.\n    mathjax_settings['process_summary'] = BeautifulSoup is not None  # will fix up summaries if math is cut off. Requires beautiful soup\n    mathjax_settings['message_style'] = 'normal'  # This value controls the verbosity of the messages in the lower left-hand corner. Set it to \"none\" to eliminate all messages\n    mathjax_settings['font_list'] = ['STIX', 'TeX'] # Include in order of preference among TeX, STIX-Web, Asana-Math, Neo-Euler, Gyre-Pagella, Gyre-Termes and Latin-Modern\n    mathjax_settings['equation_numbering'] = 'none' # AMS, auto, none\n\n    # Source for MathJax\n    mathjax_settings['source'] = \"'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML'\"\n\n    # Get the user specified settings\n    try:\n        settings = pelicanobj.settings['MATH_JAX']\n    except:\n        settings = None\n\n    # If no settings have been specified, then return the defaults\n    if not isinstance(settings, dict):\n        return mathjax_settings\n\n    # The following mathjax settings can be set via the settings dictionary\n    for key, value in ((key, settings[key]) for key in settings):\n        # Iterate over dictionary in a way that is compatible with both version 2\n        # and 3 of python\n\n        if key == 'align':\n            typeVal = isinstance(value, string_type)\n\n            if not typeVal:\n                continue\n\n            if value == 'left' or value == 'right' or value == 'center':\n                mathjax_settings[key] = value\n            else:\n                mathjax_settings[key] = 'center'\n\n        if key == 'indent':\n            mathjax_settings[key] = value\n\n        if key == 'source':\n            mathjax_settings[key] = value\n\n        if key == 'show_menu' and isinstance(value, bool):\n            mathjax_settings[key] = 'true' if value else 'false'\n\n        if key == 'message_style':\n            mathjax_settings[key] = value if value is not None else 'none'\n\n        if key == 'auto_insert' and isinstance(value, bool):\n            mathjax_settings[key] = value\n\n        if key == 'process_escapes' and isinstance(value, bool):\n            mathjax_settings[key] = 'true' if value else 'false'\n\n        if key == 'latex_preview':\n            typeVal = isinstance(value, string_type)\n\n            if not typeVal:\n                continue\n\n            mathjax_settings[key] = value\n\n        if key == 'color':\n            typeVal = isinstance(value, string_type)\n\n            if not typeVal:\n                continue\n\n            mathjax_settings[key] = value\n\n        if key == 'linebreak_automatic' and isinstance(value, bool):\n            mathjax_settings[key] = 'true' if value else 'false'\n\n        if key == 'process_summary' and isinstance(value, bool):\n            if value and BeautifulSoup is None:\n                print(\"BeautifulSoup4 is needed for summaries to be processed by render_math\\nPlease install it\")\n                value = False\n\n            mathjax_settings[key] = value\n\n        if key == 'responsive' and isinstance(value, bool):\n            mathjax_settings[key] = 'true' if value else 'false'\n\n        if key == 'responsive_break' and isinstance(value, int):\n            mathjax_settings[key] = str(value)\n\n        if key == 'tex_extensions' and isinstance(value, list):\n            # filter string values, then add '' to them\n            value = filter(lambda string: isinstance(string, string_type), value)\n            value = map(lambda string: \"'%s'\" % string, value)\n            mathjax_settings[key] = ',' + ','.join(value)\n\n        if key == 'mathjax_font':\n            typeVal = isinstance(value, string_type)\n\n            if not typeVal:\n                continue\n\n            value = value.lower()\n\n            if value == 'sanserif':\n                value = 'SansSerif'\n            elif value == 'fraktur':\n                value = 'Fraktur'\n            elif value == 'typewriter':\n                value = 'Typewriter'\n            else:\n                value = 'default'\n\n            mathjax_settings[key] = value\n\n        if key == 'font_list' and isinstance(value, list):\n            # make an array string from the list\n            value = filter(lambda string: isinstance(string, string_type), value)\n            value = map(lambda string: \",'%s'\" % string, value)\n            mathjax_settings[key] = ''.join(value)[1:]\n\n        if key == 'equation_numbering':\n            mathjax_settings[key] = value if value is not None else 'none'\n\n    return mathjax_settings"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_summary(article):\n\n    summary = article.summary\n    summary_parsed = BeautifulSoup(summary, 'html.parser')\n    math = summary_parsed.find_all(class_='math')\n\n    if len(math) > 0:\n        last_math_text = math[-1].get_text()\n        if len(last_math_text) > 3 and last_math_text[-3:] == '...':\n            content_parsed = BeautifulSoup(article._content, 'html.parser')\n            full_text = content_parsed.find_all(class_='math')[len(math)-1].get_text()\n            math[-1].string = \"%s ...\" % full_text\n            summary = summary_parsed.decode()\n\n        # clear memoization cache\n        import functools\n        if isinstance(article.get_summary, functools.partial):\n            memoize_instance = article.get_summary.func.__self__\n            memoize_instance.cache.clear()\n\n        article._summary = \"%s<script type='text/javascript'>%s</script>\" % (summary, process_summary.mathjax_script)", "response": "Ensures summaries are not cut off. Also inserts mathjax script so that math will be rendered"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload the mathjax script template from file and render with the settings", "response": "def process_mathjax_script(mathjax_settings):\n    \"\"\"Load the mathjax script template from file, and render with the settings\"\"\"\n\n    # Read the mathjax javascript template from file\n    with open (os.path.dirname(os.path.realpath(__file__))\n            + '/mathjax_script_template', 'r') as mathjax_script_template:\n        mathjax_template = mathjax_script_template.read()\n\n    return mathjax_template.format(**mathjax_settings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pelican_init(pelicanobj):\n\n    # Process settings, and set global var\n    mathjax_settings = process_settings(pelicanobj)\n\n    # Generate mathjax script\n    mathjax_script = process_mathjax_script(mathjax_settings)\n\n    # Configure Typogrify\n    configure_typogrify(pelicanobj, mathjax_settings)\n\n    # Configure Mathjax For Markdown\n    if PelicanMathJaxExtension:\n        mathjax_for_markdown(pelicanobj, mathjax_script, mathjax_settings)\n\n    # Configure Mathjax For RST\n    mathjax_for_rst(pelicanobj, mathjax_script, mathjax_settings)\n\n    # Set process_summary's mathjax_script variable\n    process_summary.mathjax_script = None\n    if mathjax_settings['process_summary']:\n        process_summary.mathjax_script = mathjax_script", "response": "Initializes the mathjax script according to the settings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef rst_add_mathjax(content):\n\n    # .rst is the only valid extension for reStructuredText files\n    _, ext = os.path.splitext(os.path.basename(content.source_path))\n    if ext != '.rst':\n        return\n\n    # If math class is present in text, add the javascript\n    # note that RST hardwires mathjax to be class \"math\"\n    if 'class=\"math\"' in content._content:\n        content._content += \"<script type='text/javascript'>%s</script>\" % rst_add_mathjax.mathjax_script", "response": "Adds mathjax script for reStructuredText files"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprocess the RST and summaries.", "response": "def process_rst_and_summaries(content_generators):\n    \"\"\"\n    Ensure mathjax script is applied to RST and summaries are\n    corrected if specified in user settings.\n\n    Handles content attached to ArticleGenerator and PageGenerator objects,\n    since the plugin doesn't know how to handle other Generator types.\n\n    For reStructuredText content, examine both articles and pages.\n    If article or page is reStructuredText and there is math present,\n    append the mathjax script.\n\n    Also process summaries if present (only applies to articles)\n    and user wants summaries processed (via user settings)\n    \"\"\"\n\n    for generator in content_generators:\n        if isinstance(generator, generators.ArticlesGenerator):\n            for article in (\n                    generator.articles +\n                    generator.translations +\n                    generator.drafts):\n                rst_add_mathjax(article)\n                #optionally fix truncated formulae in summaries.\n                if process_summary.mathjax_script is not None:\n                    process_summary(article)\n        elif isinstance(generator, generators.PagesGenerator):\n            for page in generator.pages:\n                rst_add_mathjax(page)\n            for page in generator.hidden_pages:\n                rst_add_mathjax(page)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget just path component of permalink.", "response": "def get_permalink_path(self):\n    \"\"\"Get just path component of permalink.\"\"\"\n    try:\n        first_permalink_id = next(self.get_permalink_ids_iter())\n    except StopIteration:\n        return None\n\n    return '/{settings[PERMALINK_PATH]}/{first_permalink}.html'.format(\n        settings=self.settings, first_permalink=first_permalink_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding permalink methods to object", "response": "def add_permalink_methods(content_inst):\n    '''\n    Add permalink methods to object\n    '''\n    for permalink_method in PERMALINK_METHODS:\n        setattr(\n            content_inst,\n            permalink_method.__name__,\n            permalink_method.__get__(content_inst, content_inst.__class__))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngenerates the context for the current locale.", "response": "def generate_context(self):\n        '''\n        Setup context\n        '''\n        self.permalink_output_path = os.path.join(\n            self.output_path, self.settings['PERMALINK_PATH'])\n        self.permalink_id_metadata_key = (\n            self.settings['PERMALINK_ID_METADATA_KEY'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_output(self, writer=None):\n        '''\n        Generate redirect files\n        '''\n        logger.info(\n            'Generating permalink files in %r', self.permalink_output_path)\n\n        clean_output_dir(self.permalink_output_path, [])\n        mkdir_p(self.permalink_output_path)\n        for content in itertools.chain(\n                self.context['articles'], self.context['pages']):\n\n            for permalink_id in content.get_permalink_ids_iter():\n                permalink_path = os.path.join(\n                    self.permalink_output_path, permalink_id) + '.html'\n\n                redirect_string = REDIRECT_STRING.format(\n                    url=article_url(content),\n                    title=content.title)\n                open(permalink_path, 'w').write(redirect_string)", "response": "Generate the output of the page."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getText(node, recursive=False):\n    L = [u'']\n    for n in node.childNodes:\n        if n.nodeType in (node.TEXT_NODE, node.CDATA_SECTION_NODE):\n            L.append(n.data)\n        else:\n            if not recursive:\n                return None\n        L.append(getText(n))\n    return u''.join(L)", "response": "Get all the text associated with this node."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_info(photo_id, api_key):\n    ''' Get photo informations from flickr api. '''\n    query = urlencode({\n        'method': 'flickr.photos.getInfo',\n        'api_key': api_key,\n        'photo_id': photo_id,\n        'format': 'json',\n        'nojsoncallback': '1'\n    })\n\n    r = urlopen('https://api.flickr.com/services/rest/?' + query)\n    info = json.loads(r.read().decode('utf-8'))\n\n    if info['stat'] == 'fail':\n        raise ValueError(info['message'])\n\n    return info", "response": "Get photo informations from flickr api."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate html code for a single page.", "response": "def generate_html(attrs, api_key):\n    ''' Returns html code. '''\n    # getting flickr api data\n    flickr_data = get_info(attrs['photo_id'], api_key)\n\n    # if size is not defined it will use large as image size\n    if 'size' not in attrs.keys():\n        attrs['size'] = 'large'\n\n    # if no alt is defined it will use the flickr image title\n    if 'alt' not in attrs.keys():\n        attrs['alt'] = flickr_data['photo']['title']['_content']\n\n    # return final html code\n    return '<a href=\"{}\"><img src=\"{}\" alt=\"{}\"></a>'.format(\n        flickr_data['photo']['urls']['url'][0]['_content'],\n        source_url(flickr_data['photo']['farm'],\n                   flickr_data['photo']['server'],\n                   attrs['photo_id'],\n                   flickr_data['photo']['secret'],\n                   attrs['size']),\n        attrs['alt'])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_gzip_cache(pelican):\n    '''Create a gzip cache file for every file that a webserver would\n    reasonably want to cache (e.g., text type files).\n\n    :param pelican: The Pelican instance\n    '''\n    for dirpath, _, filenames in os.walk(pelican.settings['OUTPUT_PATH']):\n        for name in filenames:\n            if should_compress(name):\n                filepath = os.path.join(dirpath, name)\n                create_gzip_file(filepath, should_overwrite(pelican.settings))", "response": "Create a gzip cache file for every file that a webserver would\n    reasonably want to cache."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a gzipped file in the same directory with a filepath. gz name.", "response": "def create_gzip_file(filepath, overwrite):\n    '''Create a gzipped file in the same directory with a filepath.gz name.\n\n    :param filepath: A file to compress\n    :param overwrite: Whether the original file should be overwritten\n    '''\n    compressed_path = filepath + '.gz'\n\n    with open(filepath, 'rb') as uncompressed:\n        gzip_compress_obj = zlib.compressobj(COMPRESSION_LEVEL,\n                                                zlib.DEFLATED, WBITS)\n\n        uncompressed_data = uncompressed.read()\n        gzipped_data = gzip_compress_obj.compress(uncompressed_data)\n        gzipped_data += gzip_compress_obj.flush()\n\n        if len(gzipped_data) >= len(uncompressed_data):\n            logger.debug('No improvement: %s' % filepath)\n            return\n\n        with open(compressed_path, 'wb') as compressed:\n            logger.debug('Compressing: %s' % filepath)\n            try:\n                compressed.write(gzipped_data)\n            except Exception as ex:\n                logger.critical('Gzip compression failed: %s' % ex)\n\n        if overwrite:\n            logger.debug('Overwriting: %s with %s' % (filepath, compressed_path))\n            os.remove(filepath)\n            os.rename(compressed_path, filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nseparate out <div class = math > from the parent tag of <p >. Anything in between is put into its own parent tag of <p >.", "response": "def correct_html(self, root, children, div_math, insert_idx, text):\n        \"\"\"Separates out <div class=\"math\"> from the parent tag <p>. Anything\n        in between is put into its own parent tag of <p>\"\"\"\n\n        current_idx = 0\n\n        for idx in div_math:\n            el = markdown.util.etree.Element('p')\n            el.text = text\n            el.extend(children[current_idx:idx])\n\n            # Test to ensure that empty <p> is not inserted  \n            if len(el) != 0 or (el.text and not el.text.isspace()):\n               root.insert(insert_idx, el)\n               insert_idx += 1\n\n            text = children[idx].tail\n            children[idx].tail = None\n            root.insert(insert_idx, children[idx])\n            insert_idx += 1\n            current_idx = idx+1\n\n        el = markdown.util.etree.Element('p')\n        el.text = text\n        el.extend(children[current_idx:])\n\n        if len(el) != 0 or (el.text and not el.text.isspace()):\n            root.insert(insert_idx, el)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsearch for <div class = math > that are children in <p > tags and corrects the invalid HTML that results", "response": "def run(self, root):\n        \"\"\"Searches for <div class=\"math\"> that are children in <p> tags and corrects\n        the invalid HTML that results\"\"\"\n\n        math_tag_class = self.pelican_mathjax_extension.getConfig('math_tag_class')\n\n        for parent in root:\n            div_math = []\n            children = list(parent)\n\n            for div in parent.findall('div'):\n                if div.get('class') == math_tag_class:\n                    div_math.append(children.index(div))\n\n            # Do not process further if no displayed math has been found\n            if not div_math:\n                continue\n\n            insert_idx = list(root).index(parent)\n            self.correct_html(root, children, div_math, insert_idx, parent.text) \n            root.remove(parent)  # Parent must be removed last for correct insertion index\n\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_commits(self):\n        '''\n        Get all commits involving this filename\n        :returns: List of commits newest to oldest\n        '''\n        if not self.is_managed_by_git():\n            return []\n        return self.git.get_commits(self.content.source_path, self.follow)", "response": "Get all commits involving this filename\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting oldest commit involving this file", "response": "def get_oldest_commit(self):\n        '''\n        Get oldest commit involving this file\n\n        :returns: Oldest commit\n        '''\n        return self.git.get_commits(self.content.source_path, self.follow)[-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets oldest commit involving this file", "response": "def get_newest_commit(self):\n        '''\n        Get oldest commit involving this file\n\n        :returns: Newest commit\n        '''\n        return self.git.get_commits(self.content.source_path, follow=False)[0]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the original filename of this content. Implies follow", "response": "def get_oldest_filename(self):\n        '''\n        Get the original filename of this content. Implies follow\n        '''\n        commit_and_name_iter = self.git.get_commits_and_names_iter(\n            self.content.source_path)\n        _commit, name = next(commit_and_name_iter)\n        return name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_oldest_commit_date(self):\n        '''\n        Get datetime of oldest commit involving this file\n\n        :returns: Datetime of oldest commit\n        '''\n        oldest_commit = self.get_oldest_commit()\n        return self.git.get_commit_date(oldest_commit, self.tz_name)", "response": "Get datetime of oldest commit involving this file\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets datetime of newest commit involving this file", "response": "def get_newest_commit_date(self):\n        '''\n        Get datetime of newest commit involving this file\n\n        :returns: Datetime of newest commit\n        '''\n        newest_commit = self.get_newest_commit()\n        return self.git.get_commit_date(newest_commit, self.tz_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _separate_header_and_content(self, text_lines):\n        no_more_header = False\n        expr_metadata = re.compile(r'^#\\+[a-zA-Z]+:.*')\n        header = []\n        content = []\n        for line in text_lines:\n            metadata = expr_metadata.match(line)\n            if metadata and not no_more_header:\n                header.append(line)\n            else:\n                no_more_header = True\n                content.append(line)\n        return header, content", "response": "This function returns the header and content from a given list of text lines."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing the metadatas from a given org text.", "response": "def _parse_metadatas(self, text_lines):\n        \"\"\"\n        From a given Org text, return the metadatas \n        Keyword Arguments:\n        text_lines -- A list, each item is a line of the texte\n        Return:\n        A dict containing metadatas\n        \"\"\"\n        if not text_lines:\n            return {}\n        expr_metadata = re.compile(r'^#\\+([a-zA-Z]+):(.*)')\n        return {\n            expr_metadata.match(line).group(1).lower()\n            : expr_metadata.match(line).group(2).strip()\n            for line in text_lines\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(self, source_path):\n        with pelican_open(source_path) as text:\n            text_lines = list(text.splitlines())\n\n        header, content = self._separate_header_and_content(text_lines)\n        metadatas = self._parse_metadatas(header)\n        metadatas_processed = {\n            key\n            : self.process_metadata(key, value)\n            for key, value in metadatas.items()\n        }\n        content_html = convert_html(\"\\n\".join(content),\n                                    highlight=self.code_highlight)\n        return content_html, metadatas_processed", "response": "Parse the contents of the file source_path and return the html and metadata as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef glyph_role(name, rawtext, text, lineno, inliner,\n               options={}, content=[]):\n    \"\"\"\n        This function defines a glyph inline role that show a glyph icon from the \n        twitter bootstrap framework\n\n        *Usage:*\n\n            :glyph:`<glyph_name>`\n\n        *Example:*\n\n            Love this music :glyph:`music` :)\n\n        Can be subclassed to include a target\n\n        *Example:*\n\n            .. role:: story_time_glyph(glyph)\n                :target: http://www.youtube.com/watch?v=5g8ykQLYnX0\n                :class: small text-info\n\n            Love this music :story_time_glyph:`music` :)\n\n    \"\"\"\n\n    target = options.get('target', None)\n    glyph_name = 'glyphicon-{}'.format(text)\n\n    if target:\n        target = utils.unescape(target)\n        new_element = nodes.reference(rawtext, ' ', refuri=target)\n    else:\n        new_element = nodes.container()\n    classes = options.setdefault('class', [])\n    classes += ['glyphicon', glyph_name]\n    for custom_class in classes:\n        new_element.set_class(custom_class)\n    return [new_element], []", "response": "This function defines a glyph inline role that shows a glyph icon from the \n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprocess the metadata dict and return a dictionary of the keys and their values.", "response": "def _parse_metadata(self, meta):\n        \"\"\"Process the metadata dict, lowercasing the keys and textilizing the\nvalue of the 'summary' key (if present).  Keys that share the same\nlowercased form will be overridden in some arbitrary order.\n\n        \"\"\"\n        output = {}\n        for name, value in meta.items():\n            name = name.lower()\n            if name == \"summary\":\n                value = textile(value)\n            output[name] = self.process_metadata(name, value)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses content and metadata of textile files.", "response": "def read(self, source_path):\n        \"\"\"Parse content and metadata of textile files.\"\"\"\n\n        with pelican_open(source_path) as text:\n            parts = text.split('----', 1)\n            if len(parts) == 2:\n                headerlines = parts[0].splitlines()\n                headerpairs = map(lambda l: l.split(':', 1), headerlines)\n                headerdict = {pair[0]: pair[1].strip()\n                              for pair in headerpairs\n                              if len(pair) == 2}\n                metadata = self._parse_metadata(headerdict)\n                content = textile(parts[1])\n            else:\n                metadata = {}\n                content = textile(text)\n\n        return content, metadata"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\noptimize jpg and png images", "response": "def optimize_images(pelican):\n    \"\"\"\n    Optimized jpg and png images\n\n    :param pelican: The Pelican instance\n    \"\"\"\n    for dirpath, _, filenames in os.walk(pelican.settings['OUTPUT_PATH']):\n        for name in filenames:\n            if os.path.splitext(name)[1] in COMMANDS.keys():\n                optimize(dirpath, name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\noptimize a file in a directory.", "response": "def optimize(dirpath, filename):\n    \"\"\"\n    Check if the name is a type of file that should be optimized.\n    And optimizes it if required.\n\n    :param dirpath: Path of the file to be optimzed\n    :param name: A file name to be optimized\n    \"\"\"\n    filepath = os.path.join(dirpath, filename)\n    logger.info('optimizing %s', filepath)\n\n    ext = os.path.splitext(filename)[1]\n    command, silent, verbose = COMMANDS[ext]\n    flags = verbose if SHOW_OUTPUT else silent\n    command = command.format(filename=filepath, flags=flags)\n    call(command, shell=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the Libravatar URL to the article s metadata.", "response": "def add_libravatar (generator, metadata):\n    \"\"\"Article generator connector for the Libravatar plugin\"\"\"\n    missing = generator.settings.get ('LIBRAVATAR_MISSING')\n    size = generator.settings.get ('LIBRAVATAR_SIZE')\n\n    ## Check the presence of the Email header\n    if 'email' not in metadata.keys ():\n        try:\n            metadata ['email'] = generator.settings.get ('AUTHOR_EMAIL')\n        except:\n            pass\n\n    ## Add the Libravatar URL\n    if metadata ['email']:\n\n        ## Compose URL using the MD5 hash\n        ## (the ascii encoding is necessary for Python3)\n        email = metadata ['email'].lower ().encode ('ascii')\n        md5 = hashlib.md5 (email).hexdigest ()\n        url = 'http://cdn.libravatar.org/avatar/' + md5\n\n        ## Add eventual \"missing picture\" option\n        if missing or size:\n            url = url + '?'\n            if missing:\n                url = url + 'd=' + missing\n                if size:\n                    url = url + '&'\n            if size:\n                url = url + 's=' + str (size)\n\n        ## Add URL to the article's metadata\n        metadata ['author_libravatar'] = url"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate diagramm and return data", "response": "def get_diag(code, command):\n    \"\"\" Generate diagramm and return data \"\"\"\n    import tempfile\n    import shutil\n    code = code + u'\\n'\n\n    try:\n        tmpdir = tempfile.mkdtemp()\n        fd, diag_name = tempfile.mkstemp(dir=tmpdir)\n\n        f = os.fdopen(fd, \"w\")\n        f.write(code.encode('utf-8'))\n        f.close()\n\n        format = _draw_mode.lower()\n        draw_name = diag_name + '.' + format\n\n        saved_argv = sys.argv\n        argv = [diag_name, '-T', format, '-o', draw_name]\n\n        if _draw_mode == 'SVG':\n            argv += ['--ignore-pil']\n\n        # Run command\n        command.main(argv)\n\n        # Read image data from file\n        file_name = diag_name + '.' + _publish_mode.lower()\n\n        with io.open(file_name, 'rb') as f:\n            data = f.read()\n            f.close()\n\n    finally:\n        for file in os.listdir(tmpdir):\n            os.unlink(tmpdir + \"/\" + file)\n\n        # os.rmdir will fail -> use shutil\n        shutil.rmtree(tmpdir)\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the blockdiag output.", "response": "def blockdiag_parser(preprocessor, tag, markup):\n    \"\"\" Blockdiag parser \"\"\"\n    m = DOT_BLOCK_RE.search(markup)\n    if m:\n        # Get diagram type and code\n        diagram = m.group('diagram').strip()\n        code = markup\n\n        # Run command\n        output = diag(code, diagram)\n\n        if output:\n            # Return Base64 encoded image\n            return '<span class=\"blockdiag\" style=\"align: center;\"><img src=\"data:image/png;base64,%s\"></span>' % base64.b64encode(output)\n    else:\n        raise ValueError('Error processing input. '\n                         'Expected syntax: {0}'.format(SYNTAX))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fetch_github_activity(gen, metadata):\n\n    if 'GITHUB_ACTIVITY_FEED' in gen.settings.keys():\n        gen.context['github_activity'] = gen.plugin_instance.fetch()", "response": "Fetch the github activity from the generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef register():\n    try:\n        signals.article_generator_init.connect(feed_parser_initialization)\n        signals.article_generator_context.connect(fetch_github_activity)\n    except ImportError:\n        logger.warning('`github_activity` failed to load dependency `feedparser`.'\n                       '`github_activity` plugin not loaded.')", "response": "Register the plugin with the signals."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fetch(self):\n\n        entries = []\n        for activity in self.activities['entries']:\n            entries.append(\n                    [element for element in [activity['title'],\n                        activity['content'][0]['value']]])\n\n        return entries[0:self.max_entries]", "response": "returns a list of html snippets fetched from github actitivy feed"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, source_path):\n        logger.debug(\"Loading graph described in \"+source_path)\n        graph = rdflib.Graph()\n        graph.load(source_path)\n        meta = {}\n        queries = [\n            f for f in listdir(self.settings[\"VOC_QUERIES_PATH\"])\n            if (isfile(join(self.settings[\"VOC_QUERIES_PATH\"], f)) \n                and f.endswith(\".sparql\"))]\n        for query_path in queries:\n            query_file_path = self.settings[\"VOC_QUERIES_PATH\"]+\"/\"+query_path\n            with open(query_file_path, \"r\") as query_file:\n                query = query_file.read()\n\n                # The name of the query identifies the elements in the context\n                query_key=query_path.split(\".\")[0]\n                result_set = graph.query(query)\n                # Each query result will be stored as a dictionnary in the\n                # vocabulary context, referenced by the query name as its key.\n                # Multiple results are stored in a list.\n                for result in result_set:\n                    if not query_key in meta.keys():\n                        meta[query_key]=result.asdict()\n                    elif type(meta[query_key]) == list:\n                        meta[query_key].append(result.asdict())\n                    else:\n                        meta[query_key]=[meta[query_key], result.asdict()]\n        meta[\"iri\"] = meta[\"lov_metadata\"][\"iri\"]\n        meta[\"description\"] = meta[\"lov_metadata\"][\"description\"]\n        meta[\"version\"] = meta[\"lov_metadata\"][\"version\"]\n        meta[\"title\"] = meta[\"lov_metadata\"][\"title\"]\n        return \"\", meta", "response": "Parse content and metadata of an rdf file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef magic_set(obj):\n    def decorator(func):\n        is_class = isinstance(obj, six.class_types)\n        args, varargs, varkw, defaults = inspect.getargspec(func)\n        if not args or args[0] not in ('self', 'cls', 'klass'):\n            # Static function/method\n            if is_class:\n                replacement = staticmethod(func)\n            else:\n                replacement = func\n        elif args[0] == 'self':\n            if is_class:\n                replacement = func\n            else:\n                def replacement(*args, **kw):\n                    return func(obj, *args, **kw)\n                try:\n                    replacement.__name__ = func.__name__\n                except:\n                    pass\n        else:\n            if is_class:\n                replacement = classmethod(func)\n            else:\n                def replacement(*args, **kw):\n                    return func(obj.__class__, *args, **kw)\n                try:\n                    replacement.__name__ = func.__name__\n                except:\n                    pass\n        setattr(obj, func.__name__, replacement)\n        return replacement\n    return decorator", "response": "A function that adds a function or method to an object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_file(src):\n    try:\n        if '://' in src or src[0:2] == '//':  # Most likely this is remote file\n            response = urllib2.urlopen(src)\n            return response.read()\n        else:\n            with open(src, 'rb') as fh:\n                return fh.read()\n    except Exception as e:\n        raise RuntimeError('Error generating base64image: {}'.format(e))", "response": "Return content from local file or remote file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef minify(pelican):\n    executable = pelican.settings.get('YUICOMPRESSOR_EXECUTABLE', 'yuicompressor')\n    for dirpath, _, filenames in os.walk(pelican.settings['OUTPUT_PATH']):\n        for name in filenames:\n            if os.path.splitext(name)[1] in ('.css','.js'):\n                filepath = os.path.join(dirpath, name)\n                logger.info('minify %s', filepath)\n                check_call([executable, '--charset', 'utf-8', filepath, '-o', filepath])", "response": "Minifies CSS and JS with YUI compressor."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef insert_into_last_element(html, element):\n    try:\n        item = fragment_fromstring(element)\n    except (ParserError, TypeError) as e:\n        item = fragment_fromstring('<span></span>')\n\n    try:\n        doc = fragments_fromstring(html)\n        doc[-1].append(item)\n\n        return ''.join(tostring(e) for e in doc)\n    except (ParserError, TypeError) as e:\n        return ''", "response": "function to insert an html element into another html fragment"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ninserts an inline read more link into the last element of the summary .", "response": "def insert_read_more_link(instance):\n    \"\"\"\n    Insert an inline \"read more\" link into the last element of the summary\n    :param instance:\n    :return:\n    \"\"\"\n\n    # only deals with Article type\n    if type(instance) != contents.Article: return\n\n\n    SUMMARY_MAX_LENGTH = instance.settings.get('SUMMARY_MAX_LENGTH')\n    READ_MORE_LINK = instance.settings.get('READ_MORE_LINK', None)\n    READ_MORE_LINK_FORMAT = instance.settings.get('READ_MORE_LINK_FORMAT',\n                                                  '<a class=\"read-more\" href=\"/{url}\">{text}</a>')\n\n    if not (SUMMARY_MAX_LENGTH and READ_MORE_LINK and READ_MORE_LINK_FORMAT): return\n\n    if hasattr(instance, '_summary') and instance._summary:\n        summary = instance._summary\n    else:\n        summary = truncate_html_words(instance.content, SUMMARY_MAX_LENGTH)\n\n    if summary != instance.content:\n        read_more_link = READ_MORE_LINK_FORMAT.format(url=instance.url, text=READ_MORE_LINK)\n        instance._summary = insert_into_last_element(summary, read_more_link)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating a generated HTML file.", "response": "def validate_files(pelican):\n    \"\"\"\n    Validate a generated HTML file\n    :param pelican: pelican object\n    \"\"\"\n    for dirpath, _, filenames in os.walk(pelican.settings['OUTPUT_PATH']):\n        for name in filenames:\n            if should_validate(name):\n                filepath = os.path.join(dirpath, name)\n                validate(filepath)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(filename):\n    import HTMLParser\n    from py_w3c.validators.html.validator import HTMLValidator\n\n    h = HTMLParser.HTMLParser()  # for unescaping WC3 messages\n\n    vld = HTMLValidator()\n    LOG.info(\"Validating: {0}\".format(filename))\n\n    # call w3c webservice\n    vld.validate_file(filename)\n\n    # display errors and warning\n    for err in vld.errors:\n        LOG.error(u'line: {0}; col: {1}; message: {2}'.\n                  format(err['line'], err['col'], h.unescape(err['message']))\n                  )\n    for err in vld.warnings:\n        LOG.warning(u'line: {0}; col: {1}; message: {2}'.\n                    format(err['line'], err['col'], h.unescape(err['message']))\n                    )", "response": "Validate a file using W3C validation service."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget appropriate wrapper factory and cache instance for path", "response": "def git_wrapper(path):\n    '''\n    Get appropriate wrapper factory and cache instance for path\n    '''\n    path = os.path.abspath(path)\n    if path not in _wrapper_cache:\n        if hasattr(Repo, 'commits'):\n            _wrapper_cache[path] = _GitWrapperLegacy(path)\n        else:\n            _wrapper_cache[path] = _GitWrapper(path)\n\n    return _wrapper_cache[path]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_file_managed_by_git(self, path):\n        '''\n        :param path: Path to check\n        :returns: True if path is managed by git\n        '''\n        status, _stdout, _stderr = self.git.execute(\n            ['git', 'ls-files', path, '--error-unmatch'],\n            with_extended_output=True,\n            with_exceptions=False)\n        return status == 0", "response": "Check if a file is managed by git"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_commits_and_names_iter(self, path):\n        '''\n        Get all commits including a given path following renames\n        '''\n        log_result = self.git.log(\n            '--pretty=%H',\n            '--follow',\n            '--name-only',\n            '--',\n            path).splitlines()\n\n        for commit_sha, _, filename in grouper(log_result, 3):\n            yield self.repo.commit(commit_sha), filename", "response": "Get all commits and names of a given path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_commits(self, path, follow=False):\n        '''\n        Get all commits including path\n\n        :param path: Path which we will find commits for\n        :param bool follow: If True we will follow path through renames\n\n        :returns: Sequence of commit objects. Newest to oldest\n        '''\n        if follow:\n            return self.get_commits_following(path)\n        else:\n            return self._get_commits(path)", "response": "Get all commits including path"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_commit_date(commit, tz_name):\n        '''\n        Get datetime of commit comitted_date\n        '''\n        return set_date_tzinfo(\n            datetime.fromtimestamp(mktime(commit.committed_date)),\n            tz_name=tz_name)", "response": "Get datetime of commit comitted_date"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nenables code to run in a context with a temporary locale", "response": "def temporary_locale(temp_locale=None):\n    '''Enable code to run in a context with a temporary locale\n\n    Resets the locale back when exiting context.\n    Can set a temporary locale if provided\n    '''\n    orig_locale = locale.setlocale(locale.LC_ALL)\n    if temp_locale is not None:\n        locale.setlocale(locale.LC_ALL, temp_locale)\n    yield\n    locale.setlocale(locale.LC_ALL, orig_locale)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef initialize_dbs(settings):\n    '''Initialize internal DBs using the Pelican settings dict\n\n    This clears the DBs for e.g. autoreload mode to work\n    '''\n    global _MAIN_SETTINGS, _MAIN_SITEURL, _MAIN_LANG, _SUBSITE_QUEUE\n    _MAIN_SETTINGS = settings\n    _MAIN_LANG = settings['DEFAULT_LANG']\n    _MAIN_SITEURL = settings['SITEURL']\n    _SUBSITE_QUEUE = settings.get('I18N_SUBSITES', {}).copy()\n    prepare_site_db_and_overrides()\n    # clear databases in case of autoreload mode\n    _SITES_RELPATH_DB.clear()\n    _NATIVE_CONTENT_URL_DB.clear()\n    _GENERATOR_DB.clear()", "response": "Initialize internal DBs using the Pelican settings dict\n    This clears the internal DBs for e. g. autoreload mode to work\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prepare_site_db_and_overrides():\n    '''Prepare overrides and create _SITE_DB\n\n    _SITE_DB.keys() need to be ready for filter_translations\n    '''\n    _SITE_DB.clear()\n    _SITE_DB[_MAIN_LANG] = _MAIN_SITEURL\n    # make sure it works for both root-relative and absolute\n    main_siteurl = '/' if _MAIN_SITEURL == '' else _MAIN_SITEURL\n    for lang, overrides in _SUBSITE_QUEUE.items():\n        if 'SITEURL' not in overrides:\n            overrides['SITEURL'] = posixpath.join(main_siteurl, lang)\n        _SITE_DB[lang] = overrides['SITEURL']\n        # default subsite hierarchy\n        if 'OUTPUT_PATH' not in overrides:\n            overrides['OUTPUT_PATH'] = os.path.join(\n                _MAIN_SETTINGS['OUTPUT_PATH'], lang)\n        if 'CACHE_PATH' not in overrides:\n            overrides['CACHE_PATH'] = os.path.join(\n                _MAIN_SETTINGS['CACHE_PATH'], lang)\n        if 'STATIC_PATHS' not in overrides:\n            overrides['STATIC_PATHS'] = []\n        if ('THEME' not in overrides and 'THEME_STATIC_DIR' not in overrides and\n                'THEME_STATIC_PATHS' not in overrides):\n            relpath = relpath_to_site(lang, _MAIN_LANG)\n            overrides['THEME_STATIC_DIR'] = posixpath.join(\n                relpath, _MAIN_SETTINGS['THEME_STATIC_DIR'])\n            overrides['THEME_STATIC_PATHS'] = []\n        # to change what is perceived as translations\n        overrides['DEFAULT_LANG'] = lang", "response": "Prepare overrides and create _SITE_DB and _SUBSITE_QUEUE."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef relpath_to_site(lang, target_lang):\n    '''Get relative path from siteurl of lang to siteurl of base_lang\n\n    the output is cached in _SITES_RELPATH_DB\n    '''\n    path = _SITES_RELPATH_DB.get((lang, target_lang), None)\n    if path is None:\n        siteurl = _SITE_DB.get(lang, _MAIN_SITEURL)\n        target_siteurl = _SITE_DB.get(target_lang, _MAIN_SITEURL)\n        path = posixpath.relpath(get_site_path(target_siteurl),\n                                 get_site_path(siteurl))\n        _SITES_RELPATH_DB[(lang, target_lang)] = path\n    return path", "response": "Get relative path from siteurl of lang to siteurl of target_lang"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform an Article to Draft", "response": "def article2draft(article):\n    '''Transform an Article to Draft'''\n    draft = Draft(article._content, article.metadata, article.settings,\n                  article.source_path, article._context)\n    draft.status = 'draft'\n    return draft"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter the content and translations lists of a generator by removing any translations which are not in the same language as the currently - generated site.", "response": "def filter_contents_translations(generator):\n    '''Filter the content and translations lists of a generator\n\n    Filters out\n        1) translations which will be generated in a different site\n        2) content that is not in the language of the currently\n        generated site but in that of a different site, content in a\n        language which has no site is generated always. The filtering\n        method bay be modified by the respective untranslated policy\n    '''\n    inspector = GeneratorInspector(generator)\n    current_lang = generator.settings['DEFAULT_LANG']\n    langs_with_sites = _SITE_DB.keys()\n    removed_contents = _GENERATOR_DB[generator]\n\n    for translations in inspector.translations_lists():\n        for translation in translations[:]:    # copy to be able to remove\n            if translation.lang in langs_with_sites:\n                translations.remove(translation)\n                removed_contents.append(translation)\n\n    hiding_func = inspector.hiding_function()\n    untrans_policy = inspector.untranslated_policy(default='hide')\n    for (contents, other_contents) in inspector.contents_list_pairs():\n        for content in other_contents: # save any hidden native content first\n            if content.lang == current_lang: # in native lang\n                # save the native URL attr formatted in the current locale\n                _NATIVE_CONTENT_URL_DB[content.source_path] = content.url\n        for content in contents[:]:        # copy for removing in loop\n            if content.lang == current_lang: # in native lang\n                # save the native URL attr formatted in the current locale\n                _NATIVE_CONTENT_URL_DB[content.source_path] = content.url\n            elif content.lang in langs_with_sites and untrans_policy != 'keep':\n                contents.remove(content)\n                if untrans_policy == 'hide':\n                    other_contents.append(hiding_func(content))\n                elif untrans_policy == 'remove':\n                    removed_contents.append(content)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef install_templates_translations(generator):\n    '''Install gettext translations in the jinja2.Environment\n\n    Only if the 'jinja2.ext.i18n' jinja2 extension is enabled\n    the translations for the current DEFAULT_LANG are installed.\n    '''\n    if 'JINJA_ENVIRONMENT' in generator.settings: # pelican 3.7+\n        jinja_extensions = generator.settings['JINJA_ENVIRONMENT'].get(\n            'extensions', [])\n    else:\n        jinja_extensions = generator.settings['JINJA_EXTENSIONS']\n\n    if 'jinja2.ext.i18n' in jinja_extensions:\n        domain = generator.settings.get('I18N_GETTEXT_DOMAIN', 'messages')\n        localedir = generator.settings.get('I18N_GETTEXT_LOCALEDIR')\n        if localedir is None:\n            localedir = os.path.join(generator.theme, 'translations')\n        current_lang = generator.settings['DEFAULT_LANG']\n        if current_lang == generator.settings.get('I18N_TEMPLATES_LANG',\n                                                  _MAIN_LANG):\n            translations = gettext.NullTranslations()\n        else:\n            langs = [current_lang]\n            try:\n                translations = gettext.translation(domain, localedir, langs)\n            except (IOError, OSError):\n                _LOGGER.error((\n                    \"Cannot find translations for language '{}' in '{}' with \"\n                    \"domain '{}'. Installing NullTranslations.\").format(\n                        langs[0], localedir, domain))\n                translations = gettext.NullTranslations()\n        newstyle = generator.settings.get('I18N_GETTEXT_NEWSTYLE', True)\n        generator.env.install_gettext_translations(translations, newstyle)", "response": "Install gettext translations in the jinja2. Environment\nauraurar Only if the JINJA_ENVIRONMENT is enabled\naurar and the JINJA_EXTENSIONS is enabled\naurar Only if the JINJA_ENVIRONMENT is enabled\naurar and the JINJA_EXTENSIONS is enabled\naurar Only if the JINJA_ENVIRONMENT is enabled\naurar and the JINJA_EXTENSIONS is enabled\n"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_variables_to_context(generator):\n    '''Adds useful iterable variables to template context'''\n    context = generator.context             # minimize attr lookup\n    context['relpath_to_site'] = relpath_to_site\n    context['main_siteurl'] = _MAIN_SITEURL\n    context['main_lang'] = _MAIN_LANG\n    context['lang_siteurls'] = _SITE_DB\n    current_lang = generator.settings['DEFAULT_LANG']\n    extra_siteurls = _SITE_DB.copy()\n    extra_siteurls.pop(current_lang)\n    context['extra_siteurls'] = extra_siteurls", "response": "Adds useful iterable variables to template context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlinking content to translations in their main language so the URL of the different subsites will be honored", "response": "def interlink_translations(content):\n    '''Link content to translations in their main language\n\n    so the URL (including localized month names) of the different subsites\n    will be honored\n    '''\n    lang = content.lang\n    # sort translations by lang\n    content.translations.sort(key=attrgetter('lang'))\n    for translation in content.translations:\n        relpath = relpath_to_site(lang, translation.lang)\n        url = _NATIVE_CONTENT_URL_DB[translation.source_path]\n        translation.override_url = posixpath.join(relpath, url)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interlink_translated_content(generator):\n    '''Make translations link to the native locations\n\n    for generators that may contain translated content\n    '''\n    inspector = GeneratorInspector(generator)\n    for content in inspector.all_contents():\n        interlink_translations(content)", "response": "Make translations link to the native locations\n    for generators that may contain translated content\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef interlink_removed_content(generator):\n    '''For all contents removed from generation queue update interlinks\n\n    link to the native location\n    '''\n    current_lang = generator.settings['DEFAULT_LANG']\n    for content in _GENERATOR_DB[generator]:\n        url = _NATIVE_CONTENT_URL_DB[content.source_path]\n        relpath = relpath_to_site(current_lang, content.lang)\n        content.override_url = posixpath.join(relpath, url)", "response": "For all contents removed from generation queue update interlinks\n\n    link to the native location"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds links to static files in the main site if necessary", "response": "def interlink_static_files(generator):\n    '''Add links to static files in the main site if necessary'''\n    if generator.settings['STATIC_PATHS'] != []:\n        return                               # customized STATIC_PATHS\n    try: # minimize attr lookup\n        static_content = generator.context['static_content']\n    except KeyError:\n        static_content = generator.context['filenames']\n    relpath = relpath_to_site(generator.settings['DEFAULT_LANG'], _MAIN_LANG)\n    for staticfile in _MAIN_STATIC_FILES:\n        if staticfile.get_relative_source_path() not in static_content:\n            staticfile = copy(staticfile) # prevent override in main site\n            staticfile.override_url = posixpath.join(relpath, staticfile.url)\n            try:\n                generator.add_source_path(staticfile, static=True)\n            except TypeError:\n                generator.add_source_path(staticfile)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the context of all generators with Ads useful variables and translations into the template context and interlink translations", "response": "def update_generators():\n    '''Update the context of all generators\n\n    Ads useful variables and translations into the template context\n    and interlink translations\n    '''\n    for generator in _GENERATOR_DB.keys():\n        install_templates_translations(generator)\n        add_variables_to_context(generator)\n        interlink_static_files(generator)\n        interlink_removed_content(generator)\n        interlink_translated_content(generator)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the Pelican class requested in settings", "response": "def get_pelican_cls(settings):\n    '''Get the Pelican class requested in settings'''\n    cls = settings['PELICAN_CLASS']\n    if isinstance(cls, six.string_types):\n        module, cls_name = cls.rsplit('.', 1)\n        module = __import__(module)\n        cls = getattr(module, cls_name)\n    return cls"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the next subsite in the queue.", "response": "def create_next_subsite(pelican_obj):\n    '''Create the next subsite using the lang-specific config\n\n    If there are no more subsites in the generation queue, update all\n    the generators (interlink translations and removed content, add\n    variables and translations to template context). Otherwise get the\n    language and overrides for next the subsite in the queue and apply\n    overrides.  Then generate the subsite using a PELICAN_CLASS\n    instance and its run method. Finally, restore the previous locale.\n    '''\n    global _MAIN_SETTINGS\n    if len(_SUBSITE_QUEUE) == 0:\n        _LOGGER.debug(\n            'i18n: Updating cross-site links and context of all generators.')\n        update_generators()\n        _MAIN_SETTINGS = None             # to initialize next time\n    else:\n        with temporary_locale():\n            settings = _MAIN_SETTINGS.copy()\n            lang, overrides = _SUBSITE_QUEUE.popitem()\n            settings.update(overrides)\n            settings = configure_settings(settings)      # to set LOCALE, etc.\n            cls = get_pelican_cls(settings)\n\n            new_pelican_obj = cls(settings)\n            _LOGGER.debug((\"Generating i18n subsite for language '{}' \"\n                           \"using class {}\").format(lang, cls))\n            new_pelican_obj.run()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering the plugin only if required signals are available.", "response": "def register():\n    '''Register the plugin only if required signals are available'''\n    for sig_name in _SIGNAL_HANDLERS_DB.keys():\n        if not hasattr(signals, sig_name):\n            _LOGGER.error((\n                'The i18n_subsites plugin requires the {} '\n                'signal available for sure in Pelican 3.4.0 and later, '\n                'plugin will not be used.').format(sig_name))\n            return\n\n    for sig_name, handler in _SIGNAL_HANDLERS_DB.items():\n        sig = getattr(signals, sig_name)\n        sig.connect(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef translations_lists(self):\n        '''Iterator over lists of content translations'''\n        return (getattr(self.generator, name) for name in\n                self.info.get('translations_lists', []))", "response": "Iterator over lists of content translations"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef contents_list_pairs(self):\n        '''Iterator over pairs of normal and hidden contents'''\n        return (tuple(getattr(self.generator, name) for name in names)\n                for names in self.info.get('contents_lists', []))", "response": "Iterator over pairs of normal and hidden contents"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the policy for untranslated content", "response": "def untranslated_policy(self, default):\n        '''Get the policy for untranslated content'''\n        return self.generator.settings.get(self.info.get('policy', None),\n                                           default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nincludes a file as part of the content of this reST file.", "response": "def run(self):\n        \"\"\"Include a file as part of the content of this reST file.\"\"\"\n        if not self.state.document.settings.file_insertion_enabled:\n            raise self.warning('\"%s\" directive disabled.' % self.name)\n        source = self.state_machine.input_lines.source(\n            self.lineno - self.state_machine.input_offset - 1)\n        source_dir = os.path.dirname(os.path.abspath(source))\n\n        path = directives.path(self.arguments[0])\n        path = os.path.normpath(os.path.join(source_dir, path))\n        path = utils.relative_path(None, path)\n        path = nodes.reprunicode(path)\n\n        encoding = self.options.get(\n            'encoding', self.state.document.settings.input_encoding)\n        e_handler = self.state.document.settings.input_encoding_error_handler\n        tab_width = self.options.get(\n            'tab-width', self.state.document.settings.tab_width)\n\n        try:\n            self.state.document.settings.record_dependencies.add(path)\n            include_file = io.FileInput(source_path=path,\n                                        encoding=encoding,\n                                        error_handler=e_handler)\n        except UnicodeEncodeError as error:\n            raise self.severe('Problems with \"%s\" directive path:\\n'\n                              'Cannot encode input file path \"%s\" '\n                              '(wrong locale?).' %\n                              (self.name, SafeString(path)))\n        except IOError as error:\n            raise self.severe('Problems with \"%s\" directive path:\\n%s.' %\n                              (self.name, ErrorString(error)))\n        startline = self.options.get('start-line', None)\n        endline = self.options.get('end-line', None)\n        try:\n            if startline or (endline is not None):\n                lines = include_file.readlines()\n                rawtext = ''.join(lines[startline:endline])\n            else:\n                rawtext = include_file.read()\n        except UnicodeError as error:\n            raise self.severe('Problem with \"%s\" directive:\\n%s' %\n                              (self.name, ErrorString(error)))\n\n        include_lines = statemachine.string2lines(rawtext, tab_width,\n                                                  convert_whitespace=True)\n\n        # default lexer to 'text'\n        lexer = self.options.get('lexer', 'text')\n\n        self.options['source'] = path\n        codeblock = Pygments(self.name,\n                             [lexer],  # arguments\n                             {},  # no options for this directive\n                             include_lines,  # content\n                             self.lineno,\n                             self.content_offset,\n                             self.block_text,\n                             self.state,\n                             self.state_machine)\n        return codeblock.run()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(self, filename):\n        QUIET = self.settings.get('RMD_READER_KNITR_QUIET', True)\n        ENCODING = self.settings.get('RMD_READER_KNITR_ENCODING', 'UTF-8')\n        CLEANUP = self.settings.get('RMD_READER_CLEANUP', True)\n        RENAME_PLOT = self.settings.get('RMD_READER_RENAME_PLOT', 'chunklabel')\n        if type(RENAME_PLOT) is bool:\n            logger.error(\"RMD_READER_RENAME_PLOT takes a string value (either chunklabel or directory), please see the readme.\")\n            if RENAME_PLOT:\n                RENAME_PLOT = 'chunklabel'\n                logger.error(\"Defaulting to chunklabel\")\n            else:\n                RENAME_PLOT = 'disabled'\n                logger.error(\"Disabling plot renaming\")\n        logger.debug(\"RMD_READER_KNITR_QUIET = %s\", QUIET)\n        logger.debug(\"RMD_READER_KNITR_ENCODING = %s\", ENCODING)\n        logger.debug(\"RMD_READER_CLEANUP = %s\", CLEANUP)\n        logger.debug(\"RMD_READER_RENAME_PLOT = %s\", RENAME_PLOT)\n        # replace single backslashes with double backslashes\n        filename = filename.replace('\\\\', '\\\\\\\\')\n        # parse Rmd file - generate md file\n        md_filename = filename.replace('.Rmd', '.aux').replace('.rmd', '.aux')\n        if RENAME_PLOT == 'chunklabel' or RENAME_PLOT == 'directory':\n            if RENAME_PLOT == 'chunklabel':\n                chunk_label = os.path.splitext(os.path.basename(filename))[0]\n                logger.debug('Chunk label: %s', chunk_label)\n            elif RENAME_PLOT == 'directory':\n                chunk_label = 'unnamed-chunk'\n                PATH = self.settings.get('PATH','%s/content' % settings.DEFAULT_CONFIG.get('PATH'))\n                src_name = os.path.splitext(os.path.relpath(filename, PATH))[0]\n                idx = KNITR.opts_chunk.names.index('set')\n                knitroptschunk = { 'fig.path': '%s-' % os.path.join(FIG_PATH, src_name) }\n                KNITR.opts_chunk[idx](**{str(k): v for k,v in knitroptschunk.items()})\n                logger.debug('Figures path: %s, chunk label: %s', knitroptschunk['fig.path'], chunk_label)\n            R_OBJECTS.r('''\nopts_knit$set(unnamed.chunk.label=\"{unnamed_chunk_label}\")\nrender_markdown()\nhook_plot <- knit_hooks$get('plot')\nknit_hooks$set(plot=function(x, options) hook_plot(paste0(\"{{static}}/\", x), options))\n            '''.format(unnamed_chunk_label=chunk_label))\n        with warnings.catch_warnings():\n            warnings.simplefilter(\"ignore\")\n            KNITR.knit(filename, md_filename, quiet=QUIET, encoding=ENCODING)\n        # read md file - create a MarkdownReader\n        md_reader = readers.MarkdownReader(self.settings)\n        content, metadata = md_reader.read(md_filename)\n        # remove md file\n        if CLEANUP:\n            os.remove(md_filename)\n        return content, metadata", "response": "Parse content and metadata of markdown files"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses a timestamp string in format YYYY - MM - DD HH : MM", "response": "def parse_tstamp(ev, field_name):\n    \"\"\"Parse a timestamp string in format \"YYYY-MM-DD HH:MM\"\n\n    :returns: datetime\n    \"\"\"\n    try:\n        return datetime.strptime(ev[field_name], '%Y-%m-%d %H:%M')\n    except Exception as e:\n        log.error(\"Unable to parse the '%s' field in the event named '%s': %s\" \\\n            % (field_name, ev['title'], e))\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_timedelta(ev):\n\n    chunks = ev['event-duration'].split()\n    tdargs = {}\n    for c in chunks:\n        try:\n            m = TIME_MULTIPLIERS[c[-1]]\n            val = int(c[:-1])\n            tdargs[m] = val\n        except KeyError:\n            log.error(\"\"\"Unknown time multiplier '%s' value in the \\\n'event-duration' field in the '%s' event. Supported multipliers \\\nare: '%s'.\"\"\" % (c, ev['title'], ' '.join(TIME_MULTIPLIERS)))\n            raise RuntimeError(\"Unknown time multiplier '%s'\" % c)\n        except ValueError:\n            log.error(\"\"\"Unable to parse '%s' value in the 'event-duration' \\\nfield in the '%s' event.\"\"\" % (c, ev['title']))\n            raise ValueError(\"Unable to parse '%s'\" % c)\n\n\n    return timedelta(**tdargs)", "response": "Parse a timedelta string in format [ <num><multiplier > 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e. g. 2h 30m\nWorkItem e."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the metadata for an event and add it to the calendar", "response": "def parse_article(generator, metadata):\n    \"\"\"Collect articles metadata to be used for building the event calendar\n\n    :returns: None\n    \"\"\"\n    if 'event-start' not in metadata:\n        return\n\n    dtstart = parse_tstamp(metadata, 'event-start')\n\n    if 'event-end' in metadata:\n        dtend = parse_tstamp(metadata, 'event-end')\n\n    elif 'event-duration' in metadata:\n        dtdelta = parse_timedelta(metadata)\n        dtend = dtstart + dtdelta\n\n    else:\n        msg = \"Either 'event-end' or 'event-duration' must be\" + \\\n            \" speciefied in the event named '%s'\" % metadata['title']\n        log.error(msg)\n        raise ValueError(msg)\n\n    events.append(Event(dtstart, dtend, metadata))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_ical_file(generator):\n    global events\n    ics_fname = generator.settings['PLUGIN_EVENTS']['ics_fname']\n    if not ics_fname:\n        return\n\n    ics_fname = os.path.join(generator.settings['OUTPUT_PATH'], ics_fname)\n    log.debug(\"Generating calendar at %s with %d events\" % (ics_fname, len(events)))\n\n    tz = generator.settings.get('TIMEZONE', 'UTC')\n    tz = pytz.timezone(tz)\n\n    ical = icalendar.Calendar()\n    ical.add('prodid', '-//My calendar product//mxm.dk//')\n    ical.add('version', '2.0')\n\n    DEFAULT_LANG = generator.settings['DEFAULT_LANG']\n    curr_events = events if not localized_events else localized_events[DEFAULT_LANG]\n\n    for e in curr_events:\n        ie = icalendar.Event(\n            summary=e.metadata['summary'],\n            dtstart=e.dtstart,\n            dtend=e.dtend,\n            dtstamp=e.metadata['date'],\n            priority=5,\n            uid=e.metadata['title'] + e.metadata['summary'],\n        )\n        if 'event-location' in e.metadata:\n            ie.add('location', e.metadata['event-location'])\n\n        ical.add_component(ie)\n\n    with open(ics_fname, 'wb') as f:\n        f.write(ical.to_ical())", "response": "Generate an iCalendar file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_localized_events(generator):\n\n    if \"i18n_subsites\" in generator.settings[\"PLUGINS\"]:\n        if not os.path.exists(generator.settings['OUTPUT_PATH']):\n            os.makedirs(generator.settings['OUTPUT_PATH'])\n\n        for e in events:\n            if \"lang\" in e.metadata:\n                localized_events[e.metadata[\"lang\"]].append(e)\n            else:\n                log.debug(\"event %s contains no lang attribute\" % (e.metadata[\"title\"],))", "response": "Generates a dictionary of localized events if i18n_subsites plugin is active"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npopulates the event_list variable to be used in jinja templates", "response": "def generate_events_list(generator):\n    \"\"\"Populate the event_list variable to be used in jinja templates\"\"\"\n\n    if not localized_events:\n        generator.context['events_list'] = sorted(events, reverse = True,\n                                                  key=lambda ev: (ev.dtstart, ev.dtend))\n    else:\n        generator.context['events_list'] = {k: sorted(v, reverse = True,\n                                                      key=lambda ev: (ev.dtstart, ev.dtend))\n                                            for k, v in localized_events.items()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef link_source_files(generator):\n    # Get all attributes from the generator that are articles or pages\n    posts = [\n        getattr(generator, attr, None) for attr in PROCESS\n        if getattr(generator, attr, None) is not None]\n    # Work on each item\n    for post in posts[0]:\n        if not 'SHOW_SOURCE_ON_SIDEBAR' in generator.settings and \\\n            not 'SHOW_SOURCE_IN_SECTION' in generator.settings:\n            return\n        # Only try this when specified in metadata or SHOW_SOURCE_ALL_POSTS\n        # override is present in settings\n        if 'SHOW_SOURCE_ALL_POSTS' in generator.settings or \\\n            'show_source' in post.metadata:\n            # Source file name can be optionally set in config\n            show_source_filename = generator.settings.get(\n                'SHOW_SOURCE_FILENAME', '{}.txt'.format(post.slug)\n                )\n            try:\n                # Get the full path to the original source file\n                source_out = os.path.join(\n                    post.settings['OUTPUT_PATH'], post.save_as\n                    )\n                # Get the path to the original source file\n                source_out_path = os.path.split(source_out)[0]\n                # Create 'copy to' destination for writing later\n                copy_to = os.path.join(\n                    source_out_path, show_source_filename\n                    )\n                # Add file to published path\n                source_url = urljoin(\n                    post.save_as, show_source_filename\n                    )\n            except Exception:\n                return\n            # Format post source dict & populate\n            out = dict()\n            out['copy_raw_from'] = post.source_path\n            out['copy_raw_to'] = copy_to\n            logger.debug('Linked %s to %s', post.source_path, copy_to)\n            source_files.append(out)\n            # Also add the source path to the post as an attribute for tpls\n            post.show_source_url = source_url", "response": "Processes each article or page object and links source files to destination files."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _copy_from_to(from_file, to_file):\n    with pelican_open(from_file) as text_in:\n        encoding = 'utf-8'\n        with open(to_file, 'w', encoding=encoding) as text_out:\n            text_out.write(text_in)\n            logger.info('Writing %s', to_file)", "response": "A very rough copy from from_file to to_file"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register():\n    signals.article_generator_finalized.connect(link_source_files)\n    signals.page_generator_finalized.connect(link_source_files)\n    signals.page_writer_finalized.connect(write_source_files)", "response": "Registers the signals that are called by the main thread."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing content and metadata of creole files", "response": "def read(self, source_path):\n        \"\"\"Parse content and metadata of creole files\"\"\"\n\n        self._metadata = {}\n        with pelican_open(source_path) as text:\n            content = creole2html(text, macros={'header': self._parse_header_macro,\n                                            'code': self._parse_code_macro})\n        return content, self._metadata"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef generate_thumbnail_download_link_vimeo(video_id_from_shortcode):\n\t\n\t# Following the Vimeo API at https://developer.vimeo.com/api#video-request, we need to request the video's metadata and get the thumbnail from that. First, then, we'll get the metadata in JSON format, and then will parse it to find the thumbnail URL.\n\tvideo_metadata = urlopen(\"https://vimeo.com/api/v2/video/\" + str(video_id_from_shortcode) + \".json\").read() # Download the video's metadata in JSON format.\n\tvideo_metadata_parsed = json.loads(video_metadata.decode('utf-8')) # Parse the JSON\n\tvideo_thumbnail_large_location = video_metadata_parsed[0]['thumbnail_large'] # Go into the JSON and get the URL of the thumbnail.\n\treturn video_thumbnail_large_location", "response": "Thumbnail URL generator for Vimeo videos."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render(self, size):\n\n        # decode the code\n        middle, corner, side, foreColor, backColor = self.decode(self.code)\n        size = int(size)\n        # make image\n        image = Image.new(\"RGB\", (size * 3, size * 3))\n        draw = ImageDraw.Draw(image)\n\n        # fill background\n        draw.rectangle((0, 0, image.size[0], image.size[1]), fill=0)\n\n        kwds = {\n            'draw': draw,\n            'size': size,\n            'foreColor': foreColor,\n            'backColor': backColor}\n        # middle patch\n        self.drawPatch((1, 1), middle[2], middle[1], middle[0], **kwds)\n\n        # side patch\n        kwds['type'] = side[0]\n        for i in range(4):\n            pos = [(1, 0), (2, 1), (1, 2), (0, 1)][i]\n            self.drawPatch(pos, side[2] + 1 + i, side[1], **kwds)\n\n        # corner patch\n        kwds['type'] = corner[0]\n        for i in range(4):\n            pos = [(0, 0), (2, 0), (2, 2), (0, 2)][i]\n            self.drawPatch(pos, corner[2] + 1 + i, corner[1], **kwds)\n\n        return image", "response": "render identicon to PIL. Image\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef make_posts(generator, metadata, url):\n    reddit = generator.get_reddit()\n    title =  lxml.html.fromstring(metadata['title']).text_content()\n    if reddit is None:\n        log.info(\"Reddit plugin not enabled\")\n        return\n    if metadata.get('status') == \"draft\": # people don't want to post drafts\n        log.debug(\"ignoring draft %s\" % title)\n        return\n\n    collection = generator.settings['REDDIT_POSTER_COLLECT_SUB']\n    sub = reddit.subreddit(collection)\n    results = sub.search(title)\n    if len([result for result in results]) > 0:\n        log.debug(\"ignoring %s because it is already on sub %s \" % (title, collection))\n        # post already was made to this sub\n        return\n    try:\n        submission = sub.submit(title, url=url, resubmit=False)\n        cross_post(reddit, submission, metadata.get('subreddit'))\n    except praw.exceptions.APIException as e:\n        log.error(\"got an api exception: %s\", e)\n    except AssertionError as e:\n        log.error(\"Received an assertion error %s\", e)", "response": "Make posts on the reddit if it s not a draft"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the reddit object.", "response": "def init_reddit(generator):\n    \"\"\"\n    this is a hack to make sure the reddit object keeps track of a session\n    trough article scanning, speeding up networking as the connection can be \n    kept alive.\n    \"\"\"\n    auth_dict = generator.settings.get('REDDIT_POSTER_AUTH')\n    if auth_dict is None:\n        log.info(\"Could not find REDDIT_POSTER_AUTH key in settings, reddit plugin won't function\")\n        generator.get_reddit = lambda: None\n        return\n\n    reddit = praw.Reddit(**auth_dict)\n    generator.get_reddit = lambda: reddit"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a url and call make posts", "response": "def content_written(generator, content):\n    \"\"\"\n    create a url and call make posts (which has less information)\n    \"\"\"\n    url = \"%s/%s\" % (generator.settings.get('SITEURL', 'http://localhost:8000'), content.url)\n    make_posts(generator, content.metadata, url)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ReduceOpacity(im, opacity):\n    assert opacity >= 0 and opacity <= 1\n    if isalpha(im):\n        im = im.copy()\n    else:\n        im = im.convert('RGBA')\n\n    alpha = im.split()[3]\n    alpha = ImageEnhance.Brightness(alpha).enhance(opacity)\n    im.putalpha(alpha)\n    return im", "response": "Reduces Opacity.\n    Returns an image with reduced opacity."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nruns generator on both pages and articles.", "response": "def detect_images_and_galleries(generators):\n    \"\"\"Runs generator on both pages and articles.\"\"\"\n    for generator in generators:\n        if isinstance(generator, ArticlesGenerator):\n            for article in itertools.chain(generator.articles, generator.translations, generator.drafts):\n                detect_image(generator, article)\n                detect_gallery(generator, article)\n        elif isinstance(generator, PagesGenerator):\n            for page in itertools.chain(generator.pages, generator.translations, generator.hidden_pages):\n                detect_image(generator, page)\n                detect_gallery(generator, page)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing the new style of registration based on GitHub Pelican issue #314.", "response": "def register():\n    \"\"\"Uses the new style of registration based on GitHub Pelican issue #314.\"\"\"\n    signals.initialized.connect(initialized)\n    try:\n        signals.content_object_init.connect(detect_content)\n        signals.all_generators_finalized.connect(detect_images_and_galleries)\n        signals.article_writer_finalized.connect(resize_photos)\n    except Exception as e:\n        logger.exception('Plugin failed to execute: {}'.format(pprint.pformat(e)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds Webassets to Jinja2 extensions in Pelican settings.", "response": "def add_jinja2_ext(pelican):\n    \"\"\"Add Webassets to Jinja2 extensions in Pelican settings.\"\"\"\n\n    if 'JINJA_ENVIRONMENT' in pelican.settings: # pelican 3.7+\n        pelican.settings['JINJA_ENVIRONMENT']['extensions'].append(AssetsExtension)\n    else:\n        pelican.settings['JINJA_EXTENSIONS'].append(AssetsExtension)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_assets_env(generator):\n\n    theme_static_dir = generator.settings['THEME_STATIC_DIR']\n    assets_destination = os.path.join(generator.output_path, theme_static_dir)\n    generator.env.assets_environment = Environment(\n        assets_destination, theme_static_dir)\n\n    if 'ASSET_CONFIG' in generator.settings:\n        for item in generator.settings['ASSET_CONFIG']:\n            generator.env.assets_environment.config[item[0]] = item[1]\n\n    if 'ASSET_BUNDLES' in generator.settings:\n        for name, args, kwargs in generator.settings['ASSET_BUNDLES']:\n            generator.env.assets_environment.register(name, *args, **kwargs)\n\n    if 'ASSET_DEBUG' in generator.settings:\n        generator.env.assets_environment.debug = generator.settings['ASSET_DEBUG']\n    elif logging.getLevelName(logger.getEffectiveLevel()) == \"DEBUG\":\n        generator.env.assets_environment.debug = True\n\n    for path in (generator.settings['THEME_STATIC_PATHS'] +\n                 generator.settings.get('ASSET_SOURCE_PATHS', [])):\n        full_path = os.path.join(generator.theme, path)\n        generator.env.assets_environment.append_path(full_path)", "response": "Define the assets environment and pass it to the generator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_pygal(data, options=[], format='svg'):\n    import pygal\n\n    chart_title = data.get('title', None)\n    chart_type = data.get('type', '').lower()\n    # Config options are pretty much proxied straight through from the JSON dict into the object\n    config = pygal.Config()\n    config_dict = data.get('config', {})\n    for key in config_dict.keys():\n        setattr(config, key, config_dict[key])\n\n    if chart_type == 'bar':\n        chart = pygal.HorizontalBar(config) if data.get('horizontal', False) else pygal.Bar(config)\n    elif chart_type == 'line':\n        chart = pygal.Line(config)\n    elif chart_type == 'pie':\n        ir=data.get('inner_radius', 0.0)\n        hp=data.get('half_pie', False)\n        chart = pygal.Pie(config, inner_radius=ir, half_pie=hp)\n    else:\n        print('undefined or unknown chart type')\n\n    if chart is not None:\n        chart.title = data.get('title', None)\n        # Do labels (if present)\n        label_data = data.get('x-labels', None)\n        if isinstance(label_data, list):\n            # use list\n            chart.x_labels = label_data\n        elif isinstance(label_data, dict):\n            # use a range\n            range_from = label_data.get('from', 0)\n            range_to = label_data.get('to', 0)\n            chart.x_labels = map(str, range(range_from, range_to))\n        # insert data\n        for data_set in data.get('data', []):\n            title = data_set.get('title', None)\n            values = data_set.get('values', None)\n            chart.add(title, values)\n        # now render\n        result = chart.render_data_uri()\n    else:\n        result = None\n    return result", "response": "Runs pygal programs and returns image data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resize_thumbnails(pelican):\n    global enabled\n    if not enabled:\n        return\n\n    in_path = _image_path(pelican)\n\n    include_regex = pelican.settings.get('THUMBNAIL_INCLUDE_REGEX')\n    if include_regex:\n        pattern = re.compile(include_regex)\n        is_included = lambda name: pattern.match(name)\n    else:\n        is_included = lambda name: not name.startswith('.')\n\n    sizes = pelican.settings.get('THUMBNAIL_SIZES', DEFAULT_THUMBNAIL_SIZES)\n    resizers = dict((k, _resizer(k, v, in_path)) for k,v in sizes.items())\n    logger.debug(\"Thumbnailer Started\")\n    for dirpath, _, filenames in os.walk(in_path):\n        for filename in filenames:\n            if is_included(filename):\n                for name, resizer in resizers.items():\n                    in_filename = path.join(dirpath, filename)\n                    out_path = get_out_path(pelican, in_path, in_filename, name)\n                    resizer.resize_file_to(\n                        in_filename,\n                        out_path, pelican.settings.get('THUMBNAIL_KEEP_NAME'))", "response": "Resize a directory tree full of images into thumbnails"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef expand_gallery(generator, metadata):\n    if \"gallery\" not in metadata or metadata['gallery'] is None:\n        return  # If no gallery specified, we do nothing\n\n    lines = [ ]\n    base_path = _image_path(generator)\n    in_path = path.join(base_path, metadata['gallery'])\n    template = generator.settings.get('GALLERY_TEMPLATE', DEFAULT_TEMPLATE)\n    thumbnail_name = generator.settings.get(\"GALLERY_THUMBNAIL\", DEFAULT_GALLERY_THUMB)\n    thumbnail_prefix = generator.settings.get(\"\")\n    resizer = _resizer(thumbnail_name, '?x?', base_path)\n    for dirpath, _, filenames in os.walk(in_path):\n        for filename in filenames:\n            if not filename.startswith('.'):\n                url = path.join(dirpath, filename).replace(base_path, \"\")[1:]\n                url = path.join('/static', generator.settings.get('IMAGE_PATH', DEFAULT_IMAGE_DIR), url).replace('\\\\', '/')\n                logger.debug(\"GALLERY: {0}\".format(url))\n                thumbnail = resizer.get_thumbnail_name(filename)\n                thumbnail = path.join('/', generator.settings.get('THUMBNAIL_DIR', DEFAULT_THUMBNAIL_DIR), thumbnail).replace('\\\\', '/')\n                lines.append(template.format(\n                    filename=filename,\n                    url=url,\n                    thumbnail=thumbnail,\n                ))\n    metadata['gallery_content'] = \"\\n\".join(lines)", "response": "Expand a gallery tag to include all of the files in IMAGE_PATH"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef resize_file_to(self, in_path, out_path, keep_filename=False):\n        if keep_filename:\n            filename = path.join(out_path, path.basename(in_path))\n        else:\n            filename = path.join(out_path, self.get_thumbnail_name(in_path))\n        out_path = path.dirname(filename)\n        if not path.exists(out_path):\n            os.makedirs(out_path)\n        if not path.exists(filename):\n            try:\n                image = Image.open(in_path)\n                thumbnail = self.resize(image)\n                thumbnail.save(filename)\n                logger.info(\"Generated Thumbnail {0}\".format(path.basename(filename)))\n            except IOError:\n                logger.info(\"Generating Thumbnail for {0} skipped\".format(path.basename(filename)))", "response": "Given a filename resize and save the image per the specification into out_path"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun graphviz programs and returns image data", "response": "def run_graphviz(program, code, options=[], format='png'):\n    \"\"\" Runs graphviz programs and returns image data\n\n        Copied from https://github.com/tkf/ipython-hierarchymagic/blob/master/hierarchymagic.py\n    \"\"\"\n    import os\n    from subprocess import Popen, PIPE\n\n    dot_args = [program] + options + ['-T', format]\n\n    if os.name == 'nt':\n        # Avoid opening shell window.\n        # * https://github.com/tkf/ipython-hierarchymagic/issues/1\n        # * http://stackoverflow.com/a/2935727/727827\n        p = Popen(dot_args, stdout=PIPE, stdin=PIPE, stderr=PIPE, creationflags=0x08000000)\n    else:\n        p = Popen(dot_args, stdout=PIPE, stdin=PIPE, stderr=PIPE)\n        wentwrong = False\n\n    try:\n        # Graphviz may close standard input when an error occurs,\n        # resulting in a broken pipe on communicate()\n        stdout, stderr = p.communicate(code.encode('utf-8'))\n    except (OSError, IOError) as err:\n        if err.errno != EPIPE:\n            raise\n        wentwrong = True\n    except IOError as err:\n        if err.errno != EINVAL:\n            raise\n        wentwrong = True\n\n    if wentwrong:\n    # in this case, read the standard output and standard error streams\n    # directly, to get the error message(s)\n        stdout, stderr = p.stdout.read(), p.stderr.read()\n        p.wait()\n\n    if p.returncode != 0:\n        raise RuntimeError('dot exited with error:\\n[stderr]\\n{0}'.format(stderr.decode('utf-8')))\n\n    return stdout"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef process_settings(pelicanobj):\n\n    # Default settings\n    inline_settings = {}\n    inline_settings['config'] = {'[]':('', 'pelican-inline')}\n\n    # Get the user specified settings\n    try:\n        settings = pelicanobj.settings['MD_INLINE']\n    except:\n        settings = None\n\n    # If settings have been specified, add them to the config\n    if isinstance(settings, dict):\n        inline_settings['config'].update(settings)\n\n    return inline_settings", "response": "Sets the user specified settings"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef inline_markdown_extension(pelicanobj, config):\n\n    # Instantiate Markdown extension and append it to the current extensions\n    try:\n        if isinstance(pelicanobj.settings.get('MD_EXTENSIONS'), list):  # pelican 3.6.3 and earlier\n            pelicanobj.settings['MD_EXTENSIONS'].append(PelicanInlineMarkdownExtension(config))\n        else:\n            pelicanobj.settings['MARKDOWN'].setdefault('extensions', []).append(PelicanInlineMarkdownExtension(config))\n    except:\n        sys.excepthook(*sys.exc_info())\n        sys.stderr.write(\"\\nError - the pelican Markdown extension failed to configure. Inline Markdown extension is non-functional.\\n\")\n        sys.stderr.flush()", "response": "Instantiates a customized Markdown extension"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngroup articles and pages into lists based on each content type.", "response": "def group_content(generator, content_type):\n    \"\"\"\n    Assembles articles and pages into lists based on each\n    article or page's content. These lists are available\n    through the global context passed to the template engine.\n\n    When multiple categories are present, splits category names\n    based on commas and trims whitespace surrounding a\n    category's name. Thus, commas may not appear within a category\n    but they can be used to delimit categories and may be surrounded by\n    arbitrary amounts of whitespace.\n\n    For each category, substitutes '_' for all whitespace and '-'\n    characters, then creates a list named `SUBSTITUTED_CATEGORY_NAME`_articles\n    or `SUBSTITUTED_CATEGORY_NAME`_pages for Articles or Pages,\n    respectively.\n\n    Note that the *original* category name must appear in the\n    `CATEGORIES_TO_COLLATE` when using this plugin with category\n    filtering enabled.\n    \"\"\"\n    category_filter = generator.settings.get('CATEGORIES_TO_COLLATE', None)\n    filtering_active = type(category_filter) in (list, tuple, set)\n\n    collations = generator.context.get('collations', defaultdict(list))\n    for content in generator.context[content_type]:\n        category_list = [c.strip() for c in content.category.name.split(',')]\n        for category in category_list:\n            if filtering_active and category not in category_filter:\n                continue\n            category = substitute_category_name(category)\n            collations['%s_%s' % (category, content_type)].append(content)\n    generator.context['collations'] = collations"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn whether a request has import permission.", "response": "def has_import_permission(self, request):\n        \"\"\"\n        Returns whether a request has import permission.\n        \"\"\"\n        IMPORT_PERMISSION_CODE = getattr(settings, 'IMPORT_EXPORT_IMPORT_PERMISSION_CODE', None)\n        if IMPORT_PERMISSION_CODE is None:\n            return True\n\n        opts = self.opts\n        codename = get_permission_codename(IMPORT_PERMISSION_CODE, opts)\n        return request.user.has_perm(\"%s.%s\" % (opts.app_label, codename))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_import_resource_kwargs(self, request, *args, **kwargs):\n        return self.get_resource_kwargs(request, *args, **kwargs)", "response": "Prepares and returns kwargs used when initializing Resource"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprocesses the actual import action.", "response": "def process_import(self, request, *args, **kwargs):\n        \"\"\"\n        Perform the actual import action (after the user has confirmed the import)\n        \"\"\"\n        if not self.has_import_permission(request):\n            raise PermissionDenied\n\n        form_type = self.get_confirm_import_form()\n        confirm_form = form_type(request.POST)\n        if confirm_form.is_valid():\n            import_formats = self.get_import_formats()\n            input_format = import_formats[\n                int(confirm_form.cleaned_data['input_format'])\n            ]()\n            tmp_storage = self.get_tmp_storage_class()(name=confirm_form.cleaned_data['import_file_name'])\n            data = tmp_storage.read(input_format.get_read_mode())\n            if not input_format.is_binary() and self.from_encoding:\n                data = force_text(data, self.from_encoding)\n            dataset = input_format.create_dataset(data)\n\n            result = self.process_dataset(dataset, confirm_form, request, *args, **kwargs)\n\n            tmp_storage.remove()\n\n            return self.process_result(result, request)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npreparing kwargs for import_data.", "response": "def get_import_data_kwargs(self, request, *args, **kwargs):\n        \"\"\"\n        Prepare kwargs for import_data.\n        \"\"\"\n        form = kwargs.get('form')\n        if form:\n            kwargs.pop('form')\n            return kwargs\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nperforming a dry_run of the import to make sure the import will not result in errors. If there where no error, save the user uploaded file to a local temp file that will be used by 'process_import' for the actual import.", "response": "def import_action(self, request, *args, **kwargs):\n        \"\"\"\n        Perform a dry_run of the import to make sure the import will not\n        result in errors.  If there where no error, save the user\n        uploaded file to a local temp file that will be used by\n        'process_import' for the actual import.\n        \"\"\"\n        if not self.has_import_permission(request):\n            raise PermissionDenied\n\n        context = self.get_import_context_data()\n\n        import_formats = self.get_import_formats()\n        form_type = self.get_import_form()\n        form_kwargs = self.get_form_kwargs(form_type, *args, **kwargs)\n        form = form_type(import_formats,\n                         request.POST or None,\n                         request.FILES or None,\n                         **form_kwargs)\n\n        if request.POST and form.is_valid():\n            input_format = import_formats[\n                int(form.cleaned_data['input_format'])\n            ]()\n            import_file = form.cleaned_data['import_file']\n            # first always write the uploaded file to disk as it may be a\n            # memory file or else based on settings upload handlers\n            tmp_storage = self.write_to_tmp_storage(import_file, input_format)\n\n            # then read the file, using the proper format-specific mode\n            # warning, big files may exceed memory\n            try:\n                data = tmp_storage.read(input_format.get_read_mode())\n                if not input_format.is_binary() and self.from_encoding:\n                    data = force_text(data, self.from_encoding)\n                dataset = input_format.create_dataset(data)\n            except UnicodeDecodeError as e:\n                return HttpResponse(_(u\"<h1>Imported file has a wrong encoding: %s</h1>\" % e))\n            except Exception as e:\n                return HttpResponse(_(u\"<h1>%s encountered while trying to read file: %s</h1>\" % (type(e).__name__, import_file.name)))\n\n            # prepare kwargs for import data, if needed\n            res_kwargs = self.get_import_resource_kwargs(request, form=form, *args, **kwargs)\n            resource = self.get_import_resource_class()(**res_kwargs)\n\n            # prepare additional kwargs for import_data, if needed\n            imp_kwargs = self.get_import_data_kwargs(request, form=form, *args, **kwargs)\n            result = resource.import_data(dataset, dry_run=True,\n                                          raise_errors=False,\n                                          file_name=import_file.name,\n                                          user=request.user,\n                                          **imp_kwargs)\n\n            context['result'] = result\n\n            if not result.has_errors() and not result.has_validation_errors():\n                initial = {\n                    'import_file_name': tmp_storage.name,\n                    'original_file_name': import_file.name,\n                    'input_format': form.cleaned_data['input_format'],\n                }\n                confirm_form = self.get_confirm_import_form()\n                initial = self.get_form_kwargs(form=form, **initial)\n                context['confirm_form'] = confirm_form(initial=initial)\n        else:\n            res_kwargs = self.get_import_resource_kwargs(request, form=form, *args, **kwargs)\n            resource = self.get_import_resource_class()(**res_kwargs)\n\n        context.update(self.admin_site.each_context(request))\n\n        context['title'] = _(\"Import\")\n        context['form'] = form\n        context['opts'] = self.model._meta\n        context['fields'] = [f.column_name for f in resource.get_user_visible_fields()]\n\n        request.current_app = self.admin_site.name\n        return TemplateResponse(request, [self.import_template_name],\n                                context)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_export_permission(self, request):\n        EXPORT_PERMISSION_CODE = getattr(settings, 'IMPORT_EXPORT_EXPORT_PERMISSION_CODE', None)\n        if EXPORT_PERMISSION_CODE is None:\n            return True\n\n        opts = self.opts\n        codename = get_permission_codename(EXPORT_PERMISSION_CODE, opts)\n        return request.user.has_perm(\"%s.%s\" % (opts.app_label, codename))", "response": "Returns whether a request has export permission."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_export_queryset(self, request):\n        list_display = self.get_list_display(request)\n        list_display_links = self.get_list_display_links(request, list_display)\n        list_filter = self.get_list_filter(request)\n        search_fields = self.get_search_fields(request)\n        if self.get_actions(request):\n            list_display = ['action_checkbox'] + list(list_display)\n\n        ChangeList = self.get_changelist(request)\n        changelist_kwargs = {\n            'request': request,\n            'model': self.model,\n            'list_display': list_display,\n            'list_display_links': list_display_links,\n            'list_filter': list_filter,\n            'date_hierarchy': self.date_hierarchy,\n            'search_fields': search_fields,\n            'list_select_related': self.list_select_related,\n            'list_per_page': self.list_per_page,\n            'list_max_show_all': self.list_max_show_all,\n            'list_editable': self.list_editable,\n            'model_admin': self,\n        }\n        if django.VERSION >= (2, 1):\n            changelist_kwargs['sortable_by'] = self.sortable_by\n        cl = ChangeList(**changelist_kwargs)\n\n        return cl.get_queryset(request)", "response": "Returns the export queryset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_export_data(self, file_format, queryset, *args, **kwargs):\n        request = kwargs.pop(\"request\")\n        if not self.has_export_permission(request):\n            raise PermissionDenied\n\n        resource_class = self.get_export_resource_class()\n        data = resource_class(**self.get_export_resource_kwargs(request)).export(queryset, *args, **kwargs)\n        export_data = file_format.export_data(data)\n        return export_data", "response": "Returns file_format representation for given queryset."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef export_admin_action(self, request, queryset):\n        export_format = request.POST.get('file_format')\n\n        if not export_format:\n            messages.warning(request, _('You must select an export format.'))\n        else:\n            formats = self.get_export_formats()\n            file_format = formats[int(export_format)]()\n\n            export_data = self.get_export_data(file_format, queryset, request=request)\n            content_type = file_format.get_content_type()\n            response = HttpResponse(export_data, content_type=content_type)\n            response['Content-Disposition'] = 'attachment; filename=%s' % (\n                self.get_export_filename(file_format),\n            )\n            return response", "response": "Exports the selected rows using file_format."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef modelresource_factory(model, resource_class=ModelResource):\n    attrs = {'model': model}\n    Meta = type(str('Meta'), (object,), attrs)\n\n    class_name = model.__name__ + str('Resource')\n\n    class_attrs = {\n        'Meta': Meta,\n    }\n\n    metaclass = ModelDeclarativeMetaclass\n    return metaclass(class_name, (resource_class,), class_attrs)", "response": "Factory for creating ModelResource class for given Django model."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_field_name(self, field):\n        for field_name, f in self.fields.items():\n            if f == field:\n                return field_name\n        raise AttributeError(\"Field %s does not exists in %s resource\" % (\n            field, self.__class__))", "response": "Returns the name of a given field in the resource."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting or initialize an existing instance.", "response": "def get_or_init_instance(self, instance_loader, row):\n        \"\"\"\n        Either fetches an already existing instance or initializes a new one.\n        \"\"\"\n        instance = self.get_instance(instance_loader, row)\n        if instance:\n            return (instance, False)\n        else:\n            return (self.init_instance(row), True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the given instance.", "response": "def validate_instance(self, instance, import_validation_errors=None, validate_unique=True):\n        \"\"\"\n        Takes any validation errors that were raised by\n        :meth:`~import_export.resources.Resource.import_obj`, and combines them\n        with validation errors raised by the instance's ``full_clean()``\n        method. The combined errors are then re-raised as single, multi-field\n        ValidationError.\n\n        If the ``clean_model_instances`` option is False, the instances's\n        ``full_clean()`` method is not called, and only the errors raised by\n        ``import_obj()`` are re-raised.\n        \"\"\"\n        if import_validation_errors is None:\n            errors = {}\n        else:\n            errors = import_validation_errors.copy()\n        if self._meta.clean_model_instances:\n            try:\n                instance.full_clean(\n                    exclude=errors.keys(),\n                    validate_unique=validate_unique,\n                )\n            except ValidationError as e:\n                errors = e.update_error_dict(errors)\n\n        if errors:\n            raise ValidationError(errors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsaves the instance to the database.", "response": "def save_instance(self, instance, using_transactions=True, dry_run=False):\n        \"\"\"\n        Takes care of saving the object to the database.\n\n        Keep in mind that this is done by calling ``instance.save()``, so\n        objects are not created in bulk!\n        \"\"\"\n        self.before_save_instance(instance, using_transactions, dry_run)\n        if not using_transactions and dry_run:\n            # we don't have transactions and we want to do a dry_run\n            pass\n        else:\n            instance.save()\n        self.after_save_instance(instance, using_transactions, dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes the instance from the cache.", "response": "def delete_instance(self, instance, using_transactions=True, dry_run=False):\n        \"\"\"\n        Calls :meth:`instance.delete` as long as ``dry_run`` is not set.\n        \"\"\"\n        self.before_delete_instance(instance, dry_run)\n        if not using_transactions and dry_run:\n            # we don't have transactions and we want to do a dry_run\n            pass\n        else:\n            instance.delete()\n        self.after_delete_instance(instance, dry_run)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_field(self, field, obj, data, is_m2m=False):\n        if field.attribute and field.column_name in data:\n            field.save(obj, data, is_m2m)", "response": "Imports the field from the data dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimports the object into the resource.", "response": "def import_obj(self, obj, data, dry_run):\n        \"\"\"\n        Traverses every field in this Resource and calls\n        :meth:`~import_export.resources.Resource.import_field`. If\n        ``import_field()`` results in a ``ValueError`` being raised for\n        one of more fields, those errors are captured and reraised as a single,\n        multi-field ValidationError.\"\"\"\n        errors = {}\n        for field in self.get_import_fields():\n            if isinstance(field.widget, widgets.ManyToManyWidget):\n                continue\n            try:\n                self.import_field(field, obj, data)\n            except ValueError as e:\n                errors[field.attribute] = ValidationError(\n                    force_text(e), code=\"invalid\")\n        if errors:\n            raise ValidationError(errors)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsaves the data for the given object to the database.", "response": "def save_m2m(self, obj, data, using_transactions, dry_run):\n        \"\"\"\n        Saves m2m fields.\n\n        Model instance need to have a primary key value before\n        a many-to-many relationship can be used.\n        \"\"\"\n        if not using_transactions and dry_run:\n            # we don't have transactions and we want to do a dry_run\n            pass\n        else:\n            for field in self.get_import_fields():\n                if not isinstance(field.widget, widgets.ManyToManyWidget):\n                    continue\n                self.import_field(field, obj, data, True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef skip_row(self, instance, original):\n        if not self._meta.skip_unchanged:\n            return False\n        for field in self.get_import_fields():\n            try:\n                # For fields that are models.fields.related.ManyRelatedManager\n                # we need to compare the results\n                if list(field.get_value(instance).all()) != list(field.get_value(original).all()):\n                    return False\n            except AttributeError:\n                if field.get_value(instance) != field.get_value(original):\n                    return False\n        return True", "response": "Returns True if the row importing should be skipped."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef import_row(self, row, instance_loader, using_transactions=True, dry_run=False, **kwargs):\n        row_result = self.get_row_result_class()()\n        try:\n            self.before_import_row(row, **kwargs)\n            instance, new = self.get_or_init_instance(instance_loader, row)\n            self.after_import_instance(instance, new, **kwargs)\n            if new:\n                row_result.import_type = RowResult.IMPORT_TYPE_NEW\n            else:\n                row_result.import_type = RowResult.IMPORT_TYPE_UPDATE\n            row_result.new_record = new\n            original = deepcopy(instance)\n            diff = self.get_diff_class()(self, original, new)\n            if self.for_delete(row, instance):\n                if new:\n                    row_result.import_type = RowResult.IMPORT_TYPE_SKIP\n                    diff.compare_with(self, None, dry_run)\n                else:\n                    row_result.import_type = RowResult.IMPORT_TYPE_DELETE\n                    self.delete_instance(instance, using_transactions, dry_run)\n                    diff.compare_with(self, None, dry_run)\n            else:\n                import_validation_errors = {}\n                try:\n                    self.import_obj(instance, row, dry_run)\n                except ValidationError as e:\n                    # Validation errors from import_obj() are passed on to\n                    # validate_instance(), where they can be combined with model\n                    # instance validation errors if necessary\n                    import_validation_errors = e.update_error_dict(import_validation_errors)\n                if self.skip_row(instance, original):\n                    row_result.import_type = RowResult.IMPORT_TYPE_SKIP\n                else:\n                    self.validate_instance(instance, import_validation_errors)\n                    self.save_instance(instance, using_transactions, dry_run)\n                    self.save_m2m(instance, row, using_transactions, dry_run)\n                    # Add object info to RowResult for LogEntry\n                    row_result.object_id = instance.pk\n                    row_result.object_repr = force_text(instance)\n                diff.compare_with(self, instance, dry_run)\n\n            row_result.diff = diff.as_html()\n            self.after_import_row(row, row_result, **kwargs)\n\n        except ValidationError as e:\n            row_result.import_type = RowResult.IMPORT_TYPE_INVALID\n            row_result.validation_error = e\n        except Exception as e:\n            row_result.import_type = RowResult.IMPORT_TYPE_ERROR\n            # There is no point logging a transaction error for each row\n            # when only the original error is likely to be relevant\n            if not isinstance(e, TransactionManagementError):\n                logger.debug(e, exc_info=e)\n            tb_info = traceback.format_exc()\n            row_result.errors.append(self.get_error_result_class()(e, tb_info, row))\n        return row_result", "response": "Imports data from a dict of data into a new record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef import_data(self, dataset, dry_run=False, raise_errors=False,\n                    use_transactions=None, collect_failed_rows=False, **kwargs):\n        \"\"\"\n        Imports data from ``tablib.Dataset``. Refer to :doc:`import_workflow`\n        for a more complete description of the whole import process.\n\n        :param dataset: A ``tablib.Dataset``\n\n        :param raise_errors: Whether errors should be printed to the end user\n            or raised regularly.\n\n        :param use_transactions: If ``True`` the import process will be processed\n            inside a transaction.\n\n        :param collect_failed_rows: If ``True`` the import process will collect\n            failed rows.\n\n        :param dry_run: If ``dry_run`` is set, or an error occurs, if a transaction\n            is being used, it will be rolled back.\n        \"\"\"\n\n        if use_transactions is None:\n            use_transactions = self.get_use_transactions()\n\n        connection = connections[DEFAULT_DB_ALIAS]\n        supports_transactions = getattr(connection.features, \"supports_transactions\", False)\n\n        if use_transactions and not supports_transactions:\n            raise ImproperlyConfigured\n\n        using_transactions = (use_transactions or dry_run) and supports_transactions\n\n        with atomic_if_using_transaction(using_transactions):\n            return self.import_data_inner(dataset, dry_run, raise_errors, using_transactions, collect_failed_rows, **kwargs)", "response": "Imports data from a tablib. Dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_m2m_widget(cls, field):\n        return functools.partial(\n            widgets.ManyToManyWidget,\n            model=get_related_model(field))", "response": "Prepare widget for m2m field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_fk_widget(cls, field):\n        return functools.partial(\n            widgets.ForeignKeyWidget,\n            model=get_related_model(field))", "response": "Prepare widget for fk and o2o fields\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the widget that would likely be associated with each django field.", "response": "def widget_from_django_field(cls, f, default=widgets.Widget):\n        \"\"\"\n        Returns the widget that would likely be associated with each\n        Django type.\n\n        Includes mapping of Postgres Array and JSON fields. In the case that\n        psycopg2 is not installed, we consume the error and process the field\n        regardless.\n        \"\"\"\n        result = default\n        internal_type = \"\"\n        if callable(getattr(f, \"get_internal_type\", None)):\n            internal_type = f.get_internal_type()\n\n        if internal_type in cls.WIDGETS_MAP:\n            result = cls.WIDGETS_MAP[internal_type]\n            if isinstance(result, str):\n                result = getattr(cls, result)(f)\n        else:\n            try:\n                from django.contrib.postgres.fields import ArrayField, JSONField\n            except ImportError:\n                # ImportError: No module named psycopg2.extras\n                class ArrayField:\n                    pass\n\n                class JSONField:\n                    pass\n\n            if isinstance(f, ArrayField):\n                return widgets.SimpleArrayWidget\n            elif isinstance(f, JSONField):\n                return widgets.JSONWidget\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the kwargs for the given field_name.", "response": "def widget_kwargs_for_field(self, field_name):\n        \"\"\"\n        Returns widget kwargs for given field_name.\n        \"\"\"\n        if self._meta.widgets:\n            return self._meta.widgets.get(field_name, {})\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef field_from_django_field(cls, field_name, django_field, readonly):\n\n        FieldWidget = cls.widget_from_django_field(django_field)\n        widget_kwargs = cls.widget_kwargs_for_field(field_name)\n        field = cls.DEFAULT_RESOURCE_FIELD(\n            attribute=field_name,\n            column_name=field_name,\n            widget=FieldWidget(**widget_kwargs),\n            readonly=readonly,\n            default=django_field.default,\n        )\n        return field", "response": "Returns a Resource Field instance for the given Django model field."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef after_import(self, dataset, result, using_transactions, dry_run, **kwargs):\n        # Adapted from django's loaddata\n        if not dry_run and any(r.import_type == RowResult.IMPORT_TYPE_NEW for r in result.rows):\n            connection = connections[DEFAULT_DB_ALIAS]\n            sequence_sql = connection.ops.sequence_reset_sql(no_style(), [self._meta.model])\n            if sequence_sql:\n                cursor = connection.cursor()\n                try:\n                    for line in sequence_sql:\n                        cursor.execute(line)\n                finally:\n                    cursor.close()", "response": "Reset the SQL sequences after new objects are imported"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ntranslate the value stored in the imported datasource to an aniso - formatted Python object and returns it.", "response": "def clean(self, data):\n        \"\"\"\n        Translates the value stored in the imported datasource to an\n        appropriate Python object and returns it.\n        \"\"\"\n        try:\n            value = data[self.column_name]\n        except KeyError:\n            raise KeyError(\"Column '%s' not found in dataset. Available \"\n                           \"columns are: %s\" % (self.column_name, list(data)))\n\n        # If ValueError is raised here, import_obj() will handle it\n        value = self.widget.clean(value, row=data)\n\n        if value in self.empty_values and self.default != NOT_PROVIDED:\n            if callable(self.default):\n                return self.default()\n            return self.default\n\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the value of the object s attribute.", "response": "def get_value(self, obj):\n        \"\"\"\n        Returns the value of the object's attribute.\n        \"\"\"\n        if self.attribute is None:\n            return None\n\n        attrs = self.attribute.split('__')\n        value = obj\n\n        for attr in attrs:\n            try:\n                value = getattr(value, attr, None)\n            except (ValueError, ObjectDoesNotExist):\n                # needs to have a primary key value before a many-to-many\n                # relationship can be used.\n                return None\n            if value is None:\n                return None\n\n        # RelatedManager and ManyRelatedManager classes are callable in\n        # Django >= 1.7 but we don't want to call them\n        if callable(value) and not isinstance(value, Manager):\n            value = value()\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save(self, obj, data, is_m2m=False):\n        if not self.readonly:\n            attrs = self.attribute.split('__')\n            for attr in attrs[:-1]:\n                obj = getattr(obj, attr, None)\n            cleaned = self.clean(data)\n            if cleaned is not None or self.saves_null_values:\n                if not is_m2m:\n                    setattr(obj, attrs[-1], cleaned)\n                else:\n                    getattr(obj, attrs[-1]).set(cleaned)", "response": "Save the data to the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef export(self, obj):\n        value = self.get_value(obj)\n        if value is None:\n            return \"\"\n        return self.widget.render(value, obj)", "response": "Returns the value of the object converted to export\n        representation."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new dataset from the first sheet.", "response": "def create_dataset(self, in_stream):\n        \"\"\"\n        Create dataset from first sheet.\n        \"\"\"\n        assert XLS_IMPORT\n        xls_book = xlrd.open_workbook(file_contents=in_stream)\n        dataset = tablib.Dataset()\n        sheet = xls_book.sheets()[0]\n\n        dataset.headers = sheet.row_values(0)\n        for i in range(1, sheet.nrows):\n            dataset.append(sheet.row_values(i))\n        return dataset"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate dataset from first sheet.", "response": "def create_dataset(self, in_stream):\n        \"\"\"\n        Create dataset from first sheet.\n        \"\"\"\n        assert XLSX_IMPORT\n        from io import BytesIO\n        xlsx_book = openpyxl.load_workbook(BytesIO(in_stream), read_only=True)\n\n        dataset = tablib.Dataset()\n        sheet = xlsx_book.active\n\n        # obtain generator\n        rows = sheet.rows\n        dataset.headers = [cell.value for cell in next(rows)]\n\n        for row in rows:\n            row_values = [cell.value for cell in row]\n            dataset.append(row_values)\n        return dataset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef field_specific_errors(self):\n        return {\n            key: value for key, value in self.error_dict.items()\n            if key != NON_FIELD_ERRORS\n        }", "response": "Returns a dictionary of field - specific validation errors for this row."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the total number of validation errors for this row.", "response": "def error_count(self):\n        \"\"\"Returns the total number of validation errors for this row.\"\"\"\n        count = 0\n        for error_list in self.error_dict.values():\n            count += len(error_list)\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn an ActionForm subclass populated with the given formats.", "response": "def export_action_form_factory(formats):\n    \"\"\"\n    Returns an ActionForm subclass containing a ChoiceField populated with\n    the given formats.\n    \"\"\"\n    class _ExportActionForm(ActionForm):\n        \"\"\"\n        Action form with export format ChoiceField.\n        \"\"\"\n        file_format = forms.ChoiceField(\n            label=_('Format'), choices=formats, required=False)\n    _ExportActionForm.__name__ = str('ExportActionForm')\n\n    return _ExportActionForm"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a queryset of all objects for this Model.", "response": "def get_queryset(self, value, row, *args, **kwargs):\n        \"\"\"\n        Returns a queryset of all objects for this Model.\n\n        Overwrite this method if you want to limit the pool of objects from\n        which the related object is retrieved.\n\n        :param value: The field's value in the datasource.\n        :param row: The datasource's current row.\n\n        As an example; if you'd like to have ForeignKeyWidget look up a Person\n        by their pre- **and** lastname column, you could subclass the widget\n        like so::\n\n            class FullNameForeignKeyWidget(ForeignKeyWidget):\n                def get_queryset(self, value, row):\n                    return self.model.objects.filter(\n                        first_name__iexact=row[\"first_name\"],\n                        last_name__iexact=row[\"last_name\"]\n                    )\n        \"\"\"\n        return self.model.objects.all()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_config_dir():\n\n    paths = []\n    if 'TMUXP_CONFIGDIR' in os.environ:\n        paths.append(os.environ['TMUXP_CONFIGDIR'])\n    if 'XDG_CONFIG_HOME' in os.environ:\n        paths.append(os.environ['XDG_CONFIG_HOME'])\n    else:\n        paths.append('~/.config/tmuxp/')\n    paths.append('~/.tmuxp')\n\n    for path in paths:\n        path = os.path.expanduser(path)\n        if os.path.isdir(path):\n            return path\n    # Return last path as default if none of the previous ones matched\n    return path", "response": "Returns tmuxp configuration directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns tmuxinator configuration directory.", "response": "def get_tmuxinator_dir():\n    \"\"\"\n    Return tmuxinator configuration directory.\n\n    Checks for ``TMUXINATOR_CONFIG`` environmental variable.\n\n    Returns\n    -------\n    str :\n        absolute path to tmuxinator config directory\n\n    See Also\n    --------\n    :meth:`tmuxp.config.import_tmuxinator`\n    \"\"\"\n    if 'TMUXINATOR_CONFIG' in os.environ:\n        return os.path.expanduser(os.environ['TMUXINATOR_CONFIG'])\n\n    return os.path.expanduser('~/.tmuxinator/')"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nvalidates the choices in the list of allowed choices.", "response": "def _validate_choices(options):\n    \"\"\"\n    Callback wrapper for validating click.prompt input.\n\n    Parameters\n    ----------\n    options : list\n        List of allowed choices\n\n    Returns\n    -------\n    :func:`callable`\n        callback function for value_proc in :func:`click.prompt`.\n\n    Raises\n    ------\n    :class:`click.BadParameter`\n    \"\"\"\n\n    def func(value):\n        if value not in options:\n            raise click.BadParameter(\n                'Possible choices are: {0}.'.format(', '.join(options))\n            )\n        return value\n\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the layout hooks for a tmux session.", "response": "def set_layout_hook(session, hook_name):\n    \"\"\"Set layout hooks to normalize layout.\n\n    References:\n\n        - tmuxp issue: https://github.com/tmux-python/tmuxp/issues/309\n        - tmux issue: https://github.com/tmux/tmux/issues/1106\n\n    tmux 2.6+ requires that the window be viewed with the client before\n    select-layout adjustments can take effect.\n\n    To handle this, this function creates temporary hook for this session to\n    iterate through all windows and select the layout.\n\n    In order for layout changes to take effect, a client must at the very\n    least be attached to the window (not just the session).\n\n    hook_name is provided to allow this to set to multiple scenarios, such\n    as 'client-attached' (which the user attaches the session). You may\n    also want 'after-switch-client' for cases where the user loads tmuxp\n    sessions inside tmux since tmuxp offers to switch for them.\n\n    Also, the hooks are set immediately unbind after they're invoked via -u.\n\n    Parameters\n    ----------\n    session : :class:`libtmux.session.Session`\n        session to bind hook to\n    hook_name : str\n        hook name to bind to, e.g. 'client-attached'\n    \"\"\"\n    cmd = ['set-hook', '-t', session.id, hook_name]\n    hook_cmd = []\n    for window in session.windows:\n        # unfortunately, select-layout won't work unless\n        # we've literally selected the window at least once\n        # with the client\n        hook_cmd.append('selectw -t {}'.format(window.id))\n        # edit: removed -t, or else it won't respect main-pane-w/h\n        hook_cmd.append('selectl'.format(window.id))\n        hook_cmd.append('selectw -p'.format(window.id))\n\n    # unset the hook immediately after executing\n    hook_cmd.append(\n        'set-hook -u -t {target_session} {hook_name}'.format(\n            target_session=session.id, hook_name=hook_name\n        )\n    )\n\n    # join the hook's commands with semicolons\n    hook_cmd = '{}'.format('; '.join(hook_cmd))\n\n    # append the hook command\n    cmd.append(hook_cmd)\n\n    # create the hook\n    session.cmd(*cmd)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_pure_name(path):\n    return (\n        not os.path.isabs(path)\n        and len(os.path.dirname(path)) == 0\n        and not os.path.splitext(path)[1]\n        and path != '.'\n        and path != ''\n    )", "response": "Checks if path is a pure name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating and translate click config arg.", "response": "def scan_config_argument(ctx, param, value, config_dir=None):\n    \"\"\"Validate / translate config name/path values for click config arg.\n\n    Wrapper on top of :func:`cli.scan_config`.\"\"\"\n    if callable(config_dir):\n        config_dir = config_dir()\n\n    if not config:\n        click.echo(\"Enter at least one CONFIG\")\n        click.echo(ctx.get_help(), color=ctx.color)\n        ctx.exit()\n\n    if isinstance(value, string_types):\n        value = scan_config(value, config_dir=config_dir)\n\n    elif isinstance(value, tuple):\n        value = tuple([scan_config(v, config_dir=config_dir) for v in value])\n\n    return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nscanning a config file for a specific language.", "response": "def scan_config(config, config_dir=None):\n    \"\"\"\n    Return the real config path or raise an exception.\n\n    If config is directory, scan for .tmuxp.{yaml,yml,json} in directory. If\n    one or more found, it will warn and pick the first.\n\n    If config is \".\", \"./\" or None, it will scan current directory.\n\n    If config is has no path and only a filename, e.g. \"myconfig.yaml\" it will\n    search config dir.\n\n    If config has no path and only a name with no extension, e.g. \"myconfig\",\n    it will scan for file name with yaml, yml and json. If multiple exist, it\n    will warn and pick the first.\n\n    Parameters\n    ----------\n    config : str\n        config file, valid examples:\n\n        - a file name, myconfig.yaml\n        - relative path, ../config.yaml or ../project\n        - a period, .\n\n    Raises\n    ------\n    :class:`click.exceptions.FileError`\n    \"\"\"\n    if not config_dir:\n        config_dir = get_config_dir()\n    path = os.path\n    exists, join, isabs = path.exists, path.join, path.isabs\n    dirname, normpath, splitext = path.dirname, path.normpath, path.splitext\n    cwd = os.getcwd()\n    is_name = False\n    file_error = None\n\n    config = os.path.expanduser(config)\n    # if purename, resolve to confg dir\n    if is_pure_name(config):\n        is_name = True\n    elif (\n        not isabs(config)\n        or len(dirname(config)) > 1\n        or config == '.'\n        or config == \"\"\n        or config == \"./\"\n    ):  # if relative, fill in full path\n        config = normpath(join(cwd, config))\n\n    # no extension, scan\n    if not splitext(config)[1]:\n        if is_name:\n            candidates = [\n                x\n                for x in [\n                    '%s%s' % (join(config_dir, config), ext)\n                    for ext in ['.yaml', '.yml', '.json']\n                ]\n                if exists(x)\n            ]\n            if not len(candidates):\n                file_error = (\n                    'config not found in config dir (yaml/yml/json) %s '\n                    'for name' % (config_dir)\n                )\n        else:\n            candidates = [\n                x\n                for x in [\n                    join(config, ext)\n                    for ext in ['.tmuxp.yaml', '.tmuxp.yml', '.tmuxp.json']\n                ]\n                if exists(x)\n            ]\n\n            if len(candidates) > 1:\n                click.secho(\n                    'Multiple .tmuxp.{yml,yaml,json} configs in %s' % dirname(config),\n                    fg=\"red\",\n                )\n                click.echo(\n                    click.wrap_text(\n                        'This is undefined behavior, use only one. '\n                        'Use file names e.g. myproject.json, coolproject.yaml. '\n                        'You can load them by filename.'\n                    )\n                )\n            elif not len(candidates):\n                file_error = 'No tmuxp files found in directory'\n        if len(candidates):\n            config = candidates[0]\n    elif not exists(config):\n        file_error = 'file not found'\n\n    if file_error:\n        raise FileError(file_error, config)\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading a tmux workspace from a tmuxp file.", "response": "def load_workspace(\n    config_file,\n    socket_name=None,\n    socket_path=None,\n    colors=None,\n    detached=False,\n    answer_yes=False,\n):\n    \"\"\"\n    Load a tmux \"workspace\" session via tmuxp file.\n\n    Parameters\n    ----------\n    config_file : str\n        absolute path to config file\n    socket_name : str, optional\n        ``tmux -L <socket-name>``\n    socket_path: str, optional\n        ``tmux -S <socket-path>``\n    colors : str, optional\n        '-2'\n            Force tmux to support 256 colors\n    detached : bool\n        Force detached state. default False.\n    answer_yes : bool\n        Assume yes when given prompt. default False.\n\n    Notes\n    -----\n\n    tmuxp will check and load a configuration file. The file will use kaptan\n    to load a JSON/YAML into a :py:obj:`dict`. Then :func:`config.expand` and\n    :func:`config.trickle` will be used to expand any shorthands, template\n    variables, or file paths relative to where the config/script is executed\n    from.\n\n    :func:`config.expand` accepts the directory of the config file, so the\n    user's configuration can resolve absolute paths relative to where the\n    config file is. In otherwords, if a config file at */var/moo/hi.yaml*\n    has *./* in its configs, we want to be sure any file path with *./* is\n    relative to */var/moo*, not the user's PWD.\n\n    A :class:`libtmux.Server` object is created. No tmux server is started yet,\n    just the object.\n\n    The prepared configuration and the server object is passed into an instance\n    of :class:`~tmuxp.workspacebuilder.WorkspaceBuilder`.\n\n    A sanity check against :meth:`libtmux.common.which` is ran. It will raise\n    an exception if tmux isn't found.\n\n    If a tmux session under the same name as ``session_name`` in the tmuxp\n    configuration exists, tmuxp offers to attach the session. Currently, tmuxp\n    does not allow appending a workspace / incremental building on top of a\n    current session (pull requests are welcome!).\n\n    :meth:`~tmuxp.workspacebuilder.WorkspaceBuilder.build` will build the session in\n    the background via using tmux's detached state (``-d``).\n\n    After the session (workspace) is built, unless the user decided to load\n    the session in the background via ``tmuxp -d`` (which is in the spirit\n    of tmux's ``-d``), we need to prompt the user to attach the session.\n\n    If the user is already inside a tmux client, which we detect via the\n    ``TMUX`` environment variable bring present, we will prompt the user to\n    switch their current client to it.\n\n    If they're outside of tmux client - in a plain-old PTY - we will\n    automatically ``attach``.\n\n    If an exception is raised during the building of the workspace, tmuxp will\n    prompt to cleanup (``$ tmux kill-session``) the session on the user's\n    behalf. An exception raised during this process means it's not easy to\n    predict how broken the session is.\n\n    .. versionchanged:: tmux 2.6+\n\n        In tmux 2.6, the way layout and proportion's work when interfacing\n        with tmux in a detached state (outside of a client) changed. Since\n        tmuxp builds workspaces in a detached state, the WorkspaceBuilder isn't\n        able to rely on functionality requiring awarness of session geometry,\n        e.g. ``set-layout``.\n\n        Thankfully, tmux is able to defer commands to run after the user\n        performs certain actions, such as loading a client via\n        ``attach-session`` or ``switch-client``.\n\n        Upon client switch, ``client-session-changed`` is triggered [1]_.\n\n    References\n    ----------\n    .. [1] cmd-switch-client.c hook. GitHub repo for tmux.\n       https://github.com/tmux/tmux/blob/2.6/cmd-switch-client.c#L132.\n       Accessed April 8th, 2018.\n    \"\"\"\n    # get the canonical path, eliminating any symlinks\n    config_file = os.path.realpath(config_file)\n\n    # kaptan allows us to open a yaml or json file as a dict\n    sconfig = kaptan.Kaptan()\n    sconfig = sconfig.import_config(config_file).get()\n    # shapes configurations relative to config / profile file location\n    sconfig = config.expand(sconfig, os.path.dirname(config_file))\n    # propagate config inheritance (e.g. session -> window, window -> pane)\n    sconfig = config.trickle(sconfig)\n\n    t = Server(  # create tmux server object\n        socket_name=socket_name, socket_path=socket_path, colors=colors\n    )\n\n    which('tmux')  # raise exception if tmux not found\n\n    try:  # load WorkspaceBuilder object for tmuxp config / tmux server\n        builder = WorkspaceBuilder(sconf=sconfig, server=t)\n    except exc.EmptyConfigException:\n        click.echo('%s is empty or parsed no config data' % config_file, err=True)\n        return\n\n    session_name = sconfig['session_name']\n\n    # if the session already exists, prompt the user to attach. tmuxp doesn't\n    # support incremental session building or appending (yet, PR's welcome!)\n    if builder.session_exists(session_name):\n        if not detached and (\n            answer_yes\n            or click.confirm(\n                '%s is already running. Attach?'\n                % click.style(session_name, fg='green'),\n                default=True,\n            )\n        ):\n            _reattach(builder.session)\n        return\n\n    try:\n        click.echo(\n            click.style('[Loading] ', fg='green')\n            + click.style(config_file, fg='blue', bold=True)\n        )\n\n        builder.build()  # load tmux session via workspace builder\n\n        if 'TMUX' in os.environ:  # tmuxp ran from inside tmux\n            if not detached and (\n                answer_yes or click.confirm('Already inside TMUX, switch to session?')\n            ):\n                # unset TMUX, save it, e.g. '/tmp/tmux-1000/default,30668,0'\n                tmux_env = os.environ.pop('TMUX')\n\n                if has_gte_version('2.6'):\n                    set_layout_hook(builder.session, 'client-session-changed')\n\n                builder.session.switch_client()  # switch client to new session\n\n                os.environ['TMUX'] = tmux_env  # set TMUX back again\n                return builder.session\n            else:  # session created in the background, from within tmux\n                if has_gte_version('2.6'):  # prepare for both cases\n                    set_layout_hook(builder.session, 'client-attached')\n                    set_layout_hook(builder.session, 'client-session-changed')\n\n                sys.exit('Session created in detached state.')\n        else:  # tmuxp ran from inside tmux\n            if has_gte_version('2.6'):\n                # if attaching for first time\n                set_layout_hook(builder.session, 'client-attached')\n\n                # for cases where user switches client for first time\n                set_layout_hook(builder.session, 'client-session-changed')\n\n            if not detached:\n                builder.session.attach_session()\n\n    except exc.TmuxpException as e:\n        import traceback\n\n        click.echo(traceback.format_exc(), err=True)\n        click.echo(e, err=True)\n\n        choice = click.prompt(\n            'Error loading workspace. (k)ill, (a)ttach, (d)etach?',\n            value_proc=_validate_choices(['k', 'a', 'd']),\n            default='k',\n        )\n\n        if choice == 'k':\n            builder.session.kill_session()\n            click.echo('Session killed.')\n        elif choice == 'a':\n            if 'TMUX' in os.environ:\n                builder.session.switch_client()\n            else:\n                builder.session.attach_session()\n        else:\n            sys.exit()\n\n    return builder.session"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cli(log_level):\n    try:\n        has_minimum_version()\n    except TmuxCommandNotFound:\n        click.echo('tmux not found. tmuxp requires you install tmux first.')\n        sys.exit()\n    except exc.TmuxpException as e:\n        click.echo(e, err=True)\n        sys.exit()\n    setup_logger(level=log_level.upper())", "response": "Manage tmux sessions.\n\n    Pass the \"--help\" argument to any command to see detailed help.\n    See detailed documentation and examples at:\n    http://tmuxp.readthedocs.io/en/latest/"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload a tmux workspace from a list of CONFIGs.", "response": "def command_load(ctx, config, socket_name, socket_path, answer_yes, detached, colors):\n    \"\"\"Load a tmux workspace from each CONFIG.\n\n    CONFIG is a specifier for a configuration file.\n\n    If CONFIG is a path to a directory, tmuxp will search it for\n    \".tmuxp.{yaml,yml,json}\".\n\n    If CONFIG is has no directory component and only a filename, e.g.\n    \"myconfig.yaml\", tmuxp will search the users's config directory for that\n    file.\n\n    If CONFIG has no directory component, and only a name with no extension,\n    e.g. \"myconfig\", tmuxp will search the users's config directory for any\n    file with the extension \".yaml\", \".yml\", or \".json\" that matches that name.\n\n    If multiple configuration files that match a given CONFIG are found, tmuxp\n    will warn and pick the first one found.\n\n    If multiple CONFIGs are provided, workspaces will be created for all of\n    them. The last one provided will be attached. The others will be created in\n    detached mode.\n    \"\"\"\n    util.oh_my_zsh_auto_title()\n\n    tmux_options = {\n        'socket_name': socket_name,\n        'socket_path': socket_path,\n        'answer_yes': answer_yes,\n        'colors': colors,\n        'detached': detached,\n    }\n\n    if not config:\n        click.echo(\"Enter at least one CONFIG\")\n        click.echo(ctx.get_help(), color=ctx.color)\n        ctx.exit()\n\n    if isinstance(config, string_types):\n        load_workspace(config, **tmux_options)\n\n    elif isinstance(config, tuple):\n        config = list(config)\n        # Load each configuration but the last to the background\n        for cfg in config[:-1]:\n            opt = tmux_options.copy()\n            opt.update({'detached': True})\n            load_workspace(cfg, **opt)\n\n        # todo: obey the -d in the cli args only if user specifies\n        load_workspace(config[-1], **tmux_options)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a tmuxp config between JSON and YAML.", "response": "def command_convert(config):\n    \"\"\"Convert a tmuxp config between JSON and YAML.\"\"\"\n\n    _, ext = os.path.splitext(config)\n    if 'json' in ext:\n        if click.confirm('convert to <%s> to yaml?' % config):\n            configparser = kaptan.Kaptan()\n            configparser.import_config(config)\n            newfile = config.replace(ext, '.yaml')\n            newconfig = configparser.export('yaml', indent=2, default_flow_style=False)\n            if click.confirm('Save config to %s?' % newfile):\n                buf = open(newfile, 'w')\n                buf.write(newconfig)\n                buf.close()\n                print('New config saved to %s' % newfile)\n    elif 'yaml' in ext:\n        if click.confirm('convert to <%s> to json?' % config):\n            configparser = kaptan.Kaptan()\n            configparser.import_config(config)\n            newfile = config.replace(ext, '.json')\n            newconfig = configparser.export('json', indent=2)\n            print(newconfig)\n            if click.confirm('Save config to <%s>?' % newfile):\n                buf = open(newfile, 'w')\n                buf.write(newconfig)\n                buf.close()\n                print('New config saved to <%s>.' % newfile)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef freeze(session):\n    sconf = {'session_name': session['session_name'], 'windows': []}\n\n    for w in session.windows:\n        wconf = {\n            'options': w.show_window_options(),\n            'window_name': w.name,\n            'layout': w.layout,\n            'panes': [],\n        }\n        if w.get('window_active', '0') == '1':\n            wconf['focus'] = 'true'\n\n        # If all panes have same path, set 'start_directory' instead\n        # of using 'cd' shell commands.\n        def pane_has_same_path(p):\n            return w.panes[0].current_path == p.current_path\n\n        if all(pane_has_same_path(p) for p in w.panes):\n            wconf['start_directory'] = w.panes[0].current_path\n\n        for p in w.panes:\n            pconf = {'shell_command': []}\n\n            if 'start_directory' not in wconf:\n                pconf['shell_command'].append('cd ' + p.current_path)\n\n            if p.get('pane_active', '0') == '1':\n                pconf['focus'] = 'true'\n\n            current_cmd = p.current_command\n\n            def filter_interpretters_and_shells():\n                return current_cmd.startswith('-') or any(\n                    current_cmd.endswith(cmd) for cmd in ['python', 'ruby', 'node']\n                )\n\n            if filter_interpretters_and_shells():\n                current_cmd = None\n\n            if current_cmd:\n                pconf['shell_command'].append(current_cmd)\n            else:\n                if not len(pconf['shell_command']):\n                    pconf = 'pane'\n\n            wconf['panes'].append(pconf)\n\n        sconf['windows'].append(wconf)\n\n    return sconf", "response": "Freeze a live tmux session and return a dict."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build(self, session=None):\n\n        if not session:\n            if not self.server:\n                raise exc.TmuxpException(\n                    'WorkspaceBuilder.build requires server to be passed '\n                    + 'on initialization, or pass in session object to here.'\n                )\n\n            if self.server.has_session(self.sconf['session_name']):\n                self.session = self.server.find_where(\n                    {'session_name': self.sconf['session_name']}\n                )\n                raise TmuxSessionExists(\n                    'Session name %s is already running.' % self.sconf['session_name']\n                )\n            else:\n                session = self.server.new_session(\n                    session_name=self.sconf['session_name']\n                )\n\n            assert self.sconf['session_name'] == session.name\n            assert len(self.sconf['session_name']) > 0\n\n        self.session = session\n        self.server = session.server\n\n        self.server._list_sessions()\n        assert self.server.has_session(session.name)\n        assert session.id\n\n        assert isinstance(session, Session)\n\n        focus = None\n\n        if 'before_script' in self.sconf:\n            try:\n                cwd = None\n\n                # we want to run the before_script file cwd'd from the\n                # session start directory, if it exists.\n                if 'start_directory' in self.sconf:\n                    cwd = self.sconf['start_directory']\n                run_before_script(self.sconf['before_script'], cwd=cwd)\n            except Exception as e:\n                self.session.kill_session()\n                raise e\n        if 'options' in self.sconf:\n            for option, value in self.sconf['options'].items():\n                self.session.set_option(option, value)\n        if 'global_options' in self.sconf:\n            for option, value in self.sconf['global_options'].items():\n                self.session.set_option(option, value, _global=True)\n        if 'environment' in self.sconf:\n            for option, value in self.sconf['environment'].items():\n                self.session.set_environment(option, value)\n\n        for w, wconf in self.iter_create_windows(session):\n            assert isinstance(w, Window)\n\n            focus_pane = None\n            for p, pconf in self.iter_create_panes(w, wconf):\n                assert isinstance(p, Pane)\n                p = p\n\n                if 'layout' in wconf:\n                    w.select_layout(wconf['layout'])\n\n                if 'focus' in pconf and pconf['focus']:\n                    focus_pane = p\n\n            if 'focus' in wconf and wconf['focus']:\n                focus = w\n\n            self.config_after_window(w, wconf)\n\n            if focus_pane:\n                focus_pane.select_pane()\n\n        if focus:\n            focus.select_window()", "response": "Builds tmux workspace in session."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\niterating through the tmuxp configuration dict and create windows.", "response": "def iter_create_windows(self, s):\n        \"\"\"\n        Return :class:`libtmux.Window` iterating through session config dict.\n\n        Generator yielding :class:`libtmux.Window` by iterating through\n        ``sconf['windows']``.\n\n        Applies ``window_options`` to window.\n\n        Parameters\n        ----------\n        session : :class:`libtmux.Session`\n            session to create windows in\n\n        Returns\n        -------\n        tuple of (:class:`libtmux.Window`, ``wconf``)\n            Newly created window, and the section from the tmuxp configuration\n            that was used to create the window.\n        \"\"\"\n        for i, wconf in enumerate(self.sconf['windows'], start=1):\n            if 'window_name' not in wconf:\n                window_name = None\n            else:\n                window_name = wconf['window_name']\n\n            w1 = None\n            if i == int(1):  # if first window, use window 1\n                w1 = s.attached_window\n                w1.move_window(99)\n                pass\n\n            if 'start_directory' in wconf:\n                sd = wconf['start_directory']\n            else:\n                sd = None\n\n            if 'window_shell' in wconf:\n                ws = wconf['window_shell']\n            else:\n                ws = None\n\n            w = s.new_window(\n                window_name=window_name,\n                start_directory=sd,\n                attach=False,  # do not move to the new window\n                window_index=wconf.get('window_index', ''),\n                window_shell=ws,\n            )\n\n            if i == int(1) and w1:  # if first window, use window 1\n                w1.kill_window()\n            assert isinstance(w, Window)\n            s.server._update_windows()\n            if 'options' in wconf and isinstance(wconf['options'], dict):\n                for key, val in wconf['options'].items():\n                    w.set_window_option(key, val)\n\n            if 'focus' in wconf and wconf['focus']:\n                w.select_window()\n\n            s.server._update_windows()\n\n            yield w, wconf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating through the config dict and create a new pane.", "response": "def iter_create_panes(self, w, wconf):\n        \"\"\"\n        Return :class:`libtmux.Pane` iterating through window config dict.\n\n        Run ``shell_command`` with ``$ tmux send-keys``.\n\n        Parameters\n        ----------\n        w : :class:`libtmux.Window`\n            window to create panes for\n        wconf : dict\n            config section for window\n\n        Returns\n        -------\n        tuple of (:class:`libtmux.Pane`, ``pconf``)\n            Newly created pane, and the section from the tmuxp configuration\n            that was used to create the pane.\n        \"\"\"\n        assert isinstance(w, Window)\n\n        pane_base_index = int(w.show_window_option('pane-base-index', g=True))\n\n        p = None\n\n        for pindex, pconf in enumerate(wconf['panes'], start=pane_base_index):\n            if pindex == int(pane_base_index):\n                p = w.attached_pane\n            else:\n\n                def get_pane_start_directory():\n\n                    if 'start_directory' in pconf:\n                        return pconf['start_directory']\n                    elif 'start_directory' in wconf:\n                        return wconf['start_directory']\n                    else:\n                        return None\n\n                p = w.split_window(\n                    attach=True, start_directory=get_pane_start_directory(), target=p.id\n                )\n\n            assert isinstance(p, Pane)\n            if 'layout' in wconf:\n                w.select_layout(wconf['layout'])\n\n            if 'suppress_history' in pconf:\n                suppress = pconf['suppress_history']\n            elif 'suppress_history' in wconf:\n                suppress = wconf['suppress_history']\n            else:\n                suppress = True\n\n            for cmd in pconf['shell_command']:\n                p.send_keys(cmd, suppress_history=suppress)\n\n            if 'focus' in pconf and pconf['focus']:\n                w.select_pane(p['pane_id'])\n\n            w.server._update_panes()\n\n            yield p, pconf"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\naction to apply to window after pane finished.", "response": "def config_after_window(self, w, wconf):\n        \"\"\"Actions to apply to window after window and pane finished.\n\n        When building a tmux session, sometimes its easier to postpone things\n        like setting options until after things are already structurally\n        prepared.\n\n        Parameters\n        ----------\n        w : :class:`libtmux.Window`\n            window to create panes for\n        wconf : dict\n            config section for window\n        \"\"\"\n        if 'options_after' in wconf and isinstance(wconf['options_after'], dict):\n            for key, val in wconf['options_after'].items():\n                w.set_window_option(key, val)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef debug_log_template(self, record):\n\n    reset = Style.RESET_ALL\n    levelname = (\n        LEVEL_COLORS.get(record.levelname)\n        + Style.BRIGHT\n        + '(%(levelname)1.1s)'\n        + Style.RESET_ALL\n        + ' '\n    )\n    asctime = (\n        '['\n        + Fore.BLACK\n        + Style.DIM\n        + Style.BRIGHT\n        + '%(asctime)s'\n        + Fore.RESET\n        + Style.RESET_ALL\n        + ']'\n    )\n    name = (\n        ' '\n        + Fore.WHITE\n        + Style.DIM\n        + Style.BRIGHT\n        + '%(name)s'\n        + Fore.RESET\n        + Style.RESET_ALL\n        + ' '\n    )\n    module_funcName = Fore.GREEN + Style.BRIGHT + '%(module)s.%(funcName)s()'\n    lineno = (\n        Fore.BLACK\n        + Style.DIM\n        + Style.BRIGHT\n        + ':'\n        + Style.RESET_ALL\n        + Fore.CYAN\n        + '%(lineno)d'\n    )\n\n    tpl = reset + levelname + asctime + name + module_funcName + lineno + reset\n\n    return tpl", "response": "Returns the prefix for the log message."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate_schema(sconf):\n\n    # verify session_name\n    if 'session_name' not in sconf:\n        raise exc.ConfigError('config requires \"session_name\"')\n\n    if 'windows' not in sconf:\n        raise exc.ConfigError('config requires list of \"windows\"')\n\n    for window in sconf['windows']:\n        if 'window_name' not in window:\n            raise exc.ConfigError('config window is missing \"window_name\"')\n\n        if 'panes' not in window:\n            raise exc.ConfigError(\n                'config window %s requires list of panes' % window['window_name']\n            )\n\n    return True", "response": "Validate that the config schema is correct."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_config_file(filename, extensions=['.yml', '.yaml', '.json']):\n    extensions = [extensions] if isinstance(extensions, string_types) else extensions\n    return any(filename.endswith(e) for e in extensions)", "response": "Checks if a file has a valid config file type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of configs in config_dir.", "response": "def in_dir(\n    config_dir=os.path.expanduser('~/.tmuxp'), extensions=['.yml', '.yaml', '.json']\n):\n    \"\"\"\n    Return a list of configs in ``config_dir``.\n\n    Parameters\n    ----------\n    config_dir : str\n        directory to search\n    extensions : list\n        filetypes to check (e.g. ``['.yaml', '.json']``).\n\n    Returns\n    -------\n    list\n    \"\"\"\n    configs = []\n\n    for filename in os.listdir(config_dir):\n        if is_config_file(filename, extensions) and not filename.startswith('.'):\n            configs.append(filename)\n\n    return configs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef in_cwd():\n    configs = []\n\n    for filename in os.listdir(os.getcwd()):\n        if filename.startswith('.tmuxp') and is_config_file(filename):\n            configs.append(filename)\n\n    return configs", "response": "Returns list of configs in current working directory."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning config in inline form, opposite of :meth:`config.expand`. Parameters ---------- sconf : dict Returns ------- dict configuration with optional inlined configs.", "response": "def inline(sconf):\n    \"\"\"\n    Return config in inline form, opposite of :meth:`config.expand`.\n\n    Parameters\n    ----------\n    sconf : dict\n\n    Returns\n    -------\n    dict\n        configuration with optional inlined configs.\n    \"\"\"\n\n    if (\n        'shell_command' in sconf\n        and isinstance(sconf['shell_command'], list)\n        and len(sconf['shell_command']) == 1\n    ):\n        sconf['shell_command'] = sconf['shell_command'][0]\n\n        if len(sconf.keys()) == int(1):\n            sconf = sconf['shell_command']\n    if (\n        'shell_command_before' in sconf\n        and isinstance(sconf['shell_command_before'], list)\n        and len(sconf['shell_command_before']) == 1\n    ):\n        sconf['shell_command_before'] = sconf['shell_command_before'][0]\n\n    # recurse into window and pane config items\n    if 'windows' in sconf:\n        sconf['windows'] = [inline(window) for window in sconf['windows']]\n    if 'panes' in sconf:\n        sconf['panes'] = [inline(pane) for pane in sconf['panes']]\n\n    return sconf"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nexpands the config file with shorthand and inline properties expanded.", "response": "def expand(sconf, cwd=None, parent=None):\n    \"\"\"Return config with shorthand and inline properties expanded.\n\n    This is necessary to keep the code in the :class:`WorkspaceBuilder` clean\n    and also allow for neat, short-hand configurations.\n\n    As a simple example, internally, tmuxp expects that config options\n    like ``shell_command`` are a list (array)::\n\n        'shell_command': ['htop']\n\n    tmuxp configs allow for it to be simply a string::\n\n        'shell_command': 'htop'\n\n    Kaptan will load JSON/YAML files into python dicts for you.\n\n    Parameters\n    ----------\n    sconf : dict\n        the configuration for the session\n    cwd : str\n        directory to expand relative paths against. should be the dir of the\n        config directory.\n    parent : str\n        (used on recursive entries) start_directory of parent window or session\n        object.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n\n    # Note: cli.py will expand configs relative to project's config directory\n    # for the first cwd argument.\n    if not cwd:\n        cwd = os.getcwd()\n\n    if 'session_name' in sconf:\n        sconf['session_name'] = expandshell(sconf['session_name'])\n    if 'window_name' in sconf:\n        sconf['window_name'] = expandshell(sconf['window_name'])\n    if 'environment' in sconf:\n        for key in sconf['environment']:\n            val = sconf['environment'][key]\n            val = expandshell(val)\n            if any(val.startswith(a) for a in ['.', './']):\n                val = os.path.normpath(os.path.join(cwd, val))\n            sconf['environment'][key] = val\n    if 'global_options' in sconf:\n        for key in sconf['global_options']:\n            val = sconf['global_options'][key]\n            if isinstance(val, string_types):\n                val = expandshell(val)\n                if any(val.startswith(a) for a in ['.', './']):\n                    val = os.path.normpath(os.path.join(cwd, val))\n            sconf['global_options'][key] = val\n    if 'options' in sconf:\n        for key in sconf['options']:\n            val = sconf['options'][key]\n            if isinstance(val, string_types):\n                val = expandshell(val)\n                if any(val.startswith(a) for a in ['.', './']):\n                    val = os.path.normpath(os.path.join(cwd, val))\n            sconf['options'][key] = val\n\n    # Any config section, session, window, pane that can contain the\n    # 'shell_command' value\n    if 'start_directory' in sconf:\n        sconf['start_directory'] = expandshell(sconf['start_directory'])\n        start_path = sconf['start_directory']\n        if any(start_path.startswith(a) for a in ['.', './']):\n            # if window has a session, or pane has a window with a\n            # start_directory of . or ./, make sure the start_directory can be\n            # relative to the parent.\n            #\n            # This is for the case where you may be loading a config from\n            # outside your shell current directory.\n            if parent:\n                cwd = parent['start_directory']\n            start_path = os.path.normpath(os.path.join(cwd, start_path))\n            sconf['start_directory'] = start_path\n\n    if 'before_script' in sconf:\n        sconf['before_script'] = expandshell(sconf['before_script'])\n        if any(sconf['before_script'].startswith(a) for a in ['.', './']):\n            sconf['before_script'] = os.path.normpath(\n                os.path.join(cwd, sconf['before_script'])\n            )\n\n    if 'shell_command' in sconf and isinstance(sconf['shell_command'], string_types):\n        sconf['shell_command'] = [sconf['shell_command']]\n\n    if 'shell_command_before' in sconf and isinstance(\n        sconf['shell_command_before'], string_types\n    ):\n        sconf['shell_command_before'] = [sconf['shell_command_before']]\n\n    if 'shell_command_before' in sconf and isinstance(\n        sconf['shell_command_before'], list\n    ):\n        sconf['shell_command_before'] = [\n            expandshell(scmd) for scmd in sconf['shell_command_before']\n        ]\n\n    # recurse into window and pane config items\n    if 'windows' in sconf:\n        sconf['windows'] = [expand(window, parent=sconf) for window in sconf['windows']]\n    elif 'panes' in sconf:\n\n        for pconf in sconf['panes']:\n            p_index = sconf['panes'].index(pconf)\n            p = copy.deepcopy(pconf)\n            pconf = sconf['panes'][p_index] = {}\n\n            if isinstance(p, string_types):\n                p = {'shell_command': [p]}\n            elif not p:\n                p = {'shell_command': []}\n\n            assert isinstance(p, dict)\n            if 'shell_command' in p:\n                cmd = p['shell_command']\n\n                if isinstance(p['shell_command'], string_types):\n                    cmd = [cmd]\n\n                if not cmd or any(a == cmd for a in [None, 'blank', 'pane']):\n                    cmd = []\n\n                if isinstance(cmd, list) and len(cmd) == int(1):\n                    if any(a in cmd for a in [None, 'blank', 'pane']):\n                        cmd = []\n\n                p['shell_command'] = cmd\n            else:\n                p['shell_command'] = []\n\n            pconf.update(p)\n        sconf['panes'] = [expand(pane, parent=sconf) for pane in sconf['panes']]\n\n    return sconf"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict with the trickled down and inherited config values.", "response": "def trickle(sconf):\n    \"\"\"Return a dict with \"trickled down\" / inherited config values.\n\n    This will only work if config has been expanded to full form with\n    :meth:`config.expand`.\n\n    tmuxp allows certain commands to be default at the session, window\n    level. shell_command_before trickles down and prepends the\n    ``shell_command`` for the pane.\n\n    Parameters\n    ----------\n    sconf : dict\n        the session configuration.\n\n    Returns\n    -------\n    dict\n    \"\"\"\n\n    # prepends a pane's ``shell_command`` list with the window and sessions'\n    # ``shell_command_before``.\n\n    if 'start_directory' in sconf:\n        session_start_directory = sconf['start_directory']\n    else:\n        session_start_directory = None\n\n    if 'suppress_history' in sconf:\n        suppress_history = sconf['suppress_history']\n    else:\n        suppress_history = None\n\n    for windowconfig in sconf['windows']:\n\n        # Prepend start_directory to relative window commands\n        if session_start_directory:\n            if 'start_directory' not in windowconfig:\n                windowconfig['start_directory'] = session_start_directory\n            else:\n                if not any(\n                    windowconfig['start_directory'].startswith(a) for a in ['~', '/']\n                ):\n                    window_start_path = os.path.join(\n                        session_start_directory, windowconfig['start_directory']\n                    )\n                    windowconfig['start_directory'] = window_start_path\n\n        # We only need to trickle to the window, workspace builder checks wconf\n        if suppress_history is not None:\n            if 'suppress_history' not in windowconfig:\n                windowconfig['suppress_history'] = suppress_history\n\n        for paneconfig in windowconfig['panes']:\n            commands_before = []\n\n            # Prepend shell_command_before to commands\n            if 'shell_command_before' in sconf:\n                commands_before.extend(sconf['shell_command_before'])\n            if 'shell_command_before' in windowconfig:\n                commands_before.extend(windowconfig['shell_command_before'])\n            if 'shell_command_before' in paneconfig:\n                commands_before.extend(paneconfig['shell_command_before'])\n\n            if 'shell_command' in paneconfig:\n                commands_before.extend(paneconfig['shell_command'])\n\n            p_index = windowconfig['panes'].index(paneconfig)\n            windowconfig['panes'][p_index]['shell_command'] = commands_before\n            # paneconfig['shell_command'] = commands_before\n\n    return sconf"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef import_tmuxinator(sconf):\n\n    tmuxp_config = {}\n\n    if 'project_name' in sconf:\n        tmuxp_config['session_name'] = sconf.pop('project_name')\n    elif 'name' in sconf:\n        tmuxp_config['session_name'] = sconf.pop('name')\n    else:\n        tmuxp_config['session_name'] = None\n\n    if 'project_root' in sconf:\n        tmuxp_config['start_directory'] = sconf.pop('project_root')\n    elif 'root' in sconf:\n        tmuxp_config['start_directory'] = sconf.pop('root')\n\n    if 'cli_args' in sconf:\n        tmuxp_config['config'] = sconf['cli_args']\n\n        if '-f' in tmuxp_config['config']:\n            tmuxp_config['config'] = tmuxp_config['config'].replace('-f', '').strip()\n    elif 'tmux_options' in sconf:\n        tmuxp_config['config'] = sconf['tmux_options']\n\n        if '-f' in tmuxp_config['config']:\n            tmuxp_config['config'] = tmuxp_config['config'].replace('-f', '').strip()\n\n    if 'socket_name' in sconf:\n        tmuxp_config['socket_name'] = sconf['socket_name']\n\n    tmuxp_config['windows'] = []\n\n    if 'tabs' in sconf:\n        sconf['windows'] = sconf.pop('tabs')\n\n    if 'pre' in sconf and 'pre_window' in sconf:\n        tmuxp_config['shell_command'] = sconf['pre']\n\n        if isinstance(sconf['pre'], string_types):\n            tmuxp_config['shell_command_before'] = [sconf['pre_window']]\n        else:\n            tmuxp_config['shell_command_before'] = sconf['pre_window']\n    elif 'pre' in sconf:\n        if isinstance(sconf['pre'], string_types):\n            tmuxp_config['shell_command_before'] = [sconf['pre']]\n        else:\n            tmuxp_config['shell_command_before'] = sconf['pre']\n\n    if 'rbenv' in sconf:\n        if 'shell_command_before' not in tmuxp_config:\n            tmuxp_config['shell_command_before'] = []\n        tmuxp_config['shell_command_before'].append('rbenv shell %s' % sconf['rbenv'])\n\n    for w in sconf['windows']:\n        for k, v in w.items():\n\n            windowdict = {'window_name': k}\n\n            if isinstance(v, string_types) or v is None:\n                windowdict['panes'] = [v]\n                tmuxp_config['windows'].append(windowdict)\n                continue\n            elif isinstance(v, list):\n                windowdict['panes'] = v\n                tmuxp_config['windows'].append(windowdict)\n                continue\n\n            if 'pre' in v:\n                windowdict['shell_command_before'] = v['pre']\n            if 'panes' in v:\n                windowdict['panes'] = v['panes']\n            if 'root' in v:\n                windowdict['start_directory'] = v['root']\n\n            if 'layout' in v:\n                windowdict['layout'] = v['layout']\n            tmuxp_config['windows'].append(windowdict)\n    return tmuxp_config", "response": "Return tmuxp config from a tmuxinator. yaml config."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nimport a tmuxp config from a teamocil config.", "response": "def import_teamocil(sconf):\n    \"\"\"Return tmuxp config from a `teamocil`_ yaml config.\n\n    .. _teamocil: https://github.com/remiprev/teamocil\n\n    Parameters\n    ----------\n    sconf : dict\n        python dict for session configuration\n\n    Notes\n    -----\n\n    Todos:\n\n    - change  'root' to a cd or start_directory\n    - width in pane -> main-pain-width\n    - with_env_var\n    - clear\n    - cmd_separator\n    \"\"\"\n\n    tmuxp_config = {}\n\n    if 'session' in sconf:\n        sconf = sconf['session']\n\n    if 'name' in sconf:\n        tmuxp_config['session_name'] = sconf['name']\n    else:\n        tmuxp_config['session_name'] = None\n\n    if 'root' in sconf:\n        tmuxp_config['start_directory'] = sconf.pop('root')\n\n    tmuxp_config['windows'] = []\n\n    for w in sconf['windows']:\n\n        windowdict = {'window_name': w['name']}\n\n        if 'clear' in w:\n            windowdict['clear'] = w['clear']\n\n        if 'filters' in w:\n            if 'before' in w['filters']:\n                for b in w['filters']['before']:\n                    windowdict['shell_command_before'] = w['filters']['before']\n            if 'after' in w['filters']:\n                for b in w['filters']['after']:\n                    windowdict['shell_command_after'] = w['filters']['after']\n\n        if 'root' in w:\n            windowdict['start_directory'] = w.pop('root')\n\n        if 'splits' in w:\n            w['panes'] = w.pop('splits')\n\n        if 'panes' in w:\n            for p in w['panes']:\n                if 'cmd' in p:\n                    p['shell_command'] = p.pop('cmd')\n                if 'width' in p:\n                    # todo support for height/width\n                    p.pop('width')\n            windowdict['panes'] = w['panes']\n\n        if 'layout' in w:\n            windowdict['layout'] = w['layout']\n        tmuxp_config['windows'].append(windowdict)\n\n    return tmuxp_config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfunctioning to wrap try / except for subprocess. check_call.", "response": "def run_before_script(script_file, cwd=None):\n    \"\"\"Function to wrap try/except for subprocess.check_call().\"\"\"\n    try:\n        proc = subprocess.Popen(\n            shlex.split(str(script_file)),\n            stderr=subprocess.PIPE,\n            stdout=subprocess.PIPE,\n            cwd=cwd,\n        )\n        for line in iter(proc.stdout.readline, b''):\n            sys.stdout.write(console_to_str(line))\n        proc.wait()\n\n        if proc.returncode:\n            stderr = proc.stderr.read()\n            proc.stderr.close()\n            stderr = console_to_str(stderr).split('\\n')\n            stderr = '\\n'.join(list(filter(None, stderr)))  # filter empty\n\n            raise exc.BeforeLoadScriptError(\n                proc.returncode, os.path.abspath(script_file), stderr\n            )\n\n        return proc.returncode\n    except OSError as e:\n        if e.errno == 2:\n            raise exc.BeforeLoadScriptNotExists(e, os.path.abspath(script_file))\n        else:\n            raise e"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if the user has set DISABLE_AUTO_TITLE to true.", "response": "def oh_my_zsh_auto_title():\n    \"\"\"Give warning and offer to fix ``DISABLE_AUTO_TITLE``.\n\n    see: https://github.com/robbyrussell/oh-my-zsh/pull/257\n\n    \"\"\"\n\n    if 'SHELL' in os.environ and 'zsh' in os.environ.get('SHELL'):\n        if os.path.exists(os.path.expanduser('~/.oh-my-zsh')):\n            # oh-my-zsh exists\n            if (\n                'DISABLE_AUTO_TITLE' not in os.environ\n                or os.environ.get('DISABLE_AUTO_TITLE') == \"false\"\n            ):\n                print(\n                    'Please set:\\n\\n'\n                    '\\texport DISABLE_AUTO_TITLE=\\'true\\'\\n\\n'\n                    'in ~/.zshrc or where your zsh profile is stored.\\n'\n                    'Remember the \"export\" at the beginning!\\n\\n'\n                    'Then create a new shell or type:\\n\\n'\n                    '\\t$ source ~/.zshrc'\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef setup():\n    l_mitogen = logging.getLogger('mitogen')\n    l_mitogen_io = logging.getLogger('mitogen.io')\n    l_ansible_mitogen = logging.getLogger('ansible_mitogen')\n\n    for logger in l_mitogen, l_mitogen_io, l_ansible_mitogen:\n        logger.handlers = [Handler(display.vvv)]\n        logger.propagate = False\n\n    if display.verbosity > 2:\n        l_ansible_mitogen.setLevel(logging.DEBUG)\n        l_mitogen.setLevel(logging.DEBUG)\n    else:\n        # Mitogen copies the active log level into new children, allowing them\n        # to filter tiny messages before they hit the network, and therefore\n        # before they wake the IO loop. Explicitly setting INFO saves ~4%\n        # running against just the local machine.\n        l_mitogen.setLevel(logging.ERROR)\n        l_ansible_mitogen.setLevel(logging.ERROR)\n\n    if display.verbosity > 3:\n        l_mitogen_io.setLevel(logging.DEBUG)", "response": "Setup the Ansible\n    logging framework."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dict_diff(old, new):\n    old_keys = viewkeys(old)\n    new_keys = viewkeys(dict(new))\n    out = {}\n    for key in new_keys - old_keys:\n        out[key] = new[key]\n    for key in old_keys - new_keys:\n        out[key] = None\n    for key in old_keys & new_keys:\n        if old[key] != new[key]:\n            out[key] = new[key]\n    return out", "response": "Return a dict representing the differences between the dicts old and new."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the default environment for an ansible local command.", "response": "def get_default_env(self):\n        \"\"\"\n        Vanilla Ansible local commands execute with an environment inherited\n        from WorkerProcess, we must emulate that.\n        \"\"\"\n        return dict_diff(\n            old=ansible_mitogen.process.MuxProcess.original_env,\n            new=os.environ,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add(self, recv):\n        if isinstance(recv, Select):\n            recv._check_no_loop(self)\n\n        self._receivers.append(recv)\n        if recv.notify is not None:\n            raise Error(self.owned_msg)\n\n        recv.notify = self._put\n        # Avoid race by polling once after installation.\n        if not recv.empty():\n            self._put(recv)", "response": "Add a receiver or a latch to the select."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving an object from the select.", "response": "def remove(self, recv):\n        \"\"\"\n        Remove an object from from the select. Note that if the receiver has\n        notified prior to :meth:`remove`, it will still be returned by a\n        subsequent :meth:`get`. This may change in a future version.\n        \"\"\"\n        try:\n            if recv.notify != self._put:\n                raise ValueError\n            self._receivers.remove(recv)\n            recv.notify = None\n        except (IndexError, ValueError):\n            raise Error(self.not_present_msg)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self):\n        for recv in self._receivers[:]:\n            self.remove(recv)\n        self._latch.close()", "response": "Close the latch and remove all receivers from the list of receivers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get(self, timeout=None, block=True):\n        return self.get_event(timeout, block).data", "response": "Get the next available event from the cache."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_event(self, timeout=None, block=True):\n        if not self._receivers:\n            raise Error(self.empty_msg)\n\n        event = Event()\n        while True:\n            recv = self._latch.get(timeout=timeout, block=block)\n            try:\n                if isinstance(recv, Select):\n                    event = recv.get_event(block=False)\n                else:\n                    event.source = recv\n                    event.data = recv.get(block=False)\n                if self._oneshot:\n                    self.remove(recv)\n                if isinstance(recv, mitogen.core.Receiver):\n                    # Remove in 0.3.x.\n                    event.data.receiver = recv\n                return event\n            except mitogen.core.TimeoutError:\n                # A receiver may have been queued with no result if another\n                # thread drained it before we woke up, or because another\n                # thread drained it between add() calling recv.empty() and\n                # self._put(). In this case just sleep again.\n                continue", "response": "Fetch the next available event from any source or raise a TimeoutError."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _stdlib_paths():\n    attr_candidates = [\n        'prefix',\n        'real_prefix',  # virtualenv: only set inside a virtual environment.\n        'base_prefix',  # venv: always set, equal to prefix if outside.\n    ]\n    prefixes = (getattr(sys, a) for a in attr_candidates if hasattr(sys, a))\n    version = 'python%s.%s' % sys.version_info[0:2]\n    return set(os.path.abspath(os.path.join(p, 'lib', version))\n               for p in prefixes)", "response": "Return a set of paths from which Python imports the standard library."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef is_stdlib_name(modname):\n    if imp.is_builtin(modname) != 0:\n        return True\n\n    module = sys.modules.get(modname)\n    if module is None:\n        return False\n\n    # six installs crap with no __file__\n    modpath = os.path.abspath(getattr(module, '__file__', ''))\n    return is_stdlib_path(modpath)", "response": "Return True if modname appears to come from the standard\n    library."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_child_modules(path):\n    it = pkgutil.iter_modules([os.path.dirname(path)])\n    return [to_text(name) for _, name, _ in it]", "response": "Return the suffixes of submodules directly neated beneath of the package\n    directory at path."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the core source code.", "response": "def _get_core_source():\n    \"\"\"\n    Master version of parent.get_core_source().\n    \"\"\"\n    source = inspect.getsource(mitogen.core)\n    return mitogen.minify.minimize_source(source)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scan_code_imports(co):\n    opit = iter_opcodes(co)\n    opit, opit2, opit3 = itertools.tee(opit, 3)\n\n    try:\n        next(opit2)\n        next(opit3)\n        next(opit3)\n    except StopIteration:\n        return\n\n    if sys.version_info >= (2, 5):\n        for oparg1, oparg2, (op3, arg3) in izip(opit, opit2, opit3):\n            if op3 == IMPORT_NAME:\n                op2, arg2 = oparg2\n                op1, arg1 = oparg1\n                if op1 == op2 == LOAD_CONST:\n                    yield (co.co_consts[arg1],\n                           co.co_names[arg3],\n                           co.co_consts[arg2] or ())\n    else:\n        # Python 2.4 did not yet have 'level', so stack format differs.\n        for oparg1, (op2, arg2) in izip(opit, opit2):\n            if op2 == IMPORT_NAME:\n                op1, arg1 = oparg1\n                if op1 == LOAD_CONST:\n                    yield (-1, co.co_names[arg2], co.co_consts[arg1] or ())", "response": "Given a code object co yields any LOAD_CONST instructions representing an Import_NAME and LOAD_FROM instructions representing an Import_FROM statement or ImportFrom statement."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _reset(cls):\n        if os.getpid() != cls._cls_pid:\n            cls._cls_pid = os.getpid()\n            cls._cls_instances_by_target.clear()\n            cls._cls_thread_by_target.clear()", "response": "Reset the watch dictionaries."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _looks_like_script(self, path):\n        fp = open(path, 'rb')\n        try:\n            sample = fp.read(512).decode('latin-1')\n            return not set(sample).difference(string.printable)\n        finally:\n            fp.close()", "response": "Return : data : True if the file at path resembles a Python script."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the path source and flag for the main module.", "response": "def _get_main_module_defective_python_3x(self, fullname):\n        \"\"\"\n        Recent versions of Python 3.x introduced an incomplete notion of\n        importer specs, and in doing so created permanent asymmetry in the\n        :mod:`pkgutil` interface handling for the `__main__` module. Therefore\n        we must handle `__main__` specially.\n        \"\"\"\n        if fullname != '__main__':\n            return None\n\n        mod = sys.modules.get(fullname)\n        if not mod:\n            return None\n\n        path = getattr(mod, '__file__', None)\n        if not (os.path.exists(path) and self._looks_like_script(path)):\n            return None\n\n        fp = open(path, 'rb')\n        try:\n            source = fp.read()\n        finally:\n            fp.close()\n\n        return path, source, False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_module_via_pkgutil(self, fullname):\n        try:\n            # Pre-'import spec' this returned None, in Python3.6 it raises\n            # ImportError.\n            loader = pkgutil.find_loader(fullname)\n        except ImportError:\n            e = sys.exc_info()[1]\n            LOG.debug('%r._get_module_via_pkgutil(%r): %s',\n                      self, fullname, e)\n            return None\n\n        IOLOG.debug('%r._get_module_via_pkgutil(%r) -> %r',\n                    self, fullname, loader)\n        if not loader:\n            return\n\n        try:\n            path = self._py_filename(loader.get_filename(fullname))\n            source = loader.get_source(fullname)\n            is_pkg = loader.is_package(fullname)\n        except (AttributeError, ImportError):\n            # - Per PEP-302, get_source() and is_package() are optional,\n            #   calling them may throw AttributeError.\n            # - get_filename() may throw ImportError if pkgutil.find_loader()\n            #   picks a \"parent\" package's loader for some crap that's been\n            #   stuffed in sys.modules, for example in the case of urllib3:\n            #       \"loader for urllib3.contrib.pyopenssl cannot handle\n            #        requests.packages.urllib3.contrib.pyopenssl\"\n            e = sys.exc_info()[1]\n            LOG.debug('%r: loading %r using %r failed: %s',\n                      self, fullname, loader, e)\n            return\n\n        if path is None or source is None:\n            return\n\n        if isinstance(source, mitogen.core.UnicodeType):\n            # get_source() returns \"string\" according to PEP-302, which was\n            # reinterpreted for Python 3 to mean a Unicode string.\n            source = source.encode('utf-8')\n\n        return path, source, is_pkg", "response": "Attempt to fetch source code via pkgutil."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nattempts to fetch source code via sys. modules.", "response": "def _get_module_via_sys_modules(self, fullname):\n        \"\"\"\n        Attempt to fetch source code via sys.modules. This is specifically to\n        support __main__, but it may catch a few more cases.\n        \"\"\"\n        module = sys.modules.get(fullname)\n        LOG.debug('_get_module_via_sys_modules(%r) -> %r', fullname, module)\n        if not isinstance(module, types.ModuleType):\n            LOG.debug('sys.modules[%r] absent or not a regular module',\n                      fullname)\n            return\n\n        path = self._py_filename(getattr(module, '__file__', ''))\n        if not path:\n            return\n\n        is_pkg = hasattr(module, '__path__')\n        try:\n            source = inspect.getsource(module)\n        except IOError:\n            # Work around inspect.getsourcelines() bug for 0-byte __init__.py\n            # files.\n            if not is_pkg:\n                raise\n            source = '\\n'\n\n        if isinstance(source, mitogen.core.UnicodeType):\n            # get_source() returns \"string\" according to PEP-302, which was\n            # reinterpreted for Python 3 to mean a Unicode string.\n            source = source.encode('utf-8')\n\n        return path, source, is_pkg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_module_via_parent_enumeration(self, fullname):\n        if fullname not in sys.modules:\n            # Don't attempt this unless a module really exists in sys.modules,\n            # else we could return junk.\n            return\n\n        pkgname, _, modname = str_rpartition(to_text(fullname), u'.')\n        pkg = sys.modules.get(pkgname)\n        if pkg is None or not hasattr(pkg, '__file__'):\n            return\n\n        pkg_path = os.path.dirname(pkg.__file__)\n        try:\n            fp, path, ext = imp.find_module(modname, [pkg_path])\n            try:\n                path = self._py_filename(path)\n                if not path:\n                    fp.close()\n                    return\n\n                source = fp.read()\n            finally:\n                if fp:\n                    fp.close()\n\n            if isinstance(source, mitogen.core.UnicodeType):\n                # get_source() returns \"string\" according to PEP-302, which was\n                # reinterpreted for Python 3 to mean a Unicode string.\n                source = source.encode('utf-8')\n            return path, source, False\n        except ImportError:\n            e = sys.exc_info()[1]\n            LOG.debug('imp.find_module(%r, %r) -> %s', modname, [pkg_path], e)", "response": "Try to fetch source code from a module via parent enumeration."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_source_override(self, fullname, path, source, is_pkg):\n        self._found_cache[fullname] = (path, source, is_pkg)", "response": "Add a source override entry to the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives the name of a loaded module fullname attempt to find its source code.", "response": "def get_module_source(self, fullname):\n        \"\"\"Given the name of a loaded module `fullname`, attempt to find its\n        source code.\n\n        :returns:\n            Tuple of `(module path, source text, is package?)`, or :data:`None`\n            if the source cannot be found.\n        \"\"\"\n        tup = self._found_cache.get(fullname)\n        if tup:\n            return tup\n\n        for method in self.get_module_methods:\n            tup = method(self, fullname)\n            if tup:\n                #LOG.debug('%r returned %r', method, tup)\n                break\n        else:\n            tup = None, None, None\n            LOG.debug('get_module_source(%r): cannot find source', fullname)\n\n        self._found_cache[fullname] = tup\n        return tup"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef resolve_relpath(self, fullname, level):\n        mod = sys.modules.get(fullname, None)\n        if hasattr(mod, '__path__'):\n            fullname += '.__init__'\n\n        if level == 0 or not fullname:\n            return ''\n\n        bits = fullname.split('.')\n        if len(bits) <= level:\n            # This would be an ImportError in real code.\n            return ''\n\n        return '.'.join(bits[:-level]) + '.'", "response": "Given an ImportFrom AST node and a canonical name return the canonical name."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_related_imports(self, fullname):\n        related = self._related_cache.get(fullname)\n        if related is not None:\n            return related\n\n        modpath, src, _ = self.get_module_source(fullname)\n        if src is None:\n            return []\n\n        maybe_names = list(self.generate_parent_names(fullname))\n\n        co = compile(src, modpath, 'exec')\n        for level, modname, namelist in scan_code_imports(co):\n            if level == -1:\n                modnames = [modname, '%s.%s' % (fullname, modname)]\n            else:\n                modnames = [\n                    '%s%s' % (self.resolve_relpath(fullname, level), modname)\n                ]\n\n            maybe_names.extend(modnames)\n            maybe_names.extend(\n                '%s.%s' % (mname, name)\n                for mname in modnames\n                for name in namelist\n            )\n\n        return self._related_cache.setdefault(fullname, sorted(\n            set(\n                mitogen.core.to_text(name)\n                for name in maybe_names\n                if sys.modules.get(name) is not None\n                and not is_stdlib_name(name)\n                and u'six.moves' not in name  # TODO: crap\n            )\n        ))", "response": "Find the list of non - stdlib modules that are directly imported by fullname plus their parents."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_related(self, fullname):\n        stack = [fullname]\n        found = set()\n\n        while stack:\n            name = stack.pop(0)\n            names = self.find_related_imports(name)\n            stack.extend(set(names).difference(set(found).union(stack)))\n            found.update(names)\n\n        found.discard(fullname)\n        return sorted(found)", "response": "Find all non - stdlib modules that are imported directly or by fullname plus their parents."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a source override to the set of modules.", "response": "def add_source_override(self, fullname, path, source, is_pkg):\n        \"\"\"\n        See :meth:`ModuleFinder.add_source_override.\n        \"\"\"\n        self._finder.add_source_override(fullname, path, source, is_pkg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving the source for the main module try to find where it is the main module and remove any code after that point. If it is not the main module raise ImportError.", "response": "def neutralize_main(self, path, src):\n        \"\"\"Given the source for the __main__ module, try to find where it\n        begins conditional execution based on a \"if __name__ == '__main__'\"\n        guard, and remove any code after that point.\"\"\"\n        match = self.MAIN_RE.search(src)\n        if match:\n            return src[:match.start()]\n\n        if b('mitogen.main(') in src:\n            return src\n\n        LOG.error(self.main_guard_msg, path)\n        raise ImportError('refused')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_stats(self):\n        return {\n            'get_module_count': self.responder.get_module_count,\n            'get_module_secs': self.responder.get_module_secs,\n            'good_load_module_count': self.responder.good_load_module_count,\n            'good_load_module_size': self.responder.good_load_module_size,\n            'bad_load_module_count': self.responder.bad_load_module_count,\n            'minify_secs': self.responder.minify_secs,\n        }", "response": "Return performance data for the module responder."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nallocates a unique context ID for the current active context.", "response": "def allocate(self):\n        \"\"\"\n        Arrange for a unique context ID to be allocated and associated with a\n        route leading to the active context. In masters, the ID is generated\n        directly, in children it is forwarded to the master via a\n        :data:`mitogen.core.ALLOCATE_ID` message.\n        \"\"\"\n        self.lock.acquire()\n        try:\n            id_ = self.next_id\n            self.next_id += 1\n            return id_\n        finally:\n            self.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnotify broker or pool of a given object.", "response": "def _notice_broker_or_pool(obj):\n    \"\"\"\n    Used by :mod:`mitogen.core` and :mod:`mitogen.service` to automatically\n    register every broker and pool on Python 2.4/2.5.\n    \"\"\"\n    if isinstance(obj, mitogen.core.Broker):\n        _brokers[obj] = True\n    else:\n        _pools[obj] = True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _cork_one(self, s, obj):\n        rsock, wsock = mitogen.parent.create_socketpair(size=4096)\n        mitogen.core.set_cloexec(rsock.fileno())\n        mitogen.core.set_cloexec(wsock.fileno())\n        mitogen.core.set_block(wsock)  # gevent\n        self._rsocks.append(rsock)\n        obj.defer(self._do_cork, s, wsock)", "response": "Construct a socketpair saving one side of it and passing the other to\n        obj to be written to by one of its threads."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _verify_one(self, rsock):\n        poller = mitogen.core.Poller()\n        poller.start_receive(rsock.fileno())\n        try:\n            while True:\n                for fd in poller.poll():\n                    return\n        finally:\n            poller.close()", "response": "Verify that the socket rsock is still alive."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\narranges for any associated brokers and pools to be paused with no locks held. This will not return until every thread acknowledges it.", "response": "def cork(self):\n        \"\"\"\n        Arrange for any associated brokers and pools to be paused with no locks\n        held. This will not return until each thread acknowledges it has ceased\n        execution.\n        \"\"\"\n        s = mitogen.core.b('CORK') * ((128 // 4) * 1024)\n        self._rsocks = []\n\n        # Pools must be paused first, as existing work may require the\n        # participation of a broker in order to complete.\n        for pool in self.pools:\n            if not pool.closed:\n                for x in range(pool.size):\n                    self._cork_one(s, pool)\n\n        for broker in self.brokers:\n            if broker._alive:\n                self._cork_one(s, broker)\n\n        # Pause until we can detect every thread has entered write().\n        for rsock in self._rsocks:\n            self._verify_one(rsock)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreconstructs a Module s canonical path by recursing through its parents.", "response": "def get_fullname(module):\n    \"\"\"\n    Reconstruct a Module's canonical path by recursing through its parents.\n    \"\"\"\n    bits = [str(module.name)]\n    while module.parent:\n        bits.append(str(module.parent.name))\n        module = module.parent\n    return '.'.join(reversed(bits))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_code(module):\n    fp = open(module.path)\n    try:\n        return compile(fp.read(), str(module.name), 'exec')\n    finally:\n        fp.close()", "response": "Compile and return a Module s code object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a module in the specified path.", "response": "def find(name, path=(), parent=None):\n    \"\"\"\n    Return a Module instance describing the first matching module found on the\n    search path.\n\n    :param str name:\n        Module name.\n    :param list path:\n        List of directory names to search for the module.\n    :param Module parent:\n        Optional module parent.\n    \"\"\"\n    assert isinstance(path, tuple)\n    head, _, tail = name.partition('.')\n    try:\n        tup = imp.find_module(head, list(path))\n    except ImportError:\n        return parent\n\n    fp, modpath, (suffix, mode, kind) = tup\n    if fp:\n        fp.close()\n\n    if parent and modpath == parent.path:\n        # 'from timeout import timeout', where 'timeout' is a function but also\n        # the name of the module being imported.\n        return None\n\n    if kind == imp.PKG_DIRECTORY:\n        modpath = os.path.join(modpath, '__init__.py')\n\n    module = Module(head, modpath, kind, parent)\n    # TODO: this code is entirely wrong on Python 3.x, but works well enough\n    # for Ansible. We need a new find_child() that only looks in the package\n    # directory, never falling back to the parent search path.\n    if tail and kind == imp.PKG_DIRECTORY:\n        return find_relative(module, tail, path)\n    return module"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef child_main(sender, delay):\n    args = ['ps', '-axwwo', 'user,pid,ppid,pgid,%cpu,rss,command']\n    while True:\n        sender.send(subprocess.check_output(args).decode())\n        time.sleep(delay)", "response": "This is the main thread of the child process. It is run in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlooping until CTRL+C is pressed, waiting for the next result delivered by the Select. Use parse_output() to turn that result ('ps' command output) into rich data, and finally repaint the screen if the repaint delay has passed.", "response": "def master_main(painter, router, select, delay):\n    \"\"\"\n    Loop until CTRL+C is pressed, waiting for the next result delivered by the\n    Select. Use parse_output() to turn that result ('ps' command output) into\n    rich data, and finally repaint the screen if the repaint delay has passed.\n    \"\"\"\n    next_paint = 0\n    while True:\n        msg = select.get()\n        parse_output(msg.receiver.host, msg.unpickle())\n        if next_paint < time.time():\n            next_paint = time.time() + delay\n            painter.paint()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main(router):\n    argv = sys.argv[1:]\n    if not len(argv):\n        print('mitop: Need a list of SSH hosts to connect to.')\n        sys.exit(1)\n\n    delay = 2.0\n    select = mitogen.select.Select(oneshot=False)\n    hosts = []\n\n    # For each hostname on the command line, create a Host instance, a Mitogen\n    # connection, a Receiver to accept messages from the host, and finally\n    # start child_main() on the host to pump messages into the receiver.\n    for hostname in argv:\n        print('Starting on', hostname)\n        host = Host()\n        host.name = hostname\n\n        if host.name == 'localhost':\n            host.context = router.local()\n        else:\n            host.context = router.ssh(hostname=host.name)\n\n        # A receiver wires up a handle (via Router.add_handler()) to an\n        # internal thread-safe queue object, which can be drained through calls\n        # to recv.get().\n        host.recv = mitogen.core.Receiver(router)\n        host.recv.host = host\n\n        # But we don't want to receive data from just one receiver, we want to\n        # receive data from many. In this case we can use a Select(). It knows\n        # how to efficiently sleep while waiting for the first message sent to\n        # many receivers.\n        select.add(host.recv)\n\n        # The inverse of a Receiver is a Sender. Unlike receivers, senders are\n        # serializable, so we can call the .to_sender() helper method to create\n        # one equivalent to our host's receiver, and pass it directly to the\n        # host as a function parameter.\n        sender = host.recv.to_sender()\n\n        # Finally invoke the function in the remote target. Since child_main()\n        # is an infinite loop, using .call() would block the parent, since\n        # child_main() never returns. Instead use .call_async(), which returns\n        # another Receiver. We also want to wait for results from it --\n        # although child_main() never returns, if it crashes the exception will\n        # be delivered instead.\n        call_recv = host.context.call_async(child_main, sender, delay)\n        call_recv.host = host\n\n        # Adding call_recv to the select will cause mitogen.core.CallError to\n        # be thrown by .get() if startup of any context fails, causing halt of\n        # master_main(), and the exception to be printed.\n        select.add(call_recv)\n        hosts.append(host)\n\n    # Painter just wraps up all the prehistory ncurses code and keeps it out of\n    # master_main().\n    painter = Painter(hosts)\n    try:\n        try:\n            master_main(painter, router, select, delay)\n        except KeyboardInterrupt:\n            # Shut down gracefully when the user presses CTRL+C.\n            pass\n    finally:\n        painter.close()", "response": "Main entry point for the main program."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef key_from_dict(**kwargs):\n    out = []\n    stack = [kwargs]\n    while stack:\n        obj = stack.pop()\n        if isinstance(obj, dict):\n            stack.extend(sorted(obj.items()))\n        elif isinstance(obj, (list, tuple)):\n            stack.extend(obj)\n        else:\n            out.append(str(obj))\n    return ''.join(out)", "response": "Return a unique string representation of a dict as quickly as possible."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresets the underlying connection to the given context.", "response": "def reset(self, context):\n        \"\"\"\n        Return a reference, forcing close and discard of the underlying\n        connection. Used for 'meta: reset_connection' or when some other error\n        is detected.\n        \"\"\"\n        LOG.debug('%r.reset(%r)', self, context)\n        self._lock.acquire()\n        try:\n            self._shutdown_unlocked(context)\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a reference to a specific context.", "response": "def put(self, context):\n        \"\"\"\n        Return a reference, making it eligable for recycling once its reference\n        count reaches zero.\n        \"\"\"\n        LOG.debug('%r.put(%r)', self, context)\n        self._lock.acquire()\n        try:\n            if self._refs_by_context.get(context, 0) == 0:\n                LOG.warning('%r.put(%r): refcount was 0. shutdown_all called?',\n                            self, context)\n                return\n            self._refs_by_context[context] -= 1\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreplying to every waiting request matching a configuration key with a response dictionary, deleting the list of waiters when done. :param str key: Result of :meth:`key_from_dict` :param dict response: Response dictionary :returns: Number of waiters that were replied to.", "response": "def _produce_response(self, key, response):\n        \"\"\"\n        Reply to every waiting request matching a configuration key with a\n        response dictionary, deleting the list of waiters when done.\n\n        :param str key:\n            Result of :meth:`key_from_dict`\n        :param dict response:\n            Response dictionary\n        :returns:\n            Number of waiters that were replied to.\n        \"\"\"\n        self._lock.acquire()\n        try:\n            latches = self._latches_by_key.pop(key)\n            count = len(latches)\n            for latch in latches:\n                latch.put(response)\n        finally:\n            self._lock.release()\n        return count"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nshutting down a context and optionally add new_context to the LRU list while holding the lock.", "response": "def _shutdown_unlocked(self, context, lru=None, new_context=None):\n        \"\"\"\n        Arrange for `context` to be shut down, and optionally add `new_context`\n        to the LRU list while holding the lock.\n        \"\"\"\n        LOG.info('%r._shutdown_unlocked(): shutting down %r', self, context)\n        context.shutdown()\n        via = self._via_by_context.get(context)\n        if via:\n            lru = self._lru_by_via.get(via)\n            if lru:\n                if context in lru:\n                    lru.remove(context)\n                if new_context:\n                    lru.append(new_context)\n        self._forget_context_unlocked(context)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the LRU list associated with the connection described by kwargs.", "response": "def _update_lru_unlocked(self, new_context, spec, via):\n        \"\"\"\n        Update the LRU (\"MRU\"?) list associated with the connection described\n        by `kwargs`, destroying the most recently created context if the list\n        is full. Finally add `new_context` to the list.\n        \"\"\"\n        self._via_by_context[new_context] = via\n\n        lru = self._lru_by_via.setdefault(via, [])\n        if len(lru) < self.max_interpreters:\n            lru.append(new_context)\n            return\n\n        for context in reversed(lru):\n            if self._refs_by_context[context] == 0:\n                break\n        else:\n            LOG.warning('via=%r reached maximum number of interpreters, '\n                        'but they are all marked as in-use.', via)\n            return\n\n        self._shutdown_unlocked(context, lru=lru, new_context=new_context)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of dicts describing every currently connected context.", "response": "def dump(self):\n        \"\"\"\n        For testing, return a list of dicts describing every currently\n        connected context.\n        \"\"\"\n        return [\n            {\n                'context_name': context.name,\n                'via': getattr(self._via_by_context.get(context),\n                               'name', None),\n                'refs': self._refs_by_context.get(context),\n            }\n            for context, key in sorted(self._key_by_context.items(),\n                                       key=lambda c_k: c_k[0].context_id)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shutdown_all(self):\n        self._lock.acquire()\n        try:\n            for context in list(self._key_by_context):\n                self._shutdown_unlocked(context)\n        finally:\n            self._lock.release()", "response": "Shut down all active and all active items."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _on_context_disconnect(self, context):\n        self._lock.acquire()\n        try:\n            LOG.info('%r: Forgetting %r due to stream disconnect', self, context)\n            self._forget_context_unlocked(context)\n        finally:\n            self._lock.release()", "response": "Forget about the noon context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _connect(self, key, spec, via=None):\n        try:\n            method = getattr(self.router, spec['method'])\n        except AttributeError:\n            raise Error('unsupported method: %(transport)s' % spec)\n\n        context = method(via=via, unidirectional=True, **spec['kwargs'])\n        if via and spec.get('enable_lru'):\n            self._update_lru(context, spec, via)\n\n        # Forget the context when its disconnect event fires.\n        mitogen.core.listen(context, 'disconnect',\n            lambda: self._on_context_disconnect(context))\n\n        self._send_module_forwards(context)\n        init_child_result = context.call(\n            ansible_mitogen.target.init_child,\n            log_level=LOG.getEffectiveLevel(),\n            candidate_temp_dirs=self._get_candidate_temp_dirs(),\n        )\n\n        if os.environ.get('MITOGEN_DUMP_THREAD_STACKS'):\n            from mitogen import debug\n            context.call(debug.dump_to_logger)\n\n        self._key_by_context[context] = key\n        self._refs_by_context[context] = 0\n        return {\n            'context': context,\n            'via': via,\n            'init_child_result': init_child_result,\n            'msg': None,\n        }", "response": "Connects to a specific Mitogen instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, msg, stack):\n        via = None\n        for spec in stack:\n            try:\n                result = self._wait_or_start(spec, via=via).get()\n                if isinstance(result, tuple):  # exc_info()\n                    reraise(*result)\n                via = result['context']\n            except mitogen.core.ChannelError:\n                return {\n                    'context': None,\n                    'init_child_result': None,\n                    'method_name': spec['method'],\n                    'msg': self.disconnect_msg,\n                }\n            except mitogen.core.StreamError as e:\n                return {\n                    'context': None,\n                    'init_child_result': None,\n                    'method_name': spec['method'],\n                    'msg': str(e),\n                }\n\n        return result", "response": "Return a Context referring to an established connection with the given message and stack."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npartitions function for Python 2. 4. 5.", "response": "def _partition(s, sep, find):\n    \"\"\"\n    (str|unicode).(partition|rpartition) for Python 2.4/2.5.\n    \"\"\"\n    idx = find(sep)\n    if idx != -1:\n        left = s[0:idx]\n        return left, sep, s[len(left)+len(sep):]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\narrange for func to be invoked when the named signal is fired by obj.", "response": "def listen(obj, name, func):\n    \"\"\"\n    Arrange for `func(*args, **kwargs)` to be invoked when the named signal is\n    fired by `obj`.\n    \"\"\"\n    signals = vars(obj).setdefault('_signals', {})\n    signals.setdefault(name, []).append(func)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef fire(obj, name, *args, **kwargs):\n    signals = vars(obj).get('_signals', {})\n    for func in signals.get(name, ()):\n        func(*args, **kwargs)", "response": "Arrange for func(*args ** kwargs ) to be invoked for every function that is registered for the named signal on obj."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the given fullname is part of a blacklisted package.", "response": "def is_blacklisted_import(importer, fullname):\n    \"\"\"\n    Return :data:`True` if `fullname` is part of a blacklisted package, or if\n    any packages have been whitelisted and `fullname` is not part of one.\n\n    NB:\n      - If a package is on both lists, then it is treated as blacklisted.\n      - If any package is whitelisted, then all non-whitelisted packages are\n        treated as blacklisted.\n    \"\"\"\n    return ((not any(fullname.startswith(s) for s in importer.whitelist)) or\n                (any(fullname.startswith(s) for s in importer.blacklist)))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_cloexec(fd):\n    flags = fcntl.fcntl(fd, fcntl.F_GETFD)\n    assert fd > 2\n    fcntl.fcntl(fd, fcntl.F_SETFD, flags | fcntl.FD_CLOEXEC)", "response": "Set the file descriptor fd to automatically close on\n   ."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_block(fd):\n    flags = fcntl.fcntl(fd, fcntl.F_GETFL)\n    fcntl.fcntl(fd, fcntl.F_SETFL, flags & ~os.O_NONBLOCK)", "response": "Inverse of set_nonblock i. e. cause fd to block the thread\n    when the underlying kernel buffer is exhausted."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef io_op(func, *args):\n    while True:\n        try:\n            return func(*args), None\n        except (select.error, OSError, IOError):\n            e = sys.exc_info()[1]\n            _vv and IOLOG.debug('io_op(%r) -> OSError: %s', func, e)\n            if e.args[0] == errno.EINTR:\n                continue\n            if e.args[0] in (errno.EIO, errno.ECONNRESET, errno.EPIPE):\n                return None, e\n            raise", "response": "Wrap a function that may raise an exception and return the result of the next IO operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfinds the class implementing module_name. class_name or raise if the module is not whitelisted.", "response": "def _find_global(self, module, func):\n        \"\"\"Return the class implementing `module_name.class_name` or raise\n        `StreamError` if the module is not whitelisted.\"\"\"\n        if module == __name__:\n            if func == '_unpickle_call_error' or func == 'CallError':\n                return _unpickle_call_error\n            elif func == '_unpickle_sender':\n                return self._unpickle_sender\n            elif func == '_unpickle_context':\n                return self._unpickle_context\n            elif func == 'Blob':\n                return Blob\n            elif func == 'Secret':\n                return Secret\n            elif func == 'Kwargs':\n                return Kwargs\n        elif module == '_codecs' and func == 'encode':\n            return self._unpickle_bytes\n        elif module == '__builtin__' and func == 'bytes':\n            return BytesType\n        raise StreamError('cannot unpickle %r/%r', module, func)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef dead(cls, reason=None, **kwargs):\n        kwargs['data'], _ = UTF8_CODEC.encode(reason or u'')\n        return cls(reply_to=IS_DEAD, **kwargs)", "response": "Syntax helper to construct a dead message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pickled(cls, obj, **kwargs):\n        self = cls(**kwargs)\n        try:\n            self.data = pickle__dumps(obj, protocol=2)\n        except pickle.PicklingError:\n            e = sys.exc_info()[1]\n            self.data = pickle__dumps(CallError(e), protocol=2)\n        return self", "response": "Constructs a pickled version of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives a message and a router and optional keyword arguments this method will send it to the router.", "response": "def reply(self, msg, router=None, **kwargs):\n        \"\"\"\n        Compose a reply to this message and send it using :attr:`router`, or\n        `router` is :attr:`router` is :data:`None`.\n\n        :param obj:\n            Either a :class:`Message`, or an object to be serialized in order\n            to construct a new message.\n        :param router:\n            Optional router to use if :attr:`router` is :data:`None`.\n        :param kwargs:\n            Optional keyword parameters overriding message fields in the reply.\n        \"\"\"\n        if not isinstance(msg, Message):\n            msg = Message.pickled(msg)\n        msg.dst_id = self.src_id\n        msg.handle = self.reply_to\n        vars(msg).update(kwargs)\n        if msg.handle:\n            (self.router or router).route(msg)\n        else:\n            LOG.debug('Message.reply(): discarding due to zero handle: %r', msg)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef unpickle(self, throw=True, throw_dead=True):\n        _vv and IOLOG.debug('%r.unpickle()', self)\n        if throw_dead and self.is_dead:\n            self._throw_dead()\n\n        obj = self._unpickled\n        if obj is Message._unpickled:\n            fp = BytesIO(self.data)\n            unpickler = _Unpickler(fp, **self.UNPICKLER_KWARGS)\n            unpickler.find_global = self._find_global\n            try:\n                # Must occur off the broker thread.\n                obj = unpickler.load()\n                self._unpickled = obj\n            except (TypeError, ValueError):\n                e = sys.exc_info()[1]\n                raise StreamError('invalid message: %s', e)\n\n        if throw:\n            if isinstance(obj, CallError):\n                raise obj\n\n        return obj", "response": "Unpickle :attr:`data`, optionally raising any exceptions present.\n\n        :param bool throw_dead:\n            If :data:`True`, raise exceptions, otherwise it is the caller's\n            responsibility.\n\n        :raises CallError:\n            The serialized data contained CallError exception.\n        :raises ChannelError:\n            The `is_dead` field was set."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef send(self, data):\n        _vv and IOLOG.debug('%r.send(%r..)', self, repr(data)[:100])\n        self.context.send(Message.pickled(data, handle=self.dst_handle))", "response": "Send data to the remote end."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        _vv and IOLOG.debug('%r.close()', self)\n        self.context.send(\n            Message.dead(\n                reason=self.explicit_close_msg,\n                handle=self.dst_handle\n            )\n        )", "response": "Send a dead message to the remote."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _on_receive(self, msg):\n        _vv and IOLOG.debug('%r._on_receive(%r)', self, msg)\n        self._latch.put(msg)\n        if self.notify:\n            self.notify(self)", "response": "Called when a message is received from the router."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a message from the receiver.", "response": "def get(self, timeout=None, block=True, throw_dead=True):\n        \"\"\"\n        Sleep waiting for a message to arrive on this receiver.\n\n        :param float timeout:\n            If not :data:`None`, specifies a timeout in seconds.\n\n        :raises mitogen.core.ChannelError:\n            The remote end indicated the channel should be closed,\n            communication with it was lost, or :meth:`close` was called in the\n            local process.\n\n        :raises mitogen.core.TimeoutError:\n            Timeout was reached.\n\n        :returns:\n            :class:`Message` that was received.\n        \"\"\"\n        _vv and IOLOG.debug('%r.get(timeout=%r, block=%r)', self, timeout, block)\n        try:\n            msg = self._latch.get(timeout=timeout, block=block)\n        except LatchError:\n            raise ChannelError(self.closed_msg)\n        if msg.is_dead and throw_dead:\n            msg._throw_dead()\n        return msg"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nupdate the linecache cache for a module.", "response": "def _update_linecache(self, path, data):\n        \"\"\"\n        The Python 2.4 linecache module, used to fetch source code for\n        tracebacks and :func:`inspect.getsource`, does not support PEP-302,\n        meaning it needs extra help to for Mitogen-loaded modules. Directly\n        populate its cache if a loaded module belongs to the Mitogen package.\n        \"\"\"\n        if PY24 and 'mitogen' in path:\n            linecache.cache[path] = (\n                len(data),\n                0.0,\n                [line+'\\n' for line in data.splitlines()],\n                path,\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef uncork(self):\n        self._send = self.context.send\n        for msg in self._buffer:\n            self._send(msg)\n        self._buffer = None", "response": "Unroutes all buffered messages."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if not self.closed:\n            _vv and IOLOG.debug('%r.close()', self)\n            self.closed = True\n            os.close(self.fd)", "response": "Close the file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read(self, n=CHUNK_SIZE):\n        if self.closed:\n            # Refuse to touch the handle after closed, it may have been reused\n            # by another thread. TODO: synchronize read()/write()/close().\n            return b('')\n        s, disconnected = io_op(os.read, self.fd, n)\n        if disconnected:\n            LOG.debug('%r.read(): disconnected: %s', self, disconnected)\n            return b('')\n        return s", "response": "Reads up to n bytes from the file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef write(self, s):\n        if self.closed or self.fd is None:\n            # Refuse to touch the handle after closed, it may have been reused\n            # by another thread.\n            return None\n\n        written, disconnected = io_op(os.write, self.fd, s)\n        if disconnected:\n            LOG.debug('%r.write(): disconnected: %s', self, disconnected)\n            return None\n        return written", "response": "Writes a string to the file descriptor and returns the number of bytes written or None if the file descriptor is closed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall by the broker when the stream time to gracefully shut down.", "response": "def on_shutdown(self, broker):\n        \"\"\"\n        Called by :meth:`Broker.shutdown` to allow the stream time to\n        gracefully shutdown. The base implementation simply called\n        :meth:`on_disconnect`.\n        \"\"\"\n        _v and LOG.debug('%r.on_shutdown()', self)\n        fire(self, 'shutdown')\n        self.on_disconnect(broker)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_disconnect(self, broker):\n        LOG.debug('%r.on_disconnect()', self)\n        if self.receive_side:\n            broker.stop_receive(self)\n            self.receive_side.close()\n        if self.transmit_side:\n            broker._stop_transmit(self)\n            self.transmit_side.close()\n        fire(self, 'disconnect')", "response": "Called by the broker when the broker disconnects from the broker."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, msg):\n        self._router.broker.defer(self._send, msg)", "response": "Send data to handle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_async(self, msg, persist=False):\n        if self.router.broker._thread == threading.currentThread():  # TODO\n            raise SystemError('Cannot making blocking call on broker thread')\n\n        receiver = Receiver(self.router, persist=persist, respondent=self)\n        msg.dst_id = self.context_id\n        msg.reply_to = receiver.handle\n\n        _v and LOG.debug('%r.send_async(%r)', self, msg)\n        self.send(msg)\n        return receiver", "response": "Arrange for msg to be delivered to this context with replies\n            directed to a newly constructed Receiver."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\narrange for msg to be delivered to this context.", "response": "def send(self, msg):\n        \"\"\"\n        Arrange for `msg` to be delivered to this context. :attr:`dst_id\n        <Message.dst_id>` is set to the target context ID.\n\n        :param Message msg:\n            Message.\n        \"\"\"\n        msg.dst_id = self.context_id\n        self.router.route(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends a message and wait for a reply.", "response": "def send_await(self, msg, deadline=None):\n        \"\"\"\n        Like :meth:`send_async`, but expect a single reply (`persist=False`)\n        delivered within `deadline` seconds.\n\n        :param mitogen.core.Message msg:\n            The message.\n        :param float deadline:\n            If not :data:`None`, seconds before timing out waiting for a reply.\n        :returns:\n            Deserialized reply.\n        :raises TimeoutError:\n            No message was received and `deadline` passed.\n        \"\"\"\n        receiver = self.send_async(msg)\n        response = receiver.get(deadline)\n        data = response.unpickle()\n        _vv and IOLOG.debug('%r._send_await() -> %r', self, data)\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of tuples for every FD registered for receive readiness.", "response": "def readers(self):\n        \"\"\"\n        Return a list of `(fd, data)` tuples for every FD registered for\n        receive readiness.\n        \"\"\"\n        return list((fd, data) for fd, (data, gen) in self._rfds.items())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef writers(self):\n        return list((fd, data) for fd, (data, gen) in self._wfds.items())", "response": "Return a list of tuples for every FD registered for\n        transmit readiness."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart receiving data from a file descriptor.", "response": "def start_receive(self, fd, data=None):\n        \"\"\"\n        Cause :meth:`poll` to yield `data` when `fd` is readable.\n        \"\"\"\n        self._rfds[fd] = (data or fd, self._generation)\n        self._update(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstops yielding readability events for fd.", "response": "def stop_receive(self, fd):\n        \"\"\"\n        Stop yielding readability events for `fd`.\n\n        Redundant calls to :meth:`stop_receive` are silently ignored, this may\n        change in future.\n        \"\"\"\n        self._rfds.pop(fd, None)\n        self._update(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef start_transmit(self, fd, data=None):\n        self._wfds[fd] = (data or fd, self._generation)\n        self._update(fd)", "response": "Start transmitting data from a file descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop_transmit(self, fd):\n        self._wfds.pop(fd, None)\n        self._update(fd)", "response": "Stop yielding writeability events for fd."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef poll(self, timeout=None):\n        _vv and IOLOG.debug('%r.poll(%r)', self, timeout)\n        self._generation += 1\n        return self._poll(timeout)", "response": "Block the calling thread until one or more FDs are ready for IO."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncleans up any files belonging to the parent process after a fork.", "response": "def _on_fork(cls):\n        \"\"\"\n        Clean up any files belonging to the parent process after a fork.\n        \"\"\"\n        cls._cls_idle_socketpairs = []\n        while cls._cls_all_sockets:\n            cls._cls_all_sockets.pop().close()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef close(self):\n        self._lock.acquire()\n        try:\n            self.closed = True\n            while self._waking < len(self._sleeping):\n                wsock, cookie = self._sleeping[self._waking]\n                self._wake(wsock, cookie)\n                self._waking += 1\n        finally:\n            self._lock.release()", "response": "Close the latch and wake all sleeping threads."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef empty(self):\n        self._lock.acquire()\n        try:\n            if self.closed:\n                raise LatchError()\n            return len(self._queue) == 0\n        finally:\n            self._lock.release()", "response": "Returns True if the queue is empty."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_socketpair(self):\n        try:\n            return self._cls_idle_socketpairs.pop()  # pop() must be atomic\n        except IndexError:\n            rsock, wsock = socket.socketpair()\n            set_cloexec(rsock.fileno())\n            set_cloexec(wsock.fileno())\n            self._cls_all_sockets.extend((rsock, wsock))\n            return rsock, wsock", "response": "Return an unused socketpair creating one if none exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a string encoding the process instance and thread ID.", "response": "def _make_cookie(self):\n        \"\"\"\n        Return a string encoding the ID of the process, instance and thread.\n        This disambiguates legitimate wake-ups, accidental writes to the FD,\n        and buggy internal FD sharing.\n        \"\"\"\n        return struct.pack(self.COOKIE_FMT, self.COOKIE_MAGIC,\n                           os.getpid(), id(self), thread.get_ident())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets the next enqueued object or sleep waiting for one.", "response": "def get(self, timeout=None, block=True):\n        \"\"\"\n        Return the next enqueued object, or sleep waiting for one.\n\n        :param float timeout:\n            If not :data:`None`, specifies a timeout in seconds.\n\n        :param bool block:\n            If :data:`False`, immediately raise\n            :class:`mitogen.core.TimeoutError` if the latch is empty.\n\n        :raises mitogen.core.LatchError:\n            :meth:`close` has been called, and the object is no longer valid.\n\n        :raises mitogen.core.TimeoutError:\n            Timeout was reached.\n\n        :returns:\n            The de-queued object.\n        \"\"\"\n        _vv and IOLOG.debug('%r.get(timeout=%r, block=%r)',\n                            self, timeout, block)\n        self._lock.acquire()\n        try:\n            if self.closed:\n                raise LatchError()\n            i = len(self._sleeping)\n            if len(self._queue) > i:\n                _vv and IOLOG.debug('%r.get() -> %r', self, self._queue[i])\n                return self._queue.pop(i)\n            if not block:\n                raise TimeoutError()\n            rsock, wsock = self._get_socketpair()\n            cookie = self._make_cookie()\n            self._sleeping.append((wsock, cookie))\n        finally:\n            self._lock.release()\n\n        poller = self.poller_class()\n        poller.start_receive(rsock.fileno())\n        try:\n            return self._get_sleep(poller, timeout, block, rsock, wsock, cookie)\n        finally:\n            poller.close()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_sleep(self, poller, timeout, block, rsock, wsock, cookie):\n        _vv and IOLOG.debug(\n            '%r._get_sleep(timeout=%r, block=%r, rfd=%d, wfd=%d)',\n            self, timeout, block, rsock.fileno(), wsock.fileno()\n        )\n\n        e = None\n        woken = None\n        try:\n            woken = list(poller.poll(timeout))\n        except Exception:\n            e = sys.exc_info()[1]\n\n        self._lock.acquire()\n        try:\n            i = self._sleeping.index((wsock, cookie))\n            del self._sleeping[i]\n            if not woken:\n                raise e or TimeoutError()\n\n            got_cookie = rsock.recv(self.COOKIE_SIZE)\n            self._cls_idle_socketpairs.append((rsock, wsock))\n\n            assert cookie == got_cookie, (\n                \"Cookie incorrect; got %r, expected %r\" \\\n                % (binascii.hexlify(got_cookie),\n                   binascii.hexlify(cookie))\n            )\n            assert i < self._waking, (\n                \"Cookie correct, but no queue element assigned.\"\n            )\n            self._waking -= 1\n            if self.closed:\n                raise LatchError()\n            _vv and IOLOG.debug('%r.get() wake -> %r', self, self._queue[i])\n            return self._queue.pop(i)\n        finally:\n            self._lock.release()", "response": "Get a wake - up entry from the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, obj=None):\n        _vv and IOLOG.debug('%r.put(%r)', self, obj)\n        self._lock.acquire()\n        try:\n            if self.closed:\n                raise LatchError()\n            self._queue.append(obj)\n\n            if self._waking < len(self._sleeping):\n                wsock, cookie = self._sleeping[self._waking]\n                self._waking += 1\n                _vv and IOLOG.debug('%r.put() -> waking wfd=%r',\n                                    self, wsock.fileno())\n                self._wake(wsock, cookie)\n            elif self.notify:\n                self.notify(self)\n        finally:\n            self._lock.release()", "response": "Enqueue an object into the queue wake the first thread waiting if one is waiting for a result."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef keep_alive(self):\n        self._lock.acquire()\n        try:\n            return len(self._deferred)\n        finally:\n            self._lock.release()", "response": "Return the number of deferred functions that are still alive."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef on_receive(self, broker):\n        _vv and IOLOG.debug('%r.on_receive()', self)\n        self._lock.acquire()\n        try:\n            self.receive_side.read(1)\n            deferred = self._deferred\n            self._deferred = []\n        finally:\n            self._lock.release()\n\n        for func, args, kwargs in deferred:\n            try:\n                func(*args, **kwargs)\n            except Exception:\n                LOG.exception('defer() crashed: %r(*%r, **%r)',\n                              func, args, kwargs)\n                self._broker.shutdown()", "response": "Called by the broker when a new message is received."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwake the multiplexer by writing a byte to the transmit side.", "response": "def _wake(self):\n        \"\"\"\n        Wake the multiplexer by writing a byte. If Broker is midway through\n        teardown, the FD may already be closed, so ignore EBADF.\n        \"\"\"\n        try:\n            self.transmit_side.write(b(' '))\n        except OSError:\n            e = sys.exc_info()[1]\n            if e.args[0] != errno.EBADF:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\narrange for func to execute on the broker thread. This function returns immediately without waiting the result of func.", "response": "def defer(self, func, *args, **kwargs):\n        \"\"\"\n        Arrange for `func()` to execute on the broker thread. This function\n        returns immediately without waiting the result of `func()`. Use\n        :meth:`defer_sync` to block until a result is available.\n\n        :raises mitogen.core.Error:\n            :meth:`defer` was called after :class:`Broker` has begun shutdown.\n        \"\"\"\n        if thread.get_ident() == self.broker_ident:\n            _vv and IOLOG.debug('%r.defer() [immediate]', self)\n            return func(*args, **kwargs)\n        if self._broker._exitted:\n            raise Error(self.broker_shutdown_msg)\n\n        _vv and IOLOG.debug('%r.defer() [fd=%r]', self, self.transmit_side.fd)\n        self._lock.acquire()\n        try:\n            if not self._deferred:\n                self._wake()\n            self._deferred.append((func, args, kwargs))\n        finally:\n            self._lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef on_shutdown(self, broker):\n        _v and LOG.debug('%r.on_shutdown()', self)\n        if not IS_WSL:\n            # #333: WSL generates invalid readiness indication on shutdown()\n            self._wsock.shutdown(socket.SHUT_WR)\n        self._wsock.close()\n        self.transmit_side.close()", "response": "Shut down the write end of the logging socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall when a DELETE_ROUTE message is received from the master.", "response": "def _on_del_route(self, msg):\n        \"\"\"\n        Stub :data:`DEL_ROUTE` handler; fires 'disconnect' events on the\n        corresponding :attr:`_context_by_id` member. This is replaced by\n        :class:`mitogen.parent.RouteMonitor` in an upgraded context.\n        \"\"\"\n        LOG.error('%r._on_del_route() %r', self, msg)\n        if msg.is_dead:\n            return\n\n        target_id_s, _, name = bytes_partition(msg.data, b(':'))\n        target_id = int(target_id_s, 10)\n        context = self._context_by_id.get(target_id)\n        if context:\n            fire(context, 'disconnect')\n        else:\n            LOG.debug('DEL_ROUTE for unknown ID %r: %r', target_id, msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a : class:`Context` referring to the current process.", "response": "def myself(self):\n        \"\"\"\n        Return a :class:`Context` referring to the current process.\n        \"\"\"\n        return self.context_class(\n            router=self,\n            context_id=mitogen.context_id,\n            name='self',\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a context object by its ID or construct a new one if create is False.", "response": "def context_by_id(self, context_id, via_id=None, create=True, name=None):\n        \"\"\"\n        Messy factory/lookup function to find a context by its ID, or construct\n        it. This will eventually be replaced by a more sensible interface.\n        \"\"\"\n        context = self._context_by_id.get(context_id)\n        if context:\n            return context\n\n        if create and via_id is not None:\n            via = self.context_by_id(via_id)\n        else:\n            via = None\n\n        self._write_lock.acquire()\n        try:\n            context = self._context_by_id.get(context_id)\n            if create and not context:\n                context = self.context_class(self, context_id, name=name)\n                context.via = via\n                self._context_by_id[context_id] = context\n        finally:\n            self._write_lock.release()\n\n        return context"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register(self, context, stream):\n        _v and LOG.debug('register(%r, %r)', context, stream)\n        self._write_lock.acquire()\n        try:\n            self._stream_by_id[context.context_id] = stream\n            self._context_by_id[context.context_id] = context\n        finally:\n            self._write_lock.release()\n\n        self.broker.start_receive(stream)\n        listen(stream, 'disconnect', lambda: self._on_stream_disconnect(stream))", "response": "Register a new context and stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the stream that should be used to communicate with a specific destination.", "response": "def stream_by_id(self, dst_id):\n        \"\"\"\n        Return the :class:`Stream` that should be used to communicate with\n        `dst_id`. If a specific route for `dst_id` is not known, a reference to\n        the parent context's stream is returned.\n        \"\"\"\n        return (\n            self._stream_by_id.get(dst_id) or\n            self._stream_by_id.get(mitogen.parent_id)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef del_handler(self, handle):\n        _, _, _, respondent = self._handle_map.pop(handle)\n        if respondent:\n            self._handles_by_respondent[respondent].discard(handle)", "response": "Removes the handle registered for handle."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_handler(self, fn, handle=None, persist=True,\n                    policy=None, respondent=None,\n                    overwrite=False):\n        \"\"\"\n        Invoke `fn(msg)` on the :class:`Broker` thread for each Message sent to\n        `handle` from this context. Unregister after one invocation if\n        `persist` is :data:`False`. If `handle` is :data:`None`, a new handle\n        is allocated and returned.\n\n        :param int handle:\n            If not :data:`None`, an explicit handle to register, usually one of\n            the ``mitogen.core.*`` constants. If unspecified, a new unused\n            handle will be allocated.\n\n        :param bool persist:\n            If :data:`False`, the handler will be unregistered after a single\n            message has been received.\n\n        :param Context respondent:\n            Context that messages to this handle are expected to be sent from.\n            If specified, arranges for a dead message to be delivered to `fn`\n            when disconnection of the context is detected.\n\n            In future `respondent` will likely also be used to prevent other\n            contexts from sending messages to the handle.\n\n        :param function policy:\n            Function invoked as `policy(msg, stream)` where `msg` is a\n            :class:`mitogen.core.Message` about to be delivered, and `stream`\n            is the :class:`mitogen.core.Stream` on which it was received. The\n            function must return :data:`True`, otherwise an error is logged and\n            delivery is refused.\n\n            Two built-in policy functions exist:\n\n            * :func:`has_parent_authority`: requires the message arrived from a\n              parent context, or a context acting with a parent context's\n              authority (``auth_id``).\n\n            * :func:`mitogen.parent.is_immediate_child`: requires the\n              message arrived from an immediately connected child, for use in\n              messaging patterns where either something becomes buggy or\n              insecure by permitting indirect upstream communication.\n\n            In case of refusal, and the message's ``reply_to`` field is\n            nonzero, a :class:`mitogen.core.CallError` is delivered to the\n            sender indicating refusal occurred.\n\n        :param bool overwrite:\n            If :data:`True`, allow existing handles to be silently overwritten.\n\n        :return:\n            `handle`, or if `handle` was :data:`None`, the newly allocated\n            handle.\n        :raises Error:\n            Attemp to register handle that was already registered.\n        \"\"\"\n        handle = handle or next(self._last_handle)\n        _vv and IOLOG.debug('%r.add_handler(%r, %r, %r)', self, fn, handle, persist)\n        if handle in self._handle_map and not overwrite:\n            raise Error(self.duplicate_handle_msg)\n\n        self._handle_map[handle] = persist, fn, policy, respondent\n        if respondent:\n            if respondent not in self._handles_by_respondent:\n                self._handles_by_respondent[respondent] = set()\n                listen(respondent, 'disconnect',\n                       lambda: self._on_respondent_disconnect(respondent))\n            self._handles_by_respondent[respondent].add(handle)\n\n        return handle", "response": "Add a handler function to the broker thread for each Message sent to a specific context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_shutdown(self, broker):\n        _v and LOG.debug('%r.on_shutdown(%r)', self, broker)\n        fire(self, 'shutdown')\n        for handle, (persist, fn) in self._handle_map.iteritems():\n            _v and LOG.debug('%r.on_shutdown(): killing %r: %r', self, handle, fn)\n            fn(Message.dead(self.broker_shutdown_msg))", "response": "Called during broker. shutdown."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nperforming source verification and dispatch the message to the appropriate destination.", "response": "def _async_route(self, msg, in_stream=None):\n        \"\"\"\n        Arrange for `msg` to be forwarded towards its destination. If its\n        destination is the local context, then arrange for it to be dispatched\n        using the local handlers.\n\n        This is a lower overhead version of :meth:`route` that may only be\n        called from the :class:`Broker` thread.\n\n        :param Stream in_stream:\n            If not :data:`None`, the stream the message arrived on. Used for\n            performing source route verification, to ensure sensitive messages\n            such as ``CALL_FUNCTION`` arrive only from trusted contexts.\n        \"\"\"\n        _vv and IOLOG.debug('%r._async_route(%r, %r)', self, msg, in_stream)\n\n        if len(msg.data) > self.max_message_size:\n            self._maybe_send_dead(msg, self.too_large_msg % (\n                self.max_message_size,\n            ))\n            return\n\n        # Perform source verification.\n        if in_stream:\n            parent = self._stream_by_id.get(mitogen.parent_id)\n            expect = self._stream_by_id.get(msg.auth_id, parent)\n            if in_stream != expect:\n                LOG.error('%r: bad auth_id: got %r via %r, not %r: %r',\n                          self, msg.auth_id, in_stream, expect, msg)\n                return\n\n            if msg.src_id != msg.auth_id:\n                expect = self._stream_by_id.get(msg.src_id, parent)\n                if in_stream != expect:\n                    LOG.error('%r: bad src_id: got %r via %r, not %r: %r',\n                              self, msg.src_id, in_stream, expect, msg)\n                    return\n\n            if in_stream.auth_id is not None:\n                msg.auth_id = in_stream.auth_id\n\n            # Maintain a set of IDs the source ever communicated with.\n            in_stream.egress_ids.add(msg.dst_id)\n\n        if msg.dst_id == mitogen.context_id:\n            return self._invoke(msg, in_stream)\n\n        out_stream = self._stream_by_id.get(msg.dst_id)\n        if out_stream is None:\n            out_stream = self._stream_by_id.get(mitogen.parent_id)\n\n        if out_stream is None:\n            self._maybe_send_dead(msg, self.no_route_msg,\n                                  msg.dst_id, mitogen.context_id)\n            return\n\n        if in_stream and self.unidirectional and not \\\n                (in_stream.is_privileged or out_stream.is_privileged):\n            self._maybe_send_dead(msg, self.unidirectional_msg,\n                in_stream.remote_id, out_stream.remote_id)\n            return\n\n        out_stream._send(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _py24_25_compat(self):\n        if sys.version_info < (2, 6):\n            # import_module() is used to avoid dep scanner.\n            os_fork = import_module('mitogen.os_fork')\n            mitogen.os_fork._notice_broker_or_pool(self)", "response": "Python 2. 4 and 2. 5 have grave difficulties with threads and fork."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart receiving data from a specific stream.", "response": "def start_receive(self, stream):\n        \"\"\"\n        Mark the :attr:`receive_side <Stream.receive_side>` on `stream` as\n        ready for reading. Safe to call from any thread. When the associated\n        file descriptor becomes ready for reading,\n        :meth:`BasicStream.on_receive` will be called.\n        \"\"\"\n        _vv and IOLOG.debug('%r.start_receive(%r)', self, stream)\n        side = stream.receive_side\n        assert side and side.fd is not None\n        self.defer(self.poller.start_receive,\n                   side.fd, (side, stream.on_receive))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstops receiving data from a stream.", "response": "def stop_receive(self, stream):\n        \"\"\"\n        Mark the :attr:`receive_side <Stream.receive_side>` on `stream` as not\n        ready for reading. Safe to call from any thread.\n        \"\"\"\n        _vv and IOLOG.debug('%r.stop_receive(%r)', self, stream)\n        self.defer(self.poller.stop_receive, stream.receive_side.fd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _start_transmit(self, stream):\n        _vv and IOLOG.debug('%r._start_transmit(%r)', self, stream)\n        side = stream.transmit_side\n        assert side and side.fd is not None\n        self.poller.start_transmit(side.fd, (side, stream.on_transmit))", "response": "Start sending messages on a given stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _stop_transmit(self, stream):\n        _vv and IOLOG.debug('%r._stop_transmit(%r)', self, stream)\n        self.poller.stop_transmit(stream.transmit_side.fd)", "response": "Stop transmitting data on the given stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef keep_alive(self):\n        it = (side.keep_alive for (_, (side, _)) in self.poller.readers)\n        return sum(it, 0)", "response": "Return True if any reader s side. keep_alive attribute is set to True."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef defer_sync(self, func):\n        latch = Latch()\n        def wrapper():\n            try:\n                latch.put(func())\n            except Exception:\n                latch.put(sys.exc_info()[1])\n        self.defer(wrapper)\n        res = latch.get()\n        if isinstance(res, Exception):\n            raise res\n        return res", "response": "Arrange for func to execute on broker thread blocking the current thread until a result or exception is available."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _call(self, stream, func):\n        try:\n            func(self)\n        except Exception:\n            LOG.exception('%r crashed', stream)\n            stream.on_disconnect(self)", "response": "Call the function func with the current instance of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute a single poll and dispatch any IO events that were triggered by the poller.", "response": "def _loop_once(self, timeout=None):\n        \"\"\"\n        Execute a single :class:`Poller` wait, dispatching any IO events that\n        caused the wait to complete.\n\n        :param float timeout:\n            If not :data:`None`, maximum time in seconds to wait for events.\n        \"\"\"\n        _vv and IOLOG.debug('%r._loop_once(%r, %r)',\n                            self, timeout, self.poller)\n        #IOLOG.debug('readers =\\n%s', pformat(self.poller.readers))\n        #IOLOG.debug('writers =\\n%s', pformat(self.poller.writers))\n        for side, func in self.poller.poll(timeout):\n            self._call(side.stream, func)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ninvoking Stream. on_shutdown for every active stream and allow all streams to be shut down.", "response": "def _broker_shutdown(self):\n        \"\"\"\n        Invoke :meth:`Stream.on_shutdown` for every active stream, then allow\n        up to :attr:`shutdown_timeout` seconds for the streams to unregister\n        themselves, logging an error if any did not unregister during the grace\n        period.\n        \"\"\"\n        for _, (side, _) in self.poller.readers + self.poller.writers:\n            self._call(side.stream, side.stream.on_shutdown)\n\n        deadline = time.time() + self.shutdown_timeout\n        while self.keep_alive() and time.time() < deadline:\n            self._loop_once(max(0, deadline - time.time()))\n\n        if self.keep_alive():\n            LOG.error('%r: some streams did not close gracefully. '\n                      'The most likely cause for this is one or '\n                      'more child processes still connected to '\n                      'our stdout/stderr pipes.', self)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _do_broker_main(self):\n        # For Python 2.4, no way to retrieve ident except on thread.\n        self._waker.broker_ident = thread.get_ident()\n        try:\n            while self._alive:\n                self._loop_once()\n\n            fire(self, 'shutdown')\n            self._broker_shutdown()\n        except Exception:\n            LOG.exception('_broker_main() crashed')\n\n        self._alive = False  # Ensure _alive is consistent on crash.\n        self._exitted = True\n        self._broker_exit()", "response": "Main function for the broker thread."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrequest broker gracefully disconnect streams and stop. Safe to call from any thread.", "response": "def shutdown(self):\n        \"\"\"\n        Request broker gracefully disconnect streams and stop. Safe to call\n        from any thread.\n        \"\"\"\n        _v and LOG.debug('%r.shutdown()', self)\n        def _shutdown():\n            self._alive = False\n        if self._alive and not self._exitted:\n            self.defer(_shutdown)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when a message is received from the service.", "response": "def _on_call_service_msg(self, msg):\n        \"\"\"\n        Stub service handler. Start a thread to import the mitogen.service\n        implementation from, and deliver the message to the newly constructed\n        pool. This must be done as CALL_SERVICE for e.g. PushFileService may\n        race with a CALL_FUNCTION blocking the main thread waiting for a result\n        from that service.\n        \"\"\"\n        if not msg.is_dead:\n            th = threading.Thread(target=self._service_stub_main, args=(msg,))\n            th.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _nullify_stdio(self):\n        fd = os.open('/dev/null', os.O_RDWR)\n        try:\n            for stdfd in (0, 1, 2):\n                if fd != stdfd:\n                    os.dup2(fd, stdfd)\n        finally:\n            if fd not in (0, 1, 2):\n                os.close(fd)", "response": "Open the null file descriptor and replace stdin and stdout and stderr temporarily."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfilter out debug lines from a file - like object.", "response": "def filter_debug(stream, it):\n    \"\"\"\n    Read line chunks from it, either yielding them directly, or building up and\n    logging individual lines if they look like SSH debug output.\n\n    This contains the mess of dealing with both line-oriented input, and partial\n    lines such as the password prompt.\n\n    Yields `(line, partial)` tuples, where `line` is the line, `partial` is\n    :data:`True` if no terminating newline character was present and no more\n    data exists in the read buffer. Consuming code can use this to unreliably\n    detect the presence of an interactive prompt.\n    \"\"\"\n    # The `partial` test is unreliable, but is only problematic when verbosity\n    # is enabled: it's possible for a combination of SSH banner, password\n    # prompt, verbose output, timing and OS buffering specifics to create a\n    # situation where an otherwise newline-terminated line appears to not be\n    # terminated, due to a partial read(). If something is broken when\n    # ssh_debug_level>0, this is the first place to look.\n    state = 'start_of_line'\n    buf = b('')\n    for chunk in it:\n        buf += chunk\n        while buf:\n            if state == 'start_of_line':\n                if len(buf) < 8:\n                    # short read near buffer limit, block awaiting at least 8\n                    # bytes so we can discern a debug line, or the minimum\n                    # interesting token from above or the bootstrap\n                    # ('password', 'MITO000\\n').\n                    break\n                elif any(buf.startswith(p) for p in DEBUG_PREFIXES):\n                    state = 'in_debug'\n                else:\n                    state = 'in_plain'\n            elif state == 'in_debug':\n                if b('\\n') not in buf:\n                    break\n                line, _, buf = bytes_partition(buf, b('\\n'))\n                LOG.debug('%s: %s', stream.name,\n                          mitogen.core.to_text(line.rstrip()))\n                state = 'start_of_line'\n            elif state == 'in_plain':\n                line, nl, buf = bytes_partition(buf, b('\\n'))\n                yield line + nl, not (nl or buf)\n                if nl:\n                    state = 'start_of_line'"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing the base class create_child and create_child_args according to whether we need a PTY or not.", "response": "def _init_create_child(self):\n        \"\"\"\n        Initialize the base class :attr:`create_child` and\n        :attr:`create_child_args` according to whether we need a PTY or not.\n        \"\"\"\n        if self._requires_pty():\n            self.create_child = mitogen.parent.hybrid_tty_create_child\n        else:\n            self.create_child = mitogen.parent.create_child\n            self.create_child_args = {\n                'stderr_pipe': True,\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef run(dest, router, args, deadline=None, econtext=None):\n    if econtext is not None:\n        mitogen.parent.upgrade_router(econtext)\n\n    context_id = router.allocate_id()\n    fakessh = mitogen.parent.Context(router, context_id)\n    fakessh.name = u'fakessh.%d' % (context_id,)\n\n    sock1, sock2 = socket.socketpair()\n\n    stream = mitogen.core.Stream(router, context_id)\n    stream.name = u'fakessh'\n    stream.accept(sock1.fileno(), sock1.fileno())\n    router.register(fakessh, stream)\n\n    # Held in socket buffer until process is booted.\n    fakessh.call_async(_fakessh_main, dest.context_id)\n\n    tmp_path = tempfile.mkdtemp(prefix='mitogen_fakessh')\n    try:\n        ssh_path = os.path.join(tmp_path, 'ssh')\n        fp = open(ssh_path, 'w')\n        try:\n            fp.write('#!%s\\n' % (mitogen.parent.get_sys_executable(),))\n            fp.write(inspect.getsource(mitogen.core))\n            fp.write('\\n')\n            fp.write('ExternalContext(%r).main()\\n' % (\n                _get_econtext_config(context, sock2),\n            ))\n        finally:\n            fp.close()\n\n        os.chmod(ssh_path, int('0755', 8))\n        env = os.environ.copy()\n        env.update({\n            'PATH': '%s:%s' % (tmp_path, env.get('PATH', '')),\n            'ARGV0': mitogen.parent.get_sys_executable(),\n            'SSH_PATH': ssh_path,\n        })\n\n        proc = subprocess.Popen(args, env=env)\n        return proc.wait()\n    finally:\n        shutil.rmtree(tmp_path)", "response": "Runs a command line command line on the specified destination context."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npatch the EventContext. add_local method to patch the EventContext. add_local method to patch the EventContext. add_local method to patch the EventContext. add_local method to patch the EventContext. add_local method to patch the EventContext. add_local method to patch the EventContext. add_local method to patch the EventContext. _local.", "response": "def _patch_awx_callback():\n    \"\"\"\n    issue #400: AWX loads a display callback that suffers from thread-safety\n    issues. Detect the presence of older AWX versions and patch the bug.\n    \"\"\"\n    # AWX uses sitecustomize.py to force-load this package. If it exists, we're\n    # running under AWX.\n    try:\n        from awx_display_callback.events import EventContext\n        from awx_display_callback.events import event_context\n    except ImportError:\n        return\n\n    if hasattr(EventContext(), '_local'):\n        # Patched version.\n        return\n\n    def patch_add_local(self, **kwargs):\n        tls = vars(self._local)\n        ctx = tls.setdefault('_ctx', {})\n        ctx.update(kwargs)\n\n    EventContext._local = threading.local()\n    EventContext.add_local = patch_add_local"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap the given action loader. get method in a new class.", "response": "def wrap_action_loader__get(name, *args, **kwargs):\n    \"\"\"\n    While the mitogen strategy is active, trap action_loader.get() calls,\n    augmenting any fetched class with ActionModuleMixin, which replaces various\n    helper methods inherited from ActionBase with implementations that avoid\n    the use of shell fragments wherever possible.\n\n    This is used instead of static subclassing as it generalizes to third party\n    action modules outside the Ansible tree.\n    \"\"\"\n    klass = action_loader__get(name, class_only=True)\n    if klass:\n        bases = (ansible_mitogen.mixins.ActionModuleMixin, klass)\n        adorned_klass = type(str(name), bases, {})\n        if kwargs.get('class_only'):\n            return adorned_klass\n        return adorned_klass(*args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wrap_connection_loader__get(name, *args, **kwargs):\n    if name in ('docker', 'kubectl', 'jail', 'local', 'lxc',\n                'lxd', 'machinectl', 'setns', 'ssh'):\n        name = 'mitogen_' + name\n    return connection_loader__get(name, *args, **kwargs)", "response": "Wrap connection_loader. get to use Mitogen_."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nwrapping a worker process in a ProfileHook that will be run in a ProfileProcess.", "response": "def wrap_worker__run(*args, **kwargs):\n    \"\"\"\n    While the strategy is active, rewrite connection_loader.get() calls for\n    some transports into requests for a compatible Mitogen transport.\n    \"\"\"\n    # Ignore parent's attempts to murder us when we still need to write\n    # profiling output.\n    if mitogen.core._profile_hook.__name__ != '_profile_hook':\n        signal.signal(signal.SIGTERM, signal.SIG_IGN)\n\n    ansible_mitogen.logging.set_process_name('task')\n    ansible_mitogen.affinity.policy.assign_worker()\n    return mitogen.core._profile_hook('WorkerProcess',\n        lambda: worker__run(*args, **kwargs)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninstall our PluginLoader monkey patches and update global variables with references to the real functions.", "response": "def _install_wrappers(self):\n        \"\"\"\n        Install our PluginLoader monkey patches and update global variables\n        with references to the real functions.\n        \"\"\"\n        global action_loader__get\n        action_loader__get = ansible_mitogen.loaders.action_loader.get\n        ansible_mitogen.loaders.action_loader.get = wrap_action_loader__get\n\n        global connection_loader__get\n        connection_loader__get = ansible_mitogen.loaders.connection_loader.get\n        ansible_mitogen.loaders.connection_loader.get = wrap_connection_loader__get\n\n        global worker__run\n        worker__run = ansible.executor.process.worker.WorkerProcess.run\n        ansible.executor.process.worker.WorkerProcess.run = wrap_worker__run"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remove_wrappers(self):\n        ansible_mitogen.loaders.action_loader.get = action_loader__get\n        ansible_mitogen.loaders.connection_loader.get = connection_loader__get\n        ansible.executor.process.worker.WorkerProcess.run = worker__run", "response": "Remove the monkey patches."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds the Mitogen plug - in directories to the ModuleLoader path avoiding the need for manual configuration.", "response": "def _add_plugin_paths(self):\n        \"\"\"\n        Add the Mitogen plug-in directories to the ModuleLoader path, avoiding\n        the need for manual configuration.\n        \"\"\"\n        base_dir = os.path.join(os.path.dirname(__file__), 'plugins')\n        ansible_mitogen.loaders.connection_loader.add_directory(\n            os.path.join(base_dir, 'connection')\n        )\n        ansible_mitogen.loaders.action_loader.add_directory(\n            os.path.join(base_dir, 'action')\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _queue_task(self, host, task, task_vars, play_context):\n        ansible_mitogen.loaders.module_loader.find_plugin(\n            name=task.action,\n            mod_type='',\n        )\n        ansible_mitogen.loaders.connection_loader.get(\n            name=play_context.connection,\n            class_only=True,\n        )\n        ansible_mitogen.loaders.action_loader.get(\n            name=task.action,\n            class_only=True,\n        )\n\n        return super(StrategyMixin, self)._queue_task(\n            host=host,\n            task=task,\n            task_vars=task_vars,\n            play_context=play_context,\n        )", "response": "This method is used to queue a task for the given host."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run(self, iterator, play_context, result=0):\n        ansible_mitogen.process.MuxProcess.start()\n        run = super(StrategyMixin, self).run\n        self._add_plugin_paths()\n        self._install_wrappers()\n        try:\n            return mitogen.core._profile_hook('Strategy',\n                lambda: run(iterator, play_context)\n            )\n        finally:\n            self._remove_wrappers()", "response": "Arrange for a mitogen.master.Router to be available for the duration of\n        the strategy's real run() method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nannotating a method to permit access to contexts matching an authorization policy.", "response": "def expose(policy):\n    \"\"\"\n    Annotate a method to permit access to contexts matching an authorization\n    policy. The annotation may be specified multiple times. Methods lacking any\n    authorization policy are not accessible.\n\n    ::\n\n        @mitogen.service.expose(policy=mitogen.service.AllowParents())\n        def unsafe_operation(self):\n            ...\n\n    :param mitogen.service.Policy policy:\n        The policy to require.\n    \"\"\"\n    def wrapper(func):\n        func.mitogen_service__policies = (\n            [policy] +\n            getattr(func, 'mitogen_service__policies', [])\n        )\n        return func\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndeferring a function to be invoked in the context of a service pool thread.", "response": "def defer(self, func, *args, **kwargs):\n        \"\"\"\n        Arrange for `func(*args, **kwargs)` to be invoked in the context of a\n        service pool thread.\n        \"\"\"\n        self._ipc_latch.put(lambda: func(*args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfetching a file from the cache.", "response": "def get(self, path):\n        \"\"\"\n        Fetch a file from the cache.\n        \"\"\"\n        assert isinstance(path, mitogen.core.UnicodeType)\n        self._lock.acquire()\n        try:\n            if path in self._cache:\n                return self._cache[path]\n            latch = mitogen.core.Latch()\n            waiters = self._waiters.setdefault(path, [])\n            waiters.append(lambda: latch.put(None))\n        finally:\n            self._lock.release()\n\n        LOG.debug('%r.get(%r) waiting for uncached file to arrive', self, path)\n        latch.get()\n        LOG.debug('%r.get(%r) -> %r', self, path, self._cache[path])\n        return self._cache[path]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npropagate paths and modules to the target context.", "response": "def propagate_paths_and_modules(self, context, paths, modules):\n        \"\"\"\n        One size fits all method to ensure a target context has been preloaded\n        with a set of small files and Python modules.\n        \"\"\"\n        for path in paths:\n            self.propagate_to(context, mitogen.core.to_text(path))\n        self.router.responder.forward_modules(context, modules)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a path for access by children.", "response": "def register(self, path):\n        \"\"\"\n        Authorize a path for access by children. Repeat calls with the same\n        path has no effect.\n\n        :param str path:\n            File path.\n        \"\"\"\n        if path not in self._paths:\n            LOG.debug('%r: registering %r', self, path)\n            self._paths.add(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nregisters a path and any subpaths for access by children.", "response": "def register_prefix(self, path):\n        \"\"\"\n        Authorize a path and any subpaths for access by children. Repeat calls\n        with the same path has no effect.\n\n        :param str path:\n            File path.\n        \"\"\"\n        if path not in self._prefixes:\n            LOG.debug('%r: registering prefix %r', self, path)\n            self._prefixes.add(path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nrespond to shutdown by sending close to every target allowing their receive loop to exit gracefully.", "response": "def on_shutdown(self):\n        \"\"\"\n        Respond to shutdown by sending close() to every target, allowing their\n        receive loop to exit and clean up gracefully.\n        \"\"\"\n        LOG.debug('%r.on_shutdown()', self)\n        for stream, state in self._state_by_stream.items():\n            state.lock.acquire()\n            try:\n                for sender, fp in reversed(state.jobs):\n                    sender.close()\n                    fp.close()\n                    state.jobs.pop()\n            finally:\n                state.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _schedule_pending_unlocked(self, state):\n        while state.jobs and state.unacked < self.window_size_bytes:\n            sender, fp = state.jobs[0]\n            s = fp.read(self.IO_SIZE)\n            if s:\n                state.unacked += len(s)\n                sender.send(mitogen.core.Blob(s))\n            else:\n                # File is done. Cause the target's receive loop to exit by\n                # closing the sender, close the file, and remove the job entry.\n                sender.close()\n                fp.close()\n                state.jobs.pop(0)", "response": "Schedule the pending transfers for a specific file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the set of all possible directory prefixes for path.", "response": "def _prefix_is_authorized(self, path):\n        \"\"\"\n        Return the set of all possible directory prefixes for `path`.\n        :func:`os.path.abspath` is used to ensure the path is absolute.\n\n        :param str path:\n            The path.\n        :returns: Set of prefixes.\n        \"\"\"\n        path = os.path.abspath(path)\n        while True:\n            if path in self._prefixes:\n                return True\n            if path == '/':\n                break\n            path = os.path.dirname(path)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nfetch a file from the given path.", "response": "def fetch(self, path, sender, msg):\n        \"\"\"\n        Start a transfer for a registered path.\n\n        :param str path:\n            File path.\n        :param mitogen.core.Sender sender:\n            Sender to receive file data.\n        :returns:\n            Dict containing the file metadata:\n\n            * ``size``: File size in bytes.\n            * ``mode``: Integer file mode.\n            * ``owner``: Owner account name on host machine.\n            * ``group``: Owner group name on host machine.\n            * ``mtime``: Floating point modification time.\n            * ``ctime``: Floating point change time.\n        :raises Error:\n            Unregistered path, or Sender did not match requestee context.\n        \"\"\"\n        if path not in self._paths and not self._prefix_is_authorized(path):\n            msg.reply(mitogen.core.CallError(\n                Error(self.unregistered_msg % (path,))\n            ))\n            return\n\n        if msg.src_id != sender.context.context_id:\n            msg.reply(mitogen.core.CallError(\n                Error(self.context_mismatch_msg)\n            ))\n            return\n\n        LOG.debug('Serving %r', path)\n\n        # Response must arrive first so requestee can begin receive loop,\n        # otherwise first ack won't arrive until all pending chunks were\n        # delivered. In that case max BDP would always be 128KiB, aka. max\n        # ~10Mbit/sec over a 100ms link.\n        try:\n            fp = open(path, 'rb', self.IO_SIZE)\n            msg.reply(self._generate_stat(path))\n        except IOError:\n            msg.reply(mitogen.core.CallError(\n                sys.exc_info()[1]\n            ))\n            return\n\n        stream = self.router.stream_by_id(sender.context.context_id)\n        state = self._state_by_stream.setdefault(stream, FileStreamState())\n        state.lock.acquire()\n        try:\n            state.jobs.append((sender, fp))\n            self._schedule_pending_unlocked(state)\n        finally:\n            state.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nacknowledge bytes received by a transfer target.", "response": "def acknowledge(self, size, msg):\n        \"\"\"\n        Acknowledge bytes received by a transfer target, scheduling new chunks\n        to keep the window full. This should be called for every chunk received\n        by the target.\n        \"\"\"\n        stream = self.router.stream_by_id(msg.src_id)\n        state = self._state_by_stream[stream]\n        state.lock.acquire()\n        try:\n            if state.unacked < size:\n                LOG.error('%r.acknowledge(src_id %d): unacked=%d < size %d',\n                          self, msg.src_id, state.unacked, size)\n            state.unacked -= min(state.unacked, size)\n            self._schedule_pending_unlocked(state)\n        finally:\n            state.lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(cls, context, path, out_fp):\n        LOG.debug('get_file(): fetching %r from %r', path, context)\n        t0 = time.time()\n        recv = mitogen.core.Receiver(router=context.router)\n        metadata = context.call_service(\n            service_name=cls.name(),\n            method_name='fetch',\n            path=path,\n            sender=recv.to_sender(),\n        )\n\n        received_bytes = 0\n        for chunk in recv:\n            s = chunk.unpickle()\n            LOG.debug('get_file(%r): received %d bytes', path, len(s))\n            context.call_service_async(\n                service_name=cls.name(),\n                method_name='acknowledge',\n                size=len(s),\n            ).close()\n            out_fp.write(s)\n            received_bytes += len(s)\n\n        ok = received_bytes == metadata['size']\n        if received_bytes < metadata['size']:\n            LOG.error('get_file(%r): receiver was closed early, controller '\n                      'may be shutting down, or the file was truncated '\n                      'during transfer. Expected %d bytes, received %d.',\n                      path, metadata['size'], received_bytes)\n        elif received_bytes > metadata['size']:\n            LOG.error('get_file(%r): the file appears to have grown '\n                      'while transfer was in progress. Expected %d '\n                      'bytes, received %d.',\n                      path, metadata['size'], received_bytes)\n\n        LOG.debug('target.get_file(): fetched %d bytes of %r from %r in %dms',\n                  metadata['size'], path, context, 1000 * (time.time() - t0))\n        return ok, metadata", "response": "Download a file from the multiplexer process."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a set of all subclasses of the given class.", "response": "def get_subclasses(klass):\n    \"\"\"\n    Rather than statically import every interesting subclass, forcing it all to\n    be transferred and potentially disrupting the debugged environment,\n    enumerate only those loaded in memory. Also returns the original class.\n    \"\"\"\n    stack = [klass]\n    seen = set()\n    while stack:\n        klass = stack.pop()\n        seen.add(klass)\n        stack.extend(klass.__subclasses__())\n    return seen"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef simplegeneric(func):\n    registry = {}\n    def wrapper(*args, **kw):\n        ob = args[0]\n        try:\n            cls = ob.__class__\n        except AttributeError:\n            cls = type(ob)\n        try:\n            mro = cls.__mro__\n        except AttributeError:\n            try:\n                class cls(cls, object):\n                    pass\n                mro = cls.__mro__[1:]\n            except TypeError:\n                mro = object,   # must be an ExtensionClass or some such  :(\n        for t in mro:\n            if t in registry:\n                return registry[t](*args, **kw)\n        else:\n            return func(*args, **kw)\n    try:\n        wrapper.__name__ = func.__name__\n    except (TypeError, AttributeError):\n        pass    # Python 2.3 doesn't allow functions to be renamed\n\n    def register(typ, func=None):\n        if func is None:\n            return lambda f: register(typ, f)\n        registry[typ] = func\n        return func\n\n    wrapper.__dict__ = func.__dict__\n    wrapper.__doc__ = func.__doc__\n    wrapper.register = register\n    return wrapper", "response": "Make a trivial single - dispatch generic function"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nyield all modules recursively and submodules of the given path.", "response": "def walk_packages(path=None, prefix='', onerror=None):\n    \"\"\"Yields (module_loader, name, ispkg) for all modules recursively\n    on path, or, if path is None, all accessible modules.\n\n    'path' should be either None or a list of paths to look for\n    modules in.\n\n    'prefix' is a string to output on the front of every module name\n    on output.\n\n    Note that this function must import all *packages* (NOT all\n    modules!) on the given path, in order to access the __path__\n    attribute to find submodules.\n\n    'onerror' is a function which gets called with one argument (the\n    name of the package which was being imported) if any exception\n    occurs while trying to import a package.  If no onerror function is\n    supplied, ImportErrors are caught and ignored, while all other\n    exceptions are propagated, terminating the search.\n\n    Examples:\n\n    # list all modules python can access\n    walk_packages()\n\n    # list all submodules of ctypes\n    walk_packages(ctypes.__path__, ctypes.__name__+'.')\n    \"\"\"\n\n    def seen(p, m={}):\n        if p in m:\n            return True\n        m[p] = True\n\n    for importer, name, ispkg in iter_modules(path, prefix):\n        yield importer, name, ispkg\n\n        if ispkg:\n            try:\n                __import__(name)\n            except ImportError:\n                if onerror is not None:\n                    onerror(name)\n            except Exception:\n                if onerror is not None:\n                    onerror(name)\n                else:\n                    raise\n            else:\n                path = getattr(sys.modules[name], '__path__', None) or []\n\n                # don't traverse path items we've seen before\n                path = [p for p in path if not seen(p)]\n\n                for item in walk_packages(path, name+'.', onerror):\n                    yield item"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_modules(path=None, prefix=''):\n\n    if path is None:\n        importers = iter_importers()\n    else:\n        importers = map(get_importer, path)\n\n    yielded = {}\n    for i in importers:\n        for name, ispkg in iter_importer_modules(i, prefix):\n            if name not in yielded:\n                yielded[name] = 1\n                yield i, name, ispkg", "response": "Yields all modules in path and all submodules in prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_importer(path_item):\n    try:\n        importer = sys.path_importer_cache[path_item]\n    except KeyError:\n        for path_hook in sys.path_hooks:\n            try:\n                importer = path_hook(path_item)\n                break\n            except ImportError:\n                pass\n        else:\n            importer = None\n        sys.path_importer_cache.setdefault(path_item, importer)\n\n    if importer is None:\n        try:\n            importer = ImpImporter(path_item)\n        except ImportError:\n            importer = None\n    return importer", "response": "Retrieve a PEP 302 importer for the given path item."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields all importers for the given module name.", "response": "def iter_importers(fullname=\"\"):\n    \"\"\"Yield PEP 302 importers for the given module name\n\n    If fullname contains a '.', the importers will be for the package\n    containing fullname, otherwise they will be importers for sys.meta_path,\n    sys.path, and Python's \"classic\" import machinery, in that order.  If\n    the named module is in a package, that package is imported as a side\n    effect of invoking this function.\n\n    Non PEP 302 mechanisms (e.g. the Windows registry) used by the\n    standard import machinery to find files in alternative locations\n    are partially supported, but are searched AFTER sys.path. Normally,\n    these locations are searched BEFORE sys.path, preventing sys.path\n    entries from shadowing them.\n\n    For this to cause a visible difference in behaviour, there must\n    be a module or package name that is accessible via both sys.path\n    and one of the non PEP 302 file system mechanisms. In this case,\n    the emulation will find the former version, while the builtin\n    import mechanism will find the latter.\n\n    Items of the following types can be affected by this discrepancy:\n        imp.C_EXTENSION, imp.PY_SOURCE, imp.PY_COMPILED, imp.PKG_DIRECTORY\n    \"\"\"\n    if fullname.startswith('.'):\n        raise ImportError(\"Relative module names not supported\")\n    if '.' in fullname:\n        # Get the containing package's __path__\n        pkg = '.'.join(fullname.split('.')[:-1])\n        if pkg not in sys.modules:\n            __import__(pkg)\n        path = getattr(sys.modules[pkg], '__path__', None) or []\n    else:\n        for importer in sys.meta_path:\n            yield importer\n        path = sys.path\n    for item in path:\n        yield get_importer(item)\n    if '.' not in fullname:\n        yield ImpImporter()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget a PEP 302 loader object for the specified module or package.", "response": "def get_loader(module_or_name):\n    \"\"\"Get a PEP 302 \"loader\" object for module_or_name\n\n    If the module or package is accessible via the normal import\n    mechanism, a wrapper around the relevant part of that machinery\n    is returned.  Returns None if the module cannot be found or imported.\n    If the named module is not already imported, its containing package\n    (if any) is imported, in order to establish the package __path__.\n\n    This function uses iter_importers(), and is thus subject to the same\n    limitations regarding platform-specific special import locations such\n    as the Windows registry.\n    \"\"\"\n    if module_or_name in sys.modules:\n        module_or_name = sys.modules[module_or_name]\n    if isinstance(module_or_name, ModuleType):\n        module = module_or_name\n        loader = getattr(module, '__loader__', None)\n        if loader is not None:\n            return loader\n        fullname = module.__name__\n    else:\n        fullname = module_or_name\n    return find_loader(fullname)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_loader(fullname):\n    for importer in iter_importers(fullname):\n        loader = importer.find_module(fullname)\n        if loader is not None:\n            return loader\n\n    return None", "response": "Find a PEP 302 loader object for fullname"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextend a path to include a single frozen version of a package.", "response": "def extend_path(path, name):\n    \"\"\"Extend a package's path.\n\n    Intended use is to place the following code in a package's __init__.py:\n\n        from pkgutil import extend_path\n        __path__ = extend_path(__path__, __name__)\n\n    This will add to the package's __path__ all subdirectories of\n    directories on sys.path named after the package.  This is useful\n    if one wants to distribute different parts of a single logical\n    package as multiple directories.\n\n    It also looks for *.pkg files beginning where * matches the name\n    argument.  This feature is similar to *.pth files (see site.py),\n    except that it doesn't special-case lines starting with 'import'.\n    A *.pkg file is trusted at face value: apart from checking for\n    duplicates, all entries found in a *.pkg file are added to the\n    path, regardless of whether they are exist the filesystem.  (This\n    is a feature.)\n\n    If the input path is not a list (as is the case for frozen\n    packages) it is returned unchanged.  The input path is not\n    modified; an extended copy is returned.  Items are only appended\n    to the copy at the end.\n\n    It is assumed that sys.path is a sequence.  Items of sys.path that\n    are not (unicode or 8-bit) strings referring to existing\n    directories are ignored.  Unicode items of sys.path that cause\n    errors when used as filenames may cause this function to raise an\n    exception (in line with os.path.isdir() behavior).\n    \"\"\"\n\n    if not isinstance(path, list):\n        # This could happen e.g. when this is called from inside a\n        # frozen package.  Return the path unchanged in that case.\n        return path\n\n    pname = os.path.join(*name.split('.')) # Reconstitute as relative path\n    # Just in case os.extsep != '.'\n    sname = os.extsep.join(name.split('.'))\n    sname_pkg = sname + os.extsep + \"pkg\"\n    init_py = \"__init__\" + os.extsep + \"py\"\n\n    path = path[:] # Start with a copy of the existing path\n\n    for dir in sys.path:\n        if not isinstance(dir, basestring) or not os.path.isdir(dir):\n            continue\n        subdir = os.path.join(dir, pname)\n        # XXX This may still add duplicate entries to path on\n        # case-insensitive filesystems\n        initfile = os.path.join(subdir, init_py)\n        if subdir not in path and os.path.isfile(initfile):\n            path.append(subdir)\n        # XXX Is this the right thing for subpackages like zope.app?\n        # It looks for a file named \"zope.app.pkg\"\n        pkgfile = os.path.join(dir, sname_pkg)\n        if os.path.isfile(pkgfile):\n            try:\n                f = open(pkgfile)\n            except IOError, msg:\n                sys.stderr.write(\"Can't open %s: %s\\n\" %\n                                 (pkgfile, msg))\n            else:\n                for line in f:\n                    line = line.rstrip('\\n')\n                    if not line or line.startswith('#'):\n                        continue\n                    path.append(line) # Don't check for existence!\n                f.close()\n\n    return path"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_data(package, resource):\n\n    loader = get_loader(package)\n    if loader is None or not hasattr(loader, 'get_data'):\n        return None\n    mod = sys.modules.get(package) or loader.load_module(package)\n    if mod is None or not hasattr(mod, '__file__'):\n        return None\n\n    # Modify the resource name to be compatible with the loader.get_data\n    # signature - an os.path format \"filename\" starting with the dirname of\n    # the package's __file__\n    parts = resource.split('/')\n    parts.insert(0, os.path.dirname(mod.__file__))\n    resource_name = os.path.join(*parts)\n    return loader.get_data(resource_name)", "response": "Get a resource from a package."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef invoke(invocation):\n    (invocation.module_path,\n     invocation.module_source) = get_module_data(invocation.module_name)\n    planner = _get_planner(invocation)\n\n    if invocation.wrap_async:\n        response = _invoke_async_task(invocation, planner)\n    elif planner.should_fork():\n        response = _invoke_isolated_task(invocation, planner)\n    else:\n        _propagate_deps(invocation, planner, invocation.connection.context)\n        response = invocation.connection.get_chain().call(\n            ansible_mitogen.target.run_module,\n            kwargs=planner.get_kwargs(),\n        )\n\n    return invocation.action._postprocess_response(response)", "response": "Find a Planner subclass corresnding to `invocation` and use it to invoke\n    the module.\n\n    :param Invocation invocation:\n    :returns:\n        Module return dict.\n    :raises ansible.errors.AnsibleError:\n        Unrecognized/unsupported module type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_kwargs(self, **kwargs):\n        new = dict((mitogen.core.UnicodeType(k), kwargs[k])\n                   for k in kwargs)\n        new.setdefault('good_temp_dir',\n            self._inv.connection.get_good_temp_dir())\n        new.setdefault('cwd', self._inv.connection.get_default_cwd())\n        new.setdefault('extra_env', self._inv.connection.get_default_env())\n        new.setdefault('emulate_tty', True)\n        new.setdefault('service_context', self._inv.connection.parent)\n        return new", "response": "Returns a dictionary of keyword arguments passed to the module s it\n            constructor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _rewrite_interpreter(self, path):\n        key = u'ansible_%s_interpreter' % os.path.basename(path).strip()\n        try:\n            template = self._inv.task_vars[key]\n        except KeyError:\n            return path\n\n        return mitogen.utils.cast(self._inv.templar.template(template))", "response": "Rewrite the interpreter binary to use the interpreter s interpreter_name variable and return the path."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef should_fork(self):\n        return (\n            super(NewStylePlanner, self).should_fork() or\n            (self._inv.task_vars.get('mitogen_task_isolation') == 'fork') or\n            (self._inv.module_name in self.ALWAYS_FORK_MODULES) or\n            (len(self.get_module_map()['custom']) > 0)\n        )", "response": "Returns True if the new - style module should be forked."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets an integer - valued environment variable key and parses as an integer otherwise return default.", "response": "def getenv_int(key, default=0):\n    \"\"\"\n    Get an integer-valued environment variable `key`, if it exists and parses\n    as an integer, otherwise return `default`.\n    \"\"\"\n    try:\n        return int(os.environ.get(key, str(default)))\n    except ValueError:\n        return default"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef save_pid(name):\n    if os.environ.get('MITOGEN_SAVE_PIDS'):\n        with open('.ansible-%s.pid' % (name,), 'w') as fp:\n            fp.write(str(os.getpid()))", "response": "Save the current process ID in the current directory."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start(cls, _init_logging=True):\n        if cls.worker_sock is not None:\n            return\n\n        if faulthandler is not None:\n            faulthandler.enable()\n\n        mitogen.utils.setup_gil()\n        cls.unix_listener_path = mitogen.unix.make_socket_path()\n        cls.worker_sock, cls.child_sock = socket.socketpair()\n        atexit.register(lambda: clean_shutdown(cls.worker_sock))\n        mitogen.core.set_cloexec(cls.worker_sock.fileno())\n        mitogen.core.set_cloexec(cls.child_sock.fileno())\n\n        cls.profiling = os.environ.get('MITOGEN_PROFILING') is not None\n        if cls.profiling:\n            mitogen.core.enable_profiling()\n        if _init_logging:\n            ansible_mitogen.logging.setup()\n\n        cls.original_env = dict(os.environ)\n        cls.child_pid = os.fork()\n        if cls.child_pid:\n            save_pid('controller')\n            ansible_mitogen.logging.set_process_name('top')\n            ansible_mitogen.affinity.policy.assign_controller()\n            cls.child_sock.close()\n            cls.child_sock = None\n            mitogen.core.io_op(cls.worker_sock.recv, 1)\n        else:\n            save_pid('mux')\n            ansible_mitogen.logging.set_process_name('mux')\n            ansible_mitogen.affinity.policy.assign_muxprocess()\n            cls.worker_sock.close()\n            cls.worker_sock = None\n            self = cls()\n            self.worker_main()", "response": "Arrange for the subprocess to be started, if it is not already running.\n\n        The parent process picks a UNIX socket path the child will use prior to\n        fork, creates a socketpair used essentially as a semaphore, then blocks\n        waiting for the child to indicate the UNIX socket is ready for use.\n\n        :param bool _init_logging:\n            For testing, if :data:`False`, don't initialize logging."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setup_simplejson(self, responder):\n        responder.whitelist_prefix('simplejson')\n\n        # issue #536: must be at end of sys.path, in case existing newer\n        # version is already loaded.\n        compat_path = os.path.join(os.path.dirname(__file__), 'compat')\n        sys.path.append(compat_path)\n\n        for fullname, is_pkg, suffix in (\n            (u'simplejson', True, '__init__.py'),\n            (u'simplejson.decoder', False, 'decoder.py'),\n            (u'simplejson.encoder', False, 'encoder.py'),\n            (u'simplejson.scanner', False, 'scanner.py'),\n        ):\n            path = os.path.join(compat_path, 'simplejson', suffix)\n            fp = open(path, 'rb')\n            try:\n                source = fp.read()\n            finally:\n                fp.close()\n\n            responder.add_source_override(\n                fullname=fullname,\n                path=path,\n                source=source,\n                is_pkg=is_pkg,\n            )", "response": "Setup simplejson for the current version of the package."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _setup_responder(self, responder):\n        responder.whitelist_prefix('ansible')\n        responder.whitelist_prefix('ansible_mitogen')\n        self._setup_simplejson(responder)\n\n        # Ansible 2.3 is compatible with Python 2.4 targets, however\n        # ansible/__init__.py is not. Instead, executor/module_common.py writes\n        # out a 2.4-compatible namespace package for unknown reasons. So we\n        # copy it here.\n        responder.add_source_override(\n            fullname='ansible',\n            path=ansible.__file__,\n            source=(ANSIBLE_PKG_OVERRIDE % (\n                ansible.__version__,\n                ansible.__author__,\n            )).encode(),\n            is_pkg=True,\n        )", "response": "Configure responder to only permit any modules that have a specific version."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _setup_master(self):\n        self.broker = mitogen.master.Broker(install_watcher=False)\n        self.router = mitogen.master.Router(\n            broker=self.broker,\n            max_message_size=4096 * 1048576,\n        )\n        self._setup_responder(self.router.responder)\n        mitogen.core.listen(self.broker, 'shutdown', self.on_broker_shutdown)\n        mitogen.core.listen(self.broker, 'exit', self.on_broker_exit)\n        self.listener = mitogen.unix.Listener(\n            router=self.router,\n            path=self.unix_listener_path,\n            backlog=C.DEFAULT_FORKS,\n        )\n        self._enable_router_debug()\n        self._enable_stack_dumps()", "response": "Construct a Router Broker and a listener."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a ContextService and a thread to service requests for it .", "response": "def _setup_services(self):\n        \"\"\"\n        Construct a ContextService and a thread to service requests for it\n        arriving from worker processes.\n        \"\"\"\n        self.pool = mitogen.service.Pool(\n            router=self.router,\n            services=[\n                mitogen.service.FileService(router=self.router),\n                mitogen.service.PushFileService(router=self.router),\n                ansible_mitogen.services.ContextService(self.router),\n                ansible_mitogen.services.ModuleDepService(self.router),\n            ],\n            size=getenv_int('MITOGEN_POOL_SIZE', default=32),\n        )\n        LOG.debug('Service pool configured: size=%d', self.pool.size)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresponds to the broker thread about to exit by sending SIGTERM to ourself. In future this should gracefully join the pool, but TERM is fine for now.", "response": "def on_broker_exit(self):\n        \"\"\"\n        Respond to the broker thread about to exit by sending SIGTERM to\n        ourself. In future this should gracefully join the pool, but TERM is\n        fine for now.\n        \"\"\"\n        if not self.profiling:\n            # In normal operation we presently kill the process because there is\n            # not yet any way to cancel connect(). When profiling, threads\n            # including the broker must shut down gracefully, otherwise pstats\n            # won't be written.\n            os.kill(os.getpid(), signal.SIGTERM)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef utf8(s):\n    if isinstance(s, mitogen.core.UnicodeType):\n        s = s.encode('utf-8')\n    return s", "response": "Coerce an object to bytes if it is Unicode."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreopens the file descriptor corresponding to the file object fp with a new one that is read - only.", "response": "def reopen_readonly(fp):\n    \"\"\"\n    Replace the file descriptor belonging to the file object `fp` with one\n    open on the same file (`fp.name`), but opened with :py:data:`os.O_RDONLY`.\n    This enables temporary files to be executed on Linux, which usually throws\n    ``ETXTBUSY`` if any writeable handle exists pointing to a file passed to\n    `execve()`.\n    \"\"\"\n    fd = os.open(fp.name, os.O_RDONLY)\n    os.dup2(fd, fp.fileno())\n    os.close(fd)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the log file and yield the key value pairs.", "response": "def _parse(self, fp):\n        \"\"\"\n        linux-pam-1.3.1/modules/pam_env/pam_env.c#L207\n        \"\"\"\n        for line in fp:\n            # '   #export foo=some var  ' -> ['#export', 'foo=some var  ']\n            bits = shlex_split(line, comments=True)\n            if (not bits) or bits[0].startswith('#'):\n                continue\n\n            if bits[0] == u'export':\n                bits.pop(0)\n\n            key, sep, value = str_partition(u' '.join(bits), u'=')\n            if key and sep:\n                yield key, value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_existing(self):\n        for key in self._keys:\n            if key in os.environ:\n                LOG.debug('%r: removing old key %r', self, key)\n                del os.environ[key]\n        self._keys = []", "response": "Remove keys that exist in the old file."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(self):\n        st = self._stat()\n        if self._st == st:\n            return\n\n        self._st = st\n        self._remove_existing()\n\n        if st is None:\n            LOG.debug('%r: file has disappeared', self)\n        else:\n            self._on_file_changed()", "response": "Checks the file has been changed and reloads the environment if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napply changes from /etc/environment files before creating a TemporaryEnvironment to snapshot environment state prior to module run.", "response": "def _setup_environ(self):\n        \"\"\"\n        Apply changes from /etc/environment files before creating a\n        TemporaryEnvironment to snapshot environment state prior to module run.\n        \"\"\"\n        _pam_env_watcher.check()\n        _etc_env_watcher.check()\n        env = dict(self.extra_env or {})\n        if self.env:\n            env.update(self.env)\n        self._env = TemporaryEnvironment(env)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run(self):\n        self.setup()\n        if self.detach:\n            self.econtext.detach()\n\n        try:\n            return self._run()\n        finally:\n            self.revert()", "response": "Run an Ansible\n        module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nrestores the original register.", "response": "def revert(self):\n        \"\"\"\n        Restore the original :func:`atexit.register`.\n        \"\"\"\n        assert atexit.register == self._atexit__register, \\\n            \"AtExitWrapper not installed.\"\n        atexit.register = self.original['register']"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _atexit__register(self, func, *targs, **kwargs):\n        if func == shutil.rmtree:\n            self.deferred.append((func, targs, kwargs))\n            return\n\n        self.original['register'](func, *targs, **kwargs)", "response": "Intercepts a function to be called when the process exits."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreverts changes made by the module to the process environment.", "response": "def revert(self):\n        \"\"\"\n        Revert changes made by the module to the process environment. This must\n        always run, as some modules (e.g. git.py) set variables like GIT_SSH\n        that must be cleared out between runs.\n        \"\"\"\n        os.environ.clear()\n        os.environ.update(self.original)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _setup_program(self):\n        filename = self._get_program_filename()\n        path = os.path.join(self.get_temp_dir(), filename)\n        self.program_fp = open(path, 'wb')\n        self.program_fp.write(self._get_program())\n        self.program_fp.flush()\n        os.chmod(self.program_fp.name, int('0700', 8))\n        reopen_readonly(self.program_fp)", "response": "Create a temporary file containing the code."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_program(self):\n        return ansible_mitogen.target.get_small_file(\n            context=self.service_context,\n            path=self.path,\n        )", "response": "Fetch the module binary from the master if necessary."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef revert(self):\n        if self.program_fp:\n            self.program_fp.close()\n        super(ProgramRunner, self).revert()", "response": "Revert the program to its original state."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a temporary file containing the module s arguments.", "response": "def _setup_args(self):\n        \"\"\"\n        Create a temporary file containing the module's arguments. The\n        arguments are formatted via :meth:`_get_args`.\n        \"\"\"\n        self.args_fp = tempfile.NamedTemporaryFile(\n            prefix='ansible_mitogen',\n            suffix='-args',\n            dir=self.get_temp_dir(),\n        )\n        self.args_fp.write(utf8(self._get_args_contents()))\n        self.args_fp.flush()\n        reopen_readonly(self.program_fp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmutates the source according to the per - task parameters.", "response": "def _rewrite_source(self, s):\n        \"\"\"\n        Mutate the source according to the per-task parameters.\n        \"\"\"\n        # While Ansible rewrites the #! using ansible_*_interpreter, it is\n        # never actually used to execute the script, instead it is a shell\n        # fragment consumed by shell/__init__.py::build_module_command().\n        new = [b('#!') + utf8(self.interpreter_fragment)]\n        if self.is_python:\n            new.append(self.b_ENCODING_STRING)\n\n        _, _, rest = bytes_partition(s, b('\\n'))\n        new.append(rest)\n        return b('\\n').join(new)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nensuring local importer and PushFileService have everything for the base module.", "response": "def _setup_imports(self):\n        \"\"\"\n        Ensure the local importer and PushFileService has everything for the\n        Ansible module before setup() completes, but before detach() is called\n        in an asynchronous task.\n\n        The master automatically streams modules towards us concurrent to the\n        runner invocation, however there is no public API to synchronize on the\n        completion of those preloads. Instead simply reuse the importer's\n        synchronization mechanism by importing everything the module will need\n        prior to detaching.\n        \"\"\"\n        for fullname, _, _ in self.module_map['custom']:\n            mitogen.core.import_module(fullname)\n        for fullname in self.module_map['builtin']:\n            mitogen.core.import_module(fullname)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _handle_magic_exception(self, mod, exc):\n        klass = getattr(mod, 'AnsibleModuleError', None)\n        if klass and isinstance(exc, klass):\n            mod.module.fail_json(**exc.results)", "response": "Handle exceptions raised by magic exceptions in the module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmimics the argument formatting behaviour of the action base.", "response": "def _get_args_contents(self):\n        \"\"\"\n        Mimic the argument formatting behaviour of\n        ActionBase._execute_module().\n        \"\"\"\n        return ' '.join(\n            '%s=%s' % (key, shlex_quote(str(self.args[key])))\n            for key in self.args\n        ) + ' '"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning ContextService arguments for an SSH connection.", "response": "def _connect_ssh(spec):\n    \"\"\"\n    Return ContextService arguments for an SSH connection.\n    \"\"\"\n    if C.HOST_KEY_CHECKING:\n        check_host_keys = 'enforce'\n    else:\n        check_host_keys = 'ignore'\n\n    # #334: tilde-expand private_key_file to avoid implementation difference\n    # between Python and OpenSSH.\n    private_key_file = spec.private_key_file()\n    if private_key_file is not None:\n        private_key_file = os.path.expanduser(private_key_file)\n\n    return {\n        'method': 'ssh',\n        'kwargs': {\n            'check_host_keys': check_host_keys,\n            'hostname': spec.remote_addr(),\n            'username': spec.remote_user(),\n            'compression': convert_bool(\n                default(spec.mitogen_ssh_compression(), True)\n            ),\n            'password': spec.password(),\n            'port': spec.port(),\n            'python_path': spec.python_path(),\n            'identity_file': private_key_file,\n            'identities_only': False,\n            'ssh_path': spec.ssh_executable(),\n            'connect_timeout': spec.ansible_ssh_timeout(),\n            'ssh_args': spec.ssh_args(),\n            'ssh_debug_level': spec.mitogen_ssh_debug_level(),\n            'remote_name': get_remote_name(spec),\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn ContextService arguments for a Docker connection.", "response": "def _connect_docker(spec):\n    \"\"\"\n    Return ContextService arguments for a Docker connection.\n    \"\"\"\n    return {\n        'method': 'docker',\n        'kwargs': {\n            'username': spec.remote_user(),\n            'container': spec.remote_addr(),\n            'python_path': spec.python_path(),\n            'connect_timeout': spec.ansible_ssh_timeout() or spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns ContextService arguments for a Kubernetes connection.", "response": "def _connect_kubectl(spec):\n    \"\"\"\n    Return ContextService arguments for a Kubernetes connection.\n    \"\"\"\n    return {\n        'method': 'kubectl',\n        'kwargs': {\n            'pod': spec.remote_addr(),\n            'python_path': spec.python_path(),\n            'connect_timeout': spec.ansible_ssh_timeout() or spec.timeout(),\n            'kubectl_path': spec.mitogen_kubectl_path(),\n            'kubectl_args': spec.extra_args(),\n            'remote_name': get_remote_name(spec),\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns ContextService arguments for a FreeBSD jail connection.", "response": "def _connect_jail(spec):\n    \"\"\"\n    Return ContextService arguments for a FreeBSD jail connection.\n    \"\"\"\n    return {\n        'method': 'jail',\n        'kwargs': {\n            'username': spec.remote_user(),\n            'container': spec.remote_addr(),\n            'python_path': spec.python_path(),\n            'connect_timeout': spec.ansible_ssh_timeout() or spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _connect_lxc(spec):\n    return {\n        'method': 'lxc',\n        'kwargs': {\n            'container': spec.remote_addr(),\n            'python_path': spec.python_path(),\n            'lxc_attach_path': spec.mitogen_lxc_attach_path(),\n            'connect_timeout': spec.ansible_ssh_timeout() or spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }", "response": "Return ContextService arguments for an LXC Classic container connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns ContextService arguments for an LXD container connection.", "response": "def _connect_lxd(spec):\n    \"\"\"\n    Return ContextService arguments for an LXD container connection.\n    \"\"\"\n    return {\n        'method': 'lxd',\n        'kwargs': {\n            'container': spec.remote_addr(),\n            'python_path': spec.python_path(),\n            'lxc_path': spec.mitogen_lxc_path(),\n            'connect_timeout': spec.ansible_ssh_timeout() or spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _connect_setns(spec, kind=None):\n    return {\n        'method': 'setns',\n        'kwargs': {\n            'container': spec.remote_addr(),\n            'username': spec.remote_user(),\n            'python_path': spec.python_path(),\n            'kind': kind or spec.mitogen_kind(),\n            'docker_path': spec.mitogen_docker_path(),\n            'lxc_path': spec.mitogen_lxc_path(),\n            'lxc_info_path': spec.mitogen_lxc_info_path(),\n            'machinectl_path': spec.mitogen_machinectl_path(),\n        }\n    }", "response": "Return ContextService arguments for a mitogen_setns connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns ContextService arguments for su as a become method.", "response": "def _connect_su(spec):\n    \"\"\"\n    Return ContextService arguments for su as a become method.\n    \"\"\"\n    return {\n        'method': 'su',\n        'enable_lru': True,\n        'kwargs': {\n            'username': spec.become_user(),\n            'password': spec.become_pass(),\n            'python_path': spec.python_path(),\n            'su_path': spec.become_exe(),\n            'connect_timeout': spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _connect_sudo(spec):\n    return {\n        'method': 'sudo',\n        'enable_lru': True,\n        'kwargs': {\n            'username': spec.become_user(),\n            'password': spec.become_pass(),\n            'python_path': spec.python_path(),\n            'sudo_path': spec.become_exe(),\n            'connect_timeout': spec.timeout(),\n            'sudo_args': spec.sudo_args(),\n            'remote_name': get_remote_name(spec),\n        }\n    }", "response": "Return ContextService arguments for sudo as a become method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn ContextService arguments for su as a first class connection.", "response": "def _connect_mitogen_su(spec):\n    \"\"\"\n    Return ContextService arguments for su as a first class connection.\n    \"\"\"\n    return {\n        'method': 'su',\n        'kwargs': {\n            'username': spec.remote_user(),\n            'password': spec.password(),\n            'python_path': spec.python_path(),\n            'su_path': spec.become_exe(),\n            'connect_timeout': spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _connect_mitogen_sudo(spec):\n    return {\n        'method': 'sudo',\n        'kwargs': {\n            'username': spec.remote_user(),\n            'password': spec.password(),\n            'python_path': spec.python_path(),\n            'sudo_path': spec.become_exe(),\n            'connect_timeout': spec.timeout(),\n            'sudo_args': spec.sudo_args(),\n            'remote_name': get_remote_name(spec),\n        }\n    }", "response": "Return ContextService arguments for sudo as a first class connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _connect_mitogen_doas(spec):\n    return {\n        'method': 'doas',\n        'kwargs': {\n            'username': spec.remote_user(),\n            'password': spec.password(),\n            'python_path': spec.python_path(),\n            'doas_path': spec.become_exe(),\n            'connect_timeout': spec.timeout(),\n            'remote_name': get_remote_name(spec),\n        }\n    }", "response": "Return ContextService arguments for a doas as a first class connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlike :meth:`mitogen.parent.CallChain.call`, but log timings.", "response": "def call(self, func, *args, **kwargs):\n        \"\"\"\n        Like :meth:`mitogen.parent.CallChain.call`, but log timings.\n        \"\"\"\n        t0 = time.time()\n        try:\n            recv = self.call_async(func, *args, **kwargs)\n            return self._rethrow(recv)\n        finally:\n            LOG.debug('Call took %d ms: %r', 1000 * (time.time() - t0),\n                      mitogen.parent.CallSpec(func, args, kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls by the action module when a new task is about to start the object.", "response": "def on_action_run(self, task_vars, delegate_to_hostname, loader_basedir):\n        \"\"\"\n        Invoked by ActionModuleMixin to indicate a new task is about to start\n        executing. We use the opportunity to grab relevant bits from the\n        task-specific data.\n\n        :param dict task_vars:\n            Task variable dictionary.\n        :param str delegate_to_hostname:\n            :data:`None`, or the template-expanded inventory hostname this task\n            is being delegated to. A similar variable exists on PlayContext\n            when ``delegate_to:`` is active, however it is unexpanded.\n        :param str loader_basedir:\n            Loader base directory; see :attr:`loader_basedir`.\n        \"\"\"\n        self.inventory_hostname = task_vars['inventory_hostname']\n        self._task_vars = task_vars\n        self.host_vars = task_vars['hostvars']\n        self.delegate_to_hostname = delegate_to_hostname\n        self.loader_basedir = loader_basedir\n        self._mitogen_reset(mode='put')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_task_var(self, key, default=None):\n        if self._task_vars:\n            if self.delegate_to_hostname is None:\n                if key in self._task_vars:\n                    return self._task_vars[key]\n            else:\n                delegated_vars = self._task_vars['ansible_delegated_vars']\n                if self.delegate_to_hostname in delegated_vars:\n                    task_vars = delegated_vars[self.delegate_to_hostname]\n                    if key in task_vars:\n                        return task_vars[key]\n\n        return default", "response": "Fetch the value of a task variable related to the current instance of the delegated - to machine."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _spec_from_via(self, proxied_inventory_name, via_spec):\n        become_user, _, inventory_name = via_spec.rpartition('@')\n        become_method, _, become_user = become_user.rpartition(':')\n\n        # must use __contains__ to avoid a TypeError for a missing host on\n        # Ansible 2.3.\n        if self.host_vars is None or inventory_name not in self.host_vars:\n            raise ansible.errors.AnsibleConnectionFailure(\n                self.unknown_via_msg % (\n                    via_spec,\n                    proxied_inventory_name,\n                )\n            )\n\n        via_vars = self.host_vars[inventory_name]\n        return ansible_mitogen.transport_config.MitogenViaSpec(\n            inventory_name=inventory_name,\n            play_context=self._play_context,\n            host_vars=dict(via_vars),  # TODO: make it lazy\n            become_method=become_method or None,\n            become_user=become_user or None,\n        )", "response": "Produce a dict connection specifiction given a string via_spec."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _stack_from_spec(self, spec, stack=(), seen_names=()):\n        if spec.inventory_name() in seen_names:\n            raise ansible.errors.AnsibleConnectionFailure(\n                self.via_cycle_msg % (\n                    spec.mitogen_via(),\n                    spec.inventory_name(),\n                    ' -> '.join(reversed(\n                        seen_names + (spec.inventory_name(),)\n                    )),\n                )\n            )\n\n        if spec.mitogen_via():\n            stack = self._stack_from_spec(\n                self._spec_from_via(spec.inventory_name(), spec.mitogen_via()),\n                stack=stack,\n                seen_names=seen_names + (spec.inventory_name(),),\n            )\n\n        stack += (CONNECTION_METHOD[spec.transport()](spec),)\n        if spec.become() and ((spec.become_user() != spec.remote_user()) or\n                              C.BECOME_ALLOW_SAME_USER):\n            stack += (CONNECTION_METHOD[spec.become_method()](spec),)\n\n        return stack", "response": "Return a tuple of ContextService parameter dictionaries corresponding to the connection described by spec and any associated connection referenced by the spec and any associated connection referenced by the spec and any associated connection referenced by the spec and any associated connection referenced by the spec."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _connect_broker(self):\n        if not self.broker:\n            self.broker = mitogen.master.Broker()\n            self.router, self.parent = mitogen.unix.connect(\n                path=ansible_mitogen.process.MuxProcess.unix_listener_path,\n                broker=self.broker,\n            )", "response": "Establish a reference to the Broker Router and parent context used for the HTTP connection."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the stack from the connection and target.", "response": "def _build_stack(self):\n        \"\"\"\n        Construct a list of dictionaries representing the connection\n        configuration between the controller and the target. This is\n        additionally used by the integration tests \"mitogen_get_stack\" action\n        to fetch the would-be connection configuration.\n        \"\"\"\n        return self._stack_from_spec(\n            ansible_mitogen.transport_config.PlayContextSpec(\n                connection=self,\n                play_context=self._play_context,\n                transport=self.transport,\n                inventory_name=self.inventory_hostname,\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconnecting a stack to the current context.", "response": "def _connect_stack(self, stack):\n        \"\"\"\n        Pass `stack` to ContextService, requesting a copy of the context object\n        representing the last tuple element. If no connection exists yet,\n        ContextService will recursively establish it before returning it or\n        throwing an error.\n\n        See :meth:`ansible_mitogen.services.ContextService.get` docstring for\n        description of the returned dictionary.\n        \"\"\"\n        try:\n            dct = self.parent.call_service(\n                service_name='ansible_mitogen.services.ContextService',\n                method_name='get',\n                stack=mitogen.utils.cast(list(stack)),\n            )\n        except mitogen.core.CallError:\n            LOG.warning('Connection failed; stack configuration was:\\n%s',\n                        pprint.pformat(stack))\n            raise\n\n        if dct['msg']:\n            if dct['method_name'] in self.become_methods:\n                raise ansible.errors.AnsibleModuleError(dct['msg'])\n            raise ansible.errors.AnsibleConnectionFailure(dct['msg'])\n\n        self.context = dct['context']\n        self.chain = CallChain(self, self.context, pipelined=True)\n        if self._play_context.become:\n            self.login_context = dct['via']\n        else:\n            self.login_context = self.context\n\n        self.init_child_result = dct['init_child_result']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _connect(self):\n        if self.connected:\n            return\n\n        self._connect_broker()\n        stack = self._build_stack()\n        self._connect_stack(stack)", "response": "Establish a connection to the master process s UNIX listener socket and create a stack of contexts that will be used to communicate with the master process s UNIX listener socket and the parent context."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nforgets everything we know about the connected context.", "response": "def _mitogen_reset(self, mode):\n        \"\"\"\n        Forget everything we know about the connected context. This function\n        cannot be called _reset() since that name is used as a public API by\n        Ansible 2.4 wait_for_connection plug-in.\n\n        :param str mode:\n            Name of ContextService method to use to discard the context, either\n            'put' or 'reset'.\n        \"\"\"\n        if not self.context:\n            return\n\n        self.chain.reset()\n        self.parent.call_service(\n            service_name='ansible_mitogen.services.ContextService',\n            method_name=mode,\n            context=self.context\n        )\n\n        self.context = None\n        self.login_context = None\n        self.init_child_result = None\n        self.chain = None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _reset_find_task_vars(self):\n        frame = sys._getframe()\n        while frame and not self._task_vars:\n            self._task_vars = frame.f_locals.get('all_vars')\n            frame = frame.f_back", "response": "Find all task variables that are set in the current context."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef reset(self):\n        if self._task_vars is None:\n            self._reset_find_task_vars()\n\n        if self._play_context.remote_addr is None:\n            # <2.5.6 incorrectly populate PlayContext for reset_connection\n            # https://github.com/ansible/ansible/issues/27520\n            raise ansible.errors.AnsibleConnectionFailure(\n                self.reset_compat_msg\n            )\n\n        self._connect()\n        self._mitogen_reset(mode='reset')\n        self._shutdown_broker()", "response": "Explicitly terminate the connection to the remote host."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the chain for executing the current call.", "response": "def get_chain(self, use_login=False, use_fork=False):\n        \"\"\"\n        Return the :class:`mitogen.parent.CallChain` to use for executing\n        function calls.\n\n        :param bool use_login:\n            If :data:`True`, always return the chain for the login account\n            rather than any active become user.\n        :param bool use_fork:\n            If :data:`True`, return the chain for the fork parent.\n        :returns mitogen.parent.CallChain:\n        \"\"\"\n        self._connect()\n        if use_login:\n            return self.login_context.default_call_chain\n        # See FORK_SUPPORTED comments in target.py.\n        if use_fork and self.init_child_result['fork_context'] is not None:\n            return self.init_child_result['fork_context'].default_call_chain\n        return self.chain"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nspawn a new child on the target context.", "response": "def spawn_isolated_child(self):\n        \"\"\"\n        Fork or launch a new child off the target context.\n\n        :returns:\n            mitogen.core.Context of the new child.\n        \"\"\"\n        return self.get_chain(use_fork=True).call(\n            ansible_mitogen.target.spawn_isolated_child\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef exec_command(self, cmd, in_data='', sudoable=True, mitogen_chdir=None):\n        emulate_tty = (not in_data and sudoable)\n        rc, stdout, stderr = self.get_chain().call(\n            ansible_mitogen.target.exec_command,\n            cmd=mitogen.utils.cast(cmd),\n            in_data=mitogen.utils.cast(in_data),\n            chdir=mitogen_chdir or self.get_default_cwd(),\n            emulate_tty=emulate_tty,\n        )\n\n        stderr += b'Shared connection to %s closed.%s' % (\n            self._play_context.remote_addr.encode(),\n            (b'\\r\\n' if emulate_tty else b'\\n'),\n        )\n        return rc, stdout, stderr", "response": "Implement exec_command by calling the corresponding\n            target function in the target."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nimplementing fetch_file by calling the corresponding read_path and write_path methods.", "response": "def fetch_file(self, in_path, out_path):\n        \"\"\"\n        Implement fetch_file() by calling the corresponding\n        ansible_mitogen.target function in the target.\n\n        :param str in_path:\n            Remote filesystem path to read.\n        :param str out_path:\n            Local filesystem path to write.\n        \"\"\"\n        output = self.get_chain().call(\n            ansible_mitogen.target.read_path,\n            mitogen.utils.cast(in_path),\n        )\n        ansible_mitogen.target.write_path(out_path, output)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef put_data(self, out_path, data, mode=None, utimes=None):\n        self.get_chain().call_no_reply(\n            ansible_mitogen.target.write_path,\n            mitogen.utils.cast(out_path),\n            mitogen.core.Blob(data),\n            mode=mode,\n            utimes=utimes,\n        )", "response": "Implement put_file method by caling the corresponding ansible_mitogen. target\n        function in the target and then calling the corresponding ansible_mitogen. target. write_file method with the data."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef put_file(self, in_path, out_path):\n        try:\n            st = os.stat(in_path)\n        except OSError as e:\n            self._throw_io_error(e, in_path)\n            raise\n\n        if not stat.S_ISREG(st.st_mode):\n            raise IOError('%r is not a regular file.' % (in_path,))\n\n        # If the file is sufficiently small, just ship it in the argument list\n        # rather than introducing an extra RTT for the child to request it from\n        # FileService.\n        if st.st_size <= self.SMALL_FILE_LIMIT:\n            try:\n                fp = open(in_path, 'rb')\n                try:\n                    s = fp.read(self.SMALL_FILE_LIMIT + 1)\n                finally:\n                    fp.close()\n            except OSError:\n                self._throw_io_error(e, in_path)\n                raise\n\n            # Ensure did not grow during read.\n            if len(s) == st.st_size:\n                return self.put_data(out_path, s, mode=st.st_mode,\n                                     utimes=(st.st_atime, st.st_mtime))\n\n        self._connect()\n        self.parent.call_service(\n            service_name='mitogen.service.FileService',\n            method_name='register',\n            path=mitogen.utils.cast(in_path)\n        )\n\n        # For now this must remain synchronous, as the action plug-in may have\n        # passed us a temporary file to transfer. A future FileService could\n        # maintain an LRU list of open file descriptors to keep the temporary\n        # file alive, but that requires more work.\n        self.get_chain().call(\n            ansible_mitogen.target.transfer_file,\n            context=self.parent,\n            in_path=in_path,\n            out_path=out_path\n        )", "response": "Implement put_file by streamily transferring the file via FileService."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _ioctl_cast(n):\n    if sys.version_info < (2, 5):\n        n, = struct.unpack('i', struct.pack('I', n))\n    return n", "response": "Convert an unsigned int to a long integer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn sys. executable if it is set otherwise return sys. executable and log a warning.", "response": "def get_sys_executable():\n    \"\"\"\n    Return :data:`sys.executable` if it is set, otherwise return\n    ``\"/usr/bin/python\"`` and log a warning.\n    \"\"\"\n    if sys.executable:\n        return sys.executable\n\n    global _sys_executable_warning_logged\n    if not _sys_executable_warning_logged:\n        LOG.warn(SYS_EXECUTABLE_MSG)\n        _sys_executable_warning_logged = True\n\n    return '/usr/bin/python'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the core source as a partial Zlib object.", "response": "def get_core_source_partial():\n    \"\"\"\n    _get_core_source() is expensive, even with @lru_cache in minify.py, threads\n    can enter it simultaneously causing severe slowdowns.\n    \"\"\"\n    global _core_source_partial\n\n    if _core_source_partial is None:\n        _core_source_lock.acquire()\n        try:\n            if _core_source_partial is None:\n                _core_source_partial = PartialZlib(\n                    _get_core_source().encode('utf-8')\n                )\n        finally:\n            _core_source_lock.release()\n\n    return _core_source_partial"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the default remote name appearing in argv [ 0 ] of remote machines.", "response": "def get_default_remote_name():\n    \"\"\"\n    Return the default name appearing in argv[0] of remote machines.\n    \"\"\"\n    s = u'%s@%s:%d'\n    s %= (getpass.getuser(), socket.gethostname(), os.getpid())\n    # In mixed UNIX/Windows environments, the username may contain slashes.\n    return s.translate({\n        ord(u'\\\\'): ord(u'_'),\n        ord(u'/'): ord(u'_')\n    })"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef cfmakeraw(tflags):\n    # BSD: https://github.com/freebsd/freebsd/blob/master/lib/libc/gen/termios.c#L162\n    # Linux: https://github.com/lattera/glibc/blob/master/termios/cfmakeraw.c#L20\n    iflag, oflag, cflag, lflag, ispeed, ospeed, cc = tflags\n    iflag &= ~flags('IMAXBEL IXOFF INPCK BRKINT PARMRK ISTRIP INLCR ICRNL IXON IGNPAR')\n    iflag &= ~flags('IGNBRK BRKINT PARMRK')\n    oflag &= ~flags('OPOST')\n    lflag &= ~flags('ECHO ECHOE ECHOK ECHONL ICANON ISIG IEXTEN NOFLSH TOSTOP PENDIN')\n    cflag &= ~flags('CSIZE PARENB')\n    cflag |= flags('CS8 CREAD')\n    return [iflag, oflag, cflag, lflag, ispeed, ospeed, cc]", "response": "Given a list returned by termios. tcgetattr return a listof\n    modified in a manner similar to the CFMakeraw C library function but\n    additionally disabling local echo."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a socketpair to use for use as a child process s UNIX stdio channels.", "response": "def create_socketpair(size=None):\n    \"\"\"\n    Create a :func:`socket.socketpair` to use for use as a child process's UNIX\n    stdio channels. As socket pairs are bidirectional, they are economical on\n    file descriptor usage as the same descriptor can be used for ``stdin`` and\n    ``stdout``. As they are sockets their buffers are tunable, allowing large\n    buffers to be configured in order to improve throughput for file transfers\n    and reduce :class:`mitogen.core.Broker` IO loop iterations.\n    \"\"\"\n    parentfp, childfp = socket.socketpair()\n    parentfp.setsockopt(socket.SOL_SOCKET,\n                        socket.SO_SNDBUF,\n                        size or mitogen.core.CHUNK_SIZE)\n    childfp.setsockopt(socket.SOL_SOCKET,\n                       socket.SO_RCVBUF,\n                       size or mitogen.core.CHUNK_SIZE)\n    return parentfp, childfp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetach a new child process from the current process.", "response": "def detach_popen(**kwargs):\n    \"\"\"\n    Use :class:`subprocess.Popen` to construct a child process, then hack the\n    Popen so that it forgets the child it created, allowing it to survive a\n    call to Popen.__del__.\n\n    If the child process is not detached, there is a race between it exitting\n    and __del__ being called. If it exits before __del__ runs, then __del__'s\n    call to :func:`os.waitpid` will capture the one and only exit event\n    delivered to this process, causing later 'legitimate' calls to fail with\n    ECHILD.\n\n    :param list close_on_error:\n        Array of integer file descriptors to close on exception.\n    :returns:\n        Process ID of the new child.\n    \"\"\"\n    # This allows Popen() to be used for e.g. graceful post-fork error\n    # handling, without tying the surrounding code into managing a Popen\n    # object, which isn't possible for at least :mod:`mitogen.fork`. This\n    # should be replaced by a swappable helper class in a future version.\n    real_preexec_fn = kwargs.pop('preexec_fn', None)\n    def preexec_fn():\n        if _preexec_hook:\n            _preexec_hook()\n        if real_preexec_fn:\n            real_preexec_fn()\n    proc = subprocess.Popen(preexec_fn=preexec_fn, **kwargs)\n    proc._child_created = False\n    return proc.pid"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a child process that will be used to communicate with the current process.", "response": "def create_child(args, merge_stdio=False, stderr_pipe=False, preexec_fn=None):\n    \"\"\"\n    Create a child process whose stdin/stdout is connected to a socket.\n\n    :param args:\n        Argument vector for execv() call.\n    :param bool merge_stdio:\n        If :data:`True`, arrange for `stderr` to be connected to the `stdout`\n        socketpair, rather than inherited from the parent process. This may be\n        necessary to ensure that not TTY is connected to any stdio handle, for\n        instance when using LXC.\n    :param bool stderr_pipe:\n        If :data:`True` and `merge_stdio` is :data:`False`, arrange for\n        `stderr` to be connected to a separate pipe, to allow any ongoing debug\n        logs generated by e.g. SSH to be outpu as the session progresses,\n        without interfering with `stdout`.\n    :returns:\n        `(pid, socket_obj, :data:`None` or pipe_fd)`\n    \"\"\"\n    parentfp, childfp = create_socketpair()\n    # When running under a monkey patches-enabled gevent, the socket module\n    # yields file descriptors who already have O_NONBLOCK, which is\n    # persisted across fork, totally breaking Python. Therefore, drop\n    # O_NONBLOCK from Python's future stdin fd.\n    mitogen.core.set_block(childfp.fileno())\n\n    stderr_r = None\n    extra = {}\n    if merge_stdio:\n        extra = {'stderr': childfp}\n    elif stderr_pipe:\n        stderr_r, stderr_w = os.pipe()\n        mitogen.core.set_cloexec(stderr_r)\n        mitogen.core.set_cloexec(stderr_w)\n        extra = {'stderr': stderr_w}\n\n    try:\n        pid = detach_popen(\n            args=args,\n            stdin=childfp,\n            stdout=childfp,\n            close_fds=True,\n            preexec_fn=preexec_fn,\n            **extra\n        )\n    except Exception:\n        childfp.close()\n        parentfp.close()\n        if stderr_pipe:\n            os.close(stderr_r)\n            os.close(stderr_w)\n        raise\n\n    if stderr_pipe:\n        os.close(stderr_w)\n    childfp.close()\n    # Decouple the socket from the lifetime of the Python socket object.\n    fd = os.dup(parentfp.fileno())\n    parentfp.close()\n\n    LOG.debug('create_child() child %d fd %d, parent %d, cmd: %s',\n              pid, fd, os.getpid(), Argv(args))\n    return pid, fd, stderr_r"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _linux_broken_devpts_openpty():\n    master_fd = None\n    try:\n        # Opening /dev/ptmx causes a PTY pair to be allocated, and the\n        # corresponding slave /dev/pts/* device to be created, owned by UID/GID\n        # matching this process.\n        master_fd = os.open('/dev/ptmx', os.O_RDWR)\n        # Clear the lock bit from the PTY. This a prehistoric feature from a\n        # time when slave device files were persistent.\n        fcntl.ioctl(master_fd, LINUX_TIOCSPTLCK, struct.pack('i', 0))\n        # Since v4.13 TIOCGPTPEER exists to open the slave in one step, but we\n        # must support older kernels. Ask for the PTY number.\n        pty_num_s = fcntl.ioctl(master_fd, LINUX_TIOCGPTN,\n                                struct.pack('i', 0))\n        pty_num, = struct.unpack('i', pty_num_s)\n        pty_name = '/dev/pts/%d' % (pty_num,)\n        # Now open it with O_NOCTTY to ensure it doesn't change our controlling\n        # TTY. Otherwise when we close the FD we get killed by the kernel, and\n        # the child we spawn that should really attach to it will get EPERM\n        # during _acquire_controlling_tty().\n        slave_fd = os.open(pty_name, os.O_RDWR|os.O_NOCTTY)\n        return master_fd, slave_fd\n    except OSError:\n        if master_fd is not None:\n            os.close(master_fd)\n        e = sys.exc_info()[1]\n        raise mitogen.core.StreamError(OPENPTY_MSG, e)", "response": "Open a PTY pair for broken Linux devices."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a new PTY object.", "response": "def openpty():\n    \"\"\"\n    Call :func:`os.openpty`, raising a descriptive error if the call fails.\n\n    :raises mitogen.core.StreamError:\n        Creating a PTY failed.\n    :returns:\n        See :func`os.openpty`.\n    \"\"\"\n    try:\n        return os.openpty()\n    except OSError:\n        e = sys.exc_info()[1]\n        if IS_LINUX and e.args[0] == errno.EPERM:\n            return _linux_broken_devpts_openpty()\n        raise mitogen.core.StreamError(OPENPTY_MSG, e)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new child process that is connected to the master end of a pseudo - terminal and the slave end of a new child process.", "response": "def tty_create_child(args):\n    \"\"\"\n    Return a file descriptor connected to the master end of a pseudo-terminal,\n    whose slave end is connected to stdin/stdout/stderr of a new child process.\n    The child is created such that the pseudo-terminal becomes its controlling\n    TTY, ensuring access to /dev/tty returns a new file descriptor open on the\n    slave end.\n\n    :param list args:\n        :py:func:`os.execl` argument list.\n\n    :returns:\n        `(pid, tty_fd, None)`\n    \"\"\"\n    master_fd, slave_fd = openpty()\n    try:\n        mitogen.core.set_block(slave_fd)\n        disable_echo(master_fd)\n        disable_echo(slave_fd)\n\n        pid = detach_popen(\n            args=args,\n            stdin=slave_fd,\n            stdout=slave_fd,\n            stderr=slave_fd,\n            preexec_fn=_acquire_controlling_tty,\n            close_fds=True,\n        )\n    except Exception:\n        os.close(master_fd)\n        os.close(slave_fd)\n        raise\n\n    os.close(slave_fd)\n    LOG.debug('tty_create_child() child %d fd %d, parent %d, cmd: %s',\n              pid, master_fd, os.getpid(), Argv(args))\n    return pid, master_fd, None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hybrid_tty_create_child(args):\n    master_fd, slave_fd = openpty()\n\n    try:\n        disable_echo(master_fd)\n        disable_echo(slave_fd)\n        mitogen.core.set_block(slave_fd)\n\n        parentfp, childfp = create_socketpair()\n        try:\n            mitogen.core.set_block(childfp)\n            pid = detach_popen(\n                args=args,\n                stdin=childfp,\n                stdout=childfp,\n                stderr=slave_fd,\n                preexec_fn=_acquire_controlling_tty,\n                close_fds=True,\n            )\n        except Exception:\n            parentfp.close()\n            childfp.close()\n            raise\n    except Exception:\n        os.close(master_fd)\n        os.close(slave_fd)\n        raise\n\n    os.close(slave_fd)\n    childfp.close()\n    # Decouple the socket from the lifetime of the Python socket object.\n    stdio_fd = os.dup(parentfp.fileno())\n    parentfp.close()\n\n    LOG.debug('hybrid_tty_create_child() pid=%d stdio=%d, tty=%d, cmd: %s',\n              pid, stdio_fd, master_fd, Argv(args))\n    return pid, stdio_fd, master_fd", "response": "This function creates a new child process that is connected to a TTY."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_all(fd, s, deadline=None):\n    timeout = None\n    written = 0\n    poller = PREFERRED_POLLER()\n    poller.start_transmit(fd)\n\n    try:\n        while written < len(s):\n            if deadline is not None:\n                timeout = max(0, deadline - time.time())\n            if timeout == 0:\n                raise mitogen.core.TimeoutError('write timed out')\n\n            if mitogen.core.PY3:\n                window = memoryview(s)[written:]\n            else:\n                window = buffer(s, written)\n\n            for fd in poller.poll(timeout):\n                n, disconnected = mitogen.core.io_op(os.write, fd, window)\n                if disconnected:\n                    raise EofError('EOF on stream during write')\n\n                written += n\n    finally:\n        poller.close()", "response": "Arrange for all of bytestring s to be written to the file descriptor fd."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef discard_until(fd, s, deadline):\n    it = iter_read([fd], deadline)\n    try:\n        for buf in it:\n            if IOLOG.level == logging.DEBUG:\n                for line in buf.splitlines():\n                    IOLOG.debug('discard_until: discarding %r', line)\n            if buf.endswith(s):\n                return\n    finally:\n        it.close()", "response": "Read chunks from fd until one is encountered that ends with s."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _upgrade_broker(broker):\n    # This function is deadly! The act of calling start_receive() generates log\n    # messages which must be silenced as the upgrade progresses, otherwise the\n    # poller state will change as it is copied, resulting in write fds that are\n    # lost. (Due to LogHandler->Router->Stream->Broker->Poller, where Stream\n    # only calls start_transmit() when transitioning from empty to non-empty\n    # buffer. If the start_transmit() is lost, writes from the child hang\n    # permanently).\n    root = logging.getLogger()\n    old_level = root.level\n    root.setLevel(logging.CRITICAL)\n\n    old = broker.poller\n    new = PREFERRED_POLLER()\n    for fd, data in old.readers:\n        new.start_receive(fd, data)\n    for fd, data in old.writers:\n        new.start_transmit(fd, data)\n\n    old.close()\n    broker.poller = new\n    root.setLevel(old_level)\n    LOG.debug('replaced %r with %r (new: %d readers, %d writers; '\n              'old: %d readers, %d writers)', old, new,\n              len(new.readers), len(new.writers),\n              len(old.readers), len(old.writers))", "response": "Upgrade the broker with the industrial\n    strength poller."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive the name of a Mitogen connection method import its implementation module and return its Stream subclass.", "response": "def stream_by_method_name(name):\n    \"\"\"\n    Given the name of a Mitogen connection method, import its implementation\n    module and return its Stream subclass.\n    \"\"\"\n    if name == u'local':\n        name = u'parent'\n    module = mitogen.core.import_module(u'mitogen.' + name)\n    return module.Stream"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nimplement the target portion of Router. _proxy_connect by upgrading the local context to a parent if it was not already.", "response": "def _proxy_connect(name, method_name, kwargs, econtext):\n    \"\"\"\n    Implements the target portion of Router._proxy_connect() by upgrading the\n    local context to a parent if it was not already, then calling back into\n    Router._connect() using the arguments passed to the parent's\n    Router.connect().\n\n    :returns:\n        Dict containing:\n        * ``id``: :data:`None`, or integer new context ID.\n        * ``name``: :data:`None`, or string name attribute of new Context.\n        * ``msg``: :data:`None`, or StreamError exception text.\n    \"\"\"\n    upgrade_router(econtext)\n\n    try:\n        context = econtext.router._connect(\n            klass=stream_by_method_name(method_name),\n            name=name,\n            **kwargs\n        )\n    except mitogen.core.StreamError:\n        return {\n            u'id': None,\n            u'name': None,\n            u'msg': 'error occurred on host %s: %s' % (\n                socket.gethostname(),\n                sys.exc_info()[1],\n            ),\n        }\n\n    return {\n        u'id': context.context_id,\n        u'name': context.name,\n        u'msg': None,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef wstatus_to_str(status):\n    if os.WIFEXITED(status):\n        return 'exited with return code %d' % (os.WEXITSTATUS(status),)\n    if os.WIFSIGNALED(status):\n        n = os.WTERMSIG(status)\n        return 'exited due to signal %d (%s)' % (n, SIGNAL_BY_NUM.get(n))\n    if os.WIFSTOPPED(status):\n        n = os.WSTOPSIG(status)\n        return 'stopped due to signal %d (%s)' % (n, SIGNAL_BY_NUM.get(n))\n    return 'unknown wait status (%d)' % (status,)", "response": "Parse and format a : func : os. waitstatus."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nappending the bytestring s to the compressor state and return the final compressed output.", "response": "def append(self, s):\n        \"\"\"\n        Append the bytestring `s` to the compressor state and return the\n        final compressed output.\n        \"\"\"\n        if self._compressor is None:\n            return zlib.compress(self.s + s, 9)\n        else:\n            compressor = self._compressor.copy()\n            out = self._out\n            out += compressor.compress(s)\n            return out + compressor.flush()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconstructs the named context running on the local machine creating it if it does not exist.", "response": "def construct(self, max_message_size, remote_name=None, python_path=None,\n                  debug=False, connect_timeout=None, profiling=False,\n                  unidirectional=False, old_router=None, **kwargs):\n        \"\"\"Get the named context running on the local machine, creating it if\n        it does not exist.\"\"\"\n        super(Stream, self).construct(**kwargs)\n        self.max_message_size = max_message_size\n        if python_path:\n            self.python_path = python_path\n        if connect_timeout:\n            self.connect_timeout = connect_timeout\n        if remote_name is None:\n            remote_name = get_default_remote_name()\n        if '/' in remote_name or '\\\\' in remote_name:\n            raise ValueError('remote_name= cannot contain slashes')\n        self.remote_name = remote_name\n        self.debug = debug\n        self.profiling = profiling\n        self.unidirectional = unidirectional\n        self.max_message_size = max_message_size\n        self.connect_deadline = time.time() + self.connect_timeout"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef on_shutdown(self, broker):\n        LOG.debug('%r closing CALL_FUNCTION channel', self)\n        self._send(\n            mitogen.core.Message(\n                src_id=mitogen.context_id,\n                dst_id=self.remote_id,\n                handle=mitogen.core.SHUTDOWN,\n            )\n        )", "response": "Request the slave gracefully shut itself down."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreaps the child process during disconnection.", "response": "def _reap_child(self):\n        \"\"\"\n        Reap the child process during disconnection.\n        \"\"\"\n        if self.detached and self.child_is_immediate_subprocess:\n            LOG.debug('%r: immediate child is detached, won\\'t reap it', self)\n            return\n\n        if self.profiling:\n            LOG.info('%r: wont kill child because profiling=True', self)\n            return\n\n        if self._reaped:\n            # on_disconnect() may be invoked more than once, for example, if\n            # there is still a pending message to be sent after the first\n            # on_disconnect() call.\n            return\n\n        try:\n            pid, status = os.waitpid(self.pid, os.WNOHANG)\n        except OSError:\n            e = sys.exc_info()[1]\n            if e.args[0] == errno.ECHILD:\n                LOG.warn('%r: waitpid(%r) produced ECHILD', self, self.pid)\n                return\n            raise\n\n        self._reaped = True\n        if pid:\n            LOG.debug('%r: PID %d %s', self, pid, wstatus_to_str(status))\n            return\n\n        if not self._router.profiling:\n            # For processes like sudo we cannot actually send sudo a signal,\n            # because it is setuid, so this is best-effort only.\n            LOG.debug('%r: child process still alive, sending SIGTERM', self)\n            try:\n                os.kill(self.pid, signal.SIGTERM)\n            except OSError:\n                e = sys.exc_info()[1]\n                if e.args[0] != errno.EPERM:\n                    raise"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_python_argv(self):\n        if isinstance(self.python_path, list):\n            return self.python_path\n        return [self.python_path]", "response": "Returns the initial argument vector elements necessary to invoke Python on the specified modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nhandles an EOF error.", "response": "def _adorn_eof_error(self, e):\n        \"\"\"\n        Used by subclasses to provide additional information in the case of a\n        failed connection.\n        \"\"\"\n        if self.eof_error_hint:\n            e.args = ('%s\\n\\n%s' % (e.args[0], self.eof_error_hint),)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef reset(self):\n        if not self.chain_id:\n            return\n\n        saved, self.chain_id = self.chain_id, None\n        try:\n            self.call_no_reply(mitogen.core.Dispatcher.forget_chain, saved)\n        finally:\n            self.chain_id = saved", "response": "Reset the internal state of the internal state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nliking call_async but do not wait for a reply to the target context.", "response": "def call_no_reply(self, fn, *args, **kwargs):\n        \"\"\"\n        Like :meth:`call_async`, but do not wait for a return value, and inform\n        the target context no reply is expected. If the call fails and\n        pipelining is disabled, the exception will be logged to the target\n        context's logging framework.\n        \"\"\"\n        LOG.debug('%r.call_no_reply(): %r', self, CallSpec(fn, args, kwargs))\n        self.context.send(self.make_msg(fn, *args, **kwargs))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef call_async(self, fn, *args, **kwargs):\n        LOG.debug('%r.call_async(): %r', self, CallSpec(fn, args, kwargs))\n        return self.context.send_async(self.make_msg(fn, *args, **kwargs))", "response": "Asynchronously calls a function on the main thread."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlikes :meth:`call_async`, but block until the return value is available. Equivalent to:: call_async(fn, *args, **kwargs).get().unpickle() :returns: The function's return value. :raises mitogen.core.CallError: An exception was raised in the remote context during execution.", "response": "def call(self, fn, *args, **kwargs):\n        \"\"\"\n        Like :meth:`call_async`, but block until the return value is available.\n        Equivalent to::\n\n            call_async(fn, *args, **kwargs).get().unpickle()\n\n        :returns:\n            The function's return value.\n        :raises mitogen.core.CallError:\n            An exception was raised in the remote context during execution.\n        \"\"\"\n        receiver = self.call_async(fn, *args, **kwargs)\n        return receiver.get().unpickle(throw_dead=False)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call_async(self, fn, *args, **kwargs):\n        return self.default_call_chain.call_async(fn, *args, **kwargs)", "response": "Call the given function asynchronously."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls the given function with the given arguments.", "response": "def call(self, fn, *args, **kwargs):\n        \"\"\"\n        See :meth:`CallChain.call`.\n        \"\"\"\n        return self.default_call_chain.call(fn, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef call_no_reply(self, fn, *args, **kwargs):\n        self.default_call_chain.call_no_reply(fn, *args, **kwargs)", "response": "Calls the specified function with no reply arguments."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef shutdown(self, wait=False):\n        LOG.debug('%r.shutdown() sending SHUTDOWN', self)\n        latch = mitogen.core.Latch()\n        mitogen.core.listen(self, 'disconnect', lambda: latch.put(None))\n        self.send(\n            mitogen.core.Message(\n                handle=mitogen.core.SHUTDOWN,\n            )\n        )\n\n        if wait:\n            latch.get()\n        else:\n            return latch", "response": "Send a SHUTDOWN message to the context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _send_one(self, stream, handle, target_id, name):\n        if not stream:\n            # We may not have a stream during shutdown.\n            return\n\n        data = str(target_id)\n        if name:\n            data = '%s:%s' % (target_id, name)\n        stream.send(\n            mitogen.core.Message(\n                handle=handle,\n                data=data.encode('utf-8'),\n                dst_id=stream.remote_id,\n            )\n        )", "response": "Compose and send an update message on a stream."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npropagate an update to the master.", "response": "def _propagate_up(self, handle, target_id, name=None):\n        \"\"\"\n        In a non-master context, propagate an update towards the master.\n\n        :param int handle:\n            :data:`mitogen.core.ADD_ROUTE` or :data:`mitogen.core.DEL_ROUTE`\n        :param int target_id:\n            ID of the connecting or disconnecting context.\n        :param str name:\n            For :data:`mitogen.core.ADD_ROUTE`, the name of the new context\n            assigned by its parent. This is used by parents to assign the\n            :attr:`mitogen.core.Context.name` attribute.\n        \"\"\"\n        if self.parent:\n            stream = self.router.stream_by_id(self.parent.context_id)\n            self._send_one(stream, handle, target_id, name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npropagates the message down to any of the streams that have ever communicated with the disconnecting ID.", "response": "def _propagate_down(self, handle, target_id):\n        \"\"\"\n        For DEL_ROUTE, we additionally want to broadcast the message to any\n        stream that has ever communicated with the disconnecting ID, so\n        core.py's :meth:`mitogen.core.Router._on_del_route` can turn the\n        message into a disconnect event.\n\n        :param int handle:\n            :data:`mitogen.core.ADD_ROUTE` or :data:`mitogen.core.DEL_ROUTE`\n        :param int target_id:\n            ID of the connecting or disconnecting context.\n        \"\"\"\n        for stream in self.router.get_streams():\n            if target_id in stream.egress_ids and (\n                    (self.parent is None) or\n                    (self.parent.context_id != stream.remote_id)\n                ):\n                self._send_one(stream, mitogen.core.DEL_ROUTE, target_id, None)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nnotice that a stream has been connected.", "response": "def notice_stream(self, stream):\n        \"\"\"\n        When this parent is responsible for a new directly connected child\n        stream, we're also responsible for broadcasting DEL_ROUTE upstream\n        if/when that child disconnects.\n        \"\"\"\n        self._routes_by_stream[stream] = set([stream.remote_id])\n        self._propagate_up(mitogen.core.ADD_ROUTE, stream.remote_id,\n                        stream.name)\n        mitogen.core.listen(\n            obj=stream,\n            name='disconnect',\n            func=lambda: self._on_stream_disconnect(stream),\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _on_stream_disconnect(self, stream):\n        # During a stream crash it is possible for disconnect signal to fire\n        # twice, in which case ignore the second instance.\n        routes = self._routes_by_stream.pop(stream, None)\n        if routes is None:\n            return\n\n        LOG.debug('%r: %r is gone; propagating DEL_ROUTE for %r',\n                  self, stream, routes)\n        for target_id in routes:\n            self.router.del_route(target_id)\n            self._propagate_up(mitogen.core.DEL_ROUTE, target_id)\n            self._propagate_down(mitogen.core.DEL_ROUTE, target_id)\n\n            context = self.router.context_by_id(target_id, create=False)\n            if context:\n                mitogen.core.fire(context, 'disconnect')", "response": "Respond to a disconnection of a local stream by propagating DEL_ROUTE for any contexts we know are attached to it."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresponding to a Mitogen. core. ADD_ROUTE message.", "response": "def _on_add_route(self, msg):\n        \"\"\"\n        Respond to :data:`mitogen.core.ADD_ROUTE` by validating the source of\n        the message, updating the local table, and propagating the message\n        upwards.\n        \"\"\"\n        if msg.is_dead:\n            return\n\n        target_id_s, _, target_name = bytes_partition(msg.data, b(':'))\n        target_name = target_name.decode()\n        target_id = int(target_id_s)\n        self.router.context_by_id(target_id).name = target_name\n        stream = self.router.stream_by_id(msg.auth_id)\n        current = self.router.stream_by_id(target_id)\n        if current and current.remote_id != mitogen.parent_id:\n            LOG.error('Cannot add duplicate route to %r via %r, '\n                      'already have existing route via %r',\n                      target_id, stream, current)\n            return\n\n        LOG.debug('Adding route to %d via %r', target_id, stream)\n        self._routes_by_stream[stream].add(target_id)\n        self.router.add_route(target_id, stream)\n        self._propagate_up(mitogen.core.ADD_ROUTE, target_id, target_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _on_del_route(self, msg):\n        if msg.is_dead:\n            return\n\n        target_id = int(msg.data)\n        registered_stream = self.router.stream_by_id(target_id)\n        if registered_stream is None:\n            return\n\n        stream = self.router.stream_by_id(msg.auth_id)\n        if registered_stream != stream:\n            LOG.error('%r: received DEL_ROUTE for %d from %r, expected %r',\n                      self, target_id, stream, registered_stream)\n            return\n\n        context = self.router.context_by_id(target_id, create=False)\n        if context:\n            LOG.debug('%r: firing local disconnect for %r', self, context)\n            mitogen.core.fire(context, 'disconnect')\n\n        LOG.debug('%r: deleting route to %d via %r', self, target_id, stream)\n        routes = self._routes_by_stream.get(stream)\n        if routes:\n            routes.discard(target_id)\n\n        self.router.del_route(target_id)\n        if stream.remote_id != mitogen.parent_id:\n            self._propagate_up(mitogen.core.DEL_ROUTE, target_id)\n        self._propagate_down(mitogen.core.DEL_ROUTE, target_id)", "response": "Handle a DEL_ROUTE message."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a snapshot of all streams in existence at time of call.", "response": "def get_streams(self):\n        \"\"\"\n        Return a snapshot of all streams in existence at time of call.\n        \"\"\"\n        self._write_lock.acquire()\n        try:\n            return itervalues(self._stream_by_id)\n        finally:\n            self._write_lock.release()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_route(self, target_id, stream):\n        LOG.debug('%r.add_route(%r, %r)', self, target_id, stream)\n        assert isinstance(target_id, int)\n        assert isinstance(stream, Stream)\n\n        self._write_lock.acquire()\n        try:\n            self._stream_by_id[target_id] = stream\n        finally:\n            self._write_lock.release()", "response": "Arrange for messages whose dst_id is target_id to be forwarded on the directly connected stream for via_id."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd 256 bits of /dev/urandom to OpenSSL's PRNG in the child, and re-seed the random package with the same data.", "response": "def fixup_prngs():\n    \"\"\"\n    Add 256 bits of /dev/urandom to OpenSSL's PRNG in the child, and re-seed\n    the random package with the same data.\n    \"\"\"\n    s = os.urandom(256 // 8)\n    random.seed(s)\n    if 'ssl' in sys.modules:\n        sys.modules['ssl'].RAND_add(s, 75.0)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nresets the logging framework to the default state.", "response": "def reset_logging_framework():\n    \"\"\"\n    After fork, ensure any logging.Handler locks are recreated, as a variety of\n    threads in the parent may have been using the logging package at the moment\n    of fork.\n\n    It is not possible to solve this problem in general; see\n    https://github.com/dw/mitogen/issues/150 for a full discussion.\n    \"\"\"\n    logging._lock = threading.RLock()\n\n    # The root logger does not appear in the loggerDict.\n    for name in [None] + list(logging.Logger.manager.loggerDict):\n        for handler in logging.getLogger(name).handlers:\n            handler.createLock()\n\n    root = logging.getLogger()\n    root.handlers = [\n        handler\n        for handler in root.handlers\n        if not isinstance(handler, mitogen.core.LogHandler)\n    ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls by the forked process when Mitogen is forked.", "response": "def on_fork():\n    \"\"\"\n    Should be called by any program integrating Mitogen each time the process\n    is forked, in the context of the new child.\n    \"\"\"\n    reset_logging_framework()  # Must be first!\n    fixup_prngs()\n    mitogen.core.Latch._on_fork()\n    mitogen.core.Side._on_fork()\n    mitogen.core.ExternalContext.service_stub_lock = threading.Lock()\n\n    mitogen__service = sys.modules.get('mitogen.service')\n    if mitogen__service:\n        mitogen__service._pool_lock = threading.Lock()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef handle_child_crash():\n    tty = open('/dev/tty', 'wb')\n    tty.write('\\n\\nFORKED CHILD PID %d CRASHED\\n%s\\n\\n' % (\n        os.getpid(),\n        traceback.format_exc(),\n    ))\n    tty.close()\n    os._exit(1)", "response": "Handle a child crash by ensuring that the relevant exception is logged to the tty."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef loads(s, encoding=None, cls=None, object_hook=None, parse_float=None,\n        parse_int=None, parse_constant=None, **kw):\n    \"\"\"Deserialize ``s`` (a ``str`` or ``unicode`` instance containing a JSON\n    document) to a Python object.\n\n    If ``s`` is a ``str`` instance and is encoded with an ASCII based encoding\n    other than utf-8 (e.g. latin-1) then an appropriate ``encoding`` name\n    must be specified. Encodings that are not ASCII based (such as UCS-2)\n    are not allowed and should be decoded to ``unicode`` first.\n\n    ``object_hook`` is an optional function that will be called with the\n    result of any object literal decode (a ``dict``). The return value of\n    ``object_hook`` will be used instead of the ``dict``. This feature\n    can be used to implement custom decoders (e.g. JSON-RPC class hinting).\n\n    ``parse_float``, if specified, will be called with the string\n    of every JSON float to be decoded. By default this is equivalent to\n    float(num_str). This can be used to use another datatype or parser\n    for JSON floats (e.g. decimal.Decimal).\n\n    ``parse_int``, if specified, will be called with the string\n    of every JSON int to be decoded. By default this is equivalent to\n    int(num_str). This can be used to use another datatype or parser\n    for JSON integers (e.g. float).\n\n    ``parse_constant``, if specified, will be called with one of the\n    following strings: -Infinity, Infinity, NaN, null, true, false.\n    This can be used to raise an exception if invalid JSON numbers\n    are encountered.\n\n    To use a custom ``JSONDecoder`` subclass, specify it with the ``cls``\n    kwarg.\n\n    \"\"\"\n    if (cls is None and encoding is None and object_hook is None and\n            parse_int is None and parse_float is None and\n            parse_constant is None and not kw):\n        return _default_decoder.decode(s)\n    if cls is None:\n        cls = JSONDecoder\n    if object_hook is not None:\n        kw['object_hook'] = object_hook\n    if parse_float is not None:\n        kw['parse_float'] = parse_float\n    if parse_int is not None:\n        kw['parse_int'] = parse_int\n    if parse_constant is not None:\n        kw['parse_constant'] = parse_constant\n    return cls(encoding=encoding, **kw).decode(s)", "response": "Deserialize a string or unicode object containing a JSON\n    document into a Python object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(log_level='INFO', profiling=_default_profiling):\n\n    def wrapper(func):\n        if func.__module__ != '__main__':\n            return func\n        import mitogen.parent\n        import mitogen.utils\n        if profiling:\n            mitogen.core.enable_profiling()\n            mitogen.master.Router.profiling = profiling\n        utils.log_to_file(level=log_level)\n        return mitogen.core._profile_hook(\n            'app.main',\n            utils.run_with_router,\n            func,\n        )\n    return wrapper", "response": "A decorator primarily useful for writing discardable test\n    scripts."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_small_file(context, path):\n    pool = mitogen.service.get_or_create_pool(router=context.router)\n    service = pool.get_service(u'mitogen.service.PushFileService')\n    return service.get(path)", "response": "Basic in - memory caching module fetcher. This function returns a small file from a file service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ntransferring a file from the multiplexer process in the local disk.", "response": "def transfer_file(context, in_path, out_path, sync=False, set_owner=False):\n    \"\"\"\n    Streamily download a file from the connection multiplexer process in the\n    controller.\n\n    :param mitogen.core.Context context:\n        Reference to the context hosting the FileService that will transmit the\n        file.\n    :param bytes in_path:\n        FileService registered name of the input file.\n    :param bytes out_path:\n        Name of the output path on the local disk.\n    :param bool sync:\n        If :data:`True`, ensure the file content and metadat are fully on disk\n        before renaming the temporary file over the existing file. This should\n        ensure in the case of system crash, either the entire old or new file\n        are visible post-reboot.\n    :param bool set_owner:\n        If :data:`True`, look up the metadata username and group on the local\n        system and file the file owner using :func:`os.fchmod`.\n    \"\"\"\n    out_path = os.path.abspath(out_path)\n    fd, tmp_path = tempfile.mkstemp(suffix='.tmp',\n                                    prefix='.ansible_mitogen_transfer-',\n                                    dir=os.path.dirname(out_path))\n    fp = os.fdopen(fd, 'wb', mitogen.core.CHUNK_SIZE)\n    LOG.debug('transfer_file(%r) temporary file: %s', out_path, tmp_path)\n\n    try:\n        try:\n            ok, metadata = mitogen.service.FileService.get(\n                context=context,\n                path=in_path,\n                out_fp=fp,\n            )\n            if not ok:\n                raise IOError('transfer of %r was interrupted.' % (in_path,))\n\n            set_file_mode(tmp_path, metadata['mode'], fd=fp.fileno())\n            if set_owner:\n                set_file_owner(tmp_path, metadata['owner'], metadata['group'],\n                               fd=fp.fileno())\n        finally:\n            fp.close()\n\n        if sync:\n            os.fsync(fp.fileno())\n        os.rename(tmp_path, out_path)\n    except BaseException:\n        os.unlink(tmp_path)\n        raise\n\n    os.utime(out_path, (metadata['atime'], metadata['mtime']))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves and delete the object at the given path.", "response": "def prune_tree(path):\n    \"\"\"\n    Like shutil.rmtree(), but log errors rather than discard them, and do not\n    waste multiple os.stat() calls discovering whether the object can be\n    deleted, just try deleting it instead.\n    \"\"\"\n    try:\n        os.unlink(path)\n        return\n    except OSError:\n        e = sys.exc_info()[1]\n        if not (os.path.isdir(path) and\n                e.args[0] in (errno.EPERM, errno.EISDIR)):\n            LOG.error('prune_tree(%r): %s', path, e)\n            return\n\n    try:\n        # Ensure write access for readonly directories. Ignore error in case\n        # path is on a weird filesystem (e.g. vfat).\n        os.chmod(path, int('0700', 8))\n    except OSError:\n        e = sys.exc_info()[1]\n        LOG.warning('prune_tree(%r): %s', path, e)\n\n    try:\n        for name in os.listdir(path):\n            if name not in ('.', '..'):\n                prune_tree(os.path.join(path, name))\n        os.rmdir(path)\n    except OSError:\n        e = sys.exc_info()[1]\n        LOG.error('prune_tree(%r): %s', path, e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_good_temp_dir(path):\n    if not os.path.exists(path):\n        try:\n            os.makedirs(path, mode=int('0700', 8))\n        except OSError:\n            e = sys.exc_info()[1]\n            LOG.debug('temp dir %r unusable: did not exist and attempting '\n                      'to create it failed: %s', path, e)\n            return False\n\n    try:\n        tmp = tempfile.NamedTemporaryFile(\n            prefix='ansible_mitogen_is_good_temp_dir',\n            dir=path,\n        )\n    except (OSError, IOError):\n        e = sys.exc_info()[1]\n        LOG.debug('temp dir %r unusable: %s', path, e)\n        return False\n\n    try:\n        try:\n            os.chmod(tmp.name, int('0700', 8))\n        except OSError:\n            e = sys.exc_info()[1]\n            LOG.debug('temp dir %r unusable: chmod failed: %s', path, e)\n            return False\n\n        try:\n            # access(.., X_OK) is sufficient to detect noexec.\n            if not os.access(tmp.name, os.X_OK):\n                raise OSError('filesystem appears to be mounted noexec')\n        except OSError:\n            e = sys.exc_info()[1]\n            LOG.debug('temp dir %r unusable: %s', path, e)\n            return False\n    finally:\n        tmp.close()\n\n    return True", "response": "Check if path can be used as a temporary directory."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef find_good_temp_dir(candidate_temp_dirs):\n    paths = [os.path.expandvars(os.path.expanduser(p))\n             for p in candidate_temp_dirs]\n    paths.extend(tempfile._candidate_tempdir_list())\n\n    for path in paths:\n        if is_good_temp_dir(path):\n            LOG.debug('Selected temp directory: %r (from %r)', path, paths)\n            return path\n\n    raise IOError(MAKE_TEMP_FAILED_MSG % {\n        'paths': '\\n    '.join(paths),\n    })", "response": "Given a list of candidate temp directories extracted from ansible. cfg then iterate over each candidate temp directory and return the path of that directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize a new child context for a master.", "response": "def init_child(econtext, log_level, candidate_temp_dirs):\n    \"\"\"\n    Called by ContextService immediately after connection; arranges for the\n    (presently) spotless Python interpreter to be forked, where the newly\n    forked interpreter becomes the parent of any newly forked future\n    interpreters.\n\n    This is necessary to prevent modules that are executed in-process from\n    polluting the global interpreter state in a way that effects explicitly\n    isolated modules.\n\n    :param int log_level:\n        Logging package level active in the master.\n    :param list[str] candidate_temp_dirs:\n        List of $variable-expanded and tilde-expanded directory names to add to\n        candidate list of temporary directories.\n\n    :returns:\n        Dict like::\n\n            {\n                'fork_context': mitogen.core.Context or None,\n                'good_temp_dir': ...\n                'home_dir': str\n            }\n\n        Where `fork_context` refers to the newly forked 'fork parent' context\n        the controller will use to start forked jobs, and `home_dir` is the\n        home directory for the active user account.\n    \"\"\"\n    # Copying the master's log level causes log messages to be filtered before\n    # they reach LogForwarder, thus reducing an influx of tiny messges waking\n    # the connection multiplexer process in the master.\n    LOG.setLevel(log_level)\n    logging.getLogger('ansible_mitogen').setLevel(log_level)\n\n    # issue #536: if the json module is available, remove simplejson from the\n    # importer whitelist to avoid confusing certain Ansible modules.\n    if json.__name__ == 'json':\n        econtext.importer.whitelist.remove('simplejson')\n\n    global _fork_parent\n    if FORK_SUPPORTED:\n        mitogen.parent.upgrade_router(econtext)\n        _fork_parent = econtext.router.fork()\n\n    global good_temp_dir\n    good_temp_dir = find_good_temp_dir(candidate_temp_dirs)\n\n    return {\n        u'fork_context': _fork_parent,\n        u'home_dir': mitogen.core.to_text(os.path.expanduser('~')),\n        u'good_temp_dir': good_temp_dir,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nspawning a new isolated child context.", "response": "def spawn_isolated_child(econtext):\n    \"\"\"\n    For helper functions executed in the fork parent context, arrange for\n    the context's router to be upgraded as necessary and for a new child to be\n    prepared.\n\n    The actual fork occurs from the 'virginal fork parent', which does not have\n    any Ansible modules loaded prior to fork, to avoid conflicts resulting from\n    custom module_utils paths.\n    \"\"\"\n    mitogen.parent.upgrade_router(econtext)\n    if FORK_SUPPORTED:\n        context = econtext.router.fork()\n    else:\n        context = econtext.router.local()\n    LOG.debug('create_fork_child() -> %r', context)\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_module(kwargs):\n    runner_name = kwargs.pop('runner_name')\n    klass = getattr(ansible_mitogen.runner, runner_name)\n    impl = klass(**mitogen.core.Kwargs(kwargs))\n    return impl.run()", "response": "Run an Ansible\n    module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a module asynchronously.", "response": "def run_module_async(kwargs, job_id, timeout_secs, started_sender, econtext):\n    \"\"\"\n    Execute a module with its run status and result written to a file,\n    terminating on the process on completion. This function must run in a child\n    forked using :func:`create_fork_child`.\n\n    @param mitogen.core.Sender started_sender:\n        A sender that will receive :data:`True` once the job has reached a\n        point where its initial job file has been written. This is required to\n        avoid a race where an overly eager controller can check for a task\n        before it has reached that point in execution, which is possible at\n        least on Python 2.4, where forking is not available for async tasks.\n    \"\"\"\n    arunner = AsyncRunner(\n        job_id,\n        timeout_secs,\n        started_sender,\n        econtext,\n        kwargs\n    )\n    arunner.run()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the user s shell", "response": "def get_user_shell():\n    \"\"\"\n    For commands executed directly via an SSH command-line, SSH looks up the\n    user's shell via getpwuid() and only defaults to /bin/sh if that field is\n    missing or empty.\n    \"\"\"\n    try:\n        pw_shell = pwd.getpwuid(os.geteuid()).pw_shell\n    except KeyError:\n        pw_shell = None\n\n    return pw_shell or '/bin/sh'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a command in a subprocess emulating the argument handling behaviour of a TTY.", "response": "def exec_args(args, in_data='', chdir=None, shell=None, emulate_tty=False):\n    \"\"\"\n    Run a command in a subprocess, emulating the argument handling behaviour of\n    SSH.\n\n    :param list[str]:\n        Argument vector.\n    :param bytes in_data:\n        Optional standard input for the command.\n    :param bool emulate_tty:\n        If :data:`True`, arrange for stdout and stderr to be merged into the\n        stdout pipe and for LF to be translated into CRLF, emulating the\n        behaviour of a TTY.\n    :return:\n        (return code, stdout bytes, stderr bytes)\n    \"\"\"\n    LOG.debug('exec_args(%r, ..., chdir=%r)', args, chdir)\n    assert isinstance(args, list)\n\n    if emulate_tty:\n        stderr = subprocess.STDOUT\n    else:\n        stderr = subprocess.PIPE\n\n    proc = subprocess.Popen(\n        args=args,\n        stdout=subprocess.PIPE,\n        stderr=stderr,\n        stdin=subprocess.PIPE,\n        cwd=chdir,\n    )\n    stdout, stderr = proc.communicate(in_data)\n\n    if emulate_tty:\n        stdout = stdout.replace(b('\\n'), b('\\r\\n'))\n    return proc.returncode, stdout, stderr or b('')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning a command in a subprocess emulating the argument handling behaviour of the SSH process.", "response": "def exec_command(cmd, in_data='', chdir=None, shell=None, emulate_tty=False):\n    \"\"\"\n    Run a command in a subprocess, emulating the argument handling behaviour of\n    SSH.\n\n    :param bytes cmd:\n        String command line, passed to user's shell.\n    :param bytes in_data:\n        Optional standard input for the command.\n    :return:\n        (return code, stdout bytes, stderr bytes)\n    \"\"\"\n    assert isinstance(cmd, mitogen.core.UnicodeType)\n    return exec_args(\n        args=[get_user_shell(), '-c', cmd],\n        in_data=in_data,\n        chdir=chdir,\n        shell=shell,\n        emulate_tty=emulate_tty,\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef write_path(path, s, owner=None, group=None, mode=None,\n               utimes=None, sync=False):\n    \"\"\"\n    Writes bytes `s` to a filesystem `path`.\n    \"\"\"\n    path = os.path.abspath(path)\n    fd, tmp_path = tempfile.mkstemp(suffix='.tmp',\n                                    prefix='.ansible_mitogen_transfer-',\n                                    dir=os.path.dirname(path))\n    fp = os.fdopen(fd, 'wb', mitogen.core.CHUNK_SIZE)\n    LOG.debug('write_path(path=%r) temporary file: %s', path, tmp_path)\n\n    try:\n        try:\n            if mode:\n                set_file_mode(tmp_path, mode, fd=fp.fileno())\n            if owner or group:\n                set_file_owner(tmp_path, owner, group, fd=fp.fileno())\n            fp.write(s)\n        finally:\n            fp.close()\n\n        if sync:\n            os.fsync(fp.fileno())\n        os.rename(tmp_path, path)\n    except BaseException:\n        os.unlink(tmp_path)\n        raise\n\n    if utimes:\n        os.utime(path, utimes)", "response": "Writes bytes to a filesystem path."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngives a symbolic file mode change specification in the style of chmod ( 1 ) mode apply changes in the specification to the numeric file mode mode.", "response": "def apply_mode_spec(spec, mode):\n    \"\"\"\n    Given a symbolic file mode change specification in the style of chmod(1)\n    `spec`, apply changes in the specification to the numeric file mode `mode`.\n    \"\"\"\n    for clause in mitogen.core.to_text(spec).split(','):\n        match = CHMOD_CLAUSE_PAT.match(clause)\n        who, op, perms = match.groups()\n        for ch in who or 'a':\n            mask = CHMOD_MASKS[ch]\n            bits = CHMOD_BITS[ch]\n            cur_perm_bits = mode & mask\n            new_perm_bits = reduce(operator.or_, (bits[p] for p in perms), 0)\n            mode &= ~mask\n            if op == '=':\n                mode |= new_perm_bits\n            elif op == '+':\n                mode |= new_perm_bits | cur_perm_bits\n            else:\n                mode |= cur_perm_bits & ~new_perm_bits\n    return mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the permissions of a file.", "response": "def set_file_mode(path, spec, fd=None):\n    \"\"\"\n    Update the permissions of a file using the same syntax as chmod(1).\n    \"\"\"\n    if isinstance(spec, int):\n        new_mode = spec\n    elif not mitogen.core.PY3 and isinstance(spec, long):\n        new_mode = spec\n    elif spec.isdigit():\n        new_mode = int(spec, 8)\n    else:\n        mode = os.stat(path).st_mode\n        new_mode = apply_mode_spec(spec, mode)\n\n    if fd is not None and hasattr(os, 'fchmod'):\n        os.fchmod(fd, new_mode)\n    else:\n        os.chmod(path, new_mode)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating an async job status file.", "response": "def _update(self, dct):\n        \"\"\"\n        Update an async job status file.\n        \"\"\"\n        LOG.info('%r._update(%r, %r)', self, self.job_id, dct)\n        dct.setdefault('ansible_job_id', self.job_id)\n        dct.setdefault('data', '')\n\n        fp = open(self.path + '.tmp', 'w')\n        try:\n            fp.write(json.dumps(dct))\n        finally:\n            fp.close()\n        os.rename(self.path + '.tmp', self.path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when the process is about to exit.", "response": "def _on_sigalrm(self, signum, frame):\n        \"\"\"\n        Respond to SIGALRM (job timeout) by updating the job file and killing\n        the process.\n        \"\"\"\n        msg = \"Job reached maximum time limit of %d seconds.\" % (\n            self.timeout_secs,\n        )\n        self._update({\n            \"failed\": 1,\n            \"finished\": 1,\n            \"msg\": msg,\n        })\n        self._timed_out = True\n        self.econtext.broker.shutdown()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _run(self):\n        self._update({\n            'started': 1,\n            'finished': 0,\n            'pid': os.getpid()\n        })\n        self.started_sender.send(True)\n\n        if self.timeout_secs > 0:\n            self._install_alarm()\n\n        dct = self._run_module()\n        if not self._timed_out:\n            # After SIGALRM fires, there is a window between broker responding\n            # to shutdown() by killing the process, and work continuing on the\n            # main thread. If main thread was asleep in at least\n            # basic.py/select.select(), an EINTR will be raised. We want to\n            # discard that exception.\n            try:\n                self._parse_result(dct)\n            except Exception:\n                self._update({\n                    \"failed\": 1,\n                    \"msg\": traceback.format_exc(),\n                    \"data\": dct['stdout'],  # temporary notice only\n                    \"stderr\": dct['stderr']\n                })", "response": "1. Immediately updates the status file to mark the job as started.\n        2. Installs a timer/signal handler to implement the time limit.\n        3. Runs as with run_module(), writing the result to the status file.\n\n        :param dict kwargs:\n            Runner keyword arguments.\n        :param str job_id:\n            String job ID.\n        :param int timeout_secs:\n            If >0, limit the task's maximum run time."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _mask_to_bytes(self, mask):\n        chunks = []\n        shiftmask = (2 ** 64) - 1\n        for x in range(16):\n            chunks.append(struct.pack('<Q', mask & shiftmask))\n            mask >>= 64\n        return mitogen.core.b('').join(chunks)", "response": "Convert the mask to a byte string."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides run to notify Connection of action - specific data.", "response": "def run(self, tmp=None, task_vars=None):\n        \"\"\"\n        Override run() to notify Connection of task-specific data, so it has a\n        chance to know e.g. the Python interpreter in use.\n        \"\"\"\n        self._connection.on_action_run(\n            task_vars=task_vars,\n            delegate_to_hostname=self._task.delegate_to,\n            loader_basedir=self._loader.get_basedir(),\n        )\n        return super(ActionModuleMixin, self).run(tmp, task_vars)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nexecutes a function and decorate its return value in the style of _low_level_execute_command.", "response": "def fake_shell(self, func, stdout=False):\n        \"\"\"\n        Execute a function and decorate its return value in the style of\n        _low_level_execute_command(). This produces a return value that looks\n        like some shell command was run, when really func() was implemented\n        entirely in Python.\n\n        If the function raises :py:class:`mitogen.core.CallError`, this will be\n        translated into a failed shell command with a non-zero exit status.\n\n        :param func:\n            Function invoked as `func()`.\n        :returns:\n            See :py:attr:`COMMAND_RESULT`.\n        \"\"\"\n        dct = self.COMMAND_RESULT.copy()\n        try:\n            rc = func()\n            if stdout:\n                dct['stdout'] = repr(rc)\n        except mitogen.core.CallError:\n            LOG.exception('While emulating a shell command')\n            dct['rc'] = 1\n            dct['stderr'] = traceback.format_exc()\n\n        return dct"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines if a file exists by directly invoking os. path. exists in the user account.", "response": "def _remote_file_exists(self, path):\n        \"\"\"\n        Determine if `path` exists by directly invoking os.path.exists() in the\n        target user account.\n        \"\"\"\n        LOG.debug('_remote_file_exists(%r)', path)\n        return self._connection.get_chain().call(\n            ansible_mitogen.target.file_exists,\n            mitogen.utils.cast(path)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a temporary directory and return its path.", "response": "def _make_tmp_path(self, remote_user=None):\n        \"\"\"\n        Create a temporary subdirectory as a child of the temporary directory\n        managed by the remote interpreter.\n        \"\"\"\n        LOG.debug('_make_tmp_path(remote_user=%r)', remote_user)\n        path = self._generate_tmp_path()\n        LOG.debug('Temporary directory: %r', path)\n        self._connection.get_chain().call_no_reply(os.mkdir, path)\n        self._connection._shell.tmpdir = path\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves the temporary path from the tree.", "response": "def _remove_tmp_path(self, tmp_path):\n        \"\"\"\n        Replace the base implementation's invocation of rm -rf, replacing it\n        with a pipelined call to :func:`ansible_mitogen.target.prune_tree`.\n        \"\"\"\n        LOG.debug('_remove_tmp_path(%r)', tmp_path)\n        if tmp_path is None and ansible.__version__ > '2.6':\n            tmp_path = self._connection._shell.tmpdir  # 06f73ad578d\n        if tmp_path is not None:\n            self._connection.get_chain().call_no_reply(\n                ansible_mitogen.target.prune_tree,\n                tmp_path,\n            )\n        self._connection._shell.tmpdir = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntransferring data to the remote path.", "response": "def _transfer_data(self, remote_path, data):\n        \"\"\"\n        Used by the base _execute_module(), and in <2.4 also by the template\n        action module, and probably others.\n        \"\"\"\n        if isinstance(data, dict):\n            data = jsonify(data)\n        if not isinstance(data, bytes):\n            data = to_bytes(data, errors='surrogate_or_strict')\n\n        LOG.debug('_transfer_data(%r, %s ..%d bytes)',\n                  remote_path, type(data), len(data))\n        self._connection.put_data(remote_path, data)\n        return remote_path"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _remote_chmod(self, paths, mode, sudoable=False):\n        LOG.debug('_remote_chmod(%r, mode=%r, sudoable=%r)',\n                  paths, mode, sudoable)\n        return self.fake_shell(lambda: mitogen.select.Select.all(\n            self._connection.get_chain().call_async(\n                ansible_mitogen.target.set_file_mode, path, mode\n            )\n            for path in paths\n        ))", "response": "Set the file mode for the given paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _remote_chown(self, paths, user, sudoable=False):\n        LOG.debug('_remote_chown(%r, user=%r, sudoable=%r)',\n                  paths, user, sudoable)\n        ent = self._connection.get_chain().call(pwd.getpwnam, user)\n        return self.fake_shell(lambda: mitogen.select.Select.all(\n            self._connection.get_chain().call_async(\n                os.chown, path, ent.pw_uid, ent.pw_gid\n            )\n            for path in paths\n        ))", "response": "Issue an asynchronous os. chown call for every path in paths then\n        format the resulting list with fake_shell()."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _remote_expand_user(self, path, sudoable=True):\n        LOG.debug('_remote_expand_user(%r, sudoable=%r)', path, sudoable)\n        if not path.startswith('~'):\n            # /home/foo -> /home/foo\n            return path\n        if sudoable or not self._play_context.become:\n            if path == '~':\n                # ~ -> /home/dmw\n                return self._connection.homedir\n            if path.startswith('~/'):\n                # ~/.ansible -> /home/dmw/.ansible\n                return os.path.join(self._connection.homedir, path[2:])\n        # ~root/.ansible -> /root/.ansible\n        return self._connection.get_chain(use_login=(not sudoable)).call(\n            os.path.expanduser,\n            mitogen.utils.cast(path),\n        )", "response": "Return the path to the user s entry in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexecutes a module in the target context.", "response": "def _execute_module(self, module_name=None, module_args=None, tmp=None,\n                        task_vars=None, persist_files=False,\n                        delete_remote_tmp=True, wrap_async=False):\n        \"\"\"\n        Collect up a module's execution environment then use it to invoke\n        target.run_module() or helpers.run_module_async() in the target\n        context.\n        \"\"\"\n        if module_name is None:\n            module_name = self._task.action\n        if module_args is None:\n            module_args = self._task.args\n        if task_vars is None:\n            task_vars = {}\n\n        self._update_module_args(module_name, module_args, task_vars)\n        env = {}\n        self._compute_environment_string(env)\n        self._temp_file_gibberish(module_args, wrap_async)\n\n        self._connection._connect()\n        result = ansible_mitogen.planner.invoke(\n            ansible_mitogen.planner.Invocation(\n                action=self,\n                connection=self._connection,\n                module_name=mitogen.core.to_text(module_name),\n                module_args=mitogen.utils.cast(module_args),\n                task_vars=task_vars,\n                templar=self._templar,\n                env=mitogen.utils.cast(env),\n                wrap_async=wrap_async,\n                timeout_secs=self.get_task_timeout_secs(),\n            )\n        )\n\n        if ansible.__version__ < '2.5' and delete_remote_tmp and \\\n                getattr(self._connection._shell, 'tmpdir', None) is not None:\n            # Built-in actions expected tmpdir to be cleaned up automatically\n            # on _execute_module().\n            self._remove_tmp_path(self._connection._shell.tmpdir)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _postprocess_response(self, result):\n        data = self._parse_returned_data(result)\n\n        # Cutpasted from the base implementation.\n        if 'stdout' in data and 'stdout_lines' not in data:\n            data['stdout_lines'] = (data['stdout'] or u'').splitlines()\n        if 'stderr' in data and 'stderr_lines' not in data:\n            data['stderr_lines'] = (data['stderr'] or u'').splitlines()\n\n        return data", "response": "Apply fixups mimicking ActionBase. _execute_module to convert the response to a dictionary of data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _low_level_execute_command(self, cmd, sudoable=True, in_data=None,\n                                   executable=None,\n                                   encoding_errors='surrogate_then_replace',\n                                   chdir=None):\n        \"\"\"\n        Override the base implementation by simply calling\n        target.exec_command() in the target context.\n        \"\"\"\n        LOG.debug('_low_level_execute_command(%r, in_data=%r, exe=%r, dir=%r)',\n                  cmd, type(in_data), executable, chdir)\n        if executable is None:  # executable defaults to False\n            executable = self._play_context.executable\n        if executable:\n            cmd = executable + ' -c ' + shlex_quote(cmd)\n\n        rc, stdout, stderr = self._connection.exec_command(\n            cmd=cmd,\n            in_data=in_data,\n            sudoable=sudoable,\n            mitogen_chdir=chdir,\n        )\n        stdout_text = to_text(stdout, errors=encoding_errors)\n\n        return {\n            'rc': rc,\n            'stdout': stdout_text,\n            'stdout_lines': stdout_text.splitlines(),\n            'stderr': stderr,\n        }", "response": "Execute a command in the target context."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef disable_site_packages():\n    for entry in sys.path[:]:\n        if 'site-packages' in entry or 'Extras' in entry:\n            sys.path.remove(entry)", "response": "Disable site - packages from sys. path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef log_to_file(path=None, io=False, level='INFO'):\n    log = logging.getLogger('')\n    if path:\n        fp = open(path, 'w', 1)\n        mitogen.core.set_cloexec(fp.fileno())\n    else:\n        fp = sys.stderr\n\n    level = os.environ.get('MITOGEN_LOG_LEVEL', level).upper()\n    io = level == 'IO'\n    if io:\n        level = 'DEBUG'\n        logging.getLogger('mitogen.io').setLevel(level)\n\n    level = getattr(logging, level, logging.INFO)\n    log.setLevel(level)\n\n    # Prevent accidental duplicate log_to_file() calls from generating\n    # duplicate output.\n    for handler_ in reversed(log.handlers):\n        if getattr(handler_, 'is_mitogen', None):\n            log.handlers.remove(handler_)\n\n    handler = logging.StreamHandler(fp)\n    handler.is_mitogen = True\n    handler.formatter = log_get_formatter()\n    log.handlers.insert(0, handler)", "response": "This function writes logs to the specified path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_with_router(func, *args, **kwargs):\n    broker = mitogen.master.Broker()\n    router = mitogen.master.Router(broker)\n    try:\n        return func(router, *args, **kwargs)\n    finally:\n        broker.shutdown()\n        broker.join()", "response": "Arrange for func to run with a temporary Router and Broker."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef with_router(func):\n    def wrapper(*args, **kwargs):\n        return run_with_router(func, *args, **kwargs)\n    if mitogen.core.PY3:\n        wrapper.func_name = func.__name__\n    else:\n        wrapper.func_name = func.func_name\n    return wrapper", "response": "Decorator version of a function that returns a new object with the router."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef cast(obj):\n    if isinstance(obj, dict):\n        return dict((cast(k), cast(v)) for k, v in iteritems(obj))\n    if isinstance(obj, (list, tuple)):\n        return [cast(v) for v in obj]\n    if isinstance(obj, PASSTHROUGH):\n        return obj\n    if isinstance(obj, mitogen.core.UnicodeType):\n        return mitogen.core.UnicodeType(obj)\n    if isinstance(obj, mitogen.core.BytesType):\n        return mitogen.core.BytesType(obj)\n\n    raise TypeError(\"Cannot serialize: %r: %r\" % (type(obj), obj))", "response": "Casts the given object to the most common Python types."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves comments and docstrings from Python source.", "response": "def minimize_source(source):\n    \"\"\"Remove comments and docstrings from Python `source`, preserving line\n    numbers and syntax of empty blocks.\n\n    :param str source:\n        The source to minimize.\n\n    :returns str:\n        The minimized source.\n    \"\"\"\n    source = mitogen.core.to_text(source)\n    tokens = tokenize.generate_tokens(StringIO(source).readline)\n    tokens = strip_comments(tokens)\n    tokens = strip_docstrings(tokens)\n    tokens = reindent(tokens)\n    return tokenize.untokenize(tokens)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef strip_comments(tokens):\n    prev_typ = None\n    prev_end_col = 0\n    for typ, tok, (start_row, start_col), (end_row, end_col), line in tokens:\n        if typ in (tokenize.NL, tokenize.NEWLINE):\n            if prev_typ in (tokenize.NL, tokenize.NEWLINE):\n                start_col = 0\n            else:\n                start_col = prev_end_col\n            end_col = start_col + 1\n        elif typ == tokenize.COMMENT and start_row > 2:\n            continue\n        prev_typ = typ\n        prev_end_col = end_col\n        yield typ, tok, (start_row, start_col), (end_row, end_col), line", "response": "Drop comment tokens from a tokenize stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef strip_docstrings(tokens):\n    stack = []\n    state = 'wait_string'\n    for t in tokens:\n        typ = t[0]\n        if state == 'wait_string':\n            if typ in (tokenize.NL, tokenize.COMMENT):\n                yield t\n            elif typ in (tokenize.DEDENT, tokenize.INDENT, tokenize.STRING):\n                stack.append(t)\n            elif typ == tokenize.NEWLINE:\n                stack.append(t)\n                start_line, end_line = stack[0][2][0], stack[-1][3][0]+1\n                for i in range(start_line, end_line):\n                    yield tokenize.NL, '\\n', (i, 0), (i,1), '\\n'\n                for t in stack:\n                    if t[0] in (tokenize.DEDENT, tokenize.INDENT):\n                        yield t[0], t[1], (i+1, t[2][1]), (i+1, t[3][1]), t[4]\n                del stack[:]\n            else:\n                stack.append(t)\n                for t in stack: yield t\n                del stack[:]\n                state = 'wait_newline'\n        elif state == 'wait_newline':\n            if typ == tokenize.NEWLINE:\n                state = 'wait_string'\n            yield t", "response": "Yields a sequence of tokens that are not part of a docstring."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreplacing existing indentation in a token steam with indent.", "response": "def reindent(tokens, indent=' '):\n    \"\"\"Replace existing indentation in a token steam, with `indent`.\n    \"\"\"\n    old_levels = []\n    old_level = 0\n    new_level = 0\n    for typ, tok, (start_row, start_col), (end_row, end_col), line in tokens:\n        if typ == tokenize.INDENT:\n            old_levels.append(old_level)\n            old_level = len(tok)\n            new_level += 1\n            tok = indent * new_level\n        elif typ == tokenize.DEDENT:\n            old_level = old_levels.pop()\n            new_level -= 1\n        start_col = max(0, start_col - old_level + new_level)\n        if start_row == end_row:\n            end_col = start_col + len(tok)\n        yield typ, tok, (start_row, start_col), (end_row, end_col), line"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_file_contents(path):\n    with open(path, 'rb') as fp:\n        # mitogen.core.Blob() is a bytes subclass with a repr() that returns a\n        # summary of the blob, rather than the raw blob data. This makes\n        # logging output *much* nicer. Unlike most custom types, blobs can be\n        # serialized.\n        return mitogen.core.Blob(fp.read())", "response": "Get the contents of a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a file from the FileService hosted by context.", "response": "def streamy_download_file(context, path):\n    \"\"\"\n    Fetch a file from the FileService hosted by `context`.\n    \"\"\"\n    bio = io.BytesIO()\n\n    # FileService.get() is not actually an exposed service method, it's just a\n    # classmethod that wraps up the complicated dance of implementing the\n    # transfer.\n    ok, metadata = mitogen.service.FileService.get(context, path, bio)\n\n    return {\n        'success': ok,\n        'metadata': metadata,\n        'size': len(bio.getvalue()),\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_password_hash(username):\n    try:\n        h = spwd.getspnam(username)\n    except KeyError:\n        return None\n\n    # mitogen.core.Secret() is a Unicode subclass with a repr() that hides the\n    # secret data. This keeps secret stuff out of logs. Like blobs, secrets can\n    # also be serialized.\n    return mitogen.core.Secret(h)", "response": "Fetch a user s password hash."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndo stuff to a remote context.", "response": "def work_on_machine(context):\n    \"\"\"\n    Do stuff to a remote context.\n    \"\"\"\n    print(\"Created context. Context ID is\", context.context_id)\n\n    # You don't need to understand any/all of this, but it's helpful to grok\n    # the whole chain:\n\n    # - Context.call() is a light wrapper around .call_async(), the wrapper\n    #   simply blocks the caller until a reply arrives.\n    # - .call_async() serializes the call signature into a message and passes\n    #   it to .send_async()\n    # - .send_async() creates a mitogen.core.Receiver() on the local router.\n    #   The receiver constructor uses Router.add_handle() to allocate a\n    #   'reply_to' handle and install a callback function that wakes the\n    #   receiver when a reply message arrives.\n    # - .send_async() puts the reply handle in Message.reply_to field and\n    #   passes it to .send()\n    # - Context.send() stamps the destination context ID into the\n    #   Message.dst_id field and passes it to Router.route()\n    # - Router.route() uses Broker.defer() to schedule _async_route(msg)\n    #   on the Broker thread.\n    # [broker thread]\n    # - The broker thread wakes and calls _async_route(msg)\n    # - Router._async_route() notices 'dst_id' is for a remote context and\n    #   looks up the stream on which messages for dst_id should be sent (may be\n    #   direct connection or not), and calls Stream.send()\n    # - Stream.send() packs the message into a bytestring, appends it to\n    #   Stream._output_buf, and calls Broker.start_transmit()\n    # - Broker finishes work, reenters IO loop. IO loop wakes due to writeable\n    #   stream.\n    # - Stream.on_transmit() writes the full/partial buffer to SSH, calls\n    #   stop_transmit() to mark the stream unwriteable once _output_buf is\n    #   empty.\n    # - Broker IO loop sleeps, no readers/writers.\n    # - Broker wakes due to SSH stream readable.\n    # - Stream.on_receive() called, reads the reply message, converts it to a\n    #   Message and passes it to Router._async_route().\n    # - Router._async_route() notices message is for local context, looks up\n    #   target handle in the .add_handle() registry.\n    # - Receiver._on_receive() called, appends message to receiver queue.\n    # [main thread]\n    # - Receiver.get() used to block the original Context.call() wakes and pops\n    #   the message from the queue.\n    # - Message data (pickled return value) is deserialized and returned to the\n    #   caller.\n    print(\"It's running on the local machine. Its PID is\",\n          context.call(os.getpid))\n\n    # Now let's call a function defined in this module. On receiving the\n    # function call request, the child attempts to import __main__, which is\n    # initially missing, causing the importer in the child to request it from\n    # its parent. That causes _this script_ to be sent as the module source\n    # over the wire.\n    print(\"Calling md5sum(/etc/passwd) in the child:\",\n          context.call(md5sum, '/etc/passwd'))\n\n    # Now let's \"transfer\" a file. The simplest way to do this is calling a\n    # function that returns the file data, which is totally fine for small\n    # files.\n    print(\"Download /etc/passwd via function call: %d bytes\" % (\n        len(context.call(get_file_contents, '/etc/passwd'))\n    ))\n\n    # And using function calls, in the other direction:\n    print(\"Upload /tmp/blah via function call: %s\" % (\n        context.call(put_file_contents, '/tmp/blah', b'blah!'),\n    ))\n\n    # Now lets transfer what might be a big files. The problem with big files\n    # is that they may not fit in RAM. This uses mitogen.services.FileService\n    # to implement streamy file transfer instead. The sender must have a\n    # 'service pool' running that will host FileService. First let's do the\n    # 'upload' direction, where the master hosts FileService.\n\n    # Steals the 'Router' reference from the context object. In a real app the\n    # pool would be constructed once at startup, this is just demo code.\n    file_service = mitogen.service.FileService(context.router)\n\n    # Start the pool.\n    pool = mitogen.service.Pool(context.router, services=[file_service])\n\n    # Grant access to a file on the local disk from unprivileged contexts.\n    # .register() is also exposed as a service method -- you can call it on a\n    # child context from any more privileged context.\n    file_service.register('/etc/passwd')\n\n    # Now call our wrapper function that knows how to handle the transfer. In a\n    # real app, this wrapper might also set ownership/modes or do any other\n    # app-specific stuff relating to the file that was transferred.\n    print(\"Streamy upload /etc/passwd: remote result: %s\" % (\n        context.call(\n            streamy_download_file,\n            # To avoid hard-wiring streamy_download_file(), we want to pass it\n            # a Context object that hosts the file service it should request\n            # files from. Router.myself() returns a Context referring to this\n            # process.\n            context=router.myself(),\n            path='/etc/passwd',\n        ),\n    ))\n\n    # Shut down the pool now we're done with it, else app will hang at exit.\n    # Once again, this should only happen once at app startup/exit, not for\n    # every file transfer!\n    pool.stop(join=True)\n\n    # Now let's do the same thing but in reverse: we use FileService on the\n    # remote download a file. This uses context.call_service(), which invokes a\n    # special code path that causes auto-initialization of a thread pool in the\n    # target, and auto-construction of the target service, but only if the\n    # service call was made by a more privileged context. We could write a\n    # helper function that runs in the remote to do all that by hand, but the\n    # library handles it for us.\n\n    # Make the file accessible. A future FileService could avoid the need for\n    # this for privileged contexts.\n    context.call_service(\n        service_name=mitogen.service.FileService,\n        method_name='register',\n        path='/etc/passwd'\n    )\n\n    # Now we can use our streamy_download_file() function in reverse -- running\n    # it from this process and having it fetch from the remote process:\n    print(\"Streamy download /etc/passwd: result: %s\" % (\n        streamy_download_file(context, '/etc/passwd'),\n    ))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse the script interpreter portion of a UNIX hashbang using the rules Linux uses.", "response": "def parse_script_interpreter(source):\n    \"\"\"\n    Parse the script interpreter portion of a UNIX hashbang using the rules\n    Linux uses.\n\n    :param str source: String like \"/usr/bin/env python\".\n\n    :returns:\n        Tuple of `(interpreter, arg)`, where `intepreter` is the script\n        interpreter and `arg` is its sole argument if present, otherwise\n        :py:data:`None`.\n    \"\"\"\n    # Find terminating newline. Assume last byte of binprm_buf if absent.\n    nl = source.find(b'\\n', 0, 128)\n    if nl == -1:\n        nl = min(128, len(source))\n\n    # Split once on the first run of whitespace. If no whitespace exists,\n    # bits just contains the interpreter filename.\n    bits = source[0:nl].strip().split(None, 1)\n    if len(bits) == 1:\n        return mitogen.core.to_text(bits[0]), None\n    return mitogen.core.to_text(bits[0]), mitogen.core.to_text(bits[1])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef quantize(image, bits_per_channel=None):\n\n    '''Reduces the number of bits per channel in the given image.'''\n\n    if bits_per_channel is None:\n        bits_per_channel = 6\n\n    assert image.dtype == np.uint8\n\n    shift = 8-bits_per_channel\n    halfbin = (1 << shift) >> 1\n\n    return ((image.astype(int) >> shift) << shift) + halfbin", "response": "Reduces the number of bits per channel in the given image."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pack_rgb(rgb):\n\n    '''Packs a 24-bit RGB triples into a single integer,\nworks on both arrays and tuples.'''\n\n    orig_shape = None\n\n    if isinstance(rgb, np.ndarray):\n        assert rgb.shape[-1] == 3\n        orig_shape = rgb.shape[:-1]\n    else:\n        assert len(rgb) == 3\n        rgb = np.array(rgb)\n\n    rgb = rgb.astype(int).reshape((-1, 3))\n\n    packed = (rgb[:, 0] << 16 |\n              rgb[:, 1] << 8 |\n              rgb[:, 2])\n\n    if orig_shape is None:\n        return packed\n    else:\n        return packed.reshape(orig_shape)", "response": "Packs a 24 - bit RGB triples into a single integer\nworks on both arrays and tuples."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unpack_rgb(packed):\n\n    '''Unpacks a single integer or array of integers into one or more\n24-bit RGB values.\n\n    '''\n\n    orig_shape = None\n\n    if isinstance(packed, np.ndarray):\n        assert packed.dtype == int\n        orig_shape = packed.shape\n        packed = packed.reshape((-1, 1))\n\n    rgb = ((packed >> 16) & 0xff,\n           (packed >> 8) & 0xff,\n           (packed) & 0xff)\n\n    if orig_shape is None:\n        return rgb\n    else:\n        return np.hstack(rgb).reshape(orig_shape + (3,))", "response": "Unpacks a single integer or array of integers into one or more\n24 - bit RGB values.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_bg_color(image, bits_per_channel=None):\n\n    '''Obtains the background color from an image or array of RGB colors\nby grouping similar colors into bins and finding the most frequent\none.\n\n    '''\n\n    assert image.shape[-1] == 3\n\n    quantized = quantize(image, bits_per_channel).astype(int)\n    packed = pack_rgb(quantized)\n\n    unique, counts = np.unique(packed, return_counts=True)\n\n    packed_mode = unique[counts.argmax()]\n\n    return unpack_rgb(packed_mode)", "response": "Obtains the background color from an image or array of RGB colors\nby grouping similar colors into bins and finding the most frequent\none."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts an RGB image or array of RGB colors to saturation and value returning each one as a separate 32 - bit floating point array or value.", "response": "def rgb_to_sv(rgb):\n\n    '''Convert an RGB image or array of RGB colors to saturation and\nvalue, returning each one as a separate 32-bit floating point array or\nvalue.\n\n    '''\n\n    if not isinstance(rgb, np.ndarray):\n        rgb = np.array(rgb)\n\n    axis = len(rgb.shape)-1\n    cmax = rgb.max(axis=axis).astype(np.float32)\n    cmin = rgb.min(axis=axis).astype(np.float32)\n    delta = cmax - cmin\n\n    saturation = delta.astype(np.float32) / cmax.astype(np.float32)\n    saturation = np.where(cmax == 0, 0, saturation)\n\n    value = cmax/255.0\n\n    return saturation, value"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef postprocess(output_filename, options):\n\n    '''Runs the postprocessing command on the file provided.'''\n\n    assert options.postprocess_cmd\n\n    base, _ = os.path.splitext(output_filename)\n    post_filename = base + options.postprocess_ext\n\n    cmd = options.postprocess_cmd\n    cmd = cmd.replace('%i', output_filename)\n    cmd = cmd.replace('%o', post_filename)\n    cmd = cmd.replace('%e', options.postprocess_ext)\n\n    subprocess_args = shlex.split(cmd)\n\n    if os.path.exists(post_filename):\n        os.unlink(post_filename)\n\n    if not options.quiet:\n        print('  running \"{}\"...'.format(cmd), end=' ')\n        sys.stdout.flush()\n\n    try:\n        result = subprocess.call(subprocess_args)\n        before = os.stat(output_filename).st_size\n        after = os.stat(post_filename).st_size\n    except OSError:\n        result = -1\n\n    if result == 0:\n\n        if not options.quiet:\n            print('{:.1f}% reduction'.format(\n                100*(1.0-float(after)/before)))\n\n        return post_filename\n\n    else:\n\n        sys.stderr.write('warning: postprocessing failed!\\n')\n        return None", "response": "Runs the postprocessing command on the file provided."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_argument_parser():\n\n    '''Parse the command-line arguments for this program.'''\n\n    parser = ArgumentParser(\n        description='convert scanned, hand-written notes to PDF')\n\n    show_default = ' (default %(default)s)'\n\n    parser.add_argument('filenames', metavar='IMAGE', nargs='+',\n                        help='files to convert')\n\n    parser.add_argument('-q', dest='quiet', action='store_true',\n                        default=False,\n                        help='reduce program output')\n\n    parser.add_argument('-b', dest='basename', metavar='BASENAME',\n                        default='page',\n                        help='output PNG filename base' + show_default)\n\n    parser.add_argument('-o', dest='pdfname', metavar='PDF',\n                        default='output.pdf',\n                        help='output PDF filename' + show_default)\n\n    parser.add_argument('-v', dest='value_threshold', metavar='PERCENT',\n                        type=percent, default='25',\n                        help='background value threshold %%'+show_default)\n\n    parser.add_argument('-s', dest='sat_threshold', metavar='PERCENT',\n                        type=percent, default='20',\n                        help='background saturation '\n                        'threshold %%'+show_default)\n\n    parser.add_argument('-n', dest='num_colors', type=int,\n                        default='8',\n                        help='number of output colors '+show_default)\n\n    parser.add_argument('-p', dest='sample_fraction',\n                        metavar='PERCENT',\n                        type=percent, default='5',\n                        help='%% of pixels to sample' + show_default)\n\n    parser.add_argument('-w', dest='white_bg', action='store_true',\n                        default=False, help='make background white')\n\n    parser.add_argument('-g', dest='global_palette',\n                        action='store_true', default=False,\n                        help='use one global palette for all pages')\n\n    parser.add_argument('-S', dest='saturate', action='store_false',\n                        default=True, help='do not saturate colors')\n\n    parser.add_argument('-K', dest='sort_numerically',\n                        action='store_false', default=True,\n                        help='keep filenames ordered as specified; '\n                        'use if you *really* want IMG_10.png to '\n                        'precede IMG_2.png')\n\n    parser.add_argument('-P', dest='postprocess_cmd', default=None,\n                        help='set postprocessing command (see -O, -C, -Q)')\n\n    parser.add_argument('-e', dest='postprocess_ext',\n                        default='_post.png',\n                        help='filename suffix/extension for '\n                        'postprocessing command')\n\n    parser.add_argument('-O', dest='postprocess_cmd',\n                        action='store_const',\n                        const='optipng -silent %i -out %o',\n                        help='same as -P \"%(const)s\"')\n\n    parser.add_argument('-C', dest='postprocess_cmd',\n                        action='store_const',\n                        const='pngcrush -q %i %o',\n                        help='same as -P \"%(const)s\"')\n\n    parser.add_argument('-Q', dest='postprocess_cmd',\n                        action='store_const',\n                        const='pngquant --ext %e %i',\n                        help='same as -P \"%(const)s\"')\n\n    parser.add_argument('-c', dest='pdf_cmd', metavar=\"COMMAND\",\n                        default='convert %i %o',\n                        help='PDF command (default \"%(default)s\")')\n\n    return parser", "response": "Parse the command - line arguments for this program."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the filenames from the command line optionally sorted by number.", "response": "def get_filenames(options):\n\n    '''Get the filenames from the command line, optionally sorted by\nnumber, so that IMG_10.png is re-arranged to come after IMG_9.png.\nThis is a nice feature because some scanner programs (like Image\nCapture on Mac OS X) automatically number files without leading zeros,\nand this way you can supply files using a wildcard and still have the\npages ordered correctly.\n\n    '''\n\n    if not options.sort_numerically:\n        return options.filenames\n\n    filenames = []\n\n    for filename in options.filenames:\n        basename = os.path.basename(filename)\n        root, _ = os.path.splitext(basename)\n        matches = re.findall(r'[0-9]+', root)\n        if matches:\n            num = int(matches[-1])\n        else:\n            num = -1\n        filenames.append((num, filename))\n\n    return [fn for (_, fn) in sorted(filenames)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load(input_filename):\n\n    '''Load an image with Pillow and convert it to numpy array. Also\nreturns the image DPI in x and y as a tuple.'''\n\n    try:\n        pil_img = Image.open(input_filename)\n    except IOError:\n        sys.stderr.write('warning: error opening {}\\n'.format(\n            input_filename))\n        return None, None\n\n    if pil_img.mode != 'RGB':\n        pil_img = pil_img.convert('RGB')\n\n    if 'dpi' in pil_img.info:\n        dpi = pil_img.info['dpi']\n    else:\n        dpi = (300, 300)\n\n    img = np.array(pil_img)\n\n    return img, dpi", "response": "Load an image with Pillow and convert it to numpy array. Also\nreturns the image DPI in x and y as a tuple."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npick a fixed percentage of pixels in the image returned in random order.", "response": "def sample_pixels(img, options):\n\n    '''Pick a fixed percentage of pixels in the image, returned in random\norder.'''\n\n    pixels = img.reshape((-1, 3))\n    num_pixels = pixels.shape[0]\n    num_samples = int(num_pixels*options.sample_fraction)\n\n    idx = np.arange(num_pixels)\n    np.random.shuffle(idx)\n\n    return pixels[idx[:num_samples]]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_fg_mask(bg_color, samples, options):\n\n    '''Determine whether each pixel in a set of samples is foreground by\ncomparing it to the background color. A pixel is classified as a\nforeground pixel if either its value or saturation differs from the\nbackground by a threshold.'''\n\n    s_bg, v_bg = rgb_to_sv(bg_color)\n    s_samples, v_samples = rgb_to_sv(samples)\n\n    s_diff = np.abs(s_bg - s_samples)\n    v_diff = np.abs(v_bg - v_samples)\n\n    return ((v_diff >= options.value_threshold) |\n            (s_diff >= options.sat_threshold))", "response": "Determine whether each pixel in a set of samples is foreground by\ncomparing it to the background color. A pixel is classified as a\nforeground pixel by either its value or saturation differs from the\nbackground by a threshold."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_palette(samples, options, return_mask=False, kmeans_iter=40):\n\n    '''Extract the palette for the set of sampled RGB values. The first\npalette entry is always the background color; the rest are determined\nfrom foreground pixels by running K-means clustering. Returns the\npalette, as well as a mask corresponding to the foreground pixels.\n\n    '''\n\n    if not options.quiet:\n        print('  getting palette...')\n\n    bg_color = get_bg_color(samples, 6)\n\n    fg_mask = get_fg_mask(bg_color, samples, options)\n\n    centers, _ = kmeans(samples[fg_mask].astype(np.float32),\n                        options.num_colors-1,\n                        iter=kmeans_iter)\n\n    palette = np.vstack((bg_color, centers)).astype(np.uint8)\n\n    if not return_mask:\n        return palette\n    else:\n        return palette, fg_mask", "response": "Extract the palette for the set of sampled RGB values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_palette(img, palette, options):\n\n    '''Apply the pallete to the given image. The first step is to set all\nbackground pixels to the background color; then, nearest-neighbor\nmatching is used to map each foreground color to the closest one in\nthe palette.\n\n    '''\n\n    if not options.quiet:\n        print('  applying palette...')\n\n    bg_color = palette[0]\n\n    fg_mask = get_fg_mask(bg_color, img, options)\n\n    orig_shape = img.shape\n\n    pixels = img.reshape((-1, 3))\n    fg_mask = fg_mask.flatten()\n\n    num_pixels = pixels.shape[0]\n\n    labels = np.zeros(num_pixels, dtype=np.uint8)\n\n    labels[fg_mask], _ = vq(pixels[fg_mask], palette)\n\n    return labels.reshape(orig_shape[:-1])", "response": "Apply the pallete to the given image."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsaves the label and palette pair out as an indexed PNG image.", "response": "def save(output_filename, labels, palette, dpi, options):\n\n    '''Save the label/palette pair out as an indexed PNG image.  This\noptionally saturates the pallete by mapping the smallest color\ncomponent to zero and the largest one to 255, and also optionally sets\nthe background color to pure white.\n\n    '''\n\n    if not options.quiet:\n        print('  saving {}...'.format(output_filename))\n\n    if options.saturate:\n        palette = palette.astype(np.float32)\n        pmin = palette.min()\n        pmax = palette.max()\n        palette = 255 * (palette - pmin)/(pmax-pmin)\n        palette = palette.astype(np.uint8)\n\n    if options.white_bg:\n        palette = palette.copy()\n        palette[0] = (255, 255, 255)\n\n    output_img = Image.fromarray(labels, 'P')\n    output_img.putpalette(palette.flatten())\n    output_img.save(output_filename, dpi=dpi)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_global_palette(filenames, options):\n\n    '''Fetch the global palette for a series of input files by merging\ntheir samples together into one large array.\n\n    '''\n\n    input_filenames = []\n\n    all_samples = []\n\n    if not options.quiet:\n        print('building global palette...')\n\n    for input_filename in filenames:\n\n        img, _ = load(input_filename)\n        if img is None:\n            continue\n\n        if not options.quiet:\n            print('  processing {}...'.format(input_filename))\n\n        samples = sample_pixels(img, options)\n        input_filenames.append(input_filename)\n        all_samples.append(samples)\n\n    num_inputs = len(input_filenames)\n\n    all_samples = [s[:int(round(float(s.shape[0])/num_inputs))]\n                   for s in all_samples]\n\n    all_samples = np.vstack(tuple(all_samples))\n\n    global_palette = get_palette(all_samples, options)\n\n    if not options.quiet:\n        print('  done\\n')\n\n    return input_filenames, global_palette", "response": "Fetch the global palette for a series of input files by merging\ntheir samples together into one large array."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef emit_pdf(outputs, options):\n\n    '''Runs the PDF conversion command to generate the PDF.'''\n\n    cmd = options.pdf_cmd\n    cmd = cmd.replace('%o', options.pdfname)\n    if len(outputs) > 2:\n        cmd_print = cmd.replace('%i', ' '.join(outputs[:2] + ['...']))\n    else:\n        cmd_print = cmd.replace('%i', ' '.join(outputs))\n    cmd = cmd.replace('%i', ' '.join(outputs))\n\n    if not options.quiet:\n        print('running PDF command \"{}\"...'.format(cmd_print))\n\n    try:\n        result = subprocess.call(shlex.split(cmd))\n    except OSError:\n        result = -1\n\n    if result == 0:\n        if not options.quiet:\n            print('  wrote', options.pdfname)\n    else:\n        sys.stderr.write('warning: PDF command failed\\n')", "response": "Runs the PDF conversion command to generate the PDF."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generic_type_name(v):\n    if isinstance(v, AstExampleRef):\n        return \"reference\"\n    elif isinstance(v, numbers.Integral):\n        # Must come before real numbers check since integrals are reals too\n        return 'integer'\n    elif isinstance(v, numbers.Real):\n        return 'float'\n    elif isinstance(v, (tuple, list)):\n        return 'list'\n    elif isinstance(v, six.string_types):\n        return 'string'\n    elif v is None:\n        return 'null'\n    else:\n        return type(v).__name__", "response": "Return a descriptive type name that isn t Python specific."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nunwraps the raw_doc into a single line of text.", "response": "def doc_unwrap(raw_doc):\n    \"\"\"\n    Applies two transformations to raw_doc:\n    1. N consecutive newlines are converted into N-1 newlines.\n    2. A lone newline is converted to a space, which basically unwraps text.\n\n    Returns a new string, or None if the input was None.\n    \"\"\"\n    if raw_doc is None:\n        return None\n    docstring = ''\n    consecutive_newlines = 0\n    # Remove all leading and trailing whitespace in the documentation block\n    for c in raw_doc.strip():\n        if c == '\\n':\n            consecutive_newlines += 1\n            if consecutive_newlines > 1:\n                docstring += c\n        else:\n            if consecutive_newlines == 1:\n                docstring += ' '\n            consecutive_newlines = 0\n            docstring += c\n    return docstring"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef unwrap_aliases(data_type):\n    unwrapped_alias = False\n    while is_alias(data_type):\n        unwrapped_alias = True\n        data_type = data_type.data_type\n    return data_type, unwrapped_alias", "response": "This method unwraps all aliases from around a DataType."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nresolving all chained nested aliases.", "response": "def resolve_aliases(data_type):\n    \"\"\"\n    Resolve all chained / nested aliases. This will recursively point\n    nested aliases to their resolved data type (first non-alias in the chain).\n\n    Note: This differs from unwrap_alias which simply identifies/returns\n    the resolved data type.\n\n    Args:\n        data_type (DataType): The target DataType/Alias to resolve.\n    Return:\n        DataType: The resolved type.\n    \"\"\"\n    if not is_alias(data_type):\n        return data_type\n\n    resolved = resolve_aliases(data_type.data_type)\n    data_type.data_type = resolved\n\n    return resolved"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef strip_alias(data_type):\n    while hasattr(data_type, 'data_type'):\n        if is_alias(data_type.data_type):\n            data_type.data_type = data_type.data_type.data_type\n            break\n        data_type = data_type.data_type", "response": "This function strips an alias from a data type chain."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef unwrap(data_type):\n    unwrapped_nullable = False\n    unwrapped_alias = False\n    while is_alias(data_type) or is_nullable_type(data_type):\n        if is_nullable_type(data_type):\n            unwrapped_nullable = True\n        if is_alias(data_type):\n            unwrapped_alias = True\n        data_type = data_type.data_type\n    return data_type, unwrapped_nullable, unwrapped_alias", "response": "Unwraps all Aliases and Nullables from around a\n    DataType."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives a Stone data type returns all custom annotations applied to it.", "response": "def get_custom_annotations_for_alias(data_type):\n    \"\"\"\n    Given a Stone data type, returns all custom annotations applied to it.\n    \"\"\"\n    # annotations can only be applied to Aliases, but they can be wrapped in\n    # Nullable. also, Aliases pointing to other Aliases don't automatically\n    # inherit their custom annotations, so we might have to traverse.\n    result = []\n    data_type, _ = unwrap_nullable(data_type)\n    while is_alias(data_type):\n        result.extend(data_type.custom_annotations)\n        data_type, _ = unwrap_nullable(data_type.data_type)\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_custom_annotations_recursive(data_type):\n    # because Stone structs can contain references to themselves (or otherwise\n    # be cyclical), we need ot keep track of the data types we've already seen\n    data_types_seen = set()\n\n    def recurse(data_type):\n        if data_type in data_types_seen:\n            return\n        data_types_seen.add(data_type)\n\n        dt, _, _ = unwrap(data_type)\n        if is_struct_type(dt) or is_union_type(dt):\n            for field in dt.fields:\n                for annotation in recurse(field.data_type):\n                    yield annotation\n                for annotation in field.custom_annotations:\n                    yield annotation\n        elif is_list_type(dt):\n            for annotation in recurse(dt.data_type):\n                yield annotation\n        elif is_map_type(dt):\n            for annotation in recurse(dt.value_data_type):\n                yield annotation\n\n        for annotation in get_custom_annotations_for_alias(data_type):\n            yield annotation\n\n    return recurse(data_type)", "response": "Given a Stone data type returns all custom annotations applied to any of these memebers and submembers."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset the attributes of the type.", "response": "def set_attributes(self, doc, fields, parent_type=None):\n        \"\"\"\n        Fields are specified as a list so that order is preserved for display\n        purposes only. (Might be used for certain serialization formats...)\n\n        :param str doc: Description of type.\n        :param list(Field) fields: Ordered list of fields for type.\n        :param Optional[Composite] parent_type: The type this type inherits\n            from.\n        \"\"\"\n        self.raw_doc = doc\n        self.doc = doc_unwrap(doc)\n        self.fields = fields\n        self.parent_type = parent_type\n        self._raw_examples = OrderedDict()\n        self._examples = OrderedDict()\n        self._fields_by_name = {}  # Dict[str, Field]\n\n        # Check that no two fields share the same name.\n        for field in self.fields:\n            if field.name in self._fields_by_name:\n                orig_lineno = self._fields_by_name[field.name]._ast_node.lineno\n                raise InvalidSpec(\"Field '%s' already defined on line %s.\" %\n                                  (field.name, orig_lineno),\n                                  field._ast_node.lineno)\n            self._fields_by_name[field.name] = field\n\n        # Check that the fields for this type do not match any of the fields of\n        # its parents.\n        cur_type = self.parent_type\n        while cur_type:\n            for field in self.fields:\n                if field.name in cur_type._fields_by_name:\n                    lineno = cur_type._fields_by_name[field.name]._ast_node.lineno\n                    raise InvalidSpec(\n                        \"Field '%s' already defined in parent '%s' on line %d.\"\n                        % (field.name, cur_type.name, lineno),\n                        field._ast_node.lineno)\n            cur_type = cur_type.parent_type\n\n        # Import namespaces containing any custom annotations\n        # Note: we don't need to do this for builtin annotations because\n        # they are treated as globals at the IR level\n        for field in self.fields:\n            for annotation in field.custom_annotations:\n                # first, check the annotation *type*\n                if annotation.annotation_type.namespace.name != self.namespace.name:\n                    self.namespace.add_imported_namespace(\n                        annotation.annotation_type.namespace,\n                        imported_annotation_type=True)\n\n                # second, check if we need to import the annotation itself\n\n                # the annotation namespace is currently not actually used in the\n                # backends, which reconstruct the annotation from the annotation\n                # type directly. This could be changed in the future, and at\n                # the IR level it makes sense to include the dependency\n                if annotation.namespace.name != self.namespace.name:\n                    self.namespace.add_imported_namespace(\n                        annotation.namespace,\n                        imported_annotation=True)\n\n        # Indicate that the attributes of the type have been populated.\n        self._is_forward_ref = False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef has_documented_type_or_fields(self, include_inherited_fields=False):\n        if self.doc:\n            return True\n        else:\n            return self.has_documented_fields(include_inherited_fields)", "response": "Returns whether this type or any of its fields are documented."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn whether at least one field is documented.", "response": "def has_documented_fields(self, include_inherited_fields=False):\n        \"\"\"Returns whether at least one field is documented.\"\"\"\n        fields = self.all_fields if include_inherited_fields else self.fields\n        for field in fields:\n            if field.doc:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an OrderedDict mapping labels to Example objects.", "response": "def get_examples(self, compact=False):\n        \"\"\"\n        Returns an OrderedDict mapping labels to Example objects.\n\n        Args:\n            compact (bool): If True, union members of void type are converted\n                to their compact representation: no \".tag\" key or containing\n                dict, just the tag as a string.\n        \"\"\"\n        # Copy it just in case the caller wants to mutate the object.\n        examples = copy.deepcopy(self._examples)\n        if not compact:\n            return examples\n\n        def make_compact(d):\n            # Traverse through dicts looking for ones that have a lone .tag\n            # key, which can be converted into the compact form.\n            if not isinstance(d, dict):\n                return\n            for key in d:\n                if isinstance(d[key], dict):\n                    inner_d = d[key]\n                    if len(inner_d) == 1 and '.tag' in inner_d:\n                        d[key] = inner_d['.tag']\n                    else:\n                        make_compact(inner_d)\n                if isinstance(d[key], list):\n                    for item in d[key]:\n                        make_compact(item)\n\n        for example in examples.values():\n            if (isinstance(example.value, dict) and\n                    len(example.value) == 1 and '.tag' in example.value):\n                # Handle the case where the top-level of the example can be\n                # made compact.\n                example.value = example.value['.tag']\n            else:\n                make_compact(example.value)\n\n        return examples"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_attributes(self, doc, fields, parent_type=None):\n\n        if parent_type:\n            assert isinstance(parent_type, Struct)\n\n        self.subtypes = []\n\n        # These are only set if this struct enumerates subtypes.\n        self._enumerated_subtypes = None  # Optional[List[Tuple[str, DataType]]]\n        self._is_catch_all = None  # Optional[Bool]\n\n        super(Struct, self).set_attributes(doc, fields, parent_type)\n\n        if self.parent_type:\n            self.parent_type.subtypes.append(self)", "response": "Set the attributes of this struct."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _filter_fields(self, filter_function):\n        fields = []\n        if self.parent_type:\n            fields.extend(self.parent_type._filter_fields(filter_function))\n        fields.extend(filter(filter_function, self.fields))\n        return fields", "response": "Utility to iterate through all fields of a type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterator that traverses all required fields in all super types first and then for this type.", "response": "def all_required_fields(self):\n        \"\"\"\n        Returns an iterator that traverses required fields in all super types\n        first, and then for this type.\n        \"\"\"\n        def required_check(f):\n            return not is_nullable_type(f.data_type) and not f.has_default\n        return self._filter_fields(required_check)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef all_optional_fields(self):\n        def optional_check(f):\n            return is_nullable_type(f.data_type) or f.has_default\n        return self._filter_fields(optional_check)", "response": "Returns an iterator that traverses all optional fields in all super types\n        first then for this type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_enumerated_subtypes(self, subtype_fields, is_catch_all):\n        assert self._enumerated_subtypes is None, \\\n            'Enumerated subtypes already set.'\n        assert isinstance(is_catch_all, bool), type(is_catch_all)\n\n        self._is_catch_all = is_catch_all\n        self._enumerated_subtypes = []\n\n        if self.parent_type:\n            raise InvalidSpec(\n                \"'%s' enumerates subtypes so it cannot extend another struct.\"\n                % self.name, self._ast_node.lineno, self._ast_node.path)\n\n        # Require that if this struct enumerates subtypes, its parent (and thus\n        # the entire hierarchy above this struct) does as well.\n        if self.parent_type and not self.parent_type.has_enumerated_subtypes():\n            raise InvalidSpec(\n                \"'%s' cannot enumerate subtypes if parent '%s' does not.\" %\n                (self.name, self.parent_type.name),\n                self._ast_node.lineno, self._ast_node.path)\n\n        enumerated_subtype_names = set()  # Set[str]\n        for subtype_field in subtype_fields:\n            path = subtype_field._ast_node.path\n            lineno = subtype_field._ast_node.lineno\n\n            # Require that a subtype only has a single type tag.\n            if subtype_field.data_type.name in enumerated_subtype_names:\n                raise InvalidSpec(\n                    \"Subtype '%s' can only be specified once.\" %\n                    subtype_field.data_type.name, lineno, path)\n\n            # Require that a subtype has this struct as its parent.\n            if subtype_field.data_type.parent_type != self:\n                raise InvalidSpec(\n                    \"'%s' is not a subtype of '%s'.\" %\n                    (subtype_field.data_type.name, self.name), lineno, path)\n\n            # Check for subtype tags that conflict with this struct's\n            # non-inherited fields.\n            if subtype_field.name in self._fields_by_name:\n                # Since the union definition comes first, use its line number\n                # as the source of the field's original declaration.\n                orig_field = self._fields_by_name[subtype_field.name]\n                raise InvalidSpec(\n                    \"Field '%s' already defined on line %d.\" %\n                    (subtype_field.name, lineno),\n                    orig_field._ast_node.lineno,\n                    orig_field._ast_node.path)\n\n            # Walk up parent tree hierarchy to ensure no field conflicts.\n            # Checks for conflicts with subtype tags and regular fields.\n            cur_type = self.parent_type\n            while cur_type:\n                if subtype_field.name in cur_type._fields_by_name:\n                    orig_field = cur_type._fields_by_name[subtype_field.name]\n                    raise InvalidSpec(\n                        \"Field '%s' already defined in parent '%s' (%s:%d).\"\n                        % (subtype_field.name, cur_type.name,\n                           orig_field._ast_node.path, orig_field._ast_node.lineno),\n                        lineno, path)\n                cur_type = cur_type.parent_type\n\n            # Note the discrepancy between `fields` which contains only the\n            # struct fields, and `_fields_by_name` which contains the struct\n            # fields and enumerated subtype fields.\n            self._fields_by_name[subtype_field.name] = subtype_field\n            enumerated_subtype_names.add(subtype_field.data_type.name)\n            self._enumerated_subtypes.append(subtype_field)\n\n        assert len(self._enumerated_subtypes) > 0\n\n        # Check that all known subtypes are listed in the enumeration.\n        for subtype in self.subtypes:\n            if subtype.name not in enumerated_subtype_names:\n                raise InvalidSpec(\n                    \"'%s' does not enumerate all subtypes, missing '%s'\" %\n                    (self.name, subtype.name),\n                    self._ast_node.lineno)", "response": "Sets the list of enumerated subtypes for this struct."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_all_subtypes_with_tags(self):\n        assert self.has_enumerated_subtypes(), 'Enumerated subtypes not set.'\n        subtypes_with_tags = []  # List[Tuple[List[String], Struct]]\n        fifo = deque([subtype_field.data_type\n                      for subtype_field in self.get_enumerated_subtypes()])\n        # Traverse down the hierarchy registering subtypes as they're found.\n        while fifo:\n            data_type = fifo.popleft()\n            subtypes_with_tags.append((data_type._get_subtype_tags(), data_type))\n            if data_type.has_enumerated_subtypes():\n                for subtype_field in data_type.get_enumerated_subtypes():\n                    fifo.append(subtype_field.data_type)\n        return subtypes_with_tags", "response": "This method returns all subtypes of this struct and all subtypes of that struct with the same tags."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_subtype_tags(self):\n        assert self.is_member_of_enumerated_subtypes_tree(), \\\n            'Not a part of a subtypes tree.'\n        cur = self.parent_type\n        cur_dt = self\n        tags = []\n        while cur:\n            assert cur.has_enumerated_subtypes()\n            for subtype_field in cur.get_enumerated_subtypes():\n                if subtype_field.data_type is cur_dt:\n                    tags.append(subtype_field.name)\n                    break\n            else:\n                assert False, 'Could not find?!'\n            cur_dt = cur\n            cur = cur.parent_type\n        tags.reverse()\n        return tuple(tags)", "response": "Returns a list of type tags that refer to this type starting from the base of the struct hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_example(self, example):\n        if self.has_enumerated_subtypes():\n            self._add_example_enumerated_subtypes_helper(example)\n        else:\n            self._add_example_helper(example)", "response": "Adds a raw example for this type."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nvalidate examples for structs with enumerated subtypes.", "response": "def _add_example_enumerated_subtypes_helper(self, example):\n        \"\"\"Validates examples for structs with enumerated subtypes.\"\"\"\n\n        if len(example.fields) != 1:\n            raise InvalidSpec(\n                'Example for struct with enumerated subtypes must only '\n                'specify one subtype tag.', example.lineno, example.path)\n\n        # Extract the only tag in the example.\n        example_field = list(example.fields.values())[0]\n        tag = example_field.name\n        val = example_field.value\n        if not isinstance(val, AstExampleRef):\n            raise InvalidSpec(\n                \"Example of struct with enumerated subtypes must be a \"\n                \"reference to a subtype's example.\",\n                example_field.lineno, example_field.path)\n\n        for subtype_field in self.get_enumerated_subtypes():\n            if subtype_field.name == tag:\n                self._raw_examples[example.label] = example\n                break\n        else:\n            raise InvalidSpec(\n                \"Unknown subtype tag '%s' in example.\" % tag,\n                example_field.lineno, example_field.path)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add_example_helper(self, example):\n\n        # Check for fields in the example that don't belong.\n        for label, example_field in example.fields.items():\n            if not any(label == f.name for f in self.all_fields):\n                raise InvalidSpec(\n                    \"Example for '%s' has unknown field '%s'.\" %\n                    (self.name, label),\n                    example_field.lineno, example_field.path,\n                )\n\n        for field in self.all_fields:\n            if field.name in example.fields:\n                example_field = example.fields[field.name]\n                try:\n                    field.data_type.check_example(example_field)\n                except InvalidSpec as e:\n                    e.msg = \"Bad example for field '{}': {}\".format(\n                        field.name, e.msg)\n                    raise\n            elif field.has_default or isinstance(field.data_type, Nullable):\n                # These don't need examples.\n                pass\n            else:\n                raise InvalidSpec(\n                    \"Missing field '%s' in example.\" % field.name,\n                    example.lineno, example.path)\n\n        self._raw_examples[example.label] = example", "response": "Validates examples for structs with enumerated subtypes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute_examples(self):\n        for label in self._raw_examples:\n            self._examples[label] = self._compute_example(label)", "response": "Compute the examples for each label in the _raw_examples attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_example_flat_helper(self, label):\n        assert label in self._raw_examples, label\n\n        example = self._raw_examples[label]\n\n        def deref_example_ref(dt, val):\n            dt, _ = unwrap_nullable(dt)\n            if not dt._has_example(val.label):\n                raise InvalidSpec(\n                    \"Reference to example for '%s' with label '%s' \"\n                    \"does not exist.\" % (dt.name, val.label),\n                    val.lineno, val.path)\n            return dt._compute_example(val.label).value\n\n        # Do a deep copy of the example because we're going to mutate it.\n        ex_val = OrderedDict()\n\n        def get_json_val(dt, val):\n            if isinstance(val, AstExampleRef):\n                # Embed references to other examples directly.\n                return deref_example_ref(dt, val)\n            elif isinstance(val, TagRef):\n                return val.union_data_type._compute_example(val.tag_name).value\n            elif isinstance(val, list):\n                dt, _ = unwrap_nullable(dt)\n                return [get_json_val(dt.data_type, v) for v in val]\n            elif isinstance(val, dict):\n                dt, _ = unwrap_nullable(dt)\n                if is_alias(dt):\n                    return val\n                return {k: get_json_val(dt.value_data_type, v) for (k, v) in val.items()}\n            else:\n                return val\n\n        for field in self.all_fields:\n            if field.name in example.fields:\n                example_field = example.fields[field.name]\n                if example_field.value is None:\n                    # Serialized format doesn't include fields with null.\n                    pass\n                else:\n                    ex_val[field.name] = get_json_val(\n                        field.data_type, example_field.value)\n            elif field.has_default:\n                ex_val[field.name] = get_json_val(\n                    field.data_type, field.default)\n\n        return Example(example.label, example.text, ex_val, ast_node=example)", "response": "Helper function that computes the example from the raw example."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_example_enumerated_subtypes(self, label):\n        assert label in self._raw_examples, label\n\n        example = self._raw_examples[label]\n\n        example_field = list(example.fields.values())[0]\n\n        for subtype_field in self.get_enumerated_subtypes():\n            if subtype_field.name == example_field.name:\n                data_type = subtype_field.data_type\n                break\n\n        ref = example_field.value\n        if not data_type._has_example(ref.label):\n            raise InvalidSpec(\n                \"Reference to example for '%s' with label '%s' does not \"\n                \"exist.\" % (data_type.name, ref.label),\n                ref.lineno, ref.path)\n\n        ordered_value = OrderedDict([('.tag', example_field.name)])\n        flat_example = data_type._compute_example_flat_helper(ref.label)\n        ordered_value.update(flat_example.value)\n        flat_example.value = ordered_value\n        return flat_example", "response": "A helper method for enumerated subtypes."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_attributes(self, doc, fields,  # pylint: disable=arguments-differ\n            parent_type=None, catch_all_field=None):\n        \"\"\"\n        :param UnionField catch_all_field: The field designated as the\n            catch-all. This field should be a member of the list of fields.\n\n        See :meth:`Composite.set_attributes` for parameter definitions.\n        \"\"\"\n        if parent_type:\n            assert isinstance(parent_type, Union)\n\n        super(Union, self).set_attributes(doc, fields, parent_type)\n\n        self.catch_all_field = catch_all_field\n        self.parent_type = parent_type", "response": "Set the attributes of the union object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef all_fields(self):\n        fields = []\n        if self.parent_type:\n            fields.extend(self.parent_type.all_fields)\n        fields.extend([f for f in self.fields])\n        return fields", "response": "Returns a list of all fields. Subtype fields come before this type s fields."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a raw example for this type.", "response": "def _add_example(self, example):\n        \"\"\"Adds a \"raw example\" for this type.\n\n        This does basic sanity checking to ensure that the example is valid\n        (required fields specified, no unknown fields, correct types, ...).\n\n        The example is not available via :meth:`get_examples` until\n        :meth:`_compute_examples` is called.\n\n        Args:\n            example (stone.frontend.ast.AstExample): An example of this\n                type.\n        \"\"\"\n        if len(example.fields) != 1:\n            raise InvalidSpec(\n                'Example for union must specify exactly one tag.',\n                example.lineno, example.path)\n\n        # Extract the only tag in the example.\n        example_field = list(example.fields.values())[0]\n        tag = example_field.name\n\n        # Find the union member that corresponds to the tag.\n        for field in self.all_fields:\n            if tag == field.name:\n                break\n        else:\n            # Error: Tag doesn't match any union member.\n            raise InvalidSpec(\n                \"Unknown tag '%s' in example.\" % tag,\n                example.lineno, example.path\n            )\n\n        # TODO: are we always guaranteed at least one field?\n        # pylint: disable=undefined-loop-variable\n        try:\n            field.data_type.check_example(example_field)\n        except InvalidSpec as e:\n            e.msg = \"Bad example for field '{}': {}\".format(\n                field.name, e.msg)\n            raise\n\n        self._raw_examples[example.label] = example"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _compute_examples(self):\n        for label in self._raw_examples:\n            self._examples[label] = self._compute_example(label)\n\n        # Add examples for each void union member.\n        for field in self.all_fields:\n            dt, _ = unwrap_nullable(field.data_type)\n            if is_void_type(dt):\n                self._examples[field.name] = \\\n                    Example(\n                        field.name, None, OrderedDict([('.tag', field.name)]))", "response": "Compute the examples for each label in _raw_examples and add them to the internal _examples attribute."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the example for the given label.", "response": "def _compute_example(self, label):\n        \"\"\"\n        From the \"raw example,\" resolves references to examples of other data\n        types to compute the final example.\n\n        Returns an Example object. The `value` attribute contains a\n        JSON-serializable representation of the example.\n        \"\"\"\n        if label in self._raw_examples:\n\n            example = self._raw_examples[label]\n\n            def deref_example_ref(dt, val):\n                dt, _ = unwrap_nullable(dt)\n                if not dt._has_example(val.label):\n                    raise InvalidSpec(\n                        \"Reference to example for '%s' with label '%s' \"\n                        \"does not exist.\" % (dt.name, val.label),\n                        val.lineno, val.path)\n                return dt._compute_example(val.label).value\n\n            def get_json_val(dt, val):\n                if isinstance(val, AstExampleRef):\n                    # Embed references to other examples directly.\n                    return deref_example_ref(dt, val)\n                elif isinstance(val, list):\n                    return [get_json_val(dt.data_type, v) for v in val]\n                else:\n                    return val\n\n            example_field = list(example.fields.values())[0]\n\n            # Do a deep copy of the example because we're going to mutate it.\n            ex_val = OrderedDict([('.tag', example_field.name)])\n\n            for field in self.all_fields:\n                if field.name == example_field.name:\n                    break\n\n            # TODO: are we always guaranteed at least one field?\n            # pylint: disable=undefined-loop-variable\n            data_type, _ = unwrap_nullable(field.data_type)\n            inner_ex_val = get_json_val(data_type, example_field.value)\n            if (isinstance(data_type, Struct) and\n                    not data_type.has_enumerated_subtypes()):\n                ex_val.update(inner_ex_val)\n            else:\n                if inner_ex_val is not None:\n                    ex_val[field.name] = inner_ex_val\n\n            return Example(example.label, example.text, ex_val, ast_node=example)\n\n        else:\n            # Try to fallback to a union member with tag matching the label\n            # with a data type that is composite or void.\n            for field in self.all_fields:\n                if label == field.name:\n                    break\n            else:\n                raise AssertionError('No example for label %r' % label)\n\n            # TODO: are we always guaranteed at least one field?\n            # pylint: disable=undefined-loop-variable\n            assert is_void_type(field.data_type)\n            return Example(\n                field.name, field.doc, OrderedDict([('.tag', field.name)]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef unique_field_data_types(self):\n        data_type_names = set()\n        for field in self.fields:\n            if not is_void_type(field.data_type):\n                if field.data_type.name in data_type_names:\n                    return False\n                else:\n                    data_type_names.add(field.data_type.name)\n        else:\n            return True", "response": "Checks if all variants have different data types. Returns True if so False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_attributes(self, doc, data_type):\n        self.raw_doc = doc\n        self.doc = doc_unwrap(doc)\n        self.data_type = data_type\n\n        # Make sure we don't have a cyclic reference.\n        # Since attributes are set one data type at a time, only the last data\n        # type to be populated in a cycle will be able to detect the cycle.\n        # Before that, the cycle will be broken by an alias with no populated\n        # source.\n        cur_data_type = data_type\n        while is_alias(cur_data_type):\n            cur_data_type = cur_data_type.data_type\n            if cur_data_type == self:\n                raise InvalidSpec(\n                    \"Alias '%s' is part of a cycle.\" % self.name,\n                    self._ast_node.lineno, self._ast_node.path)", "response": "Set attributes of the alias."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_route_attr_filter(route_attr_filter, debug=False):\n    assert isinstance(route_attr_filter, six.text_type), type(route_attr_filter)\n    parser = FilterExprParser(debug)\n    return parser.parse(route_attr_filter)", "response": "Parses the route attribute filter into a tuple of FilterExpr and a list of errors."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, data):\n        parsed_data = self.yacc.parse(\n            data, lexer=self.lexer.get_yacc_compat_lexer(), debug=self.debug)\n        self.errors = self.lexer.errors + self.errors\n        return parsed_data, self.errors", "response": "Parse the data and return the parsed data and errors."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates a module for each namespace.", "response": "def generate(self, api):\n        \"\"\"\n        Generates a module for each namespace.\n\n        Each namespace will have Obj C classes to represent data types and\n        routes in the Stone spec.\n        \"\"\"\n        rsrc_folder = os.path.join(os.path.dirname(__file__), 'obj_c_rsrc')\n        rsrc_output_folder = os.path.join(self.target_folder_path, 'Resources')\n\n        if not os.path.exists(rsrc_output_folder):\n            os.makedirs(rsrc_output_folder)\n\n        self.logger.info('Copying DBStoneValidators.{h,m} to output folder')\n        shutil.copy(\n            os.path.join(rsrc_folder, 'DBStoneValidators.h'),\n            rsrc_output_folder)\n        shutil.copy(\n            os.path.join(rsrc_folder, 'DBStoneValidators.m'),\n            rsrc_output_folder)\n        self.logger.info('Copying DBStoneSerializers.{h,m} to output folder')\n        shutil.copy(\n            os.path.join(rsrc_folder, 'DBStoneSerializers.h'),\n            rsrc_output_folder)\n        shutil.copy(\n            os.path.join(rsrc_folder, 'DBStoneSerializers.m'),\n            rsrc_output_folder)\n        self.logger.info('Copying DBStoneBase.{h,m} to output folder')\n        shutil.copy(\n            os.path.join(rsrc_folder, 'DBStoneBase.h'), rsrc_output_folder)\n        shutil.copy(\n            os.path.join(rsrc_folder, 'DBStoneBase.m'), rsrc_output_folder)\n        self.logger.info('Copying DBSerializableProtocol.h to output folder')\n        shutil.copy(\n            os.path.join(rsrc_folder, 'DBSerializableProtocol.h'),\n            rsrc_output_folder)\n\n        jazzy_cfg = None\n\n        if self.args.documentation:\n            jazzy_cfg_path = os.path.join('../Format', 'jazzy.json')\n            with open(jazzy_cfg_path) as jazzy_file:\n                jazzy_cfg = json.load(jazzy_file)\n\n            for idx, namespace in enumerate(api.namespaces.values()):\n                ns_name = fmt_public_name(namespace.name)\n                ns_dict = {\"name\": ns_name, \"children\": [], }\n                jazzy_cfg['custom_categories'].insert(idx, ns_dict)\n\n        for namespace in api.namespaces.values():\n            self.namespace_to_has_route_auth_list[namespace] = set()\n            if namespace.routes:\n                for route in namespace.routes:\n                    if route.attrs.get('auth') != 'noauth':\n                        self.namespace_to_has_route_auth_list[namespace].add(\n                            route.attrs.get('auth'))\n                    else:\n                        self.namespace_to_has_route_auth_list[namespace].add(\n                            'user')\n\n        with self.output_to_relative_path('DBSDKImportsGenerated.h'):\n            self._generate_all_imports(api)\n\n        for namespace in api.namespaces.values():\n            for data_type in namespace.linearize_data_types():\n                self.obj_name_to_namespace[data_type.name] = fmt_class_prefix(\n                    data_type)\n\n        for namespace in api.namespaces.values():\n            ns_name = fmt_public_name(namespace.name)\n            self._generate_namespace_types(namespace, jazzy_cfg)\n\n            if namespace.routes:\n                if self.args.documentation:\n                    for auth_type in self.namespace_to_has_route_auth_list[\n                            namespace]:\n                        append_to_jazzy_category_dict(\n                            jazzy_cfg, 'Routes', fmt_routes_class(ns_name, auth_type))\n                    append_to_jazzy_category_dict(\n                        jazzy_cfg, 'RouteObjects', fmt_route_obj_class(ns_name))\n                self._generate_route_objects_m(api.route_schema, namespace)\n                self._generate_route_objects_h(api.route_schema, namespace)\n\n        if self.args.documentation:\n            with self.output_to_relative_path('../../../../.jazzy.json'):\n                self.emit_raw(json.dumps(jazzy_cfg, indent=2) + '\\n')"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_namespace_types(self, namespace, jazzy_cfg):\n        ns_name = fmt_public_name(namespace.name)\n        output_path = os.path.join('ApiObjects', ns_name)\n        output_path_headers = os.path.join(output_path, 'Headers')\n\n        for data_type in namespace.linearize_data_types():\n            class_name = fmt_class_prefix(data_type)\n\n            if self.args.documentation:\n                append_to_jazzy_category_dict(jazzy_cfg, ns_name, class_name)\n                append_to_jazzy_category_dict(\n                    jazzy_cfg, 'Serializers', '{}Serializer'.format(class_name))\n\n            if is_struct_type(data_type):\n                # struct header\n                file_path = os.path.join(output_path_headers,\n                                         class_name + '.h')\n                with self.output_to_relative_path(file_path):\n                    self.emit_raw(base_file_comment)\n                    self._generate_struct_class_h(data_type)\n            elif is_union_type(data_type):\n\n                if self.args.documentation:\n                    append_to_jazzy_category_dict(\n                        jazzy_cfg, 'Tags', '{}Tag'.format(fmt_class_prefix(data_type)))\n                # union header\n                file_path = os.path.join(output_path_headers,\n                                         class_name + '.h')\n                with self.output_to_relative_path(file_path):\n                    self.emit_raw(base_file_comment)\n                    self._generate_union_class_h(data_type)\n            else:\n                raise TypeError('Can\\'t handle type %r' % type(data_type))\n\n        file_path = os.path.join(\n            output_path,\n            'DB{}Objects.m'.format(fmt_camel_upper(namespace.name)))\n        with self.output_to_relative_path(file_path):\n            self.emit_raw(base_file_comment)\n\n            description = '/// Arguments, results, and errors for the `{}` namespace.'.format(\n                fmt_camel_upper(namespace.name))\n            self.emit(description)\n\n            if self.args.exclude_from_analysis:\n                self.emit()\n                self.emit('#ifndef __clang_analyzer__')\n\n            for data_type in namespace.linearize_data_types():\n                if is_struct_type(data_type):\n                    # struct implementation\n                    self._generate_struct_class_m(data_type)\n                elif is_union_type(data_type):\n                    # union implementation\n                    self._generate_union_class_m(data_type)\n\n            if self.args.exclude_from_analysis:\n                self.emit('#endif')", "response": "Generates the types of the given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_struct_class_m(self, struct):\n        self.emit()\n        self._generate_imports_m(\n            self._get_imports_m(\n                struct,\n                default_imports=['DBStoneSerializers', 'DBStoneValidators']))\n\n        struct_name = fmt_class_prefix(struct)\n\n        self.emit('#pragma mark - API Object')\n        self.emit()\n        with self.block_m(struct_name):\n            self.emit('#pragma mark - Constructors')\n            self.emit()\n            self._generate_struct_cstor(struct)\n            self._generate_struct_cstor_default(struct)\n            self.emit('#pragma mark - Serialization methods')\n            self.emit()\n            self._generate_serializable_funcs(struct_name)\n            self.emit('#pragma mark - Description method')\n            self.emit()\n            self._generate_description_func(struct_name)\n            self.emit('#pragma mark - Copyable method')\n            self.emit()\n            self._generate_copyable_func()\n            self.emit('#pragma mark - Hash method')\n            self.emit()\n            self._generate_hash_func(struct)\n            self.emit('#pragma mark - Equality method')\n            self.emit()\n            self._generate_equality_func(struct)\n\n        self.emit()\n        self.emit()\n\n        self.emit('#pragma mark - Serializer Object')\n        self.emit()\n        with self.block_m(fmt_serial_class(struct_name)):\n            self._generate_struct_serializer(struct)\n            self._generate_struct_deserializer(struct)", "response": "Defines an Obj C implementation file that represents a struct in Stone."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining an Obj C header file that represents a struct in Stone.", "response": "def _generate_struct_class_h(self, struct):\n        \"\"\"Defines an Obj C header file that represents a struct in Stone.\"\"\"\n        self._generate_init_imports_h(struct)\n        self._generate_imports_h(self._get_imports_h(struct))\n\n        self.emit()\n        self.emit('NS_ASSUME_NONNULL_BEGIN')\n        self.emit()\n\n        self.emit('#pragma mark - API Object')\n        self.emit()\n\n        self._generate_class_comment(struct)\n\n        struct_name = fmt_class_prefix(struct)\n\n        with self.block_h_from_data_type(struct, protocol=['DBSerializable', 'NSCopying']):\n            self.emit('#pragma mark - Instance fields')\n            self.emit()\n            self._generate_struct_properties(struct.fields)\n            self.emit('#pragma mark - Constructors')\n            self.emit()\n            self._generate_struct_cstor_signature(struct)\n            self._generate_struct_cstor_signature_default(struct)\n            self._generate_init_unavailable_signature(struct)\n\n        self.emit()\n        self.emit()\n\n        self.emit('#pragma mark - Serializer Object')\n        self.emit()\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            'The serialization class for the `{}` struct.'.format(\n                fmt_class(struct.name)),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n        with self.block_h(fmt_serial_class(struct_name)):\n            self._generate_serializer_signatures(struct_name)\n\n        self.emit()\n        self.emit('NS_ASSUME_NONNULL_END')\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndefines an Obj C implementation file that represents a union in Stone.", "response": "def _generate_union_class_m(self, union):\n        \"\"\"Defines an Obj C implementation file that represents a union in Stone.\"\"\"\n        self.emit()\n        self._generate_imports_m(\n            self._get_imports_m(\n                union,\n                default_imports=['DBStoneSerializers', 'DBStoneValidators']))\n\n        union_name = fmt_class_prefix(union)\n\n        self.emit('#pragma mark - API Object')\n        self.emit()\n        with self.block_m(fmt_class_prefix(union)):\n            self._generate_synthesize_ivars(union)\n            self.emit('#pragma mark - Constructors')\n            self.emit()\n            self._generate_union_cstor_funcs(union)\n            self.emit('#pragma mark - Instance field accessors')\n            self.emit()\n            self._generate_union_tag_vars_funcs(union)\n            self.emit('#pragma mark - Tag state methods')\n            self.emit()\n            self._generate_union_tag_state_funcs(union)\n            self.emit('#pragma mark - Serialization methods')\n            self.emit()\n            self._generate_serializable_funcs(union_name)\n            self.emit('#pragma mark - Description method')\n            self.emit()\n            self._generate_description_func(union_name)\n            self.emit('#pragma mark - Copyable method')\n            self.emit()\n            self._generate_copyable_func()\n            self.emit('#pragma mark - Hash method')\n            self.emit()\n            self._generate_hash_func(union)\n            self.emit('#pragma mark - Equality method')\n            self.emit()\n            self._generate_equality_func(union)\n\n        self.emit()\n        self.emit()\n\n        self.emit('#pragma mark - Serializer Object')\n        self.emit()\n        with self.block_m(fmt_serial_class(union_name)):\n            self._generate_union_serializer(union)\n            self._generate_union_deserializer(union)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_union_class_h(self, union):\n        self._generate_init_imports_h(union)\n        self._generate_imports_h(self._get_imports_h(union))\n\n        self.emit()\n        self.emit('NS_ASSUME_NONNULL_BEGIN')\n        self.emit()\n\n        self.emit('#pragma mark - API Object')\n        self.emit()\n        self._generate_class_comment(union)\n\n        union_name = fmt_class_prefix(union)\n\n        with self.block_h_from_data_type(union, protocol=['DBSerializable', 'NSCopying']):\n            self.emit('#pragma mark - Instance fields')\n            self.emit()\n            self._generate_union_tag_state(union)\n            self._generate_union_tag_property(union)\n            self._generate_union_properties(union.all_fields)\n            self.emit('#pragma mark - Constructors')\n            self.emit()\n            self._generate_union_cstor_signatures(union, union.all_fields)\n            self._generate_init_unavailable_signature(union)\n            self.emit('#pragma mark - Tag state methods')\n            self.emit()\n            self._generate_union_tag_access_signatures(union)\n\n        self.emit()\n        self.emit()\n\n        self.emit('#pragma mark - Serializer Object')\n        self.emit()\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            'The serialization class for the `{}` union.'.format(union_name),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n        with self.block_h(fmt_serial_class(union_name)):\n            self._generate_serializer_signatures(union_name)\n\n        self.emit()\n        self.emit('NS_ASSUME_NONNULL_END')\n        self.emit()", "response": "Defines an Obj C header file that represents a union in Stone."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nemit struct standard constructor.", "response": "def _generate_struct_cstor(self, struct):\n        \"\"\"Emits struct standard constructor.\"\"\"\n        with self.block_func(\n                func=self._cstor_name_from_fields(struct.all_fields),\n                args=fmt_func_args_from_fields(struct.all_fields),\n                return_type='instancetype'):\n            for field in struct.all_fields:\n                self._generate_validator(field)\n\n            self.emit()\n\n            super_fields = [\n                f for f in struct.all_fields if f not in struct.fields\n            ]\n\n            if super_fields:\n                super_args = fmt_func_args([(fmt_var(f.name), fmt_var(f.name))\n                                            for f in super_fields])\n                self.emit('self = [super {}:{}];'.format(\n                    self._cstor_name_from_fields(super_fields), super_args))\n            else:\n                if struct.parent_type:\n                    self.emit('self = [super initDefault];')\n                else:\n                    self.emit('self = [super init];')\n            with self.block_init():\n                for field in struct.fields:\n                    field_name = fmt_var(field.name)\n\n                    if field.has_default:\n                        self.emit('_{} = {} ?: {};'.format(\n                            field_name, field_name, fmt_default_value(field)))\n                    else:\n                        self.emit('_{} = {};'.format(field_name, field_name))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_struct_cstor_default(self, struct):\n        if not self._struct_has_defaults(struct):\n            return\n\n        fields_no_default = [\n            f for f in struct.all_fields\n            if not f.has_default and not is_nullable_type(f.data_type)\n        ]\n\n        with self.block_func(\n                func=self._cstor_name_from_fields(fields_no_default),\n                args=fmt_func_args_from_fields(fields_no_default),\n                return_type='instancetype'):\n            args = ([(fmt_var(f.name), fmt_var(f.name) if not f.has_default and\n                      not is_nullable_type(f.data_type) else 'nil')\n                     for f in struct.all_fields])\n            cstor_args = fmt_func_args(args)\n            self.emit('return [self {}:{}];'.format(\n                self._cstor_name_from_fields(struct.all_fields), cstor_args))\n        self.emit()", "response": "Emits struct convenience constructor. Default arguments are omitted."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nemit the standard constructor signature to be used in the struct s header file.", "response": "def _generate_struct_cstor_signature(self, struct):\n        \"\"\"Emits struct standard constructor signature to be used in the struct's header file.\"\"\"\n        fields = struct.all_fields\n        self.emit(comment_prefix)\n        description_str = 'Full constructor for the struct (exposes all instance variables).'\n        self.emit_wrapped_text(description_str, prefix=comment_prefix)\n        signature = fmt_signature(\n            func=self._cstor_name_from_fields(fields),\n            args=self._cstor_args_from_fields(fields, is_struct=True),\n            return_type='instancetype')\n        self.emit(comment_prefix)\n        for field in struct.all_fields:\n            doc = self.process_doc(field.doc,\n                                   self._docf) if field.doc else undocumented\n            self.emit_wrapped_text(\n                '@param {} {}'.format(fmt_var(field.name), doc),\n                prefix=comment_prefix)\n        if struct.all_fields:\n            self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            '@return An initialized instance.', prefix=comment_prefix)\n        self.emit(comment_prefix)\n        self.emit('{};'.format(signature))\n\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_struct_cstor_signature_default(self, struct):\n        if not self._struct_has_defaults(struct):\n            return\n\n        fields_no_default = [\n            f for f in struct.all_fields\n            if not f.has_default and not is_nullable_type(f.data_type)\n        ]\n        signature = fmt_signature(\n            func=self._cstor_name_from_fields(fields_no_default),\n            args=self._cstor_args_from_fields(\n                fields_no_default, is_struct=True),\n            return_type='instancetype')\n\n        self.emit(comment_prefix)\n        description_str = (\n            'Convenience constructor (exposes only non-nullable '\n            'instance variables with no default value).')\n        self.emit_wrapped_text(description_str, prefix=comment_prefix)\n        self.emit(comment_prefix)\n        for field in fields_no_default:\n            doc = self.process_doc(field.doc,\n                                   self._docf) if field.doc else undocumented\n            self.emit_wrapped_text(\n                '@param {} {}'.format(fmt_var(field.name), doc),\n                prefix=comment_prefix)\n        if struct.all_fields:\n            self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            '@return An initialized instance.', prefix=comment_prefix)\n        self.emit(comment_prefix)\n\n        self.emit('{};'.format(signature))\n        self.emit()", "response": "Emits struct convenience constructor with default arguments\n            ommitted signature to be used in the struct header file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_union_cstor_funcs(self, union):\n        for field in union.all_fields:\n            enum_field_name = fmt_enum_name(field.name, union)\n            func_args = [] if is_void_type(\n                field.data_type) else fmt_func_args_from_fields([field])\n\n            with self.block_func(\n                    func=self._cstor_name_from_field(field),\n                    args=func_args,\n                    return_type='instancetype'):\n                self.emit('self = [super init];')\n                with self.block_init():\n                    self.emit('_tag = {};'.format(enum_field_name))\n                    if not is_void_type(field.data_type):\n                        self.emit('_{} = {};'.format(\n                            fmt_var(field.name), fmt_var(field.name)))\n            self.emit()", "response": "Emits standard union constructor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_union_cstor_signatures(self, union, fields):  # pylint: disable=unused-argument\n        for field in fields:\n            args = self._cstor_args_from_fields(\n                [field] if not is_void_type(field.data_type) else [])\n            signature = fmt_signature(\n                func=self._cstor_name_from_field(field),\n                args=args,\n                return_type='instancetype')\n            self.emit(comment_prefix)\n            self.emit_wrapped_text(\n                'Initializes union class with tag state of \"{}\".'.format(\n                    field.name),\n                prefix=comment_prefix)\n            self.emit(comment_prefix)\n            if field.doc:\n                doc = self.process_doc(\n                    field.doc, self._docf) if field.doc else undocumented\n                self.emit_wrapped_text(\n                    'Description of the \"{}\" tag state: {}'.format(\n                        field.name, doc),\n                    prefix=comment_prefix)\n                self.emit(comment_prefix)\n            if not is_void_type(field.data_type):\n                doc = self.process_doc(\n                    field.doc, self._docf) if field.doc else undocumented\n                self.emit_wrapped_text(\n                    '@param {} {}'.format(fmt_var(field.name), doc),\n                    prefix=comment_prefix)\n                self.emit(comment_prefix)\n            self.emit_wrapped_text(\n                '@return An initialized instance.', prefix=comment_prefix)\n            self.emit(comment_prefix)\n            self.emit('{};'.format(signature))\n            self.emit()", "response": "Emits union constructor signatures to be used in the union s header file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_union_tag_state(self, union):\n        union_name = fmt_class_prefix(union)\n        tag_type = fmt_enum_name('tag', union)\n        description_str = ('The `{}` enum type represents the possible tag '\n                           'states with which the `{}` union can exist.')\n        self.emit_wrapped_text(\n            description_str.format(tag_type, union_name),\n            prefix=comment_prefix)\n        with self.block(\n                'typedef NS_ENUM(NSInteger, {})'.format(tag_type), after=';'):\n            for field in union.all_fields:\n                doc = self.process_doc(\n                    field.doc, self._docf) if field.doc else undocumented\n                self.emit_wrapped_text(doc, prefix=comment_prefix)\n                self.emit('{},'.format(fmt_enum_name(field.name, union)))\n                self.emit()\n        self.emit()", "response": "Emits union tag enum type which stores union state."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_serializable_funcs(self, data_type_name):\n        with self.block_func(\n                func='serialize',\n                args=fmt_func_args_declaration([('instance', 'id')]),\n                return_type='nullable NSDictionary<NSString *, id>  *',\n                class_func=True):\n            func_call = fmt_func_call(\n                caller=fmt_serial_class(data_type_name),\n                callee='serialize',\n                args=fmt_func_args([('instance', 'instance')]))\n            self.emit('return {};'.format(func_call))\n        self.emit()\n\n        with self.block_func(\n                func='deserialize',\n                args=fmt_func_args_declaration([('dict', 'NSDictionary<NSString *, id>  *')]),\n                return_type='id',\n                class_func=True):\n            self.emit('return {};'.format(\n                fmt_func_call(\n                    caller=fmt_serial_class(data_type_name),\n                    callee='deserialize',\n                    args=fmt_func_args([('dict', 'dict')]))))\n        self.emit()", "response": "Emits the two struct / union functions that implement the Serializable protocol."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nemitting the signatures of the serializer object s deserializing functions.", "response": "def _generate_serializer_signatures(self, obj_name):\n        \"\"\"Emits the signatures of the serializer object's serializing functions.\"\"\"\n        serial_signature = fmt_signature(\n            func='serialize',\n            args=fmt_func_args_declaration([(\n                'instance', '{} *'.format(obj_name))]),\n            return_type='nullable NSDictionary<NSString *, id>  *',\n            class_func=True)\n        deserial_signature = fmt_signature(\n            func='deserialize',\n            args=fmt_func_args_declaration([('dict',\n                                             'NSDictionary<NSString *, id>  *')]),\n            return_type='{} *'.format(obj_name),\n            class_func=True)\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            'Serializes `{}` instances.'.format(obj_name),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            '@param instance An instance of the `{}` API object.'.format(\n                obj_name),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n        description_str = ('@return A json-compatible dictionary '\n                           'representation of the `{}` API object.')\n        self.emit_wrapped_text(\n            description_str.format(obj_name), prefix=comment_prefix)\n        self.emit(comment_prefix)\n        self.emit('{};'.format(serial_signature))\n        self.emit()\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            'Deserializes `{}` instances.'.format(obj_name),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n        description_str = ('@param dict A json-compatible dictionary '\n                           'representation of the `{}` API object.')\n        self.emit_wrapped_text(\n            description_str.format(obj_name), prefix=comment_prefix)\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            '@return An instantiation of the `{}` object.'.format(obj_name),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n        self.emit('{};'.format(deserial_signature))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a string representing the properly formatted arguments for a constructor.", "response": "def _cstor_args_from_fields(self, fields, is_struct=False):\n        \"\"\"Returns a string representing the properly formatted arguments for a constructor.\"\"\"\n        if is_struct:\n            args = [(fmt_var(f.name),\n                fmt_type(f.data_type, tag=True, has_default=f.has_default)) for f in fields]\n        else:\n            args = [(fmt_var(f.name), fmt_type(f.data_type, tag=True)) for f in fields]\n\n        return fmt_func_args_declaration(args)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_validator(self, field):\n        validator = self._determine_validator_type(field.data_type,\n                                                   fmt_var(field.name),\n                                                   field.has_default)\n        value = fmt_var(\n            field.name) if not field.has_default else '{} ?: {}'.format(\n                fmt_var(field.name), fmt_default_value(field))\n        if validator:\n            self.emit('{}({});'.format(validator, value))", "response": "Emits validator if data type has associated validator."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines the validator string for given data type.", "response": "def _determine_validator_type(self, data_type, value, has_default):\n        \"\"\"Returns validator string for given data type, else None.\"\"\"\n        data_type, nullable = unwrap_nullable(data_type)\n\n        validator = None\n\n        if is_list_type(data_type):\n            item_validator = self._determine_validator_type(\n                data_type.data_type, value, False)\n            item_validator = item_validator if item_validator else 'nil'\n\n            validator = '{}:{}'.format(\n                fmt_validator(data_type),\n                fmt_func_args([\n                    ('minItems', '@({})'.format(data_type.min_items)\n                     if data_type.min_items else 'nil'),\n                    ('maxItems', '@({})'.format(data_type.max_items)\n                     if data_type.max_items else 'nil'),\n                    ('itemValidator', item_validator),\n                ]))\n        elif is_map_type(data_type):\n            item_validator = self._determine_validator_type(\n                data_type.value_data_type, value, False)\n            item_validator = item_validator if item_validator else 'nil'\n\n            validator = '{}:{}'.format(\n                fmt_validator(data_type),\n                fmt_func_args([\n                    ('itemValidator', item_validator),\n                ]))\n        elif is_numeric_type(data_type):\n            if data_type.min_value or data_type.max_value:\n                validator = '{}:{}'.format(\n                    fmt_validator(data_type),\n                    fmt_func_args([\n                        ('minValue', '@({})'.format(data_type.min_value)\n                         if data_type.min_value else 'nil'),\n                        ('maxValue', '@({})'.format(data_type.max_value)\n                         if data_type.max_value else 'nil'),\n                    ]))\n        elif is_string_type(data_type):\n            if data_type.pattern or data_type.min_length or data_type.max_length:\n                pattern = data_type.pattern.encode('unicode_escape').replace(\n                    \"\\\"\", \"\\\\\\\"\") if data_type.pattern else None\n                validator = '{}:{}'.format(\n                    fmt_validator(data_type),\n                    fmt_func_args([\n                        ('minLength', '@({})'.format(data_type.min_length)\n                         if data_type.min_length else 'nil'),\n                        ('maxLength', '@({})'.format(data_type.max_length)\n                         if data_type.max_length else 'nil'),\n                        ('pattern', '@\"{}\"'.format(pattern)\n                         if pattern else 'nil'),\n                    ]))\n\n        if nullable:\n            if validator:\n                validator = fmt_func_call(\n                    caller='DBStoneValidators', callee=validator)\n                validator = fmt_func_call(\n                    caller='DBStoneValidators',\n                    callee='nullableValidator',\n                    args=validator)\n        else:\n            if validator:\n                validator = fmt_func_call(\n                    caller='DBStoneValidators', callee=validator)\n            else:\n                validator = 'nil'\n            if not has_default:\n                validator = fmt_func_call(\n                    caller='DBStoneValidators',\n                    callee='nonnullValidator',\n                    args=validator)\n            else:\n                validator = None\n        return validator"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_struct_serializer(self, struct):\n        struct_name = fmt_class_prefix(struct)\n\n        with self.block_func(\n                func='serialize',\n                args=fmt_func_args_declaration([('valueObj',\n                                                 '{} *'.format(struct_name))]),\n                return_type='NSDictionary<NSString *, id> *',\n                class_func=True):\n            if not struct.all_fields and not struct.has_enumerated_subtypes():\n                self.emit('#pragma unused(valueObj)')\n\n            self.emit(\n                'NSMutableDictionary *jsonDict = [[NSMutableDictionary alloc] init];'\n            )\n            self.emit()\n            for field in struct.all_fields:\n                data_type, nullable = unwrap_nullable(field.data_type)\n\n                input_value = 'valueObj.{}'.format(fmt_var(field.name))\n                serialize_call = self._fmt_serialization_call(\n                    field.data_type, input_value, True)\n\n                if not nullable:\n                    if is_primitive_type(data_type):\n                        self.emit('jsonDict[@\"{}\"] = {};'.format(\n                            field.name, input_value))\n                    else:\n                        self.emit('jsonDict[@\"{}\"] = {};'.format(\n                            field.name, serialize_call))\n                else:\n                    with self.block('if ({})'.format(input_value)):\n                        self.emit('jsonDict[@\"{}\"] = {};'.format(\n                            field.name, serialize_call))\n            self.emit()\n\n            if struct.has_enumerated_subtypes():\n                first_block = True\n                for tags, subtype in struct.get_all_subtypes_with_tags():\n                    assert len(tags) == 1, tags\n                    tag = tags[0]\n                    base_condition = '{} ([valueObj isKindOfClass:[{} class]])'\n                    with self.block(\n                            base_condition.format('if' if first_block else\n                                                  'else if',\n                                                  fmt_class_prefix(subtype))):\n                        if first_block:\n                            first_block = False\n                        func_args = fmt_func_args([('value',\n                            '({} *)valueObj'.format(\n                                fmt_class_prefix(\n                                    subtype)))])\n                        caller = fmt_serial_class(fmt_class_prefix(subtype))\n                        serialize_call = fmt_func_call(\n                            caller=caller, callee='serialize', args=func_args)\n                        self.emit('NSDictionary *subTypeFields = {};'.format(\n                            serialize_call))\n                        with self.block(\n                                'for (NSString* key in subTypeFields)'):\n                            self.emit('jsonDict[key] = subTypeFields[key];')\n                        self.emit(\n                            'jsonDict[@\".tag\"] = @\"{}\";'.format(fmt_var(tag)))\n                self.emit()\n            self.emit('return [jsonDict count] > 0 ? jsonDict : nil;')\n        self.emit()", "response": "Emits the serialize method for the serialization object for the given struct."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nemits the deserialize method for the serialization object for the given struct.", "response": "def _generate_struct_deserializer(self, struct):\n        \"\"\"Emits the deserialize method for the serialization object for the given struct.\"\"\"\n        struct_name = fmt_class_prefix(struct)\n\n        with self.block_func(\n                func='deserialize',\n                args=fmt_func_args_declaration([('valueDict',\n                                                 'NSDictionary<NSString *, id> *')]),\n                return_type='{} *'.format(struct_name),\n                class_func=True):\n            if not struct.all_fields and not struct.has_enumerated_subtypes():\n                self.emit('#pragma unused(valueDict)')\n\n            def emit_struct_deserialize_logic(struct):\n                for field in struct.all_fields:\n                    data_type, nullable = unwrap_nullable(field.data_type)\n                    input_value = 'valueDict[@\"{}\"]'.format(field.name)\n\n                    if is_primitive_type(data_type):\n                        deserialize_call = input_value\n                    else:\n                        deserialize_call = self._fmt_serialization_call(\n                            field.data_type, input_value, False)\n\n                    if nullable or field.has_default:\n                        default_value = fmt_default_value(\n                            field) if field.has_default else 'nil'\n                        if is_primitive_type(data_type):\n                            deserialize_call = '{} ?: {}'.format(\n                                input_value, default_value)\n                        else:\n                            deserialize_call = '{} ? {} : {}'.format(\n                                input_value, deserialize_call, default_value)\n\n                    self.emit('{}{} = {};'.format(\n                        fmt_type(field.data_type),\n                        fmt_var(field.name), deserialize_call))\n\n                self.emit()\n\n                deserialized_obj_args = [(fmt_var(f.name), fmt_var(f.name))\n                                         for f in struct.all_fields]\n                init_call = fmt_func_call(\n                    caller=fmt_alloc_call(caller=struct_name),\n                    callee=self._cstor_name_from_fields(struct.all_fields),\n                    args=fmt_func_args(deserialized_obj_args))\n                self.emit('return {};'.format(init_call))\n\n            if not struct.has_enumerated_subtypes():\n                emit_struct_deserialize_logic(struct)\n            else:\n                for tags, subtype in struct.get_all_subtypes_with_tags():\n                    assert len(tags) == 1, tags\n                    tag = tags[0]\n\n                    base_string = 'if ([valueDict[@\".tag\"] isEqualToString:@\"{}\"])'\n                    with self.block(base_string.format(tag)):\n                        caller = fmt_serial_class(fmt_class_prefix(subtype))\n                        args = fmt_func_args([('value', 'valueDict')])\n                        deserialize_call = fmt_func_call(\n                            caller=caller, callee='deserialize', args=args)\n                        self.emit('return {};'.format(deserialize_call))\n\n                self.emit()\n\n                if struct.is_catch_all():\n                    emit_struct_deserialize_logic(struct)\n                else:\n                    description_str = (\n                        '[NSString stringWithFormat:@\"Tag has an invalid '\n                        'value: \\\\\\\"%@\\\\\\\".\", valueDict[@\".tag\"]]')\n                    self._generate_throw_error('InvalidTag', description_str)\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nemit the serialize method for the serialization object for the given union.", "response": "def _generate_union_serializer(self, union):\n        \"\"\"Emits the serialize method for the serialization object for the given union.\"\"\"\n        union_name = fmt_class_prefix(union)\n\n        with self.block_func(\n                func='serialize',\n                args=fmt_func_args_declaration([('valueObj',\n                                                 '{} *'.format(union_name))]),\n                return_type='NSDictionary<NSString *, id> *',\n                class_func=True):\n\n            if not union.all_fields:\n                self.emit('#pragma unused(valueObj)')\n\n            self.emit(\n                'NSMutableDictionary *jsonDict = [[NSMutableDictionary alloc] init];'\n            )\n            self.emit()\n\n            first_block = True\n            for field in union.all_fields:\n                with self.block('{} ([valueObj is{}])'.format(\n                        'if' if first_block else 'else if',\n                        fmt_camel_upper(field.name))):\n                    data_type, nullable = unwrap_nullable(field.data_type)\n                    input_value = 'valueObj.{}'.format(fmt_var(field.name))\n                    serialize_call = self._fmt_serialization_call(\n                        field.data_type, input_value, True)\n\n                    def emit_serializer():\n                        if is_user_defined_type(data_type):\n                            if is_struct_type(data_type) and \\\n                                    not data_type.has_enumerated_subtypes():\n                                self.emit('jsonDict = [{} mutableCopy];'.\n                                          format(serialize_call))\n                            else:\n                                self.emit(\n                                    'jsonDict[@\"{}\"] = [{} mutableCopy];'.\n                                    format(field.name, serialize_call))\n                        elif is_primitive_type(data_type):\n                            self.emit('jsonDict[@\"{}\"] = {};'.format(\n                                field.name, input_value))\n                        else:\n                            self.emit('jsonDict[@\"{}\"] = {};'.format(\n                                field.name, serialize_call))\n\n                    if not is_void_type(data_type):\n                        if not nullable:\n                            emit_serializer()\n                        else:\n                            with self.block('if (valueObj.{})'.format(\n                                    fmt_var(field.name))):\n                                emit_serializer()\n\n                    self.emit('jsonDict[@\".tag\"] = @\"{}\";'.format(field.name))\n\n                if first_block:\n                    first_block = False\n\n            with self.block('else'):\n                if not union.closed:\n                    self.emit('jsonDict[@\".tag\"] = @\"other\";')\n                else:\n                    self._generate_throw_error(\n                        'InvalidTag',\n                        '@\"Object not properly initialized. Tag has an unknown value.\"'\n                    )\n\n            self.emit()\n            self.emit('return [jsonDict count] > 0 ? jsonDict : nil;')\n\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_union_deserializer(self, union):\n        union_name = fmt_class_prefix(union)\n\n        with self.block_func(\n                func='deserialize',\n                args=fmt_func_args_declaration([('valueDict',\n                                                 'NSDictionary<NSString *, id> *')]),\n                return_type='{} *'.format(union_name),\n                class_func=True):\n            if not union.all_fields:\n                self.emit('#pragma unused(valueDict)')\n\n            self.emit('NSString *tag = valueDict[@\".tag\"];')\n            self.emit()\n\n            first_block = True\n            for field in union.all_fields:\n                base_cond = '{} ([tag isEqualToString:@\"{}\"])'\n                with self.block(\n                        base_cond.format('if' if first_block else 'else if',\n                                         field.name)):\n                    if first_block:\n                        first_block = False\n                    if not is_void_type(field.data_type):\n                        data_type, nullable = unwrap_nullable(field.data_type)\n                        if is_struct_type(\n                                data_type\n                        ) and not data_type.has_enumerated_subtypes():\n                            input_value = 'valueDict'\n                        else:\n                            input_value = 'valueDict[@\"{}\"]'.format(field.name)\n\n                        if is_primitive_type(data_type):\n                            deserialize_call = input_value\n                        else:\n                            deserialize_call = self._fmt_serialization_call(\n                                data_type, input_value, False)\n\n                        if nullable:\n                            deserialize_call = '{} ? {} : nil'.format(\n                                input_value, deserialize_call)\n\n                        self.emit('{}{} = {};'.format(\n                            fmt_type(field.data_type),\n                            fmt_var(field.name), deserialize_call))\n                        deserialized_obj_args = [(fmt_var(field.name),\n                                                  fmt_var(field.name))]\n                    else:\n                        deserialized_obj_args = []\n\n                    args = fmt_func_args(deserialized_obj_args)\n                    callee = self._cstor_name_from_field(field)\n                    self.emit('return {};'.format(\n                        fmt_func_call(\n                            caller=fmt_alloc_call(union_name),\n                            callee=callee,\n                            args=args)))\n            with self.block('else'):\n                if not union.closed:\n                    callee = 'initWithOther'\n                    self.emit('return {};'.format(\n                        fmt_func_call(\n                            caller=fmt_alloc_call(union_name), callee=callee)))\n                else:\n                    reason = (\n                        '[NSString stringWithFormat:@\"Tag has an '\n                        'invalid value: \\\\\\\"%@\\\\\\\".\", valueDict[@\".tag\"]]')\n                    self._generate_throw_error('InvalidTag', reason)\n        self.emit()", "response": "Emits the deserialize method for the serialization object for the given union."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the appropriate serialization or deserialization method call for the given data type.", "response": "def _fmt_serialization_call(self, data_type, input_value, serialize, depth=0):\n        \"\"\"Returns the appropriate serialization / deserialization method\n        call for the given data type.\"\"\"\n        data_type, _ = unwrap_nullable(data_type)\n        serializer_func = 'serialize' if serialize else 'deserialize'\n        serializer_args = []\n\n        if is_primitive_type(data_type):\n            return input_value\n\n        if is_list_type(data_type) or is_map_type(data_type):\n            serializer_args.append(('value', input_value))\n            elem_data_type = (data_type.value_data_type if\n                is_map_type(data_type) else data_type.data_type)\n            serialization_call = self._fmt_serialization_call(\n                elem_data_type, 'elem{}'.format(depth), serialize, depth + 1)\n            data_struct_block = '^id(id elem{}) {{ return {}; }}'.format(\n                depth, serialization_call)\n            serializer_args.append(('withBlock', data_struct_block))\n        elif is_timestamp_type(data_type):\n            serializer_args.append(('value', input_value))\n            serializer_args.append(('dateFormat',\n                                    '@\"{}\"'.format(data_type.format)))\n        else:\n            serializer_args.append(('value', input_value))\n\n        return '{}'.format(\n            fmt_func_call(\n                caller=fmt_serial_obj(data_type),\n                callee=serializer_func,\n                args=fmt_func_args(serializer_args)))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nemitting implementation files for Route objects which encapsulate information about each route. These objects are passed as parameters when route calls are made.", "response": "def _generate_route_objects_m(self, route_schema, namespace):\n        \"\"\"Emits implementation files for Route objects which encapsulate information\n        regarding each route. These objects are passed as parameters when route calls are made.\"\"\"\n        output_path = 'Routes/RouteObjects/{}.m'.format(\n            fmt_route_obj_class(namespace.name))\n\n        with self.output_to_relative_path(output_path):\n            self.emit_raw(base_file_comment)\n\n            import_classes = [\n                fmt_route_obj_class(namespace.name),\n                'DBStoneBase',\n                'DBRequestErrors',\n            ]\n\n            for auth_type in self.namespace_to_has_route_auth_list[namespace]:\n                import_classes.append(\n                    fmt_routes_class(namespace.name, auth_type))\n\n            imports_classes_m = import_classes + \\\n                self._get_imports_m(\n                    self._get_namespace_route_imports(namespace, include_route_args=False), [])\n            self._generate_imports_m(imports_classes_m)\n\n            with self.block_m(fmt_route_obj_class(namespace.name)):\n                for route in namespace.routes:\n                    route_name = fmt_route_var(namespace.name, route)\n                    self.emit('static DBRoute *{};'.format(route_name))\n                self.emit()\n\n                for route in namespace.routes:\n                    route_name = fmt_route_var(namespace.name, route)\n                    if route.version == 1:\n                        route_path = route.name\n                    else:\n                        route_path = '{}_v{}'.format(route.name, route.version)\n\n                    if route.deprecated:\n                        deprecated = '@{}'.format('YES')\n                    else:\n                        deprecated = '@{}'.format('NO')\n\n                    if not is_void_type(route.result_data_type):\n                        caller = fmt_class_type(\n                            route.result_data_type, suppress_ptr=True)\n                        result_type = fmt_func_call(\n                            caller=caller, callee='class')\n                    else:\n                        result_type = 'nil'\n\n                    if not is_void_type(route.error_data_type):\n                        caller = fmt_class_type(\n                            route.error_data_type, suppress_ptr=True)\n                        error_type = fmt_func_call(\n                            caller=caller, callee='class')\n                    else:\n                        error_type = 'nil'\n\n                    if is_list_type(route.arg_data_type) or is_map_type(route.arg_data_type):\n                        dataStructSerialBlock = '^id(id dataStruct) {{ return {}; }}'.format(\n                            self._fmt_serialization_call(\n                                route.result_data_type, 'dataStruct', True))\n                    else:\n                        dataStructSerialBlock = 'nil'\n\n                    if is_list_type(route.result_data_type) or is_map_type(route.result_data_type):\n                        dataStructDeserialBlock = '^id(id dataStruct) {{ return {}; }}'.format(\n                            self._fmt_serialization_call(\n                                route.result_data_type, 'dataStruct', False))\n                    else:\n                        dataStructDeserialBlock = 'nil'\n\n                    with self.block_func(\n                            func=route_name,\n                            args=[],\n                            return_type='DBRoute *',\n                            class_func=True):\n                        with self.block('if (!{})'.format(route_name)):\n                            with self.block(\n                                    '{} = [[DBRoute alloc] init:'.format(\n                                        route_name),\n                                    delim=(None, None),\n                                    after='];'):\n                                self.emit('@\\\"{}\\\"'.format(route_path))\n                                self.emit('namespace_:@\\\"{}\\\"'.format(\n                                    namespace.name))\n                                self.emit('deprecated:{}'.format(deprecated))\n                                self.emit('resultType:{}'.format(result_type))\n                                self.emit('errorType:{}'.format(error_type))\n\n                                attrs = []\n                                for field in route_schema.fields:\n                                    attr_key = field.name\n                                    attr_val = (\"@\\\"{}\\\"\".format(route.attrs\n                                            .get(attr_key)) if route.attrs\n                                        .get(attr_key)\n                                        else 'nil')\n                                    attrs.append('@\\\"{}\\\": {}'.format(\n                                        attr_key, attr_val))\n\n                                self.generate_multiline_list(\n                                    attrs,\n                                    delim=('attrs:@{', '}'),\n                                    compact=True)\n\n                                self.emit('dataStructSerialBlock:{}'.format(\n                                    dataStructSerialBlock))\n                                self.emit('dataStructDeserialBlock:{}'.format(\n                                    dataStructDeserialBlock))\n\n                        self.emit('return {};'.format(route_name))\n                    self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nemit header files for Route objects which encapsulate information regarding each route.", "response": "def _generate_route_objects_h(\n            self,\n            route_schema,  # pylint: disable=unused-argument\n            namespace):\n        \"\"\"Emits header files for Route objects which encapsulate information\n         regarding each route. These objects are passed as parameters when route calls are made.\"\"\"\n        output_path = 'Routes/RouteObjects/{}.h'.format(\n            fmt_route_obj_class(namespace.name))\n        with self.output_to_relative_path(output_path):\n            self.emit_raw(base_file_comment)\n\n            self.emit('#import <Foundation/Foundation.h>')\n            self.emit()\n            self._generate_imports_h(['DBRoute'])\n\n            self.emit()\n            self.emit('NS_ASSUME_NONNULL_BEGIN')\n            self.emit()\n\n            self.emit(comment_prefix)\n            description_str = (\n                'Stone route objects for the {} namespace. Each route in '\n                'the {} namespace has its own static object, which contains '\n                'information about the route.')\n            self.emit_wrapped_text(\n                description_str.format(\n                    fmt_class(namespace.name), fmt_class(namespace.name)),\n                prefix=comment_prefix)\n            self.emit(comment_prefix)\n            with self.block_h(fmt_route_obj_class(namespace.name)):\n                for route in namespace.routes:\n                    route_name = fmt_route_var(namespace.name, route)\n\n                    route_obj_access_signature = fmt_signature(\n                        func=route_name,\n                        args=None,\n                        return_type='DBRoute *',\n                        class_func=True)\n                    base_str = 'Accessor method for the {} route object.'\n                    self.emit_wrapped_text(\n                        base_str.format(fmt_route_func(route)),\n                        prefix=comment_prefix)\n                    self.emit('{};'.format(route_obj_access_signature))\n                    self.emit()\n\n            self.emit()\n            self.emit('NS_ASSUME_NONNULL_END')\n            self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_union_tag_access_signatures(self, union):\n        for field in union.all_fields:\n            self.emit(comment_prefix)\n            base_str = 'Retrieves whether the union\\'s current tag state has value \"{}\".'\n            self.emit_wrapped_text(\n                base_str.format(field.name), prefix=comment_prefix)\n            self.emit(comment_prefix)\n            if not is_void_type(field.data_type):\n                warning_str = (\n                    '@note Call this method and ensure it returns true before '\n                    'accessing the `{}` property, otherwise a runtime exception '\n                    'will be thrown.')\n                self.emit_wrapped_text(\n                    warning_str.format(fmt_var(field.name)),\n                    prefix=comment_prefix)\n                self.emit(comment_prefix)\n            base_str = '@return Whether the union\\'s current tag state has value \"{}\".'\n            self.emit_wrapped_text(\n                base_str.format(field.name), prefix=comment_prefix)\n            self.emit(comment_prefix)\n\n            is_tag_signature = fmt_signature(\n                func='is{}'.format(fmt_camel_upper(field.name)),\n                args=[],\n                return_type='BOOL')\n            self.emit('{};'.format(is_tag_signature))\n            self.emit()\n\n        get_tag_name_signature = fmt_signature(\n            func='tagName', args=None, return_type='NSString *')\n\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            \"Retrieves string value of union's current tag state.\",\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n        base_str = \"@return A human-readable string representing the union's current tag state.\"\n        self.emit_wrapped_text(base_str, prefix=comment_prefix)\n        self.emit(comment_prefix)\n        self.emit('{};'.format(get_tag_name_signature))\n        self.emit()", "response": "Emits the is<TAG_NAME > methods and tagName method signatures for the union s current tag state and retrieving human - readable value of tag\n         state respectively."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nemits the is<TAG_NAME > methods and tagName method for determining the tag state of the tag state and retrieving human - readable value of tag state respectively.", "response": "def _generate_union_tag_state_funcs(self, union):\n        \"\"\"Emits the is<TAG_NAME> methods and tagName method for determining\n        tag state and retrieving human-readable value of tag state, respectively.\"\"\"\n        for field in union.all_fields:\n            enum_field_name = fmt_enum_name(field.name, union)\n\n            with self.block_func(\n                    func='is{}'.format(fmt_camel_upper(field.name)),\n                    args=[],\n                    return_type='BOOL'):\n                self.emit('return _tag == {};'.format(enum_field_name))\n            self.emit()\n\n        with self.block_func(\n                func='tagName', args=[], return_type='NSString *'):\n            with self.block('switch (_tag)'):\n                for field in union.all_fields:\n                    enum_field_name = fmt_enum_name(field.name, union)\n                    self.emit('case {}:'.format(enum_field_name))\n                    self.emit('   return @\"{}\";'.format(enum_field_name))\n            self.emit()\n            self._generate_throw_error('InvalidTag',\n                                       '@\"Tag has an unknown value.\"')\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_union_tag_vars_funcs(self, union):\n        for field in union.all_fields:\n            if not is_void_type(field.data_type):\n                enum_field_name = fmt_enum_name(field.name, union)\n\n                with self.block_func(\n                        func=fmt_camel(field.name),\n                        args=[],\n                        return_type=fmt_type(field.data_type)):\n\n                    with self.block(\n                            'if (![self is{}])'.format(\n                                fmt_camel_upper(field.name)),\n                            delim=('{', '}')):\n                        error_msg = 'Invalid tag: required {}, but was %@.'.format(\n                            enum_field_name)\n                        throw_exc = (\n                            '[NSException raise:@\"IllegalStateException\" '\n                            'format:@\"{}\", [self tagName]];')\n                        self.emit(throw_exc.format(error_msg))\n                    self.emit('return _{};'.format(fmt_var(field.name)))\n                self.emit()", "response": "Emits the getter methods for retrieving tag - specific state variables. Setters throw\nTracingEnabled an error in the event an associated tag state variable is accessed without the correct tag state variable is accessed without the correct tag state variable."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_struct_properties(self, fields):\n        for field in fields:\n            doc = self.process_doc(field.doc,\n                                   self._docf) if field.doc else undocumented\n            self.emit_wrapped_text(\n                self.process_doc(doc, self._docf), prefix=comment_prefix)\n            self.emit(fmt_property(field=field))\n            self.emit()", "response": "Emits struct instance properties from the given fields."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_union_properties(self, fields):\n        for field in fields:\n            # void types do not need properties to store additional state\n            # information\n            if not is_void_type(field.data_type):\n                doc = self.process_doc(\n                    field.doc, self._docf) if field.doc else undocumented\n                warning_str = (\n                    ' @note Ensure the `is{}` method returns true before accessing, '\n                    'otherwise a runtime exception will be raised.')\n                doc += warning_str.format(fmt_camel_upper(field.name))\n                self.emit_wrapped_text(\n                    self.process_doc(doc, self._docf), prefix=comment_prefix)\n                self.emit(fmt_property(field=field))\n                self.emit()", "response": "Emits union instance properties from the given fields."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nemit union instance property representing union state.", "response": "def _generate_union_tag_property(self, union):\n        \"\"\"Emits union instance property representing union state.\"\"\"\n        self.emit_wrapped_text(\n            'Represents the union\\'s current tag state.',\n            prefix=comment_prefix)\n        self.emit(\n            fmt_property_str(\n                prop='tag', typ='{}'.format(fmt_enum_name('tag', union))))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_class_comment(self, data_type):\n        if is_struct_type(data_type):\n            class_type = 'struct'\n        elif is_union_type(data_type):\n            class_type = 'union'\n        else:\n            raise TypeError('Can\\'t handle type %r' % type(data_type))\n\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            'The `{}` {}.'.format(fmt_class(data_type.name), class_type),\n            prefix=comment_prefix)\n\n        if data_type.doc:\n            self.emit(comment_prefix)\n            self.emit_wrapped_text(\n                self.process_doc(data_type.doc, self._docf),\n                prefix=comment_prefix)\n\n        self.emit(comment_prefix)\n        protocol_str = (\n            'This class implements the `DBSerializable` protocol '\n            '(serialize and deserialize instance methods), which is required '\n            'for all Obj-C SDK API route objects.')\n        self.emit_wrapped_text(\n            protocol_str.format(fmt_class_prefix(data_type), class_type),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)", "response": "Emits a generic class comment for a union or struct."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_throw_error(self, name, reason):\n        throw_exc = '@throw([NSException exceptionWithName:@\"{}\" reason:{} userInfo:nil]);'\n        self.emit(throw_exc.format(name, reason))", "response": "Emits a generic error throwing line."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_route_methods(self, namespaces):\n        self.cur_namespace = None\n        for namespace in namespaces:\n            if namespace.routes:\n                self.emit('# ------------------------------------------')\n                self.emit('# Routes in {} namespace'.format(namespace.name))\n                self.emit()\n                self._generate_routes(namespace)", "response": "Creates methods for the routes in each namespace. All data types\n            and routes are represented as Python classes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _generate_routes(self, namespace):\n\n        # Hack: needed for _docf()\n        self.cur_namespace = namespace\n        # list of auth_types supported in this base class.\n        # this is passed with the new -w flag\n        if self.args.auth_type is not None:\n            self.supported_auth_types = [auth_type.strip().lower() for auth_type in self.args.auth_type.split(',')]\n\n        check_route_name_conflict(namespace)\n\n        for route in namespace.routes:\n            # compatibility mode : included routes are passed by whitelist\n            # actual auth attr inluded in the route is ignored in this mode.\n            if self.supported_auth_types is None:\n                self._generate_route_helper(namespace, route)\n                if route.attrs.get('style') == 'download':\n                    self._generate_route_helper(namespace, route, True)\n            else:\n                route_auth_attr = None\n                if route.attrs is not None:\n                    route_auth_attr = route.attrs.get('auth')\n                if route_auth_attr is None:\n                    continue\n                route_auth_modes = [mode.strip().lower() for mode in route_auth_attr.split(',')]\n                for base_auth_type in self.supported_auth_types:\n                    if base_auth_type in route_auth_modes:\n                        self._generate_route_helper(namespace, route)\n                        if route.attrs.get('style') == 'download':\n                            self._generate_route_helper(namespace, route, True)\n                        break", "response": "Generates Python methods that correspond to routes in the namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates a route method that corresponds to a route.", "response": "def _generate_route_helper(self, namespace, route, download_to_file=False):\n        \"\"\"Generate a Python method that corresponds to a route.\n\n        :param namespace: Namespace that the route belongs to.\n        :param stone.ir.ApiRoute route: IR node for the route.\n        :param bool download_to_file: Whether a special version of the route\n            that downloads the response body to a file should be generated.\n            This can only be used for download-style routes.\n        \"\"\"\n        arg_data_type = route.arg_data_type\n        result_data_type = route.result_data_type\n\n        request_binary_body = route.attrs.get('style') == 'upload'\n        response_binary_body = route.attrs.get('style') == 'download'\n\n        if download_to_file:\n            assert response_binary_body, 'download_to_file can only be set ' \\\n                'for download-style routes.'\n            self._generate_route_method_decl(namespace,\n                                             route,\n                                             arg_data_type,\n                                             request_binary_body,\n                                             method_name_suffix='_to_file',\n                                             extra_args=['download_path'])\n        else:\n            self._generate_route_method_decl(namespace,\n                                             route,\n                                             arg_data_type,\n                                             request_binary_body)\n\n        with self.indent():\n            extra_request_args = None\n            extra_return_arg = None\n            footer = None\n            if request_binary_body:\n                extra_request_args = [('f',\n                                       'bytes',\n                                       'Contents to upload.')]\n            elif download_to_file:\n                extra_request_args = [('download_path',\n                                       'str',\n                                       'Path on local machine to save file.')]\n            if response_binary_body and not download_to_file:\n                extra_return_arg = ':class:`requests.models.Response`'\n                footer = DOCSTRING_CLOSE_RESPONSE\n\n            if route.doc:\n                func_docstring = self.process_doc(route.doc, self._docf)\n            else:\n                func_docstring = None\n\n            self._generate_docstring_for_func(\n                namespace,\n                arg_data_type,\n                result_data_type,\n                route.error_data_type,\n                overview=func_docstring,\n                extra_request_args=extra_request_args,\n                extra_return_arg=extra_return_arg,\n                footer=footer,\n            )\n\n            self._maybe_generate_deprecation_warning(route)\n\n            # Code to instantiate a class for the request data type\n            if is_void_type(arg_data_type):\n                self.emit('arg = None')\n            elif is_struct_type(arg_data_type):\n                self.generate_multiline_list(\n                    [f.name for f in arg_data_type.all_fields],\n                    before='arg = {}.{}'.format(\n                        fmt_namespace(arg_data_type.namespace.name),\n                        fmt_class(arg_data_type.name)),\n                )\n            elif not is_union_type(arg_data_type):\n                raise AssertionError('Unhandled request type %r' %\n                                     arg_data_type)\n\n            # Code to make the request\n            args = [\n                '{}.{}'.format(fmt_namespace(namespace.name),\n                               fmt_func(route.name, version=route.version)),\n                \"'{}'\".format(namespace.name),\n                'arg']\n            if request_binary_body:\n                args.append('f')\n            else:\n                args.append('None')\n            self.generate_multiline_list(args, 'r = self.request', compact=False)\n\n            if download_to_file:\n                self.emit('self._save_body_to_file(download_path, r[1])')\n                if is_void_type(result_data_type):\n                    self.emit('return None')\n                else:\n                    self.emit('return r[0]')\n            else:\n                if is_void_type(result_data_type):\n                    self.emit('return None')\n                else:\n                    self.emit('return r')\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_route_method_decl(\n            self, namespace, route, arg_data_type, request_binary_body,\n            method_name_suffix='', extra_args=None):\n        \"\"\"Generates the method prototype for a route.\"\"\"\n        args = ['self']\n        if extra_args:\n            args += extra_args\n        if request_binary_body:\n            args.append('f')\n        if is_struct_type(arg_data_type):\n            for field in arg_data_type.all_fields:\n                if is_nullable_type(field.data_type):\n                    args.append('{}=None'.format(field.name))\n                elif field.has_default:\n                    # TODO(kelkabany): Decide whether we really want to set the\n                    # default in the argument list. This will send the default\n                    # over the wire even if it isn't overridden. The benefit is\n                    # it locks in a default even if it is changed server-side.\n                    if is_user_defined_type(field.data_type):\n                        ns = field.data_type.namespace\n                    else:\n                        ns = None\n                    arg = '{}={}'.format(\n                        field.name,\n                        self._generate_python_value(ns, field.default))\n                    args.append(arg)\n                else:\n                    args.append(field.name)\n        elif is_union_type(arg_data_type):\n            args.append('arg')\n        elif not is_void_type(arg_data_type):\n            raise AssertionError('Unhandled request type: %r' %\n                                 arg_data_type)\n\n        method_name = fmt_func(route.name + method_name_suffix, version=route.version)\n        namespace_name = fmt_underscores(namespace.name)\n        self.generate_multiline_list(args, 'def {}_{}'.format(namespace_name, method_name), ':')", "response": "Generates the method declaration for a route."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a docstring for a function or method. This function is versatile. It will create a docstring using all the data that is provided. :param arg_data_type: The data type describing the argument to the route. The data type should be a struct, and each field will be treated as an input parameter of the method. :param result_data_type: The data type of the route result. :param error_data_type: The data type of the route result in the case of an error. :param str overview: A description of the route that will be located at the top of the docstring. :param extra_request_args: [(field name, field type, field doc), ...] Describes any additional parameters for the method that aren't a field in arg_data_type. :param str extra_return_arg: Name of an additional return type that. If this is specified, it is assumed that the return of the function will be a tuple of return_data_type and extra_return-arg. :param str footer: Additional notes at the end of the docstring.", "response": "def _generate_docstring_for_func(self, namespace, arg_data_type,\n                                     result_data_type=None, error_data_type=None,\n                                     overview=None, extra_request_args=None,\n                                     extra_return_arg=None, footer=None):\n        \"\"\"\n        Generates a docstring for a function or method.\n\n        This function is versatile. It will create a docstring using all the\n        data that is provided.\n\n        :param arg_data_type: The data type describing the argument to the\n            route. The data type should be a struct, and each field will be\n            treated as an input parameter of the method.\n        :param result_data_type: The data type of the route result.\n        :param error_data_type: The data type of the route result in the case\n            of an error.\n        :param str overview: A description of the route that will be located\n            at the top of the docstring.\n        :param extra_request_args: [(field name, field type, field doc), ...]\n            Describes any additional parameters for the method that aren't a\n            field in arg_data_type.\n        :param str extra_return_arg: Name of an additional return type that. If\n            this is specified, it is assumed that the return of the function\n            will be a tuple of return_data_type and extra_return-arg.\n        :param str footer: Additional notes at the end of the docstring.\n        \"\"\"\n        fields = [] if is_void_type(arg_data_type) else arg_data_type.fields\n        if not fields and not overview:\n            # If we don't have an overview or any input parameters, we skip the\n            # docstring altogether.\n            return\n\n        self.emit('\"\"\"')\n        if overview:\n            self.emit_wrapped_text(overview)\n\n        # Description of all input parameters\n        if extra_request_args or fields:\n            if overview:\n                # Add a blank line if we had an overview\n                self.emit()\n\n            if extra_request_args:\n                for name, data_type_name, doc in extra_request_args:\n                    if data_type_name:\n                        field_doc = ':param {} {}: {}'.format(data_type_name,\n                                                              name, doc)\n                        self.emit_wrapped_text(field_doc,\n                                               subsequent_prefix='    ')\n                    else:\n                        self.emit_wrapped_text(\n                            ':param {}: {}'.format(name, doc),\n                            subsequent_prefix='    ')\n\n            if is_struct_type(arg_data_type):\n                for field in fields:\n                    if field.doc:\n                        if is_user_defined_type(field.data_type):\n                            field_doc = ':param {}: {}'.format(\n                                field.name, self.process_doc(field.doc, self._docf))\n                        else:\n                            field_doc = ':param {} {}: {}'.format(\n                                self._format_type_in_doc(namespace, field.data_type),\n                                field.name,\n                                self.process_doc(field.doc, self._docf),\n                            )\n                        self.emit_wrapped_text(\n                            field_doc, subsequent_prefix='    ')\n                        if is_user_defined_type(field.data_type):\n                            # It's clearer to declare the type of a composite on\n                            # a separate line since it references a class in\n                            # another module\n                            self.emit(':type {}: {}'.format(\n                                field.name,\n                                self._format_type_in_doc(namespace, field.data_type),\n                            ))\n                    else:\n                        # If the field has no docstring, then just document its\n                        # type.\n                        field_doc = ':type {}: {}'.format(\n                            field.name,\n                            self._format_type_in_doc(namespace, field.data_type),\n                        )\n                        self.emit_wrapped_text(field_doc)\n\n            elif is_union_type(arg_data_type):\n                if arg_data_type.doc:\n                    self.emit_wrapped_text(':param arg: {}'.format(\n                        self.process_doc(arg_data_type.doc, self._docf)),\n                        subsequent_prefix='    ')\n                self.emit(':type arg: {}'.format(\n                    self._format_type_in_doc(namespace, arg_data_type)))\n\n        if overview and not (extra_request_args or fields):\n            # Only output an empty line if we had an overview and haven't\n            # started a section on declaring types.\n            self.emit()\n\n        if extra_return_arg:\n            # Special case where the function returns a tuple. The first\n            # element is the JSON response. The second element is the\n            # the extra_return_arg param.\n            args = []\n            if is_void_type(result_data_type):\n                args.append('None')\n            else:\n                rtype = self._format_type_in_doc(namespace,\n                                                 result_data_type)\n                args.append(rtype)\n            args.append(extra_return_arg)\n            self.generate_multiline_list(args, ':rtype: ')\n        else:\n            if is_void_type(result_data_type):\n                self.emit(':rtype: None')\n            else:\n                rtype = self._format_type_in_doc(namespace, result_data_type)\n                self.emit(':rtype: {}'.format(rtype))\n\n        if not is_void_type(error_data_type) and error_data_type.fields:\n            self.emit(':raises: :class:`{}`'.format(self.args.error_class_path))\n            self.emit()\n            # To provide more clarity to a dev who reads the docstring, suggest\n            # the route's error class. This is confusing, however, because we\n            # don't know where the error object that's raised will store\n            # the more detailed route error defined in stone.\n            error_class_name = self.args.error_class_path.rsplit('.', 1)[-1]\n            self.emit('If this raises, {} will contain:'.format(error_class_name))\n            with self.indent():\n                self.emit(self._format_type_in_doc(namespace, error_data_type))\n\n        if footer:\n            self.emit()\n            self.emit_wrapped_text(footer)\n        self.emit('\"\"\"')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _docf(self, tag, val):\n        if tag == 'type':\n            fq_val = val\n            if '.' not in val:\n                fq_val = self.cur_namespace.name + '.' + fq_val\n            return ':class:`{}.{}`'.format(self.args.types_package, fq_val)\n        elif tag == 'route':\n            if ':' in val:\n                val, version = val.split(':', 1)\n                version = int(version)\n            else:\n                version = 1\n            if '.' in val:\n                return ':meth:`{}`'.format(fmt_func(val, version=version))\n            else:\n                return ':meth:`{}_{}`'.format(\n                    self.cur_namespace.name, fmt_func(val, version=version))\n        elif tag == 'link':\n            anchor, link = val.rsplit(' ', 1)\n            return '`{} <{}>`_'.format(anchor, link)\n        elif tag == 'val':\n            if val == 'null':\n                return 'None'\n            elif val == 'true' or val == 'false':\n                return '``{}``'.format(val.capitalize())\n            else:\n                return val\n        elif tag == 'field':\n            return '``{}``'.format(val)\n        else:\n            raise RuntimeError('Unknown doc ref tag %r' % tag)", "response": "This function converts Babel doc references to Sphinx - friendly annotations."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nformats the type in the documentation.", "response": "def _format_type_in_doc(self, namespace, data_type):\n        \"\"\"\n        Returns a string that can be recognized by Sphinx as a type reference\n        in a docstring.\n        \"\"\"\n        if is_void_type(data_type):\n            return 'None'\n        elif is_user_defined_type(data_type):\n            return ':class:`{}.{}.{}`'.format(\n                self.args.types_package, namespace.name, fmt_type(data_type))\n        else:\n            return fmt_type(data_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_routes_m(self, namespace):\n        with self.block_m(\n                fmt_routes_class(namespace.name, self.args.auth_type)):\n            init_args = fmt_func_args_declaration([(\n                'client', 'id<{}>'.format(self.args.transport_client_name))])\n\n            with self.block_func(\n                    func='init', args=init_args, return_type='instancetype'):\n                self.emit('self = [super init];')\n                with self.block_init():\n                    self.emit('_client = client;')\n            self.emit()\n            style_to_request = json.loads(self.args.z__style_to_request)\n\n            for route in namespace.routes:\n                if (route.attrs.get('auth') != self.args.auth_type\n                        and route.attrs.get('auth') != 'noauth'):\n                    continue\n\n                route_type = route.attrs.get('style')\n                client_args = json.loads(self.args.client_args)\n\n                if route_type in client_args.keys():\n                    for args_data in client_args[route_type]:\n                        task_type_key, type_data_dict = tuple(args_data)\n                        task_type_name = style_to_request[task_type_key]\n\n                        func_suffix = type_data_dict[0]\n                        extra_args = [\n                            tuple(type_data[:-1])\n                            for type_data in type_data_dict[1]\n                        ]\n\n                        if (is_struct_type(route.arg_data_type) and\n                                self._struct_has_defaults(route.arg_data_type)):\n                            route_args, _ = self._get_default_route_args(\n                                namespace, route)\n                            self._generate_route_m(route, namespace,\n                                                   route_args, extra_args,\n                                                   task_type_name, func_suffix)\n\n                        route_args, _ = self._get_route_args(namespace, route)\n                        self._generate_route_m(route, namespace, route_args,\n                                               extra_args, task_type_name,\n                                               func_suffix)\n                else:\n                    task_type_name = style_to_request[route_type]\n                    if (is_struct_type(route.arg_data_type) and\n                            self._struct_has_defaults(route.arg_data_type)):\n                        route_args, _ = self._get_default_route_args(\n                            namespace, route)\n                        self._generate_route_m(route, namespace, route_args,\n                                               [], task_type_name, '')\n\n                    route_args, _ = self._get_route_args(namespace, route)\n                    self._generate_route_m(route, namespace, route_args, [],\n                                           task_type_name, '')", "response": "Generates implementation file for all routes within the namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates route method implementation for the given route.", "response": "def _generate_route_m(self, route, namespace, route_args, extra_args,\n                          task_type_name, func_suffix):\n        \"\"\"Generates route method implementation for the given route.\"\"\"\n        user_args = list(route_args)\n\n        transport_args = [\n            ('route', 'route'),\n            ('arg', 'arg' if not is_void_type(route.arg_data_type) else 'nil'),\n        ]\n\n        for name, value, typ in extra_args:\n            user_args.append((name, typ))\n            transport_args.append((name, value))\n\n        with self.block_func(\n                func='{}{}'.format(fmt_route_func(route), func_suffix),\n                args=fmt_func_args_declaration(user_args),\n                return_type='{} *'.format(task_type_name)):\n            self.emit('DBRoute *route = {}.{};'.format(\n                fmt_route_obj_class(namespace.name),\n                fmt_route_var(namespace.name, route)))\n            if is_union_type(route.arg_data_type):\n                self.emit('{} *arg = {};'.format(\n                    fmt_class_prefix(route.arg_data_type),\n                    fmt_var(route.arg_data_type.name)))\n            elif not is_void_type(route.arg_data_type):\n                init_call = fmt_func_call(\n                    caller=fmt_alloc_call(\n                        caller=fmt_class_prefix(route.arg_data_type)),\n                    callee=self._cstor_name_from_fields_names(route_args),\n                    args=fmt_func_args([(f[0], f[0]) for f in route_args]))\n                self.emit('{} *arg = {};'.format(\n                    fmt_class_prefix(route.arg_data_type), init_call))\n            request_call = fmt_func_call(\n                caller='self.client',\n                callee='request{}'.format(\n                    fmt_camel_upper(route.attrs.get('style'))),\n                args=fmt_func_args(transport_args))\n            self.emit('return {};'.format(request_call))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_routes_h(self, namespace):\n        self.emit(comment_prefix)\n        self.emit_wrapped_text(\n            'Routes for the `{}` namespace'.format(fmt_class(namespace.name)),\n            prefix=comment_prefix)\n        self.emit(comment_prefix)\n\n        self.emit()\n        self.emit('NS_ASSUME_NONNULL_BEGIN')\n        self.emit()\n\n        with self.block_h(\n                fmt_routes_class(namespace.name, self.args.auth_type)):\n            description_str = (\n                'An instance of the networking client that each '\n                'route will use to submit a request.')\n            self.emit_wrapped_text(description_str, prefix=comment_prefix)\n            self.emit(\n                fmt_property_str(\n                    prop='client',\n                    typ='id<{}>'.format(\n                        self.args.transport_client_name)))\n            self.emit()\n\n            routes_obj_args = fmt_func_args_declaration(\n                [('client',\n                  'id<{}>'.format(self.args.transport_client_name))])\n\n            init_signature = fmt_signature(\n                func='init',\n                args=routes_obj_args,\n                return_type='instancetype')\n            description_str = (\n                'Initializes the `{}` namespace container object '\n                'with a networking client.')\n            self.emit_wrapped_text(\n                description_str.format(\n                    fmt_routes_class(namespace.name, self.args.auth_type)),\n                prefix=comment_prefix)\n            self.emit('{};'.format(init_signature))\n            self.emit()\n\n            style_to_request = json.loads(self.args.z__style_to_request)\n\n            for route in namespace.routes:\n                if (route.attrs.get('auth') != self.args.auth_type\n                        and route.attrs.get('auth') != 'noauth'):\n                    continue\n\n                route_type = route.attrs.get('style')\n                client_args = json.loads(self.args.client_args)\n\n                if route_type in client_args.keys():\n                    for args_data in client_args[route_type]:\n                        task_type_key, type_data_dict = tuple(args_data)\n                        task_type_name = style_to_request[task_type_key]\n\n                        func_suffix = type_data_dict[0]\n                        extra_args = [\n                            tuple(type_data[:-1])\n                            for type_data in type_data_dict[1]\n                        ]\n                        extra_docs = [(type_data[0], type_data[-1])\n                                      for type_data in type_data_dict[1]]\n\n                        if (is_struct_type(route.arg_data_type) and\n                                self._struct_has_defaults(route.arg_data_type)):\n                            route_args, doc_list = self._get_default_route_args(\n                                namespace, route, tag=True)\n                            self._generate_route_signature(\n                                route, namespace, route_args, extra_args,\n                                doc_list + extra_docs, task_type_name,\n                                func_suffix)\n\n                        route_args, doc_list = self._get_route_args(\n                            namespace, route, tag=True)\n                        self._generate_route_signature(\n                            route, namespace, route_args, extra_args,\n                            doc_list + extra_docs, task_type_name, func_suffix)\n                else:\n                    task_type_name = style_to_request[route_type]\n                    if (is_struct_type(route.arg_data_type) and\n                            self._struct_has_defaults(route.arg_data_type)):\n                        route_args, doc_list = self._get_default_route_args(\n                            namespace, route, tag=True)\n                        self._generate_route_signature(\n                            route, namespace, route_args, [], doc_list,\n                            task_type_name, '')\n\n                    route_args, doc_list = self._get_route_args(\n                        namespace, route, tag=True)\n                    self._generate_route_signature(route, namespace,\n                                                   route_args, [], doc_list,\n                                                   task_type_name, '')\n        self.emit()\n        self.emit('NS_ASSUME_NONNULL_END')\n        self.emit()", "response": "Generates the header file for all routes within the namespace."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngenerate the route method signature for the given route.", "response": "def _generate_route_signature(\n            self,\n            route,\n            namespace,  # pylint: disable=unused-argument\n            route_args,\n            extra_args,\n            doc_list,\n            task_type_name,\n            func_suffix):\n        \"\"\"Generates route method signature for the given route.\"\"\"\n        for name, _, typ in extra_args:\n            route_args.append((name, typ))\n\n        deprecated = 'DEPRECATED: ' if route.deprecated else ''\n\n        func_name = '{}{}'.format(fmt_route_func(route), func_suffix)\n\n        self.emit(comment_prefix)\n        if route.doc:\n            route_doc = self.process_doc(route.doc, self._docf)\n        else:\n            route_doc = 'The {} route'.format(func_name)\n        self.emit_wrapped_text(\n            deprecated + route_doc, prefix=comment_prefix, width=120)\n        self.emit(comment_prefix)\n\n        for name, doc in doc_list:\n            self.emit_wrapped_text(\n                '@param {} {}'.format(name, doc if doc else undocumented),\n                prefix=comment_prefix,\n                width=120)\n        self.emit(comment_prefix)\n        output = (\n            '@return Through the response callback, the caller will ' +\n            'receive a `{}` object on success or a `{}` object on failure.')\n        output = output.format(\n            fmt_type(route.result_data_type, tag=False, no_ptr=True),\n            fmt_type(route.error_data_type, tag=False, no_ptr=True))\n        self.emit_wrapped_text(output, prefix=comment_prefix, width=120)\n        self.emit(comment_prefix)\n\n        result_type_str = fmt_type(route.result_data_type) if not is_void_type(\n            route.result_data_type) else 'DBNilObject *'\n        error_type_str = fmt_type(route.error_data_type) if not is_void_type(\n            route.error_data_type) else 'DBNilObject *'\n\n        return_type = '{}<{}, {}> *'.format(task_type_name, result_type_str,\n                                            error_type_str)\n\n        deprecated = self._get_deprecation_warning(route)\n        route_signature = fmt_signature(\n            func=func_name,\n            args=fmt_func_args_declaration(route_args),\n            return_type='{}'.format(return_type))\n        self.emit('{}{};'.format(route_signature, deprecated))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a deprecation tag / message if route is deprecated.", "response": "def _get_deprecation_warning(self, route):\n        \"\"\"Returns a deprecation tag / message, if route is deprecated.\"\"\"\n        result = ''\n        if route.deprecated:\n            msg = '{} is deprecated.'.format(fmt_route_func(route))\n            if route.deprecated.by:\n                msg += ' Use {}.'.format(fmt_var(route.deprecated.by.name))\n            result = ' __deprecated_msg(\"{}\")'.format(msg)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of name / value string pairs representing the arguments for a particular route.", "response": "def _get_route_args(self, namespace, route, tag=False):  # pylint: disable=unused-argument\n        \"\"\"Returns a list of name / value string pairs representing the arguments for\n        a particular route.\"\"\"\n        data_type, _ = unwrap_nullable(route.arg_data_type)\n        if is_struct_type(data_type):\n            arg_list = []\n            for field in data_type.all_fields:\n                arg_list.append((fmt_var(field.name), fmt_type(\n                    field.data_type, tag=tag, has_default=field.has_default)))\n\n            doc_list = [(fmt_var(f.name), self.process_doc(f.doc, self._docf))\n                        for f in data_type.fields if f.doc]\n        elif is_union_type(data_type):\n            arg_list = [(fmt_var(data_type.name), fmt_type(\n                route.arg_data_type, tag=tag))]\n\n            doc_list = [(fmt_var(data_type.name),\n                self.process_doc(data_type.doc,\n                    self._docf) if data_type.doc\n                else 'The {} union'.format(\n                    fmt_class(data_type\n                        .name)))]\n        else:\n            arg_list = []\n            doc_list = []\n\n        return arg_list, doc_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of name / value string pairs representing the default arguments for the given route.", "response": "def _get_default_route_args(\n            self,\n            namespace,  # pylint: disable=unused-argument\n            route,\n            tag=False):\n        \"\"\"Returns a list of name / value string pairs representing the default arguments for\n        a particular route.\"\"\"\n        data_type, _ = unwrap_nullable(route.arg_data_type)\n        if is_struct_type(data_type):\n            arg_list = []\n            for field in data_type.all_fields:\n                if not field.has_default and not is_nullable_type(\n                        field.data_type):\n                    arg_list.append((fmt_var(field.name), fmt_type(\n                        field.data_type, tag=tag)))\n\n            doc_list = ([(fmt_var(f.name), self.process_doc(f.doc, self._docf))\n                         for f in data_type.fields\n                         if f.doc and not f.has_default and\n                         not is_nullable_type(f.data_type)])\n        else:\n            arg_list = []\n            doc_list = []\n\n        return arg_list, doc_list"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_parser(self):\n        self.path = None\n        self.anony_defs = []\n        self.exhausted = False\n        return self", "response": "Returns a ParserFactory with the state reset so it can be used to parse again."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the given data and returns a list of all the anony - defs.", "response": "def parse(self, data, path=None):\n        \"\"\"\n        Args:\n            data (str): Raw specification text.\n            path (Optional[str]): Path to specification on filesystem. Only\n                used to tag tokens with the file they originated from.\n        \"\"\"\n        assert not self.exhausted, 'Must call get_parser() to reset state.'\n        self.path = path\n        parsed_data = self.yacc.parse(data, lexer=self.lexer, debug=self.debug)\n        # It generally makes sense for lexer errors to come first, because\n        # those can be the root of parser errors. Also, since we only show one\n        # error max right now, it's best to show the lexing one.\n        for err_msg, lineno in self.lexer.errors[::-1]:\n            self.errors.insert(0, (err_msg, lineno, self.path))\n        parsed_data.extend(self.anony_defs)\n        self.exhausted = True\n        return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nparsing a namespace keyword.", "response": "def p_namespace(self, p):\n        \"\"\"namespace : KEYWORD ID NL\n                     | KEYWORD ID NL INDENT docsection DEDENT\"\"\"\n        if p[1] == 'namespace':\n            doc = None\n            if len(p) > 4:\n                doc = p[5]\n            p[0] = AstNamespace(\n                self.path, p.lineno(1), p.lexpos(1), p[2], doc)\n        else:\n            raise ValueError('Expected namespace keyword')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nimporting : IMPORT ID NL", "response": "def p_import(self, p):\n        'import : IMPORT ID NL'\n        p[0] = AstImport(self.path, p.lineno(1), p.lexpos(1), p[2])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef p_alias(self, p):\n        if p[1] == 'alias':\n            has_annotations = len(p) > 6 and p[7] is not None\n            doc = p[8] if len(p) > 6 else None\n            p[0] = AstAlias(\n                self.path, p.lineno(1), p.lexpos(1), p[2], p[4], doc)\n            if has_annotations:\n                p[0].set_annotations(p[7])\n        else:\n            raise ValueError('Expected alias keyword')", "response": "Parse an alias keyword."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the keyword arguments.", "response": "def p_kw_args_update(self, p):\n        \"\"\"kw_args : kw_args COMMA kw_arg\"\"\"\n        p[0] = p[1]\n        for key in p[3]:\n            if key in p[1]:\n                msg = \"Keyword argument '%s' defined more than once.\" % key\n                self.errors.append((msg, p.lineno(2), self.path))\n        p[0].update(p[3])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_type_ref(self, p):\n        'type_ref : ID args nullable'\n        p[0] = AstTypeRef(\n            path=self.path,\n            lineno=p.lineno(1),\n            lexpos=p.lexpos(1),\n            name=p[1],\n            args=p[2],\n            nullable=p[3],\n            ns=None,\n        )", "response": "type_ref : ID args nullable"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_annotation_type(self, p):\n        p[0] = AstAnnotationTypeDef(\n            path=self.path,\n            lineno=p.lineno(1),\n            lexpos=p.lexpos(1),\n            name=p[2],\n            doc=p[5],\n            params=p[6])", "response": "parse an ANNOTATION_TYPE section"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_struct_patch(self, p):\n        p[0] = AstStructPatch(\n            path=self.path,\n            lineno=p.lineno(1),\n            lexpos=p.lexpos(1),\n            name=p[3],\n            fields=p[6],\n            examples=p[7])", "response": "parse an ast. STRUCT_PATCH"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_enumerated_subtype_field(self, p):\n        'subtype_field : ID type_ref NL'\n        p[0] = AstSubtypeField(\n            self.path, p.lineno(1), p.lexpos(1), p[1], p[2])", "response": "subtype_field : ID type_ref NL"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_default_option(self, p):\n        if p[1]:\n            if isinstance(p[2], AstTagRef):\n                p[0] = p[2]\n            else:\n                p[0] = p[2]", "response": "Default option for base class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfield : ID type_ref default_option NL \\ INDENT annotation_ref_list docsection anony_def_option DEDENT | ID type_ref default_option NL", "response": "def p_field(self, p):\n        \"\"\"field : ID type_ref default_option NL \\\n                    INDENT annotation_ref_list docsection anony_def_option DEDENT\n                 | ID type_ref default_option NL\"\"\"\n        has_annotations = len(p) > 5 and p[6] is not None\n        has_docstring = len(p) > 5 and p[7] is not None\n        has_anony_def = len(p) > 5 and p[8] is not None\n        p[0] = AstField(\n            self.path, p.lineno(1), p.lexpos(1), p[1], p[2])\n        if p[3] is not None:\n            if p[3] is NullToken:\n                p[0].set_default(None)\n            else:\n                p[0].set_default(p[3])\n        if has_annotations:\n            p[0].set_annotations(p[6])\n        if has_docstring:\n            p[0].set_doc(p[7])\n        if has_anony_def:\n            p[8].name = p[2].name\n            self.anony_defs.append(p[8])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_annotation_ref(self, p):\n        if len(p) < 5:\n            p[0] = AstAnnotationRef(self.path, p.lineno(1), p.lexpos(1), p[2], None)\n        else:\n            p[0] = AstAnnotationRef(self.path, p.lineno(1), p.lexpos(1), p[4], p[2])", "response": "A marker for annotation_ref."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_field_void(self, p):\n        p[0] = AstVoidField(self.path, p.lineno(1), p.lexpos(1), p[1])\n        if len(p) > 3:\n            if p[4] is not None:\n                p[0].set_annotations(p[4])\n\n            if p[5] is not None:\n                p[0].set_doc(p[5])", "response": "parse an void field"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the ROUTE statement.", "response": "def p_route(self, p):\n        \"\"\"route : ROUTE route_name route_version route_io route_deprecation NL \\\n                        INDENT docsection attrssection DEDENT\n                 | ROUTE route_name route_version route_io route_deprecation NL\"\"\"\n        p[0] = AstRouteDef(self.path, p.lineno(1), p.lexpos(1), p[2], p[3], p[5], *p[4])\n        if len(p) > 7:\n            p[0].set_doc(p[8])\n            if p[9]:\n                keys = set()\n                for attr in p[9]:\n                    if attr.name in keys:\n                        msg = \"Attribute '%s' defined more than once.\" % attr.name\n                        self.errors.append((msg, attr.lineno, attr.path))\n                    keys.add(attr.name)\n                p[0].set_attrs(p[9])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_attr_field(self, p):\n        if p[3] is NullToken:\n            p[0] = AstAttrField(\n                self.path, p.lineno(1), p.lexpos(1), p[1], None)\n        else:\n            p[0] = AstAttrField(\n                self.path, p.lineno(1), p.lexpos(1), p[1], p[3])", "response": "A tag attribute is a primitive NL\n                      | tag_ref NL"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_examples_add(self, p):\n        'examples : examples example'\n        p[0] = p[1]\n        if p[2].label in p[0]:\n            existing_ex = p[0][p[2].label]\n            self.errors.append(\n                (\"Example with label '%s' already defined on line %d.\" %\n                 (existing_ex.label, existing_ex.lineno),\n                 p[2].lineno, p[2].path))\n        p[0][p[2].label] = p[2]", "response": "examples : examples example"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_example_field_ref(self, p):\n        'example_field : ID EQ ID NL'\n        p[0] = AstExampleField(self.path, p.lineno(1), p.lexpos(1),\n            p[1], AstExampleRef(self.path, p.lineno(3), p.lexpos(3), p[3]))", "response": "example_field : ID EQ ID NL"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse an EX_MAP_ELEMENT element", "response": "def p_ex_map_elem_id(self, p):\n        \"\"\"ex_map_elem : ID\"\"\"\n        p[0] = AstExampleRef(self.path, p.lineno(1), p.lexpos(1), p[1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse an EX_MAP_PAIR expression.", "response": "def p_ex_map_pair(self, p):\n        \"\"\"ex_map_pair : ex_map_elem COLON ex_map_elem\"\"\"\n        try:\n            p[0] = {p[1]: p[3]}\n        except TypeError:\n            msg = u\"%s is an invalid hash key because it cannot be hashed.\" % repr(p[1])\n            self.errors.append((msg, p.lineno(2), self.path))\n            p[0] = {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef generate_validator_constructor(ns, data_type):\n    dt, nullable_dt = unwrap_nullable(data_type)\n    if is_list_type(dt):\n        v = generate_func_call(\n            'bv.List',\n            args=[\n                generate_validator_constructor(ns, dt.data_type)],\n            kwargs=[\n                ('min_items', dt.min_items),\n                ('max_items', dt.max_items)],\n        )\n    elif is_map_type(dt):\n        v = generate_func_call(\n            'bv.Map',\n            args=[\n                generate_validator_constructor(ns, dt.key_data_type),\n                generate_validator_constructor(ns, dt.value_data_type),\n            ]\n        )\n    elif is_numeric_type(dt):\n        v = generate_func_call(\n            'bv.{}'.format(dt.name),\n            kwargs=[\n                ('min_value', dt.min_value),\n                ('max_value', dt.max_value)],\n        )\n    elif is_string_type(dt):\n        pattern = None\n        if dt.pattern is not None:\n            pattern = repr(dt.pattern)\n        v = generate_func_call(\n            'bv.String',\n            kwargs=[\n                ('min_length', dt.min_length),\n                ('max_length', dt.max_length),\n                ('pattern', pattern)],\n        )\n    elif is_timestamp_type(dt):\n        v = generate_func_call(\n            'bv.Timestamp',\n            args=[repr(dt.format)],\n        )\n    elif is_user_defined_type(dt):\n        v = fmt_class(dt.name) + '_validator'\n        if ns.name != dt.namespace.name:\n            v = '{}.{}'.format(fmt_namespace(dt.namespace.name), v)\n    elif is_alias(dt):\n        # Assume that the alias has already been declared elsewhere.\n        name = fmt_class(dt.name) + '_validator'\n        if ns.name != dt.namespace.name:\n            name = '{}.{}'.format(fmt_namespace(dt.namespace.name), name)\n        v = name\n    elif is_boolean_type(dt) or is_bytes_type(dt) or is_void_type(dt):\n        v = generate_func_call('bv.{}'.format(dt.name))\n    else:\n        raise AssertionError('Unsupported data type: %r' % dt)\n\n    if nullable_dt:\n        return generate_func_call('bv.Nullable', args=[v])\n    else:\n        return v", "response": "Generates a string that can be used to construct a new validation object in Python."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate code to call a function.", "response": "def generate_func_call(name, args=None, kwargs=None):\n    \"\"\"\n    Generates code to call a function.\n\n    Args:\n        name (str): The function name.\n        args (list[str]): Each positional argument.\n        kwargs (list[tuple]): Each tuple is (arg: str, value: str). If\n            value is None, then the keyword argument is omitted. Otherwise,\n            if the value is not a string, then str() is called on it.\n\n    Returns:\n        str: Code to call a function.\n    \"\"\"\n    all_args = []\n    if args:\n        all_args.extend(args)\n    if kwargs:\n        all_args.extend('{}={}'.format(k, v)\n                        for k, v in kwargs if v is not None)\n    return '{}({})'.format(name, ', '.join(all_args))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate(self, api):\n        rsrc_folder = os.path.join(os.path.dirname(__file__), 'python_rsrc')\n        self.logger.info('Copying stone_validators.py to output folder')\n        shutil.copy(os.path.join(rsrc_folder, 'stone_validators.py'),\n                    self.target_folder_path)\n        self.logger.info('Copying stone_serializers.py to output folder')\n        shutil.copy(os.path.join(rsrc_folder, 'stone_serializers.py'),\n                    self.target_folder_path)\n        self.logger.info('Copying stone_base.py to output folder')\n        shutil.copy(os.path.join(rsrc_folder, 'stone_base.py'),\n                    self.target_folder_path)\n        for namespace in api.namespaces.values():\n            reserved_namespace_name = fmt_namespace(namespace.name)\n            with self.output_to_relative_path('{}.py'.format(reserved_namespace_name)):\n                self._generate_base_namespace_module(api, namespace)\n            if reserved_namespace_name != namespace.name:\n                with self.output_to_relative_path('{}.py'.format(namespace.name)):\n                    self._generate_dummy_namespace_module(reserved_namespace_name)", "response": "Generates a module for each namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_base_namespace_module(self, api, namespace):\n\n        self.cur_namespace = namespace\n        generate_module_header(self)\n\n        if namespace.doc is not None:\n            self.emit('\"\"\"')\n            self.emit_raw(namespace.doc)\n            self.emit('\"\"\"')\n            self.emit()\n\n        self.emit_raw(validators_import)\n\n        # Generate import statements for all referenced namespaces.\n        self._generate_imports_for_referenced_namespaces(namespace)\n\n        for annotation_type in namespace.annotation_types:\n            self._generate_annotation_type_class(namespace, annotation_type)\n\n        for data_type in namespace.linearize_data_types():\n            if isinstance(data_type, Struct):\n                self._generate_struct_class(namespace, data_type)\n            elif isinstance(data_type, Union):\n                self._generate_union_class(namespace, data_type)\n            else:\n                raise TypeError('Cannot handle type %r' % type(data_type))\n\n        for alias in namespace.linearize_aliases():\n            self._generate_alias_definition(namespace, alias)\n\n        # Generate the struct->subtype tag mapping at the end so that\n        # references to later-defined subtypes don't cause errors.\n        for data_type in namespace.linearize_data_types():\n            if is_struct_type(data_type):\n                self._generate_struct_class_reflection_attributes(\n                    namespace, data_type)\n                if data_type.has_enumerated_subtypes():\n                    self._generate_enumerated_subtypes_tag_mapping(\n                        namespace, data_type)\n            elif is_union_type(data_type):\n                self._generate_union_class_reflection_attributes(\n                    namespace, data_type)\n                self._generate_union_class_symbol_creators(data_type)\n\n        self._generate_routes(api.route_schema, namespace)", "response": "Generates a base namespace module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_annotation_type_class(self, ns, annotation_type):\n        # type: (ApiNamespace, AnnotationType) -> None\n        \"\"\"Defines a Python class that represents an annotation type in Stone.\"\"\"\n        self.emit('class {}(bb.AnnotationType):'.format(\n            class_name_for_annotation_type(annotation_type, ns)))\n        with self.indent():\n            if annotation_type.has_documented_type_or_params():\n                self.emit('\"\"\"')\n                if annotation_type.doc:\n                    self.emit_wrapped_text(\n                        self.process_doc(annotation_type.doc, self._docf))\n                    if annotation_type.has_documented_params():\n                        self.emit()\n                for param in annotation_type.params:\n                    if not param.doc:\n                        continue\n                    self.emit_wrapped_text(':ivar {}: {}'.format(\n                        fmt_var(param.name, True),\n                        self.process_doc(param.doc, self._docf)),\n                        subsequent_prefix='    ')\n                self.emit('\"\"\"')\n            self.emit()\n\n            self._generate_annotation_type_class_slots(annotation_type)\n            self._generate_annotation_type_class_init(ns, annotation_type)\n            self._generate_annotation_type_class_properties(ns, annotation_type)\n            self.emit()", "response": "Defines a Python class that represents an annotation type in Stone."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndefine a Python class that represents a struct in Stone.", "response": "def _generate_struct_class(self, ns, data_type):\n        # type: (ApiNamespace, Struct) -> None\n        \"\"\"Defines a Python class that represents a struct in Stone.\"\"\"\n        self.emit(self._class_declaration_for_type(ns, data_type))\n        with self.indent():\n            if data_type.has_documented_type_or_fields():\n                self.emit('\"\"\"')\n                if data_type.doc:\n                    self.emit_wrapped_text(\n                        self.process_doc(data_type.doc, self._docf))\n                    if data_type.has_documented_fields():\n                        self.emit()\n                for field in data_type.fields:\n                    if not field.doc:\n                        continue\n                    self.emit_wrapped_text(':ivar {}: {}'.format(\n                        fmt_namespaced_var(ns.name, data_type.name, field.name),\n                        self.process_doc(field.doc, self._docf)),\n                        subsequent_prefix='    ')\n                self.emit('\"\"\"')\n            self.emit()\n\n            self._generate_struct_class_slots(data_type)\n            self._generate_struct_class_has_required_fields(data_type)\n            self._generate_struct_class_init(data_type)\n            self._generate_struct_class_properties(ns, data_type)\n            self._generate_struct_class_custom_annotations(ns, data_type)\n            self._generate_struct_class_repr(data_type)\n        if data_type.has_enumerated_subtypes():\n            validator = 'StructTree'\n        else:\n            validator = 'Struct'\n        self.emit('{0}_validator = bv.{1}({0})'.format(\n            class_name_for_data_type(data_type),\n            validator,\n        ))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _func_args_from_dict(self, d):\n        filtered_d = self.filter_out_none_valued_keys(d)\n        return ', '.join(['%s=%s' % (k, v) for k, v in filtered_d.items()])", "response": "Given a Python dictionary creates a string representing arguments\n        for invoking a function. All arguments with a value of None are ignored."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a slots declaration for struct classes.", "response": "def _generate_struct_class_slots(self, data_type):\n        \"\"\"Creates a slots declaration for struct classes.\n\n        Slots are an optimization in Python. They reduce the memory footprint\n        of instances since attributes cannot be added after declaration.\n        \"\"\"\n        with self.block('__slots__ =', delim=('[', ']')):\n            for field in data_type.fields:\n                field_name = fmt_var(field.name)\n                self.emit(\"'_%s_value',\" % field_name)\n                self.emit(\"'_%s_present',\" % field_name)\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_struct_class_reflection_attributes(self, ns, data_type):\n\n        class_name = class_name_for_data_type(data_type)\n        if data_type.parent_type:\n            parent_type_class_name = class_name_for_data_type(\n                data_type.parent_type, ns)\n        else:\n            parent_type_class_name = None\n\n        for field in data_type.fields:\n            field_name = fmt_var(field.name)\n            validator_name = generate_validator_constructor(ns, field.data_type)\n            full_validator_name = '{}._{}_validator'.format(class_name, field_name)\n            self.emit('{} = {}'.format(full_validator_name, validator_name))\n            if field.redactor:\n                self._generate_redactor(full_validator_name, field.redactor)\n\n        # Generate `_all_field_names_` and `_all_fields_` for every omitted caller (and public).\n        # As an edge case, we union omitted callers with None in the case when the object has no\n        # public fields, as we still need to generate public attributes (`_field_names_` etc)\n        child_omitted_callers = data_type.get_all_omitted_callers() | {None}\n        parent_omitted_callers = data_type.parent_type.get_all_omitted_callers() if \\\n            data_type.parent_type else set([])\n\n        for omitted_caller in sorted(child_omitted_callers | parent_omitted_callers, key=str):\n            is_public = omitted_caller is None\n            map_name_prefix = '' if is_public else '_{}'.format(omitted_caller)\n            caller_in_parent = data_type.parent_type and (is_public or omitted_caller\n                                                         in parent_omitted_callers)\n\n            # generate `_all_field_names_`\n            names_map_name = '{}_field_names_'.format(map_name_prefix)\n            all_names_map_name = '_all{}_field_names_'.format(map_name_prefix)\n            if data_type.is_member_of_enumerated_subtypes_tree():\n                if is_public or omitted_caller in child_omitted_callers:\n                    self.generate_multiline_list(\n                        [\n                            \"'%s'\" % field.name\n                            for field in data_type.fields\n                            if field.omitted_caller == omitted_caller\n                        ],\n                        before='{}.{} = set('.format(class_name, names_map_name),\n                        after=')',\n                        delim=('[', ']'),\n                        compact=False)\n                if caller_in_parent:\n                    self.emit('{0}.{3} = {1}.{3}.union({0}.{2})'\n                              .format(class_name, parent_type_class_name, names_map_name,\n                                      all_names_map_name))\n                else:\n                    self.emit('{0}.{2} = {0}.{1}'.format(class_name, names_map_name,\n                                                         all_names_map_name))\n            else:\n                if caller_in_parent:\n                    before = '{0}.{1} = {2}.{1}.union(set('.format(class_name, all_names_map_name,\n                                                                   parent_type_class_name)\n                    after = '))'\n                else:\n                    before = '{}.{} = set('.format(class_name, all_names_map_name)\n                    after = ')'\n                items = [\n                    \"'%s'\" % field.name\n                    for field in data_type.fields\n                    if field.omitted_caller == omitted_caller\n                ]\n                self.generate_multiline_list(\n                    items,\n                    before=before,\n                    after=after,\n                    delim=('[', ']'),\n                    compact=False)\n\n            # generate `_all_fields_`\n            fields_map_name = '{}_fields_'.format(map_name_prefix)\n            all_fields_map_name = '_all{}_fields_'.format(map_name_prefix)\n            if data_type.is_member_of_enumerated_subtypes_tree():\n                items = []\n                for field in data_type.fields:\n                    if field.omitted_caller != omitted_caller:\n                        continue\n\n                    var_name = fmt_var(field.name)\n                    validator_name = '{}._{}_validator'.format(class_name,\n                                                               var_name)\n                    items.append(\"('{}', {})\".format(var_name, validator_name))\n                self.generate_multiline_list(\n                    items,\n                    before='{}.{} = '.format(class_name, fields_map_name),\n                    delim=('[', ']'),\n                    compact=False,\n                )\n                if caller_in_parent:\n                    self.emit('{0}.{3} = {1}.{3} + {0}.{2}'.format(\n                        class_name, parent_type_class_name, fields_map_name, all_fields_map_name))\n                else:\n                    self.emit('{0}.{2} = {0}.{1}'.format(\n                        class_name, fields_map_name, all_fields_map_name))\n            else:\n                if caller_in_parent:\n                    before = '{0}.{2} = {1}.{2} + '.format(\n                        class_name, parent_type_class_name, all_fields_map_name)\n                else:\n                    before = '{}.{} = '.format(class_name, all_fields_map_name)\n\n                items = []\n                for field in data_type.fields:\n                    if field.omitted_caller != omitted_caller:\n                        continue\n\n                    var_name = fmt_var(field.name)\n                    validator_name = '{}._{}_validator'.format(\n                        class_name, var_name)\n                    items.append(\"('{}', {})\".format(var_name, validator_name))\n                self.generate_multiline_list(\n                    items, before=before, delim=('[', ']'), compact=False)\n\n        self.emit()", "response": "Generates the reflection attributes for the struct."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the struct class initialization for the given data type.", "response": "def _generate_struct_class_init(self, data_type):\n        \"\"\"\n        Generates constructor. The constructor takes all possible fields as\n        optional arguments. Any argument that is set on construction sets the\n        corresponding field for the instance.\n        \"\"\"\n\n        args = ['self']\n        for field in data_type.all_fields:\n            field_name_reserved_check = fmt_var(field.name, True)\n            args.append('%s=None' % field_name_reserved_check)\n\n        self.generate_multiline_list(args, before='def __init__', after=':')\n\n        with self.indent():\n            lineno = self.lineno\n\n            # Call the parent constructor if a super type exists\n            if data_type.parent_type:\n                class_name = class_name_for_data_type(data_type)\n                all_parent_fields = [fmt_func(f.name, check_reserved=True)\n                              for f in data_type.parent_type.all_fields]\n                self.generate_multiline_list(\n                    all_parent_fields,\n                    before='super({}, self).__init__'.format(class_name))\n\n            # initialize each field\n            for field in data_type.fields:\n                field_var_name = fmt_var(field.name)\n                self.emit('self._{}_value = None'.format(field_var_name))\n                self.emit('self._{}_present = False'.format(field_var_name))\n\n            # handle arguments that were set\n            for field in data_type.fields:\n                field_var_name = fmt_var(field.name, True)\n                self.emit('if {} is not None:'.format(field_var_name))\n                with self.indent():\n                    self.emit('self.{0} = {0}'.format(field_var_name))\n\n            if lineno == self.lineno:\n                self.emit('pass')\n            self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_struct_class_properties(self, ns, data_type):\n        for field in data_type.fields:\n            field_name = fmt_func(field.name)\n            field_name_reserved_check = fmt_func(field.name, check_reserved=True)\n            if is_nullable_type(field.data_type):\n                field_dt = field.data_type.data_type\n                dt_nullable = True\n            else:\n                field_dt = field.data_type\n                dt_nullable = False\n\n            # generate getter for field\n            self.emit('@property')\n            self.emit('def {}(self):'.format(field_name_reserved_check))\n            with self.indent():\n                self.emit('\"\"\"')\n                if field.doc:\n                    self.emit_wrapped_text(\n                        self.process_doc(field.doc, self._docf))\n                    # Sphinx wants an extra line between the text and the\n                    # rtype declaration.\n                    self.emit()\n                self.emit(':rtype: {}'.format(\n                    self._python_type_mapping(ns, field_dt)))\n                self.emit('\"\"\"')\n                self.emit('if self._{}_present:'.format(field_name))\n                with self.indent():\n                    self.emit('return self._{}_value'.format(field_name))\n\n                self.emit('else:')\n                with self.indent():\n                    if dt_nullable:\n                        self.emit('return None')\n                    elif field.has_default:\n                        self.emit('return {}'.format(\n                            self._generate_python_value(ns, field.default)))\n                    else:\n                        self.emit(\n                            \"raise AttributeError(\\\"missing required field '%s'\\\")\"\n                            % field_name\n                        )\n            self.emit()\n\n            # generate setter for field\n            self.emit('@{}.setter'.format(field_name_reserved_check))\n            self.emit('def {}(self, val):'.format(field_name_reserved_check))\n            with self.indent():\n                if dt_nullable:\n                    self.emit('if val is None:')\n                    with self.indent():\n                        self.emit('del self.{}'.format(field_name_reserved_check))\n                        self.emit('return')\n                if is_user_defined_type(field_dt):\n                    self.emit('self._%s_validator.validate_type_only(val)' %\n                              field_name)\n                else:\n                    self.emit('val = self._{}_validator.validate(val)'.format(field_name))\n                self.emit('self._{}_value = val'.format(field_name))\n                self.emit('self._{}_present = True'.format(field_name))\n            self.emit()\n\n            # generate deleter for field\n            self.emit('@{}.deleter'.format(field_name_reserved_check))\n            self.emit('def {}(self):'.format(field_name_reserved_check))\n            with self.indent():\n                self.emit('self._{}_value = None'.format(field_name))\n                self.emit('self._{}_present = False'.format(field_name))\n            self.emit()", "response": "Generates the class properties for the struct."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_custom_annotation_instance(self, ns, annotation):\n        annotation_class = class_name_for_annotation_type(annotation.annotation_type, ns)\n        return generate_func_call(\n            annotation_class,\n            kwargs=((fmt_var(k, True), self._generate_python_value(ns, v))\n                    for k, v in annotation.kwargs.items())\n        )", "response": "Generates code to construct an instance of an annotation type with parameters from the specified annotation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _generate_custom_annotation_processors(self, ns, data_type, extra_annotations=()):\n        # annotations applied to members of this type\n        dt, _, _ = unwrap(data_type)\n        if is_struct_type(dt) or is_union_type(dt):\n            annotation_types_seen = set()\n            for annotation in get_custom_annotations_recursive(dt):\n                if annotation.annotation_type not in annotation_types_seen:\n                    yield (annotation.annotation_type,\n                           generate_func_call(\n                               'bb.make_struct_annotation_processor',\n                               args=[class_name_for_annotation_type(annotation.annotation_type, ns),\n                                     'processor']\n                           ))\n                    annotation_types_seen.add(annotation.annotation_type)\n        elif is_list_type(dt):\n            for annotation_type, recursive_processor in self._generate_custom_annotation_processors(\n                    ns, dt.data_type):\n                # every member needs to be replaced---use handwritten processor\n                yield (annotation_type,\n                       generate_func_call(\n                           'bb.make_list_annotation_processor',\n                           args=[recursive_processor]\n                       ))\n        elif is_map_type(dt):\n            for annotation_type, recursive_processor in self._generate_custom_annotation_processors(\n                    ns, dt.value_data_type):\n                # every value needs to be replaced---use handwritten processor\n                yield (annotation_type,\n                       generate_func_call(\n                           'bb.make_map_value_annotation_processor',\n                           args=[recursive_processor]\n                       ))\n\n        # annotations applied directly to this type (through aliases or\n        # passed in from the caller)\n        for annotation in itertools.chain(get_custom_annotations_for_alias(data_type),\n                                          extra_annotations):\n            yield (annotation.annotation_type,\n                   generate_func_call(\n                       'bb.partially_apply',\n                       args=['processor', self._generate_custom_annotation_instance(ns, annotation)]\n                   ))", "response": "Generates code that will run a custom function processor on every of the custom types of the specified data type."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_struct_class_custom_annotations(self, ns, data_type):\n        self.emit('def _process_custom_annotations(self, annotation_type, field_path, processor):')\n\n        with self.indent(), emit_pass_if_nothing_emitted(self):\n            self.emit(\n                (\n                    'super({}, self)._process_custom_annotations(annotation_type, field_path, '\n                    'processor)'\n                ).format(class_name_for_data_type(data_type))\n            )\n            self.emit()\n\n            for field in data_type.fields:\n                field_name = fmt_var(field.name, check_reserved=True)\n                for annotation_type, processor in self._generate_custom_annotation_processors(\n                        ns, field.data_type, field.custom_annotations):\n                    annotation_class = class_name_for_annotation_type(annotation_type, ns)\n                    self.emit('if annotation_type is {}:'.format(annotation_class))\n                    with self.indent():\n                        self.emit('self.{} = {}'.format(\n                            field_name,\n                            generate_func_call(\n                                processor,\n                                args=[\n                                    \"'{{}}.{}'.format(field_path)\".format(field_name),\n                                    'self.{}'.format(field_name),\n                                ])\n                        ))\n                    self.emit()", "response": "Generates the code for the struct class custom annotations."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_struct_class_repr(self, data_type):\n        self.emit('def __repr__(self):')\n        with self.indent():\n            if data_type.all_fields:\n                constructor_kwargs_fmt = ', '.join(\n                    '{}={{!r}}'.format(fmt_var(f.name, True))\n                    for f in data_type.all_fields)\n                self.emit(\"return '{}({})'.format(\".format(\n                    class_name_for_data_type(data_type),\n                    constructor_kwargs_fmt,\n                ))\n                with self.indent():\n                    for f in data_type.all_fields:\n                        self.emit(\"self._{}_value,\".format(fmt_var(f.name)))\n                self.emit(\")\")\n            else:\n                self.emit(\"return '%s()'\" %\n                          class_name_for_data_type(data_type))\n        self.emit()", "response": "Generates something like a class_name for the given data type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the attribute _tag_to_subtype_ and _pytype_to_tag_and_subtype_ for the enumeration of the given data type.", "response": "def _generate_enumerated_subtypes_tag_mapping(self, ns, data_type):\n        \"\"\"\n        Generates attributes needed for serializing and deserializing structs\n        with enumerated subtypes. These assignments are made after all the\n        Python class definitions to ensure that all references exist.\n        \"\"\"\n        assert data_type.has_enumerated_subtypes()\n\n        # Generate _tag_to_subtype_ attribute: Map from string type tag to\n        # the validator of the referenced subtype. Used on deserialization\n        # to look up the subtype for a given tag.\n        tag_to_subtype_items = []\n        for tags, subtype in data_type.get_all_subtypes_with_tags():\n            tag_to_subtype_items.append(\"{}: {}\".format(\n                tags,\n                generate_validator_constructor(ns, subtype)))\n\n        self.generate_multiline_list(\n            tag_to_subtype_items,\n            before='{}._tag_to_subtype_ = '.format(data_type.name),\n            delim=('{', '}'),\n            compact=False)\n\n        # Generate _pytype_to_tag_and_subtype_: Map from Python class to a\n        # tuple of (type tag, subtype). Used on serialization to lookup how a\n        # class should be encoded based on the root struct's enumerated\n        # subtypes.\n        items = []\n        for tag, subtype in data_type.get_all_subtypes_with_tags():\n            items.append(\"{0}: ({1}, {2})\".format(\n                fmt_class(subtype.name),\n                tag,\n                generate_validator_constructor(ns, subtype)))\n        self.generate_multiline_list(\n            items,\n            before='{}._pytype_to_tag_and_subtype_ = '.format(data_type.name),\n            delim=('{', '}'),\n            compact=False)\n\n        # Generate _is_catch_all_ attribute:\n        self.emit('{}._is_catch_all_ = {!r}'.format(\n            data_type.name, data_type.is_catch_all()))\n\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_union_class(self, ns, data_type):\n        # type: (ApiNamespace, Union) -> None\n        \"\"\"Defines a Python class that represents a union in Stone.\"\"\"\n        self.emit(self._class_declaration_for_type(ns, data_type))\n        with self.indent():\n            self.emit('\"\"\"')\n            if data_type.doc:\n                self.emit_wrapped_text(\n                    self.process_doc(data_type.doc, self._docf))\n                self.emit()\n\n            self.emit_wrapped_text(\n                'This class acts as a tagged union. Only one of the ``is_*`` '\n                'methods will return true. To get the associated value of a '\n                'tag (if one exists), use the corresponding ``get_*`` method.')\n\n            if data_type.has_documented_fields():\n                self.emit()\n\n            for field in data_type.fields:\n                if not field.doc:\n                    continue\n                if is_void_type(field.data_type):\n                    ivar_doc = ':ivar {}: {}'.format(\n                        fmt_namespaced_var(ns.name, data_type.name, field.name),\n                        self.process_doc(field.doc, self._docf))\n                elif is_user_defined_type(field.data_type):\n                    if data_type.namespace.name != ns.name:\n                        formatted_var = fmt_namespaced_var(ns.name, data_type.name, field.name)\n                    else:\n                        formatted_var = '{}.{}'.format(data_type.name, fmt_var(field.name))\n                    ivar_doc = ':ivar {} {}: {}'.format(\n                        fmt_class(field.data_type.name),\n                        formatted_var,\n                        self.process_doc(field.doc, self._docf))\n                else:\n                    ivar_doc = ':ivar {} {}: {}'.format(\n                        self._python_type_mapping(ns, field.data_type),\n                        fmt_namespaced_var(ns.name, data_type.name, field.name), field.doc)\n                self.emit_wrapped_text(ivar_doc, subsequent_prefix='    ')\n            self.emit('\"\"\"')\n            self.emit()\n\n            self._generate_union_class_vars(data_type)\n            self._generate_union_class_variant_creators(ns, data_type)\n            self._generate_union_class_is_set(data_type)\n            self._generate_union_class_get_helpers(ns, data_type)\n            self._generate_union_class_custom_annotations(ns, data_type)\n            self._generate_union_class_repr(data_type)\n        self.emit('{0}_validator = bv.Union({0})'.format(\n            class_name_for_data_type(data_type)\n        ))\n        self.emit()", "response": "Defines a Python class that represents a union in Stone."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the class variables needed to construct the union members of void types.", "response": "def _generate_union_class_vars(self, data_type):\n        \"\"\"\n        Adds a _catch_all_ attribute to each class. Also, adds a placeholder\n        attribute for the construction of union members of void type.\n        \"\"\"\n        lineno = self.lineno\n        if data_type.catch_all_field:\n            self.emit(\"_catch_all = '%s'\" % data_type.catch_all_field.name)\n        elif not data_type.parent_type:\n            self.emit('_catch_all = None')\n\n        # Generate stubs for class variables so that IDEs like PyCharms have an\n        # easier time detecting their existence.\n        for field in data_type.fields:\n            if is_void_type(field.data_type):\n                field_name = fmt_var(field.name)\n                self.emit('# Attribute is overwritten below the class definition')\n                self.emit('{} = None'.format(field_name))\n\n        if lineno != self.lineno:\n            self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_union_class_reflection_attributes(self, ns, data_type):\n        class_name = fmt_class(data_type.name)\n\n        for field in data_type.fields:\n            field_name = fmt_var(field.name)\n            validator_name = generate_validator_constructor(\n                ns, field.data_type)\n            full_validator_name = '{}._{}_validator'.format(class_name, field_name)\n            self.emit('{} = {}'.format(full_validator_name, validator_name))\n\n            if field.redactor:\n                self._generate_redactor(full_validator_name, field.redactor)\n\n        # generate _all_fields_ for each omitted caller (and public)\n        child_omitted_callers = data_type.get_all_omitted_callers()\n        parent_omitted_callers = data_type.parent_type.get_all_omitted_callers() if \\\n            data_type.parent_type else set([])\n\n        all_omitted_callers = child_omitted_callers | parent_omitted_callers\n        if len(all_omitted_callers) != 0:\n            self.emit('{}._permissioned_tagmaps = {}'.format(class_name, all_omitted_callers))\n        for omitted_caller in sorted(all_omitted_callers | {None}, key=str):\n            is_public = omitted_caller is None\n            tagmap_name = '_tagmap' if is_public else '_{}_tagmap'.format(omitted_caller)\n            caller_in_parent = data_type.parent_type and (is_public or omitted_caller\n                                                         in parent_omitted_callers)\n\n            with self.block('{}.{} ='.format(class_name, tagmap_name)):\n                for field in data_type.fields:\n                    if field.omitted_caller != omitted_caller:\n                        continue\n                    var_name = fmt_var(field.name)\n                    validator_name = '{}._{}_validator'.format(class_name, var_name)\n                    self.emit(\"'{}': {},\".format(var_name, validator_name))\n\n            if caller_in_parent:\n                self.emit('{0}.{1}.update({2}.{1})'.format(\n                    class_name, tagmap_name,\n                    class_name_for_data_type(data_type.parent_type, ns))\n                )\n\n        self.emit()", "response": "Generates the class attributes for each union member assigned to a validator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_union_class_variant_creators(self, ns, data_type):\n        for field in data_type.fields:\n            if not is_void_type(field.data_type):\n                field_name = fmt_func(field.name)\n                field_name_reserved_check = fmt_func(field.name, check_reserved=True)\n                if is_nullable_type(field.data_type):\n                    field_dt = field.data_type.data_type\n                else:\n                    field_dt = field.data_type\n                self.emit('@classmethod')\n                self.emit('def {}(cls, val):'.format(field_name_reserved_check))\n                with self.indent():\n                    self.emit('\"\"\"')\n                    self.emit_wrapped_text(\n                        'Create an instance of this class set to the ``%s`` '\n                        'tag with value ``val``.' % field_name)\n                    self.emit()\n                    self.emit(':param {} val:'.format(\n                        self._python_type_mapping(ns, field_dt)))\n                    self.emit(':rtype: {}'.format(\n                        self._python_type_mapping(ns, data_type)))\n                    self.emit('\"\"\"')\n                    self.emit(\"return cls('{}', val)\".format(field_name))\n                self.emit()", "response": "Generates the class methods that can be used to construct a union with the selected non - symbol."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates the get methods for union class fields.", "response": "def _generate_union_class_get_helpers(self, ns, data_type):\n        \"\"\"\n        These are the getters used to access the value of a variant, once\n        the tag has been switched on.\n        \"\"\"\n        for field in data_type.fields:\n            field_name = fmt_func(field.name)\n\n            if not is_void_type(field.data_type):\n                # generate getter for field\n                self.emit('def get_{}(self):'.format(field_name))\n                with self.indent():\n                    if is_nullable_type(field.data_type):\n                        field_dt = field.data_type.data_type\n                    else:\n                        field_dt = field.data_type\n                    self.emit('\"\"\"')\n                    if field.doc:\n                        self.emit_wrapped_text(\n                            self.process_doc(field.doc, self._docf))\n                        self.emit()\n                    self.emit(\"Only call this if :meth:`is_%s` is true.\" %\n                              field_name)\n                    # Sphinx wants an extra line between the text and the\n                    # rtype declaration.\n                    self.emit()\n                    self.emit(':rtype: {}'.format(\n                        self._python_type_mapping(ns, field_dt)))\n                    self.emit('\"\"\"')\n\n                    self.emit('if not self.is_{}():'.format(field_name))\n                    with self.indent():\n                        self.emit(\n                            'raise AttributeError(\"tag \\'{}\\' not set\")'.format(\n                                field_name))\n                    self.emit('return self._value')\n                self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _generate_union_class_custom_annotations(self, ns, data_type):\n        self.emit('def _process_custom_annotations(self, annotation_type, field_path, processor):')\n        with self.indent(), emit_pass_if_nothing_emitted(self):\n            self.emit(\n                (\n                    'super({}, self)._process_custom_annotations(annotation_type, field_path, '\n                    'processor)'\n                ).format(class_name_for_data_type(data_type))\n            )\n            self.emit()\n\n            for field in data_type.fields:\n                recursive_processors = list(self._generate_custom_annotation_processors(\n                    ns, field.data_type, field.custom_annotations))\n\n                # check if we have any annotations that apply to this field at all\n                if len(recursive_processors) == 0:\n                    continue\n\n                field_name = fmt_func(field.name)\n                self.emit('if self.is_{}():'.format(field_name))\n\n                with self.indent():\n                    for annotation_type, processor in recursive_processors:\n                        annotation_class = class_name_for_annotation_type(annotation_type, ns)\n                        self.emit('if annotation_type is {}:'.format(annotation_class))\n                        with self.indent():\n                            self.emit('self._value = {}'.format(\n                                generate_func_call(\n                                    processor,\n                                    args=[\n                                        \"'{{}}.{}'.format(field_path)\".format(field_name),\n                                        'self._value',\n                                    ])\n                            ))\n                        self.emit()", "response": "This function is used to generate the union class custom annotations."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nemit the repr function that will return a string of the class name and the selected value.", "response": "def _generate_union_class_repr(self, data_type):\n        \"\"\"\n        The __repr__() function will return a string of the class name, and\n        the selected tag.\n        \"\"\"\n        self.emit('def __repr__(self):')\n        with self.indent():\n            self.emit(\"return '{}(%r, %r)' % (self._tag, self._value)\".format(\n                class_name_for_data_type(data_type),\n            ))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_union_class_symbol_creators(self, data_type):\n        class_name = fmt_class(data_type.name)\n        lineno = self.lineno\n        for field in data_type.fields:\n            if is_void_type(field.data_type):\n                field_name = fmt_func(field.name)\n                self.emit(\"{0}.{1} = {0}('{1}')\".format(class_name, field_name))\n        if lineno != self.lineno:\n            self.emit()", "response": "Generate the class symbol creators for the union class."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding an object into JSON based on its type.", "response": "def json_encode(data_type, obj, caller_permissions=None, alias_validators=None, old_style=False,\n                should_redact=False):\n    \"\"\"Encodes an object into JSON based on its type.\n\n    Args:\n        data_type (Validator): Validator for obj.\n        obj (object): Object to be serialized.\n        caller_permissions (list): The list of raw-string caller permissions with which\n                to serialize.\n        alias_validators (Optional[Mapping[bv.Validator, Callable[[], None]]]):\n            Custom validation functions. These must raise bv.ValidationError on\n            failure.\n\n    Returns:\n        str: JSON-encoded object.\n\n    This function will also do additional validation that wasn't done by the\n    objects themselves:\n\n    1. The passed in obj may not have been validated with data_type yet.\n    2. If an object that should be a Struct was assigned to a field, its\n       type has been validated, but the presence of all required fields\n       hasn't been.\n    3. If an object that should be a Union was assigned to a field, whether\n       or not a tag has been set has not been validated.\n    4. A list may have passed validation initially, but been mutated since.\n\n    Example of serializing a struct to JSON:\n\n    struct FileRef\n       path String\n       rev String\n\n    > fr = FileRef()\n    > fr.path = 'a/b/c'\n    > fr.rev = '1234'\n    > JsonEncoder.encode(fr)\n    \"{'path': 'a/b/c', 'rev': '1234'}\"\n\n    Example of serializing a union to JSON:\n\n    union UploadMode\n        add\n        overwrite\n        update FileRef\n\n    > um = UploadMode()\n    > um.set_add()\n    > JsonEncoder.encode(um)\n    '\"add\"'\n    > um.update = fr\n    > JsonEncoder.encode(um)\n    \"{'update': {'path': 'a/b/c', 'rev': '1234'}}\"\n    \"\"\"\n    for_msgpack = False\n    serializer = StoneToJsonSerializer(\n        caller_permissions, alias_validators, for_msgpack, old_style, should_redact)\n    return serializer.encode(data_type, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef json_compat_obj_encode(data_type, obj, caller_permissions=None, alias_validators=None,\n                           old_style=False, for_msgpack=False, should_redact=False):\n    \"\"\"Encodes an object into a JSON-compatible dict based on its type.\n\n    Args:\n        data_type (Validator): Validator for obj.\n        obj (object): Object to be serialized.\n        caller_permissions (list): The list of raw-string caller permissions\n            with which to serialize.\n\n    Returns:\n        An object that when passed to json.dumps() will produce a string\n        giving the JSON-encoded object.\n\n    See json_encode() for additional information about validation.\n    \"\"\"\n    serializer = StoneToPythonPrimitiveSerializer(\n        caller_permissions, alias_validators, for_msgpack, old_style, should_redact)\n    return serializer.encode(data_type, obj)", "response": "Encodes an object into a JSON - compatible dict based on its type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nperform the reverse operation of json_encode. Args: data_type (Validator): Validator for serialized_obj. serialized_obj (str): The JSON string to deserialize. caller_permissions (list): The list of raw-string caller permissions with which to serialize. alias_validators (Optional[Mapping[bv.Validator, Callable[[], None]]]): Custom validation functions. These must raise bv.ValidationError on failure. strict (bool): If strict, then unknown struct fields will raise an error, and unknown union variants will raise an error even if a catch all field is specified. strict should only be used by a recipient of serialized JSON if it's guaranteed that its Stone specs are at least as recent as the senders it receives messages from. Returns: The returned object depends on the input data_type. - Boolean -> bool - Bytes -> bytes - Float -> float - Integer -> long - List -> list - Map -> dict - Nullable -> None or its wrapped type. - String -> unicode (PY2) or str (PY3) - Struct -> An instance of its definition attribute. - Timestamp -> datetime.datetime - Union -> An instance of its definition attribute.", "response": "def json_decode(data_type, serialized_obj, caller_permissions=None,\n                alias_validators=None, strict=True, old_style=False):\n    \"\"\"Performs the reverse operation of json_encode.\n\n    Args:\n        data_type (Validator): Validator for serialized_obj.\n        serialized_obj (str): The JSON string to deserialize.\n        caller_permissions (list): The list of raw-string caller permissions\n            with which to serialize.\n        alias_validators (Optional[Mapping[bv.Validator, Callable[[], None]]]):\n            Custom validation functions. These must raise bv.ValidationError on\n            failure.\n        strict (bool): If strict, then unknown struct fields will raise an\n            error, and unknown union variants will raise an error even if a\n            catch all field is specified. strict should only be used by a\n            recipient of serialized JSON if it's guaranteed that its Stone\n            specs are at least as recent as the senders it receives messages\n            from.\n\n    Returns:\n        The returned object depends on the input data_type.\n            - Boolean -> bool\n            - Bytes -> bytes\n            - Float -> float\n            - Integer -> long\n            - List -> list\n            - Map -> dict\n            - Nullable -> None or its wrapped type.\n            - String -> unicode (PY2) or str (PY3)\n            - Struct -> An instance of its definition attribute.\n            - Timestamp -> datetime.datetime\n            - Union -> An instance of its definition attribute.\n    \"\"\"\n    try:\n        deserialized_obj = json.loads(serialized_obj)\n    except ValueError:\n        raise bv.ValidationError('could not decode input as JSON')\n    else:\n        return json_compat_obj_decode(\n            data_type, deserialized_obj, caller_permissions=caller_permissions,\n            alias_validators=alias_validators, strict=strict, old_style=old_style)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding a JSON - compatible object into a new object.", "response": "def json_compat_obj_decode(data_type, obj, caller_permissions=None,\n                           alias_validators=None, strict=True,\n                           old_style=False, for_msgpack=False):\n    \"\"\"\n    Decodes a JSON-compatible object based on its data type into a\n    representative Python object.\n\n    Args:\n        data_type (Validator): Validator for serialized_obj.\n        obj: The JSON-compatible object to decode based on data_type.\n        caller_permissions (list): The list of raw-string caller permissions\n            with which to serialize.\n        strict (bool): If strict, then unknown struct fields will raise an\n            error, and unknown union variants will raise an error even if a\n            catch all field is specified. See json_decode() for more.\n\n    Returns:\n        See json_decode().\n    \"\"\"\n    decoder = PythonPrimitiveToStoneDecoder(caller_permissions,\n        alias_validators, for_msgpack, old_style, strict)\n\n    if isinstance(data_type, bv.Primitive):\n        return decoder.make_stone_friendly(\n            data_type, obj, True)\n    else:\n        return decoder.json_compat_obj_decode_helper(\n            data_type, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef encode_sub(self, validator, value):\n        # type: (bv.Validator, typing.Any) -> typing.Any\n        \"\"\"\n        Callback intended to be called by other ``encode`` methods to\n        delegate encoding of sub-values. Arguments have the same semantics\n        as with the ``encode`` method.\n        \"\"\"\n\n        if isinstance(validator, bv.List):\n            # Because Lists are mutable, we always validate them during\n            # serialization\n            validate_f = validator.validate  # type: typing.Callable[[typing.Any], None]\n            encode_f = self.encode_list  # type: typing.Callable[[typing.Any, typing.Any], typing.Any] # noqa: E501\n        elif isinstance(validator, bv.Map):\n            # Also validate maps during serialization because they are also mutable\n            validate_f = validator.validate\n            encode_f = self.encode_map\n        elif isinstance(validator, bv.Nullable):\n            validate_f = validator.validate\n            encode_f = self.encode_nullable\n        elif isinstance(validator, bv.Primitive):\n            validate_f = validator.validate\n            encode_f = self.encode_primitive\n        elif isinstance(validator, bv.Struct):\n            if isinstance(validator, bv.StructTree):\n                if self.caller_permissions.permissions:\n                    def validate_with_permissions(val):\n                        validator.validate_with_permissions(val, self.caller_permissions)\n\n                    validate_f = validate_with_permissions\n                else:\n                    validate_f = validator.validate\n                encode_f = self.encode_struct_tree\n            else:\n                # Fields are already validated on assignment\n                if self.caller_permissions.permissions:\n                    def validate_with_permissions(val):\n                        validator.validate_with_permissions(val, self.caller_permissions)\n\n                    validate_f = validate_with_permissions\n                else:\n                    validate_f = validator.validate_type_only\n                encode_f = self.encode_struct\n        elif isinstance(validator, bv.Union):\n            # Fields are already validated on assignment\n            validate_f = validator.validate_type_only\n            encode_f = self.encode_union\n        else:\n            raise bv.ValidationError('Unsupported data type {}'.format(type(validator).__name__))\n\n        validate_f(value)\n\n        return encode_f(validator, value)", "response": "Encodes the sub - value of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding a single object of the specified data type.", "response": "def json_compat_obj_decode_helper(self, data_type, obj):\n        \"\"\"\n        See json_compat_obj_decode() for argument descriptions.\n        \"\"\"\n        if isinstance(data_type, bv.StructTree):\n            return self.decode_struct_tree(data_type, obj)\n        elif isinstance(data_type, bv.Struct):\n            return self.decode_struct(data_type, obj)\n        elif isinstance(data_type, bv.Union):\n            if self.old_style:\n                return self.decode_union_old(data_type, obj)\n            else:\n                return self.decode_union(data_type, obj)\n        elif isinstance(data_type, bv.List):\n            return self.decode_list(\n                data_type, obj)\n        elif isinstance(data_type, bv.Map):\n            return self.decode_map(\n                data_type, obj)\n        elif isinstance(data_type, bv.Nullable):\n            return self.decode_nullable(\n                data_type, obj)\n        elif isinstance(data_type, bv.Primitive):\n            # Set validate to false because validation will be done by the\n            # containing struct or union when the field is assigned.\n            return self.make_stone_friendly(data_type, obj, False)\n        else:\n            raise AssertionError('Cannot handle type %r.' % data_type)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndecode the object into a structure.", "response": "def decode_struct(self, data_type, obj):\n        \"\"\"\n        The data_type argument must be a Struct.\n        See json_compat_obj_decode() for argument descriptions.\n        \"\"\"\n        if obj is None and data_type.has_default():\n            return data_type.get_default()\n        elif not isinstance(obj, dict):\n            raise bv.ValidationError('expected object, got %s' %\n                                     bv.generic_type_name(obj))\n        all_fields = data_type.definition._all_fields_\n        for extra_permission in self.caller_permissions.permissions:\n            all_extra_fields = '_all_{}_fields_'.format(extra_permission)\n            all_fields = all_fields + getattr(data_type.definition, all_extra_fields, [])\n\n        if self.strict:\n            all_field_names = data_type.definition._all_field_names_\n            for extra_permission in self.caller_permissions.permissions:\n                all_extra_field_names = '_all_{}_field_names_'.format(extra_permission)\n                all_field_names = all_field_names.union(\n                    getattr(data_type.definition, all_extra_field_names, {}))\n\n            for key in obj:\n                if (key not in all_field_names and\n                        not key.startswith('.tag')):\n                    raise bv.ValidationError(\"unknown field '%s'\" % key)\n        ins = data_type.definition()\n        self.decode_struct_fields(ins, all_fields, obj)\n        # Check that all required fields have been set.\n        data_type.validate_fields_only_with_permissions(ins, self.caller_permissions)\n        return ins"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef decode_struct_fields(self, ins, fields, obj):\n        for name, field_data_type in fields:\n            if name in obj:\n                try:\n                    v = self.json_compat_obj_decode_helper(field_data_type, obj[name])\n                    setattr(ins, name, v)\n                except bv.ValidationError as e:\n                    e.add_parent(name)\n                    raise\n            elif field_data_type.has_default():\n                setattr(ins, name, field_data_type.get_default())", "response": "Decode the fields of a structure into a new object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes a union into a data_type.", "response": "def decode_union(self, data_type, obj):\n        \"\"\"\n        The data_type argument must be a Union.\n        See json_compat_obj_decode() for argument descriptions.\n        \"\"\"\n        val = None\n        if isinstance(obj, six.string_types):\n            # Handles the shorthand format where the union is serialized as only\n            # the string of the tag.\n            tag = obj\n            if data_type.definition._is_tag_present(tag, self.caller_permissions):\n                val_data_type = data_type.definition._get_val_data_type(\n                    tag, self.caller_permissions)\n                if not isinstance(val_data_type, (bv.Void, bv.Nullable)):\n                    raise bv.ValidationError(\n                        \"expected object for '%s', got symbol\" % tag)\n                if tag == data_type.definition._catch_all:\n                    raise bv.ValidationError(\n                        \"unexpected use of the catch-all tag '%s'\" % tag)\n            elif not self.strict and data_type.definition._catch_all:\n                tag = data_type.definition._catch_all\n            else:\n                raise bv.ValidationError(\"unknown tag '%s'\" % tag)\n        elif isinstance(obj, dict):\n            tag, val = self.decode_union_dict(\n                data_type, obj)\n        else:\n            raise bv.ValidationError(\"expected string or object, got %s\" %\n                                     bv.generic_type_name(obj))\n        return data_type.definition(tag, val)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndecoding an old object into a new object.", "response": "def decode_union_old(self, data_type, obj):\n        \"\"\"\n        The data_type argument must be a Union.\n        See json_compat_obj_decode() for argument descriptions.\n        \"\"\"\n        val = None\n        if isinstance(obj, six.string_types):\n            # Union member has no associated value\n            tag = obj\n            if data_type.definition._is_tag_present(tag, self.caller_permissions):\n                val_data_type = data_type.definition._get_val_data_type(tag,\n                                                                        self.caller_permissions)\n                if not isinstance(val_data_type, (bv.Void, bv.Nullable)):\n                    raise bv.ValidationError(\n                        \"expected object for '%s', got symbol\" % tag)\n            else:\n                if not self.strict and data_type.definition._catch_all:\n                    tag = data_type.definition._catch_all\n                else:\n                    raise bv.ValidationError(\"unknown tag '%s'\" % tag)\n        elif isinstance(obj, dict):\n            # Union member has value\n            if len(obj) != 1:\n                raise bv.ValidationError('expected 1 key, got %s' % len(obj))\n            tag = list(obj)[0]\n            raw_val = obj[tag]\n            if data_type.definition._is_tag_present(tag, self.caller_permissions):\n                val_data_type = data_type.definition._get_val_data_type(tag,\n                                                                        self.caller_permissions)\n                if isinstance(val_data_type, bv.Nullable) and raw_val is None:\n                    val = None\n                elif isinstance(val_data_type, bv.Void):\n                    if raw_val is None or not self.strict:\n                        # If raw_val is None, then this is the more verbose\n                        # representation of a void union member. If raw_val isn't\n                        # None, then maybe the spec has changed, so check if we're\n                        # in strict mode.\n                        val = None\n                    else:\n                        raise bv.ValidationError('expected null, got %s' %\n                                                 bv.generic_type_name(raw_val))\n                else:\n                    try:\n                        val = self.json_compat_obj_decode_helper(val_data_type, raw_val)\n                    except bv.ValidationError as e:\n                        e.add_parent(tag)\n                        raise\n            else:\n                if not self.strict and data_type.definition._catch_all:\n                    tag = data_type.definition._catch_all\n                else:\n                    raise bv.ValidationError(\"unknown tag '%s'\" % tag)\n        else:\n            raise bv.ValidationError(\"expected string or object, got %s\" %\n                                     bv.generic_type_name(obj))\n        return data_type.definition(tag, val)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndecoding a StructTree object.", "response": "def decode_struct_tree(self, data_type, obj):\n        \"\"\"\n        The data_type argument must be a StructTree.\n        See json_compat_obj_decode() for argument descriptions.\n        \"\"\"\n        subtype = self.determine_struct_tree_subtype(data_type, obj)\n        return self.decode_struct(subtype, obj)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine which of the enumerated subtypes obj refers to.", "response": "def determine_struct_tree_subtype(self, data_type, obj):\n        \"\"\"\n        Searches through the JSON-object-compatible dict using the data type\n        definition to determine which of the enumerated subtypes `obj` is.\n        \"\"\"\n        if '.tag' not in obj:\n            raise bv.ValidationError(\"missing '.tag' key\")\n        if not isinstance(obj['.tag'], six.string_types):\n            raise bv.ValidationError('expected string, got %s' %\n                                     bv.generic_type_name(obj['.tag']),\n                                     parent='.tag')\n\n        # Find the subtype the tags refer to\n        full_tags_tuple = (obj['.tag'],)\n        if full_tags_tuple in data_type.definition._tag_to_subtype_:\n            subtype = data_type.definition._tag_to_subtype_[full_tags_tuple]\n            if isinstance(subtype, bv.StructTree):\n                raise bv.ValidationError(\"tag '%s' refers to non-leaf subtype\" %\n                                         ('.'.join(full_tags_tuple)))\n            return subtype\n        else:\n            if self.strict:\n                # In strict mode, the entirety of the tag hierarchy should\n                # point to a known subtype.\n                raise bv.ValidationError(\"unknown subtype '%s'\" %\n                                         '.'.join(full_tags_tuple))\n            else:\n                # If subtype was not found, use the base.\n                if data_type.definition._is_catch_all_:\n                    return data_type\n                else:\n                    raise bv.ValidationError(\n                        \"unknown subtype '%s' and '%s' is not a catch-all\" %\n                        ('.'.join(full_tags_tuple), data_type.definition.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndecodes a list of items from the JSON representation.", "response": "def decode_list(self, data_type, obj):\n        \"\"\"\n        The data_type argument must be a List.\n        See json_compat_obj_decode() for argument descriptions.\n        \"\"\"\n        if not isinstance(obj, list):\n            raise bv.ValidationError(\n                'expected list, got %s' % bv.generic_type_name(obj))\n        return [\n            self.json_compat_obj_decode_helper(data_type.item_validator, item)\n            for item in obj]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef decode_map(self, data_type, obj):\n        if not isinstance(obj, dict):\n            raise bv.ValidationError(\n                'expected dict, got %s' % bv.generic_type_name(obj))\n        return {\n            self.json_compat_obj_decode_helper(data_type.key_validator, key):\n            self.json_compat_obj_decode_helper(data_type.value_validator, value)\n            for key, value in obj.items()\n        }", "response": "Decodes a dictionary of the specified data type into a dictionary of the specified data type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef decode_nullable(self, data_type, obj):\n        if obj is not None:\n            return self.json_compat_obj_decode_helper(data_type.validator, obj)\n        else:\n            return None", "response": "Decodes a Nullable object into a value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef make_stone_friendly(self, data_type, val, validate):\n        if isinstance(data_type, bv.Timestamp):\n            try:\n                ret = datetime.datetime.strptime(val, data_type.format)\n            except (TypeError, ValueError) as e:\n                raise bv.ValidationError(e.args[0])\n        elif isinstance(data_type, bv.Bytes):\n            if self.for_msgpack:\n                if isinstance(val, six.text_type):\n                    ret = val.encode('utf-8')\n                else:\n                    ret = val\n            else:\n                try:\n                    ret = base64.b64decode(val)\n                except TypeError:\n                    raise bv.ValidationError('invalid base64-encoded bytes')\n        elif isinstance(data_type, bv.Void):\n            if self.strict and val is not None:\n                raise bv.ValidationError(\"expected null, got value\")\n            return None\n        else:\n            if validate:\n                if self.caller_permissions.permissions:\n                    data_type.validate_with_permissions(val, self.caller_permissions)\n                else:\n                    data_type.validate(val)\n            ret = val\n        if self.alias_validators is not None and data_type in self.alias_validators:\n            self.alias_validators[data_type](ret)\n        return ret", "response": "Convert a Python object to a type that will pass validation by its alias_validators validator."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsplitting a name based on capitalization dashes and underscores.", "response": "def split_words(name):\n    \"\"\"\n    Splits name based on capitalization, dashes, and underscores.\n        Example: 'GetFile' -> ['Get', 'File']\n        Example: 'get_file' -> ['get', 'file']\n    \"\"\"\n    all_words = []\n    for word in re.split(_split_words_dashes_re, name):\n        vals = _split_words_capitalization_re.findall(word)\n        if vals:\n            all_words.extend(vals)\n        else:\n            all_words.append(word)\n    return all_words"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fmt_camel(name):\n    words = split_words(name)\n    assert len(words) > 0\n    first = words.pop(0).lower()\n    return first + ''.join([word.capitalize() for word in words])", "response": "Converts a name to lower camel case. Words are identified by capitalization dashes and underscores."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_struct(self, struct_type, extra_parameters=None, nameOverride=None):\n        extra_parameters = extra_parameters if extra_parameters is not None else []\n        self._emit_jsdoc_header(struct_type.doc)\n        self.emit(\n            ' * @typedef {Object} %s' % (\n                nameOverride if nameOverride else fmt_type_name(struct_type)\n            )\n        )\n\n        # Some structs can explicitly list their subtypes. These structs\n        # have a .tag field that indicate which subtype they are.\n        if struct_type.is_member_of_enumerated_subtypes_tree():\n            if struct_type.has_enumerated_subtypes():\n                # This struct is the parent to multiple subtypes.\n                # Determine all of the possible values of the .tag\n                # property.\n                tag_values = []\n                for tags, _ in struct_type.get_all_subtypes_with_tags():\n                    for tag in tags:\n                        tag_values.append('\"%s\"' % tag)\n\n                jsdoc_tag_union = fmt_jsdoc_union(tag_values)\n                txt = '@property {%s} .tag - Tag identifying the subtype variant.' % \\\n                    jsdoc_tag_union\n                self.emit_wrapped_text(txt)\n            else:\n                # This struct is a particular subtype. Find the applicable\n                # .tag value from the parent type, which may be an\n                # arbitrary number of steps up the inheritance hierarchy.\n                parent = struct_type.parent_type\n                while not parent.has_enumerated_subtypes():\n                    parent = parent.parent_type\n                # parent now contains the closest parent type in the\n                # inheritance hierarchy that has enumerated subtypes.\n                # Determine which subtype this is.\n                for subtype in parent.get_enumerated_subtypes():\n                    if subtype.data_type == struct_type:\n                        txt = '@property {\\'%s\\'} [.tag] - Tag identifying ' \\\n                            'this subtype variant. This field is only ' \\\n                            'present when needed to discriminate ' \\\n                            'between multiple possible subtypes.' % \\\n                            subtype.name\n                        self.emit_wrapped_text(txt)\n                        break\n\n        for param_name, param_type, param_docstring in extra_parameters:\n            param_docstring = ' - %s' % param_docstring if param_docstring else ''\n            self.emit_wrapped_text(\n                '@property {%s} %s%s' % (\n                    param_type,\n                    param_name,\n                    param_docstring,\n                ),\n                prefix=' * ',\n            )\n\n        # NOTE: JSDoc @typedef does not support inheritance. Using @class would be inappropriate,\n        # since these are not nominal types backed by a constructor. Thus, we emit all_fields,\n        # which includes fields on parent types.\n        for field in struct_type.all_fields:\n            field_doc = ' - ' + field.doc if field.doc else ''\n            field_type, nullable, _ = unwrap(field.data_type)\n            field_js_type = fmt_type(field_type)\n            # Translate nullable types into optional properties.\n            field_name = '[' + field.name + ']' if nullable else field.name\n            self.emit_wrapped_text(\n                '@property {%s} %s%s' % (\n                    field_js_type,\n                    field_name,\n                    self.process_doc(field_doc, self._docf),\n                ),\n                prefix=' * ',\n            )\n\n        self.emit(' */')", "response": "Emits a JSDoc typedef for a struct."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nemitting a JSDoc typedef for a union type.", "response": "def _generate_union(self, union_type):\n        \"\"\"\n        Emits a JSDoc @typedef for a union type.\n        \"\"\"\n        union_name = fmt_type_name(union_type)\n        self._emit_jsdoc_header(union_type.doc)\n        self.emit(' * @typedef {Object} %s' % union_name)\n        variant_types = []\n        for variant in union_type.all_fields:\n            variant_types.append(\"'%s'\" % variant.name)\n            variant_data_type, _, _ = unwrap(variant.data_type)\n            # Don't emit fields for void types.\n            if not is_void_type(variant_data_type):\n                variant_doc = ' - Available if .tag is %s.' % variant.name\n                if variant.doc:\n                    variant_doc += ' ' + variant.doc\n                self.emit_wrapped_text(\n                    '@property {%s} [%s]%s' % (\n                        fmt_type(variant_data_type),\n                        variant.name,\n                        variant_doc,\n                    ),\n                    prefix=' * ',\n                )\n        jsdoc_tag_union = fmt_jsdoc_union(variant_types)\n        self.emit(' * @property {%s} .tag - Tag identifying the union variant.' % jsdoc_tag_union)\n        self.emit(' */')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_route_name_conflict(namespace):\n\n    route_by_name = {}\n    for route in namespace.routes:\n        route_name = fmt_func(route.name, version=route.version)\n        if route_name in route_by_name:\n            other_route = route_by_name[route_name]\n            raise RuntimeError(\n                'There is a name conflict between {!r} and {!r}'.format(other_route, route))\n        route_by_name[route_name] = route", "response": "Check name conflicts among generated route definitions. Raise a runtime exception when name conflicts are encountered."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the list of imports that are needed for the referenced namespaces.", "response": "def generate_imports_for_referenced_namespaces(\n        backend, namespace, insert_type_ignore=False):\n    # type: (Backend, ApiNamespace, bool) -> None\n    \"\"\"\n    Both the true Python backend and the Python PEP 484 Type Stub backend have\n    to perform the same imports.\n\n    :param insert_type_ignore: add a MyPy type-ignore comment to the imports in\n        the except: clause.\n    \"\"\"\n\n    imported_namespaces = namespace.get_imported_namespaces(consider_annotation_types=True)\n    if not imported_namespaces:\n        return\n\n    type_ignore_comment = TYPE_IGNORE_COMMENT if insert_type_ignore else \"\"\n\n    backend.emit('try:')\n    with backend.indent():\n        backend.emit('from . import (')\n        with backend.indent():\n            for ns in imported_namespaces:\n                backend.emit(fmt_namespace(ns.name) + ',')\n        backend.emit(')')\n    backend.emit('except (ImportError, SystemError, ValueError):')\n    # Fallback if imported from outside a package.\n    with backend.indent():\n        for ns in imported_namespaces:\n            backend.emit('import {namespace_name}{type_ignore_comment}'.format(\n                namespace_name=fmt_namespace(ns.name),\n                type_ignore_comment=type_ignore_comment\n            ))\n    backend.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a name that can be used to reference name in namespace name_ns from source_ns.", "response": "def prefix_with_ns_if_necessary(name, name_ns, source_ns):\n    # type: (typing.Text, ApiNamespace, ApiNamespace) -> typing.Text\n    \"\"\"\n    Returns a name that can be used to reference `name` in namespace `name_ns`\n    from `source_ns`.\n\n    If `source_ns` and `name_ns` are the same, that's just `name`. Otherwise\n    it's `name_ns`.`name`.\n    \"\"\"\n    if source_ns == name_ns:\n        return name\n    return '{}.{}'.format(fmt_namespace(name_ns.name), name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the name of the Python class that maps to a user - defined type.", "response": "def class_name_for_data_type(data_type, ns=None):\n    \"\"\"\n    Returns the name of the Python class that maps to a user-defined type.\n    The name is identical to the name in the spec.\n\n    If ``ns`` is set to a Namespace and the namespace of `data_type` does\n    not match, then a namespace prefix is added to the returned name.\n    For example, ``foreign_ns.TypeName``.\n    \"\"\"\n    assert is_user_defined_type(data_type) or is_alias(data_type), \\\n        'Expected composite type, got %r' % type(data_type)\n    name = fmt_class(data_type.name)\n    if ns:\n        return prefix_with_ns_if_necessary(name, data_type.namespace, ns)\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the class name for the given annotation type.", "response": "def class_name_for_annotation_type(annotation_type, ns=None):\n    \"\"\"\n    Same as class_name_for_data_type, but works with annotation types.\n    \"\"\"\n    assert isinstance(annotation_type, AnnotationType)\n    name = fmt_class(annotation_type.name)\n    if ns:\n        return prefix_with_ns_if_necessary(name, annotation_type.namespace, ns)\n    return name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef main():\n\n    if '--' in sys.argv:\n        cli_args = sys.argv[1:sys.argv.index('--')]\n        backend_args = sys.argv[sys.argv.index('--') + 1:]\n    else:\n        cli_args = sys.argv[1:]\n        backend_args = []\n\n    args = _cmdline_parser.parse_args(cli_args)\n    debug = False\n    if args.verbose is None:\n        logging_level = logging.WARNING\n    elif args.verbose == 1:\n        logging_level = logging.INFO\n    elif args.verbose == 2:\n        logging_level = logging.DEBUG\n        debug = True\n    else:\n        print('error: I can only be so garrulous, try -vv.', file=sys.stderr)\n        sys.exit(1)\n\n    logging.basicConfig(level=logging_level)\n\n    if args.spec and args.spec[0].startswith('+') and args.spec[0].endswith('.py'):\n        # Hack: Special case for defining a spec in Python for testing purposes\n        # Use this if you want to define a Stone spec using a Python module.\n        # The module should should contain an api variable that references a\n        # :class:`stone.api.Api` object.\n        try:\n            api = imp.load_source('api', args.api[0]).api  # pylint: disable=redefined-outer-name\n        except ImportError as e:\n            print('error: Could not import API description due to:',\n                  e, file=sys.stderr)\n            sys.exit(1)\n    else:\n        if args.spec:\n            specs = []\n            read_from_stdin = False\n            for spec_path in args.spec:\n                if spec_path == '-':\n                    read_from_stdin = True\n                elif not spec_path.endswith('.stone'):\n                    print(\"error: Specification '%s' must have a .stone extension.\"\n                          % spec_path,\n                          file=sys.stderr)\n                    sys.exit(1)\n                elif not os.path.exists(spec_path):\n                    print(\"error: Specification '%s' cannot be found.\" % spec_path,\n                          file=sys.stderr)\n                    sys.exit(1)\n                else:\n                    with open(spec_path) as f:\n                        specs.append((spec_path, f.read()))\n            if read_from_stdin and specs:\n                print(\"error: Do not specify stdin and specification files \"\n                      \"simultaneously.\", file=sys.stderr)\n                sys.exit(1)\n\n        if not args.spec or read_from_stdin:\n            specs = []\n            if debug:\n                print('Reading specification from stdin.')\n\n            if six.PY2:\n                UTF8Reader = codecs.getreader('utf8')\n                sys.stdin = UTF8Reader(sys.stdin)\n                stdin_text = sys.stdin.read()\n            else:\n                stdin_buffer = sys.stdin.buffer  # pylint: disable=no-member,useless-suppression\n                stdin_text = io.TextIOWrapper(stdin_buffer, encoding='utf-8').read()\n\n            parts = stdin_text.split('namespace')\n            if len(parts) == 1:\n                specs.append(('stdin.1', parts[0]))\n            else:\n                specs.append(\n                    ('stdin.1', '%snamespace%s' % (parts.pop(0), parts.pop(0))))\n                while parts:\n                    specs.append(('stdin.%s' % (len(specs) + 1),\n                                  'namespace%s' % parts.pop(0)))\n\n        if args.filter_by_route_attr:\n            route_filter, route_filter_errors = parse_route_attr_filter(\n                args.filter_by_route_attr, debug)\n            if route_filter_errors:\n                print('Error(s) in route filter:', file=sys.stderr)\n                for err in route_filter_errors:\n                    print(err, file=sys.stderr)\n                sys.exit(1)\n\n        else:\n            route_filter = None\n\n        if args.route_whitelist_filter:\n            with open(args.route_whitelist_filter) as f:\n                route_whitelist_filter = json.loads(f.read())\n        else:\n            route_whitelist_filter = None\n\n        try:\n            # TODO: Needs version\n            api = specs_to_ir(specs, debug=debug,\n                              route_whitelist_filter=route_whitelist_filter)\n        except InvalidSpec as e:\n            print('%s:%s: error: %s' % (e.path, e.lineno, e.msg), file=sys.stderr)\n            if debug:\n                print('A traceback is included below in case this is a bug in '\n                      'Stone.\\n', traceback.format_exc(), file=sys.stderr)\n            sys.exit(1)\n        if api is None:\n            print('You must fix the above parsing errors for generation to '\n                  'continue.', file=sys.stderr)\n            sys.exit(1)\n\n        if args.whitelist_namespace_routes:\n            for namespace_name in args.whitelist_namespace_routes:\n                if namespace_name not in api.namespaces:\n                    print('error: Whitelisted namespace missing from spec: %s' %\n                          namespace_name, file=sys.stderr)\n                    sys.exit(1)\n            for namespace in api.namespaces.values():\n                if namespace.name not in args.whitelist_namespace_routes:\n                    namespace.routes = []\n                    namespace.route_by_name = {}\n                    namespace.routes_by_name = {}\n\n        if args.blacklist_namespace_routes:\n            for namespace_name in args.blacklist_namespace_routes:\n                if namespace_name not in api.namespaces:\n                    print('error: Blacklisted namespace missing from spec: %s' %\n                          namespace_name, file=sys.stderr)\n                    sys.exit(1)\n                else:\n                    namespace = api.namespaces[namespace_name]\n                    namespace.routes = []\n                    namespace.route_by_name = {}\n                    namespace.routes_by_name = {}\n\n        if route_filter:\n            for namespace in api.namespaces.values():\n                filtered_routes = []\n                for route in namespace.routes:\n                    if route_filter.eval(route):\n                        filtered_routes.append(route)\n\n                namespace.routes = []\n                namespace.route_by_name = {}\n                namespace.routes_by_name = {}\n                for route in filtered_routes:\n                    namespace.add_route(route)\n\n        if args.attribute:\n            attrs = set(args.attribute)\n            if ':all' in attrs:\n                attrs = {field.name for field in api.route_schema.fields}\n        else:\n            attrs = set()\n\n        for namespace in api.namespaces.values():\n            for route in namespace.routes:\n                for k in list(route.attrs.keys()):\n                    if k not in attrs:\n                        del route.attrs[k]\n\n        # Remove attrs that weren't specified from the route schema\n        for field in api.route_schema.fields[:]:\n            if field.name not in attrs:\n                api.route_schema.fields.remove(field)\n                del api.route_schema._fields_by_name[field.name]\n            else:\n                attrs.remove(field.name)\n\n        # Error if specified attr isn't even a field in the route schema\n        if attrs:\n            attr = attrs.pop()\n            print('error: Attribute not defined in stone_cfg.Route: %s' %\n                  attr, file=sys.stderr)\n            sys.exit(1)\n\n    if args.backend in _builtin_backends:\n        backend_module = __import__(\n            'stone.backends.%s' % args.backend, fromlist=[''])\n    elif not os.path.exists(args.backend):\n        print(\"error: Backend '%s' cannot be found.\" % args.backend,\n              file=sys.stderr)\n        sys.exit(1)\n    elif not os.path.isfile(args.backend):\n        print(\"error: Backend '%s' must be a file.\" % args.backend,\n              file=sys.stderr)\n        sys.exit(1)\n    elif not Compiler.is_stone_backend(args.backend):\n        print(\"error: Backend '%s' must have a .stoneg.py extension.\" %\n              args.backend, file=sys.stderr)\n        sys.exit(1)\n    else:\n        # A bit hacky, but we add the folder that the backend is in to our\n        # python path to support the case where the backend imports other\n        # files in its local directory.\n        new_python_path = os.path.dirname(args.backend)\n        if new_python_path not in sys.path:\n            sys.path.append(new_python_path)\n        try:\n            backend_module = imp.load_source('user_backend', args.backend)\n        except Exception:\n            print(\"error: Importing backend '%s' module raised an exception:\" %\n                  args.backend, file=sys.stderr)\n            raise\n\n    c = Compiler(\n        api,\n        backend_module,\n        backend_args,\n        args.output,\n        clean_build=args.clean_build,\n    )\n    try:\n        c.build()\n    except BackendException as e:\n        print('%s: error: %s raised an exception:\\n%s' %\n              (args.backend, e.backend_name, e.traceback),\n              file=sys.stderr)\n        sys.exit(1)\n\n    if not sys.argv[0].endswith('stone'):\n        # If we aren't running from an entry_point, then return api to make it\n        # easier to do debugging.\n        return api", "response": "The entry point for the command line interface."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a collection of Stone specifications into a intermediate base object that can be used by Stone backends.", "response": "def specs_to_ir(specs, version='0.1b1', debug=False, route_whitelist_filter=None):\n    \"\"\"\n    Converts a collection of Stone specifications into the intermediate\n    representation used by Stone backends.\n\n    The process is: Lexer -> Parser -> Semantic Analyzer -> IR Generator.\n\n    The code is structured as:\n        1. Parser (Lexer embedded within)\n        2. IR Generator (Semantic Analyzer embedded within)\n\n    :type specs: List[Tuple[path: str, text: str]]\n    :param specs: `path` is never accessed and is only used to report the\n        location of a bad spec to the user. `spec` is the text contents of\n        a spec (.stone) file.\n\n    :raises: InvalidSpec\n\n    :returns: stone.ir.Api\n    \"\"\"\n\n    parser_factory = ParserFactory(debug=debug)\n    partial_asts = []\n\n    for path, text in specs:\n        logger.info('Parsing spec %s', path)\n        parser = parser_factory.get_parser()\n        if debug:\n            parser.test_lexing(text)\n\n        partial_ast = parser.parse(text, path)\n\n        if parser.got_errors_parsing():\n            # TODO(kelkabany): Show more than one error at a time.\n            msg, lineno, path = parser.get_errors()[0]\n            raise InvalidSpec(msg, lineno, path)\n        elif len(partial_ast) == 0:\n            logger.info('Empty spec: %s', path)\n        else:\n            partial_asts.append(partial_ast)\n\n    return IRGenerator(partial_asts, version, debug=debug,\n                       route_whitelist_filter=route_whitelist_filter).generate_IR()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fmt_type_name(data_type):\n    if is_user_defined_type(data_type):\n        return fmt_pascal('%s%s' % (data_type.namespace.name, data_type.name))\n    else:\n        fmted_type = _base_type_table.get(data_type.__class__, 'Object')\n        if is_list_type(data_type):\n            fmted_type += '.<' + fmt_type(data_type.data_type) + '>'\n        return fmted_type", "response": "Formats the JSDoc name for the given data type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nformatting a data type into a JSDoc annotation for a data type.", "response": "def fmt_type(data_type):\n    \"\"\"\n    Returns a JSDoc annotation for a data type.\n    May contain a union of enumerated subtypes.\n    \"\"\"\n    if is_struct_type(data_type) and data_type.has_enumerated_subtypes():\n        possible_types = []\n        possible_subtypes = data_type.get_all_subtypes_with_tags()\n        for _, subtype in possible_subtypes:\n            possible_types.append(fmt_type_name(subtype))\n        if data_type.is_catch_all():\n            possible_types.append(fmt_type_name(data_type))\n        return fmt_jsdoc_union(possible_types)\n    else:\n        return fmt_type_name(data_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_extra_args(self, api, extra_args_raw):\n        extra_args = {}\n\n        def invalid(msg, extra_arg_raw):\n            print('Invalid --extra-arg:%s: %s' % (msg, extra_arg_raw),\n                  file=sys.stderr)\n            sys.exit(1)\n\n        for extra_arg_raw in extra_args_raw:\n            try:\n                extra_arg = json.loads(extra_arg_raw)\n            except ValueError as e:\n                invalid(str(e), extra_arg_raw)\n\n            # Validate extra_arg JSON blob\n            if 'match' not in extra_arg:\n                invalid('No match key', extra_arg_raw)\n            elif (not isinstance(extra_arg['match'], list) or\n                  len(extra_arg['match']) != 2):\n                invalid('match key is not a list of two strings', extra_arg_raw)\n            elif (not isinstance(extra_arg['match'][0], six.text_type) or\n                  not isinstance(extra_arg['match'][1], six.text_type)):\n                print(type(extra_arg['match'][0]))\n                invalid('match values are not strings', extra_arg_raw)\n            elif 'arg_name' not in extra_arg:\n                invalid('No arg_name key', extra_arg_raw)\n            elif not isinstance(extra_arg['arg_name'], six.text_type):\n                invalid('arg_name is not a string', extra_arg_raw)\n            elif 'arg_type' not in extra_arg:\n                invalid('No arg_type key', extra_arg_raw)\n            elif not isinstance(extra_arg['arg_type'], six.text_type):\n                invalid('arg_type is not a string', extra_arg_raw)\n            elif ('arg_docstring' in extra_arg and\n                  not isinstance(extra_arg['arg_docstring'], six.text_type)):\n                invalid('arg_docstring is not a string', extra_arg_raw)\n\n            attr_key, attr_val = extra_arg['match'][0], extra_arg['match'][1]\n            extra_args.setdefault(attr_key, {})[attr_val] = \\\n                (extra_arg['arg_name'], extra_arg['arg_type'],\n                 extra_arg.get('arg_docstring'))\n\n        # Extra arguments, keyed on data type objects.\n        extra_args_for_types = {}\n        # Locate data types that contain extra arguments\n        for namespace in api.namespaces.values():\n            for route in namespace.routes:\n                extra_parameters = []\n                if is_user_defined_type(route.arg_data_type):\n                    for attr_key in route.attrs:\n                        if attr_key not in extra_args:\n                            continue\n                        attr_val = route.attrs[attr_key]\n                        if attr_val in extra_args[attr_key]:\n                            extra_parameters.append(extra_args[attr_key][attr_val])\n                if len(extra_parameters) > 0:\n                    extra_args_for_types[route.arg_data_type] = extra_parameters\n\n        return extra_args_for_types", "response": "Parses extra arguments into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_type(self, data_type, indent_spaces, extra_args):\n        if is_alias(data_type):\n            self._generate_alias_type(data_type)\n        elif is_struct_type(data_type):\n            self._generate_struct_type(data_type, indent_spaces, extra_args)\n        elif is_union_type(data_type):\n            self._generate_union_type(data_type, indent_spaces)", "response": "Generates a TypeScript type for the given data type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a TypeScript type for a stone alias.", "response": "def _generate_alias_type(self, alias_type):\n        \"\"\"\n        Generates a TypeScript type for a stone alias.\n        \"\"\"\n        namespace = alias_type.namespace\n        self.emit('export type %s = %s;' % (fmt_type_name(alias_type, namespace),\n                                     fmt_type_name(alias_type.data_type, namespace)))\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a TypeScript interface for a struct type.", "response": "def _generate_struct_type(self, struct_type, indent_spaces, extra_parameters):\n        \"\"\"\n        Generates a TypeScript interface for a stone struct.\n        \"\"\"\n        namespace = struct_type.namespace\n        if struct_type.doc:\n            self._emit_tsdoc_header(struct_type.doc)\n        parent_type = struct_type.parent_type\n        extends_line = ' extends %s' % fmt_type_name(parent_type, namespace) if parent_type else ''\n        self.emit('export interface %s%s {' % (fmt_type_name(struct_type, namespace), extends_line))\n        with self.indent(dent=indent_spaces):\n\n            for param_name, param_type, param_docstring in extra_parameters:\n                if param_docstring:\n                    self._emit_tsdoc_header(param_docstring)\n                self.emit('%s: %s;' % (param_name, param_type))\n\n            for field in struct_type.fields:\n                doc = field.doc\n                field_type, nullable = unwrap_nullable(field.data_type)\n                field_ts_type = fmt_type(field_type, namespace)\n                optional = nullable or field.has_default\n                if field.has_default:\n                    # doc may be None. If it is not empty, add newlines\n                    # before appending to it.\n                    doc = doc + '\\n\\n' if doc else ''\n                    doc = \"Defaults to %s.\" % field.default\n\n                if doc:\n                    self._emit_tsdoc_header(doc)\n                # Translate nullable types into optional properties.\n                field_name = '%s?' % field.name if optional else field.name\n                self.emit('%s: %s;' % (field_name, field_ts_type))\n\n        self.emit('}')\n        self.emit()\n\n        # Some structs can explicitly list their subtypes. These structs have a .tag field that\n        # indicate which subtype they are, which is only present when a type reference is\n        # ambiguous.\n        # Emit a special interface that contains this extra field, and refer to it whenever we\n        # encounter a reference to a type with enumerated subtypes.\n        if struct_type.is_member_of_enumerated_subtypes_tree():\n            if struct_type.has_enumerated_subtypes():\n                # This struct is the parent to multiple subtypes. Determine all of the possible\n                # values of the .tag property.\n                tag_values = []\n                for tags, _ in struct_type.get_all_subtypes_with_tags():\n                    for tag in tags:\n                        tag_values.append('\"%s\"' % tag)\n\n                tag_union = fmt_union(tag_values)\n                self._emit_tsdoc_header('Reference to the %s polymorphic type. Contains a .tag '\n                                        'property to let you discriminate between possible '\n                                        'subtypes.' % fmt_type_name(struct_type, namespace))\n                self.emit('export interface %s extends %s {' %\n                          (fmt_polymorphic_type_reference(struct_type, namespace),\n                           fmt_type_name(struct_type, namespace)))\n\n                with self.indent(dent=indent_spaces):\n                    self._emit_tsdoc_header('Tag identifying the subtype variant.')\n                    self.emit('\\'.tag\\': %s;' % tag_union)\n\n                self.emit('}')\n                self.emit()\n            else:\n                # This struct is a particular subtype. Find the applicable .tag value from the\n                # parent type, which may be an arbitrary number of steps up the inheritance\n                # hierarchy.\n                parent = struct_type.parent_type\n                while not parent.has_enumerated_subtypes():\n                    parent = parent.parent_type\n                # parent now contains the closest parent type in the inheritance hierarchy that has\n                # enumerated subtypes. Determine which subtype this is.\n                for subtype in parent.get_enumerated_subtypes():\n                    if subtype.data_type == struct_type:\n                        self._emit_tsdoc_header('Reference to the %s type, identified by the '\n                                                'value of the .tag property.' %\n                                                fmt_type_name(struct_type, namespace))\n                        self.emit('export interface %s extends %s {' %\n                                  (fmt_polymorphic_type_reference(struct_type, namespace),\n                                   fmt_type_name(struct_type, namespace)))\n\n                        with self.indent(dent=indent_spaces):\n                            self._emit_tsdoc_header('Tag identifying this subtype variant. This '\n                                                    'field is only present when needed to '\n                                                    'discriminate between multiple possible '\n                                                    'subtypes.')\n                            self.emit_wrapped_text('\\'.tag\\': \\'%s\\';' % subtype.name)\n\n                        self.emit('}')\n                        self.emit()\n                        break"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _generate_union_type(self, union_type, indent_spaces):\n        # Emit an interface for each variant. TypeScript 2.0 supports these tagged unions.\n        # https://github.com/Microsoft/TypeScript/wiki/What%27s-new-in-TypeScript#tagged-union-types\n        parent_type = union_type.parent_type\n        namespace = union_type.namespace\n        union_type_name = fmt_type_name(union_type, namespace)\n        variant_type_names = []\n        if parent_type:\n            variant_type_names.append(fmt_type_name(parent_type, namespace))\n\n        def _is_struct_without_enumerated_subtypes(data_type):\n            \"\"\"\n            :param data_type: any data type.\n            :return: True if the given data type is a struct which has no enumerated subtypes.\n            \"\"\"\n            return is_struct_type(data_type) and (\n                not data_type.has_enumerated_subtypes())\n\n        for variant in union_type.fields:\n            if variant.doc:\n                self._emit_tsdoc_header(variant.doc)\n            variant_name = '%s%s' % (union_type_name, fmt_pascal(variant.name))\n            variant_type_names.append(variant_name)\n\n            is_struct_without_enumerated_subtypes = _is_struct_without_enumerated_subtypes(\n                variant.data_type)\n\n            if is_struct_without_enumerated_subtypes:\n                self.emit('export interface %s extends %s {' % (\n                    variant_name, fmt_type(variant.data_type, namespace)))\n            else:\n                self.emit('export interface %s {' % variant_name)\n\n            with self.indent(dent=indent_spaces):\n                # Since field contains non-alphanumeric character, we need to enclose\n                # it in quotation marks.\n                self.emit(\"'.tag': '%s';\" % variant.name)\n                if is_void_type(variant.data_type) is False and (\n                    not is_struct_without_enumerated_subtypes\n                ):\n                    self.emit(\"%s: %s;\" % (variant.name, fmt_type(variant.data_type, namespace)))\n            self.emit('}')\n            self.emit()\n\n        if union_type.doc:\n            self._emit_tsdoc_header(union_type.doc)\n        self.emit('export type %s = %s;' % (union_type_name, ' | '.join(variant_type_names)))\n        self.emit()", "response": "Generates a TypeScript interface for a union type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a dict returns a new dict with all the same key / values except for keys that had values of None.", "response": "def filter_out_none_valued_keys(self, d):\n        # type: (typing.Dict[K, V]) -> typing.Dict[K, V]\n        \"\"\"Given a dict, returns a new dict with all the same key/values except\n        for keys that had values of None.\"\"\"\n        new_d = {}\n        for k, v in d.items():\n            if v is not None:\n                new_d[k] = v\n        return new_d"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef generate_multiline_list(\n        self,\n        items,               # type: typing.List[typing.Text]\n        before='',           # type: typing.Text\n        after='',            # type: typing.Text\n        delim=('(', ')'),    # type: DelimTuple\n        compact=True,        # type: bool\n        sep=',',             # type: typing.Text\n        skip_last_sep=False  # type: bool\n    ):\n        # type: (...) -> None\n        \"\"\"\n        Given a list of items, emits one item per line.\n\n        This is convenient for function prototypes and invocations, as well as\n        for instantiating arrays, sets, and maps in some languages.\n\n        TODO(kelkabany): A backend that uses tabs cannot be used with this\n            if compact is false.\n\n        Args:\n            items (list[str]): Should contain the items to generate a list of.\n            before (str): The string to come before the list of items.\n            after (str): The string to follow the list of items.\n            delim (str, str): The first element is added immediately following\n                `before`. The second element is added prior to `after`.\n            compact (bool): In compact mode, the enclosing parentheses are on\n                the same lines as the first and last list item.\n            sep (str): The string that follows each list item when compact is\n                true. If compact is false, the separator is omitted for the\n                last item.\n            skip_last_sep (bool): When compact is false, whether the last line\n                should have a trailing separator. Ignored when compact is true.\n        \"\"\"\n        assert len(delim) == 2 and isinstance(delim[0], six.text_type) and \\\n            isinstance(delim[1], six.text_type), 'delim must be a tuple of two unicode strings.'\n\n        if len(items) == 0:\n            self.emit(before + delim[0] + delim[1] + after)\n            return\n        if len(items) == 1:\n            self.emit(before + delim[0] + items[0] + delim[1] + after)\n            return\n\n        if compact:\n            self.emit(before + delim[0] + items[0] + sep)\n            def emit_list(items):\n                items = items[1:]\n                for (i, item) in enumerate(items):\n                    if i == len(items) - 1:\n                        self.emit(item + delim[1] + after)\n                    else:\n                        self.emit(item + sep)\n            if before or delim[0]:\n                with self.indent(len(before) + len(delim[0])):\n                    emit_list(items)\n            else:\n                emit_list(items)\n        else:\n            if before or delim[0]:\n                self.emit(before + delim[0])\n            with self.indent():\n                for (i, item) in enumerate(items):\n                    if i == len(items) - 1 and skip_last_sep:\n                        self.emit(item)\n                    else:\n                        self.emit(item + sep)\n            if delim[1] or after:\n                self.emit(delim[1] + after)\n            elif delim[1]:\n                self.emit(delim[1])", "response": "Generates a multiline list of items."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef block(\n        self,\n        before='',         # type: typing.Text\n        after='',          # type: typing.Text\n        delim=('{', '}'),  # type: DelimTuple\n        dent=None,         # type: typing.Optional[int]\n        allman=False       # type: bool\n    ):\n        # type: (...) -> typing.Iterator[None]\n        \"\"\"\n        A context manager that emits configurable lines before and after an\n        indented block of text.\n\n        This is convenient for class and function definitions in some\n        languages.\n\n        Args:\n            before (str): The string to be output in the first line which is\n                not indented..\n            after (str): The string to be output in the last line which is\n                not indented.\n            delim (str, str): The first element is added immediately following\n                `before` and a space. The second element is added prior to a\n                space and then `after`.\n            dent (int): The amount to indent the block. If none, the default\n                indentation increment is used (four spaces or one tab).\n            allman (bool): Indicates whether to use `Allman` style indentation,\n                or the default `K&R` style. If there is no `before` string this\n                is ignored. For more details about indent styles see\n                http://en.wikipedia.org/wiki/Indent_style\n        \"\"\"\n        assert len(delim) == 2, 'delim must be a tuple of length 2'\n        assert (isinstance(delim[0], (six.text_type, type(None))) and\n                isinstance(delim[1], (six.text_type, type(None)))), (\n            'delim must be a tuple of two optional strings.')\n\n        if before and not allman:\n            if delim[0] is not None:\n                self.emit('{} {}'.format(before, delim[0]))\n            else:\n                self.emit(before)\n        else:\n            if before:\n                self.emit(before)\n            if delim[0] is not None:\n                self.emit(delim[0])\n\n        with self.indent(dent):\n            yield\n\n        if delim[1] is not None:\n            self.emit(delim[1] + after)\n        else:\n            self.emit(after)", "response": "A context manager that emits configurable lines before and after an arbitrary number of lines."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a TypeScript type name for the given data type.", "response": "def fmt_type_name(data_type, inside_namespace=None):\n    \"\"\"\n    Produces a TypeScript type name for the given data type.\n    inside_namespace should be set to the namespace that the reference\n    occurs in, or None if this parameter is not relevant.\n    \"\"\"\n    if is_user_defined_type(data_type) or is_alias(data_type):\n        if data_type.namespace == inside_namespace:\n            return data_type.name\n        else:\n            return '%s.%s' % (data_type.namespace.name, data_type.name)\n    else:\n        fmted_type = _base_type_table.get(data_type.__class__, 'Object')\n        if is_list_type(data_type):\n            fmted_type += '<' + fmt_type(data_type.data_type, inside_namespace) + '>'\n        return fmted_type"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats a TypeScript type annotation for a data type.", "response": "def fmt_type(data_type, inside_namespace=None):\n    \"\"\"\n    Returns a TypeScript type annotation for a data type.\n    May contain a union of enumerated subtypes.\n    inside_namespace should be set to the namespace that the type reference\n    occurs in, or None if this parameter is not relevant.\n    \"\"\"\n    if is_struct_type(data_type) and data_type.has_enumerated_subtypes():\n        possible_types = []\n        possible_subtypes = data_type.get_all_subtypes_with_tags()\n        for _, subtype in possible_subtypes:\n            possible_types.append(fmt_polymorphic_type_reference(subtype, inside_namespace))\n        if data_type.is_catch_all():\n            possible_types.append(fmt_polymorphic_type_reference(data_type, inside_namespace))\n        return fmt_union(possible_types)\n    else:\n        return fmt_type_name(data_type, inside_namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nformats a documentation reference tag.", "response": "def fmt_tag(cur_namespace, tag, val):\n    \"\"\"\n    Processes a documentation reference.\n    \"\"\"\n    if tag == 'type':\n        fq_val = val\n        if '.' not in val and cur_namespace is not None:\n            fq_val = cur_namespace.name + '.' + fq_val\n        return fq_val\n    elif tag == 'route':\n        if ':' in val:\n            val, version = val.split(':', 1)\n            version = int(version)\n        else:\n            version = 1\n        return fmt_func(val, version) + \"()\"\n    elif tag == 'link':\n        anchor, link = val.rsplit(' ', 1)\n        # There's no way to have links in TSDoc, so simply use JSDoc's formatting.\n        # It's entirely possible some editors support this.\n        return '[%s]{@link %s}' % (anchor, link)\n    elif tag == 'val':\n        # Value types seem to match JavaScript (true, false, null)\n        return val\n    elif tag == 'field':\n        return val\n    else:\n        raise RuntimeError('Unknown doc ref tag %r' % tag)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a documentation string parse it and return all references to other data types.", "response": "def parse_data_types_from_doc_ref(api, doc, namespace_context, ignore_missing_entries=False):\n    \"\"\"\n    Given a documentation string, parse it and return all references to other\n    data types. If there are references to routes, include also the data types of\n    those routes.\n\n    Args:\n    - api: The API containing this doc ref.\n    - doc: The documentation string to parse.\n    - namespace_context: The namespace name relative to this documentation.\n    - ignore_missing_entries: If set, this will skip references to nonexistent data types instead\n                              of raising an exception.\n\n    Returns:\n    - a list of referenced data types\n    \"\"\"\n    output = []\n    data_types, routes_by_ns = parse_data_types_and_routes_from_doc_ref(\n        api, doc, namespace_context, ignore_missing_entries=ignore_missing_entries)\n    for d in data_types:\n        output.append(d)\n    for ns_name, routes in routes_by_ns.items():\n        try:\n            ns = api.namespaces[ns_name]\n            for r in routes:\n                for d in ns.get_route_io_data_types_for_route(r):\n                    output.append(d)\n        except KeyError:\n            if not ignore_missing_entries:\n                raise\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_route_name_and_version(route_repr):\n    if ':' in route_repr:\n        route_name, version = route_repr.split(':', 1)\n        try:\n            version = int(version)\n        except ValueError:\n            raise ValueError('Invalid route representation: {}'.format(route_repr))\n    else:\n        route_name = route_repr\n        version = 1\n    return route_name, version", "response": "Parse a route representation string and return the route name and version number."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngives a documentation string parse it and return all references to other data types and routes.", "response": "def parse_data_types_and_routes_from_doc_ref(\n    api,\n    doc,\n    namespace_context,\n    ignore_missing_entries=False\n):\n    \"\"\"\n    Given a documentation string, parse it and return all references to other\n    data types and routes.\n\n    Args:\n    - api: The API containing this doc ref.\n    - doc: The documentation string to parse.\n    - namespace_context: The namespace name relative to this documentation.\n    - ignore_missing_entries: If set, this will skip references to nonexistent data types instead\n                              of raising an exception.\n\n    Returns:\n    - a tuple of referenced data types and routes\n    \"\"\"\n    assert doc is not None\n    data_types = set()\n    routes = defaultdict(set)\n\n    for match in doc_ref_re.finditer(doc):\n        try:\n            tag = match.group('tag')\n            val = match.group('val')\n            supplied_namespace = api.namespaces[namespace_context]\n            if tag == 'field':\n                if '.' in val:\n                    type_name, __ = val.split('.', 1)\n                    doc_type = supplied_namespace.data_type_by_name[type_name]\n                    data_types.add(doc_type)\n                else:\n                    pass  # no action required, because we must be referencing the same object\n            elif tag == 'route':\n                if '.' in val:\n                    namespace_name, val = val.split('.', 1)\n                    namespace = api.namespaces[namespace_name]\n                else:\n                    namespace = supplied_namespace\n\n                try:\n                    route_name, version = parse_route_name_and_version(val)\n                except ValueError as ex:\n                    raise KeyError(str(ex))\n\n                route = namespace.routes_by_name[route_name].at_version[version]\n                routes[namespace.name].add(route)\n            elif tag == 'type':\n                if '.' in val:\n                    namespace_name, val = val.split('.', 1)\n                    doc_type = api.namespaces[namespace_name].data_type_by_name[val]\n                    data_types.add(doc_type)\n                else:\n                    doc_type = supplied_namespace.data_type_by_name[val]\n                    data_types.add(doc_type)\n        except KeyError:\n            if not ignore_missing_entries:\n                raise\n    return data_types, routes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing the text of each spec and returns an API description. Returns None if an error was encountered during parsing.", "response": "def generate_IR(self):\n        \"\"\"Parses the text of each spec and returns an API description. Returns\n        None if an error was encountered during parsing.\"\"\"\n\n        raw_api = []\n        for partial_ast in self._partial_asts:\n            namespace_ast_node = self._extract_namespace_ast_node(partial_ast)\n            namespace = self.api.ensure_namespace(namespace_ast_node.name)\n            base_name = self._get_base_name(namespace.name, namespace.name)\n            self._item_by_canonical_name[base_name] = namespace_ast_node\n            if namespace_ast_node.doc is not None:\n                namespace.add_doc(namespace_ast_node.doc)\n            raw_api.append((namespace, partial_ast))\n            self._add_data_types_and_routes_to_api(namespace, partial_ast)\n\n        self._add_imports_to_env(raw_api)\n        self._merge_patches()\n        self._populate_type_attributes()\n        self._populate_field_defaults()\n        self._populate_enumerated_subtypes()\n        self._populate_route_attributes()\n        self._populate_examples()\n        self._validate_doc_refs()\n        self._validate_annotations()\n        if self._routes is not None:\n            self._filter_namespaces_by_route_whitelist()\n\n        self.api.normalize()\n\n        return self.api"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extract_namespace_ast_node(self, desc):\n        if len(desc) == 0 or not isinstance(desc[0], AstNamespace):\n            if self._debug:\n                self._logger.info('Description: %r', desc)\n            raise InvalidSpec('First declaration in a stone must be '\n                              'a namespace. Possibly caused by preceding '\n                              'errors.', desc[0].lineno, desc[0].path)\n        for item in desc[1:]:\n            if isinstance(item, AstNamespace):\n                raise InvalidSpec('Only one namespace declaration per file.',\n                                  item[0].lineno, item[0].path)\n        return desc.pop(0)", "response": "Extracts the namespace AST node from the spec."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _add_data_types_and_routes_to_api(self, namespace, desc):\n\n        env = self._get_or_create_env(namespace.name)\n\n        for item in desc:\n            if isinstance(item, AstTypeDef):\n                api_type = self._create_type(env, item)\n                namespace.add_data_type(api_type)\n                self._check_canonical_name_available(item, namespace.name)\n            elif isinstance(item, AstStructPatch) or isinstance(item, AstUnionPatch):\n                # Handle patches later.\n                base_name = self._get_base_name(item.name, namespace.name)\n                self._patch_data_by_canonical_name[base_name] = (item, namespace)\n            elif isinstance(item, AstRouteDef):\n                route = self._create_route(env, item)\n                namespace.add_route(route)\n                self._check_canonical_name_available(item, namespace.name, allow_duplicate=True)\n            elif isinstance(item, AstImport):\n                # Handle imports later.\n                pass\n            elif isinstance(item, AstAlias):\n                alias = self._create_alias(env, item)\n                namespace.add_alias(alias)\n                self._check_canonical_name_available(item, namespace.name)\n            elif isinstance(item, AstAnnotationDef):\n                annotation = self._create_annotation(env, item)\n                namespace.add_annotation(annotation)\n                self._check_canonical_name_available(item, namespace.name)\n            elif isinstance(item, AstAnnotationTypeDef):\n                annotation_type = self._create_annotation_type(env, item)\n                namespace.add_annotation_type(annotation_type)\n                self._check_canonical_name_available(item, namespace.name)\n            else:\n                raise AssertionError('Unknown AST node type %r' %\n                                     item.__class__.__name__)", "response": "Add data types and routes to the api namespace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _add_imports_to_env(self, raw_api):\n        for namespace, desc in raw_api:\n            for item in desc:\n                if isinstance(item, AstImport):\n                    if namespace.name == item.target:\n                        raise InvalidSpec('Cannot import current namespace.',\n                                          item.lineno, item.path)\n                    if item.target not in self.api.namespaces:\n                        raise InvalidSpec(\n                            'Namespace %s is not defined in any spec.' %\n                            quote(item.target),\n                            item.lineno, item.path)\n                    env = self._get_or_create_env(namespace.name)\n                    imported_env = self._get_or_create_env(item.target)\n                    if namespace.name in imported_env:\n                        # Block circular imports. The Python backend can't\n                        # easily generate code for circular references.\n                        raise InvalidSpec(\n                            'Circular import of namespaces %s and %s '\n                            'detected.' %\n                            (quote(namespace.name), quote(item.target)),\n                            item.lineno, item.path)\n                    env[item.target] = imported_env", "response": "Adds the imports to the environment."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _create_type(self, env, item):\n        if item.name in env:\n            existing_dt = env[item.name]\n            raise InvalidSpec(\n                'Symbol %s already defined (%s:%d).' %\n                (quote(item.name), existing_dt._ast_node.path,\n                 existing_dt._ast_node.lineno), item.lineno, item.path)\n        namespace = self.api.ensure_namespace(env.namespace_name)\n        if isinstance(item, AstStructDef):\n            try:\n                api_type = Struct(name=item.name, namespace=namespace,\n                                  ast_node=item)\n            except ParameterError as e:\n                raise InvalidSpec(\n                    'Bad declaration of %s: %s' % (quote(item.name), e.args[0]),\n                    item.lineno, item.path)\n        elif isinstance(item, AstUnionDef):\n            api_type = Union(\n                name=item.name, namespace=namespace, ast_node=item,\n                closed=item.closed)\n        else:\n            raise AssertionError('Unknown type definition %r' % type(item))\n\n        env[item.name] = api_type\n        return api_type", "response": "Create a forward reference for a union or struct."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninject object patches into their original object definitions.", "response": "def _merge_patches(self):\n        \"\"\"Injects object patches into their original object definitions.\"\"\"\n        for patched_item, patched_namespace in self._patch_data_by_canonical_name.values():\n            patched_item_base_name = self._get_base_name(patched_item.name, patched_namespace.name)\n            if patched_item_base_name not in self._item_by_canonical_name:\n                raise InvalidSpec('Patch {} must correspond to a pre-existing data_type.'.format(\n                    quote(patched_item.name)), patched_item.lineno, patched_item.path)\n\n            existing_item = self._item_by_canonical_name[patched_item_base_name]\n\n            self._check_patch_type_mismatch(patched_item, existing_item)\n\n            if isinstance(patched_item, (AstStructPatch, AstUnionPatch)):\n                self._check_field_names_unique(existing_item, patched_item)\n                existing_item.fields += patched_item.fields\n                self._inject_patched_examples(existing_item, patched_item)\n            else:\n                raise AssertionError('Unknown Patch Object Type {}'.format(\n                    patched_item.__class__.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _check_patch_type_mismatch(self, patched_item, existing_item):\n        def raise_mismatch_error(patched_item, existing_item, data_type_name):\n            error_msg = ('Type mismatch. Patch {} corresponds to pre-existing '\n                'data_type {} ({}:{}) that has type other than {}.')\n            raise InvalidSpec(error_msg.format(\n                quote(patched_item.name),\n                quote(existing_item.name),\n                existing_item.path,\n                existing_item.lineno,\n                quote(data_type_name)), patched_item.lineno, patched_item.path)\n\n        if isinstance(patched_item, AstStructPatch):\n            if not isinstance(existing_item, AstStructDef):\n                raise_mismatch_error(patched_item, existing_item, 'struct')\n        elif isinstance(patched_item, AstUnionPatch):\n            if not isinstance(existing_item, AstUnionDef):\n                raise_mismatch_error(patched_item, existing_item, 'union')\n            else:\n                if existing_item.closed != patched_item.closed:\n                    raise_mismatch_error(\n                        patched_item, existing_item,\n                        'union_closed' if existing_item.closed else 'union')\n        else:\n            raise AssertionError(\n                'Unknown Patch Object Type {}'.format(patched_item.__class__.__name__))", "response": "Enforces that each patch has a corresponding already - defined data type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenforce that patched fields don t already exist.", "response": "def _check_field_names_unique(self, existing_item, patched_item):\n        \"\"\"Enforces that patched fields don't already exist.\"\"\"\n        existing_fields_by_name = {f.name: f for f in existing_item.fields}\n        for patched_field in patched_item.fields:\n            if patched_field.name in existing_fields_by_name.keys():\n                existing_field = existing_fields_by_name[patched_field.name]\n                raise InvalidSpec('Patched field {} overrides pre-existing field in {} ({}:{}).'\n                    .format(quote(patched_field.name),\n                            quote(patched_item.name),\n                            existing_field.path,\n                            existing_field.lineno), patched_field.lineno, patched_field.path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _inject_patched_examples(self, existing_item, patched_item):\n        for key, _ in patched_item.examples.items():\n            patched_example = patched_item.examples[key]\n            existing_examples = existing_item.examples\n            if key in existing_examples:\n                existing_examples[key].fields.update(patched_example.fields)\n            else:\n                error_msg = 'Example defined in patch {} must correspond to a pre-existing example.'\n                raise InvalidSpec(error_msg.format(\n                    quote(patched_item.name)), patched_example.lineno, patched_example.path)", "response": "Injects patched examples into original examples."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopulating the type attributes of all types in the namespace.", "response": "def _populate_type_attributes(self):\n        \"\"\"\n        Converts each struct, union, and route from a forward reference to a\n        full definition.\n        \"\"\"\n        for namespace in self.api.namespaces.values():\n            env = self._get_or_create_env(namespace.name)\n\n            # do annotations before everything else, since populating aliases\n            # and datatypes involves setting annotations\n            for annotation in namespace.annotations:\n                if isinstance(annotation, CustomAnnotation):\n                    loc = annotation._ast_node.lineno, annotation._ast_node.path\n                    if annotation.annotation_type_ns:\n                        if annotation.annotation_type_ns not in env:\n                            raise InvalidSpec(\n                                'Namespace %s is not imported' %\n                                quote(annotation.annotation_type_ns), *loc)\n                        annotation_type_env = env[annotation.annotation_type_ns]\n                        if not isinstance(annotation_type_env, Environment):\n                            raise InvalidSpec(\n                                '%s is not a namespace.' %\n                                quote(annotation.annotation_type_ns), *loc)\n                    else:\n                        annotation_type_env = env\n\n                    if annotation.annotation_type_name not in annotation_type_env:\n                        raise InvalidSpec(\n                            'Annotation type %s does not exist' %\n                            quote(annotation.annotation_type_name), *loc)\n\n                    annotation_type = annotation_type_env[annotation.annotation_type_name]\n\n                    if not isinstance(annotation_type, AnnotationType):\n                        raise InvalidSpec(\n                            '%s is not an annotation type' % quote(annotation.annotation_type_name),\n                            *loc\n                        )\n\n                    annotation.set_attributes(annotation_type)\n\n            for alias in namespace.aliases:\n                data_type = self._resolve_type(env, alias._ast_node.type_ref)\n                alias.set_attributes(alias._ast_node.doc, data_type)\n                annotations = [self._resolve_annotation_type(env, annotation)\n                               for annotation in alias._ast_node.annotations]\n                alias.set_annotations(annotations)\n\n            for data_type in namespace.data_types:\n                if not data_type._is_forward_ref:\n                    continue\n\n                self._resolution_in_progress.add(data_type)\n                if isinstance(data_type, Struct):\n                    self._populate_struct_type_attributes(env, data_type)\n                elif isinstance(data_type, Union):\n                    self._populate_union_type_attributes(env, data_type)\n                else:\n                    raise AssertionError('Unhandled type: %r' %\n                                         type(data_type))\n                self._resolution_in_progress.remove(data_type)\n\n        assert len(self._resolution_in_progress) == 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\npopulates the attributes of a struct with the data type.", "response": "def _populate_struct_type_attributes(self, env, data_type):\n        \"\"\"\n        Converts a forward reference of a struct into a complete definition.\n        \"\"\"\n        parent_type = None\n        extends = data_type._ast_node.extends\n        if extends:\n            # A parent type must be fully defined and not just a forward\n            # reference.\n            parent_type = self._resolve_type(env, extends, True)\n            if isinstance(parent_type, Alias):\n                # Restrict extending aliases because it's difficult to generate\n                # code for it in Python. We put all type references at the end\n                # to avoid out-of-order declaration issues, but using \"extends\"\n                # in Python forces the reference to happen earlier.\n                raise InvalidSpec(\n                    'A struct cannot extend an alias. '\n                    'Use the canonical name instead.',\n                    data_type._ast_node.lineno, data_type._ast_node.path)\n            if isinstance(parent_type, Nullable):\n                raise InvalidSpec(\n                    'A struct cannot extend a nullable type.',\n                    data_type._ast_node.lineno, data_type._ast_node.path)\n            if not isinstance(parent_type, Struct):\n                raise InvalidSpec(\n                    'A struct can only extend another struct: '\n                    '%s is not a struct.' % quote(parent_type.name),\n                    data_type._ast_node.lineno, data_type._ast_node.path)\n        api_type_fields = []\n        for stone_field in data_type._ast_node.fields:\n            api_type_field = self._create_struct_field(env, stone_field)\n            api_type_fields.append(api_type_field)\n        data_type.set_attributes(\n            data_type._ast_node.doc, api_type_fields, parent_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _populate_union_type_attributes(self, env, data_type):\n        parent_type = None\n        extends = data_type._ast_node.extends\n        if extends:\n            # A parent type must be fully defined and not just a forward\n            # reference.\n            parent_type = self._resolve_type(env, extends, True)\n            if isinstance(parent_type, Alias):\n                raise InvalidSpec(\n                    'A union cannot extend an alias. '\n                    'Use the canonical name instead.',\n                    data_type._ast_node.lineno, data_type._ast_node.path)\n            if isinstance(parent_type, Nullable):\n                raise InvalidSpec(\n                    'A union cannot extend a nullable type.',\n                    data_type._ast_node.lineno, data_type._ast_node.path)\n            if not isinstance(parent_type, Union):\n                raise InvalidSpec(\n                    'A union can only extend another union: '\n                    '%s is not a union.' % quote(parent_type.name),\n                    data_type._ast_node.lineno, data_type._ast_node.path)\n\n        api_type_fields = []\n        for stone_field in data_type._ast_node.fields:\n            if stone_field.name == 'other':\n                raise InvalidSpec(\n                    \"Union cannot define an 'other' field because it is \"\n                    \"reserved as the catch-all field for open unions.\",\n                    stone_field.lineno, stone_field.path)\n            api_type_fields.append(self._create_union_field(env, stone_field))\n\n        catch_all_field = None\n        if data_type.closed:\n            if parent_type and not parent_type.closed:\n                # Due to the reversed super type / child type relationship for\n                # unions, a child type cannot be closed if its parent is open\n                # because the parent now has an extra field that is not\n                # recognized by the child if it were substituted in for it.\n                raise InvalidSpec(\n                    \"Union cannot be closed since parent type '%s' is open.\" % (\n                        parent_type.name),\n                    data_type._ast_node.lineno, data_type._ast_node.path)\n        else:\n            if not parent_type or parent_type.closed:\n                # Create a catch-all field\n                catch_all_field = UnionField(\n                    name='other', data_type=Void(), doc=None,\n                    ast_node=data_type._ast_node, catch_all=True)\n                api_type_fields.append(catch_all_field)\n\n        data_type.set_attributes(\n            data_type._ast_node.doc, api_type_fields, parent_type, catch_all_field)", "response": "Populates the internal data structures for a union type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _populate_field_defaults(self):\n        for namespace in self.api.namespaces.values():\n            for data_type in namespace.data_types:\n                # Only struct fields can have default\n                if not isinstance(data_type, Struct):\n                    continue\n\n                for field in data_type.fields:\n                    if not field._ast_node.has_default:\n                        continue\n\n                    if isinstance(field._ast_node.default, AstTagRef):\n                        default_value = TagRef(\n                            field.data_type, field._ast_node.default.tag)\n                    else:\n                        default_value = field._ast_node.default\n                    if not (field._ast_node.type_ref.nullable and default_value is None):\n                        # Verify that the type of the default value is correct for this field\n                        try:\n                            if field.data_type.name in ('Float32', 'Float64'):\n                                # You can assign int to the default value of float type\n                                # However float type should always have default value in float\n                                default_value = float(default_value)\n                            field.data_type.check(default_value)\n                        except ValueError as e:\n                            raise InvalidSpec(\n                                'Field %s has an invalid default: %s' %\n                                (quote(field._ast_node.name), e),\n                                field._ast_node.lineno, field._ast_node.path)\n                    field.set_default(default_value)", "response": "Populate the default values of each field in the union."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npopulates the internal attributes of all routes in the environment.", "response": "def _populate_route_attributes(self):\n        \"\"\"\n        Converts all routes from forward references to complete definitions.\n        \"\"\"\n        route_schema = self._validate_stone_cfg()\n        self.api.add_route_schema(route_schema)\n        for namespace in self.api.namespaces.values():\n            env = self._get_or_create_env(namespace.name)\n            for route in namespace.routes:\n                self._populate_route_attributes_helper(env, route, route_schema)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _populate_route_attributes_helper(self, env, route, schema):\n        arg_dt = self._resolve_type(env, route._ast_node.arg_type_ref)\n        result_dt = self._resolve_type(env, route._ast_node.result_type_ref)\n        error_dt = self._resolve_type(env, route._ast_node.error_type_ref)\n\n        ast_deprecated = route._ast_node.deprecated\n        if ast_deprecated:\n            assert ast_deprecated[0]\n            new_route_name = ast_deprecated[1]\n            new_route_version = ast_deprecated[2]\n            if new_route_name:\n                assert new_route_version\n\n                is_not_defined = False\n                is_not_route = False\n                if new_route_name in env:\n                    if isinstance(env[new_route_name], ApiRoutesByVersion):\n                        if new_route_version not in env[new_route_name].at_version:\n                            is_not_defined = True\n                    else:\n                        is_not_route = True\n                else:\n                    is_not_defined = True\n\n                if is_not_defined:\n                    raise InvalidSpec(\n                        'Undefined route %s at version %d.' % (\n                            quote(new_route_name), new_route_version),\n                        route._ast_node.lineno, route._ast_node.path)\n\n                if is_not_route:\n                    raise InvalidSpec(\n                        '%s must be a route.' % quote(new_route_name),\n                        route._ast_node.lineno, route._ast_node.path)\n\n                new_route = env[new_route_name].at_version[new_route_version]\n                deprecated = DeprecationInfo(new_route)\n            else:\n                deprecated = DeprecationInfo()\n        else:\n            deprecated = None\n\n        attr_by_name = {}\n        for attr in route._ast_node.attrs:\n            attr_by_name[attr.name] = attr\n\n        try:\n            validated_attrs = schema.check_attr_repr(attr_by_name)\n        except KeyError as e:\n            raise InvalidSpec(\n                \"Route does not define attr key '%s'.\" % e.args[0],\n                route._ast_node.lineno, route._ast_node.path)\n\n        route.set_attributes(\n            deprecated=deprecated,\n            doc=route._ast_node.doc,\n            arg_data_type=arg_dt,\n            result_data_type=result_dt,\n            error_data_type=error_dt,\n            attrs=validated_attrs)", "response": "Populates the route attributes with the complete definition."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_struct_field(self, env, stone_field):\n        if isinstance(stone_field, AstVoidField):\n            raise InvalidSpec(\n                'Struct field %s cannot have a Void type.' %\n                quote(stone_field.name),\n                stone_field.lineno, stone_field.path)\n\n        data_type = self._resolve_type(env, stone_field.type_ref)\n        annotations = [self._resolve_annotation_type(env, annotation)\n                       for annotation in stone_field.annotations]\n\n        if isinstance(data_type, Void):\n            raise InvalidSpec(\n                'Struct field %s cannot have a Void type.' %\n                quote(stone_field.name),\n                stone_field.lineno, stone_field.path)\n        elif isinstance(data_type, Nullable) and stone_field.has_default:\n            raise InvalidSpec('Field %s cannot be a nullable '\n                              'type and have a default specified.' %\n                              quote(stone_field.name),\n                              stone_field.lineno, stone_field.path)\n        api_type_field = StructField(\n            name=stone_field.name,\n            data_type=data_type,\n            doc=stone_field.doc,\n            ast_node=stone_field,\n        )\n        api_type_field.set_annotations(annotations)\n        return api_type_field", "response": "Create a struct field from a struct field."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a union field.", "response": "def _create_union_field(self, env, stone_field):\n        \"\"\"\n        This function resolves symbols to objects that we've instantiated in\n        the current environment. For example, a field with data type named\n        \"String\" is pointed to a String() object.\n\n        The caller needs to ensure that this stone_field is for a Union and not\n        for a Struct.\n\n        Returns:\n            stone.data_type.UnionField: A field of a union.\n        \"\"\"\n        annotations = [self._resolve_annotation_type(env, annotation)\n                       for annotation in stone_field.annotations]\n\n        if isinstance(stone_field, AstVoidField):\n            api_type_field = UnionField(\n                name=stone_field.name, data_type=Void(), doc=stone_field.doc,\n                ast_node=stone_field)\n        else:\n            data_type = self._resolve_type(env, stone_field.type_ref)\n            if isinstance(data_type, Void):\n                raise InvalidSpec('Union member %s cannot have Void '\n                                  'type explicit, omit Void instead.' %\n                                  quote(stone_field.name),\n                                  stone_field.lineno, stone_field.path)\n            api_type_field = UnionField(\n                name=stone_field.name, data_type=data_type,\n                doc=stone_field.doc, ast_node=stone_field)\n        api_type_field.set_annotations(annotations)\n        return api_type_field"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _instantiate_data_type(self, data_type_class, data_type_args, loc):\n        assert issubclass(data_type_class, DataType), \\\n            'Expected stone.data_type.DataType, got %r' % data_type_class\n\n        argspec = inspect.getargspec(data_type_class.__init__)  # noqa: E501 # pylint: disable=deprecated-method,useless-suppression\n        argspec.args.remove('self')\n        num_args = len(argspec.args)\n        # Unfortunately, argspec.defaults is None if there are no defaults\n        num_defaults = len(argspec.defaults or ())\n\n        pos_args, kw_args = data_type_args\n\n        if (num_args - num_defaults) > len(pos_args):\n            # Report if a positional argument is missing\n            raise InvalidSpec(\n                'Missing positional argument %s for %s type' %\n                (quote(argspec.args[len(pos_args)]),\n                 quote(data_type_class.__name__)),\n                *loc)\n        elif (num_args - num_defaults) < len(pos_args):\n            # Report if there are too many positional arguments\n            raise InvalidSpec(\n                'Too many positional arguments for %s type' %\n                quote(data_type_class.__name__),\n                *loc)\n\n        # Map from arg name to bool indicating whether the arg has a default\n        args = {}\n        for i, key in enumerate(argspec.args):\n            args[key] = (i >= num_args - num_defaults)\n\n        for key in kw_args:\n            # Report any unknown keyword arguments\n            if key not in args:\n                raise InvalidSpec('Unknown argument %s to %s type.' %\n                    (quote(key), quote(data_type_class.__name__)),\n                    *loc)\n            # Report any positional args that are defined as keywords args.\n            if not args[key]:\n                raise InvalidSpec(\n                    'Positional argument %s cannot be specified as a '\n                    'keyword argument.' % quote(key),\n                    *loc)\n            del args[key]\n\n        try:\n            return data_type_class(*pos_args, **kw_args)\n        except ParameterError as e:\n            # Each data type validates its own attributes, and will raise a\n            # ParameterError if the type or value is bad.\n            raise InvalidSpec('Bad argument to %s type: %s' %\n                (quote(data_type_class.__name__), e.args[0]),\n                *loc)", "response": "This method instantiates a new data type with additional attributes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nresolve the data type referenced by type_ref.", "response": "def _resolve_type(self, env, type_ref, enforce_fully_defined=False):\n        \"\"\"\n        Resolves the data type referenced by type_ref.\n\n        If `enforce_fully_defined` is True, then the referenced type must be\n        fully populated (fields, parent_type, ...), and not simply a forward\n        reference.\n        \"\"\"\n        loc = type_ref.lineno, type_ref.path\n        orig_namespace_name = env.namespace_name\n        if type_ref.ns:\n            # TODO(kelkabany): If a spec file imports a namespace, it is\n            # available to all spec files that are part of the same namespace.\n            # Might want to introduce the concept of an environment specific\n            # to a file.\n            if type_ref.ns not in env:\n                raise InvalidSpec(\n                    'Namespace %s is not imported' % quote(type_ref.ns), *loc)\n            env = env[type_ref.ns]\n            if not isinstance(env, Environment):\n                raise InvalidSpec(\n                    '%s is not a namespace.' % quote(type_ref.ns), *loc)\n        if type_ref.name not in env:\n            raise InvalidSpec(\n                'Symbol %s is undefined.' % quote(type_ref.name), *loc)\n\n        obj = env[type_ref.name]\n        if obj is Void and type_ref.nullable:\n            raise InvalidSpec('Void cannot be marked nullable.',\n                              *loc)\n        elif inspect.isclass(obj):\n            resolved_data_type_args = self._resolve_args(env, type_ref.args)\n            data_type = self._instantiate_data_type(\n                obj, resolved_data_type_args, (type_ref.lineno, type_ref.path))\n        elif isinstance(obj, ApiRoutesByVersion):\n            raise InvalidSpec('A route cannot be referenced here.',\n                              *loc)\n        elif type_ref.args[0] or type_ref.args[1]:\n            # An instance of a type cannot have any additional\n            # attributes specified.\n            raise InvalidSpec('Attributes cannot be specified for '\n                              'instantiated type %s.' %\n                              quote(type_ref.name),\n                              *loc)\n        else:\n            data_type = env[type_ref.name]\n\n        if type_ref.ns:\n            # Add the source namespace as an import.\n            namespace = self.api.ensure_namespace(orig_namespace_name)\n            if isinstance(data_type, UserDefined):\n                namespace.add_imported_namespace(\n                    self.api.ensure_namespace(type_ref.ns),\n                    imported_data_type=True)\n            elif isinstance(data_type, Alias):\n                namespace.add_imported_namespace(\n                    self.api.ensure_namespace(type_ref.ns),\n                    imported_alias=True)\n\n        if (enforce_fully_defined and isinstance(data_type, UserDefined) and\n                data_type._is_forward_ref):\n            if data_type in self._resolution_in_progress:\n                raise InvalidSpec(\n                    'Unresolvable circular reference for type %s.' %\n                    quote(type_ref.name), *loc)\n            self._resolution_in_progress.add(data_type)\n            if isinstance(data_type, Struct):\n                self._populate_struct_type_attributes(env, data_type)\n            elif isinstance(data_type, Union):\n                self._populate_union_type_attributes(env, data_type)\n            self._resolution_in_progress.remove(data_type)\n\n        if type_ref.nullable:\n            unwrapped_dt, _ = unwrap_aliases(data_type)\n            if isinstance(unwrapped_dt, Nullable):\n                raise InvalidSpec(\n                    'Cannot mark reference to nullable type as nullable.',\n                    *loc)\n            data_type = Nullable(data_type)\n\n        return data_type"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nresolves the type referenced by annotation_ref.", "response": "def _resolve_annotation_type(self, env, annotation_ref):\n        \"\"\"\n        Resolves the annotation type referenced by annotation_ref.\n        \"\"\"\n        loc = annotation_ref.lineno, annotation_ref.path\n        if annotation_ref.ns:\n            if annotation_ref.ns not in env:\n                raise InvalidSpec(\n                    'Namespace %s is not imported' % quote(annotation_ref.ns), *loc)\n            env = env[annotation_ref.ns]\n            if not isinstance(env, Environment):\n                raise InvalidSpec(\n                    '%s is not a namespace.' % quote(annotation_ref.ns), *loc)\n\n        if annotation_ref.annotation not in env:\n            raise InvalidSpec(\n                'Annotation %s does not exist.' % quote(annotation_ref.annotation), *loc)\n\n        return env[annotation_ref.annotation]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresolving type references in data type arguments to data types in the environment.", "response": "def _resolve_args(self, env, args):\n        \"\"\"\n        Resolves type references in data type arguments to data types in\n        the environment.\n        \"\"\"\n        pos_args, kw_args = args\n\n        def check_value(v):\n            if isinstance(v, AstTypeRef):\n                return self._resolve_type(env, v)\n            else:\n                return v\n\n        new_pos_args = [check_value(pos_arg) for pos_arg in pos_args]\n        new_kw_args = {k: check_value(v) for k, v in kw_args.items()}\n        return new_pos_args, new_kw_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _create_route(self, env, item):\n        if item.name in env:\n            if isinstance(env[item.name], ApiRoutesByVersion):\n                if item.version in env[item.name].at_version:\n                    existing_dt = env[item.name].at_version[item.version]\n                    raise InvalidSpec(\n                        'Route %s at version %d already defined (%s:%d).' % (\n                            quote(item.name), item.version, existing_dt._ast_node.path,\n                            existing_dt._ast_node.lineno),\n                        item.lineno, item.path)\n            else:\n                existing_dt = env[item.name]\n                raise InvalidSpec(\n                    'Symbol %s already defined (%s:%d).' % (\n                        quote(item.name), existing_dt._ast_node.path,\n                        existing_dt._ast_node.lineno),\n                    item.lineno, item.path)\n        else:\n            env[item.name] = ApiRoutesByVersion()\n\n        route = ApiRoute(\n            name=item.name,\n            version=item.version,\n            ast_node=item,\n        )\n        env[route.name].at_version[route.version] = route\n        return route", "response": "Create a route from an AST node and add it to the environment."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npopulating every possible example for every type.", "response": "def _populate_examples(self):\n        \"\"\"Construct every possible example for every type.\n\n        This is done in two passes. The first pass assigns examples to their\n        associated types, but does not resolve references between examples for\n        different types. This is because the referenced examples may not yet\n        exist. The second pass resolves references.\n        \"\"\"\n        for namespace in self.api.namespaces.values():\n            for data_type in namespace.data_types:\n                for example in data_type._ast_node.examples.values():\n                    data_type._add_example(example)\n\n        for namespace in self.api.namespaces.values():\n            for data_type in namespace.data_types:\n                data_type._compute_examples()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _validate_doc_refs(self):\n        for namespace in self.api.namespaces.values():\n            env = self._get_or_create_env(namespace.name)\n            # Validate the doc refs of each api entity that has a doc\n            for data_type in namespace.data_types:\n                if data_type.doc:\n                    self._validate_doc_refs_helper(\n                        env,\n                        data_type.doc,\n                        (data_type._ast_node.lineno + 1, data_type._ast_node.path),\n                        data_type)\n                for field in data_type.fields:\n                    if field.doc:\n                        self._validate_doc_refs_helper(\n                            env,\n                            field.doc,\n                            (field._ast_node.lineno + 1, field._ast_node.path),\n                            data_type)\n            for route in namespace.routes:\n                if route.doc:\n                    self._validate_doc_refs_helper(\n                        env,\n                        route.doc,\n                        (route._ast_node.lineno + 1, route._ast_node.path))", "response": "Validate that all the documentation references across every namespace are formatted properly have valid values and make\n        references to valid symbols."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nvalidates that all the documentation references in a docstring are formatted properly and make references to valid symbols.", "response": "def _validate_doc_refs_helper(self, env, doc, loc, type_context=None):\n        \"\"\"\n        Validates that all the documentation references in a docstring are\n        formatted properly, have valid values, and make references to valid\n        symbols.\n\n        Args:\n            env (dict): The environment of defined symbols.\n            doc (str): The docstring to validate.\n            lineno (int): The line number the docstring begins on in the spec.\n            type_context (stone.data_type.UserDefined): If the docstring\n                belongs to a user-defined type (Struct or Union) or one of its\n                fields, set this to the type. This is needed for \"field\" doc\n                refs that don't name a type to be validated.\n        \"\"\"\n        for match in doc_ref_re.finditer(doc):\n            tag = match.group('tag')\n            val = match.group('val')\n            if tag == 'field':\n                if '.' in val:\n                    type_name, field_name = val.split('.', 1)\n                    if type_name not in env:\n                        raise InvalidSpec(\n                            'Bad doc reference to field %s of '\n                            'unknown type %s.' % (field_name, quote(type_name)),\n                            *loc)\n                    elif isinstance(env[type_name], ApiRoutesByVersion):\n                        raise InvalidSpec(\n                            'Bad doc reference to field %s of route %s.' %\n                            (quote(field_name), quote(type_name)),\n                            *loc)\n                    if isinstance(env[type_name], Environment):\n                        # Handle reference to field in imported namespace.\n                        namespace_name, type_name, field_name = val.split('.', 2)\n                        data_type_to_check = env[namespace_name][type_name]\n                    elif isinstance(env[type_name], Alias):\n                        data_type_to_check = env[type_name].data_type\n                    else:\n                        data_type_to_check = env[type_name]\n                    if not any(field.name == field_name\n                               for field in data_type_to_check.all_fields):\n                        raise InvalidSpec(\n                            'Bad doc reference to unknown field %s.' % quote(val),\n                            *loc)\n                else:\n                    # Referring to a field that's a member of this type\n                    assert type_context is not None\n                    if not any(field.name == val\n                               for field in type_context.all_fields):\n                        raise InvalidSpec(\n                            'Bad doc reference to unknown field %s.' %\n                            quote(val),\n                            *loc)\n            elif tag == 'link':\n                if not (1 < val.rfind(' ') < len(val) - 1):\n                    # There must be a space somewhere in the middle of the\n                    # string to separate the title from the uri.\n                    raise InvalidSpec(\n                        'Bad doc reference to link (need a title and '\n                        'uri separated by a space): %s.' % quote(val),\n                        *loc)\n            elif tag == 'route':\n                if '.' in val:\n                    # Handle reference to route in imported namespace.\n                    namespace_name, val = val.split('.', 1)\n                    if namespace_name not in env:\n                        raise InvalidSpec(\n                            \"Unknown doc reference to namespace '%s'.\" %\n                            namespace_name, *loc)\n                    env_to_check = env[namespace_name]\n                else:\n                    env_to_check = env\n\n                route_name, version = parse_route_name_and_version(val)\n                if route_name not in env_to_check:\n                    raise InvalidSpec(\n                        'Unknown doc reference to route {}.'.format(quote(route_name)), *loc)\n                if not isinstance(env_to_check[route_name], ApiRoutesByVersion):\n                    raise InvalidSpec(\n                        'Doc reference to type {} is not a route.'.format(quote(route_name)), *loc)\n                if version not in env_to_check[route_name].at_version:\n                    raise InvalidSpec(\n                        'Doc reference to route {} has undefined version {}.'.format(\n                            quote(route_name), version),\n                        *loc)\n            elif tag == 'type':\n                if '.' in val:\n                    # Handle reference to type in imported namespace.\n                    namespace_name, val = val.split('.', 1)\n                    if namespace_name not in env:\n                        raise InvalidSpec(\n                            \"Unknown doc reference to namespace '%s'.\" %\n                            namespace_name, *loc)\n                    env_to_check = env[namespace_name]\n                else:\n                    env_to_check = env\n                if val not in env_to_check:\n                    raise InvalidSpec(\n                        \"Unknown doc reference to type '%s'.\" % val,\n                        *loc)\n                elif not isinstance(env_to_check[val], (Struct, Union)):\n                    raise InvalidSpec(\n                        'Doc reference to type %s is not a struct or union.' %\n                        quote(val), *loc)\n            elif tag == 'val':\n                if not doc_ref_val_re.match(val):\n                    raise InvalidSpec(\n                        'Bad doc reference value %s.' % quote(val),\n                        *loc)\n            else:\n                raise InvalidSpec(\n                    'Unknown doc reference tag %s.' % quote(tag),\n                    *loc)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating that all annotations are attached to proper types and that no field or type has conflicting inherited or direct annotations.", "response": "def _validate_annotations(self):\n        \"\"\"\n        Validates that all annotations are attached to proper types and that no field\n        has conflicting inherited or direct annotations. We need to go through all reference\n        chains to make sure we don't override a redactor set on a parent alias or type\n        \"\"\"\n        for namespace in self.api.namespaces.values():\n            for data_type in namespace.data_types:\n                for field in data_type.fields:\n                    if field.redactor:\n                        self._validate_field_can_be_tagged_with_redactor(field)\n\n            for alias in namespace.aliases:\n                if alias.redactor:\n                    self._validate_object_can_be_tagged_with_redactor(alias)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the field type can be annotated and that alias does not have conflicting annotations.", "response": "def _validate_field_can_be_tagged_with_redactor(self, field):\n        \"\"\"\n        Validates that the field type can be annotated and that alias does not have\n        conflicting annotations.\n        \"\"\"\n        if is_alias(field.data_type):\n            raise InvalidSpec(\n                \"Redactors can only be applied to alias definitions, not \"\n                \"to alias references.\",\n                field._ast_node.lineno, field._ast_node.path)\n\n        self._validate_object_can_be_tagged_with_redactor(field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates that the object type can be annotated with a redactor.", "response": "def _validate_object_can_be_tagged_with_redactor(self, annotated_object):\n        \"\"\"\n        Validates that the object type can be annotated and object does not have\n        conflicting annotations.\n        \"\"\"\n        data_type = annotated_object.data_type\n        name = annotated_object.name\n        loc = annotated_object._ast_node.lineno, annotated_object._ast_node.path\n        curr_data_type = data_type\n\n        while isinstance(curr_data_type, Alias) or isinstance(curr_data_type, Nullable):\n            # aliases have redactors assocaited with the type itself\n            if hasattr(curr_data_type, 'redactor') and curr_data_type.redactor:\n                raise InvalidSpec(\"A redactor has already been defined for '%s' by '%s'.\" %\n                                  (str(name), str(curr_data_type.name)), *loc)\n\n            curr_data_type = curr_data_type.data_type\n\n        if hasattr(annotated_object, 'redactor') and annotated_object.redactor:\n            if is_map_type(curr_data_type) or is_list_type(curr_data_type):\n                while True:\n                    if is_map_type(curr_data_type):\n                        curr_data_type = curr_data_type.value_data_type\n                    else:\n                        curr_data_type = curr_data_type.data_type\n\n                    should_continue = (is_map_type(curr_data_type) or is_list_type(curr_data_type)\n                        or is_nullable_type(curr_data_type))\n\n                    if should_continue is False:\n                        break\n\n            if is_user_defined_type(curr_data_type) or is_void_type(curr_data_type):\n                raise InvalidSpec(\"Redactors can't be applied to user-defined or void types.\", *loc)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_stone_cfg(self):\n        def mk_route_schema():\n            s = Struct('Route', ApiNamespace('stone_cfg'), None)\n            s.set_attributes(None, [], None)\n            return s\n\n        try:\n            stone_cfg = self.api.namespaces.pop('stone_cfg')\n        except KeyError:\n            return mk_route_schema()\n\n        if stone_cfg.routes:\n            route = stone_cfg.routes[0]\n            raise InvalidSpec(\n                'No routes can be defined in the stone_cfg namespace.',\n                route._ast_node.lineno,\n                route._ast_node.path,\n            )\n\n        if not stone_cfg.data_types:\n            return mk_route_schema()\n\n        for data_type in stone_cfg.data_types:\n            if data_type.name != 'Route':\n                raise InvalidSpec(\n                    \"Only a struct named 'Route' can be defined in the \"\n                    \"stone_cfg namespace.\",\n                    data_type._ast_node.lineno,\n                    data_type._ast_node.path,\n                )\n\n        # TODO: are we always guaranteed at least one data type?\n        # pylint: disable=undefined-loop-variable\n        return data_type", "response": "Validate the stone_cfg namespace."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _filter_namespaces_by_route_whitelist(self):\n        assert self._routes is not None, \"Missing route whitelist\"\n        assert 'route_whitelist' in self._routes\n        assert 'datatype_whitelist' in self._routes\n\n        # Get route whitelist in canonical form\n        route_whitelist = {}\n        for namespace_name, route_reprs in self._routes['route_whitelist'].items():\n            new_route_reprs = []\n            if route_reprs == ['*']:\n                namespace = self.api.namespaces[namespace_name]\n                new_route_reprs = [route.name_with_version() for route in namespace.routes]\n            else:\n                for route_repr in route_reprs:\n                    route_name, version = parse_route_name_and_version(route_repr)\n                    if version > 1:\n                        new_route_reprs.append('{}:{}'.format(route_name, version))\n                    else:\n                        new_route_reprs.append(route_name)\n            route_whitelist[namespace_name] = new_route_reprs\n\n        # Parse the route whitelist and populate any starting data types\n        route_data_types = []\n        for namespace_name, route_reprs in route_whitelist.items():\n            # Error out if user supplied nonexistent namespace\n            if namespace_name not in self.api.namespaces:\n                raise AssertionError('Namespace %s is not defined!' % namespace_name)\n            namespace = self.api.namespaces[namespace_name]\n\n            # Parse namespace doc refs and add them to the starting data types\n            if namespace.doc is not None:\n                route_data_types.extend(\n                    parse_data_types_from_doc_ref(self.api, namespace.doc, namespace_name))\n\n            # Parse user-specified routes and add them to the starting data types\n            # Note that this may add duplicates, but that's okay, as the recursion\n            # keeps track of visited data types.\n            assert '*' not in route_reprs\n            for routes_repr in route_reprs:\n                route_name, version = parse_route_name_and_version(routes_repr)\n                if route_name not in namespace.routes_by_name or \\\n                        version not in namespace.routes_by_name[route_name].at_version:\n                    raise AssertionError('Route %s at version %d is not defined!' %\n                                         (route_name, version))\n\n                route = namespace.routes_by_name[route_name].at_version[version]\n                route_data_types.extend(namespace.get_route_io_data_types_for_route(route))\n                if route.doc is not None:\n                    route_data_types.extend(\n                        parse_data_types_from_doc_ref(self.api, route.doc, namespace_name))\n\n        # Parse the datatype whitelist and populate any starting data types\n        for namespace_name, datatype_names in self._routes['datatype_whitelist'].items():\n            if namespace_name not in self.api.namespaces:\n                raise AssertionError('Namespace %s is not defined!' % namespace_name)\n\n            # Parse namespace doc refs and add them to the starting data types\n            namespace = self.api.namespaces[namespace_name]\n            if namespace.doc is not None:\n                route_data_types.extend(\n                    parse_data_types_from_doc_ref(self.api, namespace.doc, namespace_name))\n\n            for datatype_name in datatype_names:\n                if datatype_name not in self.api.namespaces[namespace_name].data_type_by_name:\n                    raise AssertionError('Datatype %s is not defined!' % datatype_name)\n                data_type = self.api.namespaces[namespace_name].data_type_by_name[datatype_name]\n                route_data_types.append(data_type)\n\n        # Recurse on dependencies\n        output_types_by_ns, output_routes_by_ns = self._find_dependencies(route_data_types)\n\n        # Update the IR representation. This involves editing the data types and\n        # routes for each namespace.\n        for namespace in self.api.namespaces.values():\n            data_types = list(set(output_types_by_ns[namespace.name]))  # defaults to empty list\n            namespace.data_types = data_types\n            namespace.data_type_by_name = {d.name: d for d in data_types}\n\n            output_route_reprs = [output_route.name_with_version()\n                                  for output_route in output_routes_by_ns[namespace.name]]\n            if namespace.name in route_whitelist:\n                whitelisted_route_reprs = route_whitelist[namespace.name]\n                route_reprs = list(set(whitelisted_route_reprs + output_route_reprs))\n            else:\n                route_reprs = output_route_reprs\n\n            routes = []\n            for route_repr in route_reprs:\n                route_name, version = parse_route_name_and_version(route_repr)\n                route = namespace.routes_by_name[route_name].at_version[version]\n                routes.append(route)\n\n            namespace.routes = []\n            namespace.route_by_name = {}\n            namespace.routes_by_name = {}\n            for route in routes:\n                namespace.add_route(route)", "response": "Filter the user - defined datatypes by the route whitelist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef generate(self, api):\n        for namespace in api.namespaces.values():\n            # One module per namespace is created. The module takes the name\n            # of the namespace.\n            with self.output_to_relative_path('{}.py'.format(namespace.name)):\n                self._generate_namespace_module(namespace)", "response": "Generates a module for each namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a file that lists each namespace.", "response": "def generate(self, api):\n        \"\"\"Generates a file that lists each namespace.\"\"\"\n        with self.output_to_relative_path('ex1.out'):\n            for namespace in api.namespaces.values():\n                self.emit(namespace.name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a class that can be used to create a Supplement ZipFile.", "response": "def get_zip_class():\n    \"\"\"\n    Supplement ZipFile class to support context manager for Python 2.6\n    \"\"\"\n    class ContextualZipFile(zipfile.ZipFile):\n        def __enter__(self):\n            return self\n        def __exit__(self, type, value, traceback):\n            self.close\n    return zipfile.ZipFile if hasattr(zipfile.ZipFile, '__exit__') else \\\n        ContextualZipFile"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef main():\n    options = _parse_args()\n    archive = download_setuptools(\n        version=options.version,\n        download_base=options.download_base,\n        downloader_factory=options.downloader_factory,\n    )\n    return _install(archive, _build_install_args(options))", "response": "Install or upgrade setuptools and EasyInstall"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a module for each namespace.", "response": "def generate(self, api):\n        # type: (Api) -> None\n        \"\"\"\n        Generates a module for each namespace.\n\n        Each namespace will have Python classes to represent data types and\n        routes in the Stone spec.\n        \"\"\"\n        for namespace in api.namespaces.values():\n            with self.output_to_relative_path('{}.pyi'.format(fmt_namespace(namespace.name))):\n                self._generate_base_namespace_module(namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _generate_base_namespace_module(self, namespace):\n        # type: (ApiNamespace) -> None\n        \"\"\"Creates a module for the namespace. All data types and routes are\n        represented as Python classes.\"\"\"\n\n        self.cur_namespace = namespace\n        self.import_tracker.clear()\n        generate_module_header(self)\n\n        self.emit_placeholder('imports_needed_for_typing')\n        self.emit_raw(validators_import_with_type_ignore)\n\n        # Generate import statements for all referenced namespaces.\n        self._generate_imports_for_referenced_namespaces(namespace)\n\n        self._generate_typevars()\n\n        for annotation_type in namespace.annotation_types:\n            self._generate_annotation_type_class(namespace, annotation_type)\n\n        for data_type in namespace.linearize_data_types():\n            if isinstance(data_type, Struct):\n                self._generate_struct_class(namespace, data_type)\n            elif isinstance(data_type, Union):\n                self._generate_union_class(namespace, data_type)\n            else:\n                raise TypeError('Cannot handle type %r' % type(data_type))\n\n        for alias in namespace.linearize_aliases():\n            self._generate_alias_definition(namespace, alias)\n\n        self._generate_routes(namespace)\n        self._generate_imports_needed_for_typing()", "response": "Generates a base namespace module."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngenerates the type variables that are used by the type signatures for the type signatures for the current locale.", "response": "def _generate_typevars(self):\n        # type: () -> None\n        \"\"\"\n        Creates type variables that are used by the type signatures for\n        _process_custom_annotations.\n        \"\"\"\n        self.emit(\"T = TypeVar('T', bound=bb.AnnotationType)\")\n        self.emit(\"U = TypeVar('U')\")\n        self.import_tracker._register_typing_import('TypeVar')\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndefine a Python class that represents an annotation type in Stone.", "response": "def _generate_annotation_type_class(self, ns, annotation_type):\n        # type: (ApiNamespace, AnnotationType) -> None\n        \"\"\"Defines a Python class that represents an annotation type in Stone.\"\"\"\n        self.emit('class {}(object):'.format(class_name_for_annotation_type(annotation_type, ns)))\n        with self.indent():\n            self._generate_annotation_type_class_init(ns, annotation_type)\n            self._generate_annotation_type_class_properties(ns, annotation_type)\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndefining a Python class that represents a struct in Stone.", "response": "def _generate_struct_class(self, ns, data_type):\n        # type: (ApiNamespace, Struct) -> None\n        \"\"\"Defines a Python class that represents a struct in Stone.\"\"\"\n        self.emit(self._class_declaration_for_type(ns, data_type))\n        with self.indent():\n            self._generate_struct_class_init(ns, data_type)\n            self._generate_struct_class_properties(ns, data_type)\n            self._generate_struct_or_union_class_custom_annotations()\n\n        self._generate_validator_for(data_type)\n        self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _generate_union_class_variant_creators(self, ns, data_type):\n        # type: (ApiNamespace, Union) -> None\n        \"\"\"\n        Generate the following section in the 'union Shape' example:\n        @classmethod\n        def circle(cls, val: float) -> Shape: ...\n        \"\"\"\n        union_type = class_name_for_data_type(data_type)\n\n        for field in data_type.fields:\n            if not is_void_type(field.data_type):\n                field_name_reserved_check = fmt_func(field.name, check_reserved=True)\n                val_type = self.map_stone_type_to_pep484_type(ns, field.data_type)\n\n                self.emit('@classmethod')\n                self.emit('def {field_name}(cls, val: {val_type}) -> {union_type}: ...'.format(\n                    field_name=field_name_reserved_check,\n                    val_type=val_type,\n                    union_type=union_type,\n                ))\n                self.emit()", "response": "Generate the class methods that are used to create the union class variant."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the get methods for the union class.", "response": "def _generate_union_class_get_helpers(self, ns, data_type):\n        # type: (ApiNamespace, Union) -> None\n        \"\"\"\n        Generates the following section in the 'union Shape' example:\n        def get_circle(self) -> float: ...\n        \"\"\"\n        for field in data_type.fields:\n            field_name = fmt_func(field.name)\n\n            if not is_void_type(field.data_type):\n                # generate getter for field\n                val_type = self.map_stone_type_to_pep484_type(ns, field.data_type)\n\n                self.emit('def get_{field_name}(self) -> {val_type}: ...'.format(\n                    field_name=field_name,\n                    val_type=val_type,\n                ))\n                self.emit()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _generate_struct_or_union_class_custom_annotations(self):\n        self.emit('def _process_custom_annotations(')\n        with self.indent():\n            self.emit('self,')\n            self.emit('annotation_type: Type[T],')\n            self.emit('field_path: Text,')\n            self.emit('processor: Callable[[T, U], U],')\n            self.import_tracker._register_typing_import('Type')\n            self.import_tracker._register_typing_import('Text')\n            self.import_tracker._register_typing_import('Callable')\n        self.emit(') -> None: ...')\n        self.emit()", "response": "The _process_custom_annotations function allows client code to access the struct or union class custom annotations defined in the spec."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of callbacks that will be called when the type mapping is complete.", "response": "def _get_pep_484_type_mapping_callbacks(self):\n        # type: () -> OverrideDefaultTypesDict\n        \"\"\"\n        Once-per-instance, generate a mapping from\n        \"List\" -> return pep4848-compatible List[SomeType]\n        \"Nullable\" -> return pep484-compatible Optional[SomeType]\n\n        This is per-instance because we have to also call `self._register_typing_import`, because\n        we need to potentially import some things.\n        \"\"\"\n        def upon_encountering_list(ns, data_type, override_dict):\n            # type: (ApiNamespace, DataType, OverrideDefaultTypesDict) -> typing.Text\n            self.import_tracker._register_typing_import(\"List\")\n            return \"List[{}]\".format(\n                map_stone_type_to_python_type(ns, data_type, override_dict)\n            )\n\n        def upon_encountering_map(ns, map_data_type, override_dict):\n            # type: (ApiNamespace, DataType, OverrideDefaultTypesDict) -> typing.Text\n            map_type = cast(Map, map_data_type)\n            self.import_tracker._register_typing_import(\"Dict\")\n            return \"Dict[{}, {}]\".format(\n                map_stone_type_to_python_type(ns, map_type.key_data_type, override_dict),\n                map_stone_type_to_python_type(ns, map_type.value_data_type, override_dict)\n            )\n\n        def upon_encountering_nullable(ns, data_type, override_dict):\n            # type: (ApiNamespace, DataType, OverrideDefaultTypesDict) -> typing.Text\n            self.import_tracker._register_typing_import(\"Optional\")\n            return \"Optional[{}]\".format(\n                map_stone_type_to_python_type(ns, data_type, override_dict)\n            )\n\n        def upon_encountering_timestamp(\n                ns, data_type, override_dict\n        ):  # pylint: disable=unused-argument\n            # type: (ApiNamespace, DataType, OverrideDefaultTypesDict) -> typing.Text\n            self.import_tracker._register_adhoc_import(\"import datetime\")\n            return map_stone_type_to_python_type(ns, data_type)\n\n        def upon_encountering_string(\n            ns, data_type, override_dict\n        ):  # pylint: disable=unused-argument\n            # type: (...) -> typing.Text\n            self.import_tracker._register_typing_import(\"Text\")\n            return \"Text\"\n\n        callback_dict = {\n            List: upon_encountering_list,\n            Map: upon_encountering_map,\n            Nullable: upon_encountering_nullable,\n            Timestamp: upon_encountering_timestamp,\n            String: upon_encountering_string,\n        }  # type: OverrideDefaultTypesDict\n        return callback_dict"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(self, val):\n        if not isinstance(val, six.string_types):\n            raise ValidationError(\"'%s' expected to be a string, got %s\"\n                                  % (val, generic_type_name(val)))\n        if not six.PY3 and isinstance(val, str):\n            try:\n                val = val.decode('utf-8')\n            except UnicodeDecodeError:\n                raise ValidationError(\"'%s' was not valid utf-8\")\n\n        if self.max_length is not None and len(val) > self.max_length:\n            raise ValidationError(\"'%s' must be at most %d characters, got %d\"\n                                  % (val, self.max_length, len(val)))\n        if self.min_length is not None and len(val) < self.min_length:\n            raise ValidationError(\"'%s' must be at least %d characters, got %d\"\n                                  % (val, self.min_length, len(val)))\n\n        if self.pattern and not self.pattern_re.match(val):\n            raise ValidationError(\"'%s' did not match pattern '%s'\"\n                                  % (val, self.pattern))\n        return val", "response": "Validate that the value is a valid unicode string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self, val):\n        self.validate_type_only(val)\n        self.validate_fields_only(val)\n        return val", "response": "Validate that the passed in val is of the correct type and have all required fields present."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef validate_with_permissions(self, val, caller_permissions):\n        self.validate(val)\n        self.validate_fields_only_with_permissions(val, caller_permissions)\n        return val", "response": "Validate that the passed value is of the correct type and have all required permissions present."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates that the required fields are present in the object.", "response": "def validate_fields_only(self, val):\n        \"\"\"\n        To pass field validation, no required field should be missing.\n\n        This method assumes that the contents of each field have already been\n        validated on assignment, so it's merely a presence check.\n\n        FIXME(kelkabany): Since the definition object does not maintain a list\n        of which fields are required, all fields are scanned.\n        \"\"\"\n        for field_name in self.definition._all_field_names_:\n            if not hasattr(val, field_name):\n                raise ValidationError(\"missing required field '%s'\" %\n                                      field_name)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nvalidating that the type of an object is correct.", "response": "def validate_type_only(self, val):\n        \"\"\"\n        Use this when you only want to validate that the type of an object\n        is correct, but not yet validate each field.\n        \"\"\"\n        # Since the definition maintains the list of fields for serialization,\n        # we're okay with a subclass that might have extra information. This\n        # makes it easier to return one subclass for two routes, one of which\n        # relies on the parent class.\n        if not isinstance(val, self.definition):\n            raise ValidationError('expected type %s, got %s' %\n                (self.definition.__name__, generic_type_name(val)))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self, val):\n        self.validate_type_only(val)\n        if not hasattr(val, '_tag') or val._tag is None:\n            raise ValidationError('no tag set')\n        return val", "response": "Validate that the passed value is a valid tag set."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate that the type of an object is correct.", "response": "def validate_type_only(self, val):\n        \"\"\"\n        Use this when you only want to validate that the type of an object\n        is correct, but not yet validate each field.\n\n        We check whether val is a Python parent class of the definition. This\n        is because Union subtyping works in the opposite direction of Python\n        inheritance. For example, if a union U2 extends U1 in Python, this\n        validator will accept U1 in places where U2 is expected.\n        \"\"\"\n        if not issubclass(self.definition, type(val)):\n            raise ValidationError('expected type %s or subtype, got %s' %\n                (self.definition.__name__, generic_type_name(val)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ensure_namespace(self, name):\n        # type: (str) -> ApiNamespace\n        \"\"\"\n        Only creates a namespace if it hasn't yet been defined.\n\n        :param str name: Name of the namespace.\n\n        :return ApiNamespace:\n        \"\"\"\n        if name not in self.namespaces:\n            self.namespaces[name] = ApiNamespace(name)\n        return self.namespaces[name]", "response": "Ensures that the namespace with the given name is defined."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing namespaces and routes.", "response": "def normalize(self):\n        # type: () -> None\n        \"\"\"\n        Alphabetizes namespaces and routes to make spec parsing order mostly\n        irrelevant.\n        \"\"\"\n        ordered_namespaces = OrderedDict()  # type: NamespaceDict\n        # self.namespaces is currently ordered by declaration order.\n        for namespace_name in sorted(self.namespaces.keys()):\n            ordered_namespaces[namespace_name] = self.namespaces[namespace_name]\n        self.namespaces = ordered_namespaces\n\n        for namespace in self.namespaces.values():\n            namespace.normalize()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a docstring to the current namespace.", "response": "def add_doc(self, docstring):\n        # type: (six.text_type) -> None\n        \"\"\"Adds a docstring for this namespace.\n\n        The input docstring is normalized to have no leading whitespace and\n        no trailing whitespace except for a newline at the end.\n\n        If a docstring already exists, the new normalized docstring is appended\n        to the end of the existing one with two newlines separating them.\n        \"\"\"\n        assert isinstance(docstring, six.text_type), type(docstring)\n        normalized_docstring = doc_unwrap(docstring) + '\\n'\n        if self.doc is None:\n            self.doc = normalized_docstring\n        else:\n            self.doc += normalized_docstring"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_imported_namespace(self,\n                               namespace,\n                               imported_alias=False,\n                               imported_data_type=False,\n                               imported_annotation=False,\n                               imported_annotation_type=False):\n        # type: (ApiNamespace, bool, bool, bool, bool) -> None\n        \"\"\"\n        Keeps track of namespaces that this namespace imports.\n\n        Args:\n            namespace (Namespace): The imported namespace.\n            imported_alias (bool): Set if this namespace references an alias\n                in the imported namespace.\n            imported_data_type (bool): Set if this namespace references a\n                data type in the imported namespace.\n            imported_annotation (bool): Set if this namespace references a\n                annotation in the imported namespace.\n            imported_annotation_type (bool): Set if this namespace references an\n                annotation in the imported namespace, possibly indirectly (by\n                referencing an annotation elsewhere that has this type).\n        \"\"\"\n        assert self.name != namespace.name, \\\n            'Namespace cannot import itself.'\n        reason = self._imported_namespaces.setdefault(namespace, _ImportReason())\n        if imported_alias:\n            reason.alias = True\n        if imported_data_type:\n            reason.data_type = True\n        if imported_annotation:\n            reason.annotation = True\n        if imported_annotation_type:\n            reason.annotation_type = True", "response": "Adds the specified namespace to the set of imported namespaces."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef linearize_data_types(self):\n        # type: () -> typing.List[UserDefined]\n        \"\"\"\n        Returns a list of all data types used in the namespace. Because the\n        inheritance of data types can be modeled as a DAG, the list will be a\n        linearization of the DAG. It's ideal to generate data types in this\n        order so that composite types that reference other composite types are\n        defined in the correct order.\n        \"\"\"\n        linearized_data_types = []\n        seen_data_types = set()  # type: typing.Set[UserDefined]\n\n        def add_data_type(data_type):\n            # type: (UserDefined) -> None\n            if data_type in seen_data_types:\n                return\n            elif data_type.namespace != self:\n                # We're only concerned with types defined in this namespace.\n                return\n            if is_composite_type(data_type) and data_type.parent_type:\n                add_data_type(data_type.parent_type)\n            linearized_data_types.append(data_type)\n            seen_data_types.add(data_type)\n\n        for data_type in self.data_types:\n            add_data_type(data_type)\n\n        return linearized_data_types", "response": "Returns a list of all data types used in the namespace."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all aliases used in the namespace.", "response": "def linearize_aliases(self):\n        # type: () -> typing.List[Alias]\n        \"\"\"\n        Returns a list of all aliases used in the namespace. The aliases are\n        ordered to ensure that if they reference other aliases those aliases\n        come earlier in the list.\n        \"\"\"\n        linearized_aliases = []\n        seen_aliases = set()  # type: typing.Set[Alias]\n\n        def add_alias(alias):\n            # type: (Alias) -> None\n            if alias in seen_aliases:\n                return\n            elif alias.namespace != self:\n                return\n            if is_alias(alias.data_type):\n                add_alias(alias.data_type)\n            linearized_aliases.append(alias)\n            seen_aliases.add(alias)\n\n        for alias in self.aliases:\n            add_alias(alias)\n\n        return linearized_aliases"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all user - defined data types that are referenced by this route.", "response": "def get_route_io_data_types(self):\n        # type: () -> typing.List[UserDefined]\n        \"\"\"\n        Returns a list of all user-defined data types that are referenced as\n        either an argument, result, or error of a route. If a List or Nullable\n        data type is referenced, then the contained data type is returned\n        assuming it's a user-defined type.\n        \"\"\"\n        data_types = set()  # type: typing.Set[UserDefined]\n        for route in self.routes:\n            data_types |= self.get_route_io_data_types_for_route(route)\n        return sorted(data_types, key=lambda dt: dt.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_route_io_data_types_for_route(self, route):\n        # type: (ApiRoute) -> typing.Set[UserDefined]\n        \"\"\"\n        Given a route, returns a set of its argument/result/error datatypes.\n        \"\"\"\n        data_types = set()  # type: typing.Set[UserDefined]\n        for dtype in (route.arg_data_type, route.result_data_type, route.error_data_type):\n            while is_list_type(dtype) or is_nullable_type(dtype):\n                data_list_type = dtype  # type: typing.Any\n                dtype = data_list_type.data_type\n            if is_composite_type(dtype) or is_alias(dtype):\n                data_user_type = dtype  # type: typing.Any\n                data_types.add(data_user_type)\n        return data_types", "response": "Given a route returns a set of its argument result and error datatypes."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of ApiNamespace objects.", "response": "def get_imported_namespaces(self,\n                                must_have_imported_data_type=False,\n                                consider_annotations=False,\n                                consider_annotation_types=False):\n        # type: (bool, bool, bool) -> typing.List[ApiNamespace]\n        \"\"\"\n        Returns a list of Namespace objects. A namespace is a member of this\n        list if it is imported by the current namespace and a data type is\n        referenced from it. Namespaces are in ASCII order by name.\n\n        Args:\n            must_have_imported_data_type (bool): If true, result does not\n                include namespaces that were not imported for data types.\n            consider_annotations (bool): If false, result does not include\n                namespaces that were only imported for annotations\n            consider_annotation_types (bool): If false, result does not\n                include namespaces that were only imported for annotation types.\n\n        Returns:\n            List[Namespace]: A list of imported namespaces.\n        \"\"\"\n        imported_namespaces = []\n        for imported_namespace, reason in self._imported_namespaces.items():\n            if must_have_imported_data_type and not reason.data_type:\n                continue\n            if (not consider_annotations) and not (\n                    reason.data_type or reason.alias or reason.annotation_type\n            ):\n                continue\n            if (not consider_annotation_types) and not (\n                    reason.data_type or reason.alias or reason.annotation\n            ):\n                continue\n\n            imported_namespaces.append(imported_namespace)\n        imported_namespaces.sort(key=lambda n: n.name)\n        return imported_namespaces"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of Namespace objects that are members of this namespace that are imported by the route.", "response": "def get_namespaces_imported_by_route_io(self):\n        # type: () -> typing.List[ApiNamespace]\n        \"\"\"\n        Returns a list of Namespace objects. A namespace is a member of this\n        list if it is imported by the current namespace and has a data type\n        from it referenced as an argument, result, or error of a route.\n        Namespaces are in ASCII order by name.\n        \"\"\"\n        namespace_data_types = sorted(self.get_route_io_data_types(),\n                                      key=lambda dt: dt.name)\n        referenced_namespaces = set()\n        for data_type in namespace_data_types:\n            if data_type.namespace != self:\n                referenced_namespaces.add(data_type.namespace)\n        return sorted(referenced_namespaces, key=lambda n: n.name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnormalize the internal order of the internal list of resources.", "response": "def normalize(self):\n        # type: () -> None\n        \"\"\"\n        Alphabetizes routes to make route declaration order irrelevant.\n        \"\"\"\n        self.routes.sort(key=lambda route: route.name)\n        self.data_types.sort(key=lambda data_type: data_type.name)\n        self.aliases.sort(key=lambda alias: alias.name)\n        self.annotations.sort(key=lambda annotation: annotation.name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the attributes of the current object based on the attributes of the route.", "response": "def set_attributes(self, deprecated, doc, arg_data_type, result_data_type,\n                       error_data_type, attrs):\n        \"\"\"\n        Converts a forward reference definition of a route into a full\n        definition.\n\n        :param DeprecationInfo deprecated: Set if this route is deprecated.\n        :param str doc: Description of the endpoint.\n        :type arg_data_type: :class:`stone.data_type.DataType`\n        :type result_data_type: :class:`stone.data_type.DataType`\n        :type error_data_type: :class:`stone.data_type.DataType`\n        :param dict attrs: Map of string keys to values that are either int,\n            float, bool, str, or None. These are the route attributes assigned\n            in the spec.\n        \"\"\"\n        self.deprecated = deprecated\n        self.raw_doc = doc\n        self.doc = doc_unwrap(doc)\n        self.arg_data_type = arg_data_type\n        self.result_data_type = result_data_type\n        self.error_data_type = error_data_type\n        self.attrs = attrs"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget user - friendly representation of the route.", "response": "def name_with_version(self):\n        \"\"\"\n        Get user-friendly representation of the route.\n\n        :return: Route name with version suffix. The version suffix is omitted for version 1.\n        \"\"\"\n        if self.version == 1:\n            return self.name\n        else:\n            return '{}:{}'.format(self.name, self.version)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate(self, api):\n        # Create a file for each namespace.\n        for namespace in api.namespaces.values():\n            with self.output_to_relative_path('%s.stone' % namespace.name):\n                # Output a namespace header.\n                self.emit('namespace %s' % namespace.name)\n                # Output all data type (struct and union) definitions.\n                for data_type in namespace.linearize_data_types():\n                    self.generate_data_type(data_type)\n                # Output all route definitions.\n                for route in namespace.routes:\n                    self.generate_route(route)", "response": "Generate all the data types and routes for a given namespace."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef generate_data_type(self, data_type):\n        if isinstance(data_type, Struct):\n            # Output a struct definition.\n            self.emit('')\n            self.emit('struct %s' % data_type.name)\n            with self.indent():\n                if data_type.doc is not None:\n                    self.emit(self.format_string(data_type.doc))\n                for field in data_type.fields:\n                    type_repr = self.format_data_type(field.data_type)\n                    if not field.has_default:\n                        self.emit('%s %s' % (field.name, type_repr))\n                    else:\n                        self.emit('%s %s = %s' %\n                            (field.name, type_repr, self.format_value(field.default)))\n                    if field.doc is not None:\n                        with self.indent():\n                            self.emit(self.format_value(field.doc))\n        elif isinstance(data_type, Union):\n            # Output a union definition.\n            self.emit('')\n            self.emit('union %s' % data_type.name)\n            with self.indent():\n                if data_type.doc is not None:\n                    self.emit(self.format_string(data_type.doc))\n                for field in data_type.fields:\n                    name = field.name\n                    # Add a star for a catch-all field.\n                    # (There are two ways to recognize these.)\n                    if field.catch_all or field is data_type.catch_all_field:\n                        name += '*'\n                    if isinstance(field.data_type, Void):\n                        self.emit('%s' % (name))\n                    else:\n                        type_repr = self.format_data_type(field.data_type)\n                        self.emit('%s %s' % (name, type_repr))\n                    if field.doc is not None:\n                        with self.indent():\n                            self.emit(self.format_value(field.doc))\n        else:\n            # Don't know what this is.\n            self.emit('')\n            self.emit('# ??? %s' % repr(data_type))", "response": "Output a data type definition."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noutput a route definition.", "response": "def generate_route(self, route):\n        \"\"\"Output a route definition.\"\"\"\n        self.emit('')\n        self.emit('route %s (%s, %s, %s)' % (\n            route.name,\n            self.format_data_type(route.arg_data_type),\n            self.format_data_type(route.result_data_type),\n            self.format_data_type(route.error_data_type)\n        ))\n        # Output the docstring.\n        with self.indent():\n            if route.doc is not None:\n                self.emit(self.format_string(route.doc))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef format_data_type(self, data_type):\n        s = data_type.name\n        for type_class, key_list in self._data_type_map:\n            if isinstance(data_type, type_class):\n                args = []\n                for key in key_list:\n                    val = getattr(data_type, key)\n                    if val is not None:\n                        if isinstance(val, AstTypeRef):\n                            sval = val.name\n                        elif isinstance(val, DataType):\n                            sval = self.format_data_type(val)\n                        else:\n                            sval = self.format_value(val)\n                        args.append(key + '=' + sval)\n                if args:\n                    s += '(' + ', '.join(args) + ')'\n                break\n        if data_type.nullable:\n            s += '?'\n        return s", "response": "This function returns the name of the data type if it s a struct or union otherwise it returns the name of the parameters."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmap a Stone type to a Python type.", "response": "def map_stone_type_to_python_type(ns, data_type, override_dict=None):\n    # type: (ApiNamespace, DataType, typing.Optional[OverrideDefaultTypesDict]) -> typing.Text\n    \"\"\"\n    Args:\n        override_dict: lets you override the default behavior for a given type by hooking into\n            a callback. (Currently only hooked up for stone's List and Nullable)\n    \"\"\"\n    override_dict = override_dict or {}\n\n    if is_string_type(data_type):\n        string_override = override_dict.get(String, None)\n        if string_override:\n            return string_override(ns, data_type, override_dict)\n        return 'str'\n    elif is_bytes_type(data_type):\n        return 'bytes'\n    elif is_boolean_type(data_type):\n        return 'bool'\n    elif is_float_type(data_type):\n        return 'float'\n    elif is_integer_type(data_type):\n        return 'int'\n    elif is_void_type(data_type):\n        return 'None'\n    elif is_timestamp_type(data_type):\n        timestamp_override = override_dict.get(Timestamp, None)\n        if timestamp_override:\n            return timestamp_override(ns, data_type, override_dict)\n        return 'datetime.datetime'\n    elif is_alias(data_type):\n        alias_type = cast(Alias, data_type)\n        return map_stone_type_to_python_type(ns, alias_type.data_type, override_dict)\n    elif is_user_defined_type(data_type):\n        user_defined_type = cast(UserDefined, data_type)\n        class_name = class_name_for_data_type(user_defined_type)\n        if user_defined_type.namespace.name != ns.name:\n            return '{}.{}'.format(\n                fmt_namespace(user_defined_type.namespace.name), class_name)\n        else:\n            return class_name\n    elif is_list_type(data_type):\n        list_type = cast(List, data_type)\n        if List in override_dict:\n            return override_dict[List](ns, list_type.data_type, override_dict)\n\n        # PyCharm understands this description format for a list\n        return 'list of [{}]'.format(\n            map_stone_type_to_python_type(ns, list_type.data_type, override_dict)\n        )\n    elif is_map_type(data_type):\n        map_type = cast(Map, data_type)\n        if Map in override_dict:\n            return override_dict[Map](\n                ns,\n                data_type,\n                override_dict\n            )\n\n        return 'dict of [{}:{}]'.format(\n            map_stone_type_to_python_type(ns, map_type.key_data_type, override_dict),\n            map_stone_type_to_python_type(ns, map_type.value_data_type, override_dict)\n        )\n\n    elif is_nullable_type(data_type):\n        nullable_type = cast(Nullable, data_type)\n        if Nullable in override_dict:\n            return override_dict[Nullable](ns, nullable_type.data_type, override_dict)\n\n        return 'Optional[{}]'.format(\n            map_stone_type_to_python_type(ns, nullable_type.data_type, override_dict)\n        )\n    else:\n        raise TypeError('Unknown data type %r' % data_type)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_token(token_type, value, lineno, lexpos):\n    token = lex.LexToken()\n    token.type = token_type\n    token.value = value\n    token.lineno = lineno\n    token.lexpos = lexpos\n    return token", "response": "Helper for creating ply. lex. LexToken objects."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef input(self, file_data, **kwargs):\n        self.lex = lex.lex(module=self, **kwargs)\n        self.tokens_queue = []\n        self.cur_indent = 0\n        # Hack to avoid tokenization bugs caused by files that do not end in a\n        # new line.\n        self.lex.input(file_data + '\\n')", "response": "Input method for lexing the file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the next LexToken. Returns None when all tokens have been exhausted.", "response": "def token(self):\n        \"\"\"\n        Returns the next LexToken. Returns None when all tokens have been\n        exhausted.\n        \"\"\"\n\n        if self.tokens_queue:\n            self.last_token = self.tokens_queue.pop(0)\n        else:\n            r = self.lex.token()\n            if isinstance(r, MultiToken):\n                self.tokens_queue.extend(r.tokens)\n                self.last_token = self.tokens_queue.pop(0)\n            else:\n                if r is None and self.cur_indent > 0:\n                    if (self.last_token and\n                            self.last_token.type not in ('NEWLINE', 'LINE')):\n                        newline_token = _create_token(\n                            'NEWLINE', '\\n', self.lex.lineno, self.lex.lexpos)\n                        self.tokens_queue.append(newline_token)\n                    dedent_count = self.cur_indent\n                    dedent_token = _create_token(\n                        'DEDENT', '\\t', self.lex.lineno, self.lex.lexpos)\n                    self.tokens_queue.extend([dedent_token] * dedent_count)\n\n                    self.cur_indent = 0\n                    self.last_token = self.tokens_queue.pop(0)\n                else:\n                    self.last_token = r\n        return self.last_token"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef t_ANY_ID(self, token):\n        r'[a-zA-Z_][a-zA-Z0-9_-]*'\n        if token.value in self.KEYWORDS:\n            if (token.value == 'annotation_type') and self.cur_indent:\n                # annotation_type was added as a reserved keyword relatively\n                # late, when there could be identifers with the same name\n                # in existing specs. because annotation_type-the-keyword can\n                # only be used at the beginning of a non-indented line, this\n                # check lets both the keyword and the identifer coexist and\n                # maintains backward compatibility.\n                # Note: this is kind of a hack, and we should get rid of it if\n                # the lexer gets better at telling keywords from identifiers in general.\n                return token\n            token.type = self.RESERVED.get(token.value, 'KEYWORD')\n            return token\n        else:\n            return token", "response": "r Any ID in the grammar?"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the token that is the first comment of the file.", "response": "def t_INITIAL_comment(self, token):\n        r'[#][^\\n]*\\n+'\n        token.lexer.lineno += token.value.count('\\n')\n        # Scan backwards from the comment hash to figure out which type of\n        # comment this is. If we find an non-ws character, we know it was a\n        # partial line. But, if we find a newline before a non-ws character,\n        # then we know the entire line was a comment.\n        i = token.lexpos - 1\n        while i >= 0:\n            is_full_line_comment = token.lexer.lexdata[i] == '\\n'\n            is_partial_line_comment = (not is_full_line_comment and\n                                       token.lexer.lexdata[i] != ' ')\n            if is_full_line_comment or is_partial_line_comment:\n                newline_token = _create_token('NEWLINE', '\\n',\n                    token.lineno, token.lexpos + len(token.value) - 1)\n                newline_token.lexer = token.lexer\n                dent_tokens = self._create_tokens_for_next_line_dent(\n                    newline_token)\n                if is_full_line_comment:\n                    # Comment takes the full line so ignore entirely.\n                    return dent_tokens\n                elif is_partial_line_comment:\n                    # Comment is only a partial line. Preserve newline token.\n                    if dent_tokens:\n                        dent_tokens.tokens.insert(0, newline_token)\n                        return dent_tokens\n                    else:\n                        return newline_token\n            i -= 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef t_WSIGNORE_comment(self, token):\n        r'[#][^\\n]*\\n+'\n        token.lexer.lineno += token.value.count('\\n')\n        newline_token = _create_token('NEWLINE', '\\n',\n            token.lineno, token.lexpos + len(token.value) - 1)\n        newline_token.lexer = token.lexer\n        self._check_for_indent(newline_token)", "response": "t_WSIGNORE_comment - Handles the comment of the log file."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a MultiToken that is used to create the next line s indent or dedent tokens.", "response": "def _create_tokens_for_next_line_dent(self, newline_token):\n        \"\"\"\n        Starting from a newline token that isn't followed by another newline\n        token, returns any indent or dedent tokens that immediately follow.\n        If indentation doesn't change, returns None.\n        \"\"\"\n        indent_delta = self._get_next_line_indent_delta(newline_token)\n        if indent_delta is None or indent_delta == 0:\n            # Next line's indent isn't relevant OR there was no change in\n            # indentation.\n            return None\n\n        dent_type = 'INDENT' if indent_delta > 0 else 'DEDENT'\n        dent_token = _create_token(\n            dent_type, '\\t', newline_token.lineno + 1,\n            newline_token.lexpos + len(newline_token.value))\n\n        tokens = [dent_token] * abs(indent_delta)\n        self.cur_indent += indent_delta\n        return MultiToken(tokens)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the change in indentation between the next line and the current indentation level.", "response": "def _get_next_line_indent_delta(self, newline_token):\n        \"\"\"\n        Returns the change in indentation. The return units are in\n        indentations rather than spaces/tabs.\n\n        If the next line's indent isn't relevant (e.g. it's a comment),\n        returns None. Since the return value might be 0, the caller should\n        explicitly check the return type, rather than rely on truthiness.\n        \"\"\"\n        assert newline_token.type == 'NEWLINE', \\\n            'Can only search for a dent starting from a newline.'\n        next_line_pos = newline_token.lexpos + len(newline_token.value)\n        if next_line_pos == len(newline_token.lexer.lexdata):\n            # Reached end of file\n            return None\n\n        line = newline_token.lexer.lexdata[next_line_pos:].split(os.linesep, 1)[0]\n        if not line:\n            return None\n        lstripped_line = line.lstrip()\n        lstripped_line_length = len(lstripped_line)\n        if lstripped_line_length == 0:\n            # If the next line is composed of only spaces, ignore indentation.\n            return None\n        if lstripped_line[0] == '#':\n            # If it's a comment line, ignore indentation.\n            return None\n\n        indent = len(line) - lstripped_line_length\n        if indent % 4 > 0:\n            self.errors.append(\n                ('Indent is not divisible by 4.', newline_token.lexer.lineno))\n            return None\n\n        indent_delta = indent - _indent_level_to_spaces_count(self.cur_indent)\n        return indent_delta // 4"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nemit all necessary implementation file imports for the given Stone data type.", "response": "def _get_imports_m(self, data_types, default_imports):\n        \"\"\"Emits all necessary implementation file imports for the given Stone data type.\"\"\"\n        if not isinstance(data_types, list):\n            data_types = [data_types]\n\n        import_classes = default_imports\n\n        for data_type in data_types:\n            import_classes.append(fmt_class_prefix(data_type))\n\n            if data_type.parent_type:\n                import_classes.append(fmt_class_prefix(data_type.parent_type))\n\n            if is_struct_type(\n                    data_type) and data_type.has_enumerated_subtypes():\n                for _, subtype in data_type.get_all_subtypes_with_tags():\n                    import_classes.append(fmt_class_prefix(subtype))\n\n            for field in data_type.all_fields:\n                data_type, _ = unwrap_nullable(field.data_type)\n\n                # unpack list or map\n                while is_list_type(data_type) or is_map_type(data_type):\n                    data_type = (data_type.value_data_type if\n                        is_map_type(data_type) else data_type.data_type)\n\n                if is_user_defined_type(data_type):\n                    import_classes.append(fmt_class_prefix(data_type))\n\n        if import_classes:\n            import_classes = list(set(import_classes))\n            import_classes.sort()\n\n        return import_classes"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_imports_h(self, data_types):\n        if not isinstance(data_types, list):\n            data_types = [data_types]\n\n        import_classes = []\n\n        for data_type in data_types:\n\n            if is_user_defined_type(data_type):\n                import_classes.append(fmt_class_prefix(data_type))\n\n            for field in data_type.all_fields:\n                data_type, _ = unwrap_nullable(field.data_type)\n\n                # unpack list or map\n                while is_list_type(data_type) or is_map_type(data_type):\n                    data_type = (data_type.value_data_type if\n                        is_map_type(data_type) else data_type.data_type)\n\n                if is_user_defined_type(data_type):\n                    import_classes.append(fmt_class_prefix(data_type))\n\n        import_classes = list(set(import_classes))\n        import_classes.sort()\n\n        return import_classes", "response": "Emits all necessary header file imports for the given Stone data type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _struct_has_defaults(self, struct):\n        return [\n            f for f in struct.all_fields\n            if f.has_default or is_nullable_type(f.data_type)\n        ]", "response": "Returns whether the given struct has any default values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef build(self):\n        if os.path.exists(self.build_path) and not os.path.isdir(self.build_path):\n            self._logger.error('Output path must be a folder if it already exists')\n            return\n        Compiler._mkdir(self.build_path)\n        self._execute_backend_on_spec()", "response": "Creates outputs. Outputs are files made by a backend."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns True if the file name matches the format of a stone backend ie. its inner extension of the stone backend.", "response": "def is_stone_backend(cls, path):\n        \"\"\"\n        Returns True if the file name matches the format of a stone backend,\n        ie. its inner extension of \"stoneg\". For example: xyz.stoneg.py\n        \"\"\"\n        path_without_ext, _ = os.path.splitext(path)\n        _, second_ext = os.path.splitext(path_without_ext)\n        return second_ext == cls.backend_extension"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexecuting a backend on the spec.", "response": "def _execute_backend_on_spec(self):\n        \"\"\"Renders a source file into its final form.\"\"\"\n\n        api_no_aliases_cache = None\n        for attr_key in dir(self.backend_module):\n            attr_value = getattr(self.backend_module, attr_key)\n            if (inspect.isclass(attr_value) and\n                    issubclass(attr_value, Backend) and\n                    not inspect.isabstract(attr_value)):\n                self._logger.info('Running backend: %s', attr_value.__name__)\n                backend = attr_value(self.build_path, self.backend_args)\n\n                if backend.preserve_aliases:\n                    api = self.api\n                else:\n                    if not api_no_aliases_cache:\n                        api_no_aliases_cache = remove_aliases_from_api(self.api)\n                    api = api_no_aliases_cache\n\n                try:\n                    backend.generate(api)\n                except Exception:\n                    # Wrap this exception so that it isn't thought of as a bug\n                    # in the stone parser, but rather a bug in the backend.\n                    # Remove the last char of the traceback b/c it's a newline.\n                    raise BackendException(\n                        attr_value.__name__, traceback.format_exc()[:-1])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dictionary representation of the exception.", "response": "def to_dict(self):\n        \"\"\"Return a dictionary representation of the exception.\"\"\"\n        as_dict = dict(self.payload or ())\n        as_dict['message'] = self.message\n        return as_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of all columns required by the database to create the resource.", "response": "def required(cls):\n        \"\"\"Return a list of all columns required by the database to create the\n        resource.\n\n        :param cls: The Model class to gather attributes from\n        :rtype: list\n        \"\"\"\n        columns = []\n        for column in cls.__table__.columns:  # pylint: disable=no-member\n            is_autoincrement = 'int' in str(column.type).lower() and column.autoincrement\n            if (not column.nullable and not column.primary_key) or (column.primary_key and not is_autoincrement):\n                columns.append(column.name)\n        return columns"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of all nullable columns for the resource s table.", "response": "def optional(cls):\n        \"\"\"Return a list of all nullable columns for the resource's table.\n\n        :rtype: list\n        \"\"\"\n        columns = []\n        for column in cls.__table__.columns:  # pylint: disable=no-member\n            if column.nullable:\n                columns.append(column.name)\n        return columns"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the resource as a dictionary.", "response": "def to_dict(self):\n        \"\"\"Return the resource as a dictionary.\n\n        :rtype: dict\n        \"\"\"\n        result_dict = {}\n        for column in self.__table__.columns.keys():  # pylint: disable=no-member\n            value = result_dict[column] = getattr(self, column, None)\n            if isinstance(value, Decimal):\n                result_dict[column] = float(result_dict[column])\n            elif isinstance(value, datetime.datetime):\n                result_dict[column] = value.isoformat()\n        return result_dict"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a dictionary of links to related resources that should be included in the HTTP response.", "response": "def links(self):\n        \"\"\"Return a dictionary of links to related resources that should be\n        included in the *Link* header of an HTTP response.\n\n        :rtype: dict\n\n        \"\"\"\n        link_dict = {'self': self.resource_uri()}\n        for relationship in inspect(  # pylint: disable=maybe-no-member\n                self.__class__).relationships:\n            if 'collection' not in relationship.key:\n                instance = getattr(self, relationship.key)\n                if instance:\n                    link_dict[str(relationship.key)] = instance.resource_uri()\n        return link_dict"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, attributes):\n        for attribute in attributes:\n            setattr(self, attribute, attributes[attribute])\n        return self", "response": "Update the current instance based on attribute - > value items in the dictionary attributes."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a field - > data type dictionary describing this model .", "response": "def description(cls):\n        \"\"\"Return a field->data type dictionary describing this model\n        as reported by the database.\n\n        :rtype: dict\n        \"\"\"\n\n        description = {}\n        for column in cls.__table__.columns:  # pylint: disable=no-member\n            column_description = str(column.type)\n            if not column.nullable:\n                column_description += ' (required)'\n            description[column.name] = column_description\n        return description"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an application instance connected to the database described in database_uri.", "response": "def get_app(\n        database_uri,\n        exclude_tables=None,\n        user_models=None,\n        reflect_all=True,\n        read_only=False,\n        schema=None):\n    \"\"\"Return an application instance connected to the database described in\n    *database_uri*.\n\n    :param str database_uri: The URI connection string for the database\n    :param list exclude_tables: A list of tables to exclude from the API\n                                service\n    :param list user_models: A list of user-defined models to include in the\n                             API service\n    :param bool reflect_all: Include all database tables in the API service\n    :param bool read_only: Only allow HTTP GET commands for all endpoints\n    :param str schema: Use the specified named schema instead of the default\n    \"\"\"\n    app = Flask('sandman2')\n    app.config['SQLALCHEMY_DATABASE_URI'] = database_uri\n    app.config['SANDMAN2_READ_ONLY'] = read_only\n    app.config['SQLALCHEMY_TRACK_MODIFICATIONS'] = False\n    app.classes = []\n    db.init_app(app)\n    admin = Admin(app, base_template='layout.html', template_mode='bootstrap3')\n    _register_error_handlers(app)\n    if user_models:\n        with app.app_context():\n            _register_user_models(user_models, admin, schema=schema)\n    elif reflect_all:\n        with app.app_context():\n            _reflect_all(exclude_tables, admin, read_only, schema=schema)\n\n    @app.route('/')\n    def index():\n        \"\"\"Return a list of routes to the registered classes.\"\"\"\n        routes = {}\n        for cls in app.classes:\n            routes[cls.__model__.__name__] = '{}{{/{}}}'.format(\n                cls.__model__.__url__,\n                cls.__model__.primary_key())\n        return jsonify(routes)\n    return app"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register_service(cls, primary_key_type):\n    view_func = cls.as_view(cls.__name__.lower())  # pylint: disable=no-member\n    methods = set(cls.__model__.__methods__)  # pylint: disable=no-member\n\n    if 'GET' in methods:  # pylint: disable=no-member\n        current_app.add_url_rule(\n            cls.__model__.__url__ + '/', defaults={'resource_id': None},\n            view_func=view_func,\n            methods=['GET'])\n        current_app.add_url_rule(\n            '{resource}/meta'.format(resource=cls.__model__.__url__),\n            view_func=view_func,\n            methods=['GET'])\n    if 'POST' in methods:  # pylint: disable=no-member\n        current_app.add_url_rule(\n            cls.__model__.__url__ + '/', view_func=view_func, methods=['POST', ])\n    current_app.add_url_rule(\n        '{resource}/<{pk_type}:{pk}>'.format(\n            resource=cls.__model__.__url__,\n            pk='resource_id', pk_type=primary_key_type),\n        view_func=view_func,\n        methods=methods - {'POST'})\n    current_app.classes.append(cls)", "response": "Register an API service endpoint."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering all tables in the given database as services.", "response": "def _reflect_all(exclude_tables=None, admin=None, read_only=False, schema=None):\n    \"\"\"Register all tables in the given database as services.\n\n    :param list exclude_tables: A list of tables to exclude from the API\n                                service\n    \"\"\"\n    AutomapModel.prepare(  # pylint:disable=maybe-no-member\n        db.engine, reflect=True, schema=schema)\n    for cls in AutomapModel.classes:\n        if exclude_tables and cls.__table__.name in exclude_tables:\n            continue\n        if read_only:\n            cls.__methods__ = {'GET'}\n        register_model(cls, admin)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a model to be included in the API service", "response": "def register_model(cls, admin=None):\n    \"\"\"Register *cls* to be included in the API service\n\n    :param cls: Class deriving from :class:`sandman2.models.Model`\n    \"\"\"\n    cls.__url__ = '/{}'.format(cls.__name__.lower())\n    service_class = type(\n        cls.__name__ + 'Service',\n        (Service,),\n        {\n            '__model__': cls,\n        })\n\n    # inspect primary key\n    cols = list(cls().__table__.primary_key.columns)\n\n    # composite keys not supported (yet)\n    primary_key_type = 'string'\n    if len(cols) == 1:\n        col_type = cols[0].type\n        # types defined at http://flask.pocoo.org/docs/0.10/api/#url-route-registrations\n        if isinstance(col_type, sqltypes.String):\n            primary_key_type = 'string'\n        elif isinstance(col_type, sqltypes.Integer):\n            primary_key_type = 'int'\n        elif isinstance(col_type, sqltypes.Numeric):\n            primary_key_type = 'float'\n\n    # registration\n    register_service(service_class, primary_key_type)\n    if admin is not None:\n        admin.add_view(CustomAdminView(cls, db.session))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _register_user_models(user_models, admin=None, schema=None):\n    if any([issubclass(cls, AutomapModel) for cls in user_models]):\n        AutomapModel.prepare(  # pylint:disable=maybe-no-member\n                               db.engine, reflect=True, schema=schema)\n\n    for user_model in user_models:\n        register_model(user_model, admin)", "response": "Register any user - defined models with the API Service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding the proper link headers to the response object based on the contents of links.", "response": "def add_link_headers(response, links):\n    \"\"\"Return *response* with the proper link headers set, based on the contents\n    of *links*.\n\n    :param response: :class:`flask.Response` response object for links to be\n                     added\n    :param dict links: Dictionary of links to be added\n    :rtype :class:`flask.Response` :\n    \"\"\"\n    link_string = '<{}>; rel=self'.format(links['self'])\n    for link in links.values():\n        link_string += ', <{}>; rel=related'.format(link)\n    response.headers['Link'] = link_string\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a Flask Response object containing a JSON representation of resource.", "response": "def jsonify(resource):\n    \"\"\"Return a Flask ``Response`` object containing a\n    JSON representation of *resource*.\n\n    :param resource: The resource to act as the basis of the response\n    \"\"\"\n\n    response = flask.jsonify(resource.to_dict())\n    response = add_link_headers(response, resource.links())\n    return response"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid_method(model, resource=None):\n    validation_function_name = 'is_valid_{}'.format(\n        request.method.lower())\n    if hasattr(model, validation_function_name):\n        return getattr(model, validation_function_name)(request, resource)", "response": "Return the error message to be sent to the client if the request passes any user - defined validation."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an HTTP response object resulting from a HTTP DELETE call.", "response": "def delete(self, resource_id):\n        \"\"\"Return an HTTP response object resulting from a HTTP DELETE call.\n\n        :param resource_id: The value of the resource's primary key\n        \"\"\"\n        resource = self._resource(resource_id)\n        error_message = is_valid_method(self.__model__, resource)\n        if error_message:\n            raise BadRequestException(error_message)\n        db.session().delete(resource)\n        db.session().commit()\n        return self._no_content_response()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, resource_id=None):\n        if request.path.endswith('meta'):\n            return self._meta()\n\n        if resource_id is None:\n            error_message = is_valid_method(self.__model__)\n            if error_message:\n                raise BadRequestException(error_message)\n\n            if 'export' in request.args: \n                return self._export(self._all_resources())\n\n            return flask.jsonify({\n                self.__json_collection_name__: self._all_resources()\n                })\n        else:\n            resource = self._resource(resource_id)\n            error_message = is_valid_method(self.__model__, resource)\n            if error_message:\n                raise BadRequestException(error_message)\n            return jsonify(resource)", "response": "Return an HTTP response object resulting from an HTTP GET call."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef patch(self, resource_id):\n        resource = self._resource(resource_id)\n        error_message = is_valid_method(self.__model__, resource)\n        if error_message:\n            raise BadRequestException(error_message)\n        if not request.json:\n            raise BadRequestException('No JSON data received')\n        resource.update(request.json)\n        db.session().merge(resource)\n        db.session().commit()\n        return jsonify(resource)", "response": "Returns an HTTP response object resulting from an HTTP PATCH call."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef post(self):\n        resource = self.__model__.query.filter_by(**request.json).first()\n        if resource:\n            error_message = is_valid_method(self.__model__, resource)\n            if error_message:\n                raise BadRequestException(error_message)\n            return self._no_content_response()\n\n        resource = self.__model__(**request.json)  # pylint: disable=not-callable\n        error_message = is_valid_method(self.__model__, resource)\n        if error_message:\n            raise BadRequestException(error_message)\n        db.session().add(resource)\n        db.session().commit()\n        return self._created_response(resource)", "response": "Returns the JSON representation of a new resource created through\n        an HTTP POST call."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef put(self, resource_id):\n        resource = self.__model__.query.get(resource_id)\n        if resource:\n            error_message = is_valid_method(self.__model__, resource)\n            if error_message:\n                raise BadRequestException(error_message)\n            resource.update(request.json)\n            db.session().merge(resource)\n            db.session().commit()\n            return jsonify(resource)\n\n        resource = self.__model__(**request.json)  # pylint: disable=not-callable\n        error_message = is_valid_method(self.__model__, resource)\n        if error_message:\n            raise BadRequestException(error_message)\n        db.session().add(resource)\n        db.session().commit()\n        return self._created_response(resource)", "response": "Return the JSON representation of a new resource created or updated by an HTTP PUT call."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the model instance with the given resource_id.", "response": "def _resource(self, resource_id):\n        \"\"\"Return the ``sandman2.model.Model`` instance with the given\n        *resource_id*.\n\n        :rtype: :class:`sandman2.model.Model`\n        \"\"\"\n        resource = self.__model__.query.get(resource_id)\n        if not resource:\n            raise NotFoundException()\n        return resource"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the complete collection of resources as a list of dicts.", "response": "def _all_resources(self):\n        \"\"\"Return the complete collection of resources as a list of\n        dictionaries.\n\n        :rtype: :class:`sandman2.model.Model`\n        \"\"\"\n        queryset = self.__model__.query\n        args = {k: v for (k, v) in request.args.items() if k not in ('page', 'export')}\n        limit = None\n        if args:\n            filters = []\n            order = []\n            for key, value in args.items():\n                if value.startswith('%'):\n                    filters.append(getattr(self.__model__, key).like(str(value), escape='/'))\n                elif key == 'sort':\n                    direction = desc if value.startswith('-') else asc\n                    order.append(direction(getattr(self.__model__, value.lstrip('-'))))\n                elif key == 'limit':\n                    limit = int(value)\n                elif hasattr(self.__model__, key):\n                    filters.append(getattr(self.__model__, key) == value)\n                else:\n                    raise BadRequestException('Invalid field [{}]'.format(key))\n                queryset = queryset.filter(*filters).order_by(*order)\n        if 'page' in request.args:\n            resources = queryset.paginate(page=int(request.args['page']), per_page=limit).items\n        else:\n            queryset = queryset.limit(limit)\n            resources = queryset.all()\n        return [r.to_dict() for r in resources]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _export(self, collection):\n        fieldnames = collection[0].keys()\n        faux_csv = ','.join(fieldnames) + '\\r\\n'\n        for resource in collection:\n            faux_csv += ','.join((str(x) for x in resource.values())) + '\\r\\n'\n        response = make_response(faux_csv)\n        response.mimetype = 'text/csv'\n        return response", "response": "Export the resources in a list of dicts"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    parser = argparse.ArgumentParser(\n        description='Auto-generate a RESTful API service '\n        'from an existing database.'\n        )\n    parser.add_argument(\n        'URI',\n        help='Database URI in the format '\n             'postgresql+psycopg2://user:password@host/database')\n    parser.add_argument(\n        '-d',\n        '--debug',\n        help='Turn on debug logging',\n        action='store_true',\n        default=False)\n    parser.add_argument(\n        '-p',\n        '--port',\n        help='Port for service to listen on',\n        default=5000)\n    parser.add_argument(\n        '-l',\n        '--local-only',\n        help='Only provide service on localhost (will not be accessible'\n             ' from other machines)',\n        action='store_true',\n        default=False)\n    parser.add_argument(\n        '-r',\n        '--read-only',\n        help='Make all database resources read-only (i.e. only the HTTP GET method is supported)',\n        action='store_true',\n        default=False)\n    parser.add_argument(\n        '-s',\n        '--schema',\n        help='Use this named schema instead of default',\n        default=None)\n\n\n    args = parser.parse_args()\n    app = get_app(args.URI, read_only=args.read_only, schema=args.schema)\n    if args.debug:\n        app.config['DEBUG'] = True\n    if args.local_only:\n        host = '127.0.0.1'\n    else:\n        host = '0.0.0.0'\n    app.config['SECRET_KEY'] = '42'\n    app.run(host=host, port=int(args.port))", "response": "Entry point for script."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef etag(func):\n    @functools.wraps(func)\n    def wrapped(*args, **kwargs):\n        \"\"\"Call the view function and generate an ETag value, checking the\n        headers to determine what response to send.\"\"\"\n        # only for HEAD and GET requests\n        assert request.method in ['HEAD', 'GET'],\\\n            '@etag is only supported for GET requests'\n        response = func(*args, **kwargs)\n        response = make_response(response)\n        etag_value = '\"' + hashlib.md5(response.get_data()).hexdigest() + '\"'\n        response.headers['ETag'] = etag_value\n        if_match = request.headers.get('If-Match')\n        if_none_match = request.headers.get('If-None-Match')\n        if if_match:\n            etag_list = [tag.strip() for tag in if_match.split(',')]\n            if etag_value not in etag_list and '*' not in etag_list:\n                response = precondition_failed()\n        elif if_none_match:\n            etag_list = [tag.strip() for tag in if_none_match.split(',')]\n            if etag_value in etag_list or '*' in etag_list:\n                response = not_modified()\n        return response\n    return wrapped", "response": "Returns a decorator that generates proper ETag values for a response."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmerges extra attributes from LogRecord object into target dictionary.", "response": "def merge_record_extra(record, target, reserved):\n    \"\"\"\n    Merges extra attributes from LogRecord object into target dictionary\n\n    :param record: logging.LogRecord\n    :param target: dict to update\n    :param reserved: dict or list with reserved keys to skip\n    \"\"\"\n    for key, value in record.__dict__.items():\n        # this allows to have numeric keys\n        if (key not in reserved\n            and not (hasattr(key, \"startswith\")\n                     and key.startswith('_'))):\n            target[key] = value\n    return target"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nparse a string such as package. module. function and returns the function.", "response": "def _str_to_fn(self, fn_as_str):\n        \"\"\"\n        If the argument is not a string, return whatever was passed in.\n        Parses a string such as package.module.function, imports the module\n        and returns the function.\n\n        :param fn_as_str: The string to parse. If not a string, return it.\n        \"\"\"\n        if not isinstance(fn_as_str, str):\n            return fn_as_str\n\n        path, _, function = fn_as_str.rpartition('.')\n        module = importlib.import_module(path)\n        return getattr(module, function)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nparses the format string looking for substitutions", "response": "def parse(self):\n        \"\"\"\n        Parses format string looking for substitutions\n\n        This method is responsible for returning a list of fields (as strings)\n        to include in all log messages.\n        \"\"\"\n        standard_formatters = re.compile(r'\\((.+?)\\)', re.IGNORECASE)\n        return standard_formatters.findall(self._fmt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds fields to the log record.", "response": "def add_fields(self, log_record, record, message_dict):\n        \"\"\"\n        Override this method to implement custom logic for adding fields.\n        \"\"\"\n        for field in self._required_fields:\n            log_record[field] = record.__dict__.get(field)\n        log_record.update(message_dict)\n        merge_record_extra(record, log_record, reserved=self._skip_fields)\n\n        if self.timestamp:\n            key = self.timestamp if type(self.timestamp) == str else 'timestamp'\n            log_record[key] = datetime.utcnow()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a json string of the log record.", "response": "def jsonify_log_record(self, log_record):\n        \"\"\"Returns a json string of the log record.\"\"\"\n        return self.json_serializer(log_record,\n                                    default=self.json_default,\n                                    cls=self.json_encoder,\n                                    indent=self.json_indent,\n                                    ensure_ascii=self.json_ensure_ascii)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting a log record and serializes to json", "response": "def format(self, record):\n        \"\"\"Formats a log record and serializes to json\"\"\"\n        message_dict = {}\n        if isinstance(record.msg, dict):\n            message_dict = record.msg\n            record.message = None\n        else:\n            record.message = record.getMessage()\n        # only format time if needed\n        if \"asctime\" in self._required_fields:\n            record.asctime = self.formatTime(record, self.datefmt)\n\n        # Display formatted exception, but allow overriding it in the\n        # user-supplied dict.\n        if record.exc_info and not message_dict.get('exc_info'):\n            message_dict['exc_info'] = self.formatException(record.exc_info)\n        if not message_dict.get('exc_info') and record.exc_text:\n            message_dict['exc_info'] = record.exc_text\n        # Display formatted record of stack frames\n        # default format is a string returned from :func:`traceback.print_stack`\n        try:\n            if record.stack_info and not message_dict.get('stack_info'):\n                message_dict['stack_info'] = self.formatStack(record.stack_info)\n        except AttributeError:\n            # Python2.7 doesn't have stack_info.\n            pass\n\n        try:\n            log_record = OrderedDict()\n        except NameError:\n            log_record = {}\n\n        self.add_fields(log_record, record, message_dict)\n        log_record = self.process_log_record(log_record)\n\n        return \"%s%s\" % (self.prefix, self.jsonify_log_record(log_record))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting a dataframe to a list of arrays with one array for every unique index entry.", "response": "def to_array_list(df, length=None, by_id=True):\n    \"\"\"Converts a dataframe to a list of arrays, with one array for every unique index entry.\n    Index is assumed to be 0-based contiguous. If there is a missing index entry, an empty\n    numpy array is returned for it.\n    Elements in the arrays are sorted by their id.\n    :param df:\n    :param length:\n    :return:\n    \"\"\"\n\n    if by_id:\n        assert 'id' in df.columns\n\n        # if `id` is the only column, don't sort it (and don't remove it)\n        if len(df.columns) == 1:\n            by_id = False\n\n    idx = df.index.unique()\n    if length is None:\n        length = max(idx) + 1\n\n    l = [np.empty(0) for _ in xrange(length)]\n    for i in idx:\n        a = df.loc[i]\n        if by_id:\n            if isinstance(a, pd.Series):\n                a = a[1:]\n            else:\n                a = a.copy().set_index('id').sort_index()\n\n        l[i] = a.values.reshape((-1, a.shape[-1]))\n    return np.asarray(l)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bbox(self):\n        bbox = self._df[['id', 'x1', 'y1', 'x2', 'y2']].copy()\n        # TODO: Fix this to become x, y, w, h\n        if self.bbox_with_size:\n            bbox['y2'] -= bbox['y1']\n            bbox['x2'] -= bbox['x1']\n\n        \"\"\"Converts a dataframe to a list of arrays\n        :param df:\n        :param length:\n        :return:\n        \"\"\"\n\n        return to_array_list(bbox)", "response": "Converts a dataframe to a list of arrays where each element is a tuple of x y w h where the element is the bounding box of the user s entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nassigns a new identity to an objects that appear after disappearing previously.", "response": "def _split_on_reappear(cls, df, p, id_offset):\n        \"\"\"Assign a new identity to an objects that appears after disappearing previously.\n        Works on `df` in-place.\n        :param df: data frame\n        :param p: presence\n        :param id_offset: offset added to new ids\n        :return:\n        \"\"\"\n\n        next_id = id_offset + 1\n        added_ids = []\n        nt = p.sum(0)\n        start = np.argmax(p, 0)\n        end = np.argmax(np.cumsum(p, 0), 0)\n        diff = end - start + 1\n        is_contiguous = np.equal(nt, diff)\n        for id, contiguous in enumerate(is_contiguous):\n            if not contiguous:\n\n                to_change = df[df.id == id]\n                index = to_change.index\n                diff = index[1:] - index[:-1]\n                where = np.where(np.greater(diff, 1))[0]\n                for w in where:\n                    to_change.loc[w + 1:, 'id'] = next_id\n                    added_ids.append(next_id)\n                    next_id += 1\n\n                df[df.id == id] = to_change\n\n        return added_ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfinds and list data files for each sensor.", "response": "def _get_file_lists(self):\n        \"\"\"Find and list data files for each sensor.\"\"\"\n        self.cam0_files = sorted(glob.glob(\n            os.path.join(self.sequence_path, 'image_0',\n                         '*.{}'.format(self.imtype))))\n        self.cam1_files = sorted(glob.glob(\n            os.path.join(self.sequence_path, 'image_1',\n                         '*.{}'.format(self.imtype))))\n        self.cam2_files = sorted(glob.glob(\n            os.path.join(self.sequence_path, 'image_2',\n                         '*.{}'.format(self.imtype))))\n        self.cam3_files = sorted(glob.glob(\n            os.path.join(self.sequence_path, 'image_3',\n                         '*.{}'.format(self.imtype))))\n        self.velo_files = sorted(glob.glob(\n            os.path.join(self.sequence_path, 'velodyne',\n                         '*.bin')))\n\n        # Subselect the chosen range of frames, if any\n        if self.frames is not None:\n            self.cam0_files = utils.subselect_files(\n                self.cam0_files, self.frames)\n            self.cam1_files = utils.subselect_files(\n                self.cam1_files, self.frames)\n            self.cam2_files = utils.subselect_files(\n                self.cam2_files, self.frames)\n            self.cam3_files = utils.subselect_files(\n                self.cam3_files, self.frames)\n            self.velo_files = utils.subselect_files(\n                self.velo_files, self.frames)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nload and compute intrinsic and extrinsic calibration parameters.", "response": "def _load_calib(self):\n        \"\"\"Load and compute intrinsic and extrinsic calibration parameters.\"\"\"\n        # We'll build the calibration parameters as a dictionary, then\n        # convert it to a namedtuple to prevent it from being modified later\n        data = {}\n\n        # Load the calibration file\n        calib_filepath = os.path.join(self.sequence_path, 'calib.txt')\n        filedata = utils.read_calib_file(calib_filepath)\n\n        # Create 3x4 projection matrices\n        P_rect_00 = np.reshape(filedata['P0'], (3, 4))\n        P_rect_10 = np.reshape(filedata['P1'], (3, 4))\n        P_rect_20 = np.reshape(filedata['P2'], (3, 4))\n        P_rect_30 = np.reshape(filedata['P3'], (3, 4))\n\n        data['P_rect_00'] = P_rect_00\n        data['P_rect_10'] = P_rect_10\n        data['P_rect_20'] = P_rect_20\n        data['P_rect_30'] = P_rect_30\n\n        # Compute the rectified extrinsics from cam0 to camN\n        T1 = np.eye(4)\n        T1[0, 3] = P_rect_10[0, 3] / P_rect_10[0, 0]\n        T2 = np.eye(4)\n        T2[0, 3] = P_rect_20[0, 3] / P_rect_20[0, 0]\n        T3 = np.eye(4)\n        T3[0, 3] = P_rect_30[0, 3] / P_rect_30[0, 0]\n\n        # Compute the velodyne to rectified camera coordinate transforms\n        data['T_cam0_velo'] = np.reshape(filedata['Tr'], (3, 4))\n        data['T_cam0_velo'] = np.vstack([data['T_cam0_velo'], [0, 0, 0, 1]])\n        data['T_cam1_velo'] = T1.dot(data['T_cam0_velo'])\n        data['T_cam2_velo'] = T2.dot(data['T_cam0_velo'])\n        data['T_cam3_velo'] = T3.dot(data['T_cam0_velo'])\n\n        # Compute the camera intrinsics\n        data['K_cam0'] = P_rect_00[0:3, 0:3]\n        data['K_cam1'] = P_rect_10[0:3, 0:3]\n        data['K_cam2'] = P_rect_20[0:3, 0:3]\n        data['K_cam3'] = P_rect_30[0:3, 0:3]\n\n        # Compute the stereo baselines in meters by projecting the origin of\n        # each camera frame into the velodyne frame and computing the distances\n        # between them\n        p_cam = np.array([0, 0, 0, 1])\n        p_velo0 = np.linalg.inv(data['T_cam0_velo']).dot(p_cam)\n        p_velo1 = np.linalg.inv(data['T_cam1_velo']).dot(p_cam)\n        p_velo2 = np.linalg.inv(data['T_cam2_velo']).dot(p_cam)\n        p_velo3 = np.linalg.inv(data['T_cam3_velo']).dot(p_cam)\n\n        data['b_gray'] = np.linalg.norm(p_velo1 - p_velo0)  # gray baseline\n        data['b_rgb'] = np.linalg.norm(p_velo3 - p_velo2)   # rgb baseline\n\n        self.calib = namedtuple('CalibData', data.keys())(*data.values())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload timestamps from file.", "response": "def _load_timestamps(self):\n        \"\"\"Load timestamps from file.\"\"\"\n        timestamp_file = os.path.join(self.sequence_path, 'times.txt')\n\n        # Read and parse the timestamps\n        self.timestamps = []\n        with open(timestamp_file, 'r') as f:\n            for line in f.readlines():\n                t = dt.timedelta(seconds=float(line))\n                self.timestamps.append(t)\n\n        # Subselect the chosen range of frames, if any\n        if self.frames is not None:\n            self.timestamps = [self.timestamps[i] for i in self.frames]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _load_poses(self):\n        pose_file = os.path.join(self.pose_path, self.sequence + '.txt')\n\n        # Read and parse the poses\n        poses = []\n        try:\n            with open(pose_file, 'r') as f:\n                lines = f.readlines()\n                if self.frames is not None:\n                    lines = [lines[i] for i in self.frames]\n\n                for line in lines:\n                    T_w_cam0 = np.fromstring(line, dtype=float, sep=' ')\n                    T_w_cam0 = T_w_cam0.reshape(3, 4)\n                    T_w_cam0 = np.vstack((T_w_cam0, [0, 0, 0, 1]))\n                    poses.append(T_w_cam0)\n\n        except FileNotFoundError:\n            print('Ground truth poses are not available for sequence ' +\n                  self.sequence + '.')\n\n        self.poses = poses", "response": "Load ground truth poses from file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rotx(t):\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[1,  0,  0],\n                     [0,  c, -s],\n                     [0,  s,  c]])", "response": "Rotation about the x - axis."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef roty(t):\n    c = np.cos(t)\n    s = np.sin(t)\n    return np.array([[c,  0,  s],\n                     [0,  1,  0],\n                     [-s, 0,  c]])", "response": "Rotation about the y - axis."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transform_from_rot_trans(R, t):\n    R = R.reshape(3, 3)\n    t = t.reshape(3, 1)\n    return np.vstack((np.hstack([R, t]), [0, 0, 0, 1]))", "response": "Transforation matrix from rotation matrix and translation vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_calib_file(filepath):\n    data = {}\n\n    with open(filepath, 'r') as f:\n        for line in f.readlines():\n            key, value = line.split(':', 1)\n            # The only non-float values in these files are dates, which\n            # we don't care about anyway\n            try:\n                data[key] = np.array([float(x) for x in value.split()])\n            except ValueError:\n                pass\n\n    return data", "response": "Read in a calibration file and parse into a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute a homogeneous pose matrix from an OXTS packet.", "response": "def pose_from_oxts_packet(packet, scale):\n    \"\"\"Helper method to compute a SE(3) pose matrix from an OXTS packet.\n    \"\"\"\n    er = 6378137.  # earth radius (approx.) in meters\n\n    # Use a Mercator projection to get the translation vector\n    tx = scale * packet.lon * np.pi * er / 180.\n    ty = scale * er * \\\n        np.log(np.tan((90. + packet.lat) * np.pi / 360.))\n    tz = packet.alt\n    t = np.array([tx, ty, tz])\n\n    # Use the Euler angles to get the rotation matrix\n    Rx = rotx(packet.roll)\n    Ry = roty(packet.pitch)\n    Rz = rotz(packet.yaw)\n    R = Rz.dot(Ry.dot(Rx))\n\n    # Combine the translation and rotation into a homogeneous transform\n    return R, t"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_oxts_packets_and_poses(oxts_files):\n    # Scale for Mercator projection (from first lat value)\n    scale = None\n    # Origin of the global coordinate system (first GPS position)\n    origin = None\n\n    oxts = []\n\n    for filename in oxts_files:\n        with open(filename, 'r') as f:\n            for line in f.readlines():\n                line = line.split()\n                # Last five entries are flags and counts\n                line[:-5] = [float(x) for x in line[:-5]]\n                line[-5:] = [int(float(x)) for x in line[-5:]]\n\n                packet = OxtsPacket(*line)\n\n                if scale is None:\n                    scale = np.cos(packet.lat * np.pi / 180.)\n\n                R, t = pose_from_oxts_packet(packet, scale)\n\n                if origin is None:\n                    origin = t\n\n                T_w_imu = transform_from_rot_trans(R, t - origin)\n\n                oxts.append(OxtsData(packet, T_w_imu))\n\n    return oxts", "response": "Generator to read OXTS ground truth data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_velo_scan(file):\n    scan = np.fromfile(file, dtype=np.float32)\n    return scan.reshape((-1, 4))", "response": "Load and parse a velodyne binary file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a rigid transform calibration file as a numpy. array.", "response": "def _load_calib_rigid(self, filename):\n        \"\"\"Read a rigid transform calibration file as a numpy.array.\"\"\"\n        filepath = os.path.join(self.calib_path, filename)\n        data = utils.read_calib_file(filepath)\n        return utils.transform_from_rot_trans(data['R'], data['T'])"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading and compute intrinsic and extrinsic calibration parameters.", "response": "def _load_calib(self):\n        \"\"\"Load and compute intrinsic and extrinsic calibration parameters.\"\"\"\n        # We'll build the calibration parameters as a dictionary, then\n        # convert it to a namedtuple to prevent it from being modified later\n        data = {}\n\n        # Load the rigid transformation from IMU to velodyne\n        data['T_velo_imu'] = self._load_calib_rigid('calib_imu_to_velo.txt')\n\n        # Load the camera intrinsics and extrinsics\n        data.update(self._load_calib_cam_to_cam(\n            'calib_velo_to_cam.txt', 'calib_cam_to_cam.txt'))\n\n        # Pre-compute the IMU to rectified camera coordinate transforms\n        data['T_cam0_imu'] = data['T_cam0_velo'].dot(data['T_velo_imu'])\n        data['T_cam1_imu'] = data['T_cam1_velo'].dot(data['T_velo_imu'])\n        data['T_cam2_imu'] = data['T_cam2_velo'].dot(data['T_velo_imu'])\n        data['T_cam3_imu'] = data['T_cam3_velo'].dot(data['T_velo_imu'])\n\n        self.calib = namedtuple('CalibData', data.keys())(*data.values())"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nload timestamps from file.", "response": "def _load_timestamps(self):\n        \"\"\"Load timestamps from file.\"\"\"\n        timestamp_file = os.path.join(\n            self.data_path, 'oxts', 'timestamps.txt')\n\n        # Read and parse the timestamps\n        self.timestamps = []\n        with open(timestamp_file, 'r') as f:\n            for line in f.readlines():\n                # NB: datetime only supports microseconds, but KITTI timestamps\n                # give nanoseconds, so need to truncate last 4 characters to\n                # get rid of \\n (counts as 1) and extra 3 digits\n                t = dt.datetime.strptime(line[:-4], '%Y-%m-%d %H:%M:%S.%f')\n                self.timestamps.append(t)\n\n        # Subselect the chosen range of frames, if any\n        if self.frames is not None:\n            self.timestamps = [self.timestamps[i] for i in self.frames]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndetermines the file type based on the filename.", "response": "def determine_file_type(filename):\n    \"\"\"\n    :param filename: str\n\n    :rtype: FileType\n    \"\"\"\n    if filename.endswith('.cls'):\n        return FileType.CLS\n    elif filename.endswith('.go'):\n        return FileType.GO\n    elif filename.endswith('.java'):\n        return FileType.JAVA\n    elif filename.endswith('.js'):\n        return FileType.JAVASCRIPT\n    elif filename.endswith('.php'):\n        return FileType.PHP\n    elif filename.endswith('.py'):\n        return FileType.PYTHON\n    elif (\n        filename.endswith(\n            ('.yaml', '.yml'),\n        )\n    ):\n        return FileType.YAML\n    return FileType.OTHER"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef write_baseline_to_file(filename, data):\n    with open(filename, 'w') as f:  # pragma: no cover\n        f.write(format_baseline_for_output(data) + '\\n')", "response": "Writes baseline to file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_baseline_from_string(cls, string):\n        try:\n            return cls.load_baseline_from_dict(json.loads(string))\n        except (IOError, ValueError):\n            log.error('Incorrectly formatted baseline!')\n            raise", "response": "Initializes a SecretsCollection object from a string."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize a SecretsCollection object from a dictionary.", "response": "def load_baseline_from_dict(cls, data):\n        \"\"\"Initializes a SecretsCollection object from dictionary.\n\n        :type data: dict\n        :param data: properly formatted dictionary to load SecretsCollection from.\n\n        :rtype: SecretsCollection\n        :raises: IOError\n        \"\"\"\n        result = SecretsCollection()\n\n        if not all(key in data for key in (\n            'plugins_used',\n            'results',\n        )):\n            raise IOError\n\n        # In v0.12.0 `exclude_regex` got replaced by `exclude`\n        if not any(key in data for key in (\n            'exclude',\n            'exclude_regex',\n        )):\n            raise IOError\n\n        if 'exclude_regex' in data:\n            result.exclude_files = data['exclude_regex']\n        else:\n            result.exclude_files = data['exclude']['files']\n            result.exclude_lines = data['exclude']['lines']\n\n        plugins = []\n        for plugin in data['plugins_used']:\n            plugin_classname = plugin.pop('name')\n            plugins.append(initialize.from_plugin_classname(\n                plugin_classname,\n                exclude_lines_regex=result.exclude_lines,\n                **plugin\n            ))\n        result.plugins = tuple(plugins)\n\n        for filename in data['results']:\n            result.data[filename] = {}\n\n            for item in data['results'][filename]:\n                secret = PotentialSecret(\n                    item['type'],\n                    filename,\n                    secret='will be replaced',\n                    lineno=item['line_number'],\n                    is_secret=item.get('is_secret'),\n                )\n                secret.secret_hash = item['hashed_secret']\n                result.data[filename][secret] = secret\n\n        result.version = (\n            data['version']\n            if 'version' in data\n            else '0.0.0'\n        )\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscans the codebase for incremental differences.", "response": "def scan_diff(\n        self,\n        diff,\n        baseline_filename='',\n        last_commit_hash='',\n        repo_name='',\n    ):\n        \"\"\"For optimization purposes, our scanning strategy focuses on looking\n        at incremental differences, rather than re-scanning the codebase every time.\n        This function supports this, and adds information to self.data.\n\n        :type diff: str\n        :param diff: diff string.\n                     e.g. The output of `git diff <fileA> <fileB>`\n\n        :type baseline_filename: str\n        :param baseline_filename: if there are any baseline secrets, then the baseline\n                                  file will have hashes in them. By specifying it, we\n                                  can skip this clear exception.\n\n        :type last_commit_hash: str\n        :param last_commit_hash: used for logging only -- the last commit hash we saved\n\n        :type repo_name: str\n        :param repo_name: used for logging only -- the name of the repo\n        \"\"\"\n        # Local imports, so that we don't need to require unidiff for versions of\n        # detect-secrets that don't use it.\n        from unidiff import PatchSet\n        from unidiff.errors import UnidiffParseError\n\n        try:\n            patch_set = PatchSet.from_string(diff)\n        except UnidiffParseError:  # pragma: no cover\n            alert = {\n                'alert': 'UnidiffParseError',\n                'hash': last_commit_hash,\n                'repo_name': repo_name,\n            }\n            log.error(alert)\n            raise\n\n        if self.exclude_files:\n            regex = re.compile(self.exclude_files, re.IGNORECASE)\n\n        for patch_file in patch_set:\n            filename = patch_file.path\n            # If the file matches the exclude_files, we skip it\n            if self.exclude_files and regex.search(filename):\n                continue\n\n            if filename == baseline_filename:\n                continue\n\n            for results, plugin in self._results_accumulator(filename):\n                results.update(\n                    self._extract_secrets_from_patch(\n                        patch_file,\n                        plugin,\n                        filename,\n                    ),\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scan_file(self, filename, filename_key=None):\n\n        if not filename_key:\n            filename_key = filename\n\n        if os.path.islink(filename):\n            return False\n\n        try:\n            with codecs.open(filename, encoding='utf-8') as f:\n                self._extract_secrets_from_file(f, filename_key)\n\n            return True\n        except IOError:\n            log.warning(\"Unable to open file: %s\", filename)\n            return False", "response": "Scans a specified file and adds information to the internal data structure."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_secret(self, filename, secret, type_=None):\n        if filename not in self.data:\n            return None\n\n        if type_:\n            # Optimized lookup, because we know the type of secret\n            # (and therefore, its hash)\n            tmp_secret = PotentialSecret(type_, filename, secret='will be overriden')\n            tmp_secret.secret_hash = secret\n\n            if tmp_secret in self.data[filename]:\n                return self.data[filename][tmp_secret]\n\n            return None\n\n        # NOTE: We can only optimize this, if we knew the type of secret.\n        # Otherwise, we need to iterate through the set and find out.\n        for obj in self.data[filename]:\n            if obj.secret_hash == secret:\n                return obj\n\n        return None", "response": "Checks to see if a secret is found in the collection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nformatting the baseline output for the log file.", "response": "def format_for_baseline_output(self):\n        \"\"\"\n        :rtype: dict\n        \"\"\"\n        results = self.json()\n        for key in results:\n            results[key] = sorted(results[key], key=lambda x: x['line_number'])\n\n        plugins_used = list(map(\n            lambda x: x.__dict__,\n            self.plugins,\n        ))\n        plugins_used = sorted(plugins_used, key=lambda x: x['name'])\n\n        return {\n            'generated_at': strftime(\"%Y-%m-%dT%H:%M:%SZ\", gmtime()),\n            'exclude': {\n                'files': self.exclude_files,\n                'lines': self.exclude_lines,\n            },\n            'plugins_used': plugins_used,\n            'results': results,\n            'version': self.version,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield the results of the given file and adds it to the dictionary.", "response": "def _results_accumulator(self, filename):\n        \"\"\"\n        :type filename: str\n        :param filename: name of file, used as a key to store in self.data\n\n        :yields: (dict, detect_secrets.plugins.base.BasePlugin)\n                 Caller is responsible for updating the dictionary with\n                 results of plugin analysis.\n        \"\"\"\n        file_results = {}\n\n        for plugin in self.plugins:\n            yield file_results, plugin\n\n        if not file_results:\n            return\n\n        if filename not in self.data:\n            self.data[filename] = file_results\n        else:\n            self.data[filename].update(file_results)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nextracting secrets from a given file object.", "response": "def _extract_secrets_from_file(self, f, filename):\n        \"\"\"Extract secrets from a given file object.\n\n        :type f:        File object\n        :type filename: string\n        \"\"\"\n        try:\n            log.info(\"Checking file: %s\", filename)\n\n            for results, plugin in self._results_accumulator(filename):\n                results.update(plugin.analyze(f, filename))\n                f.seek(0)\n\n        except UnicodeDecodeError:\n            log.warning(\"%s failed to load.\", filename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract secrets from a given patch file object.", "response": "def _extract_secrets_from_patch(self, f, plugin, filename):\n        \"\"\"Extract secrets from a given patch file object.\n\n        Note that we only want to capture incoming secrets (so added lines).\n\n        :type f: unidiff.patch.PatchedFile\n        :type plugin: detect_secrets.plugins.base.BasePlugin\n        :type filename: str\n        \"\"\"\n        output = {}\n        for chunk in f:\n            # target_lines refers to incoming (new) changes\n            for line in chunk.target_lines():\n                if line.is_added:\n                    output.update(\n                        plugin.analyze_string(\n                            line.value,\n                            line.target_line_no,\n                            filename,\n                        ),\n                    )\n\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a mapping node with the keys and values of the object.", "response": "def _tag_dict_values(self, map_node):\n        \"\"\"\n        :type map_node: yaml.nodes.MappingNode\n        :param map_node: It looks like map_node.value contains a list of\n            pair tuples, corresponding to key,value pairs.\n        \"\"\"\n        new_values = []\n        for key, value in map_node.value:\n            if not value.tag.endswith(':str'):\n                new_values.append((key, value,))\n                continue\n\n            augmented_string = yaml.nodes.MappingNode(\n                tag=map_node.tag,\n                value=[\n                    self._create_key_value_pair_for_mapping_node_value(\n                        '__value__',\n                        value.value,\n                        'tag:yaml.org,2002:str',\n                    ),\n                    self._create_key_value_pair_for_mapping_node_value(\n                        '__line__',\n                        str(value.__line__),\n                        'tag:yaml.org,2002:int',\n                    ),\n                ],\n            )\n\n            new_values.append((key, augmented_string,))\n\n        output = yaml.nodes.MappingNode(\n            tag=map_node.tag,\n            value=new_values,\n            start_mark=map_node.start_mark,\n            end_mark=map_node.end_mark,\n            flow_style=map_node.flow_style,\n        )\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_ignored_lines(self):\n        ignored_lines = set()\n\n        for line_number, line in enumerate(self.content.split('\\n'), 1):\n            if (\n                WHITELIST_REGEX['yaml'].search(line)\n\n                or (\n                    self.exclude_lines_regex and\n                    self.exclude_lines_regex.search(line)\n                )\n            ):\n                ignored_lines.add(line_number)\n\n        return ignored_lines", "response": "Return a set of integers that refer to line numbers that were not whitelisted by the user and should be ignored."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_value_and_line_offset(self, key, values):\n        values_list = self._construct_values_list(values)\n        if not values_list:\n            return []\n\n        current_value_list_index = 0\n        output = []\n        lines_modified = False\n\n        for index, line in enumerate(self.lines):\n            # Check ignored lines before checking values, because\n            # you can write comments *after* the value.\n            if not line.strip() or self._comment_regex.match(line):\n                continue\n\n            if (\n                self.exclude_lines_regex and\n                self.exclude_lines_regex.search(line)\n            ):\n                continue\n\n            if current_value_list_index == 0:\n                first_line_regex = re.compile(r'^\\s*{}[ :=]+{}'.format(\n                    re.escape(key),\n                    re.escape(values_list[current_value_list_index]),\n                ))\n                if first_line_regex.match(line):\n                    output.append((\n                        values_list[current_value_list_index],\n                        self.line_offset + index + 1,\n                    ))\n                    current_value_list_index += 1\n                continue\n\n            if current_value_list_index == len(values_list):\n                if index == 0:\n                    index = 1  # don't want to count the same line again\n                self.line_offset += index\n                self.lines = self.lines[index:]\n                lines_modified = True\n                break\n            else:\n                output.append((\n                    values_list[current_value_list_index],\n                    self.line_offset + index + 1,\n                ))\n\n                current_value_list_index += 1\n\n        if not lines_modified:\n            # No more lines left, if loop was not explicitly left.\n            self.lines = []\n\n        return output", "response": "Returns the index of the location of the value pair in lines and the line offset of the value pair in lines."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconstruct a list of values from a string.", "response": "def _construct_values_list(values):\n        \"\"\"\n        This values_list is a strange construction, because of ini format.\n        We need to extract the values with the following supported format:\n\n            >>> key = value0\n            ...     value1\n            ...\n            ...     # comment line here\n            ...     value2\n\n        given that normally, either value0 is supplied, or (value1, value2),\n        but still allowing for all three at once.\n\n        Furthermore, with the configparser, we will get a list of values,\n        and intermediate blank lines, but no comments. This means that we can't\n        merely use the count of values' items to heuristically \"skip ahead\" lines,\n        because we still have to manually parse through this.\n\n        Therefore, we construct the values_list in the following fashion:\n            1. Keep the first value (in the example, this is `value0`)\n            2. For all other values, ignore blank lines.\n        Then, we can parse through, and look for values only.\n        \"\"\"\n        lines = values.splitlines()\n        values_list = lines[:1]\n        values_list.extend(filter(None, lines[1:]))\n        return values_list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the baseline string from the file.", "response": "def _get_baseline_string_from_file(filename):  # pragma: no cover\n    \"\"\"Breaking this function up for mockability.\"\"\"\n    try:\n        with open(filename) as f:\n            return f.read()\n\n    except IOError:\n        log.error(\n            'Unable to open baseline file: {}\\n'\n            'Please create it via\\n'\n            '   `detect-secrets scan > {}`\\n'\n            .format(filename, filename),\n        )\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef raise_exception_if_baseline_file_is_unstaged(filename):\n    try:\n        files_changed_but_not_staged = subprocess.check_output(\n            [\n                'git',\n                'diff',\n                '--name-only',\n            ],\n        ).split()\n    except subprocess.CalledProcessError:\n        # Since we don't pipe stderr, we get free logging through git.\n        raise ValueError\n\n    if filename.encode() in files_changed_but_not_staged:\n        log.error((\n            'Your baseline file ({}) is unstaged.\\n'\n            '`git add {}` to fix this.'\n        ).format(\n            filename,\n            filename,\n        ))\n\n        raise ValueError", "response": "Raises an exception if the baseline file is unstaged."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef compare_baselines(old_baseline_filename, new_baseline_filename):\n    if old_baseline_filename == new_baseline_filename:\n        raise RedundantComparisonError\n\n    old_baseline = _get_baseline_from_file(old_baseline_filename)\n    new_baseline = _get_baseline_from_file(new_baseline_filename)\n\n    _remove_nonexistent_files_from_baseline(old_baseline)\n    _remove_nonexistent_files_from_baseline(new_baseline)\n\n    # We aggregate the secrets first, so that we can display a total count.\n    secrets_to_compare = _get_secrets_to_compare(old_baseline, new_baseline)\n    total_reviews = len(secrets_to_compare)\n    current_index = 0\n\n    secret_iterator = BidirectionalIterator(secrets_to_compare)\n    for filename, secret, is_removed in secret_iterator:\n        _clear_screen()\n        current_index += 1\n\n        header = '{}      {}'\n        if is_removed:\n            plugins_used = old_baseline['plugins_used']\n            header = header.format(\n                colorize('Status:', AnsiColor.BOLD),\n                '>> {} <<'.format(\n                    colorize('REMOVED', AnsiColor.RED),\n                ),\n            )\n        else:\n            plugins_used = new_baseline['plugins_used']\n            header = header.format(\n                colorize('Status:', AnsiColor.BOLD),\n                '>> {} <<'.format(\n                    colorize('ADDED', AnsiColor.LIGHT_GREEN),\n                ),\n            )\n\n        try:\n            _print_context(\n                filename,\n                secret,\n                current_index,\n                total_reviews,\n                plugins_used,\n                additional_header_lines=header,\n                force=is_removed,\n            )\n            decision = _get_user_decision(\n                can_step_back=secret_iterator.can_step_back(),\n                prompt_secret_decision=False,\n            )\n        except SecretNotFoundOnSpecifiedLineError:\n            decision = _get_user_decision(prompt_secret_decision=False)\n\n        if decision == 'q':\n            print('Quitting...')\n            break\n\n        if decision == 'b':  # pragma: no cover\n            current_index -= 2\n            secret_iterator.step_back_on_next_iteration()", "response": "This function compares two baselines and returns a new baselines that are compatible with the old baselines."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _secret_generator(baseline):\n    for filename, secrets in baseline['results'].items():\n        for secret in secrets:\n            yield filename, secret", "response": "Generates secrets to audit from the baseline"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_secrets_to_compare(old_baseline, new_baseline):\n    def _check_string(a, b):\n        if a == b:\n            return 0\n        if a < b:\n            return -1\n        return 1\n\n    def _check_secret(a, b):\n        if a == b:\n            return 0\n\n        if a['line_number'] < b['line_number']:\n            return -1\n        elif a['line_number'] > b['line_number']:\n            return 1\n\n        return _check_string(a['hashed_secret'], b['hashed_secret'])\n\n    secrets_to_compare = []\n    for old_filename, new_filename in _comparison_generator(\n        sorted(old_baseline['results'].keys()),\n        sorted(new_baseline['results'].keys()),\n        compare_fn=_check_string,\n    ):\n        if not new_filename:\n            secrets_to_compare += list(\n                map(\n                    lambda x: (old_filename, x, True,),\n                    old_baseline['results'][old_filename],\n                ),\n            )\n            continue\n        elif not old_filename:\n            secrets_to_compare += list(\n                map(\n                    lambda x: (new_filename, x, False,),\n                    new_baseline['results'][new_filename],\n                ),\n            )\n            continue\n\n        for old_secret, new_secret in _comparison_generator(\n            old_baseline['results'][old_filename],\n            new_baseline['results'][new_filename],\n            compare_fn=_check_secret,\n        ):\n            if old_secret == new_secret:\n                # If they are the same, no point flagging it.\n                continue\n\n            if old_secret:\n                secrets_to_compare.append(\n                    (old_filename, old_secret, True,),\n                )\n            else:\n                secrets_to_compare.append(\n                    (new_filename, new_secret, False,),\n                )\n\n    return secrets_to_compare", "response": "returns a list of dicts that are used to compare two dicts"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _print_context(  # pragma: no cover\n    filename,\n    secret,\n    count,\n    total,\n    plugin_settings,\n    additional_header_lines=None,\n    force=False,\n):\n    \"\"\"\n    :type filename: str\n    :param filename: the file currently scanned.\n\n    :type secret: dict, in PotentialSecret.json() format\n    :param secret: the secret, represented in the baseline file.\n\n    :type count: int\n    :param count: current count of secrets scanned so far\n\n    :type total: int\n    :param total: total number of secrets in baseline\n\n    :type plugin_settings: list\n    :param plugin_settings: plugins used to create baseline.\n\n    :type additional_header_lines: str\n    :param additional_header_lines: any additional lines to add to the\n        header of the interactive audit display.\n\n    :type force: bool\n    :param force: if True, will print the lines of code even if it doesn't\n        find the secret expected\n\n    :raises: SecretNotFoundOnSpecifiedLineError\n    \"\"\"\n    print('{} {} {} {}\\n{} {}\\n{} {}'.format(\n        colorize('Secret:     ', AnsiColor.BOLD),\n        colorize(str(count), AnsiColor.PURPLE),\n        colorize('of', AnsiColor.BOLD),\n        colorize(str(total), AnsiColor.PURPLE),\n        colorize('Filename:   ', AnsiColor.BOLD),\n        colorize(filename, AnsiColor.PURPLE),\n        colorize('Secret Type:', AnsiColor.BOLD),\n        colorize(secret['type'], AnsiColor.PURPLE),\n    ))\n    if additional_header_lines:\n        print(additional_header_lines)\n\n    print('-' * 10)\n\n    error_obj = None\n    try:\n        secret_with_context = _get_secret_with_context(\n            filename,\n            secret,\n            plugin_settings,\n            force=force,\n        )\n        print(secret_with_context)\n    except SecretNotFoundOnSpecifiedLineError as e:\n        error_obj = e\n        print(e)\n\n    print('-' * 10)\n\n    if error_obj:\n        raise error_obj", "response": "Print the context of the current secret."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_user_decision(prompt_secret_decision=True, can_step_back=False):\n    allowable_user_input = ['s', 'q']\n    if prompt_secret_decision:\n        allowable_user_input.extend(['y', 'n'])\n    if can_step_back:\n        allowable_user_input.append('b')\n\n    user_input = None\n    while user_input not in allowable_user_input:\n        if user_input:\n            print('Invalid input.')\n\n        if 'y' in allowable_user_input:\n            user_input_string = 'Is this a valid secret? i.e. not a false-positive (y)es, (n)o, '\n        else:\n            user_input_string = 'What would you like to do? '\n        if 'b' in allowable_user_input:\n            user_input_string += '(b)ack, '\n        user_input_string += '(s)kip, (q)uit: '\n\n        user_input = input(user_input_string)\n        if user_input:\n            user_input = user_input[0].lower()\n\n    return user_input", "response": "Get user decision from the user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_secret_with_context(\n    filename,\n    secret,\n    plugin_settings,\n    lines_of_context=5,\n    force=False,\n):\n    \"\"\"\n    Displays the secret, with surrounding lines of code for better context.\n\n    :type filename: str\n    :param filename: filename where secret resides in\n\n    :type secret: dict, PotentialSecret.json() format\n    :param secret: the secret listed in baseline\n\n    :type plugin_settings: list\n    :param plugin_settings: plugins used to create baseline.\n\n    :type lines_of_context: int\n    :param lines_of_context: number of lines displayed before and after\n        secret.\n\n    :type force: bool\n    :param force: if True, will print the lines of code even if it doesn't\n        find the secret expected\n\n    :raises: SecretNotFoundOnSpecifiedLineError\n    \"\"\"\n    snippet = CodeSnippetHighlighter().get_code_snippet(\n        filename,\n        secret['line_number'],\n        lines_of_context=lines_of_context,\n    )\n\n    try:\n        raw_secret_value = get_raw_secret_value(\n            snippet.target_line,\n            secret,\n            plugin_settings,\n            filename,\n        )\n\n        snippet.highlight_line(raw_secret_value)\n    except SecretNotFoundOnSpecifiedLineError:\n        if not force:\n            raise\n\n        snippet.target_line = colorize(\n            snippet.target_line,\n            AnsiColor.BOLD,\n        )\n\n    return snippet.add_line_numbers()", "response": "Returns a new secret with the given lines of code for better context."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_raw_secret_value(\n    secret_line,\n    secret,\n    plugin_settings,\n    filename,\n):\n    \"\"\"\n    :type secret_line: str\n    :param secret_line: the line on which the secret is found\n\n    :type secret: dict\n    :param secret: see caller's docstring\n\n    :type plugin_settings: list\n    :param plugin_settings: see caller's docstring\n\n    :type filename: str\n    :param filename: this is needed, because PotentialSecret uses this\n        as a means of comparing whether two secrets are equal.\n    \"\"\"\n    plugin = initialize.from_secret_type(\n        secret['type'],\n        plugin_settings,\n    )\n\n    for raw_secret in raw_secret_generator(\n        plugin,\n        secret_line,\n        filetype=determine_file_type(filename),\n    ):\n        secret_obj = PotentialSecret(\n            plugin.secret_type,\n            filename,\n            secret=raw_secret,\n        )\n\n        # There could be more than two secrets on the same line.\n        # We only want to highlight the right one.\n        if secret_obj.secret_hash == secret['hashed_secret']:\n            return raw_secret\n    else:\n        raise SecretNotFoundOnSpecifiedLineError(secret['line_number'])", "response": "returns the raw secret value for a given secret line"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef raw_secret_generator(plugin, secret_line, filetype):\n    for raw_secret in plugin.secret_generator(secret_line, filetype=filetype):\n        yield raw_secret\n\n    if issubclass(plugin.__class__, HighEntropyStringsPlugin):\n        with plugin.non_quoted_string_regex(strict=False):\n            for raw_secret in plugin.secret_generator(secret_line):\n                yield raw_secret", "response": "Generates raw secrets by re - scanning the line with the specified plugin\n\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_code_snippet(self, filename, line_number, lines_of_context=5):\n        secret_line_index = line_number - 1\n        end_line = secret_line_index + lines_of_context + 1\n\n        if secret_line_index <= lines_of_context:\n            start_line = 0\n            index_of_secret_in_output = secret_line_index\n        else:\n            start_line = secret_line_index - lines_of_context\n            index_of_secret_in_output = lines_of_context\n\n        return CodeSnippet(\n            list(\n                itertools.islice(\n                    self._get_lines_in_file(filename),\n                    start_line,\n                    end_line,\n                ),\n            ),\n            start_line,\n            index_of_secret_in_output,\n        )", "response": "This method returns a CodeSnippet object for the given filename line number and number of lines of context."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_lines_in_file(self, filename):\n        with codecs.open(filename, encoding='utf-8') as file:\n            return file.read().splitlines()", "response": "Returns a list of lines in a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhighlight the line of the target text based on the given payload.", "response": "def highlight_line(self, payload):\n        \"\"\"\n        :type payload: str\n        :param payload: string to highlight, on chosen line\n        \"\"\"\n        index_of_payload = self.target_line.lower().index(payload.lower())\n        end_of_payload = index_of_payload + len(payload)\n\n        self.target_line = u'{}{}{}'.format(\n            self.target_line[:index_of_payload],\n            self.apply_highlight(self.target_line[index_of_payload:end_of_payload]),\n            self.target_line[end_of_payload:],\n        )\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef analyze(self, file, filename):\n        potential_secrets = {}\n        for line_num, line in enumerate(file.readlines(), start=1):\n            secrets = self.analyze_string(line, line_num, filename)\n            potential_secrets.update(secrets)\n\n        return potential_secrets", "response": "Analyze the file and return a dictionary of potential secret key sets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nanalyzing a string and return a dictionary of the PotentialSecret data.", "response": "def analyze_string(self, string, line_num, filename):\n        \"\"\"\n        :param string:    string; the line to analyze\n        :param line_num:  integer; line number that is currently being analyzed\n        :param filename:  string; name of file being analyzed\n        :returns:         dictionary\n\n        NOTE: line_num and filename are used for PotentialSecret creation only.\n        \"\"\"\n        if (\n            any(\n                whitelist_regex.search(string) for whitelist_regex in WHITELIST_REGEXES\n            )\n\n            or (\n                self.exclude_lines_regex and\n                self.exclude_lines_regex.search(string)\n            )\n        ):\n            return {}\n\n        return self.analyze_string_content(\n            string,\n            line_num,\n            filename,\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_logger(name=None, format_string=None):\n    logging.captureWarnings(True)\n    log = logging.getLogger(name)\n\n    # Bind custom method to instance.\n    # Source: https://stackoverflow.com/a/2982\n    log.set_debug_level = _set_debug_level.__get__(log)\n    log.set_debug_level(0)\n\n    if not format_string:\n        format_string = '[%(module)s]\\t%(levelname)s\\t%(message)s'\n\n    # Setting up log formats\n    log.handlers = []\n    handler = logging.StreamHandler(sys.stderr)\n    handler.setFormatter(\n        logging.Formatter(format_string),\n    )\n    log.addHandler(handler)\n\n    return log", "response": "Returns a logger instance."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_debug_level(self, debug_level):\n    mapping = {\n        0: logging.ERROR,\n        1: logging.INFO,\n        2: logging.DEBUG,\n    }\n\n    self.setLevel(\n        mapping[min(debug_level, 2)],\n    )", "response": "Configure the debug level of the log\n   ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef consolidate_args(args):\n        # Using `--hex-limit` as a canary to identify whether this\n        # consolidation is appropriate.\n        if not hasattr(args, 'hex_limit'):\n            return\n\n        active_plugins = {}\n        is_using_default_value = {}\n\n        for plugin in PluginOptions.all_plugins:\n            arg_name = PluginOptions._convert_flag_text_to_argument_name(\n                plugin.disable_flag_text,\n            )\n\n            # Remove disabled plugins\n            is_disabled = getattr(args, arg_name, False)\n            delattr(args, arg_name)\n            if is_disabled:\n                continue\n\n            # Consolidate related args\n            related_args = {}\n            for related_arg_tuple in plugin.related_args:\n                try:\n                    flag_name, default_value = related_arg_tuple\n                except ValueError:\n                    flag_name = related_arg_tuple\n                    default_value = None\n\n                arg_name = PluginOptions._convert_flag_text_to_argument_name(\n                    flag_name,\n                )\n\n                related_args[arg_name] = getattr(args, arg_name)\n                delattr(args, arg_name)\n\n                if default_value and related_args[arg_name] is None:\n                    related_args[arg_name] = default_value\n                    is_using_default_value[arg_name] = True\n\n            active_plugins.update({\n                plugin.classname: related_args,\n            })\n\n        args.plugins = active_plugins\n        args.is_using_default_value = is_using_default_value", "response": "This function consolidates all of the arguments related to configuring plugins and saves the consolidated args in args. plugins."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _argparse_minmax_type(self, string):\n        value = float(string)\n        if value < 0 or value > 8:\n            raise argparse.ArgumentTypeError(\n                '%s must be between 0.0 and 8.0' % string,\n            )\n\n        return value", "response": "Custom type for argparse to enforce value limits"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calculate_shannon_entropy(self, data):\n        if not data:  # pragma: no cover\n            return 0\n\n        entropy = 0\n        for x in self.charset:\n            p_x = float(data.count(x)) / len(data)\n            if p_x > 0:\n                entropy += - p_x * math.log(p_x, 2)\n\n        return entropy", "response": "Calculates the Shannon entropy of a given string."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef analyze_string_content(self, string, line_num, filename):\n        output = {}\n\n        for result in self.secret_generator(string):\n            if self._is_sequential_string(result):\n                continue\n            secret = PotentialSecret(self.secret_type, filename, result, line_num)\n            output[secret] = secret\n\n        return output", "response": "Searches string for custom pattern and captures all high entropy strings that match self. regex and captures all high entropy strings that match self. entropy_limit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef non_quoted_string_regex(self, strict=True):\n        old_regex = self.regex\n\n        regex_alternative = r'([{}]+)'.format(re.escape(self.charset))\n        if strict:\n            regex_alternative = r'^' + regex_alternative + r'$'\n\n        self.regex = re.compile(regex_alternative)\n\n        try:\n            yield\n        finally:\n            self.regex = old_regex", "response": "A context manager that creates a regex that matches a non - quoted string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _analyze_ini_file(self, add_header=False):\n        def wrapped(file, filename):\n            potential_secrets = {}\n\n            with self.non_quoted_string_regex():\n                for value, lineno in IniFileParser(\n                    file,\n                    add_header,\n                    exclude_lines_regex=self.exclude_lines_regex,\n                ).iterator():\n                    potential_secrets.update(self.analyze_string(\n                        value,\n                        lineno,\n                        filename,\n                    ))\n\n            return potential_secrets\n\n        return wrapped", "response": "A function that returns a dictionary of the information from an INI file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _analyze_yaml_file(self, file, filename):\n        if os.path.splitext(filename)[1] not in YAML_EXTENSIONS:\n            # The yaml parser is pretty powerful. It eagerly\n            # parses things when it's not even a yaml file. Therefore,\n            # we use this heuristic to quit early if appropriate.\n            raise yaml.YAMLError\n\n        parser = YamlFileParser(\n            file,\n            exclude_lines_regex=self.exclude_lines_regex,\n        )\n        data = parser.json()\n        ignored_lines = parser.get_ignored_lines()\n        potential_secrets = {}\n\n        to_search = [data]\n        with self.non_quoted_string_regex():\n            while len(to_search) > 0:\n                item = to_search.pop()\n\n                try:\n                    if '__line__' in item and not item['__line__'] in ignored_lines:\n                        potential_secrets.update(\n                            self.analyze_string(\n                                item['__value__'],\n                                item['__line__'],\n                                filename,\n                            ),\n                        )\n\n                    if '__line__' in item:\n                        continue\n\n                    for key in item:\n                        obj = item[key] if isinstance(item, dict) else key\n                        if isinstance(obj, dict):\n                            to_search.append(obj)\n                except TypeError:\n                    pass\n\n        return potential_secrets", "response": "Analyze a yaml file and return a dictionary of the keys that are not in the ignore_lines_regex list."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the shannon entropy for a string of length 1.", "response": "def calculate_shannon_entropy(self, data):\n        \"\"\"\n        In our investigations, we have found that when the input is all digits,\n        the number of false positives we get greatly exceeds realistic true\n        positive scenarios.\n\n        Therefore, this tries to capture this heuristic mathemetically.\n\n        We do this by noting that the maximum shannon entropy for this charset\n        is ~3.32 (e.g. \"0123456789\", with every digit different), and we want\n        to lower that below the standard limit, 3. However, at the same time,\n        we also want to accommodate the fact that longer strings have a higher\n        chance of being a true positive, which means \"01234567890123456789\"\n        should be closer to the maximum entropy than the shorter version.\n        \"\"\"\n        entropy = super(HexHighEntropyString, self).calculate_shannon_entropy(data)\n        if len(data) == 1:\n            return entropy\n\n        try:\n            int(data)\n\n            # This multiplier was determined through trial and error, with the\n            # intent of keeping it simple, yet achieving our goals.\n            entropy -= 1.2 / math.log(len(data), 2)\n        except ValueError:\n            pass\n\n        return entropy"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninitialize the SecretsCollection object with the given plugins and exclude_files_regex exclude_lines_regex and path.", "response": "def initialize(\n    plugins,\n    exclude_files_regex=None,\n    exclude_lines_regex=None,\n    path='.',\n    scan_all_files=False,\n):\n    \"\"\"Scans the entire codebase for secrets, and returns a\n    SecretsCollection object.\n\n    :type plugins: tuple of detect_secrets.plugins.base.BasePlugin\n    :param plugins: rules to initialize the SecretsCollection with.\n\n    :type exclude_files_regex: str|None\n    :type exclude_lines_regex: str|None\n    :type path: str\n    :type scan_all_files: bool\n\n    :rtype: SecretsCollection\n    \"\"\"\n    output = SecretsCollection(\n        plugins,\n        exclude_files=exclude_files_regex,\n        exclude_lines=exclude_lines_regex,\n    )\n\n    if os.path.isfile(path):\n        # This option allows for much easier adhoc usage.\n        files_to_scan = [path]\n    elif scan_all_files:\n        files_to_scan = _get_files_recursively(path)\n    else:\n        files_to_scan = _get_git_tracked_files(path)\n\n    if not files_to_scan:\n        return output\n\n    if exclude_files_regex:\n        exclude_files_regex = re.compile(exclude_files_regex, re.IGNORECASE)\n        files_to_scan = filter(\n            lambda file: (\n                not exclude_files_regex.search(file)\n            ),\n            files_to_scan,\n        )\n\n    for file in files_to_scan:\n        output.scan_file(file)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_secrets_not_in_baseline(results, baseline):\n    exclude_files_regex = None\n    if baseline.exclude_files:\n        exclude_files_regex = re.compile(baseline.exclude_files, re.IGNORECASE)\n\n    new_secrets = SecretsCollection()\n    for filename in results.data:\n        if exclude_files_regex and exclude_files_regex.search(filename):\n            continue\n\n        if filename not in baseline.data:\n            # We don't have a previous record of this file, so obviously\n            # everything is new.\n            new_secrets.data[filename] = results.data[filename]\n            continue\n\n        # The __hash__ method of PotentialSecret makes this work\n        filtered_results = {\n            secret: secret\n            for secret in results.data[filename]\n            if secret not in baseline.data[filename]\n        }\n\n        if filtered_results:\n            new_secrets.data[filename] = filtered_results\n\n    return new_secrets", "response": "Returns a SecretsCollection containing only the secrets that are not in the baseline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nnote filelist is not a comprehensive list of all files in the repo (because we can't be sure whether --all-files is passed in as a parameter to pre-commit). :type results: SecretsCollection :type baseline: SecretsCollection :type filelist: list(str) :param filelist: filenames that are scanned. :rtype: bool :returns: True if baseline was updated", "response": "def trim_baseline_of_removed_secrets(results, baseline, filelist):\n    \"\"\"\n    NOTE: filelist is not a comprehensive list of all files in the repo\n    (because we can't be sure whether --all-files is passed in as a\n    parameter to pre-commit).\n\n    :type results: SecretsCollection\n    :type baseline: SecretsCollection\n\n    :type filelist: list(str)\n    :param filelist: filenames that are scanned.\n\n    :rtype: bool\n    :returns: True if baseline was updated\n    \"\"\"\n    updated = False\n    for filename in filelist:\n        if filename not in baseline.data:\n            # Nothing to modify, because not even there in the first place.\n            continue\n\n        if filename not in results.data:\n            # All secrets relating to that file was removed.\n            # We know this because:\n            #   1. It's a file that was scanned (in filelist)\n            #   2. It was in the baseline\n            #   3. It has no results now.\n            del baseline.data[filename]\n            updated = True\n            continue\n\n        # We clone the baseline, so that we can modify the baseline,\n        # without messing up the iteration.\n        for baseline_secret in baseline.data[filename].copy():\n            new_secret_found = results.get_secret(\n                filename,\n                baseline_secret.secret_hash,\n                baseline_secret.type,\n            )\n\n            if not new_secret_found:\n                # No longer in results, so can remove from baseline\n                old_secret_to_delete = baseline.get_secret(\n                    filename,\n                    baseline_secret.secret_hash,\n                    baseline_secret.type,\n                )\n                del baseline.data[filename][old_secret_to_delete]\n                updated = True\n\n            elif new_secret_found.lineno != baseline_secret.lineno:\n                # Secret moved around, should update baseline with new location\n                old_secret_to_update = baseline.get_secret(\n                    filename,\n                    baseline_secret.secret_hash,\n                    baseline_secret.type,\n                )\n                old_secret_to_update.lineno = new_secret_found.lineno\n                updated = True\n\n    return updated"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_results(old_results, new_results):\n    for filename, old_secrets in old_results.items():\n        if filename not in new_results:\n            continue\n\n        old_secrets_mapping = dict()\n        for old_secret in old_secrets:\n            old_secrets_mapping[old_secret['hashed_secret']] = old_secret\n\n        for new_secret in new_results[filename]:\n            if new_secret['hashed_secret'] not in old_secrets_mapping:\n                # We don't join the two secret sets, because if the newer\n                # result set did not discover an old secret, it probably\n                # moved.\n                continue\n\n            old_secret = old_secrets_mapping[new_secret['hashed_secret']]\n            # Only propagate 'is_secret' if it's not already there\n            if 'is_secret' in old_secret and 'is_secret' not in new_secret:\n                new_secret['is_secret'] = old_secret['is_secret']\n\n    return new_results", "response": "Update results in baseline with latest information."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef format_baseline_for_output(baseline):\n    for filename, secret_list in baseline['results'].items():\n        baseline['results'][filename] = sorted(\n            secret_list,\n            key=lambda x: (x['line_number'], x['hashed_secret'],),\n        )\n\n    return json.dumps(\n        baseline,\n        indent=2,\n        sort_keys=True,\n        separators=(',', ': '),\n    )", "response": "Formats the baseline for output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_git_tracked_files(rootdir='.'):\n    try:\n        with open(os.devnull, 'w') as fnull:\n            git_files = subprocess.check_output(\n                [\n                    'git',\n                    'ls-files',\n                    rootdir,\n                ],\n                stderr=fnull,\n            )\n\n        return set(git_files.decode('utf-8').split())\n    except subprocess.CalledProcessError:\n        return None", "response": "This function returns a set of filepaths to files which git currently tracks."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _perform_scan(args, plugins):\n    old_baseline = _get_existing_baseline(args.import_filename)\n    if old_baseline:\n        plugins = initialize.merge_plugin_from_baseline(\n            _get_plugin_from_baseline(old_baseline), args,\n        )\n\n    # Favors `--exclude-files` and `--exclude-lines` CLI arguments\n    # over existing baseline's regexes (if given)\n    if old_baseline:\n        if not args.exclude_files:\n            args.exclude_files = _get_exclude_files(old_baseline)\n\n        if (\n            not args.exclude_lines\n            and old_baseline.get('exclude')\n        ):\n            args.exclude_lines = old_baseline['exclude']['lines']\n\n    # If we have knowledge of an existing baseline file, we should use\n    # that knowledge and add it to our exclude_files regex.\n    if args.import_filename:\n        _add_baseline_to_exclude_files(args)\n\n    new_baseline = baseline.initialize(\n        plugins=plugins,\n        exclude_files_regex=args.exclude_files,\n        exclude_lines_regex=args.exclude_lines,\n        path=args.path,\n        scan_all_files=args.all_files,\n    ).format_for_baseline_output()\n\n    if old_baseline:\n        new_baseline = baseline.merge_baseline(\n            old_baseline,\n            new_baseline,\n        )\n\n    return new_baseline", "response": "Perform a scan of the current node s baseline file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds baseline to args. exclude_files in - place.", "response": "def _add_baseline_to_exclude_files(args):\n    \"\"\"\n    Modifies args.exclude_files in-place.\n    \"\"\"\n    baseline_name_regex = r'^{}$'.format(args.import_filename[0])\n\n    if not args.exclude_files:\n        args.exclude_files = baseline_name_regex\n    elif baseline_name_regex not in args.exclude_files:\n        args.exclude_files += r'|{}'.format(baseline_name_regex)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a tuple of all the plugins that are available in the ParserBuilder.", "response": "def from_parser_builder(plugins_dict, exclude_lines_regex=None):\n    \"\"\"\n    :param plugins_dict: plugins dictionary received from ParserBuilder.\n        See example in tests.core.usage_test.\n\n    :type exclude_lines_regex: str|None\n    :param exclude_lines_regex: optional regex for ignored lines.\n\n    :returns: tuple of initialized plugins\n    \"\"\"\n    output = []\n    for plugin_name in plugins_dict:\n        output.append(from_plugin_classname(\n            plugin_name,\n            exclude_lines_regex=exclude_lines_regex,\n            **plugins_dict[plugin_name]\n        ))\n\n    return tuple(output)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nyielding the parameters that are prioritized by the plugins.", "response": "def _get_prioritized_parameters(plugins_dict, is_using_default_value_map, prefer_default=True):\n    \"\"\"\n    :type plugins_dict: dict(plugin_name => plugin_params)\n    :param plugin_dict: mapping of plugin name to all plugin params\n\n    :type is_using_default_value_map: dict(str => bool)\n    :param is_using_default_value_map: mapping of parameter name to whether its value is derived\n        from a default value.\n\n    :param prefer_default: if True, will yield if plugin parameters are from default values.\n        Otherwise, will yield if plugin parameters are *not* from default values.\n    \"\"\"\n    for plugin_name, plugin_params in plugins_dict.items():\n        for param_name, param_value in plugin_params.items():\n            is_using_default = is_using_default_value_map.get(param_name, False)\n            if is_using_default == prefer_default:\n                yield plugin_name, param_name, param_value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef merge_plugin_from_baseline(baseline_plugins, args):\n    def _remove_key(d, key):\n        r = dict(d)\n        r.pop(key)\n        return r\n\n    baseline_plugins_dict = {\n        vars(plugin)[\"name\"]: _remove_key(vars(plugin), \"name\")\n        for plugin in baseline_plugins\n    }\n\n    # Use input plugin as starting point\n    if args.use_all_plugins:\n        # input param and default param are used\n        plugins_dict = dict(args.plugins)\n\n        # baseline param priority > default\n        for plugin_name, param_name, param_value in _get_prioritized_parameters(\n            baseline_plugins_dict,\n            args.is_using_default_value,\n            prefer_default=True,\n        ):\n            try:\n                plugins_dict[plugin_name][param_name] = param_value\n            except KeyError:\n                log.warning(\n                    'Baseline contain plugin %s which is not in all plugins! Ignoring...'\n                    % (plugin_name),\n                )\n\n        return from_parser_builder(\n            plugins_dict,\n            exclude_lines_regex=args.exclude_lines,\n        )\n\n    # Use baseline plugin as starting point\n    disabled_plugins = PluginOptions.get_disabled_plugins(args)\n    plugins_dict = {\n        plugin_name: plugin_params\n        for plugin_name, plugin_params in baseline_plugins_dict.items()\n        if plugin_name not in disabled_plugins\n    }\n\n    # input param priority > baseline\n    input_plugins_dict = dict(args.plugins)\n    for plugin_name, param_name, param_value in _get_prioritized_parameters(\n        input_plugins_dict,\n        args.is_using_default_value,\n        prefer_default=False,\n    ):\n        try:\n            plugins_dict[plugin_name][param_name] = param_value\n        except KeyError:\n            log.warning(\n                '%s specified, but %s not configured! Ignoring...'\n                % (\"\".join([\"--\", param_name.replace(\"_\", \"-\")]), plugin_name),\n            )\n\n    return from_parser_builder(\n        plugins_dict,\n        exclude_lines_regex=args.exclude_lines,\n    )", "response": "This function takes a list of BasePlugin instances from baseline file and returns a tuple of BasePlugin instances."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_plugin_classname(plugin_classname, exclude_lines_regex=None, **kwargs):\n    klass = globals()[plugin_classname]\n\n    # Make sure the instance is a BasePlugin type, before creating it.\n    if not issubclass(klass, BasePlugin):\n        raise TypeError\n\n    try:\n        instance = klass(exclude_lines_regex=exclude_lines_regex, **kwargs)\n    except TypeError:\n        log.warning(\n            'Unable to initialize plugin!',\n        )\n        raise\n\n    return instance", "response": "Initializes a base plugin class from a classname and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_secret_type(secret_type, settings):\n    mapping = _get_mapping_from_secret_type_to_class_name()\n    try:\n        classname = mapping[secret_type]\n    except KeyError:\n        return None\n\n    for plugin in settings:\n        if plugin['name'] == classname:\n            plugin_init_vars = plugin.copy()\n            plugin_init_vars.pop('name')\n\n            return from_plugin_classname(\n                classname,\n                **plugin_init_vars\n            )", "response": "Returns a list of all plugins used for a given secret type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a mapping from secret_type to plugin classname", "response": "def _get_mapping_from_secret_type_to_class_name():\n    \"\"\"Returns secret_type => plugin classname\"\"\"\n    mapping = {}\n    for key, value in globals().items():\n        try:\n            if issubclass(value, BasePlugin) and value != BasePlugin:\n                mapping[value.secret_type] = key\n        except TypeError:\n            pass\n\n    return mapping"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninversing error function of the inverse error function.", "response": "def erfinv(x, a=.147):\n    \"\"\"Approximation of the inverse error function\n    https://en.wikipedia.org/wiki/Error_function\n    #Approximation_with_elementary_functions\n    \"\"\"\n    lnx = log(1 - x * x)\n    part1 = (2 / (a * pi) + lnx / 2)\n    part2 = lnx / a\n    sgn = 1 if x > 0 else -1\n    return sgn * sqrt(sqrt(part1 * part1 - part2) - part1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef confidence_interval_continuous(\n        point_estimate, stddev, sample_size, confidence=.95, **kwargs\n):\n    \"\"\"Continuous confidence interval from sample size and standard error\"\"\"\n    alpha = ppf((confidence + 1) / 2, sample_size - 1)\n\n    margin = stddev / sqrt(sample_size)\n    return (point_estimate - alpha * margin, point_estimate + alpha * margin)", "response": "Continuous confidence interval from sample size and standard error"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef confidence_interval_dichotomous(\n        point_estimate,\n        sample_size,\n        confidence=.95,\n        bias=False,\n        percentage=True,\n        **kwargs\n):\n    \"\"\"Dichotomous confidence interval from sample size and maybe a bias\"\"\"\n    alpha = ppf((confidence + 1) / 2, sample_size - 1)\n    p = point_estimate\n    if percentage:\n        p /= 100\n\n    margin = sqrt(p * (1 - p) / sample_size)\n    if bias:\n        margin += .5 / sample_size\n    if percentage:\n        margin *= 100\n\n    return (point_estimate - alpha * margin, point_estimate + alpha * margin)", "response": "Dichotomous confidence interval from sample size and maybe a bias"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw a dot line for the given serie.", "response": "def dot(self, serie, r_max):\n        \"\"\"Draw a dot line\"\"\"\n        serie_node = self.svg.serie(serie)\n        view_values = list(map(self.view, serie.points))\n        for i, value in safe_enumerate(serie.values):\n            x, y = view_values[i]\n\n            if self.logarithmic:\n                log10min = log10(self._min) - 1\n                log10max = log10(self._max or 1)\n\n                if value != 0:\n                    size = r_max * ((log10(abs(value)) - log10min) /\n                                    (log10max - log10min))\n                else:\n                    size = 0\n            else:\n                size = r_max * (abs(value) / (self._max or 1))\n\n            metadata = serie.metadata.get(i)\n            dots = decorate(\n                self.svg, self.svg.node(serie_node['plot'], class_=\"dots\"),\n                metadata\n            )\n            alter(\n                self.svg.node(\n                    dots,\n                    'circle',\n                    cx=x,\n                    cy=y,\n                    r=size,\n                    class_='dot reactive tooltip-trigger' +\n                    (' negative' if value < 0 else '')\n                ), metadata\n            )\n\n            val = self._format(serie, i)\n            self._tooltip_data(\n                dots, val, x, y, 'centered', self._get_x_label(i)\n            )\n            self._static_value(serie_node, val, x, y, metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes x max and y scale and set labels", "response": "def _compute(self):\n        \"\"\"Compute y min and max and y scale and set labels\"\"\"\n        x_len = self._len\n        y_len = self._order\n        self._box.xmax = x_len\n        self._box.ymax = y_len\n\n        self._x_pos = [n / 2 for n in range(1, 2 * x_len, 2)]\n        self._y_pos = [n / 2 for n in reversed(range(1, 2 * y_len, 2))]\n\n        for j, serie in enumerate(self.series):\n            serie.points = [(self._x_pos[i], self._y_pos[j])\n                            for i in range(x_len)]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign a view to current graph", "response": "def _set_view(self):\n        \"\"\"Assign a view to current graph\"\"\"\n        view_class = ReverseView if self.inverse_y_axis else View\n\n        self.view = view_class(\n            self.width - self.margin_box.x, self.height - self.margin_box.y,\n            self._box\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _values(self):\n        return [abs(val) for val in super(Dot, self)._values if val != 0]", "response": "Get the values of the series."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _max(self):\n        return (\n            self.range[1] if (self.range and self.range[1] is not None) else\n            (max(map(abs, self._values)) if self._values else None)\n        )", "response": "Get the maximum value of the current set of values"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nplots all dots for series", "response": "def _plot(self):\n        \"\"\"Plot all dots for series\"\"\"\n        r_max = min(\n            self.view.x(1) - self.view.x(0),\n            (self.view.y(0) or 0) - self.view.y(1)\n        ) / (2 * 1.05)\n        for serie in self.series:\n            self.dot(serie, r_max)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the values of the series.", "response": "def _values(self):\n        \"\"\"Getter for series values (flattened)\"\"\"\n        return [\n            val[1] for serie in self.series for val in\n            (serie.interpolated if self.interpolate else serie.points)\n            if val[1] is not None and (not self.logarithmic or val[1] > 0)\n        ]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _secondary_values(self):\n        return [\n            val[1] for serie in self.secondary_series for val in\n            (serie.interpolated if self.interpolate else serie.points)\n            if val[1] is not None and (not self.logarithmic or val[1] > 0)\n        ]", "response": "Get the secondary values in the flattened format"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fill(self, values):\n        zero = self.view.y(min(max(self.zero, self._box.ymin), self._box.ymax))\n\n        # Check to see if the data has been padded with \"none's\"\n        # Fill doesn't work correctly otherwise\n        end = len(values) - 1\n        while end > 0:\n            x, y = values[end]\n            if self.missing_value_fill_truncation == \"either\":\n                if x is not None and y is not None:\n                    break\n            elif self.missing_value_fill_truncation == \"x\":\n                if x is not None:\n                    break\n            elif self.missing_value_fill_truncation == \"y\":\n                if y is not None:\n                    break\n            else:\n                raise ValueError(\n                    \"Invalid value ({}) for config key \"\n                    \"'missing_value_fill_truncation';\"\n                    \" Use 'x', 'y' or 'either'\".format(\n                        self.missing_value_fill_truncation\n                    )\n                )\n            end -= 1\n\n        return ([(values[0][0], zero)] + values + [(values[end][0], zero)])", "response": "Fill the line with the given values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndrawing the line serie", "response": "def line(self, serie, rescale=False):\n        \"\"\"Draw the line serie\"\"\"\n        serie_node = self.svg.serie(serie)\n        if rescale and self.secondary_series:\n            points = self._rescale(serie.points)\n        else:\n            points = serie.points\n        view_values = list(map(self.view, points))\n        if serie.show_dots:\n            for i, (x, y) in enumerate(view_values):\n                if None in (x, y):\n                    continue\n                if self.logarithmic:\n                    if points[i][1] is None or points[i][1] <= 0:\n                        continue\n                if (serie.show_only_major_dots and self.x_labels\n                        and i < len(self.x_labels)\n                        and self.x_labels[i] not in self._x_labels_major):\n                    continue\n\n                metadata = serie.metadata.get(i)\n                classes = []\n                if x > self.view.width / 2:\n                    classes.append('left')\n                if y > self.view.height / 2:\n                    classes.append('top')\n                classes = ' '.join(classes)\n\n                self._confidence_interval(\n                    serie_node['overlay'], x, y, serie.values[i], metadata\n                )\n\n                dots = decorate(\n                    self.svg,\n                    self.svg.node(serie_node['overlay'], class_=\"dots\"),\n                    metadata\n                )\n\n                val = self._format(serie, i)\n                alter(\n                    self.svg.transposable_node(\n                        dots,\n                        'circle',\n                        cx=x,\n                        cy=y,\n                        r=serie.dots_size,\n                        class_='dot reactive tooltip-trigger'\n                    ), metadata\n                )\n                self._tooltip_data(\n                    dots, val, x, y, xlabel=self._get_x_label(i)\n                )\n                self._static_value(\n                    serie_node, val, x + self.style.value_font_size,\n                    y + self.style.value_font_size, metadata\n                )\n\n        if serie.stroke:\n            if self.interpolate:\n                points = serie.interpolated\n                if rescale and self.secondary_series:\n                    points = self._rescale(points)\n                view_values = list(map(self.view, points))\n            if serie.fill:\n                view_values = self._fill(view_values)\n\n            if serie.allow_interruptions:\n                # view_values are in form [(x1, y1), (x2, y2)]. We\n                # need to split that into multiple sequences if a\n                # None is present here\n\n                sequences = []\n                cur_sequence = []\n                for x, y in view_values:\n                    if y is None and len(cur_sequence) > 0:\n                        # emit current subsequence\n                        sequences.append(cur_sequence)\n                        cur_sequence = []\n                    elif y is None:  # just discard\n                        continue\n                    else:\n                        cur_sequence.append((x, y))  # append the element\n\n                if len(cur_sequence) > 0:  # emit last possible sequence\n                    sequences.append(cur_sequence)\n            else:\n                # plain vanilla rendering\n                sequences = [view_values]\n            if self.logarithmic:\n                for seq in sequences:\n                    for ele in seq[::-1]:\n                        y = points[seq.index(ele)][1]\n                        if y is None or y <= 0:\n                            del seq[seq.index(ele)]\n            for seq in sequences:\n                self.svg.line(\n                    serie_node['plot'],\n                    seq,\n                    close=self._self_close,\n                    class_='line reactive' +\n                    (' nofill' if not serie.fill else '')\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute(self):\n        # X Labels\n        if self.horizontal:\n            self._x_pos = [\n                x / (self._len - 1) for x in range(self._len)\n            ][::-1] if self._len != 1 else [.5]  # Center if only one value\n        else:\n            self._x_pos = [\n                x / (self._len - 1) for x in range(self._len)\n            ] if self._len != 1 else [.5]  # Center if only one value\n\n        self._points(self._x_pos)\n\n        if self.include_x_axis:\n            # Y Label\n            self._box.ymin = min(self._min or 0, 0)\n            self._box.ymax = max(self._max or 0, 0)\n        else:\n            self._box.ymin = self._min\n            self._box.ymax = self._max", "response": "Compute y min max and y scale and set labels"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a timestamp from a date in python 3 and python 2", "response": "def timestamp(x):\n    \"\"\"Get a timestamp from a date in python 3 and python 2\"\"\"\n    if x.tzinfo is None:\n        # Naive dates to utc\n        x = x.replace(tzinfo=utc)\n\n    if hasattr(x, 'timestamp'):\n        return x.timestamp()\n    else:\n        return (x - datetime(1970, 1, 1, tzinfo=utc)).total_seconds()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef coerce(self, value):\n        if self.type == Style:\n            return value\n        elif self.type == list:\n            return self.type(\n                map(self.subtype, map(lambda x: x.strip(), value.split(',')))\n            )\n        elif self.type == dict:\n            rv = {}\n            for pair in value.split(','):\n                key, val = pair.split(':')\n                key = key.strip()\n                val = val.strip()\n                try:\n                    rv[key] = self.subtype(val)\n                except Exception:\n                    rv[key] = val\n            return rv\n        return self.type(value)", "response": "Cast a string into this key type"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the config with the given dictionary", "response": "def _update(self, kwargs):\n        \"\"\"Update the config with the given dictionary\"\"\"\n        from pygal.util import merge\n        dir_self_set = set(dir(self))\n        merge(\n            self.__dict__,\n            dict([(k, v) for (k, v) in kwargs.items()\n                  if not k.startswith('_') and k in dir_self_set])\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _get_separated_values(self, secondary=False):\n        series = self.secondary_series if secondary else self.series\n        transposed = list(zip(*[serie.values for serie in series]))\n        positive_vals = [\n            sum([val for val in vals if val is not None and val >= self.zero])\n            for vals in transposed\n        ]\n        negative_vals = [\n            sum([val for val in vals if val is not None and val < self.zero])\n            for vals in transposed\n        ]\n        return positive_vals, negative_vals", "response": "Separate values between positives and negatives stacked"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _compute(self):\n        positive_vals, negative_vals = self._get_separated_values()\n\n        if self.logarithmic:\n            positive_vals = list(\n                filter(lambda x: x > self.zero, positive_vals)\n            )\n            negative_vals = list(\n                filter(lambda x: x > self.zero, negative_vals)\n            )\n\n        self._compute_box(positive_vals, negative_vals)\n        positive_vals = positive_vals or [self.zero]\n        negative_vals = negative_vals or [self.zero]\n\n        self._x_pos = [\n            x / self._len for x in range(self._len + 1)\n        ] if self._len > 1 else [0, 1]  # Center if only one value\n\n        self._points(self._x_pos)\n\n        self.negative_cumulation = [0] * self._len\n        self.positive_cumulation = [0] * self._len\n\n        if self.secondary_series:\n            positive_vals, negative_vals = self._get_separated_values(True)\n            positive_vals = positive_vals or [self.zero]\n            negative_vals = negative_vals or [self.zero]\n            self.secondary_negative_cumulation = [0] * self._len\n            self.secondary_positive_cumulation = [0] * self._len\n            self._pre_compute_secondary(positive_vals, negative_vals)\n\n        self._x_pos = [(i + .5) / self._len for i in range(self._len)]", "response": "Compute y min max and y scale and set labels"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomputes secondary y min and max", "response": "def _pre_compute_secondary(self, positive_vals, negative_vals):\n        \"\"\"Compute secondary y min and max\"\"\"\n        self._secondary_min = (\n            negative_vals and min(min(negative_vals), self.zero)\n        ) or self.zero\n        self._secondary_max = (\n            positive_vals and max(max(positive_vals), self.zero)\n        ) or self.zero"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw bars for series and secondary series", "response": "def _plot(self):\n        \"\"\"Draw bars for series and secondary series\"\"\"\n        for serie in self.series[::-1 if self.stack_from_top else 1]:\n            self.bar(serie)\n        for serie in self.secondary_series[::-1 if self.stack_from_top else 1]:\n            self.bar(serie, True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a serie to this graph compat api", "response": "def add(self, title, values, **kwargs):\n        \"\"\"Add a serie to this graph, compat api\"\"\"\n        if not is_list_like(values) and not isinstance(values, dict):\n            values = [values]\n        kwargs['title'] = title\n        self.raw_series.append((values, kwargs))\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef render(self, is_unicode=False, **kwargs):\n        self.setup(**kwargs)\n        svg = self.svg.render(\n            is_unicode=is_unicode, pretty_print=self.pretty_print\n        )\n        self.teardown()\n        return svg", "response": "Render the graph and return the svg string"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_tree(self, **kwargs):\n        self.setup(**kwargs)\n        svg = self.svg.root\n        for f in self.xml_filters:\n            svg = f(svg)\n        self.teardown()\n        return svg", "response": "Render the graph and return the lxml etree"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_table(self, **kwargs):\n        # Import here to avoid lxml import\n        try:\n            from pygal.table import Table\n        except ImportError:\n            raise ImportError('You must install lxml to use render table')\n        return Table(self).render(**kwargs)", "response": "Render the data as a html table"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_pyquery(self, **kwargs):\n        from pyquery import PyQuery as pq\n        return pq(self.render(**kwargs), parser='html')", "response": "Render the graph and return a pyquery wrapped tree"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender the graph open it in your browser with black magic", "response": "def render_in_browser(self, **kwargs):\n        \"\"\"Render the graph, open it in your browser with black magic\"\"\"\n        try:\n            from lxml.html import open_in_browser\n        except ImportError:\n            raise ImportError('You must install lxml to use render in browser')\n        kwargs.setdefault('force_uri_protocol', 'https')\n        open_in_browser(self.render_tree(**kwargs), encoding='utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef render_response(self, **kwargs):\n        from flask import Response\n        return Response(self.render(**kwargs), mimetype='image/svg+xml')", "response": "Render the graph and return a Flask response"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef render_django_response(self, **kwargs):\n        from django.http import HttpResponse\n        return HttpResponse(\n            self.render(**kwargs), content_type='image/svg+xml'\n        )", "response": "Render the graph and return a Django response"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\noutput a base 64 encoded data uri", "response": "def render_data_uri(self, **kwargs):\n        \"\"\"Output a base 64 encoded data uri\"\"\"\n        # Force protocol as data uri have none\n        kwargs.setdefault('force_uri_protocol', 'https')\n        return \"data:image/svg+xml;charset=utf-8;base64,%s\" % (\n            base64.b64encode(self.render(**kwargs)\n                             ).decode('utf-8').replace('\\n', '')\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render_to_file(self, filename, **kwargs):\n        with io.open(filename, 'w', encoding='utf-8') as f:\n            f.write(self.render(is_unicode=True, **kwargs))", "response": "Render the graph and write it to filename"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_to_png(self, filename=None, dpi=72, **kwargs):\n        import cairosvg\n        return cairosvg.svg2png(\n            bytestring=self.render(**kwargs), write_to=filename, dpi=dpi\n        )", "response": "Render the graph and write it to filename"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef render_sparktext(self, relative_to=None):\n        bars = u('\u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588')\n        if len(self.raw_series) == 0:\n            return u('')\n        values = list(self.raw_series[0][0])\n        if len(values) == 0:\n            return u('')\n\n        chart = u('')\n        values = list(map(lambda x: max(x, 0), values))\n\n        vmax = max(values)\n        if relative_to is None:\n            relative_to = min(values)\n\n        if (vmax - relative_to) == 0:\n            chart = bars[0] * len(values)\n            return chart\n\n        divisions = len(bars) - 1\n        for value in values:\n            chart += bars[int(\n                divisions * (value - relative_to) / (vmax - relative_to)\n            )]\n        return chart", "response": "Make a mini text sparkline from chart"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _values(self):\n        if self.interpolate:\n            return [\n                val[0] for serie in self.series for val in serie.interpolated\n            ]\n        else:\n            return super(Line, self)._values", "response": "Getter for series values (flattened)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nassign a view to current graph", "response": "def _set_view(self):\n        \"\"\"Assign a view to current graph\"\"\"\n        if self.logarithmic:\n            view_class = PolarLogView\n        else:\n            view_class = PolarView\n\n        self.view = view_class(\n            self.width - self.margin_box.x, self.height - self.margin_box.y,\n            self._box\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides x axis to make it polar", "response": "def _x_axis(self, draw_axes=True):\n        \"\"\"Override x axis to make it polar\"\"\"\n        if not self._x_labels or not self.show_x_labels:\n            return\n\n        axis = self.svg.node(\n            self.nodes['plot'],\n            class_=\"axis x web%s\" %\n            (' always_show' if self.show_x_guides else '')\n        )\n        format_ = lambda x: '%f %f' % x\n        center = self.view((0, 0))\n        r = self._rmax\n\n        # Can't simply determine truncation\n        truncation = self.truncate_label or 25\n\n        for label, theta in self._x_labels:\n            major = label in self._x_labels_major\n            if not (self.show_minor_x_labels or major):\n                continue\n            guides = self.svg.node(axis, class_='guides')\n            end = self.view((r, theta))\n\n            self.svg.node(\n                guides,\n                'path',\n                d='M%s L%s' % (format_(center), format_(end)),\n                class_='%s%sline' %\n                ('axis ' if label == \"0\" else '', 'major ' if major else '')\n            )\n\n            r_txt = (1 - self._box.__class__.margin) * self._box.ymax\n            pos_text = self.view((r_txt, theta))\n            text = self.svg.node(\n                guides,\n                'text',\n                x=pos_text[0],\n                y=pos_text[1],\n                class_='major' if major else ''\n            )\n            text.text = truncate(label, truncation)\n            if text.text != label:\n                self.svg.node(guides, 'title').text = label\n            else:\n                self.svg.node(\n                    guides,\n                    'title',\n                ).text = self._x_format(theta)\n\n            angle = -theta + pi / 2\n            if cos(angle) < 0:\n                angle -= pi\n            text.attrib['transform'] = 'rotate(%f %s)' % (\n                self.x_label_rotation or deg(angle), format_(pos_text)\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _y_axis(self, draw_axes=True):\n        if not self._y_labels or not self.show_y_labels:\n            return\n\n        axis = self.svg.node(self.nodes['plot'], class_=\"axis y web\")\n\n        for label, r in reversed(self._y_labels):\n            major = r in self._y_labels_major\n            if not (self.show_minor_y_labels or major):\n                continue\n            guides = self.svg.node(\n                axis,\n                class_='%sguides' %\n                ('logarithmic ' if self.logarithmic else '')\n            )\n            if self.show_y_guides:\n                self.svg.line(\n                    guides, [self.view((r, theta)) for theta in self._x_pos],\n                    close=True,\n                    class_='%sguide line' % ('major ' if major else '')\n                )\n            x, y = self.view((r, self._x_pos[0]))\n            x -= 5\n            text = self.svg.node(\n                guides, 'text', x=x, y=y, class_='major' if major else ''\n            )\n            text.text = label\n\n            if self.y_label_rotation:\n                text.attrib[\n                    'transform'\n                ] = \"rotate(%d %f %f)\" % (self.y_label_rotation, x, y)\n\n            self.svg.node(\n                guides,\n                'title',\n            ).text = self._y_format(r)", "response": "Override y axis to make it polar"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncompute r min max and labels position", "response": "def _compute(self):\n        \"\"\"Compute r min max and labels position\"\"\"\n        delta = 2 * pi / self._len if self._len else 0\n        self._x_pos = [.5 * pi + i * delta for i in range(self._len + 1)]\n        for serie in self.all_series:\n            serie.points = [(v, self._x_pos[i])\n                            for i, v in enumerate(serie.values)]\n            if self.interpolate:\n                extended_x_pos = ([.5 * pi - delta] + self._x_pos)\n                extended_vals = (serie.values[-1:] + serie.values)\n                serie.interpolated = list(\n                    map(\n                        tuple,\n                        map(\n                            reversed,\n                            self._interpolate(extended_x_pos, extended_vals)\n                        )\n                    )\n                )\n\n        # x labels space\n        self._box.margin *= 2\n        self._rmin = self.zero\n        self._rmax = self._max or 1\n        self._box.set_polar_box(self._rmin, self._rmax)\n        self._self_close = True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _value_format(self, value):\n        return '%s: %s' % (\n            self.area_names.get(self.adapt_code(value[0]), '?'),\n            self._y_format(value[1])\n        )", "response": "Format value for map value display."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _plot(self):\n        map = etree.fromstring(self.svg_map)\n        map.set('width', str(self.view.width))\n        map.set('height', str(self.view.height))\n\n        for i, serie in enumerate(self.series):\n            safe_vals = list(\n                filter(lambda x: x is not None, cut(serie.values, 1))\n            )\n            if not safe_vals:\n                continue\n            min_ = min(safe_vals)\n            max_ = max(safe_vals)\n            for j, (area_code, value) in self.enumerate_values(serie):\n                area_code = self.adapt_code(area_code)\n                if value is None:\n                    continue\n                if max_ == min_:\n                    ratio = 1\n                else:\n                    ratio = .3 + .7 * (value - min_) / (max_ - min_)\n\n                areae = map.findall(\n                    \".//*[@class='%s%s %s map-element']\" %\n                    (self.area_prefix, area_code, self.kind)\n                )\n\n                if not areae:\n                    continue\n\n                for area in areae:\n                    cls = area.get('class', '').split(' ')\n                    cls.append('color-%d' % i)\n                    cls.append('serie-%d' % i)\n                    cls.append('series')\n                    area.set('class', ' '.join(cls))\n                    area.set('style', 'fill-opacity: %f' % ratio)\n\n                    metadata = serie.metadata.get(j)\n\n                    if metadata:\n                        node = decorate(self.svg, area, metadata)\n                        if node != area:\n                            area.remove(node)\n                            for g in map:\n                                if area not in g:\n                                    continue\n                                index = list(g).index(area)\n                                g.remove(area)\n                                node.append(area)\n                                g.insert(index, node)\n\n                    for node in area:\n                        cls = node.get('class', '').split(' ')\n                        cls.append('reactive')\n                        cls.append('tooltip-trigger')\n                        cls.append('map-area')\n                        node.set('class', ' '.join(cls))\n                        alter(node, metadata)\n\n                    val = self._format(serie, j)\n                    self._tooltip_data(area, val, 0, 0, 'auto')\n\n        self.nodes['plot'].append(map)", "response": "Insert a map in the chart and apply data on it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrendering the HTMTL table of the chart.", "response": "def render(self, total=False, transpose=False, style=False):\n        \"\"\"Render the HTMTL table of the chart.\n\n        `total` can be specified to include data sums\n        `transpose` make labels becomes columns\n        `style` include scoped style for the table\n\n        \"\"\"\n        self.chart.setup()\n        ln = self.chart._len\n        html = HTML()\n        attrs = {}\n\n        if style:\n            attrs['id'] = 'table-%s' % uuid.uuid4()\n\n        table = []\n\n        _ = lambda x: x if x is not None else ''\n\n        if self.chart.x_labels:\n            labels = [None] + list(self.chart.x_labels)\n            if len(labels) < ln:\n                labels += [None] * (ln + 1 - len(labels))\n            if len(labels) > ln + 1:\n                labels = labels[:ln + 1]\n            table.append(labels)\n\n        if total:\n            if len(table):\n                table[0].append('Total')\n            else:\n                table.append([None] * (ln + 1) + ['Total'])\n            acc = [0] * (ln + 1)\n\n        for i, serie in enumerate(self.chart.all_series):\n            row = [serie.title]\n            if total:\n                sum_ = 0\n            for j, value in enumerate(serie.values):\n                if total:\n                    v = value or 0\n                    acc[j] += v\n                    sum_ += v\n                row.append(self.chart._format(serie, j))\n            if total:\n                acc[-1] += sum_\n                row.append(self.chart._serie_format(serie, sum_))\n            table.append(row)\n\n        width = ln + 1\n        if total:\n            width += 1\n            table.append(['Total'])\n            for val in acc:\n                table[-1].append(self.chart._serie_format(serie, val))\n\n        # Align values\n        len_ = max([len(r) for r in table] or [0])\n\n        for i, row in enumerate(table[:]):\n            len_ = len(row)\n            if len_ < width:\n                table[i] = row + [None] * (width - len_)\n\n        if not transpose:\n            table = list(zip(*table))\n\n        thead = []\n        tbody = []\n        tfoot = []\n\n        if not transpose or self.chart.x_labels:\n            # There's always series title but not always x_labels\n            thead = [table[0]]\n            tbody = table[1:]\n        else:\n            tbody = table\n\n        if total:\n            tfoot = [tbody[-1]]\n            tbody = tbody[:-1]\n\n        parts = []\n        if thead:\n            parts.append(\n                html.thead(\n                    *[html.tr(*[html.th(_(col)) for col in r]) for r in thead]\n                )\n            )\n        if tbody:\n            parts.append(\n                html.tbody(\n                    *[html.tr(*[html.td(_(col)) for col in r]) for r in tbody]\n                )\n            )\n        if tfoot:\n            parts.append(\n                html.tfoot(\n                    *[html.tr(*[html.th(_(col)) for col in r]) for r in tfoot]\n                )\n            )\n\n        table = tostring(html.table(*parts, **attrs))\n        if style:\n            if style is True:\n                css = '''\n                #{{ id }} {\n                    border-collapse: collapse;\n                    border-spacing: 0;\n                    empty-cells: show;\n                    border: 1px solid #cbcbcb;\n                }\n                #{{ id }} td, #{{ id }} th {\n                    border-left: 1px solid #cbcbcb;\n                    border-width: 0 0 0 1px;\n                    margin: 0;\n                    padding: 0.5em 1em;\n                }\n                #{{ id }} td:first-child, #{{ id }} th:first-child {\n                    border-left-width: 0;\n                }\n                #{{ id }} thead, #{{ id }} tfoot {\n                    color: #000;\n                    text-align: left;\n                    vertical-align: bottom;\n                }\n                #{{ id }} thead {\n                    background: #e0e0e0;\n                }\n                #{{ id }} tfoot {\n                    background: #ededed;\n                }\n                #{{ id }} tr:nth-child(2n-1) td {\n                    background-color: #f2f2f2;\n                }\n                '''\n            else:\n                css = style\n            table = tostring(\n                html.style(template(css, **attrs), scoped='scoped')\n            ) + table\n        table = table.decode('utf-8')\n        self.chart.teardown()\n        return table"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing all the serie slices", "response": "def _plot(self):\n        \"\"\"Draw all the serie slices\"\"\"\n        squares = self._squares()\n        sq_dimensions = self.add_squares(squares)\n\n        for index, serie in enumerate(self.series):\n            current_square = self._current_square(squares, index)\n            self.gaugify(serie, squares, sq_dimensions, current_square)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nformatting the value and cumulation.", "response": "def _value_format(self, value, serie, index):\n        \"\"\"\n        Display value and cumulation\n        \"\"\"\n        sum_ = serie.points[index][1]\n        if serie in self.series and (\n                self.stack_from_top\n                and self.series.index(serie) == self._order - 1\n                or not self.stack_from_top and self.series.index(serie) == 0):\n            return super(StackedLine, self)._value_format(value)\n        return '%s (+%s)' % (self._y_format(sum_), self._y_format(value))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding extra values to fill the line", "response": "def _fill(self, values):\n        \"\"\"Add extra values to fill the line\"\"\"\n        if not self._previous_line:\n            self._previous_line = values\n            return super(StackedLine, self)._fill(values)\n        new_values = values + list(reversed(self._previous_line))\n        self._previous_line = values\n        return new_values"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _points(self, x_pos):\n        for series_group in (self.series, self.secondary_series):\n            accumulation = [0] * self._len\n            for serie in series_group[::-1 if self.stack_from_top else 1]:\n                accumulation = list(map(sum, zip(accumulation, serie.values)))\n                serie.points = [(x_pos[i], v)\n                                for i, v in enumerate(accumulation)]\n                if serie.points and self.interpolate:\n                    serie.interpolated = self._interpolate(x_pos, accumulation)\n                else:\n                    serie.interpolated = []", "response": "Convert given data values into drawable points and interpolated points if interpolate option is specified."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _plot(self):\n        for serie in self.series[::-1 if self.stack_from_top else 1]:\n            self.line(serie)\n        for serie in self.secondary_series[::-1 if self.stack_from_top else 1]:\n            self.line(serie, True)", "response": "Plot stacked serie lines and stacked secondary lines"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_styles(self):\n        colors = self.graph.style.get_colors(self.id, self.graph._order)\n        strokes = self.get_strokes()\n        all_css = []\n        auto_css = ['file://base.css']\n\n        if self.graph.style._google_fonts:\n            auto_css.append(\n                '//fonts.googleapis.com/css?family=%s' %\n                quote_plus('|'.join(self.graph.style._google_fonts))\n            )\n\n        for css in auto_css + list(self.graph.css):\n            css_text = None\n            if css.startswith('inline:'):\n                css_text = css[len('inline:'):]\n            elif css.startswith('file://'):\n                css = css[len('file://'):]\n\n                if not os.path.exists(css):\n                    css = os.path.join(os.path.dirname(__file__), 'css', css)\n\n                with io.open(css, encoding='utf-8') as f:\n                    css_text = template(\n                        f.read(),\n                        style=self.graph.style,\n                        colors=colors,\n                        strokes=strokes,\n                        id=self.id\n                    )\n\n            if css_text is not None:\n                if not self.graph.pretty_print:\n                    css_text = minify_css(css_text)\n                all_css.append(css_text)\n            else:\n                if css.startswith('//') and self.graph.force_uri_protocol:\n                    css = '%s:%s' % (self.graph.force_uri_protocol, css)\n                self.processing_instructions.append(\n                    etree.PI(u('xml-stylesheet'), u('href=\"%s\"' % css))\n                )\n        self.node(\n            self.defs, 'style', type='text/css'\n        ).text = '\\n'.join(all_css)", "response": "Add the css to the svg"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding the js to the svg", "response": "def add_scripts(self):\n        \"\"\"Add the js to the svg\"\"\"\n        common_script = self.node(self.defs, 'script', type='text/javascript')\n\n        def get_js_dict():\n            return dict(\n                (k, getattr(self.graph.state, k))\n                for k in dir(self.graph.config)\n                if not k.startswith('_') and hasattr(self.graph.state, k)\n                and not hasattr(getattr(self.graph.state, k), '__call__')\n            )\n\n        def json_default(o):\n            if isinstance(o, (datetime, date)):\n                return o.isoformat()\n            if hasattr(o, 'to_dict'):\n                return o.to_dict()\n            return json.JSONEncoder().default(o)\n\n        dct = get_js_dict()\n        # Config adds\n        dct['legends'] = [\n            l.get('title') if isinstance(l, dict) else l\n            for l in self.graph._legends + self.graph._secondary_legends\n        ]\n\n        common_js = 'window.pygal = window.pygal || {};'\n        common_js += 'window.pygal.config = window.pygal.config || {};'\n        if self.graph.no_prefix:\n            common_js += 'window.pygal.config = '\n        else:\n            common_js += 'window.pygal.config[%r] = ' % self.graph.uuid\n\n        common_script.text = common_js + json.dumps(dct, default=json_default)\n\n        for js in self.graph.js:\n            if js.startswith('file://'):\n                script = self.node(self.defs, 'script', type='text/javascript')\n                with io.open(js[len('file://'):], encoding='utf-8') as f:\n                    script.text = f.read()\n            else:\n                if js.startswith('//') and self.graph.force_uri_protocol:\n                    js = '%s:%s' % (self.graph.force_uri_protocol, js)\n                self.node(self.defs, 'script', type='text/javascript', href=js)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a new svg node", "response": "def node(self, parent=None, tag='g', attrib=None, **extras):\n        \"\"\"Make a new svg node\"\"\"\n        if parent is None:\n            parent = self.root\n        attrib = attrib or {}\n        attrib.update(extras)\n\n        def in_attrib_and_number(key):\n            return key in attrib and isinstance(attrib[key], Number)\n\n        for pos, dim in (('x', 'width'), ('y', 'height')):\n            if in_attrib_and_number(dim) and attrib[dim] < 0:\n                attrib[dim] = -attrib[dim]\n                if in_attrib_and_number(pos):\n                    attrib[pos] = attrib[pos] - attrib[dim]\n\n        for key, value in dict(attrib).items():\n            if value is None:\n                del attrib[key]\n\n            attrib[key] = to_str(value)\n            if key.endswith('_'):\n                attrib[key.rstrip('_')] = attrib[key]\n                del attrib[key]\n            elif key == 'href':\n                attrib[etree.QName('http://www.w3.org/1999/xlink',\n                                   key)] = attrib[key]\n                del attrib[key]\n        return etree.SubElement(parent, tag, attrib)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transposable_node(self, parent=None, tag='g', attrib=None, **extras):\n        if self.graph.horizontal:\n            for key1, key2 in (('x', 'y'), ('width', 'height'), ('cx', 'cy')):\n                attr1 = extras.get(key1, None)\n                attr2 = extras.get(key2, None)\n                if attr2:\n                    extras[key1] = attr2\n                elif attr1:\n                    del extras[key1]\n                if attr1:\n                    extras[key2] = attr1\n                elif attr2:\n                    del extras[key2]\n        return self.node(parent, tag, attrib, **extras)", "response": "Make a new svg node which can be transposed if horizontal"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef line(self, node, coords, close=False, **kwargs):\n        line_len = len(coords)\n        if len([c for c in coords if c[1] is not None]) < 2:\n            return\n        root = 'M%s L%s Z' if close else 'M%s L%s'\n        origin_index = 0\n        while origin_index < line_len and None in coords[origin_index]:\n            origin_index += 1\n        if origin_index == line_len:\n            return\n        if self.graph.horizontal:\n            coord_format = lambda xy: '%f %f' % (xy[1], xy[0])\n        else:\n            coord_format = lambda xy: '%f %f' % xy\n\n        origin = coord_format(coords[origin_index])\n        line = ' '.join([\n            coord_format(c) for c in coords[origin_index + 1:] if None not in c\n        ])\n        return self.node(node, 'path', d=root % (origin, line), **kwargs)", "response": "Draw a svg line"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef slice(\n            self, serie_node, node, radius, small_radius, angle, start_angle,\n            center, val, i, metadata\n    ):\n        \"\"\"Draw a pie slice\"\"\"\n        if angle == 2 * pi:\n            angle = nearly_2pi\n\n        if angle > 0:\n            to = [\n                coord_abs_project(center, radius, start_angle),\n                coord_abs_project(center, radius, start_angle + angle),\n                coord_abs_project(center, small_radius, start_angle + angle),\n                coord_abs_project(center, small_radius, start_angle)\n            ]\n            rv = self.node(\n                node,\n                'path',\n                d='M%s A%s 0 %d 1 %s L%s A%s 0 %d 0 %s z' % (\n                    to[0], coord_dual(radius), int(angle > pi), to[1], to[2],\n                    coord_dual(small_radius), int(angle > pi), to[3]\n                ),\n                class_='slice reactive tooltip-trigger'\n            )\n        else:\n            rv = None\n        x, y = coord_diff(\n            center,\n            coord_project((radius + small_radius) / 2, start_angle + angle / 2)\n        )\n\n        self.graph._tooltip_data(\n            node, val, x, y, \"centered\", self.graph._x_labels\n            and self.graph._x_labels[i][0]\n        )\n        if angle >= 0.3:  # 0.3 radians is about 17 degrees\n            self.graph._static_value(serie_node, val, x, y, metadata)\n        return rv", "response": "Draw a pie slice"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pre_render(self):\n        self.add_styles()\n        self.add_scripts()\n        self.root.set(\n            'viewBox', '0 0 %d %d' % (self.graph.width, self.graph.height)\n        )\n        if self.graph.explicit_size:\n            self.root.set('width', str(self.graph.width))\n            self.root.set('height', str(self.graph.height))", "response": "Pre - render the image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the no data text to the svg", "response": "def draw_no_data(self):\n        \"\"\"Write the no data text to the svg\"\"\"\n        no_data = self.node(\n            self.graph.nodes['text_overlay'],\n            'text',\n            x=self.graph.view.width / 2,\n            y=self.graph.view.height / 2,\n            class_='no_data'\n        )\n        no_data.text = self.graph.no_data_text"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render(self, is_unicode=False, pretty_print=False):\n        for f in self.graph.xml_filters:\n            self.root = f(self.root)\n        args = {'encoding': 'utf-8'}\n\n        svg = b''\n        if etree.lxml:\n            args['pretty_print'] = pretty_print\n\n        if not self.graph.disable_xml_declaration:\n            svg = b\"<?xml version='1.0' encoding='utf-8'?>\\n\"\n\n        if not self.graph.disable_xml_declaration:\n            svg += b'\\n'.join([\n                etree.tostring(pi, **args)\n                for pi in self.processing_instructions\n            ])\n\n        svg += etree.tostring(self.root, **args)\n\n        if self.graph.disable_xml_declaration or is_unicode:\n            svg = svg.decode('utf-8')\n        return svg", "response": "Render the XML document."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_strokes(self):\n\n        def stroke_dict_to_css(stroke, i=None):\n            \"\"\"Return a css style for the given option\"\"\"\n            css = [\n                '%s.series%s {\\n' %\n                (self.id, '.serie-%d' % i if i is not None else '')\n            ]\n            for key in ('width', 'linejoin', 'linecap', 'dasharray',\n                        'dashoffset'):\n                if stroke.get(key):\n                    css.append('  stroke-%s: %s;\\n' % (key, stroke[key]))\n            css.append('}')\n            return '\\n'.join(css)\n\n        css = []\n        if self.graph.stroke_style is not None:\n            css.append(stroke_dict_to_css(self.graph.stroke_style))\n        for serie in self.graph.series:\n            if serie.stroke_style is not None:\n                css.append(stroke_dict_to_css(serie.stroke_style, serie.index))\n\n        for secondary_serie in self.graph.secondary_series:\n            if secondary_serie.stroke_style is not None:\n                css.append(\n                    stroke_dict_to_css(\n                        secondary_serie.stroke_style, secondary_serie.index\n                    )\n                )\n        return '\\n'.join(css)", "response": "Return a css snippet containing all stroke options"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _min(self):\n        return (\n            self.range[0] if (self.range and self.range[0] is not None) else\n            (min(self.yvals) if self.yvals else None)\n        )", "response": "Getter for the minimum series value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the maximum value of the current instance of the class.", "response": "def _max(self):\n        \"\"\"Getter for the maximum series value\"\"\"\n        return (\n            self.range[1] if (self.range and self.range[1] is not None) else\n            (max(self.yvals) if self.yvals else None)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute x min max and x scale and set labels", "response": "def _compute(self):\n        \"\"\"Compute x/y min and max and x/y scale and set labels\"\"\"\n        if self.xvals:\n            if self.xrange:\n                x_adapter = reduce(compose, self._x_adapters) if getattr(\n                    self, '_x_adapters', None\n                ) else ident\n\n                xmin = x_adapter(self.xrange[0])\n                xmax = x_adapter(self.xrange[1])\n\n            else:\n                xmin = min(self.xvals)\n                xmax = max(self.xvals)\n            xrng = (xmax - xmin)\n        else:\n            xrng = None\n\n        if self.yvals:\n            ymin = self._min\n            ymax = self._max\n\n            if self.include_x_axis:\n                ymin = min(ymin or 0, 0)\n                ymax = max(ymax or 0, 0)\n\n            yrng = (ymax - ymin)\n        else:\n            yrng = None\n\n        for serie in self.all_series:\n            serie.points = serie.values\n            if self.interpolate:\n                vals = list(\n                    zip(\n                        *sorted(\n                            filter(lambda t: None not in t, serie.points),\n                            key=lambda x: x[0]\n                        )\n                    )\n                )\n                serie.interpolated = self._interpolate(vals[0], vals[1])\n\n        if self.interpolate:\n            self.xvals = [\n                val[0] for serie in self.all_series\n                for val in serie.interpolated\n            ]\n            self.yvals = [\n                val[1] for serie in self.series for val in serie.interpolated\n            ]\n            if self.xvals:\n                xmin = min(self.xvals)\n                xmax = max(self.xvals)\n                xrng = (xmax - xmin)\n            else:\n                xrng = None\n\n        # these values can also be 0 (zero), so testing explicitly for None\n        if xrng is not None:\n            self._box.xmin, self._box.xmax = xmin, xmax\n\n        if yrng is not None:\n            self._box.ymin, self._box.ymax = ymin, ymax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfind the module that is loaded by the load_module function.", "response": "def find_module(self, fullname, path=None):\n        \"\"\"\n        Tell if the module to load can be loaded by\n        the load_module function, ie: if it is a ``pygal.maps.*``\n        module.\n        \"\"\"\n        if fullname.startswith('pygal.maps.') and hasattr(\n                maps, fullname.split('.')[2]):\n            return self\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_module(self, name):\n        if name not in sys.modules:\n            sys.modules[name] = getattr(maps, name.split('.')[2])\n        return sys.modules[name]", "response": "Load the pygal. maps. name module from the previously loaded plugin\n           "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfilter sequence to return only major considered numbers", "response": "def majorize(values):\n    \"\"\"Filter sequence to return only major considered numbers\"\"\"\n    sorted_values = sorted(values)\n    if len(values) <= 3 or (\n            abs(2 * sorted_values[1] - sorted_values[0] - sorted_values[2]) >\n            abs(1.5 * (sorted_values[1] - sorted_values[0]))):\n        return []\n    values_step = sorted_values[1] - sorted_values[0]\n    full_range = sorted_values[-1] - sorted_values[0]\n    step = 10**int(log10(full_range))\n    if step == values_step:\n        step *= 10\n    step_factor = 10**(int(log10(step)) + 1)\n    if round(step * step_factor) % (round(values_step * step_factor) or 1):\n        # TODO: Find lower common multiple instead\n        step *= values_step\n    if full_range <= 2 * step:\n        step *= .5\n    elif full_range >= 5 * step:\n        step *= 5\n    major_values = [\n        value for value in values if value / step == round(value / step)\n    ]\n    return [value for value in sorted_values if value in major_values]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nround a number to a given precision", "response": "def round_to_int(number, precision):\n    \"\"\"Round a number to a precision\"\"\"\n    precision = int(precision)\n    rounded = (int(number) + precision / 2) // precision * precision\n    return rounded"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef round_to_float(number, precision):\n    rounded = Decimal(str(floor((number + precision / 2) // precision))\n                      ) * Decimal(str(precision))\n    return float(rounded)", "response": "Round a float to a precision"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrounding a number or a float to a precision", "response": "def round_to_scale(number, precision):\n    \"\"\"Round a number or a float to a precision\"\"\"\n    if precision < 1:\n        return round_to_float(number, precision)\n    return round_to_int(number, precision)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncut a list by index or arg", "response": "def cut(list_, index=0):\n    \"\"\"Cut a list by index or arg\"\"\"\n    if isinstance(index, int):\n        cut_ = lambda x: x[index]\n    else:\n        cut_ = lambda x: getattr(x, index)\n    return list(map(cut_, list_))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _swap_curly(string):\n    return (\n        string.replace('{{ ', '{{').replace('{{', '\\x00').replace('{', '{{')\n        .replace('\\x00', '{').replace(' }}', '}}').replace('}}', '\\x00')\n        .replace('}', '}}').replace('\\x00', '}')\n    )", "response": "Swap single and double curly brackets"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_logarithmic_scale(min_, max_, min_scale, max_scale):\n    if max_ <= 0 or min_ <= 0:\n        return []\n    min_order = int(floor(log10(min_)))\n    max_order = int(ceil(log10(max_)))\n    positions = []\n    amplitude = max_order - min_order\n    if amplitude <= 1:\n        return []\n    detail = 10.\n    while amplitude * detail < min_scale * 5:\n        detail *= 2\n    while amplitude * detail > max_scale * 3:\n        detail /= 2\n    for order in range(min_order, max_order + 1):\n        for i in range(int(detail)):\n            tick = (10 * i / detail or 1) * 10**order\n            tick = round_to_scale(tick, tick)\n            if min_ <= tick <= max_ and tick not in positions:\n                positions.append(tick)\n    return positions", "response": "Compute an optimal scale for logarithmic."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncompute an optimal scale between min and max", "response": "def compute_scale(min_, max_, logarithmic, order_min, min_scale, max_scale):\n    \"\"\"Compute an optimal scale between min and max\"\"\"\n    if min_ == 0 and max_ == 0:\n        return [0]\n    if max_ - min_ == 0:\n        return [min_]\n    if logarithmic:\n        log_scale = compute_logarithmic_scale(min_, max_, min_scale, max_scale)\n        if log_scale:\n            return log_scale\n            # else we fallback to normal scalling\n\n    order = round(log10(max(abs(min_), abs(max_)))) - 1\n    if order_min is not None and order < order_min:\n        order = order_min\n    else:\n        while ((max_ - min_) / (10**order) < min_scale\n               and (order_min is None or order > order_min)):\n            order -= 1\n    step = float(10**order)\n    while (max_ - min_) / step > max_scale:\n        step *= 2.\n    positions = []\n    position = round_to_scale(min_, step)\n    while position < (max_ + step):\n        rounded = round_to_scale(position, step)\n        if min_ <= rounded <= max_:\n            if rounded not in positions:\n                positions.append(rounded)\n        position += step\n    if len(positions) < 2:\n        return [min_, max_]\n    return positions"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decorate(svg, node, metadata):\n    if not metadata:\n        return node\n    xlink = metadata.get('xlink')\n    if xlink:\n        if not isinstance(xlink, dict):\n            xlink = {'href': xlink, 'target': '_blank'}\n        node = svg.node(node, 'a', **xlink)\n        svg.node(\n            node, 'desc', class_='xlink'\n        ).text = to_unicode(xlink.get('href'))\n\n    if 'tooltip' in metadata:\n        svg.node(node, 'title').text = to_unicode(metadata['tooltip'])\n\n    if 'color' in metadata:\n        color = metadata.pop('color')\n        node.attrib['style'] = 'fill: %s; stroke: %s' % (color, color)\n\n    if 'style' in metadata:\n        node.attrib['style'] = metadata.pop('style')\n\n    if 'label' in metadata and metadata['label']:\n        svg.node(\n            node, 'desc', class_='label'\n        ).text = to_unicode(metadata['label'])\n    return node", "response": "Add metedata next to a node"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef alter(node, metadata):\n    if node is not None and metadata and 'node' in metadata:\n        node.attrib.update(\n            dict((k, str(v)) for k, v in metadata['node'].items())\n        )", "response": "Override nodes attributes from metadata node mapping"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef truncate(string, index):\n    if len(string) > index and index > 0:\n        string = string[:index - 1] + u('\u2026')\n    return string", "response": "Truncate a string at index and add..."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncompose a function f and g.", "response": "def compose(f, g):\n    \"\"\"Chain functions\"\"\"\n    fun = lambda *args, **kwargs: f(g(*args, **kwargs))\n    fun.__name__ = \"%s o %s\" % (f.__name__, g.__name__)\n    return fun"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef split_title(title, width, title_fs):\n    titles = []\n    if not title:\n        return titles\n    size = reverse_text_len(width, title_fs * 1.1)\n    title_lines = title.split(\"\\n\")\n    for title_line in title_lines:\n        while len(title_line) > size:\n            title_part = title_line[:size]\n            i = title_part.rfind(' ')\n            if i == -1:\n                i = len(title_part)\n            titles.append(title_part[:i])\n            title_line = title_line[i:].strip()\n        titles.append(title_line)\n    return titles", "response": "Split a string for a specified width and font size"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats value for dual value display.", "response": "def _value_format(self, value, serie):\n        \"\"\"\n        Format value for dual value display.\n        \"\"\"\n        if self.box_mode == \"extremes\":\n            return (\n                'Min: %s\\nQ1 : %s\\nQ2 : %s\\nQ3 : %s\\nMax: %s' %\n                tuple(map(self._y_format, serie.points[1:6]))\n            )\n        elif self.box_mode in [\"tukey\", \"stdev\", \"pstdev\"]:\n            return (\n                'Min: %s\\nLower Whisker: %s\\nQ1: %s\\nQ2: %s\\nQ3: %s\\n'\n                'Upper Whisker: %s\\nMax: %s' %\n                tuple(map(self._y_format, serie.points))\n            )\n        elif self.box_mode == '1.5IQR':\n            # 1.5IQR mode\n            return 'Q1: %s\\nQ2: %s\\nQ3: %s' % tuple(\n                map(self._y_format, serie.points[2:5])\n            )\n        else:\n            return self._y_format(serie.points)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the parameters necessary for later steps within the rendering process.", "response": "def _compute(self):\n        \"\"\"\n        Compute parameters necessary for later steps\n        within the rendering process\n        \"\"\"\n        for serie in self.series:\n            serie.points, serie.outliers = \\\n                self._box_points(serie.values, self.box_mode)\n\n        self._x_pos = [(i + .5) / self._order for i in range(self._order)]\n\n        if self._min:\n            self._box.ymin = min(self._min, self.zero)\n        if self._max:\n            self._box.ymax = max(self._max, self.zero)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _boxf(self, serie):\n        serie_node = self.svg.serie(serie)\n        # Note: q0 and q4 do not literally mean the zero-th quartile\n        # and the fourth quartile, but rather the distance from 1.5 times\n        # the inter-quartile range to Q1 and Q3, respectively.\n        boxes = self.svg.node(serie_node['plot'], class_=\"boxes\")\n\n        metadata = serie.metadata.get(0)\n\n        box = decorate(self.svg, self.svg.node(boxes, class_='box'), metadata)\n\n        val = self._format(serie, 0)\n\n        x_center, y_center = self._draw_box(\n            box, serie.points[1:6], serie.outliers, serie.index, metadata\n        )\n        self._tooltip_data(\n            box, val, x_center, y_center, \"centered\",\n            self._get_x_label(serie.index)\n        )\n        self._static_value(serie_node, val, x_center, y_center, metadata)", "response": "For a specific series draw the box plot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _draw_box(self, parent_node, quartiles, outliers, box_index, metadata):\n        width = (self.view.x(1) - self.view.x(0)) / self._order\n        series_margin = width * self._series_margin\n        left_edge = self.view.x(0) + width * box_index + series_margin\n        width -= 2 * series_margin\n\n        # draw lines for whiskers - bottom, median, and top\n        for i, whisker in enumerate((quartiles[0], quartiles[2],\n                                     quartiles[4])):\n            whisker_width = width if i == 1 else width / 2\n            shift = (width - whisker_width) / 2\n            xs = left_edge + shift\n            xe = left_edge + width - shift\n            alter(\n                self.svg.line(\n                    parent_node,\n                    coords=[(xs, self.view.y(whisker)),\n                            (xe, self.view.y(whisker))],\n                    class_='reactive tooltip-trigger',\n                    attrib={'stroke-width': 3}\n                ), metadata\n            )\n\n        # draw lines connecting whiskers to box (Q1 and Q3)\n        alter(\n            self.svg.line(\n                parent_node,\n                coords=[(left_edge + width / 2, self.view.y(quartiles[0])),\n                        (left_edge + width / 2, self.view.y(quartiles[1]))],\n                class_='reactive tooltip-trigger',\n                attrib={'stroke-width': 2}\n            ), metadata\n        )\n        alter(\n            self.svg.line(\n                parent_node,\n                coords=[(left_edge + width / 2, self.view.y(quartiles[4])),\n                        (left_edge + width / 2, self.view.y(quartiles[3]))],\n                class_='reactive tooltip-trigger',\n                attrib={'stroke-width': 2}\n            ), metadata\n        )\n\n        # box, bounded by Q1 and Q3\n        alter(\n            self.svg.node(\n                parent_node,\n                tag='rect',\n                x=left_edge,\n                y=self.view.y(quartiles[1]),\n                height=self.view.y(quartiles[3]) - self.view.y(quartiles[1]),\n                width=width,\n                class_='subtle-fill reactive tooltip-trigger'\n            ), metadata\n        )\n\n        # draw outliers\n        for o in outliers:\n            alter(\n                self.svg.node(\n                    parent_node,\n                    tag='circle',\n                    cx=left_edge + width / 2,\n                    cy=self.view.y(o),\n                    r=3,\n                    class_='subtle-fill reactive tooltip-trigger'\n                ), metadata\n            )\n\n        return (\n            left_edge + width / 2,\n            self.view.y(sum(quartiles) / len(quartiles))\n        )", "response": "Draw a box plot on the main svg."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _box_points(values, mode='extremes'):\n\n        def median(seq):\n            n = len(seq)\n            if n % 2 == 0:  # seq has an even length\n                return (seq[n // 2] + seq[n // 2 - 1]) / 2\n            else:  # seq has an odd length\n                return seq[n // 2]\n\n        def mean(seq):\n            return sum(seq) / len(seq)\n\n        def stdev(seq):\n            m = mean(seq)\n            l = len(seq)\n            v = sum((n - m)**2 for n in seq) / (l - 1)  # variance\n            return v**0.5  # sqrt\n\n        def pstdev(seq):\n            m = mean(seq)\n            l = len(seq)\n            v = sum((n - m)**2 for n in seq) / l  # variance\n            return v**0.5  # sqrt\n\n        outliers = []\n        # sort the copy in case the originals must stay in original order\n        s = sorted([x for x in values if x is not None])\n        n = len(s)\n        if not n:\n            return (0, 0, 0, 0, 0, 0, 0), []\n        elif n == 1:\n            return (s[0], s[0], s[0], s[0], s[0], s[0], s[0]), []\n        else:\n            q2 = median(s)\n            # See 'Method 3' in http://en.wikipedia.org/wiki/Quartile\n            if n % 2 == 0:  # even\n                q1 = median(s[:n // 2])\n                q3 = median(s[n // 2:])\n            else:  # odd\n                if n == 1:  # special case\n                    q1 = s[0]\n                    q3 = s[0]\n                elif n % 4 == 1:  # n is of form 4n + 1 where n >= 1\n                    m = (n - 1) // 4\n                    q1 = 0.25 * s[m - 1] + 0.75 * s[m]\n                    q3 = 0.75 * s[3 * m] + 0.25 * s[3 * m + 1]\n                else:  # n is of form 4n + 3 where n >= 1\n                    m = (n - 3) // 4\n                    q1 = 0.75 * s[m] + 0.25 * s[m + 1]\n                    q3 = 0.25 * s[3 * m + 1] + 0.75 * s[3 * m + 2]\n\n            iqr = q3 - q1\n            min_s = s[0]\n            max_s = s[-1]\n            if mode == 'extremes':\n                q0 = min_s\n                q4 = max_s\n            elif mode == 'tukey':\n                # the lowest datum still within 1.5 IQR of the lower quartile,\n                # and the highest datum still within 1.5 IQR of the upper\n                # quartile [Tukey box plot, Wikipedia ]\n                b0 = bisect_left(s, q1 - 1.5 * iqr)\n                b4 = bisect_right(s, q3 + 1.5 * iqr)\n                q0 = s[b0]\n                q4 = s[b4 - 1]\n                outliers = s[:b0] + s[b4:]\n            elif mode == 'stdev':\n                # one standard deviation above and below the mean of the data\n                sd = stdev(s)\n                b0 = bisect_left(s, q2 - sd)\n                b4 = bisect_right(s, q2 + sd)\n                q0 = s[b0]\n                q4 = s[b4 - 1]\n                outliers = s[:b0] + s[b4:]\n            elif mode == 'pstdev':\n                # one population standard deviation above and below\n                # the mean of the data\n                sdp = pstdev(s)\n                b0 = bisect_left(s, q2 - sdp)\n                b4 = bisect_right(s, q2 + sdp)\n                q0 = s[b0]\n                q4 = s[b4 - 1]\n                outliers = s[:b0] + s[b4:]\n            elif mode == '1.5IQR':\n                # 1.5IQR mode\n                q0 = q1 - 1.5 * iqr\n                q4 = q3 + 1.5 * iqr\n            return (min_s, q0, q1, q2, q3, q4, max_s), outliers", "response": "Return a list of 7 - tuples of minimum maximum minimum and maximum for a list of numeric values."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _axes(self):\n        self.view._force_vertical = True\n        super(HorizontalGraph, self)._axes()\n        self.view._force_vertical = False", "response": "Set the _force_vertical flag when rendering axes"}
