{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prior_to_xarray(self):\n        data = self.prior\n        if not isinstance(data, dict):\n            raise TypeError(\"DictConverter.prior is not a dictionary\")\n\n        return dict_to_dataset(data, library=None, coords=self.coords, dims=self.dims)", "response": "Convert prior samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sample_stats_prior_to_xarray(self):\n        data = self.sample_stats_prior\n        if not isinstance(data, dict):\n            raise TypeError(\"DictConverter.sample_stats_prior is not a dictionary\")\n\n        return dict_to_dataset(data, library=None, coords=self.coords, dims=self.dims)", "response": "Convert sample_stats_prior samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prior_predictive_to_xarray(self):\n        data = self.prior_predictive\n        if not isinstance(data, dict):\n            raise TypeError(\"DictConverter.prior_predictive is not a dictionary\")\n\n        return dict_to_dataset(data, library=None, coords=self.coords, dims=self.dims)", "response": "Convert prior_predictive samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts observed_data to xarray.", "response": "def observed_data_to_xarray(self):\n        \"\"\"Convert observed_data to xarray.\"\"\"\n        data = self.observed_data\n        if not isinstance(data, dict):\n            raise TypeError(\"DictConverter.observed_data is not a dictionary\")\n        if self.dims is None:\n            dims = {}\n        else:\n            dims = self.dims\n        observed_data = dict()\n        for key, vals in data.items():\n            vals = np.atleast_1d(vals)\n            val_dims = dims.get(key)\n            val_dims, coords = generate_dims_coords(\n                vals.shape, key, dims=val_dims, coords=self.coords\n            )\n            observed_data[key] = xr.DataArray(vals, dims=val_dims, coords=coords)\n        return xr.Dataset(data_vars=observed_data, attrs=make_attrs(library=None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a summary plot of the comparison of two tables.", "response": "def plot_compare(\r\n    comp_df,\r\n    insample_dev=True,\r\n    plot_standard_error=True,\r\n    plot_ic_diff=True,\r\n    figsize=None,\r\n    textsize=None,\r\n    plot_kwargs=None,\r\n    ax=None,\r\n):\r\n    \"\"\"\r\n    Summary plot for model comparison.\r\n\r\n    This plot is in the style of the one used in the book Statistical Rethinking (Chapter 6)\r\n    by Richard McElreath.\r\n\r\n    Notes\r\n    -----\r\n    Defaults to comparing Widely Accepted Information Criterion (WAIC) if present in comp_df column,\r\n    otherwise compares Leave-one-out (loo)\r\n\r\n\r\n    Parameters\r\n    ----------\r\n    comp_df: pd.DataFrame\r\n        Result of the `az.compare()` method\r\n    insample_dev : bool, optional\r\n        Plot in-sample deviance, that is the value of the information criteria without the\r\n        penalization given by the effective number of parameters (pIC). Defaults to True\r\n    plot_standard_error : bool, optional\r\n        Plot the standard error of the information criteria estimate. Defaults to True\r\n    plot_ic_diff : bool, optional\r\n        Plot standard error of the difference in information criteria between each model\r\n         and the top-ranked model. Defaults to True\r\n    figsize : tuple, optional\r\n        If None, size is (6, num of models) inches\r\n    textsize: float\r\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\r\n        on figsize.\r\n    plot_kwargs : dict, optional\r\n        Optional arguments for plot elements. Currently accepts 'color_ic',\r\n        'marker_ic', 'color_insample_dev', 'marker_insample_dev', 'color_dse',\r\n        'marker_dse', 'ls_min_ic' 'color_ls_min_ic',  'fontsize'\r\n    ax : axes, optional\r\n        Matplotlib axes\r\n\r\n    Returns\r\n    -------\r\n    ax : matplotlib axes\r\n\r\n\r\n    Examples\r\n    --------\r\n    Show default compare plot\r\n\r\n    .. plot::\r\n        :context: close-figs\r\n\r\n        >>> import arviz as az\r\n        >>> model_compare = az.compare({'Centered 8 schools': az.load_arviz_data('centered_eight'),\r\n        >>>                  'Non-centered 8 schools': az.load_arviz_data('non_centered_eight')})\r\n        >>> az.plot_compare(model_compare)\r\n\r\n    Plot standard error and information criteria difference only\r\n\r\n    .. plot::\r\n        :context: close-figs\r\n\r\n        >>> az.plot_compare(model_compare, insample_dev=False)\r\n\r\n    \"\"\"\r\n    if figsize is None:\r\n        figsize = (6, len(comp_df))\r\n\r\n    figsize, ax_labelsize, _, xt_labelsize, linewidth, _ = _scale_fig_size(figsize, textsize, 1, 1)\r\n\r\n    if ax is None:\r\n        _, ax = plt.subplots(figsize=figsize, constrained_layout=True)\r\n\r\n    if plot_kwargs is None:\r\n        plot_kwargs = {}\r\n\r\n    yticks_pos, step = np.linspace(0, -1, (comp_df.shape[0] * 2) - 1, retstep=True)\r\n    yticks_pos[1::2] = yticks_pos[1::2] + step / 2\r\n\r\n    yticks_labels = [\"\"] * len(yticks_pos)\r\n\r\n    _information_criterion = [\"waic\", \"loo\"]\r\n    for information_criterion in _information_criterion:\r\n        if information_criterion in comp_df.columns:\r\n            break\r\n    else:\r\n        raise ValueError(\r\n            \"comp_df must contain one of the following\"\r\n            \" information criterion: {}\".format(_information_criterion)\r\n        )\r\n\r\n    if plot_ic_diff:\r\n        yticks_labels[0] = comp_df.index[0]\r\n        yticks_labels[2::2] = comp_df.index[1:]\r\n        ax.set_yticks(yticks_pos)\r\n        ax.errorbar(\r\n            x=comp_df[information_criterion].iloc[1:],\r\n            y=yticks_pos[1::2],\r\n            xerr=comp_df.dse[1:],\r\n            color=plot_kwargs.get(\"color_dse\", \"grey\"),\r\n            fmt=plot_kwargs.get(\"marker_dse\", \"^\"),\r\n            mew=linewidth,\r\n            elinewidth=linewidth,\r\n        )\r\n\r\n    else:\r\n        yticks_labels = comp_df.index\r\n        ax.set_yticks(yticks_pos[::2])\r\n\r\n    if plot_standard_error:\r\n        ax.errorbar(\r\n            x=comp_df[information_criterion],\r\n            y=yticks_pos[::2],\r\n            xerr=comp_df.se,\r\n            color=plot_kwargs.get(\"color_ic\", \"k\"),\r\n            fmt=plot_kwargs.get(\"marker_ic\", \"o\"),\r\n            mfc=\"None\",\r\n            mew=linewidth,\r\n            lw=linewidth,\r\n        )\r\n    else:\r\n        ax.plot(\r\n            comp_df[information_criterion],\r\n            yticks_pos[::2],\r\n            color=plot_kwargs.get(\"color_ic\", \"k\"),\r\n            marker=plot_kwargs.get(\"marker_ic\", \"o\"),\r\n            mfc=\"None\",\r\n            mew=linewidth,\r\n            lw=0,\r\n        )\r\n\r\n    if insample_dev:\r\n        ax.plot(\r\n            comp_df[information_criterion] - (2 * comp_df[\"p_\" + information_criterion]),\r\n            yticks_pos[::2],\r\n            color=plot_kwargs.get(\"color_insample_dev\", \"k\"),\r\n            marker=plot_kwargs.get(\"marker_insample_dev\", \"o\"),\r\n            mew=linewidth,\r\n            lw=0,\r\n        )\r\n\r\n    ax.axvline(\r\n        comp_df[information_criterion].iloc[0],\r\n        ls=plot_kwargs.get(\"ls_min_ic\", \"--\"),\r\n        color=plot_kwargs.get(\"color_ls_min_ic\", \"grey\"),\r\n        lw=linewidth,\r\n    )\r\n\r\n    scale_col = information_criterion + \"_scale\"\r\n    if scale_col in comp_df:\r\n        scale = comp_df[scale_col].iloc[0].capitalize()\r\n    else:\r\n        scale = \"Deviance\"\r\n    ax.set_xlabel(scale, fontsize=ax_labelsize)\r\n    ax.set_yticklabels(yticks_labels)\r\n    ax.set_ylim(-1 + step, 0 - step)\r\n    ax.tick_params(labelsize=xt_labelsize)\r\n\r\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef plot_parallel(\n    data,\n    var_names=None,\n    coords=None,\n    figsize=None,\n    textsize=None,\n    legend=True,\n    colornd=\"k\",\n    colord=\"C1\",\n    shadend=0.025,\n    ax=None,\n    norm_method=None,\n):\n    \"\"\"\n    Plot parallel coordinates plot showing posterior points with and without divergences.\n\n    Described by https://arxiv.org/abs/1709.01449, suggested by Ari Hartikainen\n\n    Parameters\n    ----------\n    data : obj\n        Any object that can be converted to an az.InferenceData object\n        Refer to documentation of az.convert_to_dataset for details\n    var_names : list of variable names\n        Variables to be plotted, if None all variable are plotted. Can be used to change the order\n        of the plotted variables\n    coords : mapping, optional\n        Coordinates of var_names to be plotted. Passed to `Dataset.sel`\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    legend : bool\n        Flag for plotting legend (defaults to True)\n    colornd : valid matplotlib color\n        color for non-divergent points. Defaults to 'k'\n    colord : valid matplotlib color\n        color for divergent points. Defaults to 'C1'\n    shadend : float\n        Alpha blending value for non-divergent points, between 0 (invisible) and 1 (opaque).\n        Defaults to .025\n    ax : axes\n        Matplotlib axes.\n    norm_method : str\n        Method for normalizing the data. Methods include normal, minmax and rank.\n        Defaults to none.\n\n    Returns\n    -------\n    ax : matplotlib axes\n\n    Examples\n    --------\n    Plot default parallel plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_parallel(data, var_names=[\"mu\", \"tau\"])\n\n\n    Plot parallel plot with normalization\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_parallel(data, var_names=[\"mu\", \"tau\"], norm_method='normal')\n\n    \"\"\"\n    if coords is None:\n        coords = {}\n\n    # Get diverging draws and combine chains\n    divergent_data = convert_to_dataset(data, group=\"sample_stats\")\n    _, diverging_mask = xarray_to_ndarray(divergent_data, var_names=(\"diverging\",), combined=True)\n    diverging_mask = np.squeeze(diverging_mask)\n\n    # Get posterior draws and combine chains\n    posterior_data = convert_to_dataset(data, group=\"posterior\")\n    var_names = _var_names(var_names, posterior_data)\n    var_names, _posterior = xarray_to_ndarray(\n        get_coords(posterior_data, coords), var_names=var_names, combined=True\n    )\n    if len(var_names) < 2:\n        raise ValueError(\"This plot needs at least two variables\")\n    if norm_method is not None:\n        if norm_method == \"normal\":\n            mean = np.mean(_posterior, axis=1)\n            standard_deviation = np.std(_posterior, axis=1)\n            for i in range(0, np.shape(mean)[0]):\n                _posterior[i, :] = (_posterior[i, :] - mean[i]) / standard_deviation[i]\n        elif norm_method == \"minmax\":\n            min_elem = np.min(_posterior, axis=1)\n            max_elem = np.max(_posterior, axis=1)\n            for i in range(0, np.shape(min_elem)[0]):\n                _posterior[i, :] = ((_posterior[i, :]) - min_elem[i]) / (max_elem[i] - min_elem[i])\n        elif norm_method == \"rank\":\n            _posterior = rankdata(_posterior, axis=1)\n        else:\n            raise ValueError(\"{} is not supported. Use normal, minmax or rank.\".format(norm_method))\n\n    figsize, _, _, xt_labelsize, _, _ = _scale_fig_size(figsize, textsize, 1, 1)\n\n    if ax is None:\n        _, ax = plt.subplots(figsize=figsize, constrained_layout=True)\n\n    ax.plot(_posterior[:, ~diverging_mask], color=colornd, alpha=shadend)\n\n    if np.any(diverging_mask):\n        ax.plot(_posterior[:, diverging_mask], color=colord, lw=1)\n\n    ax.tick_params(labelsize=textsize)\n    ax.set_xticks(range(len(var_names)))\n    ax.set_xticklabels(var_names)\n\n    if legend:\n        ax.plot([], color=colornd, label=\"non-divergent\")\n        if np.any(diverging_mask):\n            ax.plot([], color=colord, label=\"divergent\")\n        ax.legend(fontsize=xt_labelsize)\n\n    return ax", "response": "Plots the posterior points with and without divergences."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts sampling information from the configuration comments.", "response": "def _process_configuration(comments):\n    \"\"\"Extract sampling information.\"\"\"\n    num_samples = None\n    num_warmup = None\n    save_warmup = None\n    for comment in comments:\n        comment = comment.strip(\"#\").strip()\n        if comment.startswith(\"num_samples\"):\n            num_samples = int(comment.strip(\"num_samples = \").strip(\"(Default)\"))\n        elif comment.startswith(\"num_warmup\"):\n            num_warmup = int(comment.strip(\"num_warmup = \").strip(\"(Default)\"))\n        elif comment.startswith(\"save_warmup\"):\n            save_warmup = bool(int(comment.strip(\"save_warmup = \").strip(\"(Default)\")))\n        elif comment.startswith(\"thin\"):\n            thin = int(comment.strip(\"thin = \").strip(\"(Default)\"))\n\n    return {\n        \"num_samples\": num_samples,\n        \"num_warmup\": num_warmup,\n        \"save_warmup\": save_warmup,\n        \"thin\": thin,\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _read_output(path):\n    chains = []\n    configuration_info = []\n    adaptation_info = []\n    timing_info = []\n    i = 0\n    # Read (first) configuration and adaption\n    with open(path, \"r\") as f_obj:\n        column_names = False\n        for i, line in enumerate(f_obj):\n            line = line.strip()\n            if line.startswith(\"#\"):\n                if column_names:\n                    adaptation_info.append(line.strip())\n                else:\n                    configuration_info.append(line.strip())\n            elif not column_names:\n                column_names = True\n                pconf = _process_configuration(configuration_info)\n                if pconf[\"save_warmup\"]:\n                    warmup_range = range(pconf[\"num_warmup\"] // pconf[\"thin\"])\n                    for _, _ in zip(warmup_range, f_obj):\n                        continue\n            else:\n                break\n\n    # Read data\n    with open(path, \"r\") as f_obj:\n        df = pd.read_csv(f_obj, comment=\"#\")\n\n    # split dataframe if header found multiple times\n    if df.iloc[:, 0].dtype.kind == \"O\":\n        first_col = df.columns[0]\n        col_locations = first_col == df.loc[:, first_col]\n        col_locations = list(col_locations.loc[col_locations].index)\n        dfs = []\n        for idx, last_idx in zip(col_locations, [-1] + list(col_locations[:-1])):\n            df_ = deepcopy(df.loc[last_idx + 1 : idx - 1, :])\n            for col in df_.columns:\n                df_.loc[:, col] = pd.to_numeric(df_.loc[:, col])\n            if len(df_):\n                dfs.append(df_.reset_index(drop=True))\n            df = df.loc[idx + 1 :, :]\n        for col in df.columns:\n            df.loc[:, col] = pd.to_numeric(df.loc[:, col])\n        dfs.append(df)\n    else:\n        dfs = [df]\n\n    for j, df in enumerate(dfs):\n        if j == 0:\n            # Read timing info (first) from the end of the file\n            line_num = i + df.shape[0] + 1\n            for k in range(5):\n                line = linecache.getline(path, line_num + k).strip()\n                if len(line):\n                    timing_info.append(line)\n            configuration_info_len = len(configuration_info)\n            adaptation_info_len = len(adaptation_info)\n            timing_info_len = len(timing_info)\n            num_of_samples = df.shape[0]\n            header_count = 1\n            last_line_num = (\n                configuration_info_len\n                + adaptation_info_len\n                + timing_info_len\n                + num_of_samples\n                + header_count\n            )\n        else:\n            # header location found in the dataframe (not first)\n            configuration_info = []\n            adaptation_info = []\n            timing_info = []\n\n            # line number for the next dataframe in csv\n            line_num = last_line_num + 1\n\n            # row ranges\n            config_start = line_num\n            config_end = config_start + configuration_info_len\n\n            # read configuration_info\n            for reading_line in range(config_start, config_end):\n                line = linecache.getline(path, reading_line)\n                if line.startswith(\"#\"):\n                    configuration_info.append(line)\n                else:\n                    msg = (\n                        \"Invalid input file. \"\n                        \"Header information missing from combined csv. \"\n                        \"Configuration: {}\".format(path)\n                    )\n                    raise ValueError(msg)\n\n            pconf = _process_configuration(configuration_info)\n            warmup_rows = pconf[\"save_warmup\"] * pconf[\"num_warmup\"] // pconf[\"thin\"]\n            adaption_start = config_end + 1 + warmup_rows\n            adaption_end = adaption_start + adaptation_info_len\n            # read adaptation_info\n            for reading_line in range(adaption_start, adaption_end):\n                line = linecache.getline(path, reading_line)\n                if line.startswith(\"#\"):\n                    adaptation_info.append(line)\n                else:\n                    msg = (\n                        \"Invalid input file. \"\n                        \"Header information missing from combined csv. \"\n                        \"Adaptation: {}\".format(path)\n                    )\n                    raise ValueError(msg)\n\n            timing_start = adaption_end + len(df) - warmup_rows\n            timing_end = timing_start + timing_info_len\n            # read timing_info\n            raise_timing_error = False\n            for reading_line in range(timing_start, timing_end):\n                line = linecache.getline(path, reading_line)\n                if line.startswith(\"#\"):\n                    timing_info.append(line)\n                else:\n                    raise_timing_error = True\n                    break\n            no_elapsed_time = not any(\"elapsed time\" in row.lower() for row in timing_info)\n            if raise_timing_error or no_elapsed_time:\n                msg = (\n                    \"Invalid input file. \"\n                    \"Header information missing from combined csv. \"\n                    \"Timing: {}\".format(path)\n                )\n                raise ValueError(msg)\n\n            last_line_num = reading_line\n\n        # Remove warmup\n        if pconf[\"save_warmup\"]:\n            saved_samples = pconf[\"num_samples\"] // pconf[\"thin\"]\n            df = df.iloc[-saved_samples:, :]\n\n        # Split data to sample_stats and sample\n        sample_stats_columns = [col for col in df.columns if col.endswith(\"__\")]\n        sample_columns = [col for col in df.columns if col not in sample_stats_columns]\n\n        sample_stats = df.loc[:, sample_stats_columns]\n        sample_df = df.loc[:, sample_columns]\n\n        chains.append((sample_df, sample_stats, configuration_info, adaptation_info, timing_info))\n\n    return chains", "response": "Read CmdStan output. csv.\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransform datastring to key values pair.", "response": "def _process_data_var(string):\n    \"\"\"Transform datastring to key, values pair.\n\n    All values are transformed to floating point values.\n\n    Parameters\n    ----------\n    string : str\n\n    Returns\n    -------\n    Tuple[Str, Str]\n        key, values pair\n    \"\"\"\n    key, var = string.split(\"<-\")\n    if \"structure\" in var:\n        var, dim = var.replace(\"structure(\", \"\").replace(\",\", \"\").split(\".Dim\")\n        # dtype = int if '.' not in var and 'e' not in var.lower() else float\n        dtype = float\n        var = var.replace(\"c(\", \"\").replace(\")\", \"\").strip().split()\n        dim = dim.replace(\"=\", \"\").replace(\"c(\", \"\").replace(\")\", \"\").strip().split()\n        dim = tuple(map(int, dim))\n        var = np.fromiter(map(dtype, var), dtype).reshape(dim, order=\"F\")\n    elif \"c(\" in var:\n        # dtype = int if '.' not in var and 'e' not in var.lower() else float\n        dtype = float\n        var = var.replace(\"c(\", \"\").replace(\")\", \"\").split(\",\")\n        var = np.fromiter(map(dtype, var), dtype)\n    else:\n        # dtype = int if '.' not in var and 'e' not in var.lower() else float\n        dtype = float\n        var = dtype(var)\n    return key.strip(), var"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _read_data(path):\n    data = {}\n    with open(path, \"r\") as f_obj:\n        var = \"\"\n        for line in f_obj:\n            if \"<-\" in line:\n                if len(var):\n                    key, var = _process_data_var(var)\n                    data[key] = var\n                var = \"\"\n            var += \" \" + line.strip()\n        if len(var):\n            key, var = _process_data_var(var)\n            data[key] = var\n    return data", "response": "Read Rdump formatted data and transform to Python dictionary."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _unpack_dataframes(dfs):\n    col_groups = defaultdict(list)\n    columns = dfs[0].columns\n    for col in columns:\n        key, *loc = col.split(\".\")\n        loc = tuple(int(i) - 1 for i in loc)\n        col_groups[key].append((col, loc))\n\n    chains = len(dfs)\n    draws = len(dfs[0])\n    sample = {}\n    for key, cols_locs in col_groups.items():\n        ndim = np.array([loc for _, loc in cols_locs]).max(0) + 1\n        sample[key] = np.full((chains, draws, *ndim), np.nan)\n        for col, loc in cols_locs:\n            for chain_id, df in enumerate(dfs):\n                draw = df[col].values\n                if loc == ():\n                    sample[key][chain_id, :] = draw\n                else:\n                    axis1_all = range(sample[key].shape[1])\n                    slicer = (chain_id, axis1_all, *loc)\n                    sample[key][slicer] = draw\n    return sample", "response": "Transform a list of pandas. DataFrames to dictionary containing ndarrays."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert CmdStan data into an InferenceData object.", "response": "def from_cmdstan(\n    posterior=None,\n    *,\n    posterior_predictive=None,\n    prior=None,\n    prior_predictive=None,\n    observed_data=None,\n    observed_data_var=None,\n    log_likelihood=None,\n    coords=None,\n    dims=None\n):\n    \"\"\"Convert CmdStan data into an InferenceData object.\n\n    Parameters\n    ----------\n    posterior : List[str]\n        List of paths to output.csv files.\n        CSV file can be stacked csv containing all the chains\n\n            cat output*.csv > combined_output.csv\n\n    posterior_predictive : str, List[Str]\n        Posterior predictive samples for the fit. If endswith \".csv\" assumes file.\n    prior : List[str]\n        List of paths to output.csv files\n        CSV file can be stacked csv containing all the chains.\n\n            cat output*.csv > combined_output.csv\n\n    prior_predictive : str, List[Str]\n        Prior predictive samples for the fit. If endswith \".csv\" assumes file.\n    observed_data : str\n        Observed data used in the sampling. Path to data file in Rdump format.\n    observed_data_var : str, List[str]\n        Variable(s) used for slicing observed_data. If not defined, all\n        data variables are imported.\n    log_likelihood : str\n        Pointwise log_likelihood for the data.\n    coords : dict[str, iterable]\n        A dictionary containing the values that are used as index. The key\n        is the name of the dimension, the values are the index values.\n    dims : dict[str, List(str)]\n        A mapping from variables to a list of coordinate names for the variable.\n\n    Returns\n    -------\n    InferenceData object\n    \"\"\"\n    return CmdStanConverter(\n        posterior=posterior,\n        posterior_predictive=posterior_predictive,\n        prior=prior,\n        prior_predictive=prior_predictive,\n        observed_data=observed_data,\n        observed_data_var=observed_data_var,\n        log_likelihood=log_likelihood,\n        coords=coords,\n        dims=dims,\n    ).to_inference_data()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _parse_posterior(self):\n        paths = self.posterior_\n        if isinstance(paths, str):\n            paths = [paths]\n        chain_data = []\n        for path in paths:\n            parsed_output = _read_output(path)\n            for sample, sample_stats, config, adaptation, timing in parsed_output:\n                chain_data.append(\n                    {\n                        \"sample\": sample,\n                        \"sample_stats\": sample_stats,\n                        \"configuration_info\": config,\n                        \"adaptation_info\": adaptation,\n                        \"timing_info\": timing,\n                    }\n                )\n        self.posterior = [item[\"sample\"] for item in chain_data]\n        self.sample_stats = [item[\"sample_stats\"] for item in chain_data]", "response": "Read csv paths to list of dataframes."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads CSV files to list of dataframes.", "response": "def _parse_prior(self):\n        \"\"\"Read csv paths to list of dataframes.\"\"\"\n        paths = self.prior_\n        if isinstance(paths, str):\n            paths = [paths]\n        chain_data = []\n        for path in paths:\n            parsed_output = _read_output(path)\n            for sample, sample_stats, config, adaptation, timing in parsed_output:\n                chain_data.append(\n                    {\n                        \"sample\": sample,\n                        \"sample_stats\": sample_stats,\n                        \"configuration_info\": config,\n                        \"adaptation_info\": adaptation,\n                        \"timing_info\": timing,\n                    }\n                )\n        self.prior = [item[\"sample\"] for item in chain_data]\n        self.sample_stats_prior = [item[\"sample_stats\"] for item in chain_data]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef posterior_to_xarray(self):\n        columns = self.posterior[0].columns\n\n        # filter posterior_predictive and log_likelihood\n        posterior_predictive = self.posterior_predictive\n        if posterior_predictive is None or (\n            isinstance(posterior_predictive, str) and posterior_predictive.lower().endswith(\".csv\")\n        ):\n            posterior_predictive = []\n        elif isinstance(posterior_predictive, str):\n            posterior_predictive = [\n                col for col in columns if posterior_predictive == col.split(\".\")[0]\n            ]\n        else:\n            posterior_predictive = [\n                col\n                for col in columns\n                if any(item == col.split(\".\")[0] for item in posterior_predictive)\n            ]\n\n        log_likelihood = self.log_likelihood\n        if log_likelihood is None:\n            log_likelihood = []\n        else:\n            log_likelihood = [col for col in columns if log_likelihood == col.split(\".\")[0]]\n\n        invalid_cols = posterior_predictive + log_likelihood\n        valid_cols = [col for col in columns if col not in invalid_cols]\n        data = _unpack_dataframes([item[valid_cols] for item in self.posterior])\n        return dict_to_dataset(data, coords=self.coords, dims=self.dims)", "response": "Extract posterior samples from output csv."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sample_stats_to_xarray(self):\n        dtypes = {\"divergent__\": bool, \"n_leapfrog__\": np.int64, \"treedepth__\": np.int64}\n\n        # copy dims and coords\n        dims = deepcopy(self.dims) if self.dims is not None else {}\n        coords = deepcopy(self.coords) if self.coords is not None else {}\n\n        sampler_params = self.sample_stats\n        log_likelihood = self.log_likelihood\n        if isinstance(log_likelihood, str):\n            log_likelihood_cols = [\n                col for col in self.posterior[0].columns if log_likelihood == col.split(\".\")[0]\n            ]\n            log_likelihood_vals = [item[log_likelihood_cols] for item in self.posterior]\n\n            # Add log_likelihood to sampler_params\n            for i, _ in enumerate(sampler_params):\n                # slice log_likelihood to keep dimensions\n                for col in log_likelihood_cols:\n                    col_ll = col.replace(log_likelihood, \"log_likelihood\")\n                    sampler_params[i][col_ll] = log_likelihood_vals[i][col]\n\n            # change dims and coords for log_likelihood if defined\n            if log_likelihood in dims:\n                dims[\"log_likelihood\"] = dims.pop(log_likelihood)\n\n            log_likelihood_dims = np.array(\n                [list(map(int, col.split(\".\")[1:])) for col in log_likelihood_cols]\n            )\n            max_dims = log_likelihood_dims.max(0)\n            max_dims = max_dims if hasattr(max_dims, \"__iter__\") else (max_dims,)\n            default_dim_names, _ = generate_dims_coords(shape=max_dims, var_name=log_likelihood)\n            log_likelihood_dim_names, _ = generate_dims_coords(\n                shape=max_dims, var_name=\"log_likelihood\"\n            )\n            for default_dim_name, log_likelihood_dim_name in zip(\n                default_dim_names, log_likelihood_dim_names\n            ):\n                if default_dim_name in coords:\n                    coords[log_likelihood_dim_name] = coords.pop(default_dim_name)\n\n        for j, s_params in enumerate(sampler_params):\n            rename_dict = {}\n            for key in s_params:\n                key_, *end = key.split(\".\")\n                name = re.sub(\"__$\", \"\", key_)\n                name = \"diverging\" if name == \"divergent\" else name\n                rename_dict[key] = \".\".join((name, *end))\n                sampler_params[j][key] = s_params[key].astype(dtypes.get(key))\n            sampler_params[j] = sampler_params[j].rename(columns=rename_dict)\n        data = _unpack_dataframes(sampler_params)\n        return dict_to_dataset(data, coords=coords, dims=dims)", "response": "Extract sample_stats from fit."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef posterior_predictive_to_xarray(self):\n        posterior_predictive = self.posterior_predictive\n        columns = self.posterior[0].columns\n        if (\n            isinstance(posterior_predictive, (tuple, list))\n            and posterior_predictive[0].endswith(\".csv\")\n        ) or (isinstance(posterior_predictive, str) and posterior_predictive.endswith(\".csv\")):\n            if isinstance(posterior_predictive, str):\n                posterior_predictive = [posterior_predictive]\n            chain_data = []\n            for path in posterior_predictive:\n                parsed_output = _read_output(path)\n                for sample, *_ in parsed_output:\n                    chain_data.append(sample)\n            data = _unpack_dataframes(chain_data)\n        else:\n            if isinstance(posterior_predictive, str):\n                posterior_predictive = [posterior_predictive]\n            posterior_predictive_cols = [\n                col\n                for col in columns\n                if any(item == col.split(\".\")[0] for item in posterior_predictive)\n            ]\n            data = _unpack_dataframes([item[posterior_predictive_cols] for item in self.posterior])\n        return dict_to_dataset(data, coords=self.coords, dims=self.dims)", "response": "Convert posterior_predictive samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prior_to_xarray(self):\n        # filter prior_predictive\n        prior_predictive = self.prior_predictive\n        columns = self.prior[0].columns\n        if prior_predictive is None or (\n            isinstance(prior_predictive, str) and prior_predictive.lower().endswith(\".csv\")\n        ):\n            prior_predictive = []\n        elif isinstance(prior_predictive, str):\n            prior_predictive = [col for col in columns if prior_predictive == col.split(\".\")[0]]\n        else:\n            prior_predictive = [\n                col\n                for col in columns\n                if any(item == col.split(\".\")[0] for item in prior_predictive)\n            ]\n\n        invalid_cols = prior_predictive\n        valid_cols = [col for col in columns if col not in invalid_cols]\n        data = _unpack_dataframes([item[valid_cols] for item in self.prior])\n        return dict_to_dataset(data, coords=self.coords, dims=self.dims)", "response": "Convert prior samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sample_stats_prior_to_xarray(self):\n        dtypes = {\"divergent__\": bool, \"n_leapfrog__\": np.int64, \"treedepth__\": np.int64}\n\n        # copy dims and coords\n        dims = deepcopy(self.dims) if self.dims is not None else {}\n        coords = deepcopy(self.coords) if self.coords is not None else {}\n\n        sampler_params = self.sample_stats_prior\n        for j, s_params in enumerate(sampler_params):\n            rename_dict = {}\n            for key in s_params:\n                key_, *end = key.split(\".\")\n                name = re.sub(\"__$\", \"\", key_)\n                name = \"diverging\" if name == \"divergent\" else name\n                rename_dict[key] = \".\".join((name, *end))\n                sampler_params[j][key] = s_params[key].astype(dtypes.get(key))\n            sampler_params[j] = sampler_params[j].rename(columns=rename_dict)\n        data = _unpack_dataframes(sampler_params)\n        return dict_to_dataset(data, coords=coords, dims=dims)", "response": "Extract sample_stats from fit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts prior_predictive samples to xarray.", "response": "def prior_predictive_to_xarray(self):\n        \"\"\"Convert prior_predictive samples to xarray.\"\"\"\n        prior_predictive = self.prior_predictive\n\n        if (\n            isinstance(prior_predictive, (tuple, list)) and prior_predictive[0].endswith(\".csv\")\n        ) or (isinstance(prior_predictive, str) and prior_predictive.endswith(\".csv\")):\n            if isinstance(prior_predictive, str):\n                prior_predictive = [prior_predictive]\n            chain_data = []\n            for path in prior_predictive:\n                parsed_output = _read_output(path)\n                for sample, *_ in parsed_output:\n                    chain_data.append(sample)\n            data = _unpack_dataframes(chain_data)\n        else:\n            if isinstance(prior_predictive, str):\n                prior_predictive = [prior_predictive]\n            prior_predictive_cols = [\n                col\n                for col in self.prior[0].columns\n                if any(item == col.split(\".\")[0] for item in prior_predictive)\n            ]\n            data = _unpack_dataframes([item[prior_predictive_cols] for item in self.prior])\n        return dict_to_dataset(data, coords=self.coords, dims=self.dims)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef observed_data_to_xarray(self):\n        observed_data_raw = _read_data(self.observed_data)\n        variables = self.observed_data_var\n        if isinstance(variables, str):\n            variables = [variables]\n        observed_data = {}\n        for key, vals in observed_data_raw.items():\n            if variables is not None and key not in variables:\n                continue\n            vals = np.atleast_1d(vals)\n            val_dims = self.dims.get(key)\n            val_dims, coords = generate_dims_coords(\n                vals.shape, key, dims=val_dims, coords=self.coords\n            )\n            observed_data[key] = xr.DataArray(vals, dims=val_dims, coords=coords)\n        return xr.Dataset(data_vars=observed_data)", "response": "Convert observed data to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_draws(fit, variables=None, ignore=None):\r\n    if ignore is None:\r\n        ignore = []\r\n    if fit.mode == 1:\r\n        msg = \"Model in mode 'test_grad'. Sampling is not conducted.\"\r\n        raise AttributeError(msg)\r\n\r\n    if fit.mode == 2 or fit.sim.get(\"samples\") is None:\r\n        msg = \"Fit doesn't contain samples.\"\r\n        raise AttributeError(msg)\r\n\r\n    dtypes = infer_dtypes(fit)\r\n\r\n    if variables is None:\r\n        variables = fit.sim[\"pars_oi\"]\r\n    elif isinstance(variables, str):\r\n        variables = [variables]\r\n    variables = list(variables)\r\n\r\n    for var, dim in zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]):\r\n        if var in variables and np.prod(dim) == 0:\r\n            del variables[variables.index(var)]\r\n\r\n    ndraws = [s - w for s, w in zip(fit.sim[\"n_save\"], fit.sim[\"warmup2\"])]\r\n    nchain = len(fit.sim[\"samples\"])\r\n\r\n    # check if the values are in 0-based (<=2.17) or 1-based indexing (>=2.18)\r\n    shift = 1\r\n    if any(fit.sim[\"dims_oi\"]):\r\n        # choose variable with lowest number of dims > 1\r\n        par_idx = min((dim, i) for i, dim in enumerate(fit.sim[\"dims_oi\"]) if dim)[1]\r\n        offset = int(sum(map(np.product, fit.sim[\"dims_oi\"][:par_idx])))\r\n        par_offset = int(np.product(fit.sim[\"dims_oi\"][par_idx]))\r\n        par_keys = fit.sim[\"fnames_oi\"][offset : offset + par_offset]\r\n        shift = len(par_keys)\r\n        for item in par_keys:\r\n            _, shape = item.replace(\"]\", \"\").split(\"[\")\r\n            shape_idx_min = min(int(shape_value) for shape_value in shape.split(\",\"))\r\n            if shape_idx_min < shift:\r\n                shift = shape_idx_min\r\n        # If shift is higher than 1, this will probably mean that Stan\r\n        # has implemented sparse structure (saves only non-zero parts),\r\n        # but let's hope that dims are still corresponding the full shape\r\n        shift = int(min(shift, 1))\r\n\r\n    var_keys = OrderedDict((var, []) for var in fit.sim[\"pars_oi\"])\r\n    for key in fit.sim[\"fnames_oi\"]:\r\n        var, *tails = key.split(\"[\")\r\n        loc = [Ellipsis]\r\n        for tail in tails:\r\n            loc = []\r\n            for i in tail[:-1].split(\",\"):\r\n                loc.append(int(i) - shift)\r\n        var_keys[var].append((key, loc))\r\n\r\n    shapes = dict(zip(fit.sim[\"pars_oi\"], fit.sim[\"dims_oi\"]))\r\n\r\n    variables = [var for var in variables if var not in ignore]\r\n\r\n    data = OrderedDict()\r\n\r\n    for var in variables:\r\n        if var in data:\r\n            continue\r\n        keys_locs = var_keys.get(var, [(var, [Ellipsis])])\r\n        shape = shapes.get(var, [])\r\n        dtype = dtypes.get(var)\r\n\r\n        ndraw = max(ndraws)\r\n        ary_shape = [nchain, ndraw] + shape\r\n        ary = np.empty(ary_shape, dtype=dtype, order=\"F\")\r\n        for chain, (pyholder, ndraw) in enumerate(zip(fit.sim[\"samples\"], ndraws)):\r\n            axes = [chain, slice(None)]\r\n            for key, loc in keys_locs:\r\n                ary_slice = tuple(axes + loc)\r\n                ary[ary_slice] = pyholder.chains[key][-ndraw:]\r\n        data[var] = ary\r\n\r\n    return data", "response": "Extract draws from PyStan fit."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_sample_stats(fit, log_likelihood=None):\r\n    dtypes = {\"divergent__\": bool, \"n_leapfrog__\": np.int64, \"treedepth__\": np.int64}\r\n\r\n    ndraws = [s - w for s, w in zip(fit.sim[\"n_save\"], fit.sim[\"warmup2\"])]\r\n\r\n    extraction = OrderedDict()\r\n    for chain, (pyholder, ndraws) in enumerate(zip(fit.sim[\"samples\"], ndraws)):\r\n        if chain == 0:\r\n            for key in pyholder[\"sampler_param_names\"]:\r\n                extraction[key] = []\r\n        for key, values in zip(pyholder[\"sampler_param_names\"], pyholder[\"sampler_params\"]):\r\n            extraction[key].append(values[-ndraws:])\r\n\r\n    data = OrderedDict()\r\n    for key, values in extraction.items():\r\n        values = np.stack(values, axis=0)\r\n        dtype = dtypes.get(key)\r\n        values = values.astype(dtype)\r\n        name = re.sub(\"__$\", \"\", key)\r\n        name = \"diverging\" if name == \"divergent\" else name\r\n        data[name] = values\r\n\r\n    # log_likelihood\r\n    if log_likelihood is not None:\r\n        log_likelihood_data = get_draws(fit, variables=log_likelihood)\r\n        data[\"log_likelihood\"] = log_likelihood_data[log_likelihood]\r\n\r\n    # lp__\r\n    stat_lp = get_draws(fit, variables=\"lp__\")\r\n    data[\"lp\"] = stat_lp[\"lp__\"]\r\n\r\n    return data", "response": "Extract sample stats from PyStan fit."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_draws_stan3(fit, model=None, variables=None, ignore=None):\r\n    if ignore is None:\r\n        ignore = []\r\n\r\n    dtypes = {}\r\n    if model is not None:\r\n        dtypes = infer_dtypes(fit, model)\r\n\r\n    if variables is None:\r\n        variables = fit.param_names\r\n    elif isinstance(variables, str):\r\n        variables = [variables]\r\n    variables = list(variables)\r\n\r\n    data = OrderedDict()\r\n\r\n    for var in variables:\r\n        if var in data:\r\n            continue\r\n        dtype = dtypes.get(var)\r\n\r\n        # in future fix the correct number of draws if fit.save_warmup is True\r\n        new_shape = (*fit.dims[fit.param_names.index(var)], -1, fit.num_chains)\r\n        values = fit._draws[fit._parameter_indexes(var), :]  # pylint: disable=protected-access\r\n        values = values.reshape(new_shape, order=\"F\")\r\n        values = np.moveaxis(values, [-2, -1], [1, 0])\r\n        values = values.astype(dtype)\r\n        data[var] = values\r\n\r\n    return data", "response": "Extract draws from PyStan3 fit."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract sample stats from PyStan3 fit.", "response": "def get_sample_stats_stan3(fit, model=None, log_likelihood=None):\r\n    \"\"\"Extract sample stats from PyStan3 fit.\"\"\"\r\n    dtypes = {\"divergent__\": bool, \"n_leapfrog__\": np.int64, \"treedepth__\": np.int64}\r\n\r\n    data = OrderedDict()\r\n    for key in fit.sample_and_sampler_param_names:\r\n        new_shape = -1, fit.num_chains\r\n        values = fit._draws[fit._parameter_indexes(key)]  # pylint: disable=protected-access\r\n        values = values.reshape(new_shape, order=\"F\")\r\n        values = np.moveaxis(values, [-2, -1], [1, 0])\r\n        dtype = dtypes.get(key)\r\n        values = values.astype(dtype)\r\n        name = re.sub(\"__$\", \"\", key)\r\n        name = \"diverging\" if name == \"divergent\" else name\r\n        data[name] = values\r\n\r\n    # log_likelihood\r\n    if log_likelihood is not None:\r\n        log_likelihood_data = get_draws_stan3(fit, model=model, variables=log_likelihood)\r\n        data[\"log_likelihood\"] = log_likelihood_data[log_likelihood]\r\n\r\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfunctions infer_dtypes from Stan model code.", "response": "def infer_dtypes(fit, model=None):\r\n    \"\"\"Infer dtypes from Stan model code.\r\n\r\n    Function strips out generated quantities block and searchs for `int`\r\n    dtypes after stripping out comments inside the block.\r\n    \"\"\"\r\n    pattern_remove_comments = re.compile(\r\n        r'//.*?$|/\\*.*?\\*/|\\'(?:\\\\.|[^\\\\\\'])*\\'|\"(?:\\\\.|[^\\\\\"])*\"', re.DOTALL | re.MULTILINE\r\n    )\r\n    stan_integer = r\"int\"\r\n    stan_limits = r\"(?:\\<[^\\>]+\\>)*\"  # ignore group: 0 or more <....>\r\n    stan_param = r\"([^;=\\s\\[]+)\"  # capture group: ends= \";\", \"=\", \"[\" or whitespace\r\n    stan_ws = r\"\\s*\"  # 0 or more whitespace\r\n    pattern_int = re.compile(\r\n        \"\".join((stan_integer, stan_ws, stan_limits, stan_ws, stan_param)), re.IGNORECASE\r\n    )\r\n    if model is None:\r\n        stan_code = fit.get_stancode()\r\n        model_pars = fit.model_pars\r\n    else:\r\n        stan_code = model.program_code\r\n        model_pars = fit.param_names\r\n    # remove deprecated comments\r\n    stan_code = \"\\n\".join(\r\n        line if \"#\" not in line else line[: line.find(\"#\")] for line in stan_code.splitlines()\r\n    )\r\n    stan_code = re.sub(pattern_remove_comments, \"\", stan_code)\r\n    stan_code = stan_code.split(\"generated quantities\")[-1]\r\n    dtypes = re.findall(pattern_int, stan_code)\r\n    dtypes = {item.strip(): \"int\" for item in dtypes if item.strip() in model_pars}\r\n    return dtypes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_pystan(\r\n    posterior=None,\r\n    *,\r\n    posterior_predictive=None,\r\n    prior=None,\r\n    prior_predictive=None,\r\n    observed_data=None,\r\n    log_likelihood=None,\r\n    coords=None,\r\n    dims=None,\r\n    posterior_model=None,\r\n    prior_model=None\r\n):\r\n    \"\"\"Convert PyStan data into an InferenceData object.\r\n\r\n    Parameters\r\n    ----------\r\n    posterior : StanFit4Model or stan.fit.Fit\r\n        PyStan fit object for posterior.\r\n    posterior_predictive : str, a list of str\r\n        Posterior predictive samples for the posterior.\r\n    prior : StanFit4Model or stan.fit.Fit\r\n        PyStan fit object for prior.\r\n    prior_predictive : str, a list of str\r\n        Posterior predictive samples for the prior.\r\n    observed_data : str or a list of str\r\n        observed data used in the sampling.\r\n        Observed data is extracted from the `posterior.data`.\r\n        PyStan3 needs model object for the extraction.\r\n        See `posterior_model`.\r\n    log_likelihood : str\r\n        Pointwise log_likelihood for the data.\r\n        log_likelihood is extracted from the posterior.\r\n    coords : dict[str, iterable]\r\n        A dictionary containing the values that are used as index. The key\r\n        is the name of the dimension, the values are the index values.\r\n    dims : dict[str, List(str)]\r\n        A mapping from variables to a list of coordinate names for the variable.\r\n    posterior_model : stan.model.Model\r\n        PyStan3 specific model object. Needed for automatic dtype parsing\r\n        and for the extraction of observed data.\r\n    prior_model : stan.model.Model\r\n        PyStan3 specific model object. Needed for automatic dtype parsing.\r\n\r\n    Returns\r\n    -------\r\n    InferenceData object\r\n    \"\"\"\r\n    check_posterior = (posterior is not None) and (type(posterior).__module__ == \"stan.fit\")\r\n    check_prior = (prior is not None) and (type(prior).__module__ == \"stan.fit\")\r\n    if check_posterior or check_prior:\r\n        return PyStan3Converter(\r\n            posterior=posterior,\r\n            posterior_model=posterior_model,\r\n            posterior_predictive=posterior_predictive,\r\n            prior=prior,\r\n            prior_model=prior_model,\r\n            prior_predictive=prior_predictive,\r\n            observed_data=observed_data,\r\n            log_likelihood=log_likelihood,\r\n            coords=coords,\r\n            dims=dims,\r\n        ).to_inference_data()\r\n    else:\r\n        return PyStanConverter(\r\n            posterior=posterior,\r\n            posterior_predictive=posterior_predictive,\r\n            prior=prior,\r\n            prior_predictive=prior_predictive,\r\n            observed_data=observed_data,\r\n            log_likelihood=log_likelihood,\r\n            coords=coords,\r\n            dims=dims,\r\n        ).to_inference_data()", "response": "Convert PyStan data into an InferenceData object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef posterior_to_xarray(self):\r\n        posterior = self.posterior\r\n        # filter posterior_predictive and log_likelihood\r\n        posterior_predictive = self.posterior_predictive\r\n        if posterior_predictive is None:\r\n            posterior_predictive = []\r\n        elif isinstance(posterior_predictive, str):\r\n            posterior_predictive = [posterior_predictive]\r\n        log_likelihood = self.log_likelihood\r\n        if not isinstance(log_likelihood, str):\r\n            log_likelihood = []\r\n        else:\r\n            log_likelihood = [log_likelihood]\r\n\r\n        ignore = posterior_predictive + log_likelihood + [\"lp__\"]\r\n\r\n        data = get_draws(posterior, ignore=ignore)\r\n\r\n        return dict_to_dataset(data, library=self.pystan, coords=self.coords, dims=self.dims)", "response": "Extract posterior samples from fit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract sample_stats from posterior.", "response": "def sample_stats_to_xarray(self):\r\n        \"\"\"Extract sample_stats from posterior.\"\"\"\r\n        posterior = self.posterior\r\n\r\n        # copy dims and coords\r\n        dims = deepcopy(self.dims) if self.dims is not None else {}\r\n        coords = deepcopy(self.coords) if self.coords is not None else {}\r\n\r\n        # log_likelihood\r\n        log_likelihood = self.log_likelihood\r\n        if log_likelihood is not None:\r\n            if isinstance(log_likelihood, str) and log_likelihood in dims:\r\n                dims[\"log_likelihood\"] = dims.pop(log_likelihood)\r\n\r\n        data = get_sample_stats(posterior, log_likelihood)\r\n\r\n        return dict_to_dataset(data, library=self.pystan, coords=coords, dims=dims)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting posterior_predictive samples to xarray.", "response": "def posterior_predictive_to_xarray(self):\r\n        \"\"\"Convert posterior_predictive samples to xarray.\"\"\"\r\n        posterior = self.posterior\r\n        posterior_predictive = self.posterior_predictive\r\n        data = get_draws(posterior, variables=posterior_predictive)\r\n        return dict_to_dataset(data, library=self.pystan, coords=self.coords, dims=self.dims)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prior_to_xarray(self):\r\n        prior = self.prior\r\n        # filter posterior_predictive and log_likelihood\r\n        prior_predictive = self.prior_predictive\r\n        if prior_predictive is None:\r\n            prior_predictive = []\r\n        elif isinstance(prior_predictive, str):\r\n            prior_predictive = [prior_predictive]\r\n\r\n        ignore = prior_predictive + [\"lp__\"]\r\n\r\n        data = get_draws(prior, ignore=ignore)\r\n        return dict_to_dataset(data, library=self.pystan, coords=self.coords, dims=self.dims)", "response": "Convert prior samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nextract sample_stats_prior from prior.", "response": "def sample_stats_prior_to_xarray(self):\r\n        \"\"\"Extract sample_stats_prior from prior.\"\"\"\r\n        prior = self.prior\r\n        data = get_sample_stats(prior)\r\n        return dict_to_dataset(data, library=self.pystan, coords=self.coords, dims=self.dims)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting prior_predictive samples to xarray.", "response": "def prior_predictive_to_xarray(self):\r\n        \"\"\"Convert prior_predictive samples to xarray.\"\"\"\r\n        prior = self.prior\r\n        prior_predictive = self.prior_predictive\r\n        data = get_draws(prior, variables=prior_predictive)\r\n        return dict_to_dataset(data, library=self.pystan, coords=self.coords, dims=self.dims)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting posterior samples from fit.", "response": "def posterior_to_xarray(self):\r\n        \"\"\"Extract posterior samples from fit.\"\"\"\r\n        posterior = self.posterior\r\n        posterior_model = self.posterior_model\r\n        # filter posterior_predictive and log_likelihood\r\n        posterior_predictive = self.posterior_predictive\r\n        if posterior_predictive is None:\r\n            posterior_predictive = []\r\n        elif isinstance(posterior_predictive, str):\r\n            posterior_predictive = [posterior_predictive]\r\n        log_likelihood = self.log_likelihood\r\n        if not isinstance(log_likelihood, str):\r\n            log_likelihood = []\r\n        else:\r\n            log_likelihood = [log_likelihood]\r\n\r\n        ignore = posterior_predictive + log_likelihood\r\n\r\n        data = get_draws_stan3(posterior, model=posterior_model, ignore=ignore)\r\n\r\n        return dict_to_dataset(data, library=self.stan, coords=self.coords, dims=self.dims)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nextracts sample_stats from posterior model and return xarray.", "response": "def sample_stats_to_xarray(self):\r\n        \"\"\"Extract sample_stats from posterior.\"\"\"\r\n        posterior = self.posterior\r\n        posterior_model = self.posterior_model\r\n        # copy dims and coords\r\n        dims = deepcopy(self.dims) if self.dims is not None else {}\r\n        coords = deepcopy(self.coords) if self.coords is not None else {}\r\n\r\n        # log_likelihood\r\n        log_likelihood = self.log_likelihood\r\n        if log_likelihood is not None:\r\n            if isinstance(log_likelihood, str) and log_likelihood in dims:\r\n                dims[\"log_likelihood\"] = dims.pop(log_likelihood)\r\n\r\n        data = get_sample_stats_stan3(\r\n            posterior, model=posterior_model, log_likelihood=log_likelihood\r\n        )\r\n\r\n        return dict_to_dataset(data, library=self.stan, coords=coords, dims=dims)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconverts posterior_predictive samples to xarray.", "response": "def posterior_predictive_to_xarray(self):\r\n        \"\"\"Convert posterior_predictive samples to xarray.\"\"\"\r\n        posterior = self.posterior\r\n        posterior_model = self.posterior_model\r\n        posterior_predictive = self.posterior_predictive\r\n        data = get_draws_stan3(posterior, model=posterior_model, variables=posterior_predictive)\r\n        return dict_to_dataset(data, library=self.stan, coords=self.coords, dims=self.dims)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef prior_to_xarray(self):\r\n        prior = self.prior\r\n        prior_model = self.prior_model\r\n        # filter posterior_predictive and log_likelihood\r\n        prior_predictive = self.prior_predictive\r\n        if prior_predictive is None:\r\n            prior_predictive = []\r\n        elif isinstance(prior_predictive, str):\r\n            prior_predictive = [prior_predictive]\r\n\r\n        ignore = prior_predictive\r\n\r\n        data = get_draws_stan3(prior, model=prior_model, ignore=ignore)\r\n        return dict_to_dataset(data, library=self.stan, coords=self.coords, dims=self.dims)", "response": "Convert prior samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nextract sample_stats_prior from prior.", "response": "def sample_stats_prior_to_xarray(self):\r\n        \"\"\"Extract sample_stats_prior from prior.\"\"\"\r\n        prior = self.prior\r\n        prior_model = self.prior_model\r\n        data = get_sample_stats_stan3(prior, model=prior_model)\r\n        return dict_to_dataset(data, library=self.stan, coords=self.coords, dims=self.dims)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prior_predictive_to_xarray(self):\r\n        prior = self.prior\r\n        prior_model = self.prior_model\r\n        prior_predictive = self.prior_predictive\r\n        data = get_draws_stan3(prior, model=prior_model, variables=prior_predictive)\r\n        return dict_to_dataset(data, library=self.stan, coords=self.coords, dims=self.dims)", "response": "Convert prior_predictive samples to xarray."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert observed data to xarray.", "response": "def observed_data_to_xarray(self):\r\n        \"\"\"Convert observed data to xarray.\"\"\"\r\n        posterior_model = self.posterior_model\r\n        if self.dims is None:\r\n            dims = {}\r\n        else:\r\n            dims = self.dims\r\n        observed_names = self.observed_data\r\n        if isinstance(observed_names, str):\r\n            observed_names = [observed_names]\r\n        observed_data = OrderedDict()\r\n        for key in observed_names:\r\n            vals = np.atleast_1d(posterior_model.data[key])\r\n            val_dims = dims.get(key)\r\n            val_dims, coords = generate_dims_coords(\r\n                vals.shape, key, dims=val_dims, coords=self.coords\r\n            )\r\n            observed_data[key] = xr.DataArray(vals, dims=val_dims, coords=coords)\r\n        return xr.Dataset(data_vars=observed_data, attrs=make_attrs(library=self.stan))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nextract observed and latent variable names from the MCMC trace.", "response": "def _get_var_names(posterior):\n    \"\"\"Extract latent and observed variable names from pyro.MCMC.\n\n    Parameters\n    ----------\n    posterior : pyro.MCMC\n        Fitted MCMC object from Pyro\n\n    Returns\n    -------\n    list[str], list[str]\n    observed and latent variable names from the MCMC trace.\n    \"\"\"\n    sample_point = posterior.exec_traces[0]\n    nodes = [node for node in sample_point.nodes.values() if node[\"type\"] == \"sample\"]\n    observed = [node[\"name\"] for node in nodes if node[\"is_observed\"]]\n    latent = [node[\"name\"] for node in nodes if not node[\"is_observed\"]]\n    return observed, latent"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_pyro(posterior=None, *, coords=None, dims=None):\n    return PyroConverter(posterior=posterior, coords=coords, dims=dims).to_inference_data()", "response": "Convert pyro data into an InferenceData object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef posterior_to_xarray(self):\n        # Do not make pyro a requirement\n        from pyro.infer import EmpiricalMarginal\n\n        try:  # Try pyro>=0.3 release syntax\n            data = {\n                name: np.expand_dims(samples.enumerate_support().squeeze(), 0)\n                if self.posterior.num_chains == 1\n                else samples.enumerate_support().squeeze()\n                for name, samples in self.posterior.marginal(\n                    sites=self.latent_vars\n                ).empirical.items()\n            }\n        except AttributeError:  # Use pyro<0.3 release syntax\n            data = {}\n            for var_name in self.latent_vars:\n                # pylint: disable=no-member\n                samples = EmpiricalMarginal(\n                    self.posterior, sites=var_name\n                ).get_samples_and_weights()[0]\n                data[var_name] = np.expand_dims(samples.numpy().squeeze(), 0)\n        return dict_to_dataset(data, library=self.pyro, coords=self.coords, dims=self.dims)", "response": "Convert the posterior to an xarray dataset."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbar plot of the autocorrelation function for a sequence of data.", "response": "def plot_autocorr(data, var_names=None, max_lag=100, combined=False, figsize=None, textsize=None):\n    \"\"\"Bar plot of the autocorrelation function for a sequence of data.\n\n    Useful in particular for posteriors from MCMC samples which may display correlation.\n\n    Parameters\n    ----------\n    data : obj\n        Any object that can be converted to an az.InferenceData object\n        Refer to documentation of az.convert_to_dataset for details\n    var_names : list of variable names, optional\n        Variables to be plotted, if None all variable are plotted.\n        Vector-value stochastics are handled automatically.\n    max_lag : int, optional\n        Maximum lag to calculate autocorrelation. Defaults to 100.\n    combined : bool\n        Flag for combining multiple chains into a single chain. If False (default), chains will be\n        plotted separately.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n        Note this is not used if ax is supplied.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n\n    Returns\n    -------\n    axes : matplotlib axes\n\n    Examples\n    --------\n    Plot default autocorrelation\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_autocorr(data)\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'] )\n\n\n    Combine chains collapsing by variable\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'], combined=True)\n\n\n    Specify maximum lag (x axis bound)\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_autocorr(data, var_names=['mu', 'tau'], max_lag=200, combined=True)\n    \"\"\"\n    data = convert_to_dataset(data, group=\"posterior\")\n    var_names = _var_names(var_names, data)\n\n    plotters = list(xarray_var_iter(data, var_names, combined))\n    length_plotters = len(plotters)\n    rows, cols = default_grid(length_plotters)\n\n    figsize, _, titlesize, xt_labelsize, linewidth, _ = _scale_fig_size(\n        figsize, textsize, rows, cols\n    )\n\n    _, axes = _create_axes_grid(\n        length_plotters, rows, cols, figsize=figsize, squeeze=False, sharex=True, sharey=True\n    )\n\n    axes = np.atleast_2d(axes)  # in case of only 1 plot\n    for (var_name, selection, x), ax in zip(plotters, axes.flatten()):\n        x_prime = x\n\n        if combined:\n            x_prime = x.flatten()\n\n        y = autocorr(x_prime)\n\n        ax.vlines(x=np.arange(0, max_lag), ymin=0, ymax=y[0:max_lag], lw=linewidth)\n        ax.hlines(0, 0, max_lag, \"steelblue\")\n        ax.set_title(make_label(var_name, selection), fontsize=titlesize, wrap=True)\n        ax.tick_params(labelsize=xt_labelsize)\n\n    if axes.size > 0:\n        axes[0, 0].set_xlim(0, max_lag)\n        axes[0, 0].set_ylim(-1, 1)\n\n    return axes"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot the energy transition distribution and marginal energy distribution in a KDE or histogram.", "response": "def plot_energy(\n    data,\n    kind=\"kde\",\n    bfmi=True,\n    figsize=None,\n    legend=True,\n    fill_alpha=(1, 0.75),\n    fill_color=(\"C0\", \"C5\"),\n    bw=4.5,\n    textsize=None,\n    fill_kwargs=None,\n    plot_kwargs=None,\n    ax=None,\n):\n    \"\"\"Plot energy transition distribution and marginal energy distribution in HMC algorithms.\n\n    This may help to diagnose poor exploration by gradient-based algorithms like HMC or NUTS.\n\n    Parameters\n    ----------\n    data : xarray dataset, or object that can be converted (must represent\n           `sample_stats` and have an `energy` variable)\n    kind : str\n        Type of plot to display (kde or histogram)\n    bfmi : bool\n        If True add to the plot the value of the estimated Bayesian fraction of missing information\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    legend : bool\n        Flag for plotting legend (defaults to True)\n    fill_alpha : tuple of floats\n        Alpha blending value for the shaded area under the curve, between 0\n        (no shade) and 1 (opaque). Defaults to (1, .75)\n    fill_color : tuple of valid matplotlib color\n        Color for Marginal energy distribution and Energy transition distribution.\n        Defaults to ('C0', 'C5')\n    bw : float\n        Bandwidth scaling factor for the KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule\n        of thumb (the default rule used by SciPy). Only works if `kind='kde'`\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    fill_kwargs : dicts, optional\n        Additional keywords passed to `arviz.plot_kde` (to control the shade)\n    plot_kwargs : dicts, optional\n        Additional keywords passed to `arviz.plot_kde` or `plt.hist` (if type='hist')\n    ax : axes\n        Matplotlib axes.\n\n    Returns\n    -------\n    ax : matplotlib axes\n\n    Examples\n    --------\n    Plot a default energy plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('centered_eight')\n        >>> az.plot_energy(data)\n\n    Represent energy plot via histograms\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_energy(data, kind='hist')\n\n    \"\"\"\n    energy = convert_to_dataset(data, group=\"sample_stats\").energy.values\n\n    if ax is None:\n        _, ax = plt.subplots(figsize=figsize, constrained_layout=True)\n\n    if fill_kwargs is None:\n        fill_kwargs = {}\n\n    if plot_kwargs is None:\n        plot_kwargs = {}\n\n    figsize, _, _, xt_labelsize, linewidth, _ = _scale_fig_size(figsize, textsize, 1, 1)\n\n    series = zip(\n        fill_alpha,\n        fill_color,\n        (\"Marginal Energy\", \"Energy transition\"),\n        (energy - energy.mean(), np.diff(energy)),\n    )\n\n    if kind == \"kde\":\n        for alpha, color, label, value in series:\n            fill_kwargs[\"alpha\"] = alpha\n            fill_kwargs[\"color\"] = color\n            plot_kwargs.setdefault(\"color\", color)\n            plot_kwargs.setdefault(\"alpha\", 0)\n            plot_kwargs.setdefault(\"linewidth\", linewidth)\n            plot_kde(\n                value,\n                bw=bw,\n                label=label,\n                textsize=xt_labelsize,\n                plot_kwargs=plot_kwargs,\n                fill_kwargs=fill_kwargs,\n                ax=ax,\n            )\n\n    elif kind == \"hist\":\n        for alpha, color, label, value in series:\n            ax.hist(\n                value.flatten(),\n                bins=\"auto\",\n                density=True,\n                alpha=alpha,\n                label=label,\n                color=color,\n                **plot_kwargs\n            )\n\n    else:\n        raise ValueError(\"Plot type {} not recognized.\".format(kind))\n\n    if bfmi:\n        for idx, val in enumerate(e_bfmi(energy)):\n            ax.plot([], label=\"chain {:>2} BFMI = {:.2f}\".format(idx, val), alpha=0)\n\n    ax.set_xticks([])\n    ax.set_yticks([])\n\n    if legend:\n        ax.legend()\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot_forest(\n    data,\n    kind=\"forestplot\",\n    model_names=None,\n    var_names=None,\n    combined=False,\n    credible_interval=0.94,\n    rope=None,\n    quartiles=True,\n    ess=False,\n    r_hat=False,\n    colors=\"cycle\",\n    textsize=None,\n    linewidth=None,\n    markersize=None,\n    ridgeplot_alpha=None,\n    ridgeplot_overlap=2,\n    figsize=None,\n):\n    \"\"\"Forest plot to compare credible intervals from a number of distributions.\n\n    Generates a forest plot of 100*(credible_interval)% credible intervals from\n    a trace or list of traces.\n\n    Parameters\n    ----------\n    data : obj or list[obj]\n        Any object that can be converted to an az.InferenceData object\n        Refer to documentation of az.convert_to_dataset for details\n    kind : str\n        Choose kind of plot for main axis. Supports \"forestplot\" or \"ridgeplot\"\n    model_names : list[str], optional\n        List with names for the models in the list of data. Useful when\n        plotting more that one dataset\n    var_names: list[str], optional\n        List of variables to plot (defaults to None, which results in all\n        variables plotted)\n    combined : bool\n        Flag for combining multiple chains into a single chain. If False (default),\n        chains will be plotted separately.\n    credible_interval : float, optional\n        Credible interval to plot. Defaults to 0.94.\n    rope: tuple or dictionary of tuples\n        Lower and upper values of the Region Of Practical Equivalence. If a list with one\n        interval only is provided, the ROPE will be displayed across the y-axis. If more than one\n        interval is provided the length of the list should match the number of variables.\n    quartiles : bool, optional\n        Flag for plotting the interquartile range, in addition to the credible_interval intervals.\n        Defaults to True\n    r_hat : bool, optional\n        Flag for plotting Split R-hat statistics. Requires 2 or more chains. Defaults to False\n    ess : bool, optional\n        Flag for plotting the effective sample size. Requires 2 or more chains. Defaults to False\n    colors : list or string, optional\n        list with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically chose a color per model from the\n        matplotlibs cycle. If a single color is passed, eg 'k', 'C2', 'red' this color will be used\n        for all models. Defauls to 'cycle'.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    linewidth : int\n        Line width throughout. If None it will be autoscaled based on figsize.\n    markersize : int\n        Markersize throughout. If None it will be autoscaled based on figsize.\n    ridgeplot_alpha : float\n        Transparency for ridgeplot fill.  If 0, border is colored by model, otherwise\n        a black outline is used.\n    ridgeplot_overlap : float\n        Overlap height for ridgeplots.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n\n    Returns\n    -------\n    gridspec : matplotlib GridSpec\n\n    Examples\n    --------\n    Forestp\u013aot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered_data = az.load_arviz_data('non_centered_eight')\n        >>> fig, axes = az.plot_forest(non_centered_data,\n        >>>                            kind='forestplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_overlap=3,\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n\n    Ridgeplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> fig, axes = az.plot_forest(non_centered_data,\n        >>>                            kind='ridgeplot',\n        >>>                            var_names=['theta'],\n        >>>                            combined=True,\n        >>>                            ridgeplot_overlap=3,\n        >>>                            colors='white',\n        >>>                            figsize=(9, 7))\n        >>> axes[0].set_title('Estimated theta for 8 schools model')\n    \"\"\"\n    if not isinstance(data, (list, tuple)):\n        data = [data]\n\n    datasets = [convert_to_dataset(datum) for datum in reversed(data)]\n\n    var_names = _var_names(var_names, datasets)\n\n    ncols, width_ratios = 1, [3]\n\n    if ess:\n        ncols += 1\n        width_ratios.append(1)\n\n    if r_hat:\n        ncols += 1\n        width_ratios.append(1)\n\n    plot_handler = PlotHandler(\n        datasets, var_names=var_names, model_names=model_names, combined=combined, colors=colors\n    )\n\n    if figsize is None:\n        figsize = (min(12, sum(width_ratios) * 2), plot_handler.fig_height())\n\n    (figsize, _, titlesize, xt_labelsize, auto_linewidth, auto_markersize) = _scale_fig_size(\n        figsize, textsize, 1.1, 1\n    )\n\n    if linewidth is None:\n        linewidth = auto_linewidth\n\n    if markersize is None:\n        markersize = auto_markersize\n\n    fig, axes = plt.subplots(\n        nrows=1,\n        ncols=ncols,\n        figsize=figsize,\n        gridspec_kw={\"width_ratios\": width_ratios},\n        sharey=True,\n        constrained_layout=True,\n    )\n\n    axes = np.atleast_1d(axes)\n    if kind == \"forestplot\":\n        plot_handler.forestplot(\n            credible_interval,\n            quartiles,\n            xt_labelsize,\n            titlesize,\n            linewidth,\n            markersize,\n            axes[0],\n            rope,\n        )\n    elif kind == \"ridgeplot\":\n        plot_handler.ridgeplot(ridgeplot_overlap, linewidth, ridgeplot_alpha, axes[0])\n    else:\n        raise TypeError(\n            \"Argument 'kind' must be one of 'forestplot' or \"\n            \"'ridgeplot' (you provided {})\".format(kind)\n        )\n\n    idx = 1\n    if ess:\n        plot_handler.plot_neff(axes[idx], xt_labelsize, titlesize, markersize)\n        idx += 1\n\n    if r_hat:\n        plot_handler.plot_rhat(axes[idx], xt_labelsize, titlesize, markersize)\n        idx += 1\n\n    for ax in axes:\n        ax.grid(False)\n        # Remove ticklines on y-axes\n        for ticks in ax.yaxis.get_major_ticks():\n            ticks.tick1On = False\n            ticks.tick2On = False\n\n        for loc, spine in ax.spines.items():\n            if loc in [\"left\", \"right\"]:\n                spine.set_visible(False)\n\n        if len(plot_handler.data) > 1:\n            plot_handler.make_bands(ax)\n\n    labels, ticks = plot_handler.labels_and_ticks()\n    axes[0].set_yticks(ticks)\n    axes[0].set_yticklabels(labels)\n    all_plotters = list(plot_handler.plotters.values())\n    y_max = plot_handler.y_max() - all_plotters[-1].group_offset\n    if kind == \"ridgeplot\":  # space at the top\n        y_max += ridgeplot_overlap\n    axes[0].set_ylim(-all_plotters[0].group_offset, y_max)\n\n    return fig, axes", "response": "Generates a forest plot of the credible intervals from a set of data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninitializing an object for each variable to be plotted.", "response": "def make_plotters(self):\n        \"\"\"Initialize an object for each variable to be plotted.\"\"\"\n        plotters, y = {}, 0\n        for var_name in self.var_names:\n            plotters[var_name] = VarHandler(\n                var_name,\n                self.data,\n                y,\n                model_names=self.model_names,\n                combined=self.combined,\n                colors=self.colors,\n            )\n            y = plotters[var_name].y_max()\n        return plotters"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef labels_and_ticks(self):\n        labels, idxs = [], []\n        for plotter in self.plotters.values():\n            sub_labels, sub_idxs, _, _ = plotter.labels_ticks_and_vals()\n            labels.append(sub_labels)\n            idxs.append(sub_idxs)\n        return np.concatenate(labels), np.concatenate(idxs)", "response": "Collect labels and ticks from plotters."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef display_multiple_ropes(self, rope, ax, y, linewidth, rope_var):\n        vals = dict(rope[rope_var][0])[\"rope\"]\n        ax.plot(\n            vals,\n            (y + 0.05, y + 0.05),\n            lw=linewidth * 2,\n            color=\"C2\",\n            solid_capstyle=\"round\",\n            zorder=0,\n            alpha=0.7,\n        )\n        return ax", "response": "Display ROPE when more than one interval is provided."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndraws ridgeplot for each plotter.", "response": "def ridgeplot(self, mult, linewidth, alpha, ax):\n        \"\"\"Draw ridgeplot for each plotter.\n\n        Parameters\n        ----------\n        mult : float\n            How much to multiply height by. Set this to greater than 1 to have some overlap.\n        linewidth : float\n            Width of line on border of ridges\n        alpha : float\n            Transparency of ridges\n        ax : Axes\n            Axes to draw on\n        \"\"\"\n        if alpha is None:\n            alpha = 1.0\n        zorder = 0\n        for plotter in self.plotters.values():\n            for x, y_min, y_max, color in plotter.ridgeplot(mult):\n                if alpha == 0:\n                    border = color\n                else:\n                    border = \"k\"\n                ax.plot(x, y_max, \"-\", linewidth=linewidth, color=border, zorder=zorder)\n                ax.plot(x, y_min, \"-\", linewidth=linewidth, color=border, zorder=zorder)\n                ax.fill_between(x, y_min, y_max, alpha=alpha, color=color, zorder=zorder)\n                zorder -= 1\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws forestplot for each plotter.", "response": "def forestplot(\n        self, credible_interval, quartiles, xt_labelsize, titlesize, linewidth, markersize, ax, rope\n    ):\n        \"\"\"Draw forestplot for each plotter.\n\n        Parameters\n        ----------\n        credible_interval : float\n            How wide each line should be\n        quartiles : bool\n            Whether to mark quartiles\n        xt_textsize : float\n            Size of tick text\n        titlesize : float\n            Size of title text\n        linewidth : float\n            Width of forestplot line\n        markersize : float\n            Size of marker in center of forestplot line\n        ax : Axes\n            Axes to draw on\n        \"\"\"\n        # Quantiles to be calculated\n        endpoint = 100 * (1 - credible_interval) / 2\n        if quartiles:\n            qlist = [endpoint, 25, 50, 75, 100 - endpoint]\n        else:\n            qlist = [endpoint, 50, 100 - endpoint]\n\n        for plotter in self.plotters.values():\n            for y, rope_var, values, color in plotter.treeplot(qlist, credible_interval):\n                if isinstance(rope, dict):\n                    self.display_multiple_ropes(rope, ax, y, linewidth, rope_var)\n\n                mid = len(values) // 2\n                param_iter = zip(\n                    np.linspace(2 * linewidth, linewidth, mid, endpoint=True)[-1::-1], range(mid)\n                )\n                for width, j in param_iter:\n                    ax.hlines(y, values[j], values[-(j + 1)], linewidth=width, color=color)\n                ax.plot(\n                    values[mid],\n                    y,\n                    \"o\",\n                    mfc=ax.get_facecolor(),\n                    markersize=markersize * 0.75,\n                    color=color,\n                )\n        ax.tick_params(labelsize=xt_labelsize)\n        ax.set_title(\n            \"{:.1%} Credible Interval\".format(credible_interval), fontsize=titlesize, wrap=True\n        )\n        if rope is None or isinstance(rope, dict):\n            return\n        elif len(rope) == 2:\n            ax.axvspan(rope[0], rope[1], 0, self.y_max(), color=\"C2\", alpha=0.5)\n        else:\n            raise ValueError(\n                \"Argument `rope` must be None, a dictionary like\"\n                '{\"var_name\": {\"rope\": (lo, hi)}}, or an '\n                \"iterable of length 2\"\n            )\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot_neff(self, ax, xt_labelsize, titlesize, markersize):\n        for plotter in self.plotters.values():\n            for y, ess, color in plotter.ess():\n                if ess is not None:\n                    ax.plot(\n                        ess,\n                        y,\n                        \"o\",\n                        color=color,\n                        clip_on=False,\n                        markersize=markersize,\n                        markeredgecolor=\"k\",\n                    )\n        ax.set_xlim(left=0)\n        ax.set_title(\"ess\", fontsize=titlesize, wrap=True)\n        ax.tick_params(labelsize=xt_labelsize)\n        return ax", "response": "Draw effective n for each plotter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_rhat(self, ax, xt_labelsize, titlesize, markersize):\n        for plotter in self.plotters.values():\n            for y, r_hat, color in plotter.r_hat():\n                if r_hat is not None:\n                    ax.plot(r_hat, y, \"o\", color=color, markersize=markersize, markeredgecolor=\"k\")\n        ax.set_xlim(left=0.9, right=2.1)\n        ax.set_xticks([1, 2])\n        ax.tick_params(labelsize=xt_labelsize)\n        ax.set_title(\"r_hat\", fontsize=titlesize, wrap=True)\n        return ax", "response": "Draw r - hat for each plotter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndraw shaded horizontal bands for each plotter.", "response": "def make_bands(self, ax):\n        \"\"\"Draw shaded horizontal bands for each plotter.\"\"\"\n        y_vals, y_prev, is_zero = [0], None, False\n        prev_color_index = 0\n        for plotter in self.plotters.values():\n            for y, *_, color in plotter.iterator():\n                if self.colors.index(color) < prev_color_index:\n                    if not is_zero and y_prev is not None:\n                        y_vals.append((y + y_prev) * 0.5)\n                        is_zero = True\n                else:\n                    is_zero = False\n                prev_color_index = self.colors.index(color)\n                y_prev = y\n\n        offset = plotter.group_offset  # pylint: disable=undefined-loop-variable\n\n        y_vals.append(y_prev + offset)\n        for idx, (y_start, y_stop) in enumerate(pairwise(y_vals)):\n            ax.axhspan(y_start, y_stop, color=\"k\", alpha=0.1 * (idx % 2))\n        return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfigures out the height of this plot.", "response": "def fig_height(self):\n        \"\"\"Figure out the height of this plot.\"\"\"\n        # hand-tuned\n        return (\n            4\n            + len(self.data) * len(self.var_names)\n            - 1\n            + 0.1 * sum(1 for j in self.plotters.values() for _ in j.iterator())\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef iterator(self):\n        if self.combined:\n            grouped_data = [[(0, datum)] for datum in self.data]\n            skip_dims = {\"chain\"}\n        else:\n            grouped_data = [datum.groupby(\"chain\") for datum in self.data]\n            skip_dims = set()\n\n        label_dict = OrderedDict()\n        for name, grouped_datum in zip(self.model_names, grouped_data):\n            for _, sub_data in grouped_datum:\n                datum_iter = xarray_var_iter(\n                    sub_data,\n                    var_names=[self.var_name],\n                    skip_dims=skip_dims,\n                    reverse_selections=True,\n                )\n                for _, selection, values in datum_iter:\n                    label = make_label(self.var_name, selection, position=\"beside\")\n                    if label not in label_dict:\n                        label_dict[label] = OrderedDict()\n                    if name not in label_dict[label]:\n                        label_dict[label][name] = []\n                    label_dict[label][name].append(values)\n\n        y = self.y_start\n        for label, model_data in label_dict.items():\n            for model_name, value_list in model_data.items():\n                if model_name:\n                    row_label = \"{}: {}\".format(model_name, label)\n                else:\n                    row_label = label\n                for values in value_list:\n                    yield y, row_label, label, values, self.model_color[model_name]\n                    y += self.chain_offset\n                y += self.var_offset\n            y += self.group_offset", "response": "Iterate over models and chains for each variable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting labels ticks values and colors for the variable.", "response": "def labels_ticks_and_vals(self):\n        \"\"\"Get labels, ticks, values, and colors for the variable.\"\"\"\n        y_ticks = defaultdict(list)\n        for y, label, _, vals, color in self.iterator():\n            y_ticks[label].append((y, vals, color))\n        labels, ticks, vals, colors = [], [], [], []\n        for label, data in y_ticks.items():\n            labels.append(label)\n            ticks.append(np.mean([j[0] for j in data]))\n            vals.append(np.vstack([j[1] for j in data]))\n            colors.append(data[0][2])  # the colors are all the same\n        return labels, ticks, vals, colors"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef treeplot(self, qlist, credible_interval):\n        for y, _, label, values, color in self.iterator():\n            ntiles = np.percentile(values.flatten(), qlist)\n            ntiles[0], ntiles[-1] = hpd(values.flatten(), credible_interval)\n            yield y, label, ntiles, color", "response": "Get data for each treeplot for the variable."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ridgeplot(self, mult):\n        xvals, yvals, pdfs, colors = [], [], [], []\n        for y, *_, values, color in self.iterator():\n            yvals.append(y)\n            colors.append(color)\n            values = values.flatten()\n            density, lower, upper = _fast_kde(values)\n            xvals.append(np.linspace(lower, upper, len(density)))\n            pdfs.append(density)\n\n        scaling = max(j.max() for j in pdfs)\n        for y, x, pdf, color in zip(yvals, xvals, pdfs, colors):\n            y = y * np.ones_like(x)\n            yield x, y, mult * pdf / scaling + y, color", "response": "Get data for each ridgeplot for the variable."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ess(self):\n        _, y_vals, values, colors = self.labels_ticks_and_vals()\n        for y, value, color in zip(y_vals, values, colors):\n            if value.ndim != 2 or value.shape[0] < 2:\n                yield y, None, color\n            else:\n                yield y, _get_ess(value), color", "response": "Get effective n data for the variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef r_hat(self):\n        _, y_vals, values, colors = self.labels_ticks_and_vals()\n        for y, value, color in zip(y_vals, values, colors):\n            if value.ndim != 2 or value.shape[0] < 2:\n                yield y, None, color\n            else:\n                yield y, _get_split_rhat(value), color", "response": "Get rhat data for the variable."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the maximum y value for the variable.", "response": "def y_max(self):\n        \"\"\"Get max y value for the variable.\"\"\"\n        end_y = max(y for y, *_ in self.iterator())\n\n        if self.combined:\n            end_y += self.group_offset\n\n        return end_y + 2 * self.group_offset"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplot the khat of the given array of khats.", "response": "def plot_khat(\n    khats, figsize=None, textsize=None, markersize=None, ax=None, hlines_kwargs=None, **kwargs\n):\n    \"\"\"\n    Plot Pareto tail indices.\n\n    Parameters\n    ----------\n    khats : array\n        Pareto tail indices.\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    markersize: int\n        markersize for scatter plot. Defaults to `None` in which case it will\n        be chosen based on autoscaling for figsize.\n    ax: axes, opt\n      Matplotlib axes\n    hlines_kwargs: dictionary\n      Additional keywords passed to ax.hlines\n    kwargs :\n      Additional keywords passed to ax.scatter\n\n    Returns\n    -------\n    ax : axes\n      Matplotlib axes.\n\n    Examples\n    --------\n    Plot a default khat plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered_eight = az.load_arviz_data('centered_eight')\n        >>> pareto_k = az.loo(centered_eight, pointwise=True)['pareto_k']\n        >>> az.plot_khat(pareto_k)\n\n    \"\"\"\n    if hlines_kwargs is None:\n        hlines_kwargs = {}\n\n    (figsize, ax_labelsize, _, xt_labelsize, linewidth, scaled_markersize) = _scale_fig_size(\n        figsize, textsize\n    )\n\n    if markersize is None:\n        markersize = scaled_markersize\n\n    if ax is None:\n        _, ax = plt.subplots(1, 1, figsize=figsize, constrained_layout=True)\n\n    ax.hlines(\n        [0, 0.5, 0.7, 1],\n        xmin=-1,\n        xmax=len(khats) + 1,\n        alpha=0.25,\n        linewidth=linewidth,\n        **hlines_kwargs\n    )\n\n    alphas = 0.5 + 0.5 * (khats > 0.5)\n    rgba_c = np.zeros((len(khats), 4))\n    rgba_c[:, 2] = 0.8\n    rgba_c[:, 3] = alphas\n    ax.scatter(np.arange(len(khats)), khats, c=rgba_c, marker=\"+\", s=markersize, **kwargs)\n    ax.set_xlabel(\"Data point\", fontsize=ax_labelsize)\n    ax.set_ylabel(r\"Shape parameter \u03ba\", fontsize=ax_labelsize)\n    ax.tick_params(labelsize=xt_labelsize)\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating a KDE plot for continuous variables and histograms for discrete ones.", "response": "def plot_density(\n    data,\n    group=\"posterior\",\n    data_labels=None,\n    var_names=None,\n    credible_interval=0.94,\n    point_estimate=\"mean\",\n    colors=\"cycle\",\n    outline=True,\n    hpd_markers=\"\",\n    shade=0.0,\n    bw=4.5,\n    figsize=None,\n    textsize=None,\n):\n    \"\"\"Generate KDE plots for continuous variables and histograms for discrete ones.\n\n    Plots are truncated at their 100*(1-alpha)% credible intervals. Plots are grouped per variable\n    and colors assigned to models.\n\n    Parameters\n    ----------\n    data : Union[Object, Iterator[Object]]\n        Any object that can be converted to an az.InferenceData object, or an Iterator returning\n        a sequence of such objects.\n        Refer to documentation of az.convert_to_dataset for details about such objects.\n    group: Optional[str]\n        Specifies which InferenceData group should be plotted.  Defaults to 'posterior'.\n        Alternative values include 'prior' and any other strings used as dataset keys in the\n        InferenceData.\n    data_labels : Optional[List[str]]\n        List with names for the datasets passed as \"data.\" Useful when plotting more than one\n        dataset.  Must be the same shape as the data parameter.  Defaults to None.\n    var_names: Optional[List[str]]\n        List of variables to plot.  If multiple datasets are supplied and var_names is not None,\n        will print the same set of variables for each dataset.  Defaults to None, which results in\n        all the variables being plotted.\n    credible_interval : float\n        Credible intervals. Should be in the interval (0, 1]. Defaults to 0.94.\n    point_estimate : Optional[str]\n        Plot point estimate per variable. Values should be 'mean', 'median' or None.\n        Defaults to 'mean'.\n    colors : Optional[Union[List[str],str]]\n        List with valid matplotlib colors, one color per model. Alternative a string can be passed.\n        If the string is `cycle`, it will automatically choose a color per model from matplolib's\n        cycle. If a single color is passed, e.g. 'k', 'C2' or 'red' this color will be used for all\n        models. Defaults to `cycle`.\n    outline : bool\n        Use a line to draw KDEs and histograms. Default to True\n    hpd_markers : str\n        A valid `matplotlib.markers` like 'v', used to indicate the limits of the hpd interval.\n        Defaults to empty string (no marker).\n    shade : Optional[float]\n        Alpha blending value for the shaded area under the curve, between 0 (no shade) and 1\n        (opaque). Defaults to 0.\n    bw : Optional[float]\n        Bandwidth scaling factor for the KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule\n        of thumb (the default rule used by SciPy).\n    figsize : Optional[Tuple[int, int]]\n        Figure size. If None it will be defined automatically.\n    textsize: Optional[float]\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n\n    Returns\n    -------\n    ax : Matplotlib axes\n\n\n    Examples\n    --------\n    Plot default density plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> az.plot_density([centered, non_centered])\n\n    Plot subset variables by specifying variable name exactly\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"])\n\n    Plot a specific `az.InferenceData` group\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], group=\"prior\")\n\n    Specify credible interval\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], credible_interval=.5)\n\n    Shade plots and/or remove outlines\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], outline=False, shade=.8)\n\n    Specify binwidth for kernel density estimation\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_density([centered, non_centered], var_names=[\"mu\"], bw=.9)\n    \"\"\"\n    if not isinstance(data, (list, tuple)):\n        datasets = [convert_to_dataset(data, group=group)]\n    else:\n        datasets = [convert_to_dataset(datum, group=group) for datum in data]\n\n    var_names = _var_names(var_names, datasets)\n\n    if point_estimate not in (\"mean\", \"median\", None):\n        raise ValueError(\n            \"Point estimate should be 'mean',\" \"median' or None, not {}\".format(point_estimate)\n        )\n\n    n_data = len(datasets)\n\n    if data_labels is None:\n        if n_data > 1:\n            data_labels = [\"{}\".format(idx) for idx in range(n_data)]\n        else:\n            data_labels = [\"\"]\n    elif len(data_labels) != n_data:\n        raise ValueError(\n            \"The number of names for the models ({}) \"\n            \"does not match the number of models ({})\".format(len(data_labels), n_data)\n        )\n\n    if colors == \"cycle\":\n        colors = [\"C{}\".format(idx % 10) for idx in range(n_data)]\n    elif isinstance(colors, str):\n        colors = [colors for _ in range(n_data)]\n\n    if not 1 >= credible_interval > 0:\n        raise ValueError(\"The value of credible_interval should be in the interval (0, 1]\")\n\n    to_plot = [list(xarray_var_iter(data, var_names, combined=True)) for data in datasets]\n    all_labels = []\n    length_plotters = []\n    for plotters in to_plot:\n        length_plotters.append(len(plotters))\n        for var_name, selection, _ in plotters:\n            label = make_label(var_name, selection)\n            if label not in all_labels:\n                all_labels.append(label)\n    length_plotters = max(length_plotters)\n    rows, cols = default_grid(length_plotters, max_cols=3)\n\n    (figsize, _, titlesize, xt_labelsize, linewidth, markersize) = _scale_fig_size(\n        figsize, textsize, rows, cols\n    )\n\n    _, ax = _create_axes_grid(length_plotters, rows, cols, figsize=figsize, squeeze=False)\n\n    axis_map = {label: ax_ for label, ax_ in zip(all_labels, ax.flatten())}\n    for m_idx, plotters in enumerate(to_plot):\n        for var_name, selection, values in plotters:\n            label = make_label(var_name, selection)\n            _d_helper(\n                values.flatten(),\n                label,\n                colors[m_idx],\n                bw,\n                titlesize,\n                xt_labelsize,\n                linewidth,\n                markersize,\n                credible_interval,\n                point_estimate,\n                hpd_markers,\n                outline,\n                shade,\n                axis_map[label],\n            )\n\n    if n_data > 1:\n        for m_idx, label in enumerate(data_labels):\n            ax[0].plot([], label=label, c=colors[m_idx], markersize=markersize)\n        ax[0].legend(fontsize=xt_labelsize)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _d_helper(\n    vec,\n    vname,\n    color,\n    bw,\n    titlesize,\n    xt_labelsize,\n    linewidth,\n    markersize,\n    credible_interval,\n    point_estimate,\n    hpd_markers,\n    outline,\n    shade,\n    ax,\n):\n    \"\"\"Plot an individual dimension.\n\n    Parameters\n    ----------\n    vec : array\n        1D array from trace\n    vname : str\n        variable name\n    color : str\n        matplotlib color\n    bw : float\n        Bandwidth scaling factor. Should be larger than 0. The higher this number the smoother the\n        KDE will be. Defaults to 4.5 which is essentially the same as the Scott's rule of thumb\n        (the default used rule by SciPy).\n    titlesize : float\n        font size for title\n    xt_labelsize : float\n       fontsize for xticks\n    linewidth : float\n        Thickness of lines\n    markersize : float\n        Size of markers\n    credible_interval : float\n        Credible intervals. Defaults to 0.94\n    point_estimate : str or None\n        'mean' or 'median'\n    shade : float\n        Alpha blending value for the shaded area under the curve, between 0 (no shade) and 1\n        (opaque). Defaults to 0.\n    ax : matplotlib axes\n    \"\"\"\n    if vec.dtype.kind == \"f\":\n        if credible_interval != 1:\n            hpd_ = hpd(vec, credible_interval)\n            new_vec = vec[(vec >= hpd_[0]) & (vec <= hpd_[1])]\n        else:\n            new_vec = vec\n\n        density, xmin, xmax = _fast_kde(new_vec, bw=bw)\n        density *= credible_interval\n        x = np.linspace(xmin, xmax, len(density))\n        ymin = density[0]\n        ymax = density[-1]\n\n        if outline:\n            ax.plot(x, density, color=color, lw=linewidth)\n            ax.plot([xmin, xmin], [-ymin / 100, ymin], color=color, ls=\"-\", lw=linewidth)\n            ax.plot([xmax, xmax], [-ymax / 100, ymax], color=color, ls=\"-\", lw=linewidth)\n\n        if shade:\n            ax.fill_between(x, density, color=color, alpha=shade)\n\n    else:\n        xmin, xmax = hpd(vec, credible_interval)\n        bins = range(xmin, xmax + 2)\n        if outline:\n            ax.hist(vec, bins=bins, color=color, histtype=\"step\", align=\"left\")\n        if shade:\n            ax.hist(vec, bins=bins, color=color, alpha=shade)\n\n    if hpd_markers:\n        ax.plot(xmin, 0, hpd_markers, color=color, markeredgecolor=\"k\", markersize=markersize)\n        ax.plot(xmax, 0, hpd_markers, color=color, markeredgecolor=\"k\", markersize=markersize)\n\n    if point_estimate is not None:\n        if point_estimate == \"mean\":\n            est = np.mean(vec)\n        elif point_estimate == \"median\":\n            est = np.median(vec)\n        ax.plot(est, 0, \"o\", color=color, markeredgecolor=\"k\", markersize=markersize)\n\n    ax.set_yticks([])\n    ax.set_title(vname, fontsize=titlesize, wrap=True)\n    for pos in [\"left\", \"right\", \"top\"]:\n        ax.spines[pos].set_visible(False)\n    ax.tick_params(labelsize=xt_labelsize)", "response": "Helper function for plot_d."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nplot a 1D or 2D KDE plot for the given set of values.", "response": "def plot_kde(\n    values,\n    values2=None,\n    cumulative=False,\n    rug=False,\n    label=None,\n    bw=4.5,\n    quantiles=None,\n    rotated=False,\n    contour=True,\n    fill_last=True,\n    textsize=None,\n    plot_kwargs=None,\n    fill_kwargs=None,\n    rug_kwargs=None,\n    contour_kwargs=None,\n    contourf_kwargs=None,\n    pcolormesh_kwargs=None,\n    ax=None,\n    legend=True,\n):\n    \"\"\"1D or 2D KDE plot taking into account boundary conditions.\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE will be estimated\n    cumulative : bool\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE\n    rug : bool\n        If True adds a rugplot. Defaults to False. Ignored for 2D KDE\n    label : string\n        Text to include as part of the legend\n    bw : float\n        Bandwidth scaling factor for 1D KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's\n        rule of thumb (the default rule used by SciPy).\n    quantiles : list\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n        Defaults to None.\n    rotated : bool\n        Whether to rotate the 1D KDE plot 90 degrees.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE. Defaults to True.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to True.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use fill_kwargs={'alpha': 0} to disable fill).\n        Ignored for 2D KDE\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if rug=False or for 2D KDE\n        Use `space` keyword (float) to control the position of the rugplot. The larger this number\n        the lower the rugplot.\n    contour_kwargs : dict\n        Keywords passed to ax.contour. Ignored for 1D KDE.\n    contourf_kwargs : dict\n        Keywords passed to ax.contourf. Ignored for 1D KDE.\n    pcolormesh_kwargs : dict\n        Keywords passed to ax.pcolormesh. Ignored for 1D KDE.\n    ax : matplotlib axes\n    legend : bool\n        Add legend to the figure. By default True.\n\n    Returns\n    -------\n    ax : matplotlib axes\n\n    Examples\n    --------\n    Plot default KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> non_centered = az.load_arviz_data('non_centered_eight')\n        >>> mu_posterior = np.concatenate(non_centered.posterior[\"mu\"].values)\n        >>> az.plot_kde(mu_posterior)\n\n\n    Plot KDE with rugplot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rug=True)\n\n\n    Plot a cumulative distribution\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, cumulative=True)\n\n\n\n    Rotate plot 90 degrees\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, rotated=True)\n\n\n    Plot 2d contour KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> tau_posterior = np.concatenate(non_centered.posterior[\"tau\"].values)\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior)\n\n    Remove fill for last contour in 2d KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, fill_last=False)\n\n    Plot 2d smooth KDE\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_kde(mu_posterior, values2=tau_posterior, contour=False)\n\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    figsize = ax.get_figure().get_size_inches()\n\n    figsize, *_, xt_labelsize, linewidth, markersize = _scale_fig_size(figsize, textsize, 1, 1)\n\n    if isinstance(values, xr.Dataset):\n        raise ValueError(\n            \"Xarray dataset object detected.Use plot_posterior, plot_density, plot_joint\"\n            \"or plot_pair instead of plot_kde\"\n        )\n    if isinstance(values, InferenceData):\n        raise ValueError(\" Inference Data object detected. Use plot_posterior instead of plot_kde\")\n\n    if values2 is None:\n        if plot_kwargs is None:\n            plot_kwargs = {}\n        plot_kwargs.setdefault(\"color\", \"C0\")\n\n        default_color = plot_kwargs.get(\"color\")\n\n        if fill_kwargs is None:\n            fill_kwargs = {}\n\n        fill_kwargs.setdefault(\"color\", default_color)\n\n        if rug_kwargs is None:\n            rug_kwargs = {}\n        rug_kwargs.setdefault(\"marker\", \"_\" if rotated else \"|\")\n        rug_kwargs.setdefault(\"linestyle\", \"None\")\n        rug_kwargs.setdefault(\"color\", default_color)\n        rug_kwargs.setdefault(\"space\", 0.2)\n\n        plot_kwargs.setdefault(\"linewidth\", linewidth)\n        rug_kwargs.setdefault(\"markersize\", 2 * markersize)\n\n        density, lower, upper = _fast_kde(values, cumulative, bw)\n\n        rug_space = max(density) * rug_kwargs.pop(\"space\")\n\n        x = np.linspace(lower, upper, len(density))\n\n        if cumulative:\n            density_q = density\n        else:\n            density_q = density.cumsum() / density.sum()\n        fill_func = ax.fill_between\n        fill_x, fill_y = x, density\n        if rotated:\n            x, density = density, x\n            fill_func = ax.fill_betweenx\n\n        ax.tick_params(labelsize=xt_labelsize)\n\n        if rotated:\n            ax.set_xlim(0, auto=True)\n            rug_x, rug_y = np.zeros_like(values) - rug_space, values\n        else:\n            ax.set_ylim(0, auto=True)\n            rug_x, rug_y = values, np.zeros_like(values) - rug_space\n\n        if rug:\n            ax.plot(rug_x, rug_y, **rug_kwargs)\n\n        if quantiles is not None:\n            fill_kwargs.setdefault(\"alpha\", 0.75)\n\n            idx = [np.sum(density_q < quant) for quant in quantiles]\n\n            fill_func(\n                fill_x,\n                fill_y,\n                where=np.isin(fill_x, fill_x[idx], invert=True, assume_unique=True),\n                **fill_kwargs\n            )\n        else:\n            fill_kwargs.setdefault(\"alpha\", 0)\n            ax.plot(x, density, label=label, **plot_kwargs)\n            fill_func(fill_x, fill_y, **fill_kwargs)\n\n        if legend and label:\n            legend_element = [Patch(edgecolor=default_color, label=label)]\n            ax.legend(handles=legend_element)\n    else:\n        if contour_kwargs is None:\n            contour_kwargs = {}\n        contour_kwargs.setdefault(\"colors\", \"0.5\")\n        if contourf_kwargs is None:\n            contourf_kwargs = {}\n        if pcolormesh_kwargs is None:\n            pcolormesh_kwargs = {}\n\n        gridsize = (128, 128) if contour else (256, 256)\n\n        density, xmin, xmax, ymin, ymax = _fast_kde_2d(values, values2, gridsize=gridsize)\n        g_s = complex(gridsize[0])\n        x_x, y_y = np.mgrid[xmin:xmax:g_s, ymin:ymax:g_s]\n\n        ax.grid(False)\n        ax.set_xlim(xmin, xmax)\n        ax.set_ylim(ymin, ymax)\n        if contour:\n            qcfs = ax.contourf(x_x, y_y, density, antialiased=True, **contourf_kwargs)\n            qcs = ax.contour(x_x, y_y, density, **contour_kwargs)\n            if not fill_last:\n                qcfs.collections[0].set_alpha(0)\n                qcs.collections[0].set_alpha(0)\n        else:\n            ax.pcolormesh(x_x, y_y, density, **pcolormesh_kwargs)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _fast_kde(x, cumulative=False, bw=4.5, xmin=None, xmax=None):\n    x = np.asarray(x, dtype=float)\n    x = x[np.isfinite(x)]\n    if x.size == 0:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    len_x = len(x)\n    n_points = 200 if (xmin or xmax) is None else 500\n\n    if xmin is None:\n        xmin = np.min(x)\n    if xmax is None:\n        xmax = np.max(x)\n\n    assert np.min(x) >= xmin\n    assert np.max(x) <= xmax\n\n    log_len_x = np.log(len_x) * bw\n\n    n_bins = min(int(len_x ** (1 / 3) * log_len_x * 2), n_points)\n    if n_bins < 2:\n        warnings.warn(\"kde plot failed, you may want to check your data\")\n        return np.array([np.nan]), np.nan, np.nan\n\n    d_x = (xmax - xmin) / (n_bins - 1)\n    grid = _histogram(x, n_bins, range_hist=(xmin, xmax))\n\n    scotts_factor = len_x ** (-0.2)\n    kern_nx = int(scotts_factor * 2 * np.pi * log_len_x)\n    kernel = gaussian(kern_nx, scotts_factor * log_len_x)\n\n    npad = min(n_bins, 2 * kern_nx)\n    grid = np.concatenate([grid[npad:0:-1], grid, grid[n_bins : n_bins - npad : -1]])\n    density = convolve(grid, kernel, mode=\"same\", method=\"direct\")[npad : npad + n_bins]\n    norm_factor = len_x * d_x * (2 * np.pi * log_len_x ** 2 * scotts_factor ** 2) ** 0.5\n\n    density /= norm_factor\n\n    if cumulative:\n        density = density.cumsum() / density.sum()\n\n    return density, xmin, xmax", "response": "Fast Fourier transform - based Gaussian kernel density estimate."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nhandles var_names input across arviz.", "response": "def _var_names(var_names, data):\n    \"\"\"Handle var_names input across arviz.\n\n    Parameters\n    ----------\n    var_names: str, list, or None\n    data : xarray.Dataset\n        Posterior data in an xarray\n    Returns\n    -------\n    var_name: list or None\n    \"\"\"\n    if var_names is not None:\n\n        if isinstance(var_names, str):\n            var_names = [var_names]\n\n        if isinstance(data, (list, tuple)):\n            all_vars = []\n            for dataset in data:\n                dataset_vars = list(dataset.data_vars)\n                for var in dataset_vars:\n                    if var not in all_vars:\n                        all_vars.append(var)\n        else:\n            all_vars = list(data.data_vars)\n\n        excluded_vars = [i[1:] for i in var_names if i.startswith(\"~\") and i not in all_vars]\n\n        all_vars_tilde = [i for i in all_vars if i.startswith(\"~\")]\n\n        if all_vars_tilde:\n            warnings.warn(\n                \"\"\"ArviZ treats '~' as a negation character for variable selection.\n                   Your model has variables names starting with '~', {0}. Please double check\n                   your results to ensure all variables are included\"\"\".format(\n                    \", \".join(all_vars_tilde)\n                )\n            )\n\n        if excluded_vars:\n            var_names = [i for i in all_vars if i not in excluded_vars]\n\n    return var_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses numba s jit decorator if numba is installed.", "response": "def conditional_jit(function=None, **kwargs):  # noqa: D202\n    \"\"\"Use numba's jit decorator if numba is installed.\n\n    Notes\n    -----\n        If called without arguments  then return wrapped function.\n\n        @conditional_jit\n        def my_func():\n            return\n\n        else called with arguments\n\n        @conditional_jit(nopython=True)\n        def my_func():\n            return\n\n    \"\"\"\n\n    def wrapper(function):\n        try:\n            numba = importlib.import_module(\"numba\")\n            return numba.jit(**kwargs)(function)\n\n        except ImportError:\n            return function\n\n    if function:\n        return wrapper(function)\n    else:\n        return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nplotting a scatter or hexbin matrix of the sampled parameters.", "response": "def plot_pair(\n    data,\n    var_names=None,\n    coords=None,\n    figsize=None,\n    textsize=None,\n    kind=\"scatter\",\n    gridsize=\"auto\",\n    contour=True,\n    fill_last=True,\n    divergences=False,\n    colorbar=False,\n    ax=None,\n    divergences_kwargs=None,\n    plot_kwargs=None,\n):\n    \"\"\"\n    Plot a scatter or hexbin matrix of the sampled parameters.\n\n    Parameters\n    ----------\n    data : obj\n        Any object that can be converted to an az.InferenceData object\n        Refer to documentation of az.convert_to_dataset for details\n    var_names : list of variable names\n        Variables to be plotted, if None all variable are plotted\n    coords : mapping, optional\n        Coordinates of var_names to be plotted. Passed to `Dataset.sel`\n    figsize : figure size tuple\n        If None, size is (8 + numvars, 8 + numvars)\n    textsize: int\n        Text size for labels. If None it will be autoscaled based on figsize.\n    kind : str\n        Type of plot to display (kde or hexbin)\n    gridsize : int or (int, int), optional\n        Only works for kind=hexbin.\n        The number of hexagons in the x-direction. The corresponding number of hexagons in the\n        y-direction is chosen such that the hexagons are approximately regular.\n        Alternatively, gridsize can be a tuple with two elements specifying the number of hexagons\n        in the x-direction and the y-direction.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE. Defaults to True.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to True.\n    divergences : Boolean\n        If True divergences will be plotted in a different color\n    colorbar : bool\n        If True a colorbar will be included as part of the plot (Defaults to False).\n        Only works when kind=hexbin\n    ax: axes\n        Matplotlib axes\n    divergences_kwargs : dicts, optional\n        Additional keywords passed to ax.scatter for divergences\n    plot_kwargs : dicts, optional\n        Additional keywords passed to ax.plot, az.plot_kde or ax.hexbin\n    Returns\n    -------\n    ax : matplotlib axes\n\n    Examples\n    --------\n    KDE Pair Plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> centered = az.load_arviz_data('centered_eight')\n        >>> coords = {'school': ['Choate', 'Deerfield']}\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu', 'tau'],\n        >>>             kind='kde',\n        >>>             coords=coords,\n        >>>             divergences=True,\n        >>>             textsize=18)\n\n    Hexbin pair plot\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        >>>             var_names=['theta', 'mu'],\n        >>>             coords=coords,\n        >>>             textsize=18,\n        >>>             kind='hexbin')\n\n    Pair plot showing divergences\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_pair(centered,\n        ...             var_names=['theta', 'mu', 'tau'],\n        ...             coords=coords,\n        ...             divergences=True,\n        ...             textsize=18)\n    \"\"\"\n    valid_kinds = [\"scatter\", \"kde\", \"hexbin\"]\n    if kind not in valid_kinds:\n        raise ValueError(\n            (\"Plot type {} not recognized.\" \"Plot type must be in {}\").format(kind, valid_kinds)\n        )\n\n    if coords is None:\n        coords = {}\n\n    if plot_kwargs is None:\n        plot_kwargs = {}\n\n    if kind == \"scatter\":\n        plot_kwargs.setdefault(\"marker\", \".\")\n        plot_kwargs.setdefault(\"lw\", 0)\n\n    if divergences_kwargs is None:\n        divergences_kwargs = {}\n\n    divergences_kwargs.setdefault(\"marker\", \"o\")\n    divergences_kwargs.setdefault(\"markeredgecolor\", \"k\")\n    divergences_kwargs.setdefault(\"color\", \"C1\")\n    divergences_kwargs.setdefault(\"lw\", 0)\n\n    # Get posterior draws and combine chains\n    data = convert_to_inference_data(data)\n    posterior_data = convert_to_dataset(data, group=\"posterior\")\n    var_names = _var_names(var_names, posterior_data)\n    flat_var_names, _posterior = xarray_to_ndarray(\n        get_coords(posterior_data, coords), var_names=var_names, combined=True\n    )\n\n    # Get diverging draws and combine chains\n    if divergences:\n        if hasattr(data, \"sample_stats\") and hasattr(data.sample_stats, \"diverging\"):\n            divergent_data = convert_to_dataset(data, group=\"sample_stats\")\n            _, diverging_mask = xarray_to_ndarray(\n                divergent_data, var_names=(\"diverging\",), combined=True\n            )\n            diverging_mask = np.squeeze(diverging_mask)\n        else:\n            divergences = False\n            warnings.warn(\n                \"Divergences data not found, plotting without divergences. \"\n                \"Make sure the sample method provides divergences data and \"\n                \"that it is present in the `diverging` field of `sample_stats` \"\n                \"or set divergences=False\",\n                SyntaxWarning,\n            )\n\n    if gridsize == \"auto\":\n        gridsize = int(len(_posterior[0]) ** 0.35)\n\n    numvars = len(flat_var_names)\n\n    if numvars < 2:\n        raise Exception(\"Number of variables to be plotted must be 2 or greater.\")\n\n    if numvars == 2:\n        (figsize, ax_labelsize, _, xt_labelsize, _, _) = _scale_fig_size(\n            figsize, textsize, numvars - 1, numvars - 1\n        )\n\n        if ax is None:\n            fig, ax = plt.subplots(figsize=figsize, constrained_layout=True)\n\n        if kind == \"scatter\":\n            ax.plot(_posterior[0], _posterior[1], **plot_kwargs)\n        elif kind == \"kde\":\n            plot_kde(\n                _posterior[0],\n                _posterior[1],\n                contour=contour,\n                fill_last=fill_last,\n                ax=ax,\n                **plot_kwargs\n            )\n        else:\n            hexbin = ax.hexbin(\n                _posterior[0], _posterior[1], mincnt=1, gridsize=gridsize, **plot_kwargs\n            )\n            ax.grid(False)\n\n        if kind == \"hexbin\" and colorbar:\n            cbar = ax.figure.colorbar(hexbin, ticks=[hexbin.norm.vmin, hexbin.norm.vmax], ax=ax)\n            cbar.ax.set_yticklabels([\"low\", \"high\"], fontsize=ax_labelsize)\n\n        if divergences:\n            ax.plot(\n                _posterior[0][diverging_mask], _posterior[1][diverging_mask], **divergences_kwargs\n            )\n\n        ax.set_xlabel(\"{}\".format(flat_var_names[0]), fontsize=ax_labelsize, wrap=True)\n        ax.set_ylabel(\"{}\".format(flat_var_names[1]), fontsize=ax_labelsize, wrap=True)\n        ax.tick_params(labelsize=xt_labelsize)\n\n    else:\n        (figsize, ax_labelsize, _, xt_labelsize, _, _) = _scale_fig_size(\n            figsize, textsize, numvars - 2, numvars - 2\n        )\n\n        if ax is None:\n            fig, ax = plt.subplots(\n                numvars - 1, numvars - 1, figsize=figsize, constrained_layout=True\n            )\n        hexbin_values = []\n        for i in range(0, numvars - 1):\n            var1 = _posterior[i]\n\n            for j in range(0, numvars - 1):\n                if j < i:\n                    ax[j, i].axis(\"off\")\n                    continue\n\n                var2 = _posterior[j + 1]\n\n                if kind == \"scatter\":\n                    ax[j, i].plot(var1, var2, **plot_kwargs)\n\n                elif kind == \"kde\":\n                    plot_kde(\n                        var1, var2, contour=contour, fill_last=fill_last, ax=ax[j, i], **plot_kwargs\n                    )\n\n                else:\n                    ax[j, i].grid(False)\n                    hexbin = ax[j, i].hexbin(var1, var2, mincnt=1, gridsize=gridsize, **plot_kwargs)\n                if kind == \"hexbin\" and colorbar:\n                    hexbin_values.append(hexbin.norm.vmin)\n                    hexbin_values.append(hexbin.norm.vmax)\n                    if j == i == 0 and colorbar:\n                        divider = make_axes_locatable(ax[0, 1])\n                        cax = divider.append_axes(\"left\", size=\"7%\")\n                        cbar = fig.colorbar(\n                            hexbin, ticks=[hexbin.norm.vmin, hexbin.norm.vmax], cax=cax\n                        )\n                        cbar.ax.set_yticklabels([\"low\", \"high\"], fontsize=ax_labelsize)\n\n                if divergences:\n                    ax[j, i].plot(var1[diverging_mask], var2[diverging_mask], **divergences_kwargs)\n\n                if j + 1 != numvars - 1:\n                    ax[j, i].axes.get_xaxis().set_major_formatter(NullFormatter())\n                else:\n                    ax[j, i].set_xlabel(\n                        \"{}\".format(flat_var_names[i]), fontsize=ax_labelsize, wrap=True\n                    )\n                if i != 0:\n                    ax[j, i].axes.get_yaxis().set_major_formatter(NullFormatter())\n                else:\n                    ax[j, i].set_ylabel(\n                        \"{}\".format(flat_var_names[j + 1]), fontsize=ax_labelsize, wrap=True\n                    )\n\n                ax[j, i].tick_params(labelsize=xt_labelsize)\n\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef extract_docstring(self):\n        lines = open(self.filename).readlines()\n        start_row = 0\n        if lines[0].startswith('#!'):\n            lines.pop(0)\n            start_row = 1\n\n        docstring = ''\n        first_par = ''\n        line_iter = lines.__iter__()\n        tokens = tokenize.generate_tokens(lambda: next(line_iter))\n        for tok_type, tok_content, _, (erow, _), _ in tokens:\n            tok_type = token.tok_name[tok_type]\n            if tok_type in ('NEWLINE', 'COMMENT', 'NL', 'INDENT', 'DEDENT'):\n                continue\n            elif tok_type == 'STRING':\n                docstring = eval(tok_content)\n                # If the docstring is formatted with several paragraphs,\n                # extract the first one:\n                paragraphs = '\\n'.join(line.rstrip()\n                                       for line in docstring.split('\\n')\n                                       ).split('\\n\\n')\n                if len(paragraphs) > 0:\n                    first_par = paragraphs[0]\n            break\n\n        thumbloc = None\n        for i, line in enumerate(docstring.split(\"\\n\")):\n            m = re.match(r\"^_thumb: (\\.\\d+),\\s*(\\.\\d+)\", line)\n            if m:\n                thumbloc = float(m.group(1)), float(m.group(2))\n                break\n        if thumbloc is not None:\n            self.thumbloc = thumbloc\n            docstring = \"\\n\".join([l for l in docstring.split(\"\\n\")\n                                   if not l.startswith(\"_thumb\")])\n\n        self.docstring = docstring\n        self.short_desc = first_par\n        self.end_line = erow + 1 + start_row", "response": "Extracts a module - level docstring from the file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplot a 2D or 3D histogram or kernel density estimates for a given set of values.", "response": "def plot_dist(\n    values,\n    values2=None,\n    color=\"C0\",\n    kind=\"auto\",\n    cumulative=False,\n    label=None,\n    rotated=False,\n    rug=False,\n    bw=4.5,\n    quantiles=None,\n    contour=True,\n    fill_last=True,\n    textsize=None,\n    plot_kwargs=None,\n    fill_kwargs=None,\n    rug_kwargs=None,\n    contour_kwargs=None,\n    hist_kwargs=None,\n    ax=None,\n):\n    \"\"\"Plot distribution as histogram or kernel density estimates.\n\n    By default continuous variables are plotted using KDEs and discrete ones using histograms\n\n    Parameters\n    ----------\n    values : array-like\n        Values to plot\n    values2 : array-like, optional\n        Values to plot. If present, a 2D KDE or a hexbin will be estimated\n    color : string\n        valid matplotlib color\n    kind : string\n        By default (\"auto\") continuous variables are plotted using KDEs and discrete ones using\n        histograms. To override this use \"hist\" to plot histograms and \"density\" for KDEs\n    cumulative : bool\n        If true plot the estimated cumulative distribution function. Defaults to False.\n        Ignored for 2D KDE\n    label : string\n        Text to include as part of the legend\n    rotated : bool\n        Whether to rotate the 1D KDE plot 90 degrees.\n    rug : bool\n        If True adds a rugplot. Defaults to False. Ignored for 2D KDE\n    bw : float\n        Bandwidth scaling factor for 1D KDE. Should be larger than 0. The higher this number the\n        smoother the KDE will be. Defaults to 4.5 which is essentially the same as the Scott's\n        rule of thumb (the default rule used by SciPy).\n    quantiles : list\n        Quantiles in ascending order used to segment the KDE. Use [.25, .5, .75] for quartiles.\n        Defaults to None.\n    contour : bool\n        If True plot the 2D KDE using contours, otherwise plot a smooth 2D KDE. Defaults to True.\n    fill_last : bool\n        If True fill the last contour of the 2D KDE plot. Defaults to True.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be autoscaled based\n        on figsize.\n    plot_kwargs : dict\n        Keywords passed to the pdf line of a 1D KDE.\n    fill_kwargs : dict\n        Keywords passed to the fill under the line (use fill_kwargs={'alpha': 0} to disable fill).\n        Ignored for 2D KDE\n    rug_kwargs : dict\n        Keywords passed to the rug plot. Ignored if rug=False or for 2D KDE\n        Use `space` keyword (float) to control the position of the rugplot. The larger this number\n        the lower the rugplot.\n    contour_kwargs : dict\n        Keywords passed to the contourplot. Ignored for 1D KDE.\n    hist_kwargs : dict\n        Keywords passed to the histogram.\n    ax : matplotlib axes\n\n    Returns\n    -------\n    ax : matplotlib axes\n    \"\"\"\n    if ax is None:\n        ax = plt.gca()\n\n    if hist_kwargs is None:\n        hist_kwargs = {}\n    hist_kwargs.setdefault(\"bins\", None)\n    hist_kwargs.setdefault(\"cumulative\", cumulative)\n    hist_kwargs.setdefault(\"color\", color)\n    hist_kwargs.setdefault(\"label\", label)\n    hist_kwargs.setdefault(\"rwidth\", 0.9)\n    hist_kwargs.setdefault(\"align\", \"left\")\n    hist_kwargs.setdefault(\"density\", True)\n\n    if plot_kwargs is None:\n        plot_kwargs = {}\n\n    if rotated:\n        hist_kwargs.setdefault(\"orientation\", \"horizontal\")\n    else:\n        hist_kwargs.setdefault(\"orientation\", \"vertical\")\n\n    if kind == \"auto\":\n        kind = \"hist\" if values.dtype.kind == \"i\" else \"density\"\n\n    if kind == \"hist\":\n        _histplot_op(\n            values=values, values2=values2, rotated=rotated, ax=ax, hist_kwargs=hist_kwargs\n        )\n    elif kind == \"density\":\n        plot_kwargs.setdefault(\"color\", color)\n        legend = label is not None\n\n        plot_kde(\n            values,\n            values2,\n            cumulative=cumulative,\n            rug=rug,\n            label=label,\n            bw=bw,\n            quantiles=quantiles,\n            rotated=rotated,\n            contour=contour,\n            legend=legend,\n            fill_last=fill_last,\n            textsize=textsize,\n            plot_kwargs=plot_kwargs,\n            fill_kwargs=fill_kwargs,\n            rug_kwargs=rug_kwargs,\n            contour_kwargs=contour_kwargs,\n            ax=ax,\n        )\n    return ax"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _histplot_op(values, values2, rotated, ax, hist_kwargs):\n    if values2 is not None:\n        raise NotImplementedError(\"Insert hexbin plot here\")\n\n    bins = hist_kwargs.pop(\"bins\")\n    if bins is None:\n        bins = get_bins(values)\n    ax.hist(values, bins=bins, **hist_kwargs)\n    if rotated:\n        ax.set_yticks(bins[:-1])\n    else:\n        ax.set_xticks(bins[:-1])\n    if hist_kwargs[\"label\"] is not None:\n        ax.legend()\n    return ax", "response": "Add a histogram for the data to the axes."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nplotting posterior predictive checks for a set of observed variables.", "response": "def plot_ppc(\n    data,\n    kind=\"density\",\n    alpha=None,\n    mean=True,\n    figsize=None,\n    textsize=None,\n    data_pairs=None,\n    var_names=None,\n    coords=None,\n    flatten=None,\n    flatten_pp=None,\n    num_pp_samples=None,\n    random_seed=None,\n    jitter=None,\n    animated=False,\n    animation_kwargs=None,\n    legend=True,\n):\n    \"\"\"\n    Plot for posterior predictive checks.\n\n    Parameters\n    ----------\n    data : az.InferenceData object\n        InferenceData object containing the observed and posterior\n        predictive data.\n    kind : str\n        Type of plot to display (density, cumulative, or scatter). Defaults to density.\n    alpha : float\n        Opacity of posterior predictive density curves. Defaults to 0.2 for kind = density\n        and cumulative, for scatter defaults to 0.7\n    mean : bool\n        Whether or not to plot the mean posterior predictive distribution. Defaults to True\n    figsize : tuple\n        Figure size. If None it will be defined automatically.\n    textsize: float\n        Text size scaling factor for labels, titles and lines. If None it will be\n        autoscaled based on figsize.\n    data_pairs : dict\n        Dictionary containing relations between observed data and posterior predictive data.\n        Dictionary structure:\n        Key = data var_name\n        Value = posterior predictive var_name\n        For example, `data_pairs = {'y' : 'y_hat'}`\n        If None, it will assume that the observed data and the posterior\n        predictive data have the same variable name.\n    var_names : list\n        List of variables to be plotted. Defaults to all observed variables in the\n        model if None.\n    coords : dict\n        Dictionary mapping dimensions to selected coordinates to be plotted.\n        Dimensions without a mapping specified will include all coordinates for\n        that dimension. Defaults to including all coordinates for all\n        dimensions if None.\n    flatten : list\n        List of dimensions to flatten in observed_data. Only flattens across the coordinates\n        specified in the coords argument. Defaults to flattening all of the dimensions.\n    flatten_pp : list\n        List of dimensions to flatten in posterior_predictive. Only flattens across the coordinates\n        specified in the coords argument. Defaults to flattening all of the dimensions.\n        Dimensions should match flatten excluding dimensions for data_pairs parameters.\n        If flatten is defined and flatten_pp is None, then `flatten_pp=flatten`.\n    num_pp_samples : int\n        The number of posterior predictive samples to plot. For `kind` = 'scatter' and\n        `animation = False` if defaults to a maximum of 5 samples and will set jitter to 0.7\n        unless defined otherwise. Otherwise it defaults to all provided samples.\n    random_seed : int\n        Random number generator seed passed to numpy.random.seed to allow\n        reproducibility of the plot. By default, no seed will be provided\n        and the plot will change each call if a random sample is specified\n        by `num_pp_samples`.\n    jitter : float\n        If kind is \"scatter\", jitter will add random uniform noise to the height\n        of the ppc samples and observed data. By default 0.\n    animated : bool\n        Create an animation of one posterior predictive sample per frame. Defaults to False.\n    animation_kwargs : dict\n        Keywords passed to `animation.FuncAnimation`.\n    legend : bool\n        Add legend to figure. By default True.\n\n    Returns\n    -------\n    axes : matplotlib axes\n\n    Examples\n    --------\n    Plot the observed data KDE overlaid on posterior predictive KDEs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> import arviz as az\n        >>> data = az.load_arviz_data('radon')\n        >>> az.plot_ppc(data)\n\n    Plot the overlay with empirical CDFs.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='cumulative')\n\n    Use the coords and flatten parameters to plot selected variable dimensions\n    across multiple plots.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, coords={'observed_county': ['ANOKA', 'BELTRAMI']}, flatten=[])\n\n    Plot the overlay using a stacked scatter plot that is particularly useful\n    when the sample sizes are small.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, kind='scatter', flatten=[],\n        >>>             coords={'observed_county': ['AITKIN', 'BELTRAMI']})\n\n    Plot random posterior predictive sub-samples.\n\n    .. plot::\n        :context: close-figs\n\n        >>> az.plot_ppc(data, num_pp_samples=30, random_seed=7)\n    \"\"\"\n    for group in (\"posterior_predictive\", \"observed_data\"):\n        if not hasattr(data, group):\n            raise TypeError(\n                '`data` argument must have the group \"{group}\" for ppcplot'.format(group=group)\n            )\n\n    if kind.lower() not in (\"density\", \"cumulative\", \"scatter\"):\n        raise TypeError(\"`kind` argument must be either `density`, `cumulative`, or `scatter`\")\n\n    if data_pairs is None:\n        data_pairs = {}\n\n    if animation_kwargs is None:\n        animation_kwargs = {}\n    if platform.system() == \"Linux\":\n        animation_kwargs.setdefault(\"blit\", True)\n    else:\n        animation_kwargs.setdefault(\"blit\", False)\n\n    if animated and animation_kwargs[\"blit\"] and platform.system() != \"Linux\":\n        _log.warning(\n            \"If you experience problems rendering the animation try setting\"\n            \"`animation_kwargs({'blit':False}) or changing the plotting backend (e.g. to TkAgg)\"\n        )\n\n    if alpha is None:\n        if animated:\n            alpha = 1\n        else:\n            if kind.lower() == \"scatter\":\n                alpha = 0.7\n            else:\n                alpha = 0.2\n\n    if jitter is None:\n        jitter = 0.0\n    assert jitter >= 0.0\n\n    observed = data.observed_data\n    posterior_predictive = data.posterior_predictive\n\n    if var_names is None:\n        var_names = observed.data_vars\n    var_names = _var_names(var_names, observed)\n    pp_var_names = [data_pairs.get(var, var) for var in var_names]\n\n    if flatten_pp is None and flatten is None:\n        flatten_pp = list(posterior_predictive.dims.keys())\n    elif flatten_pp is None:\n        flatten_pp = flatten\n    if flatten is None:\n        flatten = list(observed.dims.keys())\n\n    if coords is None:\n        coords = {}\n\n    if random_seed is not None:\n        np.random.seed(random_seed)\n\n    total_pp_samples = posterior_predictive.sizes[\"chain\"] * posterior_predictive.sizes[\"draw\"]\n    if num_pp_samples is None:\n        if kind == \"scatter\" and not animated:\n            num_pp_samples = min(5, total_pp_samples)\n        else:\n            num_pp_samples = total_pp_samples\n\n    if (\n        not isinstance(num_pp_samples, Integral)\n        or num_pp_samples < 1\n        or num_pp_samples > total_pp_samples\n    ):\n        raise TypeError(\n            \"`num_pp_samples` must be an integer between 1 and \"\n            + \"{limit}.\".format(limit=total_pp_samples)\n        )\n\n    pp_sample_ix = np.random.choice(total_pp_samples, size=num_pp_samples, replace=False)\n\n    for key in coords.keys():\n        coords[key] = np.where(np.in1d(observed[key], coords[key]))[0]\n\n    obs_plotters = list(\n        xarray_var_iter(\n            observed.isel(coords), skip_dims=set(flatten), var_names=var_names, combined=True\n        )\n    )\n    pp_plotters = list(\n        xarray_var_iter(\n            posterior_predictive.isel(coords),\n            var_names=pp_var_names,\n            skip_dims=set(flatten_pp),\n            combined=True,\n        )\n    )\n    length_plotters = len(obs_plotters)\n    rows, cols = default_grid(length_plotters)\n\n    (figsize, ax_labelsize, _, xt_labelsize, linewidth, markersize) = _scale_fig_size(\n        figsize, textsize, rows, cols\n    )\n\n    fig, axes = _create_axes_grid(length_plotters, rows, cols, figsize=figsize)\n\n    for i, ax in enumerate(axes):\n        var_name, selection, obs_vals = obs_plotters[i]\n        pp_var_name, _, pp_vals = pp_plotters[i]\n        dtype = posterior_predictive[pp_var_name].dtype.kind\n\n        # flatten non-specified dimensions\n        obs_vals = obs_vals.flatten()\n        pp_vals = pp_vals.reshape(total_pp_samples, -1)\n        pp_sampled_vals = pp_vals[pp_sample_ix]\n\n        if kind == \"density\":\n            plot_kwargs = {\"color\": \"C5\", \"alpha\": alpha, \"linewidth\": 0.5 * linewidth}\n            if dtype == \"i\":\n                plot_kwargs[\"drawstyle\"] = \"steps-pre\"\n            ax.plot([], color=\"C5\", label=\"Posterior predictive {}\".format(pp_var_name))\n\n            if dtype == \"f\":\n                plot_kde(\n                    obs_vals,\n                    label=\"Observed {}\".format(var_name),\n                    plot_kwargs={\"color\": \"k\", \"linewidth\": linewidth, \"zorder\": 3},\n                    fill_kwargs={\"alpha\": 0},\n                    ax=ax,\n                    legend=legend,\n                )\n            else:\n                nbins = round(len(obs_vals) ** 0.5)\n                hist, bin_edges = np.histogram(obs_vals, bins=nbins, density=True)\n                hist = np.concatenate((hist[:1], hist))\n                ax.plot(\n                    bin_edges,\n                    hist,\n                    label=\"Observed {}\".format(var_name),\n                    color=\"k\",\n                    linewidth=linewidth,\n                    zorder=3,\n                    drawstyle=plot_kwargs[\"drawstyle\"],\n                )\n\n            if animated:\n                animate, init = _set_animation(\n                    pp_sampled_vals, ax, dtype=dtype, kind=kind, plot_kwargs=plot_kwargs\n                )\n\n            else:\n                # run plot_kde manually with one plot call\n                pp_densities = []\n                for vals in pp_sampled_vals:\n                    vals = np.array([vals]).flatten()\n                    if dtype == \"f\":\n                        pp_density, lower, upper = _fast_kde(vals)\n                        pp_x = np.linspace(lower, upper, len(pp_density))\n                        pp_densities.extend([pp_x, pp_density])\n                    else:\n                        nbins = round(len(vals) ** 0.5)\n                        hist, bin_edges = np.histogram(vals, bins=nbins, density=True)\n                        hist = np.concatenate((hist[:1], hist))\n                        pp_densities.extend([bin_edges, hist])\n\n                ax.plot(*pp_densities, **plot_kwargs)\n\n            if mean:\n                if dtype == \"f\":\n                    plot_kde(\n                        pp_vals.flatten(),\n                        plot_kwargs={\n                            \"color\": \"C0\",\n                            \"linestyle\": \"--\",\n                            \"linewidth\": linewidth,\n                            \"zorder\": 2,\n                        },\n                        label=\"Posterior predictive mean {}\".format(pp_var_name),\n                        ax=ax,\n                        legend=legend,\n                    )\n                else:\n                    vals = pp_vals.flatten()\n                    nbins = round(len(vals) ** 0.5)\n                    hist, bin_edges = np.histogram(vals, bins=nbins, density=True)\n                    hist = np.concatenate((hist[:1], hist))\n                    ax.plot(\n                        bin_edges,\n                        hist,\n                        color=\"C0\",\n                        linewidth=linewidth,\n                        label=\"Posterior predictive mean {}\".format(pp_var_name),\n                        zorder=2,\n                        linestyle=\"--\",\n                        drawstyle=plot_kwargs[\"drawstyle\"],\n                    )\n            ax.tick_params(labelsize=xt_labelsize)\n            ax.set_yticks([])\n\n        elif kind == \"cumulative\":\n            drawstyle = \"default\" if dtype == \"f\" else \"steps-pre\"\n            ax.plot(\n                *_empirical_cdf(obs_vals),\n                color=\"k\",\n                linewidth=linewidth,\n                label=\"Observed {}\".format(var_name),\n                drawstyle=drawstyle,\n                zorder=3\n            )\n            if animated:\n                animate, init = _set_animation(\n                    pp_sampled_vals,\n                    ax,\n                    kind=kind,\n                    alpha=alpha,\n                    drawstyle=drawstyle,\n                    linewidth=linewidth,\n                )\n\n            else:\n                # run plot_kde manually with one plot call\n                pp_densities = []\n                for vals in pp_sampled_vals:\n                    vals = np.array([vals]).flatten()\n                    pp_x, pp_density = _empirical_cdf(vals)\n                    pp_densities.extend([pp_x, pp_density])\n\n                ax.plot(\n                    *pp_densities, alpha=alpha, color=\"C5\", drawstyle=drawstyle, linewidth=linewidth\n                )\n            ax.plot([], color=\"C5\", label=\"Posterior predictive {}\".format(pp_var_name))\n            if mean:\n                ax.plot(\n                    *_empirical_cdf(pp_vals.flatten()),\n                    color=\"C0\",\n                    linestyle=\"--\",\n                    linewidth=linewidth,\n                    drawstyle=drawstyle,\n                    label=\"Posterior predictive mean {}\".format(pp_var_name)\n                )\n            ax.set_yticks([0, 0.5, 1])\n\n        elif kind == \"scatter\":\n            if mean:\n                if dtype == \"f\":\n                    plot_kde(\n                        pp_vals.flatten(),\n                        plot_kwargs={\n                            \"color\": \"C0\",\n                            \"linestyle\": \"--\",\n                            \"linewidth\": linewidth,\n                            \"zorder\": 3,\n                        },\n                        label=\"Posterior predictive mean {}\".format(pp_var_name),\n                        ax=ax,\n                        legend=legend,\n                    )\n                else:\n                    vals = pp_vals.flatten()\n                    nbins = round(len(vals) ** 0.5)\n                    hist, bin_edges = np.histogram(vals, bins=nbins, density=True)\n                    hist = np.concatenate((hist[:1], hist))\n                    ax.plot(\n                        bin_edges,\n                        hist,\n                        color=\"C0\",\n                        linewidth=linewidth,\n                        label=\"Posterior predictive mean {}\".format(pp_var_name),\n                        zorder=3,\n                        linestyle=\"--\",\n                        drawstyle=\"steps-pre\",\n                    )\n\n            _, limit = ax.get_ylim()\n            limit *= 1.05\n            y_rows = np.linspace(0, limit, num_pp_samples + 1)\n            jitter_scale = y_rows[1] - y_rows[0]\n            scale_low = 0\n            scale_high = jitter_scale * jitter\n\n            obs_yvals = np.zeros_like(obs_vals, dtype=np.float64)\n            if jitter:\n                obs_yvals += np.random.uniform(low=scale_low, high=scale_high, size=len(obs_vals))\n            ax.plot(\n                obs_vals,\n                obs_yvals,\n                \"o\",\n                color=\"C0\",\n                markersize=markersize,\n                alpha=alpha,\n                label=\"Observed {}\".format(var_name),\n                zorder=4,\n            )\n\n            if animated:\n                animate, init = _set_animation(\n                    pp_sampled_vals,\n                    ax,\n                    kind=kind,\n                    height=y_rows.mean() * 0.5,\n                    markersize=markersize,\n                )\n\n            else:\n                for vals, y in zip(pp_sampled_vals, y_rows[1:]):\n                    vals = np.ravel(vals)\n                    yvals = np.full_like(vals, y, dtype=np.float64)\n                    if jitter:\n                        yvals += np.random.uniform(low=scale_low, high=scale_high, size=len(vals))\n                    ax.plot(\n                        vals, yvals, \"o\", zorder=2, color=\"C5\", markersize=markersize, alpha=alpha\n                    )\n\n            ax.plot([], \"C5o\", label=\"Posterior predictive {}\".format(pp_var_name))\n\n            ax.set_yticks([])\n\n        if var_name != pp_var_name:\n            xlabel = \"{} / {}\".format(var_name, pp_var_name)\n        else:\n            xlabel = var_name\n        ax.set_xlabel(make_label(xlabel, selection), fontsize=ax_labelsize)\n\n        if legend:\n            if i == 0:\n                ax.legend(fontsize=xt_labelsize * 0.75)\n            else:\n                ax.legend([])\n\n    if animated:\n        ani = animation.FuncAnimation(\n            fig, animate, np.arange(0, num_pp_samples), init_func=init, **animation_kwargs\n        )\n        return axes, ani\n    else:\n        return axes"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_netcdf(data, filename, *, group=\"posterior\", coords=None, dims=None):\n    inference_data = convert_to_inference_data(data, group=group, coords=coords, dims=dims)\n    file_name = inference_data.to_netcdf(filename)\n    return file_name", "response": "Save data as a netcdf file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave data to a netcdf file.", "response": "def save_data(data, filename, *, group=\"posterior\", coords=None, dims=None):\n    \"\"\"Save dataset as a netcdf file.\n\n    WARNING: Only idempotent in case `data` is InferenceData\n\n    Parameters\n    ----------\n    data : InferenceData, or any object accepted by `convert_to_inference_data`\n        Object to be saved\n    filename : str\n        name or path of the file to load trace\n    group : str (optional)\n        In case `data` is not InferenceData, this is the group it will be saved to\n    coords : dict (optional)\n        See `convert_to_inference_data`\n    dims : dict (optional)\n        See `convert_to_inference_data`\n\n    Returns\n    -------\n    str\n        filename saved to\n\n    Note\n    ----\n    This function is deprecated and will be removed in 0.4.\n    Use `to_netcdf` instead.\n    \"\"\"\n    warnings.warn(\n        \"The 'save_data' function is deprecated as of 0.3.2, use 'to_netcdf' instead\",\n        DeprecationWarning,\n    )\n    return to_netcdf(data=data, filename=filename, group=group, coords=coords, dims=dims)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nconverting any array into a 2d numpy array.", "response": "def make_2d(ary):\n    \"\"\"Convert any array into a 2d numpy array.\n\n    In case the array is already more than 2 dimensional, will ravel the\n    dimensions after the first.\n    \"\"\"\n    dim_0, *_ = np.atleast_1d(ary).shape\n    return ary.reshape(dim_0, -1, order=\"F\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _scale_fig_size(figsize, textsize, rows=1, cols=1):\n    params = mpl.rcParams\n    rc_width, rc_height = tuple(params[\"figure.figsize\"])\n    rc_ax_labelsize = params[\"axes.labelsize\"]\n    rc_titlesize = params[\"axes.titlesize\"]\n    rc_xt_labelsize = params[\"xtick.labelsize\"]\n    rc_linewidth = params[\"lines.linewidth\"]\n    rc_markersize = params[\"lines.markersize\"]\n    if isinstance(rc_ax_labelsize, str):\n        rc_ax_labelsize = 15\n    if isinstance(rc_titlesize, str):\n        rc_titlesize = 16\n    if isinstance(rc_xt_labelsize, str):\n        rc_xt_labelsize = 14\n\n    if figsize is None:\n        width, height = rc_width, rc_height\n        sff = 1 if (rows == cols == 1) else 1.15\n        width = width * cols * sff\n        height = height * rows * sff\n    else:\n        width, height = figsize\n\n    if textsize is not None:\n        scale_factor = textsize / rc_xt_labelsize\n    elif rows == cols == 1:\n        scale_factor = ((width * height) / (rc_width * rc_height)) ** 0.5\n    else:\n        scale_factor = 1\n\n    ax_labelsize = rc_ax_labelsize * scale_factor\n    titlesize = rc_titlesize * scale_factor\n    xt_labelsize = rc_xt_labelsize * scale_factor\n    linewidth = rc_linewidth * scale_factor\n    markersize = rc_markersize * scale_factor\n\n    return (width, height), ax_labelsize, titlesize, xt_labelsize, linewidth, markersize", "response": "Scale the figure properties according to rows and cols."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_bins(values):\n    x_min = values.min().astype(int)\n    x_max = values.max().astype(int)\n\n    # Sturges histogram bin estimator\n    bins_sturges = (x_max - x_min) / (np.log2(values.size) + 1)\n\n    # The Freedman-Diaconis histogram bin estimator.\n    iqr = np.subtract(*np.percentile(values, [75, 25]))  # pylint: disable=assignment-from-no-return\n    bins_fd = 2 * iqr * values.size ** (-1 / 3)\n\n    width = round(np.max([1, bins_sturges, bins_fd])).astype(int)\n\n    return np.arange(x_min, x_max + width + 1, width)", "response": "Compute the number of bins for discrete variables."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef default_grid(n_items, max_cols=4, min_cols=3):  # noqa: D202\n\n    def in_bounds(val):\n        return np.clip(val, min_cols, max_cols)\n\n    if n_items <= max_cols:\n        return 1, n_items\n    ideal = in_bounds(round(n_items ** 0.5))\n\n    for offset in (0, 1, -1, 2, -2):\n        cols = in_bounds(ideal + offset)\n        rows, extra = divmod(n_items, cols)\n        if extra == 0:\n            return rows, cols\n    return n_items // ideal + 1, ideal", "response": "Make a grid for the items of the current node."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_axes_grid(length_plotters, rows, cols, **kwargs):\n    kwargs.setdefault(\"constrained_layout\", True)\n    fig, ax = plt.subplots(rows, cols, **kwargs)\n    ax = np.ravel(ax)\n    extra = (rows * cols) - length_plotters\n    if extra:\n        for i in range(1, extra + 1):\n            ax[-i].set_axis_off()\n        ax = ax[:-extra]\n    return fig, ax", "response": "Create figure and axes for grids with multiple plots."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving duplicates from list while preserving order.", "response": "def purge_duplicates(list_in):\n    \"\"\"Remove duplicates from list while preserving order.\n\n    Parameters\n    ----------\n    list_in: Iterable\n\n    Returns\n    -------\n    list\n        List of first occurences in order\n    \"\"\"\n    _list = []\n    for item in list_in:\n        if item not in _list:\n            _list.append(item)\n    return _list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef xarray_var_iter(data, var_names=None, combined=False, skip_dims=None, reverse_selections=False):\n    if skip_dims is None:\n        skip_dims = set()\n\n    if combined:\n        skip_dims = skip_dims.union({\"chain\", \"draw\"})\n    else:\n        skip_dims.add(\"draw\")\n\n    if var_names is None:\n        if isinstance(data, xr.Dataset):\n            var_names = list(data.data_vars)\n        elif isinstance(data, xr.DataArray):\n            var_names = [data.name]\n            data = {data.name: data}\n\n    for var_name in var_names:\n        if var_name in data:\n            new_dims = [dim for dim in data[var_name].dims if dim not in skip_dims]\n            vals = [purge_duplicates(data[var_name][dim].values) for dim in new_dims]\n            dims = [{k: v for k, v in zip(new_dims, prod)} for prod in product(*vals)]\n            if reverse_selections:\n                dims = reversed(dims)\n\n            for selection in dims:\n                yield var_name, selection, data[var_name].sel(**selection).values", "response": "Convert xarray data to an iterator over vectors."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntaking xarray data and unpacks into variables and data into list and numpy array respectively.", "response": "def xarray_to_ndarray(data, *, var_names=None, combined=True):\n    \"\"\"Take xarray data and unpacks into variables and data into list and numpy array respectively.\n\n    Assumes that chain and draw are in coordinates\n\n    Parameters\n    ----------\n    data: xarray.DataSet\n        Data in an xarray from an InferenceData object. Examples include posterior or sample_stats\n\n    var_names: iter\n        Should be a subset of data.data_vars not including chain and draws. Defaults to all of them\n\n    combined: bool\n        Whether to combine chain into one array\n\n    Returns\n    -------\n    var_names: list\n        List of variable names\n    data: np.array\n        Data values\n    \"\"\"\n    unpacked_data, unpacked_var_names, = [], []\n\n    # Merge chains and variables\n    for var_name, selection, data_array in xarray_var_iter(\n        data, var_names=var_names, combined=combined\n    ):\n        unpacked_data.append(data_array.flatten())\n        unpacked_var_names.append(make_label(var_name, selection))\n\n    return unpacked_var_names, np.array(unpacked_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a supported object to an xarray dataset.", "response": "def convert_to_dataset(obj, *, group=\"posterior\", coords=None, dims=None):\n    \"\"\"Convert a supported object to an xarray dataset.\n\n    This function is idempotent, in that it will return xarray.Dataset functions\n    unchanged. Raises `ValueError` if the desired group can not be extracted.\n\n    Note this goes through a DataInference object. See `convert_to_inference_data`\n    for more details. Raises ValueError if it can not work out the desired\n    conversion.\n\n    Parameters\n    ----------\n    obj : dict, str, np.ndarray, xr.Dataset, pystan fit, pymc3 trace\n        A supported object to convert to InferenceData:\n            InferenceData: returns unchanged\n            str: Attempts to load the netcdf dataset from disk\n            pystan fit: Automatically extracts data\n            pymc3 trace: Automatically extracts data\n            xarray.Dataset: adds to InferenceData as only group\n            dict: creates an xarray dataset as the only group\n            numpy array: creates an xarray dataset as the only group, gives the\n                         array an arbitrary name\n    group : str\n        If `obj` is a dict or numpy array, assigns the resulting xarray\n        dataset to this group.\n    coords : dict[str, iterable]\n        A dictionary containing the values that are used as index. The key\n        is the name of the dimension, the values are the index values.\n    dims : dict[str, List(str)]\n        A mapping from variables to a list of coordinate names for the variable\n\n    Returns\n    -------\n    xarray.Dataset\n    \"\"\"\n    inference_data = convert_to_inference_data(obj, group=group, coords=coords, dims=dims)\n    dataset = getattr(inference_data, group, None)\n    if dataset is None:\n        raise ValueError(\n            \"Can not extract {group} from {obj}! See {filename} for other \"\n            \"conversion utilities.\".format(group=group, obj=obj, filename=__file__)\n        )\n    return dataset"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sturges_formula(dataset, mult=1):\n    return int(np.ceil(mult * np.log2(dataset.draw.size)) + 1)", "response": "Use Sturges formula to determine number of bins."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_rank(data, var_names=None, coords=None, bins=None, ref_line=True, figsize=None, axes=None):\n    posterior_data = convert_to_dataset(data, group=\"posterior\")\n    if coords is not None:\n        posterior_data = posterior_data.sel(**coords)\n    var_names = _var_names(var_names, posterior_data)\n    plotters = list(xarray_var_iter(posterior_data, var_names=var_names, combined=True))\n\n    if bins is None:\n        # Use double Sturges' formula\n        bins = _sturges_formula(posterior_data, mult=2)\n\n    if axes is None:\n        rows, cols = default_grid(len(plotters))\n\n        figsize, ax_labelsize, titlesize, _, _, _ = _scale_fig_size(\n            figsize, None, rows=rows, cols=cols\n        )\n        _, axes = _create_axes_grid(len(plotters), rows, cols, figsize=figsize, squeeze=False)\n\n    for ax, (var_name, selection, var_data) in zip(axes.ravel(), plotters):\n        ranks = scipy.stats.rankdata(var_data).reshape(var_data.shape)\n        all_counts = []\n        for row in ranks:\n            counts, bin_ary = np.histogram(row, bins=bins, range=(0, ranks.size))\n            all_counts.append(counts)\n        all_counts = np.array(all_counts)\n        gap = all_counts.max() * 1.05\n        width = bin_ary[1] - bin_ary[0]\n\n        # Center the bins\n        bin_ary = (bin_ary[1:] + bin_ary[:-1]) / 2\n\n        y_ticks = []\n        for idx, counts in enumerate(all_counts):\n            y_ticks.append(idx * gap)\n            if ref_line:\n                # Line where data is uniform\n                ax.axhline(y=y_ticks[-1] + counts.mean(), linestyle=\"--\", color=\"C1\")\n            # fake an x-axis\n            ax.axhline(y=y_ticks[-1], color=\"k\", lw=1)\n            ax.bar(\n                bin_ary,\n                counts,\n                bottom=y_ticks[-1],\n                width=width,\n                align=\"center\",\n                color=\"C0\",\n                edgecolor=ax.get_facecolor(),\n            )\n        ax.set_xlabel(\"Rank (all chains)\", fontsize=ax_labelsize)\n        ax.set_ylabel(\"Chain\", fontsize=ax_labelsize)\n        ax.set_yticks(y_ticks)\n        ax.set_yticklabels(np.arange(len(y_ticks)))\n        ax.set_title(make_label(var_name, selection), fontsize=titlesize)\n\n    return axes", "response": "Plot the rank order statistics of the chains."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CreateAdsWithCustomizations(client, adgroup_ids, feed_name):\n  # Get the AdGroupAdService\n  adgroup_ad_service = client.GetService('AdGroupAdService', 'v201809')\n\n  expanded_text_ad = {\n      'xsi_type': 'ExpandedTextAd',\n      'headlinePart1': 'Luxury Cruise to {=%s.Name}' % feed_name,\n      'headlinePart2': 'Only {=%s.Price}' % feed_name,\n      'description': 'Offer ends in {=countdown(%s.Date)}!' % feed_name,\n      'finalUrls': ['http://www.example.com'],\n  }\n\n  # We add the same ad to both ad groups. When they serve, they will show\n  # different values, since they match different feed items.\n  operations = [{\n      'operator': 'ADD',\n      'operand': {\n          'adGroupId': adgroup,\n          'ad': expanded_text_ad\n      }\n  } for adgroup in adgroup_ids]\n\n  response = adgroup_ad_service.mutate(operations)\n\n  if response and 'value' in response:\n    for ad in response['value']:\n      print ('Created an ad with ID \"%s\", type \"%s\", and status \"%s\".'\n             % (ad['ad']['id'], ad['ad']['Ad.Type'], ad['status']))\n  else:\n    raise errors.GoogleAdsError('No ads were added.')", "response": "Creates ExpandedTextAds that use customizations for the specified AdGroups."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new AdCustomizerFeed.", "response": "def CreateCustomizerFeed(client, feed_name):\n  \"\"\"Creates a new AdCustomizerFeed.\n\n  Args:\n    client: an AdWordsClient instance.\n    feed_name: the name for the new AdCustomizerFeed.\n\n  Returns:\n    The new AdCustomizerFeed.\n  \"\"\"\n  # Get the AdCustomizerFeedService\n  ad_customizer_feed_service = client.GetService('AdCustomizerFeedService',\n                                                 'v201809')\n  customizer_feed = {\n      'feedName': feed_name,\n      'feedAttributes': [\n          {'type': 'STRING', 'name': 'Name'},\n          {'type': 'STRING', 'name': 'Price'},\n          {'type': 'DATE_TIME', 'name': 'Date'}\n      ]\n  }\n\n  feed_service_operation = {\n      'operator': 'ADD',\n      'operand': customizer_feed\n  }\n\n  response = ad_customizer_feed_service.mutate([feed_service_operation])\n\n  if response and 'value' in response:\n    feed = response['value'][0]\n    feed_data = {\n        'feedId': feed['feedId'],\n        'nameId': feed['feedAttributes'][0]['id'],\n        'priceId': feed['feedAttributes'][1]['id'],\n        'dateId': feed['feedAttributes'][2]['id']\n    }\n    print ('Feed with name \"%s\" and ID %s was added with:\\n'\n           '\\tName attribute ID %s and price attribute ID %s and date attribute'\n           'ID %s') % (feed['feedName'], feed['feedId'], feed_data['nameId'],\n                       feed_data['priceId'], feed_data['dateId'])\n    return feed\n  else:\n    raise errors.GoogleAdsError('No feeds were added')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef RestrictFeedItemToAdGroup(client, feed_item, adgroup_id):\n  # Get the FeedItemTargetService\n  feed_item_target_service = client.GetService(\n      'FeedItemTargetService', 'v201809')\n\n  # Optional: Restrict the first feed item to only serve with ads for the\n  # specified ad group ID.\n  ad_group_target = {\n      'xsi_type': 'FeedItemAdGroupTarget',\n      'feedId': feed_item['feedId'],\n      'feedItemId': feed_item['feedItemId'],\n      'adGroupId': adgroup_id\n  }\n\n  operation = {'operator': 'ADD', 'operand': ad_group_target}\n\n  response = feed_item_target_service.mutate([operation])\n  new_ad_group_target = response['value'][0]\n\n  print('Feed item target for feed ID %s and feed item ID %s was created to '\n        'restrict serving to ad group ID %s' %\n        (new_ad_group_target['feedId'],\n         new_ad_group_target['feedItemId'],\n         new_ad_group_target['adGroupId']))", "response": "Restricts the feed item to an ad group."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CreateCustomizerFeedItems(client, adgroup_ids, ad_customizer_feed):\n  # Get the FeedItemService\n  feed_item_service = client.GetService('FeedItemService', 'v201809')\n  now = datetime.now()\n  mars_date = datetime(now.year, now.month, 1, 0, 0)\n  venus_date = datetime(now.year, now.month, 15, 0, 0)\n  time_format = '%Y%m%d %H%M%S'\n\n  feed_item_operations = [\n      CreateFeedItemAddOperation(\n          'Mars', '$1234.56', mars_date.strftime(time_format),\n          ad_customizer_feed),\n      CreateFeedItemAddOperation(\n          'Venus', '$1450.00', venus_date.strftime(time_format),\n          ad_customizer_feed)\n  ]\n\n  response = feed_item_service.mutate(feed_item_operations)\n\n  if 'value' in response:\n    for feed_item in response['value']:\n      print 'Added FeedItem with ID %d.' % feed_item['feedItemId']\n  else:\n    raise errors.GoogleAdsError('No FeedItems were added.')\n\n  for feed_item, adgroup_id in zip(response['value'], adgroup_ids):\n    RestrictFeedItemToAdGroup(client, feed_item, adgroup_id)", "response": "Creates the FeedItems for the specified AdGroups and a customizer Feed."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef CreateFeedItemAddOperation(name, price, date, ad_customizer_feed):\n  feed_item = {\n      'feedId': ad_customizer_feed['feedId'],\n      'attributeValues': [\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][0]['id'],\n              'stringValue': name\n          },\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][1]['id'],\n              'stringValue': price\n          },\n          {\n              'feedAttributeId': ad_customizer_feed['feedAttributes'][2]['id'],\n              'stringValue': date\n          }\n      ]\n  }\n\n  operation = {\n      'operator': 'ADD',\n      'operand': feed_item\n  }\n\n  return operation", "response": "Creates a FeedItemOperation.\n\n  The generated FeedItemOperation will create a FeedItem with the specified\n  values when sent to FeedItemService.mutate.\n\n  Args:\n    name: the value for the name attribute of the FeedItem.\n    price: the value for the price attribute of the FeedItem.\n    date: the value for the date attribute of the FeedItem.\n    ad_customizer_feed: the AdCustomizerFeed we're associating the FeedItems\n        with.\n\n  Returns:\n    A new FeedItemOperation for adding a FeedItem."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating an AdWordsClient with information stored in a yaml string.", "response": "def LoadFromString(cls, yaml_doc):\n    \"\"\"Creates an AdWordsClient with information stored in a yaml string.\n\n    Args:\n      yaml_doc: The yaml string containing the cached AdWords data.\n\n    Returns:\n      An AdWordsClient initialized with the values cached in the string.\n\n    Raises:\n      A GoogleAdsValueError if the given yaml string does not contain the\n      information necessary to instantiate a client object - either a\n      required key was missing or an OAuth2 key was missing.\n    \"\"\"\n    return cls(**googleads.common.LoadFromString(\n        yaml_doc, cls._YAML_KEY, cls._REQUIRED_INIT_VALUES,\n        cls._OPTIONAL_INIT_VALUES))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef LoadFromStorage(cls, path=None):\n    if path is None:\n      path = os.path.join(os.path.expanduser('~'), 'googleads.yaml')\n\n    return cls(**googleads.common.LoadFromStorage(\n        path, cls._YAML_KEY, cls._REQUIRED_INIT_VALUES,\n        cls._OPTIONAL_INIT_VALUES))", "response": "Creates an AdWordsClient with information stored in a yaml file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new service client for the given service name and version.", "response": "def GetService(self, service_name, version=None, server=None):\n    \"\"\"Creates a service client for the given service.\n\n    Args:\n      service_name: A string identifying which AdWords service to create a\n          service client for.\n      [optional]\n      version: A string identifying the AdWords version to connect to. This\n          defaults to what is currently the latest version. This will be updated\n          in future releases to point to what is then the latest version.\n      server: A string identifying the webserver hosting the AdWords API.\n\n    Returns:\n      A googleads.common.GoogleSoapService instance which has the headers\n      and proxy configured for use.\n\n    Raises:\n      A GoogleAdsValueError if the service or version provided do not exist.\n    \"\"\"\n    if not server:\n      server = _DEFAULT_ENDPOINT\n    server = server.rstrip('/')\n\n    if not version:\n      version = sorted(_SERVICE_MAP.keys())[-1]\n\n    try:\n      version_service_mapping = _SERVICE_MAP[version][service_name]\n    except KeyError:\n      msg_fmt = 'Unrecognized %s for the AdWords API. Given: %s Supported: %s'\n\n      if version in _SERVICE_MAP:\n        raise googleads.errors.GoogleAdsValueError(\n            msg_fmt % ('service', service_name, _SERVICE_MAP[version].keys()))\n      else:\n        raise googleads.errors.GoogleAdsValueError(\n            msg_fmt % ('version', version, _SERVICE_MAP.keys()))\n\n    service = googleads.common.GetServiceClassForLibrary(self.soap_impl)(\n        self._SOAP_SERVICE_FORMAT % (\n            server, version_service_mapping, version, service_name),\n        _AdWordsHeaderHandler(\n            self, version, self.enable_compression, self.custom_http_headers),\n        _AdWordsPacker,\n        self.proxy_config,\n        self.timeout,\n        version,\n        cache=self.cache)\n\n    return service"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a BatchJobHelper to work with the AdWords API.", "response": "def GetBatchJobHelper(self, version=sorted(_SERVICE_MAP.keys())[-1],\n                        server=None):\n    \"\"\"Returns a BatchJobHelper to work with the BatchJobService.\n\n      This is a convenience method. It is functionally identical to calling\n      BatchJobHelper(adwords_client, version).\n\n    Args:\n      [optional]\n      version: A string identifying the AdWords version to connect to. This\n          defaults to what is currently the latest version. This will be updated\n          in future releases to point to what is then the latest version.\n      server: A string identifying the webserver hosting the AdWords API.\n\n    Returns:\n      An initialized BatchJobHelper tied to this client.\n    \"\"\"\n    if not server:\n      server = _DEFAULT_ENDPOINT\n\n    request_builder = BatchJobHelper.GetRequestBuilder(\n        self, version=version, server=server)\n    response_parser = BatchJobHelper.GetResponseParser()\n\n    return BatchJobHelper(request_builder, response_parser)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a downloader for AdWords reports.", "response": "def GetReportDownloader(self, version=sorted(_SERVICE_MAP.keys())[-1],\n                          server=None):\n    \"\"\"Creates a downloader for AdWords reports.\n\n    This is a convenience method. It is functionally identical to calling\n    ReportDownloader(adwords_client, version, server).\n\n    Args:\n      [optional]\n      version: A string identifying the AdWords version to connect to. This\n          defaults to what is currently the latest version. This will be updated\n          in future releases to point to what is then the latest version.\n      server: A string identifying the webserver hosting the AdWords API.\n\n    Returns:\n      A ReportDownloader tied to this AdWordsClient, ready to download reports.\n    \"\"\"\n    if not server:\n      server = _DEFAULT_ENDPOINT\n\n    return ReportDownloader(self, version, server)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetSOAPHeaders(self, create_method):\n    header = create_method(self._SOAP_HEADER_CLASS % self._version)\n    header.clientCustomerId = self._adwords_client.client_customer_id\n    header.developerToken = self._adwords_client.developer_token\n    header.userAgent = ''.join([\n        self._adwords_client.user_agent,\n        googleads.common.GenerateLibSig(self._PRODUCT_SIG)])\n    header.validateOnly = self._adwords_client.validate_only\n    header.partialFailure = self._adwords_client.partial_failure\n    return header", "response": "Returns the SOAP headers required for request authorization."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetHTTPHeaders(self):\n    http_headers = self._adwords_client.oauth2_client.CreateHttpHeader()\n    if self.enable_compression:\n      http_headers['accept-encoding'] = 'gzip'\n\n    http_headers.update(self.custom_http_headers)\n\n    return http_headers", "response": "Returns the HTTP headers required for request authorization."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dictionary of headers for a report download request.", "response": "def GetReportDownloadHeaders(self, **kwargs):\n    \"\"\"Returns a dictionary of headers for a report download request.\n\n    Note that the given keyword arguments will override any settings configured\n    from the googleads.yaml file.\n\n    Args:\n      **kwargs: Optional keyword arguments.\n\n    Keyword Arguments:\n      client_customer_id: A string containing a client_customer_id intended to\n        override the default value set by the AdWordsClient.\n      include_zero_impressions: A boolean indicating whether the report should\n        show rows with zero impressions.\n      skip_report_header: A boolean indicating whether to include a header row\n          containing the report name and date range. If false or not specified,\n          report output will include the header row.\n      skip_column_header: A boolean indicating whether to include column names\n          in reports. If false or not specified, report output will include the\n          column names.\n      skip_report_summary: A boolean indicating whether to include a summary row\n          containing the report totals. If false or not specified, report output\n          will include the summary row.\n      use_raw_enum_values: A boolean indicating whether to return enum field\n          values as enums instead of display values.\n\n    Returns:\n      A dictionary containing the headers configured for downloading a report.\n\n    Raises:\n      GoogleAdsValueError: If one or more of the report header keyword arguments\n        is invalid.\n    \"\"\"\n    headers = self._adwords_client.oauth2_client.CreateHttpHeader()\n    headers.update({\n        'Content-type': self._CONTENT_TYPE,\n        'developerToken': str(self._adwords_client.developer_token),\n        'clientCustomerId': str(kwargs.get(\n            'client_customer_id', self._adwords_client.client_customer_id)),\n        'User-Agent': ''.join([\n            self._adwords_client.user_agent,\n            googleads.common.GenerateLibSig(self._PRODUCT_SIG),\n            ',gzip'])\n    })\n    headers.update(self.custom_http_headers)\n\n    updated_kwargs = dict(self._adwords_client.report_download_headers)\n    updated_kwargs.update(kwargs)\n\n    for kw in updated_kwargs:\n      try:\n        headers[_REPORT_HEADER_KWARGS[kw]] = str(updated_kwargs[kw])\n      except KeyError:\n        raise googleads.errors.GoogleAdsValueError(\n            'The provided keyword \"%s\" is invalid. Accepted keywords are: %s'\n            % (kw, _REPORT_HEADER_KWARGS.keys()))\n\n    return headers"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef Pack(cls, obj, version):\n    if isinstance(obj, ServiceQuery):\n      return str(obj)\n    return obj", "response": "Packs the given object using AdWords - specific logic."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Load(cls, file_input, client=None):\n    if client is None:\n      client = AdWordsClient.LoadFromStorage()\n\n    try:\n      data = yaml.safe_load(file_input)\n    except yaml.YAMLError as e:\n      raise googleads.errors.GoogleAdsError(\n          'Error loading IncrementalUploadHelper from file: %s' % str(e))\n\n    try:\n      request_builder = BatchJobHelper.GetRequestBuilder(\n          client, version=data['version'], server=data['server']\n      )\n\n      return cls(request_builder, data['upload_url'],\n                 current_content_length=data['current_content_length'],\n                 is_last=data['is_last'])\n    except KeyError as e:\n      raise googleads.errors.GoogleAdsValueError(\n          'Can\\'t parse IncrementalUploadHelper from file. Required field '\n          '\"%s\" is missing.' % e.message)", "response": "Loads an IncrementalUploadHelper from the given file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring that the URL used to upload operations is properly initialized.", "response": "def _InitializeURL(self, upload_url, current_content_length):\n    \"\"\"Ensures that the URL used to upload operations is properly initialized.\n\n    Args:\n      upload_url: a string url.\n      current_content_length: an integer identifying the current content length\n        of data uploaded to the Batch Job.\n\n    Returns:\n      An initialized string URL, or the provided string URL if the URL has\n      already been initialized.\n    \"\"\"\n    # If initialization is not necessary, return the provided upload_url.\n    if current_content_length != 0:\n      return upload_url\n\n    headers = {\n        'Content-Type': 'application/xml',\n        'Content-Length': 0,\n        'x-goog-resumable': 'start'\n    }\n\n    # Send an HTTP POST request to the given upload_url\n    req = urllib2.Request(upload_url, data={}, headers=headers)\n    resp = self._url_opener.open(req)\n\n    return resp.headers['location']"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef Dump(self, output):\n    data = {\n        'current_content_length': self._current_content_length,\n        'is_last': self._is_last,\n        'server': self._request_builder.GetServer(),\n        'upload_url': self._upload_url,\n        'version': self._request_builder.GetVersion()\n    }\n\n    try:\n      yaml.dump(data, output)\n    except yaml.YAMLError as e:\n      raise googleads.errors.GoogleAdsError(\n          'Error dumping IncrementalUploadHelper to file: %s' % str(e))", "response": "Serialize the IncrementalUploadHelper and store in file - like object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef UploadOperations(self, operations, is_last=False):\n    if self._is_last:\n      raise googleads.errors.AdWordsBatchJobServiceInvalidOperationError(\n          'Can\\'t add new operations to a completed incremental upload.')\n    # Build the request\n    req = self._request_builder.BuildUploadRequest(\n        self._upload_url, operations,\n        current_content_length=self._current_content_length, is_last=is_last)\n    # Make the request, ignoring the urllib2.HTTPError raised due to HTTP status\n    # code 308 (for resumable uploads).\n    try:\n      _batch_job_logger.debug('Outgoing request: %s %s %s',\n                              req.get_full_url(), req.headers, req.data)\n\n      self._url_opener.open(req)\n\n      if _batch_job_logger.isEnabledFor(logging.INFO):\n        _batch_job_logger.info('Request summary: %s',\n                               self._ExtractRequestSummaryFields(req))\n    except urllib2.HTTPError as e:\n      if e.code != 308:\n        if _batch_job_logger.isEnabledFor(logging.WARNING):\n          _batch_job_logger.warning(\n              'Request summary: %s',\n              self._ExtractRequestSummaryFields(req, error=e))\n        raise\n    # Update upload status.\n    self._current_content_length += len(req.data)\n    self._is_last = is_last", "response": "Uploads operations to the given uploadUrl in incremental steps."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _ExtractRequestSummaryFields(self, request, error=None):\n    headers = request.headers\n    summary_fields = {\n        'server': request.get_full_url(),\n        'contentRange': headers['Content-range'],\n        'contentLength': headers['Content-length']\n    }\n\n    if error:\n      summary_fields['isError'] = True\n      summary_fields['errorMessage'] = error.reason\n    else:\n      summary_fields['isError'] = False\n\n    return summary_fields", "response": "Extracts the fields used in the summary logs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a WHERE builder using a provided field.", "response": "def Where(self, field):\n    \"\"\"Creates a WHERE builder using a provided field.\n\n    Args:\n      field: the field to be added as an argument in the WHERE clause.\n\n    Returns:\n      The created WHERE builder.\n    \"\"\"\n    where_builder = _WhereBuilder(self, field)\n    self.where_builders.append(where_builder)\n    return where_builder"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the type of the WHERE clause as equal to.", "response": "def EqualTo(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"equal to\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, '=')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef NotEqualTo(self, value):\n    self._awql = self._CreateSingleValueCondition(value, '!=')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as not equal to."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the type of the WHERE clause as greater than.", "response": "def GreaterThan(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"greater than\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, '>')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GreaterThanOrEqualTo(self, value):\n    self._awql = self._CreateSingleValueCondition(value, '>=')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as greater than or equal to."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef LessThan(self, value):\n    self._awql = self._CreateSingleValueCondition(value, '<')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as less than."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef LessThanOrEqualTo(self, value):\n    self._awql = self._CreateSingleValueCondition(value, '<=')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as less than or equal to."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the type of the WHERE clause as starts with.", "response": "def StartsWith(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"starts with\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, 'STARTS_WITH')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef StartsWithIgnoreCase(self, value):\n    self._awql = self._CreateSingleValueCondition(value,\n                                                  'STARTS_WITH_IGNORE_CASE')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as starts with ignore case."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the type of the WHERE clause as contains.", "response": "def Contains(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"contains\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, 'CONTAINS')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the type of the WHERE clause as contains ignore case.", "response": "def ContainsIgnoreCase(self, value):\n    \"\"\"Sets the type of the WHERE clause as \"contains ignore case\".\n\n    Args:\n      value: The value to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateSingleValueCondition(value, 'CONTAINS_IGNORE_CASE')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef DoesNotContain(self, value):\n    self._awql = self._CreateSingleValueCondition(value, 'DOES_NOT_CONTAIN')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as does not contain."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DoesNotContainIgnoreCase(self, value):\n    self._awql = self._CreateSingleValueCondition(\n        value, 'DOES_NOT_CONTAIN_IGNORE_CASE')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as does not contain ignore case."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef In(self, *values):\n    self._awql = self._CreateMultipleValuesCondition(values, 'IN')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as in."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the type of the WHERE clause as in.", "response": "def NotIn(self, *values):\n    \"\"\"Sets the type of the WHERE clause as \"in\".\n\n    Args:\n      *values: The values to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateMultipleValuesCondition(values, 'NOT_IN')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ContainsAny(self, *values):\n    self._awql = self._CreateMultipleValuesCondition(values, 'CONTAINS_ANY')\n    return self._query_builder", "response": "Sets the type of the WHERE clause as contains any."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the type of the WHERE clause as contains none.", "response": "def ContainsNone(self, *values):\n    \"\"\"Sets the type of the WHERE clause as \"contains none\".\n\n    Args:\n      *values: The values to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateMultipleValuesCondition(values, 'CONTAINS_NONE')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting the type of the WHERE clause as contains all.", "response": "def ContainsAll(self, *values):\n    \"\"\"Sets the type of the WHERE clause as \"contains all\".\n\n    Args:\n      *values: The values to be used in the WHERE condition.\n\n    Returns:\n      The query builder that this WHERE builder links to.\n    \"\"\"\n    self._awql = self._CreateMultipleValuesCondition(values, 'CONTAINS_ALL')\n    return self._query_builder"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a single - value condition with the provided value and operator.", "response": "def _CreateSingleValueCondition(self, value, operator):\n    \"\"\"Creates a single-value condition with the provided value and operator.\"\"\"\n    if isinstance(value, str) or isinstance(value, unicode):\n      value = '\"%s\"' % value\n    return '%s %s %s' % (self._field, operator, value)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a condition with the provided list of values and operator.", "response": "def _CreateMultipleValuesCondition(self, values, operator):\n    \"\"\"Creates a condition with the provided list of values and operator.\"\"\"\n    values = ['\"%s\"' % value if isinstance(value, str) or\n              isinstance(value, unicode) else str(value) for value in values]\n    return '%s %s [%s]' % (self._field, operator, ', '.join(values))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef NextPage(self, page=None):\n    if self._start_index is None:\n      raise ValueError('Cannot page through query with no LIMIT clause.')\n\n    # DataService has a different paging mechanism, resulting in different\n    # method of determining if there is still a page left.\n    page_size = None\n    if (page and self._PAGE_TYPE in page\n        and page[self._PAGE_TYPE] in self._BID_LANDSCAPE_PAGES):\n      page_size = sum([len(bid_landscape[self._LANDSCAPE_POINTS])\n                       for bid_landscape in page[self._ENTRIES]])\n\n    increment = page_size or self._page_size\n    self._start_index += increment\n    return self", "response": "Sets the LIMIT clause of the AWQL to the next page."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef HasNext(self, page):\n    if self._start_index is None:\n      raise ValueError('Cannot page through query with no LIMIT clause.')\n    if page is None:\n      raise ValueError('The passed page cannot be None.')\n\n    # DataService has a different paging mechanism, resulting in different\n    # method of determining if there is still a page left.\n    if (self._PAGE_TYPE in page\n        and page[self._PAGE_TYPE] in self._BID_LANDSCAPE_PAGES):\n      if self._ENTRIES in page:\n        total_landscape_points = sum([len(bid_landscape[self._LANDSCAPE_POINTS])\n                                      for bid_landscape in page[self._ENTRIES]])\n      else:\n        total_landscape_points = 0\n      return total_landscape_points >= self._page_size\n\n    if not self._total_num_entries:\n      self._total_num_entries = page[self._TOTAL_NUM_ENTRIES]\n    return self._start_index + self._page_size < self._total_num_entries", "response": "Checks if there is a page left to query."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef Pager(self, service):\n    has_page = True\n    while has_page:\n      page = service.query(self)\n      yield page\n      has_page = self.HasNext(page)\n      if has_page:\n        self.NextPage()", "response": "A generator for this service query and the provided service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninitializing application user. Retrieve existing user credentials from datastore or add new user. Returns: AppUser instance of the application user.", "response": "def InitUser():\n  \"\"\"Initialize application user.\n\n  Retrieve existing user credentials from datastore or add new user.\n\n  Returns:\n    AppUser instance of the application user.\n  \"\"\"\n  result = AppUser.query(AppUser.user == users.get_current_user()).fetch()\n\n  if result:\n    app_user = result[0]\n  else:\n    app_user = AppUser(user=users.get_current_user(),\n                       email=users.get_current_user().email())\n    app_user.put()\n\n  return app_user"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the credentials associated with application user.", "response": "def UpdateUserCredentials(client_id, client_secret, refresh_token,\n                          adwords_manager_cid, developer_token):\n  \"\"\"Update the credentials associated with application user.\n\n  Args:\n    client_id: str Client Id retrieved from the developer's console.\n    client_secret: str Client Secret retrieved from the developer's console.\n    refresh_token: str Refresh token generated with the above client id/secret.\n    adwords_manager_cid: str Customer Id for the AdWords manager account.\n    developer_token: str Developer Token for the AdWords account.\n  \"\"\"\n  app_user = AppUser.query(AppUser.user == users.get_current_user()).fetch()[0]\n\n  app_user.client_id = client_id\n  app_user.client_secret = client_secret\n  app_user.refresh_token = refresh_token\n  app_user.adwords_manager_cid = adwords_manager_cid\n  app_user.developer_token = developer_token\n\n  app_user.put()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a default partition.", "response": "def CreateDefaultPartition(client, ad_group_id):\n  \"\"\"Creates a default partition.\n\n  Args:\n    client: an AdWordsClient instance.\n    ad_group_id: an integer ID for an ad group.\n  \"\"\"\n  ad_group_criterion_service = client.GetService('AdGroupCriterionService',\n                                                 version='v201809')\n\n  operations = [{\n      'operator': 'ADD',\n      'operand': {\n          'xsi_type': 'BiddableAdGroupCriterion',\n          'adGroupId': ad_group_id,\n          # Make sure that caseValue and parentCriterionId are left unspecified.\n          # This makes this partition as generic as possible to use as a\n          # fallback when others don't match.\n          'criterion': {\n              'xsi_type': 'ProductPartition',\n              'partitionType': 'UNIT'\n          },\n          'biddingStrategyConfiguration': {\n              'bids': [{\n                  'xsi_type': 'CpcBid',\n                  'bid': {\n                      'microAmount': 500000\n                  }\n              }]\n          }\n      }\n  }]\n\n  ad_group_criterion = ad_group_criterion_service.mutate(operations)['value'][0]\n\n  print ('Ad group criterion with ID \"%d\" in ad group with ID \"%d\" was added.'\n         % (ad_group_criterion['criterion']['id'],\n            ad_group_criterion['adGroupId']))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ConvertValueForCsv(pql_value):\n  if 'value' in pql_value:\n    field = pql_value['value']\n  elif 'values' in pql_value:\n    field = pql_value['values']\n  else:\n    field = None\n\n  if field:\n    if isinstance(field, list):\n      return ','.join(['\"%s\"' % str(ConvertValueForCsv(single_field))\n                       for single_field in field])\n    else:\n      class_type = ad_manager.AdManagerClassType(pql_value)\n\n      if class_type == 'TextValue':\n        return field.replace('\"', '\"\"').encode('UTF8')\n      elif class_type == 'NumberValue':\n        return float(field) if '.' in field else int(field)\n      elif class_type == 'DateTimeValue':\n        return ConvertDateTimeToOffset(field)\n      elif class_type == 'DateValue':\n        return date(int(field['date']['year']),\n                    int(field['date']['month']),\n                    int(field['date']['day'])).isoformat()\n      else:\n        return field\n  else:\n    return '-'", "response": "Converts a single field value from a Value object to a CSV suitable format."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts the PQL formatted response for a dateTime object.", "response": "def ConvertDateTimeToOffset(date_time_value):\n  \"\"\"Converts the PQL formatted response for a dateTime object.\n\n  Output conforms to ISO 8061 format, e.g. 'YYYY-MM-DDTHH:MM:SSz.'\n\n  Args:\n    date_time_value: dict The date time value from the PQL response.\n\n  Returns:\n    str: A string representation of the date time value uniform to\n        ReportService.\n  \"\"\"\n  date_time_obj = datetime(int(date_time_value['date']['year']),\n                           int(date_time_value['date']['month']),\n                           int(date_time_value['date']['day']),\n                           int(date_time_value['hour']),\n                           int(date_time_value['minute']),\n                           int(date_time_value['second']))\n  date_time_str = pytz.timezone(\n      date_time_value['timeZoneId']).localize(date_time_obj).isoformat()\n\n  if date_time_str[-5:] == '00:00':\n    return date_time_str[:-6] + 'Z'\n  else:\n    return date_time_str"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a user identifier from the specified type and value.", "response": "def _CreateUserIdentifier(identifier_type=None, value=None):\n  \"\"\"Creates a user identifier from the specified type and value.\n\n  Args:\n    identifier_type: a str specifying the type of user identifier.\n    value: a str value of the identifier; to be hashed using SHA-256 if needed.\n\n  Returns:\n    A dict specifying a user identifier, with a value hashed using SHA-256 if\n      needed.\n  \"\"\"\n  if identifier_type in _HASHED_IDENTIFIER_TYPES:\n    # If the user identifier type is a hashed type, normalize and hash the\n    # value.\n    value = hashlib.sha256(value.strip().lower()).hexdigest()\n\n  user_identifier = {\n      'userIdentifierType': identifier_type,\n      'value': value\n  }\n\n  return user_identifier"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _GetFieldPathElementIndex(api_error, field):\n  field_path_elements = api_error['fieldPathElements']\n\n  if field_path_elements:\n    found_index = [field_path_element['index']\n                   for field_path_element in field_path_elements\n                   if field_path_element['field'] == field]\n    if found_index:\n      return found_index\n\n  return None", "response": "Returns the index of a given field in the AdWords\n      API."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _ApplySudsJurkoAppenderPatch(self):\n    def PatchedAppend(self, parent, content):\n      obj = content.value\n      child = self.node(content)\n      parent.append(child)\n      for item in obj:\n        cont = suds.mx.Content(tag=item[0], value=item[1])\n        suds.mx.appender.Appender.append(self, child, cont)\n\n    suds.mx.appender.ObjectAppender.append = PatchedAppend", "response": "Applies a Monkey Patch to the suds. mx. appender module.\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _ApplySudsJurkoSendPatch(self):\n    def GetInflateStream(msg):\n      stream = io.BytesIO()\n      stream.write(msg)\n      stream.flush()\n      stream.seek(0)\n      return gzip.GzipFile(fileobj=stream, mode='rb')\n\n    def PatchedHttpTransportSend(self, request):\n      \"\"\"Patch for HttpTransport.send to enable gzip compression.\"\"\"\n      msg = request.message\n      http_transport = suds.transport.http.HttpTransport\n      url = http_transport._HttpTransport__get_request_url(request)\n      headers = request.headers\n      u2request = urllib2.Request(url, msg, headers)\n      self.addcookies(u2request)\n      self.proxy = self.options.proxy\n      request.headers.update(u2request.headers)\n      suds.transport.http.log.debug('sending:\\n%s', request)\n      try:\n        fp = self.u2open(u2request)\n      except urllib2.HTTPError, e:\n        if e.code in (202, 204):\n          return None\n        else:\n          if e.headers.get('content-encoding') == 'gzip':\n            # If gzip encoding is used, decompress here.\n            # Need to read and recreate a stream because urllib result objects\n            # don't fully implement the file-like API\n            e.fp = GetInflateStream(e.fp.read())\n\n          raise suds.transport.TransportError(e.msg, e.code, e.fp)\n\n      self.getcookies(fp, u2request)\n      # Note: Python 2 returns httplib.HTTPMessage, and Python 3 returns\n      # http.client.HTTPMessage, which differ slightly.\n      headers = (fp.headers.dict if sys.version_info < (3, 0) else fp.headers)\n      result = suds.transport.Reply(200, headers, fp.read())\n\n      if result.headers.get('content-encoding') == 'gzip':\n        # If gzip encoding is used, decompress here.\n        result.message = GetInflateStream(result.message).read()\n\n      suds.transport.http.log.debug('received:\\n%s', result)\n      return result\n\n    suds.transport.http.HttpTransport.send = PatchedHttpTransportSend", "response": "Adds a Monkey Patch to the suds. transport. http module."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\noverrides the ingress function for response logging.", "response": "def ingress(self, envelope, http_headers, operation):\n    \"\"\"Overrides the ingress function for response logging.\n\n    Args:\n      envelope: An Element with the SOAP request data.\n      http_headers: A dict of the current http headers.\n      operation: The SoapOperation instance.\n\n    Returns:\n      A tuple of the envelope and headers.\n    \"\"\"\n    if self._logger.isEnabledFor(logging.DEBUG):\n      self._logger.debug(_RESPONSE_XML_LOG_LINE,\n                         etree.tostring(envelope, pretty_print=True))\n\n    if self._logger.isEnabledFor(logging.WARN):\n      warn_data = {}\n      header = envelope.find(_HEADER_XPATH)\n      fault = envelope.find(_FAULT_XPATH)\n      if fault is not None:\n        warn_data['faultMessage'] = fault.find('faultstring').text\n\n        if header is not None:\n          header_data = {\n              re.sub(_REMOVE_NS_REGEXP, '', child.tag): child.text\n              for child in header[0]}\n          warn_data.update(header_data)\n\n        if 'serviceName' not in warn_data:\n          warn_data['serviceName'] = operation.binding.wsdl.services.keys()[0]\n\n        if 'methodName' not in warn_data:\n          warn_data['methodName'] = operation.name\n\n        self._logger.warn('Error summary: %s', warn_data)\n\n    return envelope, http_headers"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\noverriding the egress function ror request logging.", "response": "def egress(self, envelope, http_headers, operation, binding_options):\n    \"\"\"Overrides the egress function ror request logging.\n\n    Args:\n      envelope: An Element with the SOAP request data.\n      http_headers: A dict of the current http headers.\n      operation: The SoapOperation instance.\n      binding_options: An options dict for the SOAP binding.\n\n    Returns:\n      A tuple of the envelope and headers.\n    \"\"\"\n    if self._logger.isEnabledFor(logging.INFO):\n      service_name = operation.binding.wsdl.services.keys()[0]\n      self._logger.info(_REQUEST_LOG_LINE, service_name, operation.name,\n                        binding_options['address'])\n\n    if self._logger.isEnabledFor(logging.DEBUG):\n      http_headers_safe = http_headers.copy()\n      if self._AUTHORIZATION_HEADER in http_headers_safe:\n        http_headers_safe[self._AUTHORIZATION_HEADER] = self._REDACTED\n\n      request_string = etree.tostring(envelope, pretty_print=True)\n      safe_request = self._DEVELOPER_TOKEN_SUB.sub(\n          self._REDACTED, request_string.decode('utf-8'))\n      self._logger.debug(\n          _REQUEST_XML_LOG_LINE, http_headers_safe, safe_request)\n\n    return envelope, http_headers"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef RestrictFeedItemToGeoTarget(client, feed_item, location_id):\n  # Retrieve the FeedItemTargetService\n  feed_item_target_service = client.GetService(\n      'FeedItemTargetService', version='v201809')\n\n  # Optional: Restrict the first feed item to only serve with ads for the\n  # specified geo target.\n  criterion_target = {\n      'xsi_type': 'FeedItemCriterionTarget',\n      'feedId': feed_item['feedId'],\n      'feedItemId': feed_item['feedItemId'],\n      # These IDs can be found in the documentation or retrieved with the\n      # LocationCriterionService.\n      'criterion': {\n          'xsi_type': 'Location',\n          'id': location_id\n      }\n  }\n\n  operation = {'operator': 'ADD', 'operand': criterion_target}\n\n  response = feed_item_target_service.mutate([operation])\n  new_location_target = response['value'][0]\n  print('Feed item target for feed ID %d and feed item ID %d was created to '\n        'restrict serving to location ID %d.' %\n        (new_location_target['feedId'],\n         new_location_target['feedItemId'],\n         new_location_target['criterion']['id']))", "response": "Restrict a feed item to a geo target location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Smart Shopping campaign.", "response": "def CreateSmartCampaign(client, budget_id, merchant_id):\n  \"\"\"Adds a new Smart Shopping campaign.\n\n  Args:\n    client: an AdWordsClient instance.\n    budget_id: the str ID of the budget to be associated with the Shopping\n      campaign.\n    merchant_id: the str ID of the merchant account to be associated with the\n      Shopping campaign.\n  Returns:\n    A campaign ID.\n  \"\"\"\n  campaign_service = client.GetService('CampaignService', version='v201809')\n  # Create campaign with required and optional settings.\n  campaign = {\n      'name': 'Shopping campaign #%s' % uuid.uuid4(),\n      # The advertisingChannelType is what makes this a Shopping campaign.\n      'advertisingChannelType': 'SHOPPING',\n      # Sets the advertisingChannelSubType to SHOPPING_GOAL_OPTIMIZED_ADS to\n      # make this a Smart Shopping campaign.\n      'advertisingChannelSubType': 'SHOPPING_GOAL_OPTIMIZED_ADS',\n      # Recommendation: Set the campaign to PAUSED when creating it to stop the\n      # ads from immediately serving. Set to ENABLED once you've added targeting\n      # and the ads are ready to serve.\n      'status': 'PAUSED',\n      # Set portfolio budget (required).\n      'budget': {'budgetId': budget_id},\n      # Set a bidding strategy. Only MAXIMIZE_CONVERSION_VALUE is supported.\n      'biddingStrategyConfiguration': {\n          'biddingStrategyType': 'MAXIMIZE_CONVERSION_VALUE'\n      },\n      'settings': [{\n          # All Shopping campaigns need a ShoppingSetting.\n          'xsi_type': 'ShoppingSetting',\n          'salesCountry': 'US',\n          'merchantId': merchant_id\n      }]\n  }\n\n  campaign_operations = [{\n      'operator': 'ADD',\n      'operand': campaign\n  }]\n\n  result = campaign_service.mutate(campaign_operations)['value'][0]\n\n  print ('Smart Shopping campaign with name \"%s\" and ID \"%s\" was added.'\n         % (result['name'], result['id']))\n\n  return result['id']"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new Smart Shopping ad group.", "response": "def CreateSmartShoppingAdGroup(client, campaign_id):\n  \"\"\"Adds a new Smart Shopping ad group.\n\n  Args:\n    client: an AdWordsClient instance.\n    campaign_id: the str ID of a Smart Shopping campaign.\n  Returns:\n    An ad group ID.\n  \"\"\"\n  ad_group_service = client.GetService('AdGroupService', version='v201809')\n  # Create the ad group.\n  ad_group = {\n      'campaignId': campaign_id,\n      'name': 'Smart Shopping ad group #%s' % uuid.uuid4(),\n      # Set the ad group type to SHOPPING_GOAL_OPTIMIZED_ADS.\n      'adGroupType': 'SHOPPING_GOAL_OPTIMIZED_ADS'\n  }\n\n  adgroup_operations = {\n      'operator': 'ADD',\n      'operand': ad_group\n  }\n\n  # Make the mutate request to add the AdGroup to the Smart Shopping campaign.\n  ad_group = ad_group_service.mutate(adgroup_operations)['value'][0]\n  ad_group_id = ad_group['id']\n\n  print ('AdGroup with name \"%s\" and ID \"%s\" was added.'\n         % (ad_group['name'], ad_group_id))\n\n  return ad_group_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CreateSmartShoppingAd(client, ad_group_id):\n  ad_group_ad_service = client.GetService('AdGroupAdService', version='v201809')\n  # Create an AdGroup Ad.\n  adgroup_ad = {\n      'adGroupId': ad_group_id,\n      # Create a Smart Shopping ad (Goal-optimized Shopping ad).\n      'ad': {\n          'xsi_type': 'GoalOptimizedShoppingAd'\n      }\n  }\n\n  ad_operation = {\n      'operator': 'ADD',\n      'operand': adgroup_ad\n  }\n\n  # Make the mutate request to add the Smart Shopping ad to the AdGroup.\n  ad_result = ad_group_ad_service.mutate([ad_operation])\n\n  for adgroup_ad in ad_result['value']:\n    print 'Smart Shopping ad with ID \"%s\" was added.' % adgroup_ad['ad']['id']", "response": "Adds a new Smart Shopping ad."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AddMonths(start_date, months):\n  current_date = start_date\n  i = 0\n  while i < months:\n    month_days = calendar.monthrange(current_date.year, current_date.month)[1]\n    current_date += timedelta(days=month_days)\n    i += 1\n  return current_date", "response": "A simple utility for adding months to a given start date."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef DisplayEstimate(message, min_estimate, max_estimate):\n  # Find the mean of the min and max values.\n  mean_avg_cpc = (_CalculateMean(min_estimate['averageCpc']['microAmount'],\n                                 max_estimate['averageCpc']['microAmount'])\n                  if 'averageCpc' in min_estimate\n                  and min_estimate['averageCpc'] else None)\n  mean_avg_pos = (_CalculateMean(min_estimate['averagePosition'],\n                                 max_estimate['averagePosition'])\n                  if 'averagePosition' in min_estimate\n                  and min_estimate['averagePosition'] else None)\n  mean_clicks = _CalculateMean(min_estimate['clicksPerDay'],\n                               max_estimate['clicksPerDay'])\n  mean_total_cost = _CalculateMean(min_estimate['totalCost']['microAmount'],\n                                   max_estimate['totalCost']['microAmount'])\n\n  print message\n  print '  Estimated average CPC: %s' % _FormatMean(mean_avg_cpc)\n  print '  Estimated ad position: %s' % _FormatMean(mean_avg_pos)\n  print '  Estimated daily clicks: %s' % _FormatMean(mean_clicks)\n  print '  Estimated daily cost: %s' % _FormatMean(mean_total_cost)", "response": "Displays mean average cpc position clicks and total cost for the given estimate."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Build(self):\n    if all((self.client_type, self.client_id, self.client_secret,\n            self.auth_uri, self.token_uri)):\n      client_config = {\n          self.client_type: {\n              'client_id': self.client_id,\n              'client_secret': self.client_secret,\n              'auth_uri': self.auth_uri,\n              'token_uri': self.token_uri\n          }\n      }\n    else:\n      raise ValueError('Required field is missing.')\n\n    return client_config", "response": "Builds a client config dictionary used in the OAuth 2. 0 flow."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats a SOAP DateTime object for printing.", "response": "def FormatSOAPDateTime(value):\n  \"\"\"Format a SOAP DateTime object for printing.\n\n  Args:\n    value: The DateTime object to format.\n\n  Returns:\n    A string representing the value.\n  \"\"\"\n  value_date = value['date']\n  return '%s-%s-%s %s:%s:%s (%s)' % (\n      value_date['year'], value_date['month'], value_date['day'],\n      value['hour'], value['minute'], value['second'], value['timeZoneId'])"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CalculateForecastStats(matched, available, possible=None):\n  if matched > 0:\n    available_percent = (float(available) / matched) * 100.\n  else:\n    available_percent = 0\n\n  if possible is not None:\n    if matched > 0:\n      possible_percent = (possible/float(matched)) * 100.\n    else:\n      possible_percent = 0\n  else:\n    possible_percent = None\n\n  return available_percent, possible_percent", "response": "Calculates the forecast percentage stats."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a shopping campaign with the given budget and merchant IDs.", "response": "def CreateShoppingCampaign(client, budget_id, merchant_id):\n  \"\"\"Creates a shopping campaign with the given budget and merchant IDs.\n\n  Args:\n    client: an AdWordsClient instance.\n    budget_id: the str ID of the budget to be associated with the shopping\n      campaign.\n    merchant_id: the str ID of the merchant account to be associated with the\n      shopping campaign.\n\n  Returns:\n    The created Shopping Campaign as a sudsobject.\n  \"\"\"\n  campaign_service = client.GetService('CampaignService', 'v201809')\n\n  campaign = {\n      'name': 'Shopping campaign #%s' % uuid.uuid4(),\n      # The advertisingChannelType is what makes this a shopping campaign\n      'advertisingChannelType': 'SHOPPING',\n      # Recommendation: Set the campaign to PAUSED when creating it to stop the\n      # ads from immediately serving. Set to ENABLED once you've added targeting\n      # and the ads are ready to serve.\n      'status': 'PAUSED',\n      # Set portfolio budget (required)\n      'budget': {\n          'budgetId': budget_id\n      },\n      'biddingStrategyConfiguration': {\n          'biddingStrategyType': 'MANUAL_CPC'\n      },\n      'settings': [\n          # All shopping campaigns need a ShoppingSetting\n          {\n              'xsi_type': 'ShoppingSetting',\n              'salesCountry': 'US',\n              'campaignPriority': '0',\n              'merchantId': merchant_id,\n              # Set to \"True\" to enable Local Inventory Ads in your campaign.\n              'enableLocal': True\n          }\n      ]\n  }\n\n  campaign_operations = [{\n      'operator': 'ADD',\n      'operand': campaign\n  }]\n\n  campaign = campaign_service.mutate(campaign_operations)['value'][0]\n  print ('Campaign with name \"%s\" and ID \"%s\" was added.'\n         % (campaign['name'], campaign['id']))\n\n  return campaign"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate an AdGroup for the given shopping campaign ID.", "response": "def CreateAdGroup(client, campaign_id):\n  \"\"\"Creates an AdGroup for the given shopping campaign ID.\n\n  Args:\n    client: an AdWordsClient instance.\n    campaign_id: the str ID of a shopping campaign.\n\n  Returns:\n    The created AdGroup as a sudsobject.\n  \"\"\"\n  ad_group_service = client.GetService('AdGroupService', 'v201809')\n\n  adgroup = {\n      # Required: Set the ad group type to SHOPPING_SHOWCASE_ADS\n      'adGroupType': 'SHOPPING_SHOWCASE_ADS',\n      'campaignId': campaign_id,\n      'name': 'AdGroup #%s' % uuid.uuid4(),\n      # REQUIRED: Set the ad group's bidding strategy configuration.\n      'biddingStrategyConfiguration': {\n          # Showcase ads require either ManualCpc or EnhancedCpc.\n          'biddingStrategyType': 'MANUAL_CPC',\n          # Optional: Set the bids\n          'bids': [{\n              'xsi_type': 'CpcBid',\n              'bid': {\n                  'microAmount': 100000\n              }\n          }]\n      }\n  }\n\n  adgroup_operations = {\n      'operator': 'ADD',\n      'operand': adgroup\n  }\n\n  # Make the mutate request to add the AdGroup to the Shopping Campaign\n  adgroup = ad_group_service.mutate(adgroup_operations)['value'][0]\n\n  print ('AdGroup with name \"%s\" and ID \"%s\" was added.'\n         % (adgroup['name'], adgroup['id']))\n\n  return adgroup"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a showcase add for the given AdGroup with the given images.", "response": "def CreateShowcaseAd(client, adgroup, expanded_image_filepath,\n                     collapsed_image_filepath):\n  \"\"\"Creates a showcase add for the given AdGroup with the given images.\n\n  Args:\n    client: an AdWordsClient instance.\n    adgroup: a dict or suds object defining an AdGroup for a Shopping Campaign.\n    expanded_image_filepath: a str filepath to a .jpg file that will be used as\n      the Showcase Ad's expandedImage.\n    collapsed_image_filepath: a str filepath to a .jpg file that will be used as\n      the Showcase Ad's collapsedImage.\n\n  Returns:\n    The created Showcase Ad as a sudsobject.\n  \"\"\"\n  ad_group_ad_service = client.GetService('AdGroupAdService', 'v201809')\n\n  showcase_ad = {\n      'adGroupId': adgroup['id'],\n      'ad': {\n          'xsi_type': 'ShowcaseAd',\n          'Ad.Type': 'ShowcaseAd',\n          # Required: set the ad's name, final URLs, and display URL.\n          'name': 'Showcase ad #%s' % uuid.uuid4(),\n          'finalUrls': 'http://example.com/showcase',\n          'displayUrl': 'example.com',\n          # Required: Set the ad's expanded image.\n          'expandedImage': {\n              'mediaId': UploadImage(client, expanded_image_filepath)['mediaId']\n          },\n          # Optional: Set the collapsed image.\n          'collapsedImage': {\n              'mediaId':\n                  UploadImage(client, collapsed_image_filepath)['mediaId']\n          }\n      }\n  }\n\n  ad_operation = {\n      'operator': 'ADD',\n      'operand': showcase_ad\n  }\n\n  # Make the mutate request to add the ProductAd to the AdGroup\n  showcase_ad = ad_group_ad_service.mutate([ad_operation])['value'][0]\n\n  print 'ShowcaseAd with ID \"%s\" was added.' % showcase_ad['ad']['id']\n\n  return showcase_ad"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef UploadImage(client, filepath):\n  media_service = client.GetService('MediaService', 'v201809')\n\n  with open(filepath, 'rb') as image_handle:\n    image_data = image_handle.read().decode('utf-8')\n\n  image = [{\n      'xsi_type': 'Image',\n      'data': image_data,\n      'type': 'IMAGE'\n  }]\n\n  image = media_service.upload(image)[0]\n\n  return image", "response": "Uploads a. jpg image with the given filepath via the AdWords MediaService."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef CreateProductPartition(client, adgroup_id):\n  ad_group_criterion_service = client.GetService('AdGroupCriterionService',\n                                                 'v201809')\n  helper = ProductPartitionHelper(adgroup_id)\n  root = helper.CreateSubdivision()\n\n  new_product_canonical_condition = {\n      'xsi_type': 'ProductCanonicalCondition',\n      'condition': 'NEW'\n  }\n\n  used_product_canonical_condition = {\n      'xsi_type': 'ProductCanonicalCondition',\n      'condition': 'USED'\n  }\n\n  other_product_canonical_condition = {\n      'xsi_type': 'ProductCanonicalCondition',\n  }\n\n  helper.CreateUnit(root, new_product_canonical_condition)\n  helper.CreateUnit(root, used_product_canonical_condition)\n  helper.CreateUnit(root, other_product_canonical_condition)\n\n  result = ad_group_criterion_service.mutate(helper.operations)\n  return result['value']", "response": "Creates a ProductPartition tree for the given AdGroup ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a subdivision node.", "response": "def CreateSubdivision(self, parent=None, value=None):\n    \"\"\"Creates a subdivision node.\n\n    Args:\n      parent: The node that should be this node's parent.\n      value: The value being partitioned on.\n    Returns:\n      A new subdivision node.\n    \"\"\"\n    division = {\n        'xsi_type': 'ProductPartition',\n        'partitionType': 'SUBDIVISION',\n        'id': str(self.next_id)\n    }\n\n    # The root has neither a parent nor a value.\n    if parent is not None:\n      division['parentCriterionId'] = parent['id']\n      division['caseValue'] = value\n\n    adgroup_criterion = {\n        'xsi_type': 'BiddableAdGroupCriterion',\n        'adGroupId': self.adgroup_id,\n        'criterion': division\n    }\n\n    self.CreateAddOperation(adgroup_criterion)\n    self.next_id -= 1\n\n    return division"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CreateUnit(self, parent=None, value=None, bid_amount=None):\n    unit = {\n        'xsi_type': 'ProductPartition',\n        'partitionType': 'UNIT'\n    }\n\n    # The root node has neither a parent nor a value.\n    if parent is not None:\n      unit['parentCriterionId'] = parent['id']\n      unit['caseValue'] = value\n\n    if bid_amount is not None and bid_amount > 0:\n      # Note: Showcase ads require that the campaign has a ManualCpc\n      # BiddingStrategyConfiguration.\n      bidding_strategy_configuration = {\n          'bids': [{\n              'xsi_type': 'CpcBid',\n              'bid': {\n                  'xsi_type': 'Money',\n                  'microAmount': str(bid_amount)\n              }\n          }]\n      }\n\n      adgroup_criterion = {\n          'xsi_type': 'BiddableAdGroupCriterion',\n          'biddingStrategyConfiguration': bidding_strategy_configuration\n      }\n    else:\n      adgroup_criterion = {\n          'xsi_type': 'NegativeAdGroupCriterion'\n      }\n\n    adgroup_criterion['adGroupId'] = self.adgroup_id\n    adgroup_criterion['criterion'] = unit\n\n    self.CreateAddOperation(adgroup_criterion)\n\n    return unit", "response": "Creates a new unit node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving and display the access and refresh token.", "response": "def main(client_id, client_secret, scopes):\n  \"\"\"Retrieve and display the access and refresh token.\"\"\"\n  client_config = ClientConfigBuilder(\n      client_type=ClientConfigBuilder.CLIENT_TYPE_WEB, client_id=client_id,\n      client_secret=client_secret)\n\n  flow = InstalledAppFlow.from_client_config(\n      client_config.Build(), scopes=scopes)\n  # Note that from_client_config will not produce a flow with the\n  # redirect_uris (if any) set in the client_config. This must be set\n  # separately.\n  flow.redirect_uri = _REDIRECT_URI\n\n  auth_url, _ = flow.authorization_url(prompt='consent')\n\n  print ('Log into the Google Account you use to access your Ad Manager account'\n         'and go to the following URL: \\n%s\\n' % (auth_url))\n  print 'After approving the token enter the verification code (if specified).'\n  code = raw_input('Code: ').strip()\n\n  try:\n    flow.fetch_token(code=code)\n  except InvalidGrantError as ex:\n    print 'Authentication has failed: %s' % ex\n    sys.exit(1)\n\n  print 'Access token: %s' % flow.credentials.token\n  print 'Refresh token: %s' % flow.credentials.refresh_token"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate the budget. Args: client: an AdWordsClient instance. Returns: a suds.sudsobject.Object representation of the created budget.", "response": "def _CreateBudget(client):\n  \"\"\"Creates the budget.\n\n  Args:\n    client: an AdWordsClient instance.\n\n  Returns:\n    a suds.sudsobject.Object representation of the created budget.\n  \"\"\"\n  budget_service = client.GetService('BudgetService', version='v201809')\n\n  # Create the campaign budget\n  operation = {\n      'operand': {\n          'name': 'Interplanetary Cruise Budget #%d' % uuid.uuid4(),\n          'deliveryMethod': 'STANDARD',\n          'amount': {\n              'microAmount': 500000\n          }\n      },\n      'operator': 'ADD'\n  }\n\n  budget = budget_service.mutate([operation])['value'][0]\n\n  print 'Budget with ID \"%d\" and name \"%s\" was created.' % (\n      budget['budgetId'], budget['name'])\n\n  return budget"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CreateCampaign(client, budget):\n  campaign_service = client.GetService('CampaignService')\n\n  operations = [{\n      'operator': 'ADD',\n      'operand': {\n          'name': 'Interplanetary Cruise #%d' % uuid.uuid4(),\n          # Recommendation: Set the campaign to PAUSED when creating it to\n          # prevent the ads from immediately serving. Set to ENABLED once you've\n          # added targeting and the ads are ready to serve.\n          'status': 'PAUSED',\n          'advertisingChannelType': 'SEARCH',\n          'biddingStrategyConfiguration': {\n              'biddingStrategyType': 'MANUAL_CPC',\n          },\n          'budget': budget,\n          # Required: Set the campaign's Dynamic Search Ad settings.\n          'settings': [{\n              'xsi_type': 'DynamicSearchAdsSetting',\n              # Required: Set the domain name and language.\n              'domainName': 'example.com',\n              'languageCode': 'en'\n          }],\n          # Optional: Set the start date.\n          'startDate': (datetime.datetime.now() +\n                        datetime.timedelta(1)).strftime('%Y%m%d'),\n          # Optional: Set the end date.\n          'endDate': (datetime.datetime.now() +\n                      datetime.timedelta(365)).strftime('%Y%m%d'),\n      }\n  }]\n\n  campaign = campaign_service.mutate(operations)['value'][0]\n  campaign_id = campaign['id']\n\n  print 'Campaign with ID \"%d\" and name \"%s\" was added.' % (\n      campaign_id, campaign['name'])\n\n  return campaign_id", "response": "Creates the campaign.\n\n  Args:\n    client: an AdWordsClient instance.\n    budget: a suds.sudsobject.Object representation of a created budget.\n\n  Returns:\n    An integer campaign ID."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _CreateAdGroup(client, campaign_id):\n  ad_group_service = client.GetService('AdGroupService')\n\n  operations = [{\n      'operator': 'ADD',\n      'operand': {\n          'campaignId': campaign_id,\n          'adGroupType': 'SEARCH_DYNAMIC_ADS',\n          'name': 'Earth to Mars Cruises #%d' % uuid.uuid4(),\n          'status': 'PAUSED',\n          'biddingStrategyConfiguration': {\n              'bids': [{\n                  'xsi_type': 'CpcBid',\n                  'bid': {\n                      'microAmount': '3000000'\n                  },\n              }]\n          }\n      }\n  }]\n\n  ad_group = ad_group_service.mutate(operations)['value'][0]\n  ad_group_id = ad_group['id']\n\n  print 'Ad group with ID \"%d\" and name \"%s\" was created.' % (\n      ad_group_id, ad_group['name'])\n\n  return ad_group_id", "response": "Creates an ad group."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating the expanded Dynamic Search Ad.", "response": "def _CreateExpandedDSA(client, ad_group_id):\n  \"\"\"Creates the expanded Dynamic Search Ad.\n\n  Args:\n    client: an AdwordsClient instance.\n    ad_group_id: an integer ID of the ad group in which the DSA is added.\n  \"\"\"\n  # Get the AdGroupAdService.\n  ad_group_ad_service = client.GetService('AdGroupAdService')\n\n  # Create the operation\n  operations = [{\n      'operator': 'ADD',\n      'operand': {\n          'xsi_type': 'AdGroupAd',\n          'adGroupId': ad_group_id,\n          # Create the expanded dynamic search ad. This ad will have its\n          # headline and final URL auto-generated at serving time according to\n          # domain name specific information provided by DynamicSearchAdsSetting\n          # at the campaign level.\n          'ad': {\n              'xsi_type': 'ExpandedDynamicSearchAd',\n              # Set the ad description.\n              'description': 'Buy your tickets now!',\n              'description2': 'Discount ends soon'\n          },\n          # Optional: Set the status.\n          'status': 'PAUSED',\n      }\n  }]\n\n  # Create the ad.\n  ad = ad_group_ad_service.mutate(operations)['value'][0]['ad']\n\n  # Display the results.\n  print ('Expanded dynamic search ad with ID \"%d\", description \"%s\", and '\n         'description 2 \"%s\" was added'\n         % (ad['id'], ad['description'], ad['description2']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a web page criterion to the Dynamic Search Ads.", "response": "def _AddWebPageCriteria(client, ad_group_id):\n  \"\"\"Adds a web page criterion to target Dynamic Search Ads.\n\n  Args:\n    client: an AdWordsClient instance.\n    ad_group_id: an integer ID of the ad group the criteria is being added to.\n  \"\"\"\n  ad_group_criterion_service = client.GetService('AdGroupCriterionService',\n                                                 version='v201809')\n\n  operations = [{\n      'operator': 'ADD',\n      # Create biddable ad group criterion.\n      'operand': {\n          'xsi_type': 'BiddableAdGroupCriterion',\n          'adGroupId': ad_group_id,\n          # Create a webpage criterion for special offers for children.\n          'criterion': {\n              'xsi_type': 'Webpage',\n              'parameter': {\n                  'criterionName': 'Special offers for children.',\n                  'conditions': [\n                      {\n                          'operand': 'URL',\n                          'argument': '/marscruise/children'\n                      },\n                      {\n                          'operand': 'PAGE_TITLE',\n                          'argument': 'Special Offer'\n                      }\n                  ]\n              }\n          },\n          'userStatus': 'PAUSED',\n          # Optional: set a custom bid.\n          'biddingStrategyConfiguration': {\n              'bids': [{\n                  'xsi_type': 'CpcBid',\n                  'bid': {\n                      'microAmount': 10000000L\n                  }\n              }]\n          }\n      }\n  }]\n\n  criterion = ad_group_criterion_service.mutate(operations)['value'][0]\n\n  print 'Webpage criterion with ID \"%d\" was added to ad group ID \"%d\".' % (\n      criterion['criterion']['id'], criterion['adGroupId'])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef UploadImageAsset(client, url):\n  # Initialize appropriate service.\n  asset_service = client.GetService('AssetService', version='v201809')\n\n  # Download the image.\n  image_request = requests.get(url)\n\n  # Create the image asset.\n  image_asset = {\n      'xsi_type': 'ImageAsset',\n      'imageData': image_request.content,\n      # This field is optional, and if provided should be unique.\n      # 'assetName': 'Image asset ' + str(uuid.uuid4()),\n  }\n\n  # Create the operation.\n  operation = {\n      'operator': 'ADD',\n      'operand': image_asset\n  }\n\n  # Create the asset and return the ID.\n  result = asset_service.mutate([operation])\n\n  return result['value'][0]['assetId']", "response": "Uploads the image from the specified url."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngenerate a library signature suitable for a user - supplied user - agent field.", "response": "def GenerateLibSig(short_name):\n  \"\"\"Generates a library signature suitable for a user agent field.\n\n  Args:\n    short_name: The short, product-specific string name for the library.\n  Returns:\n    A library signature string to append to user-supplied user-agent value.\n  \"\"\"\n  with _UTILITY_LOCK:\n    utilities_used = ', '.join([utility for utility\n                                in sorted(_utility_registry)])\n    _utility_registry.Clear()\n\n  if utilities_used:\n    return ' (%s, %s, %s, %s)' % (short_name, _COMMON_LIB_SIG, _PYTHON_VERSION,\n                                  utilities_used)\n  else:\n    return ' (%s, %s, %s)' % (short_name, _COMMON_LIB_SIG, _PYTHON_VERSION)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef LoadFromString(yaml_doc, product_yaml_key, required_client_values,\n                   optional_product_values):\n  \"\"\"Loads the data necessary for instantiating a client from file storage.\n\n  In addition to the required_client_values argument, the yaml file must supply\n  the keys used to create OAuth2 credentials. It may also optionally set proxy\n  configurations.\n\n  Args:\n    yaml_doc: the yaml document whose keys should be used.\n    product_yaml_key: The key to read in the yaml as a string.\n    required_client_values: A tuple of strings representing values which must\n      be in the yaml file for a supported API. If one of these keys is not in\n      the yaml file, an error will  be raised.\n    optional_product_values: A tuple of strings representing optional values\n      which may be in the yaml file.\n\n  Returns:\n    A dictionary map of the keys in the yaml file to their values. This will not\n    contain the keys used for OAuth2 client creation and instead will have a\n    GoogleOAuth2Client object stored in the 'oauth2_client' field.\n\n  Raises:\n    A GoogleAdsValueError if the given yaml file does not contain the\n    information necessary to instantiate a client object - either a\n    required_client_values key was missing or an OAuth2 key was missing.\n  \"\"\"\n  data = yaml.safe_load(yaml_doc) or {}\n\n  if 'dfp' in data:\n    raise googleads.errors.GoogleAdsValueError(\n        'Please replace the \"dfp\" key in the configuration YAML string with'\n        '\"ad_manager\" to fix this issue.')\n\n  logging_config = data.get(_LOGGING_KEY)\n  if logging_config:\n    logging.config.dictConfig(logging_config)\n\n  try:\n    product_data = data[product_yaml_key]\n  except KeyError:\n    raise googleads.errors.GoogleAdsValueError(\n        'The \"%s\" configuration is missing'\n        % (product_yaml_key,))\n\n  if not isinstance(product_data, dict):\n    raise googleads.errors.GoogleAdsValueError(\n        'The \"%s\" configuration is empty or invalid'\n        % (product_yaml_key,))\n\n  IncludeUtilitiesInUserAgent(data.get(_UTILITY_REGISTER_YAML_KEY, True))\n\n  original_keys = list(product_data.keys())\n  client_kwargs = {}\n  try:\n    for key in required_client_values:\n      client_kwargs[key] = product_data[key]\n      del product_data[key]\n  except KeyError:\n    raise googleads.errors.GoogleAdsValueError(\n        'Some of the required values are missing. Required '\n        'values are: %s, actual values are %s'\n        % (required_client_values, original_keys))\n\n  proxy_config_data = data.get(_PROXY_CONFIG_KEY, {})\n  proxy_config = _ExtractProxyConfig(product_yaml_key, proxy_config_data)\n  client_kwargs['proxy_config'] = proxy_config\n  client_kwargs['oauth2_client'] = _ExtractOAuth2Client(\n      product_yaml_key, product_data, proxy_config)\n\n  client_kwargs[ENABLE_COMPRESSION_KEY] = data.get(\n      ENABLE_COMPRESSION_KEY, False)\n\n  client_kwargs[CUSTOM_HEADERS_KEY] = data.get(CUSTOM_HEADERS_KEY, None)\n\n  if SOAP_IMPLEMENTATION_KEY in data:\n    client_kwargs[SOAP_IMPLEMENTATION_KEY] = data[SOAP_IMPLEMENTATION_KEY]\n\n  for value in optional_product_values:\n    if value in product_data:\n      client_kwargs[value] = product_data[value]\n      del product_data[value]\n\n  if product_data:\n    warnings.warn('Could not recognize the following keys: %s. '\n                  'They were ignored.' % (product_data,), stacklevel=3)\n\n  return client_kwargs", "response": "Loads the data necessary for instantiating a GoogleOAuth2Client from a given YAML document."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef LoadFromStorage(path, product_yaml_key, required_client_values,\n                    optional_product_values):\n  \"\"\"Loads the data necessary for instantiating a client from file storage.\n\n  In addition to the required_client_values argument, the yaml file must supply\n  the keys used to create OAuth2 credentials. It may also optionally set proxy\n  configurations.\n\n  Args:\n    path: A path string to the yaml document whose keys should be used.\n    product_yaml_key: The key to read in the yaml as a string.\n    required_client_values: A tuple of strings representing values which must\n      be in the yaml file for a supported API. If one of these keys is not in\n      the yaml file, an error will  be raised.\n    optional_product_values: A tuple of strings representing optional values\n      which may be in the yaml file.\n\n  Returns:\n    A dictionary map of the keys in the yaml file to their values. This will not\n    contain the keys used for OAuth2 client creation and instead will have a\n    GoogleOAuth2Client object stored in the 'oauth2_client' field.\n\n  Raises:\n    A GoogleAdsValueError if the given yaml file does not contain the\n    information necessary to instantiate a client object - either a\n    required_client_values key was missing or an OAuth2 key was missing.\n  \"\"\"\n\n  if not os.path.isabs(path):\n    path = os.path.expanduser(path)\n\n  try:\n    with open(path, 'rb') as handle:\n      yaml_doc = handle.read()\n  except IOError:\n    raise googleads.errors.GoogleAdsValueError(\n        'Given yaml file, %s, could not be opened.' % path)\n\n  try:\n    client_kwargs = LoadFromString(yaml_doc, product_yaml_key,\n                                   required_client_values,\n                                   optional_product_values)\n  except googleads.errors.GoogleAdsValueError as e:\n    raise googleads.errors.GoogleAdsValueError(\n        'Given yaml file, %s, could not find some keys. %s' % (path, e))\n\n  return client_kwargs", "response": "Loads the data necessary for instantiating a GoogleOAuth2Client object from a given file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nextract an OAuth2Client subclass from the given product_data.", "response": "def _ExtractOAuth2Client(product_yaml_key, product_data, proxy_config):\n  \"\"\"Generates an GoogleOAuth2Client subclass using the given product_data.\n\n  Args:\n    product_yaml_key: a string key identifying the product being configured.\n    product_data: a dict containing the configurations for a given product.\n    proxy_config: a ProxyConfig instance.\n\n  Returns:\n    An instantiated GoogleOAuth2Client subclass.\n\n  Raises:\n    A GoogleAdsValueError if the OAuth2 configuration for the given product is\n    misconfigured.\n  \"\"\"\n  oauth2_kwargs = {\n      'proxy_config': proxy_config\n  }\n\n  if all(config in product_data for config in _OAUTH2_INSTALLED_APP_KEYS):\n    oauth2_args = [\n        product_data['client_id'], product_data['client_secret'],\n        product_data['refresh_token']\n    ]\n    oauth2_client = googleads.oauth2.GoogleRefreshTokenClient\n    for key in _OAUTH2_INSTALLED_APP_KEYS:\n      del product_data[key]\n  elif all(config in product_data for config in _OAUTH2_SERVICE_ACCT_KEYS):\n    oauth2_args = [\n        product_data['path_to_private_key_file'],\n        googleads.oauth2.GetAPIScope(product_yaml_key),\n    ]\n    oauth2_kwargs.update({\n        'sub': product_data.get('delegated_account')\n    })\n    oauth2_client = googleads.oauth2.GoogleServiceAccountClient\n    for key in _OAUTH2_SERVICE_ACCT_KEYS:\n      del product_data[key]\n    for optional_key in _OAUTH2_SERVICE_ACCT_KEYS_OPTIONAL:\n      if optional_key in product_data:\n        del product_data[optional_key]\n  else:\n    raise googleads.errors.GoogleAdsValueError(\n        'Your yaml file is incorrectly configured for OAuth2. You need to '\n        'specify credentials for either the installed application flow (%s) '\n        'or service account flow (%s).' %\n        (_OAUTH2_INSTALLED_APP_KEYS, _OAUTH2_SERVICE_ACCT_KEYS))\n\n  return oauth2_client(*oauth2_args, **oauth2_kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts a ProxyConfig instance from the given proxy_config_data.", "response": "def _ExtractProxyConfig(product_yaml_key, proxy_config_data):\n  \"\"\"Returns an initialized ProxyConfig using the given proxy_config_data.\n\n  Args:\n    product_yaml_key: a string indicating the client being loaded.\n    proxy_config_data: a dict containing the contents of proxy_config from the\n      YAML file.\n\n  Returns:\n    If there is a proxy to configure in proxy_config, this will return a\n    ProxyConfig instance with those settings. Otherwise, it will return None.\n\n  Raises:\n    A GoogleAdsValueError if one of the required keys specified by _PROXY_KEYS\n    is missing.\n  \"\"\"\n  cafile = proxy_config_data.get('cafile', None)\n  disable_certificate_validation = proxy_config_data.get(\n      'disable_certificate_validation', False)\n\n  http_proxy = proxy_config_data.get(_HTTP_PROXY_YAML_KEY)\n  https_proxy = proxy_config_data.get(_HTTPS_PROXY_YAML_KEY)\n  proxy_config = ProxyConfig(\n      http_proxy=http_proxy,\n      https_proxy=https_proxy,\n      cafile=cafile,\n      disable_certificate_validation=disable_certificate_validation)\n\n  return proxy_config"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\npack a SOAP request into the format we want for suds.", "response": "def _PackForSuds(obj, factory, packer=None, version=None):\n  \"\"\"Packs SOAP input into the format we want for suds.\n\n  The main goal here is to pack dictionaries with an 'xsi_type' key into\n  objects. This allows dictionary syntax to be used even with complex types\n  extending other complex types. The contents of dictionaries and lists/tuples\n  are recursively packed. Mutable types are copied - we don't mutate the input.\n\n  Args:\n    obj: A parameter for a SOAP request which will be packed. If this is\n        a dictionary or list, the contents will recursively be packed. If this\n        is not a dictionary or list, the contents will be recursively searched\n        for instances of unpacked dictionaries or lists.\n    factory: The suds.client.Factory object which can create instances of the\n        classes generated from the WSDL.\n    packer: An optional subclass of googleads.common.SoapPacker that provides\n        customized packing logic.\n    version: the version of the current API, e.g. 'v201811'\n\n  Returns:\n    If the given obj was a dictionary that contained the 'xsi_type' key, this\n    will be an instance of a class generated from the WSDL. Otherwise, this will\n    be the same data type as the input obj was.\n  \"\"\"\n  if packer:\n    obj = packer.Pack(obj, version)\n\n  if obj in ({}, None):\n    # Force suds to serialize empty objects. There are legitimate use cases for\n    # this, for example passing in an empty SearchCriteria object to a DFA\n    # search method in order to select everything.\n    return suds.null()\n  elif isinstance(obj, dict):\n    if 'xsi_type' in obj:\n      try:\n        new_obj = factory.create(obj['xsi_type'])\n      except suds.TypeNotFound:\n        new_obj = factory.create(':'.join(['ns0', obj['xsi_type']]))\n      # Suds sends an empty XML element for enum types which are not set. None\n      # of Google's Ads APIs will accept this. Initializing all of the fields in\n      # a suds object to None will ensure that they don't get serialized at all\n      # unless the user sets a value. User values explicitly set to None will be\n      # packed into a suds.null() object.\n      for param, _ in new_obj:\n        # Another problem is that the suds.mx.appender.ObjectAppender won't\n        # serialize object types with no fields set, but both AdWords and Ad\n        # Manager rely on sending objects with just the xsi:type set. The\n        # below \"if\" statement is an ugly hack that gets this to work in all(?)\n        # situations by taking advantage of the fact that these classes\n        # generally all have a type field. The only other option is to monkey\n        # patch ObjectAppender.\n        if param.endswith('.Type'):\n          setattr(new_obj, param, obj['xsi_type'])\n        else:\n          setattr(new_obj, param, None)\n      for key in obj:\n        if key == 'xsi_type': continue\n        setattr(new_obj, key, _PackForSuds(obj[key], factory,\n                                           packer=packer))\n    else:\n      new_obj = {}\n      for key in obj:\n        new_obj[key] = _PackForSuds(obj[key], factory,\n                                    packer=packer)\n    return new_obj\n  elif isinstance(obj, (list, tuple)):\n    return [_PackForSuds(item, factory,\n                         packer=packer) for item in obj]\n  else:\n    _RecurseOverObject(obj, factory)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _RecurseOverObject(obj, factory, parent=None):\n  if _IsSudsIterable(obj):\n    # Since in-place modification of the Suds object is taking place, the\n    # iterator should be done over a frozen copy of the unpacked fields.\n    copy_of_obj = tuple(obj)\n    for item in copy_of_obj:\n      if _IsSudsIterable(item):\n        if 'xsi_type' in item:\n          if isinstance(obj, tuple):\n            parent[obj[0]] = _PackForSuds(obj[1], factory)\n          else:\n            obj.remove(item)\n            obj.append(_PackForSuds(item, factory))\n        _RecurseOverObject(item, factory, obj)", "response": "Recursively walks over a nested structure to look for changes in Suds objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ExtractRequestSummaryFields(document):\n  headers = document.childAtPath('Header/RequestHeader')\n  body = document.childAtPath('Body')\n\n  summary_fields = {\n      'methodName': body.getChildren()[0].name\n  }\n\n  # Extract AdWords-specific fields if they exist.\n  # Note: We need to check if None because this will always evaluate False.\n  client_customer_id = headers.getChild('clientCustomerId')\n  if client_customer_id is not None:\n    summary_fields['clientCustomerId'] = client_customer_id.text\n\n  # Extract Ad Manager-specific fields if they exist.\n  # Note: We need to check if None because this will always evaluate False.\n  network_code = headers.getChild('networkCode')\n  if network_code is not None:\n    summary_fields['networkCode'] = network_code.text\n\n  return summary_fields", "response": "Extracts the logging fields from the request s suds. sax. Element.\n document."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ExtractResponseSummaryFields(document):\n  headers = document.childAtPath('Envelope/Header/ResponseHeader')\n  body = document.childAtPath('Envelope/Body')\n  summary_fields = {}\n\n  if headers is not None:\n    summary_fields['requestId'] = headers.getChild('requestId').text\n    summary_fields['responseTime'] = headers.getChild('responseTime').text\n\n    # Extract AdWords-specific summary fields if they are present.\n    # Note: We need to check if None because this will always evaluate False.\n    service_name = headers.getChild('serviceName')\n    if service_name is not None:\n      summary_fields['serviceName'] = service_name.text\n\n    method_name = headers.getChild('methodName')\n    if method_name is not None:\n      summary_fields['methodName'] = method_name.text\n\n    operations = headers.getChild('operations')\n    if operations is not None:\n      summary_fields['operations'] = operations.text\n\n  if body is not None:\n    # Extract fault if it exists.\n    fault = body.getChild('Fault')\n    if fault is not None:\n      summary_fields['isFault'] = True\n      # Cap length of faultstring to 16k characters for summary.\n      summary_fields['faultMessage'] = fault.getChild(\n          'faultstring').text[:16000]\n    else:\n      summary_fields['isFault'] = False\n\n  return summary_fields", "response": "Extracts the summary fields from the response s suds. sax. document. Document."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _InitSSLContext(self, cafile=None,\n                      disable_ssl_certificate_validation=False):\n    \"\"\"Creates a ssl.SSLContext with the given settings.\n\n    Args:\n      cafile: A str identifying the resolved path to the cafile. If not set,\n        this will use the system default cafile.\n      disable_ssl_certificate_validation: A boolean indicating whether\n        certificate verification is disabled. For security purposes, it is\n        highly recommended that certificate verification remain enabled.\n\n    Returns:\n      An ssl.SSLContext instance, or None if the version of Python being used\n      doesn't support it.\n    \"\"\"\n    # Attempt to create a context; this should succeed in Python 2 versions\n    # 2.7.9+ and Python 3 versions 3.4+.\n    try:\n      if disable_ssl_certificate_validation:\n        ssl._create_default_https_context = ssl._create_unverified_context\n        ssl_context = ssl.create_default_context()\n      else:\n        ssl_context = ssl.create_default_context(cafile=cafile)\n    except AttributeError:\n      # Earlier versions lack ssl.create_default_context()\n      # Rather than raising the exception, no context will be provided for\n      # legacy support. Of course, this means no certificate validation is\n      # taking place!\n      return None\n\n    return ssl_context", "response": "Initializes an ssl. SSLContext instance with the given settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetHandlers(self):\n    handlers = []\n\n    if self.ssl_context:\n      handlers.append(urllib2.HTTPSHandler(context=self.ssl_context))\n\n    if self.proxies:\n      handlers.append(urllib2.ProxyHandler(self.proxies))\n\n    return handlers", "response": "Retrieve the appropriate urllib2 handlers for the given configuration."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef u2handlers(self):\n    # Start with the default set of handlers.\n    return_handlers = suds.transport.http.HttpTransport.u2handlers(self)\n    return_handlers.extend(self.handlers)\n\n    return return_handlers", "response": "Get a collection of urllib2 handlers to be installed in the opener."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning an XML string representing a SOAP complex type.", "response": "def GetSoapXMLForComplexType(self, type_name, value):\n    \"\"\"Return an XML string representing a SOAP complex type.\n\n    Args:\n      type_name: The name of the type with namespace prefix if necessary.\n      value: A python dictionary to hydrate the type instance with.\n\n    Returns:\n      A string containing the SOAP XML for the type.\n    \"\"\"\n    schema = self.suds_client.wsdl.schema\n    definition_type = schema.elements[(type_name, self._namespace_override)]\n    marshaller = suds.mx.literal.Literal(schema)\n    content = suds.mx.Content(\n        tag=type_name, value=value,\n        name=type_name, type=definition_type)\n    data = marshaller.process(content)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetSoapXMLForComplexType(self, type_name, value):\n    element = self.schema.get_element(\n        '{%s}%s' % (self._namespace_override, type_name))\n    result_element = self._element_maker(element.qname.localname)\n    element_value = element(**value)\n    element.type.render(result_element, element_value)\n    data = lxml.etree.tostring(result_element).strip()\n    return data", "response": "Returns an XML string representing a SOAP complex type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef GetRequestXML(self, method, *args):\n    self.suds_client.set_options(nosend=True)\n    service_request = (getattr(self, method))(*args).envelope\n    self.suds_client.set_options(nosend=False)\n    return lxml.etree.fromstring(service_request)", "response": "Get the raw SOAP XML for a request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the headers for the underlying client.", "response": "def SetHeaders(self, soap_headers, http_headers):\n    \"\"\"Set the headers for the underlying client.\n\n    Args:\n      soap_headers: A SOAP element for the SOAP headers.\n      http_headers: A dictionary for the http headers.\n    \"\"\"\n    self.suds_client.set_options(soapheaders=soap_headers, headers=http_headers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _WsdlHasMethod(self, method_name):\n    return method_name in self.suds_client.wsdl.services[0].ports[0].methods", "response": "Returns True if the wsdl contains a method."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a method that can be used to make a SOAP request.", "response": "def _CreateMethod(self, method_name):\n    \"\"\"Create a method wrapping an invocation to the SOAP service.\n\n    Args:\n      method_name: A string identifying the name of the SOAP method to call.\n\n    Returns:\n      A callable that can be used to make the desired SOAP request.\n    \"\"\"\n    soap_service_method = getattr(self.suds_client.service, method_name)\n\n    def MakeSoapRequest(*args):\n      \"\"\"Perform a SOAP call.\"\"\"\n      AddToUtilityRegistry('suds')\n      self.SetHeaders(\n          self._header_handler.GetSOAPHeaders(self.CreateSoapElementForType),\n          self._header_handler.GetHTTPHeaders())\n\n      try:\n        return soap_service_method(\n            *[_PackForSuds(arg, self.suds_client.factory,\n                           self._packer) for arg in args])\n      except suds.WebFault as e:\n        if _logger.isEnabledFor(logging.WARNING):\n          _logger.warning('Response summary - %s',\n                          _ExtractResponseSummaryFields(e.document))\n\n        _logger.debug('SOAP response:\\n%s', e.document.str())\n\n        if not hasattr(e.fault, 'detail'):\n          exc = (googleads.errors.\n                 GoogleAdsServerFault(e.document, message=e.fault.faultstring))\n          raise exc  # Done this way for 2to3\n\n        # Before re-throwing the WebFault exception, an error object needs to be\n        # wrapped in a list for safe iteration.\n        fault = e.fault.detail.ApiExceptionFault\n        if not hasattr(fault, 'errors') or fault.errors is None:\n          exc = (googleads.errors.\n                 GoogleAdsServerFault(e.document, message=e.fault.faultstring))\n          raise exc  # Done this way for 2to3\n\n        obj = fault.errors\n        if not isinstance(obj, list):\n          fault.errors = [obj]\n\n        exc = googleads.errors.GoogleAdsServerFault(e.document, fault.errors,\n                                                    message=e.fault.faultstring)\n        raise exc  # Done this way for 2to3\n\n    return MakeSoapRequest"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\noverride the egress function to set our headers.", "response": "def egress(self, envelope, http_headers, operation, binding_options):\n    \"\"\"Overriding the egress function to set our headers.\n\n    Args:\n      envelope: An Element with the SOAP request data.\n      http_headers: A dict of the current http headers.\n      operation: The SoapOperation instance.\n      binding_options: An options dict for the SOAP binding.\n\n    Returns:\n      A tuple of the envelope and headers.\n    \"\"\"\n    custom_headers = self._header_handler.GetHTTPHeaders()\n    http_headers.update(custom_headers)\n    return envelope, http_headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the raw SOAP XML for a request.", "response": "def GetRequestXML(self, method, *args):\n    \"\"\"Get the raw SOAP XML for a request.\n\n    Args:\n      method: The method name.\n      *args: A list of arguments to be passed to the method.\n\n    Returns:\n      An element containing the raw XML that would be sent as the request.\n    \"\"\"\n    packed_args = self._PackArguments(method, args, set_type_attrs=True)\n    headers = self._GetZeepFormattedSOAPHeaders()\n\n    return self.zeep_client.create_message(\n        self.zeep_client.service, method, *packed_args, _soapheaders=headers)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _WsdlHasMethod(self, method_name):\n    try:\n      self._method_bindings.get(method_name)\n      return True\n    except ValueError:\n      return False", "response": "Determines if a method is in the wsdl."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string with the namespace of the service binding in the WSDL.", "response": "def _GetBindingNamespace(self):\n    \"\"\"Return a string with the namespace of the service binding in the WSDL.\"\"\"\n    return (list(self.zeep_client.wsdl.bindings.itervalues())[0]\n            .port_name.namespace)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npack the list of python dictionaries containing arguments into XML objects.", "response": "def _PackArguments(self, method_name, args, set_type_attrs=False):\n    \"\"\"Properly pack input dictionaries for zeep.\n\n    Pack a list of python dictionaries into XML objects. Dictionaries which\n    contain an 'xsi_type' entry are converted into that type instead of the\n    argument default. This allows creation of complex objects which include\n    inherited types.\n\n    Args:\n      method_name: The name of the method that will be called.\n      args: A list of dictionaries containing arguments to the method.\n      set_type_attrs: A boolean indicating whether or not attributes that end\n        in .Type should be set. This is only necessary for batch job service.\n\n    Returns:\n      A list of XML objects that can be passed to zeep.\n    \"\"\"\n    # Get the params for the method to find the initial types to instantiate.\n    op_params = self.zeep_client.get_element(\n        '{%s}%s' % (self._GetBindingNamespace(), method_name)).type.elements\n    result = [self._PackArgumentsHelper(param, param_data, set_type_attrs)\n              for ((_, param), param_data) in izip(op_params, args)]\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _IsBase64(cls, s):\n    try:\n      if base64.b64encode(base64.b64decode(s)).decode('utf-8') == s:\n        return True\n    except (TypeError, binascii.Error):\n      pass\n    return False", "response": "An imperfect but decent method for determining if a string is base64."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _DiscoverElementTypeFromLocalname(self, type_localname):\n    elem_type = None\n    last_exception = None\n    for ns_prefix in self.zeep_client.wsdl.types.prefix_map.values():\n      try:\n        elem_type = self.zeep_client.get_type(\n            '{%s}%s' % (ns_prefix, type_localname))\n      except zeep.exceptions.LookupError as e:\n        last_exception = e\n        continue\n      break\n    if not elem_type:\n      raise last_exception\n    return elem_type", "response": "Searches all namespaces for a type with the specified name."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a complex type from the given data.", "response": "def _CreateComplexTypeFromData(\n      self, elem_type, type_is_override, data, set_type_attrs):\n    \"\"\"Initialize a SOAP element with specific data.\n\n    Args:\n      elem_type: The type of the element to create.\n      type_is_override: A boolean specifying if the type is being overridden.\n      data: The data to hydrate the type with.\n      set_type_attrs: A boolean indicating whether or not attributes that end\n        in .Type should be set. This is only necessary for batch job service.\n\n    Returns:\n      An fully initialized SOAP element.\n    \"\"\"\n    elem_arguments = dict(elem_type.elements)\n\n    # A post order traversal of the original data, need to instantiate from\n    # the bottom up.\n    instantiated_arguments = {\n        k: self._PackArgumentsHelper(elem_arguments[k], v, set_type_attrs)\n        for k, v in data if k != 'xsi_type'}\n    if set_type_attrs:\n      found_type_attr = next((e_name for e_name, _ in elem_type.elements\n                              if e_name.endswith('.Type')), None)\n      if found_type_attr and type_is_override:\n        instantiated_arguments[found_type_attr] = elem_type.qname.localname\n    # Now go back through the tree instantiating SOAP types as we go.\n    return elem_type(**instantiated_arguments)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a dict with SOAP headers in the right format for zeep.", "response": "def _GetZeepFormattedSOAPHeaders(self):\n    \"\"\"Returns a dict with SOAP headers in the right format for zeep.\"\"\"\n    headers = self._header_handler.GetSOAPHeaders(self.CreateSoapElementForType)\n    soap_headers = {'RequestHeader': headers}\n    return soap_headers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a method that can be used to make a SOAP request.", "response": "def _CreateMethod(self, method_name):\n    \"\"\"Create a method wrapping an invocation to the SOAP service.\n\n    Args:\n      method_name: A string identifying the name of the SOAP method to call.\n\n    Returns:\n      A callable that can be used to make the desired SOAP request.\n    \"\"\"\n    soap_service_method = self.zeep_client.service[method_name]\n\n    def MakeSoapRequest(*args):\n      AddToUtilityRegistry('zeep')\n      soap_headers = self._GetZeepFormattedSOAPHeaders()\n      packed_args = self._PackArguments(method_name, args)\n      try:\n        return soap_service_method(\n            *packed_args, _soapheaders=soap_headers)['body']['rval']\n      except zeep.exceptions.Fault as e:\n        error_list = ()\n        if e.detail is not None:\n          underlying_exception = e.detail.find(\n              '{%s}ApiExceptionFault' % self._GetBindingNamespace())\n          fault_type = self.zeep_client.get_element(\n              '{%s}ApiExceptionFault' % self._GetBindingNamespace())\n          fault = fault_type.parse(\n              underlying_exception, self.zeep_client.wsdl.types)\n          error_list = fault.errors or error_list\n        raise googleads.errors.GoogleAdsServerFault(\n            e.detail, errors=error_list, message=e.message)\n    return MakeSoapRequest"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef DisplayTree(node, children, level=0):\n  value = ''\n  node_type = ''\n\n  if 'caseValue' in node:\n    case_value = node['caseValue']\n    node_type = case_value['ProductDimension.Type']\n\n    if node_type == 'ProductCanonicalCondition':\n      value = (case_value['condition'] if 'condition' in case_value\n               else 'OTHER')\n    elif node_type == 'ProductBiddingCategory':\n      value = '%s(%s)' % (case_value['type'], case_value['value']\n                          if 'value' in case_value else 'OTHER')\n    else:\n      value = (case_value['value'] if 'value' in case_value else 'OTHER')\n\n  print ('%sid: %s, node_type: %s, value: %s\\n'\n         % (' ' * level, node['id'], node_type, value))\n\n  for child_node in children[node['id']]:\n    DisplayTree(child_node, children, level + 1)", "response": "Recursively display a node and each of its children."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_all_ad_units(inventory_service):\n  # Create a statement to get all ad units.\n  statement = (ad_manager.StatementBuilder(version='v201811')\n               .OrderBy('id', ascending=True))\n\n  # Pull down all ad units into a list\n  keep_iterating = True\n  total_results = 0\n  found_ad_units = []\n  while keep_iterating:\n    page = inventory_service.getAdUnitsByStatement(statement.ToStatement())\n    if 'results' in page and len(page['results']):\n      total_results = page['totalResultSetSize']\n      found_ad_units.extend(page['results'])\n\n    statement.offset += statement.limit\n    keep_iterating = statement.offset < total_results\n\n  return found_ad_units", "response": "Download all ad units.\n Returns a list containing all ad units."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef display_hierarchy(root_ad_unit, all_ad_units):\n  # Create a dict mapping the ids of parents to lists of their children.\n  parent_id_to_children = collections.defaultdict(list)\n  for ad_unit in all_ad_units:\n    if 'parentId' in ad_unit:\n      parent_id_to_children[ad_unit['parentId']].append(ad_unit)\n  parent_id_to_children = dict(parent_id_to_children)\n\n  display_hierarchy_helper(root_ad_unit, parent_id_to_children, 0)", "response": "Display the ad units as a tree."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef display_hierarchy_helper(root, parent_id_to_children, depth):\n  print '%s%s (%s)' % ('%s+--' % ('|'.join(['  '] * depth)),\n                       root['name'], root['id'])\n\n  # Recurse for each child of this root that has children.\n  for child in parent_id_to_children.get(root['id'], []):\n    display_hierarchy_helper(child, parent_id_to_children, depth + 1)", "response": "Recursive helper for displaying the hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _CreateCampaignGroup(client):\n  # Get the CampaignGroupService.\n  campaign_group_service = client.GetService('CampaignGroupService',\n                                             version='v201809')\n\n  # Create the operation.\n  operations = [{\n      'operator': 'ADD',\n      # Create the campaign group.\n      'operand': {\n          'name': 'Mars campaign group #%d' % uuid.uuid4()\n      }\n  }]\n\n  campaign_group = campaign_group_service.mutate(operations)['value'][0]\n  campaign_group_id = campaign_group['id']\n\n  # Display the results.\n  print 'Campaign group with ID \"%d\" and name \"%s\" was created.' % (\n      campaign_group_id, campaign_group['name'])\n\n  return campaign_group_id", "response": "Creates a campaign group."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _AddCampaignsToGroup(client, campaign_group_id, campaign_ids):\n  # Get the CampaignService.\n  campaign_service = client.GetService('CampaignService', version='v201809')\n\n  # Create the operations.\n  operations = [{\n      'operator': 'SET',\n      'operand': {\n          'id': campaign_id,\n          'campaignGroupId': campaign_group_id\n      }\n  } for campaign_id in campaign_ids]\n\n  campaign_service.mutate(operations)\n\n  # Display the results.\n  print ('The following campaign IDs were added to the campaign group with ID '\n         '\"%d\":\\n\\t%s' % (campaign_group_id, campaign_ids))", "response": "Adds multiple campaigns to a campaign group."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _CreatePerformanceTarget(client, campaign_group_id):\n  # Get the CampaignGroupPerformanceTargetService.\n  cgpt_service = client.GetService('CampaignGroupPerformanceTargetService',\n                                   version='v201809')\n\n  # Create the operation.\n  operations = [{\n      'operator': 'ADD',\n      # Create the performance target.\n      'operand': {\n          'campaignGroupId': campaign_group_id,\n          'performanceTarget': {\n              # Keep the CPC for the campaigns < $3.\n              'efficiencyTargetType': 'CPC_LESS_THAN_OR_EQUAL_TO',\n              'efficiencyTargetValue': 3000000,\n              # Keep the maximum spend under $50.\n              'spendTargetType': 'MAXIMUM',\n              'spendTarget': {\n                  'microAmount': 500000000\n              },\n              # Aim for at least 3000 clicks.\n              'volumeGoalType': 'MAXIMIZE_CLICKS',\n              'volumeTargetValue': 3000,\n              # Start the performance target today, and run it for the next 90\n              # days.\n              'startDate': datetime.datetime.now().strftime('%Y%m%d'),\n              'endDate': (datetime.datetime.now() +\n                          datetime.timedelta(90)).strftime('%Y%m%d')\n          }\n      }\n  }]\n\n  cgpt = cgpt_service.mutate(operations)['value'][0]\n\n  # Display the results.\n  print ('Campaign performance target with ID \"%d\" was added for campaign '\n         'group ID \"%d\".' % (cgpt['id'], cgpt['campaignGroupId']))", "response": "Creates a performance target for the given campaign group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetService(self, service_name, version=sorted(_SERVICE_MAP.keys())[-1],\n                 server=None):\n    \"\"\"Creates a service client for the given service.\n\n    Args:\n      service_name: A string identifying which Ad Manager service to create a\n          service client for.\n      [optional]\n      version: A string identifying the Ad Manager version to connect to. This\n          defaults to what is currently the latest version. This will be\n          updated in future releases to point to what is then the\n          latest version.\n      server: A string identifying the webserver hosting the Ad Manager API.\n\n    Returns:\n      A googleads.common.GoogleSoapService instance which has the headers\n      and proxy configured for use.\n\n    Raises:\n      A GoogleAdsValueError if the service or version provided do not exist.\n    \"\"\"\n    if not server:\n      server = DEFAULT_ENDPOINT\n\n    server = server[:-1] if server[-1] == '/' else server\n\n    try:\n      service = googleads.common.GetServiceClassForLibrary(self.soap_impl)(\n          self._SOAP_SERVICE_FORMAT % (server, version, service_name),\n          self._header_handler,\n          _AdManagerPacker,\n          self.proxy_config,\n          self.timeout,\n          version,\n          cache=self.cache)\n\n      return service\n    except googleads.errors.GoogleAdsSoapTransportError:\n      if version in _SERVICE_MAP:\n        if service_name in _SERVICE_MAP[version]:\n          raise\n        else:\n          raise googleads.errors.GoogleAdsValueError(\n              'Unrecognized service for the Ad Manager API. Service given: %s '\n              'Supported services: %s'\n              % (service_name, _SERVICE_MAP[version]))\n      else:\n        raise googleads.errors.GoogleAdsValueError(\n            'Unrecognized version of the Ad Manager API. Version given: %s '\n            'Supported versions: %s' % (version, _SERVICE_MAP.keys()))", "response": "Creates a service client for the given service name and version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetDataDownloader(self, version=sorted(_SERVICE_MAP.keys())[-1],\n                        server=None):\n    \"\"\"Creates a downloader for Ad Manager reports and PQL result sets.\n\n    This is a convenience method. It is functionally identical to calling\n    DataDownloader(ad_manager_client, version, server)\n\n    Args:\n      [optional]\n      version: A string identifying the Ad Manager version to connect to.\n          This defaults to what is currently the latest version. This will be\n          updated in future releases to point to what is then the\n          latest version.\n      server: A string identifying the webserver hosting the Ad Manager API.\n\n    Returns:\n      A DataDownloader tied to this AdManagerClient, ready to download reports.\n    \"\"\"\n    if not server:\n      server = DEFAULT_ENDPOINT\n\n    return DataDownloader(self, version, server)", "response": "Creates a downloader for the Ad Manager reports and PQL result sets."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the SOAP headers required for request authorization.", "response": "def GetSOAPHeaders(self, create_method):\n    \"\"\"Returns the SOAP headers required for request authorization.\n\n    Args:\n      create_method: The SOAP library specific method used to instantiate SOAP\n      objects.\n\n    Returns:\n      A SOAP object containing the headers.\n    \"\"\"\n    header = create_method(self._SOAP_HEADER_CLASS)\n    header.networkCode = self._ad_manager_client.network_code\n    header.applicationName = ''.join([\n        self._ad_manager_client.application_name,\n        googleads.common.GenerateLibSig(self._PRODUCT_SIG)])\n    return header"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npacks the given object using Ad Manager - specific logic.", "response": "def Pack(cls, obj, version):\n    \"\"\"Pack the given object using Ad Manager-specific logic.\n\n    Args:\n      obj: an object to be packed for SOAP using Ad Manager-specific logic, if\n          applicable.\n      version: the version of the current API, e.g. 'v201811'\n\n    Returns:\n      The given object packed with Ad Manager-specific logic for SOAP,\n      if applicable. Otherwise, returns the given object unmodified.\n    \"\"\"\n    if isinstance(obj, (datetime.datetime, datetime.date)):\n      return cls.AdManagerDateTimePacker(obj, version)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AdManagerDateTimePacker(cls, value, version):\n\n    if isinstance(value, datetime.datetime):\n      if value.tzinfo is None:\n        raise googleads.errors.GoogleAdsValueError(\n            'Datetime %s is not timezone aware.' % value\n        )\n      return {\n          'date': cls.AdManagerDateTimePacker(value.date(), version),\n          'hour': value.hour,\n          'minute': value.minute,\n          'second': value.second,\n          # As of V201811, timeZoneID was renamed timeZoneId\n          'timeZoneId' if version >= 'v201811' else 'timeZoneID':\n              value.tzinfo.zone,\n      }\n    elif isinstance(value, datetime.date):\n      return {'year': value.year, 'month': value.month, 'day': value.day}", "response": "Returns a dict formatted for Ad Manager SOAP based on date or datetime."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting a dictionary of python types into a list of PQL types.", "response": "def GetQueryValuesFromDict(cls, d, version=sorted(_SERVICE_MAP.keys())[-1]):\n    \"\"\"Converts a dict of python types into a list of PQL types.\n\n    Args:\n      d: A dictionary of variable names to python types.\n      version: A string identifying the Ad Manager version the values object\n          is compatible with. This defaults to what is currently the latest\n          version. This will be updated in future releases to point to what is\n          then the latest version.\n\n    Returns:\n      A list of variables formatted for PQL statements which are compatible with\n      a particular API version.\n    \"\"\"\n    return [{\n        'key': key,\n        'value': cls.GetValueRepresentation(value, version)\n    } for key, value in d.iteritems()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert a single python value to its PQL representation.", "response": "def GetValueRepresentation(cls, value,\n                             version=sorted(_SERVICE_MAP.keys())[-1]):\n    \"\"\"Converts a single python value to its PQL representation.\n\n    Args:\n      value: A python value.\n      version: A string identifying the Ad Manager version the value object\n          is compatible with. This defaults to what is currently the latest\n          version. This will be updated in future releases to point to what is\n          then the latest version.\n\n    Returns:\n      The value formatted for PQL statements which are compatible with a\n      particular API version.\n    \"\"\"\n    if isinstance(value, str) or isinstance(value, unicode):\n      return {'value': value, 'xsi_type': 'TextValue'}\n    elif isinstance(value, bool):\n      return {'value': value, 'xsi_type': 'BooleanValue'}\n    elif isinstance(value, numbers.Number):\n      return {'value': value, 'xsi_type': 'NumberValue'}\n    # It's important that datetime is checked for before date\n    # because isinstance(datetime.datetime.now(), datetime.date) is True\n    elif isinstance(value, datetime.datetime):\n      if value.tzinfo is None:\n        raise googleads.errors.GoogleAdsValueError(\n            'Datetime %s is not timezone aware.' % value\n        )\n\n      return {\n          'xsi_type': 'DateTimeValue',\n          'value': {\n              'date': {\n                  'year': value.year,\n                  'month': value.month,\n                  'day': value.day,\n              },\n              'hour': value.hour,\n              'minute': value.minute,\n              'second': value.second,\n              'timeZoneId' if version >= 'v201811' else 'timeZoneID':\n                  value.tzinfo.zone,\n          }\n      }\n    elif isinstance(value, datetime.date):\n      return {\n          'xsi_type': 'DateValue',\n          'value': {\n              'year': value.year,\n              'month': value.month,\n              'day': value.day,\n          }\n      }\n    elif isinstance(value, list):\n      if value and not all(isinstance(x, type(value[0])) for x in value):\n        raise googleads.errors.GoogleAdsValueError('Cannot pass more than one '\n                                                   'type in a set.')\n\n      return {\n          'xsi_type': 'SetValue',\n          'values': [cls.GetValueRepresentation(v, version) for v in value]\n      }\n    else:\n      raise googleads.errors.GoogleAdsValueError(\n          'Can\\'t represent unknown type: %s.' % type(value))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _GetReportService(self):\n    if not self._report_service:\n      self._report_service = self._ad_manager_client.GetService(\n          'ReportService', self._version, self._server)\n    return self._report_service", "response": "Returns a report service client."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _GetPqlService(self):\n    if not self._pql_service:\n      self._pql_service = self._ad_manager_client.GetService(\n          'PublisherQueryLanguageService', self._version, self._server)\n    return self._pql_service", "response": "Lazily initializes a PQL service client."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef WaitForReport(self, report_job):\n    service = self._GetReportService()\n    report_job_id = service.runReportJob(report_job)['id']\n\n    if self._version > 'v201502':\n      status = service.getReportJobStatus(report_job_id)\n    else:\n      status = service.getReportJob(report_job_id)['reportJobStatus']\n\n    while status != 'COMPLETED' and status != 'FAILED':\n      _data_downloader_logger.debug('Report job status: %s', status)\n      time.sleep(30)\n      if self._version > 'v201502':\n        status = service.getReportJobStatus(report_job_id)\n      else:\n        status = service.getReportJob(report_job_id)['reportJobStatus']\n\n    if status == 'FAILED':\n      raise googleads.errors.AdManagerReportError(report_job_id)\n    else:\n      _data_downloader_logger.debug('Report has completed successfully')\n      return report_job_id", "response": "Runs a report and waits for it to finish generating."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef DownloadReportToFile(self, report_job_id, export_format, outfile,\n                           include_report_properties=False,\n                           include_totals_row=None, use_gzip_compression=True):\n    \"\"\"Downloads report data and writes it to a file.\n\n    The report job must be completed before calling this function.\n\n    Args:\n      report_job_id: The ID of the report job to wait for, as a string.\n      export_format: The export format for the report file, as a string.\n      outfile: A writeable, file-like object to write to.\n      include_report_properties: Whether or not to include the report\n        properties (e.g. network, user, date generated...)\n        in the generated report.\n      include_totals_row: Whether or not to include the totals row.\n      use_gzip_compression: Whether or not to use gzip compression.\n    \"\"\"\n    service = self._GetReportService()\n\n    if include_totals_row is None:  # True unless CSV export if not specified\n      include_totals_row = True if export_format != 'CSV_DUMP' else False\n    opts = {\n        'exportFormat': export_format,\n        'includeReportProperties': include_report_properties,\n        'includeTotalsRow': include_totals_row,\n        'useGzipCompression': use_gzip_compression\n    }\n    report_url = service.getReportDownloadUrlWithOptions(report_job_id, opts)\n    _data_downloader_logger.info('Request Summary: Report job ID: %s, %s',\n                                 report_job_id, opts)\n\n    response = self.url_opener.open(report_url)\n\n    _data_downloader_logger.debug(\n        'Incoming response: %s %s REDACTED REPORT DATA', response.code,\n        response.msg)\n\n    while True:\n      chunk = response.read(_CHUNK_SIZE)\n      if not chunk: break\n      outfile.write(chunk)", "response": "Downloads report data and writes it to a file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading the results of a PQL query to a list.", "response": "def DownloadPqlResultToList(self, pql_query, values=None):\n    \"\"\"Downloads the results of a PQL query to a list.\n\n    Args:\n      pql_query: str a statement filter to apply (the query should not include\n                 the limit or the offset)\n      [optional]\n      values: A dict of python objects or a list of raw SOAP values to bind\n              to the pql_query.\n\n    Returns:\n      a list of lists with the first being the header row and each subsequent\n      list being a row of results.\n    \"\"\"\n    results = []\n    self._PageThroughPqlSet(pql_query, results.append, values)\n    return results"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload the results of a PQL query to CSV.", "response": "def DownloadPqlResultToCsv(self, pql_query, file_handle, values=None):\n    \"\"\"Downloads the results of a PQL query to CSV.\n\n    Args:\n      pql_query: str a statement filter to apply (the query should not include\n                 the limit or the offset)\n      file_handle: file the file object to write to.\n      [optional]\n      values: A dict of python objects or a list of raw SOAP values to bind\n              to the pql_query.\n    \"\"\"\n    pql_writer = csv.writer(file_handle, delimiter=',',\n                            quotechar='\"', quoting=csv.QUOTE_ALL)\n    self._PageThroughPqlSet(pql_query, pql_writer.writerow, values)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _ConvertValueForCsv(self, pql_value):\n    if 'value' in pql_value:\n      field = pql_value['value']\n    elif 'values' in pql_value:\n      field = pql_value['values']\n    else:\n      field = None\n\n    if field:\n      if isinstance(field, list):\n        if all(AdManagerClassType(single_field) == AdManagerClassType(field[0])\n               for single_field in field):\n          return ','.join([\n              '\"%s\"' % str(self._ConvertValueForCsv(single_field))\n              for single_field in field])\n        else:\n          raise googleads.errors.GoogleAdsValueError(\n              'The set value returned contains unsupported mix value types')\n\n      class_type = AdManagerClassType(pql_value)\n\n      if class_type == 'TextValue':\n        s = field.replace('\"', '\"\"')\n\n        # Encode UTF-8 characters for Python 2 only.\n        if sys.version_info.major < 3:\n          s = s.encode('UTF8')\n        return s\n      elif class_type == 'NumberValue':\n        return float(field) if '.' in field else int(field)\n      elif class_type == 'DateTimeValue':\n        return self._ConvertDateTimeToOffset(field)\n      elif class_type == 'DateValue':\n        return datetime.date(int(field['date']['year']),\n                             int(field['date']['month']),\n                             int(field['date']['day'])).isoformat()\n      else:\n        return field\n    else:\n      return '-'", "response": "Convert a single field value from a Value object to a CSV suitable format."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npaging through a pql query and performs an action.", "response": "def _PageThroughPqlSet(self, pql_query, output_function, values):\n    \"\"\"Pages through a pql_query and performs an action (output_function).\n\n    Args:\n      pql_query: str a statement filter to apply (the query should not include\n                 the limit or the offset)\n      output_function: the function to call to output the results (csv or in\n                       memory)\n      values: A dict of python objects or a list of raw SOAP values to bind\n              to the pql_query.\n    \"\"\"\n    if isinstance(values, dict):\n      values = PQLHelper.GetQueryValuesFromDict(values, self._version)\n\n    pql_service = self._GetPqlService()\n    current_offset = 0\n\n    while True:\n      query_w_limit_offset = '%s LIMIT %d OFFSET %d' % (pql_query,\n                                                        SUGGESTED_PAGE_LIMIT,\n                                                        current_offset)\n      response = pql_service.select({'query': query_w_limit_offset,\n                                     'values': values})\n\n      if 'rows' in response:\n        # Write the header row only on first pull\n        if current_offset == 0:\n          header = response['columnTypes']\n          output_function([label['labelName'] for label in header])\n\n        entities = response['rows']\n        result_set_size = len(entities)\n\n        for entity in entities:\n          output_function([self._ConvertValueForCsv(value) for value\n                           in entity['values']])\n\n        current_offset += result_set_size\n        if result_set_size != SUGGESTED_PAGE_LIMIT:\n          break\n      else:\n        break"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _ConvertDateTimeToOffset(self, date_time_value):\n    date_time_obj = datetime.datetime(int(date_time_value['date']['year']),\n                                      int(date_time_value['date']['month']),\n                                      int(date_time_value['date']['day']),\n                                      int(date_time_value['hour']),\n                                      int(date_time_value['minute']),\n                                      int(date_time_value['second']))\n    # v201808 is the last Ad Manager version to use timeZoneID.\n    if self._version > 'v201808':\n      time_zone_str = 'timeZoneId'\n    else:\n      time_zone_str = 'timeZoneID'\n    date_time_str = pytz.timezone(\n        date_time_value[time_zone_str]).localize(date_time_obj).isoformat()\n\n    if date_time_str[-5:] == '00:00':\n      return date_time_str[:-6] + 'Z'\n    else:\n      return date_time_str", "response": "Converts a date time value from the PQL formatted response to an offset string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a new BatchJob to upload operations to.", "response": "def AddBatchJob(client):\n  \"\"\"Add a new BatchJob to upload operations to.\n\n  Args:\n    client: an instantiated AdWordsClient used to retrieve the BatchJob.\n\n  Returns:\n    The new BatchJob created by the request.\n  \"\"\"\n  # Initialize appropriate service.\n  batch_job_service = client.GetService('BatchJobService', version='v201809')\n  # Create a BatchJob.\n  batch_job_operations = [{\n      'operand': {},\n      'operator': 'ADD'\n  }]\n  return batch_job_service.mutate(batch_job_operations)['value'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nbuilding the operations needed to add a Keyword Criterion to each AdGroup.", "response": "def BuildAdGroupCriterionOperations(adgroup_id):\n  \"\"\"Builds the operations adding a Keyword Criterion to each AdGroup.\n\n  Args:\n    adgroup_id: an integer identifying an AdGroup to associate the keywords\n      with.\n\n  Returns:\n    a list containing the operations that will create a new Keyword Criterion\n    associated with each provided AdGroup.\n  \"\"\"\n  criterion_operations = [\n      {\n          # You must specify the xsi_type of operations run by the\n          # BatchJobService.\n          'xsi_type': 'AdGroupCriterionOperation',\n          'operand': {\n              'xsi_type': 'BiddableAdGroupCriterion',\n              'adGroupId': adgroup_id,\n              'criterion': {\n                  'xsi_type': 'Keyword',\n                  # Make 10% of keywords invalid to demonstrate error handling.\n                  'text': 'mars%s%s' % (uuid.uuid4(),\n                                        '!!!' if i % 10 == 0 else ''),\n                  'matchType': 'BROAD'\n              }\n          },\n          'operator': 'ADD'\n      }\n      for i in range(KEYWORD_COUNT)]\n\n  return criterion_operations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncanceling the given BatchJob.", "response": "def CancelBatchJob(client, batch_job, max_poll_attempts=MAX_POLL_ATTEMPTS):\n  \"\"\"Cancels the given BatchJob.\n\n  Args:\n    client: an instantiated AdWordsClient used to cancel the BatchJob.\n    batch_job: a BatchJob to be canceled.\n    max_poll_attempts: an int defining the number of times the BatchJob will be\n      checked to determine whether it has been canceled.\n  \"\"\"\n  batch_job_service = client.GetService('BatchJobService', 'v201809')\n  batch_job['status'] = 'CANCELING'\n\n  operation = {\n      'operator': 'SET',\n      'operand': batch_job\n  }\n\n  batch_job_service.mutate([operation])\n\n  # Verify that the Batch Job cancels.\n  poll_attempt = 0\n\n  while (poll_attempt in range(max_poll_attempts) and\n         batch_job['status'] != 'CANCELED'):\n    sleep_interval = (30 * (2 ** poll_attempt) +\n                      (random.randint(0, 10000) / 1000))\n    print ('Batch Job not finished canceling, sleeping for %s seconds.'\n           % sleep_interval)\n    time.sleep(sleep_interval)\n    batch_job = GetBatchJob(client, batch_job['id'])\n    poll_attempt += 1\n\n  if batch_job['status'] == 'CANCELED':\n    print ('Batch Job with ID \"%d\" has been successfully canceled.' %\n           batch_job['id'])\n  else:\n    print ('Batch Job with ID \"%d\" failed to cancel after polling %d times.'\n           % (batch_job['id'], max_poll_attempts))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef GetBatchJob(client, batch_job_id):\n  batch_job_service = client.GetService('BatchJobService', 'v201809')\n\n  selector = {\n      'fields': ['Id', 'Status', 'DownloadUrl'],\n      'predicates': [\n          {\n              'field': 'Id',\n              'operator': 'EQUALS',\n              'values': [batch_job_id]\n          }\n      ]\n  }\n\n  return batch_job_service.get(selector)['entries'][0]", "response": "Retrieves the BatchJob with the given id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetBatchJobDownloadUrlWhenReady(client, batch_job_id,\n                                    max_poll_attempts=MAX_POLL_ATTEMPTS):\n  \"\"\"Retrieves the downloadUrl when the BatchJob is complete.\n\n  Args:\n    client: an instantiated AdWordsClient used to poll the BatchJob.\n    batch_job_id: a long identifying the BatchJob to be polled.\n    max_poll_attempts: an int defining the number of times the BatchJob will be\n      checked to determine whether it has completed.\n\n  Returns:\n    A str containing the downloadUrl of the completed BatchJob.\n\n  Raises:\n    Exception: If the BatchJob hasn't finished after the maximum poll attempts\n      have been made.\n  \"\"\"\n  batch_job = GetBatchJob(client, batch_job_id)\n  if batch_job['status'] == 'CANCELED':\n    raise Exception('Batch Job with ID \"%s\" was canceled before completing.'\n                    % batch_job_id)\n\n  poll_attempt = 0\n\n  while (poll_attempt in range(max_poll_attempts) and\n         batch_job['status'] in PENDING_STATUSES):\n    sleep_interval = (30 * (2 ** poll_attempt) +\n                      (random.randint(0, 10000) / 1000))\n    print 'Batch Job not ready, sleeping for %s seconds.' % sleep_interval\n    time.sleep(sleep_interval)\n    batch_job = GetBatchJob(client, batch_job_id)\n    poll_attempt += 1\n\n    if 'downloadUrl' in batch_job:\n      url = batch_job['downloadUrl']['url']\n      print ('Batch Job with Id \"%s\", Status \"%s\", and DownloadUrl \"%s\" ready.'\n             % (batch_job['id'], batch_job['status'], url))\n      return url\n\n  print ('BatchJob with ID \"%s\" is being canceled because it was in a pending '\n         'state after polling %d times.' % (batch_job_id, max_poll_attempts))\n  CancelBatchJob(client, batch_job)", "response": "Retrieves the downloadUrl when the BatchJob is ready."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef PrintResponse(batch_job_helper, response_xml):\n  response = batch_job_helper.ParseResponse(response_xml)\n\n  if 'rval' in response['mutateResponse']:\n    for data in response['mutateResponse']['rval']:\n      if 'errorList' in data:\n        print 'Operation %s - FAILURE:' % data['index']\n        print '\\terrorType=%s' % data['errorList']['errors']['ApiError.Type']\n        print '\\ttrigger=%s' % data['errorList']['errors']['trigger']\n        print '\\terrorString=%s' % data['errorList']['errors']['errorString']\n        print '\\tfieldPath=%s' % data['errorList']['errors']['fieldPath']\n        print '\\treason=%s' % data['errorList']['errors']['reason']\n      if 'result' in data:\n        print 'Operation %s - SUCCESS.' % data['index']", "response": "Prints the BatchJobService response."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef AddAdGroup(self, client_customer_id, campaign_id, name, status):\n    self.client.SetClientCustomerId(client_customer_id)\n\n    ad_group_service = self.client.GetService('AdGroupService')\n    operations = [{\n        'operator': 'ADD',\n        'operand': {\n            'campaignId': campaign_id,\n            'name': name,\n            'status': status\n        }\n    }]\n    ad_group_service.mutate(operations)", "response": "Creates a new AdGroup."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a new Budget to the list of micros.", "response": "def AddBudget(self, client_customer_id, micro_amount):\n    \"\"\"Create a new Budget with the given microAmount.\n\n    Args:\n      client_customer_id: str Client Customer Id used to create Budget.\n      micro_amount: str The budget represented in micros.\n\n    Returns:\n      str BudgetId of the newly created Budget.\n    \"\"\"\n    self.client.SetClientCustomerId(client_customer_id)\n\n    budget_service = self.client.GetService('BudgetService')\n\n    operations = [{\n        'operator': 'ADD',\n        'operand': {\n            'name': 'Budget #%s' % time.time(),\n            'amount': {\n                'microAmount': micro_amount\n            },\n            'deliveryMethod': 'STANDARD'\n        }\n    }]\n\n    return budget_service.mutate(operations)['value'][0]['budgetId']"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a new Campaign to the client account.", "response": "def AddCampaign(self, client_customer_id, campaign_name, ad_channel_type,\n                  budget):\n    \"\"\"Add a Campaign to the client account.\n\n    Args:\n      client_customer_id: str Client Customer Id to use when creating Campaign.\n      campaign_name: str Name of the campaign to be added.\n      ad_channel_type: str Primary serving target the campaign's ads.\n      budget: str a budget amount (in micros) to use.\n    \"\"\"\n    self.client.SetClientCustomerId(client_customer_id)\n    campaign_service = self.client.GetService('CampaignService')\n    budget_id = self.AddBudget(client_customer_id, budget)\n\n    operations = [{\n        'operator': 'ADD',\n        'operand': {\n            'name': campaign_name,\n            'status': 'PAUSED',\n            'biddingStrategyConfiguration': {\n                'biddingStrategyType': 'MANUAL_CPC',\n                'biddingScheme': {\n                    'xsi_type': 'ManualCpcBiddingScheme',\n                    'enhancedCpcEnabled': 'false'\n                }\n            },\n            'budget': {\n                'budgetId': budget_id\n            },\n            'advertisingChannelType': ad_channel_type\n        }\n    }]\n\n    campaign_service.mutate(operations)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetAccounts(self):\n    selector = {\n        'fields': ['CustomerId', 'CanManageClients']\n    }\n\n    accounts = self.client.GetService('ManagedCustomerService').get(selector)\n\n    return accounts['entries']", "response": "Returns the client accounts associated with the user s manager account."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetAdGroups(self, client_customer_id, campaign_id):\n    self.client.SetClientCustomerId(client_customer_id)\n    selector = {\n        'fields': ['Id', 'Name', 'Status'],\n        'predicates': [\n            {\n                'field': 'CampaignId',\n                'operator': 'EQUALS',\n                'values': [campaign_id]\n            },\n            {\n                'field': 'Status',\n                'operator': 'NOT_EQUALS',\n                'values': ['REMOVED']\n            }\n        ]\n    }\n    adgroups = self.client.GetService('AdGroupService').get(selector)\n\n    if int(adgroups['totalNumEntries']) > 0:\n      return adgroups['entries']\n    else:\n      return None", "response": "Retrieves all AdGroups for the given campaign."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a Budget with the associated budgetId.", "response": "def GetBudget(self, client_customer_id, budget_id):\n    \"\"\"Return a Budget with the associated budgetId.\n\n    Args:\n      client_customer_id: str Client Customer Id to which the budget belongs.\n      budget_id: str id of the budget we want to examine.\n\n    Returns:\n      Budget A Budget data object.\n    \"\"\"\n    self.client.SetClientCustomerId(client_customer_id)\n    selector = {\n        'fields': ['BudgetId', 'BudgetName', 'BudgetStatus', 'Amount',\n                   'DeliveryMethod', 'BudgetReferenceCount',\n                   'IsBudgetExplicitlyShared'],\n        'predicates': [\n            {\n                'field': 'BudgetId',\n                'operator': 'EQUALS',\n                'values': [budget_id]\n            }\n        ]\n    }\n    budgets = self.client.GetService('BudgetService').get(selector)\n\n    if int(budgets['totalNumEntries']) > 0:\n      return budgets['entries'][0]\n    else:\n      return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetCampaigns(self, client_customer_id):\n    self.client.SetClientCustomerId(client_customer_id)\n    # A somewhat hackish workaround for \"The read operation timed out\" error,\n    # which could be triggered on AppEngine's end if the request is too large\n    # and is taking too long.\n    max_tries = 3\n    today = time.strftime('%Y%m%d', time.localtime())\n    for i in xrange(1, max_tries + 1):\n      try:\n        selector = {\n            'fields': ['Id', 'Name', 'Status', 'BudgetId', 'Amount'],\n            'predicates': [\n                {\n                    'field': 'Status',\n                    'operator': 'NOT_EQUALS',\n                    'values': ['REMOVED']\n                }\n            ],\n            'dateRange': {\n                'min': today,\n                'max': today\n            }\n        }\n        campaigns = self.client.GetService('CampaignService').get(selector)\n        if int(campaigns['totalNumEntries']) > 0:\n          return campaigns['entries']\n        else:\n          return None\n      except Exception, e:\n        if i == max_tries:\n          raise GoogleAdsError(e)\n        continue", "response": "Retrieves a list of Campaigns that have not been removed."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating a Budget with the given budgetId.", "response": "def UpdateBudget(self, client_customer_id, budget_id, micro_amount,\n                   delivery_method):\n    \"\"\"Update a Budget with the given budgetId.\n\n    Args:\n      client_customer_id: str Client Customer Id used to update Budget.\n      budget_id: str Id of the budget to be updated.\n      micro_amount: str New value for the microAmount field.\n      delivery_method: str New value for the deliveryMethod field.\n    \"\"\"\n    self.client.SetClientCustomerId(client_customer_id)\n    operations = [{\n        'operator': 'SET',\n        'operand': {\n            'budgetId': budget_id,\n            'amount': {\n                'microAmount': micro_amount\n            },\n            'deliveryMethod': delivery_method\n        }\n    }]\n    self.client.GetService('BudgetService').mutate(operations)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef CreateExtensionSetting(client, feed_items, campaign_feed, feed_item_ids,\n                           platform_restrictions=None):\n  \"\"\"Creates the extension setting for a list of Feed Items.\n\n  Args:\n    client: an AdWordsClient instance.\n    feed_items: the list of all Feed Items.\n    campaign_feed: the original Campaign Feed.\n    feed_item_ids: the Ids of the feed items for which extension settings should\n        be created.\n    platform_restrictions: an optional Platform Restriction for the Feed items.\n  \"\"\"\n  campaign_extension_setting_service = client.GetService(\n      'CampaignExtensionSettingService', 'v201809')\n\n  extension_feed_items = [{\n      CreateSitelinkFeedItem(feed_items, feed_item_id)\n  } for feed_item_id in feed_item_ids]\n\n  extension_setting = {\n      'extensions': extension_feed_items\n  }\n\n  if platform_restrictions:\n    extension_setting['platformRestrictions'] = platform_restrictions\n\n  campaign_extension_setting = {\n      'campaignId': campaign_feed['campaignId'],\n      'extensionType': 'SITELINK',\n      'extensionSetting': extension_setting\n  }\n\n  operation = {\n      'operand': campaign_extension_setting,\n      'operator': 'ADD'\n  }\n\n  campaign_extension_setting_service.mutate([operation])", "response": "Creates the extension setting for a list of Feed Items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a Sitelink Feed Item.", "response": "def CreateSitelinkFeedItem(feed_items, feed_item_id):\n  \"\"\"Creates a Sitelink Feed Item.\n\n  Args:\n    feed_items: a list of all Feed Items.\n    feed_item_id: the Id of a specific Feed Item for which a Sitelink Feed Item\n        should be created.\n\n  Returns:\n    The new Sitelink Feed Item.\n  \"\"\"\n  site_link_from_feed = feed_items[feed_item_id]\n  site_link_feed_item = {\n      'sitelinkText': site_link_from_feed['text'],\n      'sitelinkLine2': site_link_from_feed['line2'],\n      'sitelinkLine3': site_link_from_feed['line3'],\n  }\n\n  if 'finalUrls' in site_link_from_feed and site_link_from_feed['finalUrls']:\n    site_link_feed_item['sitelinkFinalUrls'] = {\n        'urls': site_link_from_feed['finalUrls']\n    }\n\n    if 'finalMobileUrls' in site_link_from_feed:\n      site_link_feed_item['sitelinkFinalMobileUrls'] = {\n          'urls': site_link_from_feed['finalMobileUrls']\n      }\n\n    site_link_feed_item['sitelinkTrackingUrlTemplate'] = (\n        site_link_from_feed['trackingUrlTemplate'])\n  else:\n    site_link_feed_item['sitelinkUrl'] = site_link_from_feed['url']\n\n  return site_link_feed_item"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete a campaign feed.", "response": "def DeleteCampaignFeed(client, campaign_feed):\n  \"\"\"Deletes a campaign feed.\n\n  Args:\n    client: an AdWordsClient instance.\n    campaign_feed: the campaign feed to delete.\n  \"\"\"\n  campaign_feed_service = client.GetService('CampaignFeedService', 'v201809')\n\n  operation = {\n      'operand': campaign_feed,\n      'operator': 'REMOVE'\n  }\n\n  campaign_feed_service.mutate([operation])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef DeleteOldFeedItems(client, feed_item_ids, feed):\n  if not feed_item_ids:\n    return\n\n  feed_item_service = client.GetService('FeedItemService', 'v201809')\n\n  operations = [{\n      'operator': 'REMOVE',\n      'operand': {\n          'feedId': feed['id'],\n          'feedItemId': feed_item_id\n      }\n  } for feed_item_id in feed_item_ids]\n\n  feed_item_service.mutate(operations)", "response": "Deletes the old feed items for which extension settings have been created."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrieving a list of Feed Item Ids used by a given Campaign Feed.", "response": "def GetCampaignFeeds(client, feed, placeholder_type):\n  \"\"\"Get a list of Feed Item Ids used by a campaign via a given Campaign Feed.\n\n  Args:\n    client: an AdWordsClient instance.\n    feed: a Campaign Feed.\n    placeholder_type: the Placeholder Type.\n\n  Returns:\n    A list of Feed Item Ids.\n  \"\"\"\n  campaign_feed_service = client.GetService('CampaignFeedService', 'v201809')\n\n  campaign_feeds = []\n  more_pages = True\n\n  selector = {\n      'fields': ['CampaignId', 'MatchingFunction', 'PlaceholderTypes'],\n      'predicates': [\n          {\n              'field': 'Status',\n              'operator': 'EQUALS',\n              'values': ['ENABLED']\n          },\n          {\n              'field': 'FeedId',\n              'operator': 'EQUALS',\n              'values': [feed['id']]\n          },\n          {\n              'field': 'PlaceholderTypes',\n              'operator': 'CONTAINS_ANY',\n              'values': [placeholder_type]\n          }\n      ],\n      'paging': {\n          'startIndex': 0,\n          'numberResults': PAGE_SIZE\n      }\n  }\n\n  while more_pages:\n    page = campaign_feed_service.get(selector)\n\n    if 'entries' in page:\n      campaign_feeds.extend(page['entries'])\n\n    selector['paging']['startIndex'] += PAGE_SIZE\n    more_pages = selector['paging']['startIndex'] < int(page['totalNumEntries'])\n\n  return campaign_feeds"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a list of all enabled Feeds.", "response": "def GetFeeds(client):\n  \"\"\"Returns a list of all enabled Feeds.\n\n  Args:\n    client: an AdWordsClient instance.\n\n  Returns:\n    A list containing all enabled Feeds.\n  \"\"\"\n  feed_service = client.GetService('FeedService', 'v201809')\n\n  feeds = []\n  more_pages = True\n\n  selector = {\n      'fields': ['Id', 'Name', 'Attributes'],\n      'predicates': [\n          {\n              'field': 'Origin',\n              'operator': 'EQUALS',\n              'values': ['USER']\n          },\n          {\n              'field': 'FeedStatus',\n              'operator': 'EQUALS',\n              'values': ['ENABLED']\n          }\n      ],\n      'paging': {\n          'startIndex': 0,\n          'numberResults': PAGE_SIZE\n      }\n  }\n\n  while more_pages:\n    page = feed_service.get(selector)\n\n    if 'entries' in page:\n      feeds.extend(page['entries'])\n\n    selector['paging']['startIndex'] += PAGE_SIZE\n    more_pages = selector['paging']['startIndex'] < int(page['totalNumEntries'])\n\n  return feeds"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef GetFeedItems(client, feed):\n  feed_item_service = client.GetService('FeedItemService', 'v201809')\n\n  feed_items = []\n  more_pages = True\n\n  selector = {\n      'fields': ['FeedItemId', 'AttributeValues'],\n      'predicates': [\n          {\n              'field': 'Status',\n              'operator': 'EQUALS',\n              'values': ['ENABLED']\n          },\n          {\n              'field': 'FeedId',\n              'operator': 'EQUALS',\n              'values': [feed['id']]\n          }\n      ],\n      'paging': {\n          'startIndex': 0,\n          'numberResults': PAGE_SIZE\n      }\n  }\n\n  while more_pages:\n    page = feed_item_service.get(selector)\n\n    if 'entries' in page:\n      feed_items.extend(page['entries'])\n\n    selector['paging']['startIndex'] += PAGE_SIZE\n    more_pages = selector['paging']['startIndex'] < int(page['totalNumEntries'])\n\n  return feed_items", "response": "Retrieves the Feed Items associated with a given Feed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetFeedItemIdsForCampaign(campaign_feed):\n  feed_item_ids = set()\n\n  try:\n    lhs_operand = campaign_feed['matchingFunction']['lhsOperand']\n  except KeyError:\n    lhs_operand = None\n\n  if (lhs_operand and lhs_operand[0]['FunctionArgumentOperand.Type'] ==\n      'RequestContextOperand'):\n    request_context_operand = lhs_operand[0]\n\n    if (request_context_operand['contextType'] == 'FEED_ITEM_ID' and\n        campaign_feed['matchingFunction']['operator'] == 'IN'):\n      for argument in campaign_feed['matchingFunction']['rhsOperand']:\n        if argument['xsi_type'] == 'ConstantOperand':\n          feed_item_ids.add(argument['longValue'])\n\n  return feed_item_ids", "response": "Returns the Feed Item Ids used by a given Campaign Feed."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef GetFeedMapping(client, feed, placeholder_type):\n  feed_mapping_service = client.GetService('FeedMappingService', 'v201809')\n\n  attribute_mappings = {}\n  more_pages = True\n\n  selector = {\n      'fields': ['FeedMappingId', 'AttributeFieldMappings'],\n      'predicates': [\n          {\n              'field': 'FeedId',\n              'operator': 'EQUALS',\n              'values': [feed['id']]\n          },\n          {\n              'field': 'PlaceholderType',\n              'operator': 'EQUALS',\n              'values': [placeholder_type]\n          }\n      ],\n      'paging': {\n          'startIndex': 0,\n          'numberResults': PAGE_SIZE\n      }\n  }\n\n  while more_pages:\n    page = feed_mapping_service.get(selector)\n\n    if 'entries' in page:\n      # Normally, a feed attribute is mapped only to one field. However, you may\n      # map it to more than one field if needed.\n      for feed_mapping in page['entries']:\n        for attribute_mapping in feed_mapping['attributeFieldMappings']:\n          # Since attribute mappings can have multiple values for each key,\n          # we use a list to store the values.\n          if attribute_mapping['feedAttributeId'] in attribute_mappings:\n            attribute_mappings[attribute_mapping['feedAttributeId']].append(\n                attribute_mapping['fieldId'])\n          else:\n            attribute_mappings[attribute_mapping['feedAttributeId']] = [\n                attribute_mapping['fieldId']]\n\n    selector['paging']['startIndex'] += PAGE_SIZE\n    more_pages = selector['paging']['startIndex'] < int(page['totalNumEntries'])\n\n  return attribute_mappings", "response": "Retrieves the Feed Mapping for a given Feed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve the Platform Restricts for a given Campaign Feed.", "response": "def GetPlatformRestrictions(campaign_feed):\n  \"\"\"Get the Platform Restrictions for a given Campaign Feed.\n\n  Args:\n    campaign_feed: the Campaign Feed we are retrieving Platform Restrictons for.\n\n  Returns:\n    The Platform Restrictions for the given feed.\n  \"\"\"\n  platform_restrictions = None\n\n  if campaign_feed['matchingFunction']['operator'] == 'AND':\n    for argument in campaign_feed['matchingFunction']['lhsOperand']:\n      # Check if matchingFunction is EQUALS(CONTEXT.DEVICE, 'Mobile')\n      if argument['value']['operator'] == 'EQUALS':\n        request_context_operand = argument['value']['lhsOperand'][0]\n\n        if (request_context_operand and\n            request_context_operand == 'DEVICE_PLATFORM'):\n                  # This needs to be capitalized for ExtensionSettingPlatform.\n          platform_restrictions = argument['value']['rhsOperand'][0].upper()\n\n  return platform_restrictions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetSitelinksFromFeed(client, feed):\n  # Retrieve the feed's attribute mapping.\n  feed_mappings = GetFeedMapping(client, feed, PLACEHOLDER_TYPE_SITELINKS)\n\n  feed_items = {}\n\n  for feed_item in GetFeedItems(client, feed):\n    site_link_from_feed = {}\n\n    for attribute_value in feed_item['attributeValues']:\n      if attribute_value['feedAttributeId'] in feed_mappings:\n        for field_id in feed_mappings[attribute_value['feedAttributeId']]:\n          if field_id == SITE_LINK_FIELDS['TEXT']:\n            site_link_from_feed['text'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['URL']:\n            site_link_from_feed['url'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['FINAL_URLS']:\n            site_link_from_feed['finalUrls'] = attribute_value['stringValues']\n          elif field_id == SITE_LINK_FIELDS['FINAL_MOBILE_URLS']:\n            site_link_from_feed['finalMobileUrls'] = attribute_value[\n                'stringValues']\n          elif field_id == SITE_LINK_FIELDS['TRACKING_URL_TEMPLATE']:\n            site_link_from_feed['trackingUrlTemplate'] = attribute_value[\n                'stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE2']:\n            site_link_from_feed['line2'] = attribute_value['stringValue']\n          elif field_id == SITE_LINK_FIELDS['LINE3']:\n            site_link_from_feed['line3'] = attribute_value['stringValue']\n          else:\n            print 'No applicable Site Link Field found for Id: %s' % field_id\n\n    feed_items[feed_item['feedItemId']] = site_link_from_feed\n\n  return feed_items", "response": "Retrieves the sitelinks from a feed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef GetVersion():\n  with open(os.path.join('googleads', 'common.py')) as versions_file:\n    source = versions_file.read()\n  return re.search('\\\\nVERSION = \\'(.*?)\\'', source).group(1)", "response": "Gets the version from googleads. py.\n\n"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nbuild the operations that will add a Keyword Criterion to each AdGroup.", "response": "def BuildAdGroupCriterionOperations(adgroup_operations, number_of_keywords=1):\n  \"\"\"Builds the operations adding a Keyword Criterion to each AdGroup.\n\n  Args:\n    adgroup_operations: a list containing the operations that will add AdGroups.\n    number_of_keywords: an int defining the number of Keywords to be created.\n\n  Returns:\n    a list containing the operations that will create a new Keyword Criterion\n    associated with each provided AdGroup.\n  \"\"\"\n  criterion_operations = [\n      {\n          # The xsi_type of the operation can usually be guessed by the API\n          # because a given service only handles one type of operation.\n          # However, batch jobs process operations of different types, so\n          # the xsi_type must always be explicitly defined for these\n          # operations.\n          'xsi_type': 'AdGroupCriterionOperation',\n          'operand': {\n              'xsi_type': 'BiddableAdGroupCriterion',\n              'adGroupId': adgroup_operation['operand']['id'],\n              'criterion': {\n                  'xsi_type': 'Keyword',\n                  # Make 50% of keywords invalid to demonstrate error handling.\n                  'text': 'mars%s%s' % (i, '!!!' if i % 2 == 0 else ''),\n                  'matchType': 'BROAD'\n              }\n          },\n          'operator': 'ADD'\n      }\n      for adgroup_operation in adgroup_operations\n      for i in range(number_of_keywords)]\n\n  return criterion_operations"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the operations that will add desired number of AdGroups to given Campaigns.", "response": "def BuildAdGroupOperations(batch_job_helper,\n                           campaign_operations, number_of_adgroups=1):\n  \"\"\"Builds the operations adding desired number of AdGroups to given Campaigns.\n\n  Note: When the AdGroups are created, they will have a different Id than those\n  generated here as a temporary Id. This is just used to identify them in the\n  BatchJobService.\n\n  Args:\n    batch_job_helper: a BatchJobHelper instance.\n    campaign_operations: a list containing the operations that will add\n      Campaigns.\n    number_of_adgroups: an int defining the number of AdGroups to be created per\n      Campaign.\n\n  Returns:\n    a list containing the operations that will add the desired number of\n    AdGroups to each of the provided Campaigns.\n  \"\"\"\n  adgroup_operations = [\n      {\n          # The xsi_type of the operation can usually be guessed by the API\n          # because a given service only handles one type of operation.\n          # However, batch jobs process operations of different types, so\n          # the xsi_type must always be explicitly defined for these\n          # operations.\n          'xsi_type': 'AdGroupOperation',\n          'operand': {\n              'campaignId': campaign_operation['operand']['id'],\n              'id': batch_job_helper.GetId(),\n              'name': 'Batch Ad Group #%s' % uuid.uuid4(),\n              'biddingStrategyConfiguration': {\n                  'bids': [\n                      {\n                          'xsi_type': 'CpcBid',\n                          'bid': {\n                              'microAmount': 10000000\n                          }\n                      }\n                  ]\n              }\n          },\n          'operator': 'ADD'\n      }\n      for campaign_operation in campaign_operations\n      for _ in range(number_of_adgroups)]\n\n  return adgroup_operations"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef BuildCampaignOperations(batch_job_helper,\n                            budget_operations, number_of_campaigns=1):\n  \"\"\"Builds the operations needed to create a new Campaign.\n\n  Note: When the Campaigns are created, they will have a different Id than those\n  generated here as a temporary Id. This is just used to identify them in the\n  BatchJobService.\n\n  Args:\n    batch_job_helper: a BatchJobHelper instance.\n    budget_operations: a list containing the operation that will add the budget\n      used by these Campaigns.\n    number_of_campaigns: an int number defining the number of campaigns to be\n      created.\n\n  Returns:\n    a list containing the operations to create the desired number of Campaigns.\n  \"\"\"\n  # Grab the temporary budgetId to associate with the new Campaigns.\n  budget_id = budget_operations[0]['operand']['budgetId']\n\n  campaign_operations = [\n      {\n          # The xsi_type of the operation can usually be guessed by the API\n          # because a given service only handles one type of operation.\n          # However, batch jobs process operations of different types, so\n          # the xsi_type must always be explicitly defined for these\n          # operations.\n          'xsi_type': 'CampaignOperation',\n          'operand': {\n              'name': 'Batch Campaign #%s' % uuid.uuid4(),\n              # Recommendation: Set the campaign to PAUSED when creating it to\n              # stop the ads from immediately serving. Set to ENABLED once\n              # you've added targeting and the ads are ready to serve.\n              'status': 'PAUSED',\n              # This is a temporary Id used by the BatchJobService to identify\n              # the Campaigns for operations that require a campaignId.\n              'id': batch_job_helper.GetId(),\n              'advertisingChannelType': 'SEARCH',\n              # Note that only the budgetId is required\n              'budget': {\n                  'budgetId': budget_id\n              },\n              'biddingStrategyConfiguration': {\n                  'biddingStrategyType': 'MANUAL_CPC'\n              }\n          },\n          'operator': 'ADD'\n      }\n      for _ in range(number_of_campaigns)]\n\n  return campaign_operations", "response": "Builds the operations needed to create a new Campaign."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _CreateImage(media_service, opener, url):\n  # Note: The utf-8 decode is for 2to3 Python 3 compatibility.\n  image_data = opener.open(url).read().decode('utf-8')\n  image = {\n      'type': 'IMAGE',\n      'data': image_data,\n      'xsi_type': 'Image'\n  }\n\n  return media_service.upload(image)[0]", "response": "Creates an image and uploads it to the server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _CreateDynamicDisplayAdSettings(media_service, opener):\n  image = _CreateImage(media_service, opener, 'https://goo.gl/dEvQeF')\n\n  logo = {\n      'type': 'IMAGE',\n      'mediaId': image['mediaId'],\n      'xsi_type': 'Image'\n  }\n\n  dynamic_settings = {\n      'landscapeLogoImage': logo,\n      'pricePrefix': 'as low as',\n      'promoText': 'Free shipping!',\n      'xsi_type': 'DynamicSettings',\n  }\n\n  return dynamic_settings", "response": "Creates dynamic display ad settings."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef CreateBudget(client):\n  budget_service = client.GetService('BudgetService', version='v201809')\n\n  # Create a budget.\n  budget = {\n      'name': 'Interplanetary Cruise App Budget #%s' % uuid.uuid4(),\n      'amount': {\n          'microAmount': '50000000'\n      },\n      'deliveryMethod': 'STANDARD',\n      'isExplicitlyShared': False\n  }\n\n  budget_operations = [{\n      'operator': 'ADD',\n      'operand': budget\n  }]\n\n  # Create the budget and return its ID.\n  budget_id = budget_service.mutate(budget_operations)['value'][0]['budgetId']\n\n  return budget_id", "response": "Creates a budget and returns its budgetId."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef SetCampaignTargetingCriteria(client, campaign):\n  campaign_criterion_service = client.GetService('CampaignCriterionService')\n\n  # Create locations. The IDs can be found in the documentation or retrieved\n  # with the LocationCriterionService.\n  criteria = [\n      {\n          'xsi_type': 'Location',\n          'id': 21137  # California\n      },\n      {\n          'xsi_type': 'Location',\n          'id': 2484  # Mexico\n      },\n      {\n          'xsi_type': 'Language',\n          'id': 1000  # English\n      },\n      {\n          'xsi_type': 'Language',\n          'id': 1003  # Spanish\n      }\n  ]\n\n  operations = [{\n      'operator': 'ADD',\n      'operand': {\n          'campaignId': campaign['id'],\n          'criterion': criterion\n      }\n  } for criterion in criteria]\n\n  response = campaign_criterion_service.mutate(operations)\n\n  if response and 'value' in response:\n    # Display the added campaign targets.\n    for criterion in response['value']:\n      print ('Campaign criteria of type \"%s\" and id \"%s\" was added.'\n             % (criterion['criterion']['type'],\n                criterion['criterion']['id']))", "response": "Sets the targeting criteria for the given campaign."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay an account tree.", "response": "def DisplayAccountTree(account, accounts, links, depth=0):\n  \"\"\"Displays an account tree.\n\n  Args:\n    account: dict The account to display.\n    accounts: dict Map from customerId to account.\n    links: dict Map from customerId to child links.\n    depth: int Depth of the current account in the tree.\n  \"\"\"\n  prefix = '-' * depth * 2\n  print '%s%s, %s' % (prefix, account['customerId'], account['name'])\n  if account['customerId'] in links:\n    for child_link in links[account['customerId']]:\n      child_account = accounts[child_link['clientCustomerId']]\n      DisplayAccountTree(child_account, accounts, links, depth + 1)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CreatePriceTableRow(header, description, final_url, price_in_micros,\n                        currency_code, price_unit, final_mobile_url=None):\n  \"\"\"Helper function to generate a single row of a price table.\n\n  Args:\n    header: A str containing the header text of this row.\n    description: A str description of this row in the price table.\n    final_url: A str containing the final URL after all cross domain redirects.\n    price_in_micros: An int indicating the price of the given currency in\n      micros.\n    currency_code: A str indicating the currency code being used.\n    price_unit: A str enum indicating the price unit for this row.\n    final_mobile_url: A str containing the final mobile URL after all cross\n      domain redirects.\n\n  Returns:\n    A dictionary containing the contents of the generated price table row.\n  \"\"\"\n  table_row = {\n      'header': header,\n      'description': description,\n      'finalUrls': {'urls': [final_url]},\n      'price': {\n          'money': {\n              'microAmount': price_in_micros,\n          },\n          'currencyCode': currency_code\n      },\n      'priceUnit': price_unit,\n      'xsi_type': 'PriceTableRow'\n  }\n\n  if final_mobile_url:\n    table_row['finalMobileUrls'] = {\n        'urls': [final_mobile_url]\n    }\n\n  return table_row", "response": "This function creates a single row of a price table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CreateBiddingStrategy(client):\n  # Initialize appropriate service.\n  bidding_strategy_service = client.GetService(\n      'BiddingStrategyService', version='v201809')\n\n  # Create a shared bidding strategy.\n  shared_bidding_strategy = {\n      'name': 'Maximize Clicks %s' % uuid.uuid4(),\n      'biddingScheme': {\n          'xsi_type': 'TargetSpendBiddingScheme',\n          # Optionally set additional bidding scheme parameters.\n          'bidCeiling': {\n              'microAmount': '2000000'\n          }\n      }\n  }\n\n  # Create operation.\n  operation = {\n      'operator': 'ADD',\n      'operand': shared_bidding_strategy\n  }\n\n  response = bidding_strategy_service.mutate([operation])\n  new_bidding_strategy = response['value'][0]\n\n  print ('Shared bidding strategy with name \"%s\" and ID \"%s\" of type \"%s\"'\n         'was created.' %\n         (new_bidding_strategy['name'], new_bidding_strategy['id'],\n          new_bidding_strategy['biddingScheme']['BiddingScheme.Type']))\n\n  return new_bidding_strategy", "response": "Creates a bidding strategy object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an explicit budget to be used only to create the Campaign.", "response": "def CreateSharedBudget(client):\n  \"\"\"Creates an explicit budget to be used only to create the Campaign.\n\n  Args:\n    client: AdWordsClient the client to run the example with.\n\n  Returns:\n    dict An object representing a shared budget.\n  \"\"\"\n  # Initialize appropriate service.\n  budget_service = client.GetService('BudgetService', version='v201809')\n\n  # Create a shared budget\n  budget = {\n      'name': 'Shared Interplanetary Budget #%s' % uuid.uuid4(),\n      'amount': {\n          'microAmount': '2000000'\n      },\n      'deliveryMethod': 'STANDARD',\n      'isExplicitlyShared': 'true'\n  }\n\n  # Create operation.\n  operation = {\n      'operator': 'ADD',\n      'operand': budget\n  }\n\n  response = budget_service.mutate([operation])\n  return response['value'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef CreateCampaignWithBiddingStrategy(client, bidding_strategy_id, budget_id):\n  # Initialize appropriate service.\n  campaign_service = client.GetService('CampaignService', version='v201809')\n\n  # Create campaign.\n  campaign = {\n      'name': 'Interplanetary Cruise #%s' % uuid.uuid4(),\n      'budget': {\n          'budgetId': budget_id\n      },\n      'biddingStrategyConfiguration': {\n          'biddingStrategyId': bidding_strategy_id\n      },\n      'advertisingChannelType': 'SEARCH',\n      'networkSetting': {\n          'targetGoogleSearch': 'true',\n          'targetSearchNetwork': 'true',\n          'targetContentNetwork': 'true'\n      }\n  }\n\n  # Create operation.\n  operation = {\n      'operator': 'ADD',\n      'operand': campaign\n  }\n\n  response = campaign_service.mutate([operation])\n  new_campaign = response['value'][0]\n\n  print ('Campaign with name \"%s\", ID \"%s\" and bidding scheme ID \"%s\" '\n         'was created.' %\n         (new_campaign['name'], new_campaign['id'],\n          new_campaign['biddingStrategyConfiguration']['biddingStrategyId']))\n\n  return new_campaign", "response": "Creates a new Campaign with a Shared Bidding Strategy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _DownloadReport(process_id, report_download_directory, customer_id,\n                    report_definition):\n  \"\"\"Helper function used by ReportWorker to download customer report.\n\n  Note that multiprocessing differs between Windows / Unix environments. A\n  Process or its subclasses in Windows must be serializable with pickle, but\n  that is not possible for AdWordsClient or ReportDownloader. This top-level\n  function is used as a work-around for Windows support.\n\n  Args:\n    process_id: The PID of the process downloading the report.\n    report_download_directory: A string indicating the directory where you\n        would like to download the reports.\n    customer_id: A str AdWords customer ID for which the report is being\n        downloaded.\n    report_definition: A dict containing the report definition to be used.\n\n  Returns:\n    A tuple indicating a boolean success/failure status, and dict request\n    context.\n  \"\"\"\n  report_downloader = (googleads.adwords.AdWordsClient.LoadFromStorage()\n                       .GetReportDownloader())\n\n  filepath = os.path.join(report_download_directory,\n                          'adgroup_%d.csv' % customer_id)\n  retry_count = 0\n\n  while True:\n    print ('[%d/%d] Loading report for customer ID \"%s\" into \"%s\"...'\n           % (process_id, retry_count, customer_id, filepath))\n    try:\n      with open(filepath, 'wb') as handler:\n        report_downloader.DownloadReport(\n            report_definition, output=handler,\n            client_customer_id=customer_id)\n      return (True, {'customerId': customer_id})\n    except googleads.errors.AdWordsReportError as e:\n      if e.code == 500 and retry_count < MAX_RETRIES:\n        time.sleep(retry_count * BACKOFF_FACTOR)\n      else:\n        print ('Report failed for customer ID \"%s\" with code \"%d\" after \"%d\" '\n               'retries.' % (customer_id, e.code, retry_count+1))\n        return (False, {'customerId': customer_id, 'code': e.code,\n                        'message': e.message})", "response": "Download a report from the AdWords server."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef GetCustomerIDs(client):\n  # For this example, we will use ManagedCustomerService to get all IDs in\n  # hierarchy that do not belong to MCC accounts.\n  managed_customer_service = client.GetService('ManagedCustomerService',\n                                               version='v201809')\n\n  offset = 0\n\n  # Get the account hierarchy for this account.\n  selector = {\n      'fields': ['CustomerId'],\n      'predicates': [{\n          'field': 'CanManageClients',\n          'operator': 'EQUALS',\n          'values': [False]\n      }],\n      'paging': {\n          'startIndex': str(offset),\n          'numberResults': str(PAGE_SIZE)\n      }\n  }\n\n  # Using Queue to balance load between processes.\n  queue = multiprocessing.Queue()\n  more_pages = True\n\n  while more_pages:\n    page = managed_customer_service.get(selector)\n\n    if page and 'entries' in page and page['entries']:\n      for entry in page['entries']:\n        queue.put(entry['customerId'])\n    else:\n      raise Exception('Can\\'t retrieve any customer ID.')\n    offset += PAGE_SIZE\n    selector['paging']['startIndex'] = str(offset)\n    more_pages = offset < int(page['totalNumEntries'])\n\n  return queue", "response": "Retrieves all CustomerIds in the account hierarchy."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new Display Network campaign.", "response": "def CreateCampaign(client, merchant_id, budget_id):\n  \"\"\"Creates a new Display Network campaign.\n\n  Args:\n    client: an AdWordsClient instance.\n    merchant_id: a int merchant center ID.\n    budget_id: a int budget ID.\n\n  Returns:\n    The campaign that was successfully created.\n  \"\"\"\n  campaign_service = client.GetService('CampaignService', 'v201809')\n\n  campaign = {\n      'name': 'Shopping campaign #%d' % uuid.uuid4(),\n      # Dynamic remarketing campaigns are only available on the Google Display\n      # Network.\n      'advertisingChannelType': 'DISPLAY',\n      'status': 'PAUSED',\n      'budget': {\n          'budgetId': budget_id\n      },\n      # This example uses a Manual CPC bidding strategy, but you should select\n      # the strategy that best aligns with your sales goals. More details here:\n      # https://support.google.com/adwords/answer/2472725\n      'biddingStrategyConfiguration': {\n          'biddingStrategyType': 'MANUAL_CPC'\n      },\n      'settings': [{\n          'xsi_type': 'ShoppingSetting',\n          # Campaigns with numerically higher priorities take precedence over\n          # those with lower priorities.\n          'campaignPriority': 0,\n          'merchantId': merchant_id,\n          # Display network campaigns do not support partition by country. The\n          # only supported value is \"ZZ\". This signals that products from all\n          # countries are available in this campaign. The actual products which\n          # serve are based on the products tagged in the user list entry.\n          'salesCountry': 'ZZ',\n          # Optional: Enable local inventory ads (items for sale in physical\n          # stores.)\n          'enableLocal': True,\n      }]\n  }\n\n  operations = [{\n      'operator': 'ADD',\n      'operand': campaign\n  }]\n\n  return campaign_service.mutate(operations)['value'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef CreateAdGroup(client, campaign_id):\n  ad_group_service = client.GetService('AdGroupService', 'v201809')\n\n  ad_group = {\n      'name': 'Dynamic remarketing ad group',\n      'campaignId': campaign_id,\n      'status': 'ENABLED'\n  }\n\n  operations = [{\n      'operator': 'ADD',\n      'operand': ad_group\n  }]\n\n  return ad_group_service.mutate(operations)['value'][0]", "response": "Creates a dynamic remarketing ad group."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CreateAd(client, opener, ad_group_id):\n  ad_group_ad_service = client.GetService('AdGroupAdService', 'v201809')\n  media_service = client.GetService('MediaService', 'v201809')\n\n  marketing_image_id = _CreateImage(\n      media_service, opener, 'https://goo.gl/3b9Wfh')\n  logo_image_id = _CreateImage(media_service, opener, 'https://goo.gl/mtt54n')\n\n  ad = {\n      'xsi_type': 'ResponsiveDisplayAd',\n      # This ad format doesn't allow the creation of an image using the\n      # Image.data field. An image must first be created using the MediaService,\n      # and Image.mediaId must be populated when creating the ad.\n      'marketingImage': {\n          'xsi_type': 'Image',\n          'mediaId': marketing_image_id\n      },\n      'shortHeadline': 'Travel',\n      'longHeadline': 'Travel the World',\n      'description': 'Take to the air!',\n      'businessName': 'Interplanetary Cruises',\n      'finalUrls': ['http://wwww.example.com'],\n      # Optional: Call to action text.\n      # Valid texts: https://support.google.com/adwords/answer/7005917\n      'callToActionText': 'Apply Now',\n      # Optional: Set dynamic display ad settings, composed of landscape logo\n      # image, promotion text, and price prefix.\n      'dynamicDisplayAdSettings': CreateDynamicDisplayAdSettings(\n          client, opener),\n      # Optional: Create a logo image and set it to the ad.\n      'logoImage': {\n          'xsi_type': 'Image',\n          'mediaId': logo_image_id\n      },\n      # Optional: Create a square marketing image and set it to the ad.\n      'squareMarketingImage': {\n          'xsi_type': 'Image',\n          'mediaId': logo_image_id\n      },\n      # Whitelisted accounts only: Set color settings using hexadecimal values.\n      # Set allowFlexibleColor to False if you want your ads to render by always\n      # using your colors strictly.\n      # 'mainColor': '#000fff',\n      # 'accentColor': '#fff000',\n      # 'allowFlexibleColor': False,\n      # Whitelisted accounts only: Set the format setting that the ad will be\n      # served in.\n      # 'formatSetting': 'NON_NATIVE'\n  }\n\n  ad_group_ad = {\n      'ad': ad,\n      'adGroupId': ad_group_id\n  }\n\n  operations = [{\n      'operation': 'ADD',\n      'operand': ad_group_ad\n  }]\n\n  return ad_group_ad_service.mutate(operations)['value'][0]", "response": "Creates a ResponsiveDisplayAd.\n\n  Args:\n    client: an AdWordsClient instance.\n    opener: an OpenerDirector instance.\n    ad_group_id: an int ad group ID.\n\n  Returns:\n    The ad group ad that was successfully created."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds the provided user list to the provided ad group.", "response": "def AttachUserList(client, ad_group_id, user_list_id):\n  \"\"\"Links the provided ad group and user list.\n\n  Args:\n    client: an AdWordsClient instance.\n    ad_group_id: an int ad group ID.\n    user_list_id: an int user list ID.\n\n  Returns:\n    The ad group criterion that was successfully created.\n  \"\"\"\n  ad_group_criterion_service = client.GetService(\n      'AdGroupCriterionService', 'v201809')\n\n  user_list = {\n      'xsi_type': 'CriterionUserList',\n      'userListId': user_list_id\n  }\n\n  ad_group_criterion = {\n      'xsi_type': 'BiddableAdGroupCriterion',\n      'criterion': user_list,\n      'adGroupId': ad_group_id\n  }\n\n  operations = [{\n      'operator': 'ADD',\n      'operand': ad_group_criterion\n  }]\n\n  return ad_group_criterion_service.mutate(operations)['value'][0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef CreateDynamicDisplayAdSettings(client, opener):\n  media_service = client.GetService('MediaService', 'v201809')\n\n  logo = {\n      'xsi_type': 'Image',\n      'mediaId': _CreateImage(media_service, opener, 'https://goo.gl/dEvQeF')\n  }\n\n  dynamic_settings = {\n      'landscapeLogoImage': logo,\n      'pricePrefix': 'as low as',\n      'promoText': 'Free shipping!'\n  }\n\n  return dynamic_settings", "response": "Creates dynamic display ad settings."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the scope for the given API name.", "response": "def GetAPIScope(api_name):\n  \"\"\"Retrieves the scope for the given API name.\n\n  Args:\n    api_name: A string identifying the name of the API we want to retrieve a\n        scope for.\n\n  Returns:\n    A string that is the scope for the given API name.\n\n  Raises:\n    GoogleAdsValueError: If the given api_name is invalid; accepted values are\n        \"adwords\" and \"ad_manager\".\n  \"\"\"\n  try:\n    return SCOPES[api_name]\n  except KeyError:\n    raise googleads.errors.GoogleAdsValueError(\n        'Invalid API name \"%s\" provided. Acceptable values are: %s' %\n        (api_name, SCOPES.keys()))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates an OAuth2 Bearer header.", "response": "def CreateHttpHeader(self):\n    \"\"\"Creates an OAuth2 HTTP header.\n\n    The OAuth2 credentials will be refreshed as necessary. In the event that\n    the credentials fail to refresh, a message is logged but no exception is\n    raised.\n\n    Returns:\n      A dictionary containing one entry: the OAuth2 Bearer header under the\n      'Authorization' key.\n\n    Raises:\n      GoogleAdsError: If the access token has expired.\n    \"\"\"\n    oauth2_header = {}\n\n    if self.creds.expired:\n      raise googleads.errors.GoogleAdsError('Access token has expired.')\n\n    self.creds.apply(oauth2_header)\n    return oauth2_header"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates an OAuth2 HTTP header.", "response": "def CreateHttpHeader(self):\n    \"\"\"Creates an OAuth2 HTTP header.\n\n    The OAuth2 credentials will be refreshed as necessary. In the event that\n    the credentials fail to refresh, a message is logged but no exception is\n    raised.\n\n    Returns:\n      A dictionary containing one entry: the OAuth2 Bearer header under the\n      'Authorization' key.\n\n    Raises:\n      google.auth.exceptions.RefreshError: If the refresh fails.\n    \"\"\"\n    oauth2_header = {}\n\n    if self.creds.expiry is None or self.creds.expired:\n      self.Refresh()\n\n    self.creds.apply(oauth2_header)\n    return oauth2_header"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing the Refresh Token to retrieve and set a new Access Token.", "response": "def Refresh(self):\n    \"\"\"Uses the Refresh Token to retrieve and set a new Access Token.\n\n    Raises:\n      google.auth.exceptions.RefreshError: If the refresh fails.\n    \"\"\"\n    with requests.Session() as session:\n      session.proxies = self.proxy_config.proxies\n      session.verify = not self.proxy_config.disable_certificate_validation\n      session.cert = self.proxy_config.cafile\n\n      self.creds.refresh(\n          google.auth.transport.requests.Request(session=session))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _CreateFeed(client):\n  # Get the FeedService.\n  feed_service = client.GetService('FeedService', version='v201809')\n\n  # Create operation.\n  operation = {\n      # Create the feed.\n      'operand': {\n          'name': 'DSA Feed %s' % uuid.uuid4(),\n          # Create attributes.\n          'attributes': [\n              {'type': 'URL_LIST', 'name': 'Page URL'},\n              {'type': 'STRING_LIST', 'name': 'Label'}\n          ],\n          'origin': 'USER'\n      },\n      'operator': 'ADD'\n  }\n\n  # Add the feed.\n  feed = feed_service.mutate([operation])['value'][0]\n  return _DSAFeedDetails(feed['id'], feed['attributes'][0]['id'],\n                         feed['attributes'][1]['id'])", "response": "Creates the feed for DSA page URLs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _CreateFeedMapping(client, feed_details):\n  # Get the FeedMappingService.\n  feed_mapping_service = client.GetService('FeedMappingService',\n                                           version='v201809')\n\n  # Create the operation.\n  operation = {\n      # Create the feed mapping.\n      'operand': {\n          'criterionType': DSA_PAGE_FEED_CRITERION_TYPE,\n          'feedId': feed_details.feed_id,\n          # Map the feedAttributeIds to the fieldId constants.\n          'attributeFieldMappings': [\n              {\n                  'feedAttributeId': feed_details.url_attribute_id,\n                  'fieldId': DSA_PAGE_URLS_FIELD_ID\n              },\n              {\n                  'feedAttributeId': feed_details.label_attribute_id,\n                  'fieldId': DSA_LABEL_FIELD_ID\n              }\n          ]\n      },\n      'operator': 'ADD'\n  }\n\n  # Add the feed mapping.\n  feed_mapping_service.mutate([operation])", "response": "Creates the feed mapping for DSA page feeds."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates the page URLs in the DSA page feed.", "response": "def _CreateFeedItems(client, feed_details, label_name):\n  \"\"\"Creates the page URLs in the DSA page feed.\n\n  Args:\n    client: an AdWordsClient instance.\n    feed_details: a _DSAFeedDetails instance.\n    label_name: a str containing the page feed URL label.\n  \"\"\"\n  # Get the FeedItemService.\n  feed_item_service = client.GetService('FeedItemService', version='v201809')\n\n  # For page feed URL recommendations and rules, see:\n  # https://support.google.com/adwords/answer/7166527\n  urls = ('http://www.example.com/discounts/rental-cars?id={feeditem}',\n          'http://www.example.com/discounts/hotel-deals?id={feeditem}',\n          'http://www.example.com/discounts/flight-deals?id={feeditem}')\n\n  # Create the operation.\n  operations = [{\n      # Create the feed item.\n      'operand': {\n          'feedId': feed_details.feed_id,\n          'attributeValues': [\n              {\n                  'feedAttributeId': feed_details.url_attribute_id,\n                  'stringValues': [url]\n              },\n              {\n                  'feedAttributeId': feed_details.label_attribute_id,\n                  'stringValues': [label_name]\n              }\n          ]\n      },\n      'operator': 'ADD'\n  } for url in urls]\n\n  # Add the feed item.\n  feed_item_service.mutate(operations)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the campaign DSA setting to DSA pagefeeds.", "response": "def _UpdateCampaignDSASetting(client, campaign_id, feed_id):\n  \"\"\"Updates the campaign DSA setting to DSA pagefeeds.\n\n  Args:\n    client: an AdWordsClient instance.\n    campaign_id: a str Campaign ID.\n    feed_id: a str page Feed ID.\n\n  Raises:\n    ValueError: If the given campaign is found not to be a dynamic search ad\n    campaign.\n  \"\"\"\n  # Get the CampaignService.\n  campaign_service = client.GetService('CampaignService', version='v201809')\n\n  selector = {\n      'fields': ['Id', 'Settings'],\n      'predicates': [{\n          'field': 'Id',\n          'operator': 'EQUALS',\n          'values': [campaign_id]\n      }]\n  }\n\n  response = campaign_service.get(selector)\n\n  if response['totalNumEntries']:\n    campaign = response['entries'][0]\n  else:\n    raise ValueError('No campaign with ID \"%d\" exists.' % campaign_id)\n\n  if not campaign['settings']:\n    raise ValueError('This is not a DSA campaign.')\n\n  dsa_setting = None\n\n  campaign_settings = campaign['settings']\n\n  for setting in campaign_settings:\n    if setting['Setting.Type'] == 'DynamicSearchAdsSetting':\n      dsa_setting = setting\n      break\n\n  if dsa_setting is None:\n    raise ValueError('This is not a DSA campaign.')\n\n  dsa_setting['pageFeed'] = {\n      'feedIds': [feed_id]\n  }\n\n  # Optional: Specify whether only the supplied URLs should be used with your\n  # Dynamic Search Ads.\n  dsa_setting['useSuppliedUrlsOnly'] = True\n\n  operation = {\n      'operand': {\n          'id': campaign_id,\n          'settings': campaign_settings\n      },\n      'operator': 'SET'\n  }\n\n  campaign_service.mutate([operation])\n  print 'DSA page feed for campaign ID \"%d\" was updated with feed ID \"%d\".' % (\n      campaign_id, feed_id)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _AddDSATargeting(client, ad_group_id, label_name):\n  # Get the AdGroupCriterionService.\n  ad_group_criterion_service = client.GetService('AdGroupCriterionService',\n                                                 version='v201809')\n\n  # Create the operation.\n  operation = {\n      'operand': {\n          'xsi_type': 'BiddableAdGroupCriterion',\n          'adGroupId': ad_group_id,\n          # Create a webpage criterion.\n          'criterion': {\n              'xsi_type': 'Webpage',\n              'parameter': {\n                  'criterionName': 'Test criterion',\n                  # Add a condition for label=specified_label_name.\n                  'conditions': [{\n                      'operand': 'CUSTOM_LABEL',\n                      'argument': label_name\n                  }],\n              }\n          },\n          # Set a custom bid for this criterion\n          'biddingStrategyConfiguration': {\n              'bids': [{\n                  'xsi_type': 'CpcBid',\n                  'bid': {\n                      'microAmount': 1500000\n                  }\n              }]\n          }\n      },\n      'operator': 'ADD'\n  }\n\n  criterion = ad_group_criterion_service.mutate([operation])['value'][0]\n  print 'Web page criterion with ID \"%d\" and status \"%s\" was created.' % (\n      criterion['criterion']['id'], criterion['userStatus'])\n  return criterion", "response": "Adds a custom targeting for the page feed URLs based on a list of labels."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef unpack_bitstring(length, is_float, is_signed, bits):\n    # type: (int, bool, bool, typing.Any) -> typing.Union[float, int]\n    \"\"\"\n    returns a value calculated from bits\n    :param length: length of signal in bits\n    :param is_float: value is float\n    :param bits: value as bits (array/iterable)\n    :param is_signed: value is signed\n    :return:\n    \"\"\"\n\n    if is_float:\n        types = {\n            32: '>f',\n            64: '>d'\n        }\n\n        float_type = types[length]\n        value, = struct.unpack(float_type, bytearray(int(''.join(b), 2)  for b in grouper(bits, 8)))\n    else:\n        value = int(bits, 2)\n\n        if is_signed and bits[0] == '1':\n            value -= (1 << len(bits))\n\n    return value", "response": "Unpacks a bitstring into a value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pack_bitstring(length, is_float, value, signed):\n    if is_float:\n        types = {\n            32: '>f',\n            64: '>d'\n        }\n\n        float_type = types[length]\n        x = bytearray(struct.pack(float_type, value))\n        bitstring = ''.join('{:08b}'.format(b) for b in x)\n    else:\n        b = '{:0{}b}'.format(int((2<<length )+ value), length)\n        bitstring = b[-length:]\n\n    return bitstring", "response": "packs a bitstring into the bits\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_default(self, default):  # type: (typing.Any) -> None\n        if default is not None and len(default) > 1 and default[0] == '\"' and default[-1] == '\"':\n            default = default[1:-1]\n        self.defaultValue = default", "response": "Set Definition default value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting base type of a given signal.", "response": "def get_base_type_of_signal(signal):\n    # type: (canmatrix.Signal) -> typing.Tuple[str, int]\n    \"\"\"Get signal arxml-type and size based on the Signal properties.\"\"\"\n    if signal.is_float:\n        if signal.size > 32:\n            create_type = \"double\"\n            size = 64\n        else:\n            create_type = \"single\"\n            size = 32\n    else:\n        if signal.size > 32:\n            if signal.is_signed:\n                create_type = \"sint64\"\n            else:\n                create_type = \"uint64\"\n            size = 64                            \n        elif signal.size > 16:\n            if signal.is_signed:\n                create_type = \"sint32\"\n            else:\n                create_type = \"uint32\"\n            size = 32                            \n        elif signal.size > 8:\n            if signal.is_signed:\n                create_type = \"sint16\"\n            else:\n                create_type = \"uint16\"\n            size = 16\n        else:\n            if signal.is_signed:\n                create_type = \"sint8\"\n            else:\n                create_type = \"uint8\"\n            size = 8\n    return create_type, size"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fill_tree_from_xml(tag, ar_tree, namespace):\n    # type: (_Element, ArTree, str) -> None\n    \"\"\"Parse the xml tree into ArTree objects.\"\"\"\n    for child in tag:  # type: _Element\n        name_elem = child.find('./' + namespace + 'SHORT-NAME')\n        # long_name = child.find('./' + namespace + 'LONG-NAME')\n        if name_elem is not None and child is not None:\n            fill_tree_from_xml(child, ar_tree.append_child(name_elem.text, child), namespace)\n        if name_elem is None and child is not None:\n            fill_tree_from_xml(child, ar_tree, namespace)", "response": "Parse the xml tree into ArTree objects."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef ar_path_to_x_path(ar_path, dest_element=None):\n    # type: (str, typing.Optional[str]) -> str\n    \"\"\"Get path in translation-dictionary.\"\"\"\n    ar_path_elements = ar_path.strip('/').split('/')\n    xpath = \".\"\n\n    for element in ar_path_elements[:-1]:\n        xpath += \"//A:SHORT-NAME[text()='\" + element + \"']/..\"\n    if dest_element:\n        xpath += \"//A:\" + dest_element + \"/A:SHORT-NAME[text()='\" + ar_path_elements[-1] + \"']/..\"\n    else:\n        xpath += \"//A:SHORT-NAME[text()='\" + ar_path_elements[-1] + \"']/..\"\n\n    return xpath", "response": "Get path in translation - dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_element_by_path(tree, path_and_name, namespace):\n    # type: (_Element, str, str) -> typing.Union[_Element, None]\n    \"\"\"Find sub-element of given path with given short name.\"\"\"\n    global xml_element_cache\n    namespace_map = {'A': namespace[1:-1]}\n    base_path, element_name = path_and_name.rsplit('/', 1)\n    if base_path in xml_element_cache:\n        base_element = xml_element_cache[base_path]\n    else:\n        base_xpath = ar_path_to_x_path(base_path)\n        elems = tree.xpath(base_xpath, namespaces=namespace_map)\n        base_element = elems[0] if elems else None\n        xml_element_cache[base_path] = base_element\n\n    element_found = None\n    if base_element is not None:\n        element_found = base_element.xpath(\n            \".//A:SHORT-NAME[text()='{name}']/..\".format(name=element_name),\n            namespaces=namespace_map)[0]\n    return element_found", "response": "Find sub - element of given path with given short name."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget element from ArTree by path.", "response": "def get_cached_element_by_path(data_tree, path):\n    # type: (ArTree, str) -> typing.Optional[_Element]\n    \"\"\"Get element from ArTree by path.\"\"\"\n    if not isinstance(data_tree, ArTree):\n        logger.warning(\"%s not called with ArTree, return None\", get_cached_element_by_path.__name__)\n        return None\n    ptr = data_tree\n    for name in path.split('/'):\n        if ptr is None:\n            return None\n        if name.strip():\n            ptr = ptr.get_child_by_name(name)\n    return ptr.ref if ptr else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget first sub - child with given name.", "response": "def get_child(parent, tag_name, root_or_cache, namespace):\n    # type: (_Element, str, _DocRoot, str) -> typing.Optional[_Element]\n    \"\"\"Get first sub-child or referenced sub-child with given name.\"\"\"\n    # logger.debug(\"get_child: \" + tag_name)\n    if parent is None:\n        return None\n    ret = parent.find('.//' + namespace + tag_name)\n    if ret is None:  # no direct element - try reference\n        reference = parent.find('.//' + namespace + tag_name + '-REF')\n        if reference is not None:\n            if isinstance(root_or_cache, ArTree):\n                ret = get_cached_element_by_path(root_or_cache, reference.text)\n            else:\n                ret = get_element_by_path(root_or_cache, reference.text, namespace)\n    return ret"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_element_name(parent, ns):\n    # type: (_Element, str) -> str\n    \"\"\"Get element short name.\"\"\"\n    name = parent.find('./' + ns + 'SHORT-NAME')\n    if name is not None and name.text is not None:\n        return name.text\n    return \"\"", "response": "Get element short name."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the signals from the XML file.", "response": "def get_signals(signal_array, frame, root_or_cache, ns, multiplex_id, float_factory):\n    # type: (typing.Sequence[_Element], canmatrix.Frame, _DocRoot, str, _MultiplexId, typing.Callable) -> None\n    \"\"\"Add signals from xml to the Frame.\"\"\"\n    global signal_rxs\n    group_id = 1\n    if signal_array is None:  # Empty signalarray - nothing to do\n        return\n    for signal in signal_array:\n        compu_method = None\n        motorola = get_child(signal, \"PACKING-BYTE-ORDER\", root_or_cache, ns)\n        start_bit = get_child(signal, \"START-POSITION\", root_or_cache, ns)\n\n        isignal = get_child(signal, \"SIGNAL\", root_or_cache, ns)\n        if isignal is None:\n            isignal = get_child(signal, \"I-SIGNAL\", root_or_cache, ns)\n        if isignal is None:\n            isignal = get_child(signal, \"I-SIGNAL-GROUP\", root_or_cache, ns)\n            if isignal is not None:\n                logger.debug(\"get_signals: found I-SIGNAL-GROUP \")\n\n                isignal_array = find_children_by_path(isignal, \"I-SIGNAL\", root_or_cache, ns)\n                get_sys_signals(isignal, isignal_array, frame, group_id, ns)\n                group_id = group_id + 1\n                continue\n        if isignal is None:\n            logger.debug(\n                'Frame %s, no isignal for %s found',\n                frame.name, get_child(signal, \"SHORT-NAME\", root_or_cache, ns).text)\n\n        base_type = get_child(isignal, \"BASE-TYPE\", root_or_cache, ns)\n        signal_name = None  # type: typing.Optional[str]\n        signal_name_elem = get_child(isignal, \"LONG-NAME\", root_or_cache, ns)\n        if signal_name_elem is not None:\n            signal_name_elem = get_child(signal_name_elem, \"L-4\", root_or_cache, ns)\n            if signal_name_elem is not None:\n                signal_name = signal_name_elem.text\n        system_signal = get_child(isignal, \"SYSTEM-SIGNAL\", root_or_cache, ns)\n        if system_signal is None:\n            logger.debug('Frame %s, signal %s has no system-signal', frame.name, isignal.tag)\n\n        if \"SYSTEM-SIGNAL-GROUP\" in system_signal.tag:\n            system_signals = find_children_by_path(system_signal, \"SYSTEM-SIGNAL-REFS/SYSTEM-SIGNAL\", root_or_cache, ns)\n            get_sys_signals(system_signal, system_signals, frame, group_id, ns)\n            group_id = group_id + 1\n            continue\n\n        length = get_child(isignal, \"LENGTH\", root_or_cache, ns)\n        if length is None:\n            length = get_child(system_signal, \"LENGTH\", root_or_cache, ns)\n\n        name = get_child(system_signal, \"SHORT-NAME\", root_or_cache, ns)\n        unit_element = get_child(isignal, \"UNIT\", root_or_cache, ns)\n        display_name = get_child(unit_element, \"DISPLAY-NAME\", root_or_cache, ns)\n        if display_name is not None:\n            signal_unit = display_name.text\n        else:\n            signal_unit = \"\"\n\n        signal_min = None  # type: canmatrix.types.OptionalPhysicalValue\n        signal_max = None  # type: canmatrix.types.OptionalPhysicalValue\n        receiver = []  # type: typing.List[str]\n\n        signal_description = get_element_desc(system_signal, root_or_cache, ns)\n\n        datatype = get_child(system_signal, \"DATA-TYPE\", root_or_cache, ns)\n        if datatype is None:  # AR4?\n            data_constr = None\n            compu_method = None\n            base_type = None\n            for test_signal in [isignal, system_signal]:\n                if data_constr is None:\n                    data_constr = get_child(test_signal, \"DATA-CONSTR\", root_or_cache, ns)\n                if compu_method is None:\n                    compu_method = get_child(test_signal, \"COMPU-METHOD\", root_or_cache, ns)\n                if base_type is None:\n                    base_type = get_child(test_signal, \"BASE-TYPE\", root_or_cache, ns)\n\n\n\n            lower = get_child(data_constr, \"LOWER-LIMIT\", root_or_cache, ns)\n            upper = get_child(data_constr, \"UPPER-LIMIT\", root_or_cache, ns)\n            encoding = None  # TODO - find encoding in AR4\n        else:\n            lower = get_child(datatype, \"LOWER-LIMIT\", root_or_cache, ns)\n            upper = get_child(datatype, \"UPPER-LIMIT\", root_or_cache, ns)\n            encoding = get_child(datatype, \"ENCODING\", root_or_cache, ns)\n\n        if encoding is not None and (encoding.text == \"SINGLE\" or encoding.text == \"DOUBLE\"):\n            is_float = True\n        else:\n            is_float = False\n        \n        if lower is not None and upper is not None:\n            signal_min = float_factory(lower.text)\n            signal_max = float_factory(upper.text)\n\n        datdefprops = get_child(datatype, \"SW-DATA-DEF-PROPS\", root_or_cache, ns)\n\n        if compu_method is None:\n            compu_method = get_child(datdefprops, \"COMPU-METHOD\", root_or_cache, ns)\n        if compu_method is None:  # AR4\n            compu_method = get_child(isignal, \"COMPU-METHOD\", root_or_cache, ns)\n            base_type = get_child(isignal, \"BASE-TYPE\", root_or_cache, ns)\n            encoding = get_child(base_type, \"BASE-TYPE-ENCODING\", root_or_cache, ns)\n            if encoding is not None and encoding.text == \"IEEE754\":\n                is_float = True\n        if compu_method is None:\n            logger.debug('No Compmethod found!! - try alternate scheme 1.')\n            networkrep = get_child(isignal, \"NETWORK-REPRESENTATION-PROPS\", root_or_cache, ns)\n            data_def_props_var = get_child(networkrep, \"SW-DATA-DEF-PROPS-VARIANTS\", root_or_cache, ns)\n            data_def_props_cond = get_child(data_def_props_var, \"SW-DATA-DEF-PROPS-CONDITIONAL\", root_or_cache, ns)\n            if data_def_props_cond is not None:\n                try:\n                    compu_method = get_child(data_def_props_cond, \"COMPU-METHOD\", root_or_cache, ns)\n                except:\n                    logger.debug('No valid compu method found for this - check ARXML file!!')\n                    compu_method = None\n        #####################################################################################################\n        # no found compu-method fuzzy search in systemsignal:\n        #####################################################################################################\n        if compu_method is None:\n            logger.debug('No Compmethod found!! - fuzzy search in syssignal.')\n            compu_method = get_child(system_signal, \"COMPU-METHOD\", root_or_cache, ns)\n\n        # decode compuMethod:\n        (values, factor, offset, unit_elem, const) = decode_compu_method(compu_method, root_or_cache, ns, float_factory)\n\n        if signal_min is not None:\n            signal_min *= factor\n            signal_min += offset\n        if signal_max is not None:\n            signal_max *= factor\n            signal_max += offset\n\n        if base_type is None:\n            base_type = get_child(datdefprops, \"BASE-TYPE\", root_or_cache, ns)\n        if base_type is not None:\n            type_name = get_element_name(base_type, ns)\n            if type_name[0] == 'u':\n                is_signed = False  # unsigned\n            else:\n                is_signed = True  # signed\n        else:\n            is_signed = True  # signed\n\n        if unit_elem is not None:\n            longname = get_child(unit_elem, \"LONG-NAME\", root_or_cache, ns)\n        #####################################################################################################\n        # Modification to support obtaining the Signals Unit by DISPLAY-NAME. 07June16\n        #####################################################################################################\n            display_name = None\n            try:\n                display_name = get_child(unit_elem, \"DISPLAY-NAME\", root_or_cache, ns)\n            except:\n                logger.debug('No Unit Display name found!! - using long name')\n            if display_name is not None:\n                signal_unit = display_name.text\n            else:\n                l4 = get_child(longname, \"L-4\", root_or_cache, ns)\n                if l4 is not None:\n                    signal_unit = l4.text\n\n        init_list = find_children_by_path(system_signal, \"INIT-VALUE/VALUE\", root_or_cache, ns)\n\n        if not init_list:\n            init_list = find_children_by_path(isignal, \"INIT-VALUE/NUMERICAL-VALUE-SPECIFICATION/VALUE\", root_or_cache, ns)  # #AR4.2\n        if init_list:\n            initvalue = init_list[0]\n        else:\n            initvalue = None\n\n        is_little_endian = False\n        if motorola is not None:\n            if motorola.text == 'MOST-SIGNIFICANT-BYTE-LAST':\n                is_little_endian = True\n        else:\n            logger.debug('no name byte order for signal' + name.text)\n\n        if name is None:\n            logger.debug('no name for signal given')\n        if start_bit is None:\n            logger.debug('no startBit for signal given')\n        if length is None:\n            logger.debug('no length for signal given')\n\n        if start_bit is not None:\n            new_signal = canmatrix.Signal(\n                name.text,\n                start_bit=int(start_bit.text),\n                size=int(length.text),\n                is_little_endian=is_little_endian,\n                is_signed=is_signed,\n                factor=factor,\n                offset=offset,\n                unit=signal_unit,\n                receivers=receiver,\n                multiplex=multiplex_id,\n                comment=signal_description,\n                is_float=is_float)\n\n            if signal_min is not None:\n                new_signal.min = signal_min\n            if signal_max is not None:\n                new_signal.max = signal_max\n\n            if new_signal.is_little_endian == 0:\n                # startbit of motorola coded signals are MSB in arxml\n                new_signal.set_startbit(int(start_bit.text), bitNumbering=1)\n\n            # save signal, to determin receiver-ECUs for this signal later\n            signal_rxs[system_signal] = new_signal\n\n            if base_type is not None:\n                temp = get_child(base_type, \"SHORT-NAME\", root_or_cache, ns)\n                if temp is not None and \"boolean\" == temp.text:\n                    new_signal.add_values(1, \"TRUE\")\n                    new_signal.add_values(0, \"FALSE\")\n\n            if initvalue is not None and initvalue.text is not None:\n                initvalue.text = canmatrix.utils.guess_value(initvalue.text)\n                new_signal._initValue = float_factory(initvalue.text)\n                new_signal.add_attribute(\"GenSigStartValue\", str(new_signal._initValue))\n            else:\n                new_signal._initValue = 0\n\n            for key, value in list(values.items()):\n                new_signal.add_values(key, value)\n            if signal_name is not None:\n                new_signal.add_attribute(\"LongName\", signal_name)\n            frame.add_signal(new_signal)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_element_desc(element, ar_tree, ns):\n    # type: (_Element, _DocRoot, str) -> str\n    \"\"\"Get element description from XML.\"\"\"\n    desc = get_child(element, \"DESC\", ar_tree, ns)\n    txt = get_child(desc, 'L-2[@L=\"DE\"]', ar_tree, ns)\n    if txt is None:\n        txt = get_child(desc, 'L-2[@L=\"EN\"]', ar_tree, ns)\n    if txt is None:\n        txt = get_child(desc, 'L-2', ar_tree, ns)\n    if txt is not None:\n        return txt.text\n    else:\n        return \"\"", "response": "Get element description from XML."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ecuc_extract_signal(signal_node, ns):\n    # type: (_Element, str) -> canmatrix.Signal\n    \"\"\"Extract signal from ECUc file.\"\"\"\n    attributes = signal_node.findall(\".//\" + ns + \"DEFINITION-REF\")  # type: typing.Sequence[_Element]\n    start_bit = None\n    size = 0\n    is_little = False\n    # endianness = None\n    # init_value = 0\n    # signal_type = None\n    # timeout = 0\n    for attribute in attributes:\n        if attribute.text.endswith(\"ComBitPosition\"):\n            start_bit = int(attribute.getparent().find(\".//\" + ns + \"VALUE\").text)\n        if attribute.text.endswith(\"ComBitSize\"):\n            size = int(attribute.getparent().find(\".//\" + ns + \"VALUE\").text)\n        if attribute.text.endswith(\"ComSignalEndianness\"):\n            endianness = attribute.getparent().find(\".//\" + ns + \"VALUE\").text\n            is_little = \"LITTLE_ENDIAN\" in endianness\n        if attribute.text.endswith(\"ComSignalInitValue\"):\n            init_value = int(attribute.getparent().find(\".//\" + ns + \"VALUE\").text)\n        if attribute.text.endswith(\"ComSignalType\"):\n            signal_type = attribute.getparent().find(\".//\" + ns + \"VALUE\").text\n        if attribute.text.endswith(\"ComTimeout\"):\n            timeout = int(attribute.getparent().find(\".//\" + ns + \"VALUE\").text)\n    return canmatrix.Signal(get_element_name(signal_node, ns), start_bit=start_bit, size=size, is_little_endian=is_little)", "response": "Extracts a signal from the ECUc file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append_child(self, name, child):  # type: (str, typing.Any) -> ArTree\n        temp = ArTree(name, child)\n        self._array.append(temp)\n        return temp", "response": "Append a child to the tree and return it."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_log_level(logger, level):  # type: (logging.Logger, int) -> None\n    if level > 2:\n        level = 2\n    if level < -1:\n        level = -1\n\n    levels = {\n        -1: logging.ERROR,\n        0: logging.WARN,\n        1: logging.INFO,\n        2: logging.DEBUG\n    }\n    logger.setLevel(levels[level])", "response": "Dynamic reconfiguration of the log level"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndumping a CANMatrix object to a file.", "response": "def dump(db, f, **options):  # type: (canmatrix.CanMatrix, typing.IO, **typing.Any) -> None\n    \"\"\"\n    export canmatrix-object as .sym file (compatible to PEAK-Systems)\n    \"\"\"\n    global enum_dict\n    global enums\n    sym_encoding = options.get('symExportEncoding', 'iso-8859-1')\n\n    enum_dict = {}\n    enums = \"{ENUMS}\\n\"\n\n    header = \"\"\"FormatVersion=5.0 // Do not edit this line!\nTitle=\\\"canmatrix-Export\\\"\n\"\"\"\n    f.write(header.encode(sym_encoding))\n\n    def send_receive(for_frame):\n        return (\n            for_frame.attributes.get('Sendable', 'True') == 'True',\n            for_frame.attributes.get('Receivable', 'True') == 'True',\n        )\n\n    sections = collections.OrderedDict((\n        ('SEND', tuple(f for f in db.frames if send_receive(f) == (True, False))),\n        ('RECEIVE', tuple(f for f in db.frames if send_receive(f) == (False, True))),\n        ('SENDRECEIVE', tuple(f for f in db.frames if send_receive(f) == (True, True))),\n    ))\n\n    output = '\\n'\n\n    for name, frames in sections.items():\n        if len(frames) == 0:\n            continue\n\n        # Frames\n        output += \"{{{}}}\\n\\n\".format(name)\n\n        # trigger all frames\n        for frame in frames:\n            name = \"[\" + frame.name + \"]\\n\"\n\n            if frame.arbitration_id.extended == 1:\n                id_type = \"ID=%08Xh\" % frame.arbitration_id.id\n            else:\n                id_type = \"ID=%03Xh\" % frame.arbitration_id.id\n            if frame.comment is not None and len(frame.comment) > 0:\n                id_type += \"\\t// \" + \\\n                    frame.comment.replace('\\n', ' ').replace('\\r', ' ')\n            id_type += \"\\n\"\n            if frame.arbitration_id.extended == 1:\n                id_type += \"Type=Extended\\n\"\n            else:\n                id_type += \"Type=Standard\\n\"\n\n            # check if frame has multiplexed signals\n            multiplex = 0\n            for signal in frame.signals:\n                if signal.multiplex is not None:\n                    multiplex = 1\n\n            if multiplex == 1:\n                # search for multiplexor in frame:\n                for signal in frame.signals:\n                    if signal.multiplex == 'Multiplexor':\n                        mux_signal = signal\n\n                # ticker all possible mux-groups as i (0 - 2^ (number of bits of multiplexor))\n                first = 0\n                for i in range(0, 1 << int(mux_signal.size)):\n                    found = 0\n                    mux_out = \"\"\n                    # ticker all signals\n                    for signal in frame.signals:\n                        # if signal is in mux-group i\n                        if signal.multiplex == i:\n                            mux_out = name\n                            if first == 0:\n                                mux_out += id_type\n                                first = 1\n                            mux_out += \"DLC=%d\\n\" % frame.size\n                            if \"GenMsgCycleTime\" in db.frame_defines:\n                                cycle_time = frame.attribute(\"GenMsgCycleTime\", db=db)\n                                if cycle_time is not None:\n                                    mux_out += \"CycleTime=\" + str(cycle_time) + \"\\n\"\n\n                            mux_name = frame.mux_names.get(i, mux_signal.name + \"%d\" % i)\n\n                            mux_out += \"Mux=\" + mux_name\n                            start_bit = mux_signal.get_startbit()\n                            s = str(i)\n                            if len(s) > 1:\n                                length = len(\n                                    '{:X}'.format(int(mux_signal.calc_max()))\n                                )\n                                s = '{:0{}X}h'.format(i, length)\n                            if not signal.is_little_endian:\n                                # Motorola\n                                mux_out += \" %d,%d %s -m\" % (start_bit, mux_signal.size, s)\n                            else:\n                                mux_out += \" %d,%d %s\" % (start_bit, mux_signal.size, s)\n                            if not mux_out.endswith('h'):\n                                mux_out += ' '\n                            if i in mux_signal.comments:\n                                comment = mux_signal.comments.get(i)\n                                if len(comment) > 0:\n                                    mux_out += '\\t// ' + comment\n                            mux_out += \"\\n\"\n                            found = 1\n                            break\n\n                    if found == 1:\n                        for signal in frame.signals:\n                            if signal.multiplex == i or signal.multiplex is None:\n                                mux_out += create_signal(db, signal)\n                        output += mux_out + \"\\n\"\n\n            else:\n                # no multiplex signals in frame, just 'normal' signals\n                output += name\n                output += id_type\n                output += \"DLC=%d\\n\" % frame.size\n                if \"GenMsgCycleTime\" in db.frame_defines:\n                    cycle_time = frame.attribute(\"GenMsgCycleTime\", db=db)\n                    if cycle_time is not None:\n                        output += \"CycleTime=\" + str(cycle_time) + \"\\n\"\n                for signal in frame.signals:\n                    output += create_signal(db, signal)\n                output += \"\\n\"\n    enums += '\\n'.join(sorted(enum_dict.values()))\n    # write output file\n    f.write((enums + '\\n').encode(sym_encoding))\n    f.write(output.encode(sym_encoding))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list_pgn(db):\n    # type: (canmatrix.CanMatrix) -> typing.Tuple[typing.List[int], typing.List[canmatrix.ArbitrationId]]\n    \"\"\"\n    Get all PGN values for given frame.\n\n    :param db: CanMatrix database\n    :return: tuple of [pgn] and [arbitration_id]\n    \"\"\"\n    id_list = [x.arbitration_id for x in db.frames]\n    pgn_list = [arb_id.pgn for arb_id in id_list]\n    return pgn_list, id_list", "response": "Get all PGN values for given frame."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ids_sharing_same_pgn(id_x, pgn_x, id_y, pgn_y):\n    # type: (typing.Sequence[canmatrix.ArbitrationId], typing.Sequence[int], typing.Sequence[canmatrix.ArbitrationId], typing.Sequence[int]) -> typing.Iterable[typing.Tuple[canmatrix.ArbitrationId, canmatrix.ArbitrationId]]\n    \"\"\"Yield arbitration ids which has the same pgn.\"\"\"\n    for id_a, pgn_a in zip(id_x, pgn_x):\n        for id_b, pgn_b in zip(id_y, pgn_y):\n            if pgn_a == pgn_b:\n                yield (id_a, id_b)", "response": "Yields arbitration ids which share the same pgn."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nguesses the value for a common string.", "response": "def guess_value(text_value):  # type: (str) -> str\n    \"\"\"\n    Get string value for common strings.\n    Method is far from complete but helping with odd arxml files.\n\n    :param text_value: value in text like \"true\"\n    :return: string for value like \"1\"\n    \"\"\"\n    if sys.version_info >= (3, 0):\n        text_value = text_value.casefold()\n    else:\n        text_value = text_value.lower()\n    if text_value in [\"false\", \"off\"]:\n        return \"0\"\n    elif text_value in [\"true\", \"on\"]:\n        return \"1\"\n    return text_value"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate populate and return the VersioneerConfig object.", "response": "def get_config():\n    \"\"\"Create, populate and return the VersioneerConfig() object.\"\"\"\n    # these strings are filled in when 'setup.py versioneer' creates\n    # _version.py\n    cfg = VersioneerConfig()\n    cfg.VCS = \"git\"\n    cfg.style = \"pep440-post\"\n    cfg.tag_prefix = \"\"\n    cfg.parentdir_prefix = \"None\"\n    cfg.versionfile_source = \"canmatrix/_version.py\"\n    cfg.verbose = False\n    return cfg"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef register_vcs_handler(vcs, method):  # tyoe: (str, str) -> typing.Callable  # decorator\n    def decorate(f):\n        \"\"\"Store f in HANDLERS[vcs][method].\"\"\"\n        if vcs not in HANDLERS:\n            HANDLERS[vcs] = {}\n        HANDLERS[vcs][method] = f\n        return f\n    return decorate", "response": "Decorator to mark a method as the handler for a particular VCS."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets version from git describe and _version. py.", "response": "def git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    \"\"\"Get version from 'git describe' in the root of the source tree.\n\n    This only gets called if the git-archive 'subst' keywords were *not*\n    expanded, and _version.py hasn't already been rewritten with a short\n    version string, meaning we're inside a checked out source tree.\n    \"\"\"\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n\n    out, rc = run_command(GITS, [\"rev-parse\", \"--git-dir\"], cwd=root,\n                          hide_stderr=True)\n    if rc != 0:\n        if verbose:\n            print(\"Directory %s not under git control\" % root)\n        raise NotThisMethod(\"'git rev-parse --git-dir' returned error\")\n\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out, rc = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                          \"--always\", \"--long\",\n                                          \"--match\", \"%s*\" % tag_prefix],\n                                   cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out, rc = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out, rc = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                    cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    # commit date: see ISO-8601 comment in git_versions_from_keywords()\n    date = run_command(GITS, [\"show\", \"-s\", \"--format=%ci\", \"HEAD\"],\n                       cwd=root)[0].strip()\n    pieces[\"date\"] = date.strip().replace(\" \", \"T\", 1).replace(\" \", \"\", 1)\n\n    return pieces"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_versions():\n    # I am in _version.py, which lives at ROOT/VERSIONFILE_SOURCE. If we have\n    # __file__, we can work backwards from there to the root. Some\n    # py2exe/bbfreeze/non-CPython implementations don't do __file__, in which\n    # case we can only use expanded keywords.\n\n    cfg = get_config()\n    verbose = cfg.verbose\n\n    try:\n        return git_versions_from_keywords(get_keywords(), cfg.tag_prefix,\n                                          verbose)\n    except NotThisMethod:\n        pass\n\n    try:\n        root = os.path.realpath(__file__)\n        # versionfile_source is the relative path from the top of the source\n        # tree (where the .git directory might live) to this file. Invert\n        # this to find the root from __file__.\n        for i in cfg.versionfile_source.split('/'):\n            root = os.path.dirname(root)\n    except NameError:\n        return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n                \"dirty\": None,\n                \"error\": \"unable to find root of source tree\",\n                \"date\": None}\n\n    try:\n        pieces = git_pieces_from_vcs(cfg.tag_prefix, root, verbose)\n        return render(pieces, cfg.style)\n    except NotThisMethod:\n        pass\n\n    try:\n        if cfg.parentdir_prefix:\n            return versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n    except NotThisMethod:\n        pass\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None,\n            \"error\": \"unable to compute version\", \"date\": None}", "response": "Get version information or return default if unable to do so."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef copy_ecu(ecu_or_glob, source_db, target_db):\n    # type: (typing.Union[cm.Ecu, str], cm.CanMatrix, cm.CanMatrix) -> None\n    \"\"\"\n    Copy ECU(s) identified by Name or as Object from source CAN matrix to target CAN matrix.\n    This function additionally copy all relevant Defines.\n\n    :param ecu_or_glob: Ecu instance or glob pattern for Ecu name\n    :param source_db: Source CAN matrix\n    :param target_db: Destination CAN matrix\n    \"\"\"\n    # check whether ecu_or_glob is object or symbolic name\n    if isinstance(ecu_or_glob, cm.Ecu):\n        ecu_list = [ecu_or_glob]\n    else:\n        ecu_list = source_db.glob_ecus(ecu_or_glob)\n\n    for ecu in ecu_list:\n        target_db.add_ecu(copy.deepcopy(ecu))\n\n        # copy all ecu-defines\n        for attribute in ecu.attributes:\n            if attribute not in target_db.ecu_defines:\n                target_db.add_ecu_defines(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.ecu_defines[attribute].definition))\n                target_db.add_define_default(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.ecu_defines[attribute].defaultValue))\n            # update enum data types if needed:\n            if source_db.ecu_defines[attribute].type == 'ENUM':\n                temp_attr = ecu.attribute(attribute, db=source_db)\n                if temp_attr not in target_db.ecu_defines[attribute].values:\n                    target_db.ecu_defines[attribute].values.append(copy.deepcopy(temp_attr))\n                    target_db.ecu_defines[attribute].update()", "response": "Copies an Ecu instance or glob pattern from source CAN matrix to target CAN matrix."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy_ecu_with_frames(ecu_or_glob, source_db, target_db):\n    # type: (typing.Union[cm.Ecu, str], cm.CanMatrix, cm.CanMatrix) -> None\n    \"\"\"\n    Copy ECU(s) identified by Name or as Object from source CAN matrix to target CAN matrix.\n    This function additionally copy all relevant Frames and Defines.\n\n    :param ecu_or_glob: Ecu instance or glob pattern for Ecu name\n    :param source_db: Source CAN matrix\n    :param target_db: Destination CAN matrix\n    \"\"\"\n    # check whether ecu_or_glob is object or symbolic name\n    if isinstance(ecu_or_glob, cm.Ecu):\n        ecu_list = [ecu_or_glob]\n    else:\n        ecu_list = source_db.glob_ecus(ecu_or_glob)\n\n    for ecu in ecu_list:\n        logger.info(\"Copying ECU \" + ecu.name)\n\n        target_db.add_ecu(copy.deepcopy(ecu))\n\n        # copy tx-frames\n        for frame in source_db.frames:\n            if ecu.name in frame.transmitters:\n                copy_frame(frame.arbitration_id, source_db, target_db)\n\n        # copy rx-frames\n        for frame in source_db.frames:\n            for signal in frame.signals:\n                if ecu.name in signal.receivers:\n                    copy_frame(frame.arbitration_id, source_db, target_db)\n                    break\n\n        # copy all ECU defines\n        for attribute in ecu.attributes:\n            if attribute not in target_db.ecu_defines:\n                target_db.add_ecu_defines(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.ecu_defines[attribute].definition))\n                target_db.add_define_default(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.ecu_defines[attribute].defaultValue))\n            # update enum-data types if needed:\n            if source_db.ecu_defines[attribute].type == 'ENUM':\n                temp_attr = ecu.attribute(attribute, db=source_db)\n                if temp_attr not in target_db.ecu_defines[attribute].values:\n                    target_db.ecu_defines[attribute].values.append(copy.deepcopy(temp_attr))\n                    target_db.ecu_defines[attribute].update()", "response": "This function copies all relevant Frames and Defines from source ECU to target ECU."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncopy all the signals identified by the given glob pattern from source CAN matrix to target CAN matrix.", "response": "def copy_signal(signal_glob, source_db, target_db):\n    # type: (str, cm.CanMatrix, cm.CanMatrix) -> None\n    \"\"\"\n    Copy Signals identified by name from source CAN matrix to target CAN matrix.\n    In target CanMatrix the signal is put without frame, just on top level.\n\n    :param signal_glob: Signal glob pattern\n    :param source_db: Source CAN matrix\n    :param target_db: Destination CAN matrix\n    \"\"\"\n    for frame in source_db.frames:\n        for signal in frame.glob_signals(signal_glob):\n            target_db.add_signal(signal)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncopy a Frame from source CAN matrix to target CAN matrix.", "response": "def copy_frame(frame_id, source_db, target_db):\n    # type: (cm.ArbitrationId, cm.CanMatrix, cm.CanMatrix) -> bool\n    \"\"\"\n    Copy a Frame identified by ArbitrationId from source CAN matrix to target CAN matrix.\n    This function additionally copy all relevant ECUs and Defines.\n\n    :param frame_id: Frame arbitration od\n    :param source_db: Source CAN matrix\n    :param target_db: Destination CAN matrix\n    \"\"\"\n    frame_list = [source_db.frame_by_id(frame_id)]\n\n    for frame in frame_list:\n        logger.info(\"Copying Frame \" + frame.name)\n\n        if target_db.frame_by_id(frame.arbitration_id) is not None:\n            # frame already in target_db...\n            return False\n\n        # copy Frame-Object:\n        target_db.add_frame(copy.deepcopy(frame))\n\n        # ECUs:\n        # each transmitter of Frame could be ECU that is not listed already\n        for transmitter in frame.transmitters:\n            target_ecu = target_db.ecu_by_name(transmitter)\n            source_ecu = source_db.ecu_by_name(transmitter)\n            if source_ecu is not None and target_ecu is None:\n                copy_ecu(source_ecu, source_db, target_db)\n\n        # trigger all signals of Frame\n        for sig in frame.signals:\n            # each receiver of Signal could be ECU that is not listed already\n            for receiver in sig.receivers:\n                target_ecu = target_db.ecu_by_name(receiver)\n                source_ecu = source_db.ecu_by_name(receiver)\n                if source_ecu is not None and target_ecu is None:\n                    copy_ecu(source_ecu, source_db, target_db)\n\n        # copy all frame-defines\n        attributes = frame.attributes\n        for attribute in attributes:\n            if attribute not in target_db.frame_defines:\n                target_db.add_frame_defines(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.frame_defines[attribute].definition))\n                target_db.add_define_default(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.frame_defines[attribute].defaultValue))\n            # update enum data types if needed:\n            if source_db.frame_defines[attribute].type == 'ENUM':\n                temp_attr = frame.attribute(attribute, db=source_db)\n                if temp_attr not in target_db.frame_defines[attribute].values:\n                    target_db.frame_defines[attribute].values.append(copy.deepcopy(temp_attr))\n                    target_db.frame_defines[attribute].update()\n\n        # trigger all signals of Frame\n        for sig in frame.signals:\n            # delete all 'unknown' attributes\n            for attribute in sig.attributes:\n                target_db.add_signal_defines(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.signal_defines[attribute].definition))\n                target_db.add_define_default(\n                    copy.deepcopy(attribute), copy.deepcopy(source_db.signal_defines[attribute].defaultValue))\n                # update enum data types if needed:\n                if source_db.signal_defines[attribute].type == 'ENUM':\n                    temp_attr = sig.attribute(attribute, db=source_db)\n                    if temp_attr not in target_db.signal_defines[attribute].values:\n                        target_db.signal_defines[attribute].values.append(copy.deepcopy(temp_attr))\n                        target_db.signal_defines[attribute].update()\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the XML field object of the given name.", "response": "def get_field(self, name):\n        \"\"\"\n        Gets the XML field object of the given name.\n        \"\"\"\n        # Quicker in case the exact name was used.\n        field = self._all_fields.get(name)\n        if field is not None:\n            return field\n\n        for field_name, field in self._all_fields.items():\n            if self._sanitize_field_name(name) == self._sanitize_field_name(field_name):\n                return field"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the value of the given field.", "response": "def get_field_value(self, name, raw=False):\n        \"\"\"\n        Tries getting the value of the given field.\n        Tries it in the following order: show (standard nice display), value (raw value), showname (extended nice display).\n\n        :param name: The name of the field\n        :param raw: Only return raw value\n        :return: str of value\n        \"\"\"\n        field = self.get_field(name)\n        if field is None:\n            return\n\n        if raw:\n            return field.raw_value\n\n        return field"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _sanitize_field_name(self, field_name):\n        field_name = field_name.replace(self._field_prefix, '')\n        return field_name.replace('.', '_').replace('-', '_').lower()", "response": "Sanitize a field name for use in a XML field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield all lines that represent the fields of the layer.", "response": "def _get_all_field_lines(self):\n        \"\"\"\n        Returns all lines that represent the fields of the layer (both their names and values).\n        \"\"\"\n        for field in self._get_all_fields_with_alternates():\n            # Change to yield from\n            for line in self._get_field_or_layer_repr(field):\n                yield line"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a field by its showname", "response": "def get_field_by_showname(self, showname):\n        \"\"\"\n        Gets a field by its \"showname\"\n        (the name that appears in Wireshark's detailed display i.e. in 'User-Agent: Mozilla...', 'User-Agent' is the\n         showname)\n\n         Returns None if not found.\n        \"\"\"\n        for field in self._get_all_fields_with_alternates():\n            if field.showname_key == showname:\n                # Return it if \"XXX: whatever == XXX\"\n                return field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_field(self, name):\n        # We only make the wrappers here (lazily) to avoid creating a ton of objects needlessly.\n        field = self._wrapped_fields.get(name)\n        if field is None:\n            is_fake = False\n            field = self._get_internal_field_by_name(name)\n            if field is None:\n                # Might be a \"fake\" field in JSON\n                is_fake = self._is_fake_field(name)\n                if not is_fake:\n                    raise AttributeError(\"No such field %s\" % name)\n            field = self._make_wrapped_field(name, field, is_fake=is_fake)\n            self._wrapped_fields[name] = field\n        return field", "response": "Gets a field by its full or partial name."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_internal_field_by_name(self, name):\n        field = self._all_fields.get(name, self._all_fields.get('%s.%s' % (self._full_name, name)))\n        if field is not None:\n            return field\n        for field_name in self._all_fields:\n            # Specific name\n            if field_name.endswith('.%s' % name):\n                return self._all_fields[field_name]", "response": "Gets the internal field by name."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate the field lazily.", "response": "def _make_wrapped_field(self, name, field, is_fake=False, full_name=None):\n        \"\"\"Creates the field lazily.\n\n        If it's a simple field, wraps it in a container that adds extra features.\n        If it's a nested layer, creates a layer for it.\n        If it's an intermediate layer, copies over the relevant fields and creates a new layer for\n        it.\n        \"\"\"\n        if not full_name:\n            full_name = '%s.%s' % (self._full_name, name)\n\n        if is_fake:\n            # Populate with all fields that are supposed to be inside of it\n            field = {key: value for key, value in self._all_fields.items()\n                     if key.startswith(full_name)}\n        if isinstance(field, dict):\n            if name.endswith('_tree'):\n                name = name.replace('_tree', '')\n                full_name = '%s.%s' % (self._full_name, name)\n            return JsonLayer(name, field, full_name=full_name, is_intermediate=is_fake)\n        elif isinstance(field, list):\n            # For whatever reason in list-type object it goes back to using the original parent name\n            return [self._make_wrapped_field(name, field_part,\n                                             full_name=self._full_name.split('.')[0])\n                    for field_part in field]\n\n        return LayerFieldsContainer(LayerField(name=name, value=field))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck whether the given dotted name is a field name.", "response": "def has_field(self, dotted_name):\n        \"\"\"\n        Checks whether the layer has the given field name.\n        Can get a dotted name, i.e. layer.sublayer.subsublayer.field\n        \"\"\"\n        parts = dotted_name.split('.')\n        cur_layer = self\n        for part in parts:\n            if part in cur_layer.field_names:\n                cur_layer = cur_layer.get_field(part)\n            else:\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_parameters(self, packet_count=None):\n        params = super(InMemCapture, self).get_parameters(packet_count=packet_count)\n        params += ['-i', '-']\n        return params", "response": "Returns the special tshark parameters to be used according to the configuration of this class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_packets(self, binary_packets):\n        if not binary_packets:\n            raise ValueError(\"Must supply at least one packet\")\n        parsed_packets = []\n\n        if not self._current_tshark:\n            self.eventloop.run_until_complete(self._get_tshark_process())\n        for binary_packet in binary_packets:\n            self._write_packet(binary_packet)\n\n        def callback(pkt):\n            parsed_packets.append(pkt)\n            if len(parsed_packets) == len(binary_packets):\n                raise StopCapture()\n\n        self.eventloop.run_until_complete(self._get_parsed_packet_from_tshark(callback))\n        return parsed_packets", "response": "Parses binary packets and returns a list of parsed packets."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef feed_packet(self, binary_packet, linktype=LinkTypes.ETHERNET):\n        warnings.warn(\"Deprecated method. Use InMemCapture.parse_packet() instead.\")\n        self._current_linktype = linktype\n        pkt = self.parse_packet(binary_packet)\n        self.close()\n        self._packets.append(pkt)\n        return pkt", "response": "Feeds a binary packet into the capture."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse a list of binary packets and returns the parsed packets.", "response": "def feed_packets(self, binary_packets, linktype=LinkTypes.ETHERNET):\n        \"\"\"\n        Gets a list of binary packets, parses them using tshark and returns their parsed values.\n        Keeps the packets in the internal packet list as well.\n\n        By default, assumes the packets are ethernet packets. For another link type, supply the linktype argument (most\n        can be found in the class LinkTypes)\n        \"\"\"\n        self._current_linktype = linktype\n        parsed_packets = self.parse_packets(binary_packets)\n        self._packets.extend(parsed_packets)\n        self.close()\n        return parsed_packets"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_process_path(tshark_path=None, process_name=\"tshark\"):\n    config = get_config()\n    possible_paths = [config.get(process_name, \"%s_path\" % process_name)]\n\n    # Add the user provided path to the search list\n    if tshark_path is not None:\n        possible_paths.insert(0, tshark_path)\n\n    # Windows search order: configuration file's path, common paths.\n    if sys.platform.startswith('win'):\n        for env in ('ProgramFiles(x86)', 'ProgramFiles'):\n            program_files = os.getenv(env)\n            if program_files is not None:\n                possible_paths.append(\n                    os.path.join(program_files, 'Wireshark', '%s.exe' % process_name)\n                )\n    # Linux, etc. search order: configuration file's path, the system's path\n    else:\n        os_path = os.getenv(\n            'PATH',\n            '/usr/bin:/usr/sbin:/usr/lib/tshark:/usr/local/bin'\n        )\n        for path in os_path.split(':'):\n            possible_paths.append(os.path.join(path, process_name))\n\n    for path in possible_paths:\n        if os.path.exists(path):\n            if sys.platform.startswith('win'):\n                path = path.replace(\"\\\\\", \"/\")\n            return path\n    raise TSharkNotFoundException(\n        'TShark not found. Try adding its location to the configuration file. '\n        'Searched these paths: {}'.format(possible_paths)\n    )", "response": "Returns the path of the tshark executable."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of interface numbers from the output tshark - D. Used internally to capture on multiple interfaces.", "response": "def get_tshark_interfaces(tshark_path=None):\n    \"\"\"\n    Returns a list of interface numbers from the output tshark -D. Used\n    internally to capture on multiple interfaces.\n    \"\"\"\n    parameters = [get_process_path(tshark_path), '-D']\n    with open(os.devnull, 'w') as null:\n        tshark_interfaces = subprocess.check_output(parameters, stderr=null).decode(\"utf-8\")\n\n    return [line.split('.')[0] for line in tshark_interfaces.splitlines()]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_parameters(self, packet_count=None):\n        params = super(PipeCapture, self).get_parameters(packet_count=packet_count)\n        params += ['-r', '-']\n        return params", "response": "Returns the special tshark parameters to be used according to the configuration of this class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_multiple_layers(self, layer_name):\n        return [layer for layer in self.layers if layer.layer_name.lower() == layer_name.lower()]", "response": "Returns a list of all the layers in the packet that are of the specified layer type."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef duplicate_object_hook(ordered_pairs):\n    json_dict = {}\n    for key, val in ordered_pairs:\n        existing_val = json_dict.get(key)\n        if not existing_val:\n            json_dict[key] = val\n        else:\n            if isinstance(existing_val, list):\n                existing_val.append(val)\n            else:\n                json_dict[key] = [existing_val, val]\n\n    return json_dict", "response": "Make lists out of duplicate keys."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef next(self):\n        if not self.keep_packets:\n            return self._packet_generator.send(None)\n        elif self._current_packet >= len(self._packets):\n            packet = self._packet_generator.send(None)\n            self._packets += [packet]\n        return super(FileCapture, self).next_packet()", "response": "Returns the next packet in the cap."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef packet_from_xml_packet(xml_pkt, psml_structure=None):\n    if not isinstance(xml_pkt, lxml.objectify.ObjectifiedElement):\n        parser = lxml.objectify.makeparser(huge_tree=True)\n        xml_pkt = lxml.objectify.fromstring(xml_pkt, parser)\n    if psml_structure:\n        return _packet_from_psml_packet(xml_pkt, psml_structure)\n    return _packet_from_pdml_packet(xml_pkt)", "response": "Gets a TShark XML packet object or string and returns a pyshark Packet object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_parameters(self, packet_count=None):\n        params = super(LiveRingCapture, self).get_parameters(packet_count=packet_count)\n        params += ['-b', 'filesize:' + str(self.ring_file_size), '-b', 'files:' + str(self.num_ring_files), '-w', self.ring_file_name, '-P']\n        return params", "response": "Returns the special tshark parameters to be used according to the configuration of this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_parameters(self, packet_count=None):\n        params = super(LiveCapture, self).get_parameters(packet_count=packet_count)\n        # Read from STDIN\n        params += ['-r', '-']\n        return params", "response": "Returns the special tshark parameters to be used according to the configuration of this class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the packets from the source and adds them to the internal list.", "response": "def load_packets(self, packet_count=0, timeout=None):\n        \"\"\"\n        Reads the packets from the source (cap, interface, etc.) and adds it to the internal list.\n        If 0 as the packet_count is given, reads forever\n\n        :param packet_count: The amount of packets to add to the packet list (0 to read forever)\n        :param timeout: If given, automatically stops after a given amount of time.\n        \"\"\"\n        initial_packet_amount = len(self._packets)\n\n        def keep_packet(pkt):\n            self._packets.append(pkt)\n\n            if packet_count != 0 and len(self._packets) - initial_packet_amount >= packet_count:\n                raise StopCapture()\n\n        try:\n            self.apply_on_packets(keep_packet, timeout=timeout)\n            self.loaded = True\n        except concurrent.futures.TimeoutError:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_debug(self, set_to=True):\n        if set_to:\n            StreamHandler(sys.stdout).push_application()\n            self._log.level = logbook.DEBUG\n        self.debug = set_to", "response": "Sets the capture to debug mode."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset up an eventloop as the current one according to the OS.", "response": "def _setup_eventloop(self):\n        \"\"\"\n        Sets up a new eventloop as the current one according to the OS.\n        \"\"\"\n        if os.name == 'nt':\n            self.eventloop = asyncio.ProactorEventLoop()\n        else:\n            self.eventloop = asyncio.new_event_loop()\n        asyncio.set_event_loop(self.eventloop)\n        if os.name == 'posix' and isinstance(threading.current_thread(), threading._MainThread):\n            asyncio.get_child_watcher().attach_loop(self.eventloop)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nextracting the tag from the given data.", "response": "def _extract_tag_from_data(self, data, tag_name=b'packet'):\n        \"\"\"Gets data containing a (part of) tshark xml.\n\n        If the given tag is found in it, returns the tag data and the remaining data.\n        Otherwise returns None and the same data.\n\n        :param data: string of a partial tshark xml.\n        :return: a tuple of (tag, data). tag will be None if none is found.\n        \"\"\"\n        opening_tag = b'<' + tag_name + b'>'\n        closing_tag = opening_tag.replace(b'<', b'</')\n        tag_end = data.find(closing_tag)\n        if tag_end != -1:\n            tag_end += len(closing_tag)\n            tag_start = data.find(opening_tag)\n            return data[tag_start:tag_end], data[tag_end:]\n        return None, data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _packets_from_tshark_sync(self, packet_count=None, existing_process=None):\n        # NOTE: This has code duplication with the async version, think about how to solve this\n        tshark_process = existing_process or self.eventloop.run_until_complete(self._get_tshark_process())\n        psml_structure, data = self.eventloop.run_until_complete(self._get_psml_struct(tshark_process.stdout))\n        packets_captured = 0\n\n        data = b''\n        try:\n            while True:\n                try:\n                    packet, data = self.eventloop.run_until_complete(\n                        self._get_packet_from_stream(tshark_process.stdout, data, psml_structure=psml_structure,\n                                                     got_first_packet=packets_captured > 0))\n\n                except EOFError:\n                    self._log.debug('EOF reached (sync)')\n                    break\n\n                if packet:\n                    packets_captured += 1\n                    yield packet\n                if packet_count and packets_captured >= packet_count:\n                    break\n        finally:\n            self.eventloop.run_until_complete(self._cleanup_subprocess(tshark_process))", "response": "Returns a generator of packets from the tshark process."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef apply_on_packets(self, callback, timeout=None, packet_count=None):\n        coro = self.packets_from_tshark(callback, packet_count=packet_count)\n        if timeout is not None:\n            coro = asyncio.wait_for(coro, timeout)\n        return self.eventloop.run_until_complete(coro)", "response": "Runs through all packets and calls the given callback with each one as it is read."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def packets_from_tshark(self, packet_callback, packet_count=None, close_tshark=True):\n        tshark_process = await self._get_tshark_process(packet_count=packet_count)\n        try:\n            await self._go_through_packets_from_fd(tshark_process.stdout, packet_callback, packet_count=packet_count)\n        except StopCapture:\n            pass\n        finally:\n            if close_tshark:\n                await self._close_async()", "response": "A coroutine which runs the given callback on each packet that is received from the tshark process and closes the process when it is done."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def _go_through_packets_from_fd(self, fd, packet_callback, packet_count=None):\n        packets_captured = 0\n        self._log.debug('Starting to go through packets')\n\n        psml_struct, data = await self._get_psml_struct(fd)\n\n        while True:\n            try:\n                packet, data = await self._get_packet_from_stream(fd, data, got_first_packet=packets_captured > 0,\n                                                                  psml_structure=psml_struct)\n            except EOFError:\n                self._log.debug('EOF reached')\n                break\n\n            if packet:\n                packets_captured += 1\n                try:\n                    packet_callback(packet)\n                except StopCapture:\n                    self._log.debug('User-initiated capture stop in callback')\n                    break\n\n            if packet_count and packets_captured >= packet_count:\n                break", "response": "A coroutine which goes through a stream and calls a given callback for each XML packet seen in it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread the PSML structure from the file and returns it.", "response": "async def _get_psml_struct(self, fd):\n        \"\"\"Gets the current PSML (packet summary xml) structure in a tuple ((None, leftover_data)),\n        only if the capture is configured to return it, else returns (None, leftover_data).\n\n        A coroutine.\n        \"\"\"\n        data = b''\n        psml_struct = None\n\n        if self._only_summaries:\n            # If summaries are read, we need the psdml structure which appears on top of the file.\n            while not psml_struct:\n                new_data = await fd.read(self.SUMMARIES_BATCH_SIZE)\n                data += new_data\n                psml_struct, data = self._extract_tag_from_data(data, b'structure')\n                if psml_struct:\n                    psml_struct = psml_structure_from_xml(psml_struct)\n                elif not new_data:\n                    return None, data\n            return psml_struct, data\n        else:\n            return None, data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _get_packet_from_stream(self, stream, existing_data, got_first_packet=True, psml_structure=None):\n        # yield each packet in existing_data\n        if self.use_json:\n            packet, existing_data = self._extract_packet_json_from_data(existing_data,\n                                                                        got_first_packet=got_first_packet)\n        else:\n            packet, existing_data = self._extract_tag_from_data(existing_data)\n\n        if packet:\n            if self.use_json:\n                packet = packet_from_json_packet(packet)\n            else:\n                packet = packet_from_xml_packet(packet, psml_structure=psml_structure)\n            return packet, existing_data\n\n        new_data = await stream.read(self.DEFAULT_BATCH_SIZE)\n        existing_data += new_data\n\n        if not new_data:\n            # Reached EOF\n            raise EOFError()\n        return None, existing_data", "response": "A coroutine which returns a single packet if it can be read from the given StreamReader."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def _get_tshark_process(self, packet_count=None, stdin=None):\n        if self.use_json:\n            output_type = 'json'\n            if not tshark_supports_json(self.tshark_path):\n                raise TSharkVersionException(\"JSON only supported on Wireshark >= 2.2.0\")\n        else:\n            output_type = 'psml' if self._only_summaries else 'pdml'\n        parameters = [self._get_tshark_path(), '-l', '-n', '-T', output_type] + \\\n            self.get_parameters(packet_count=packet_count)\n\n        self._log.debug('Creating TShark subprocess with parameters: ' + ' '.join(parameters))\n        self._log.debug('Executable: %s' % parameters[0])\n        tshark_process = await asyncio.create_subprocess_exec(*parameters,\n                                                              stdout=subprocess.PIPE,\n                                                              stderr=self._stderr_output(),\n                                                              stdin=stdin)\n        self._created_new_process(parameters, tshark_process)\n        return tshark_process", "response": "Returns a new tshark process with previously - set parameters."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def _cleanup_subprocess(self, process):\n        if process.returncode is None:\n            try:\n                process.kill()\n                return await asyncio.wait_for(process.wait(), 1)\n            except concurrent.futures.TimeoutError:\n                self._log.debug('Waiting for process to close failed, may have zombie process.')\n            except ProcessLookupError:\n                pass\n            except OSError:\n                if os.name != 'nt':\n                    raise\n        elif process.returncode > 0:\n            raise TSharkCrashException('TShark seems to have crashed (retcode: %d). '\n                                       'Try rerunning in debug mode [ capture_obj.set_debug() ] or try updating tshark.'\n                                       % process.returncode)", "response": "Clean up a process and all its pipes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parameters(self, packet_count=None):\n        params = []\n        if self._capture_filter:\n            params += ['-f', self._capture_filter]\n        if self._display_filter:\n            params += [get_tshark_display_filter_flag(self.tshark_path), self._display_filter]\n        # Raw is only enabled when JSON is also enabled.\n        if self.include_raw:\n            params += [\"-x\"]\n        if packet_count:\n            params += ['-c', str(packet_count)]\n        if self._custom_parameters:\n            for key, val in self._custom_parameters.items():\n                params += [key, val]\n        if all(self.encryption):\n            params += ['-o', 'wlan.enable_decryption:TRUE', '-o', 'uat:80211_keys:\"' + self.encryption[1] + '\",\"' +\n                                                                  self.encryption[0] + '\"']\n        if self._override_prefs:\n            for preference_name, preference_value in self._override_prefs.items():\n                if all(self.encryption) and preference_name in ('wlan.enable_decryption', 'uat:80211_keys'):\n                    continue  # skip if override preferences also given via --encryption options\n                params += ['-o', '{0}:{1}'.format(preference_name, preference_value)]\n\n        if self._output_file:\n            params += ['-w', self._output_file]\n\n        if self._decode_as:\n            for criterion, decode_as_proto in self._decode_as.items():\n                params += ['-d', ','.join([criterion.strip(), decode_as_proto.strip()])]\n\n        if self._disable_protocol:\n            params += ['--disable-protocol', self._disable_protocol.strip()]\n\n        return params", "response": "Returns the special tshark parameters to be used according to the configuration of this class."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_default_value(self):\n        val = self.show\n        if not val:\n            val = self.raw_value\n        if not val:\n            val = self.showname\n        return val", "response": "Gets the default value for the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef binary_value(self):\n        str_raw_value = str(self.raw_value)\n        if len(str_raw_value) % 2 == 1:\n            str_raw_value = '0' + str_raw_value\n\n        return binascii.unhexlify(str_raw_value)", "response": "Converts this field to binary"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef callgraph(G, stmt_list):\n    func_list = []\n    for stmt in stmt_list:\n        try:\n            G.add_node(stmt.head.ident.name)\n            func_list.append(stmt)\n        except:\n            pass\n    for func in func_list:\n        assert isinstance(func,node.function)\n        func_name = func.head.ident.name\n        #resolve.resolve(func)\n        for s in node.postorder(func):\n            if (s.__class__ is node.funcall and\n                s.func_expr.__class__ is  node.ident):\n                #if s.func_expr.name in G.nodes():\n                    G.add_edge(func_name,s.func_expr.name)", "response": "Build a callgraph of stmt_list ignoring any built - in functions"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef recordtype(typename, field_names, verbose=False, **default_kwds):\n    '''Returns a new class with named fields.\n\n    @keyword field_defaults: A mapping from (a subset of) field names to default \n        values.\n    @keyword default: If provided, the default value for all fields without an\n        explicit default in `field_defaults`.\n\n    >>> Point = recordtype('Point', 'x y', default=0)\n    >>> Point.__doc__           # docstring for the new class\n    'Point(x, y)'\n    >>> Point()                 # instantiate with defaults\n    Point(x=0, y=0)\n    >>> p = Point(11, y=22)     # instantiate with positional args or keywords\n    >>> p[0] + p.y              # accessible by name and index\n    33\n    >>> p.x = 100; p[1] =200    # modifiable by name and index\n    >>> p\n    Point(x=100, y=200)\n    >>> x, y = p               # unpack\n    >>> x, y\n    (100, 200)\n    >>> d = p.todict()         # convert to a dictionary\n    >>> d['x']\n    100\n    >>> Point(**d) == p        # convert from a dictionary\n    True\n    '''\n    # Parse and validate the field names.  Validation serves two purposes,\n    # generating informative error messages and preventing template injection attacks.\n    if isinstance(field_names, str):\n        # names separated by whitespace and/or commas\n        field_names = field_names.replace(',', ' ').split()\n    field_names = tuple(map(str, field_names))\n    if not field_names:\n        raise ValueError('Records must have at least one field')\n    for name in (typename,) + field_names:\n        if not min(c.isalnum() or c=='_' for c in name):\n            raise ValueError('Type names and field names can only contain '\n                             'alphanumeric characters and underscores: %r' % name)\n        if iskeyword(name):\n            raise ValueError('Type names and field names cannot be a keyword: %r'\n                             % name)\n        if name[0].isdigit():\n            raise ValueError('Type names and field names cannot start with a '\n                             'number: %r' % name)\n    seen_names = set()\n    for name in field_names:\n        if name.startswith('_'):\n            raise ValueError('Field names cannot start with an underscore: %r'\n                             % name)\n        if name in seen_names:\n            raise ValueError('Encountered duplicate field name: %r' % name)\n        seen_names.add(name)\n    # determine the func_defaults of __init__\n    field_defaults = default_kwds.pop('field_defaults', {})\n    if 'default' in default_kwds:\n        default = default_kwds.pop('default')\n        init_defaults = tuple(field_defaults.get(f,default) for f in field_names)\n    elif not field_defaults:\n        init_defaults = None\n    else:\n        default_fields = field_names[-len(field_defaults):]\n        if set(default_fields) != set(field_defaults):\n            raise ValueError('Missing default parameter values')\n        init_defaults = tuple(field_defaults[f] for f in default_fields)\n    if default_kwds:\n        raise ValueError('Invalid keyword arguments: %s' % default_kwds)\n    # Create and fill-in the class template\n    numfields = len(field_names)\n    argtxt = ', '.join(field_names)\n    reprtxt = ', '.join('%s=%%r' % f for f in field_names)\n    dicttxt = ', '.join('%r: self.%s' % (f,f) for f in field_names)\n    tupletxt = repr(tuple('self.%s' % f for f in field_names)).replace(\"'\",'')\n    inittxt = '; '.join('self.%s=%s' % (f,f) for f in field_names)\n    itertxt = '; '.join('yield self.%s' % f for f in field_names)\n    eqtxt   = ' and '.join('self.%s==other.%s' % (f,f) for f in field_names)\n    template = dedent('''\n        class %(typename)s(object):\n            '%(typename)s(%(argtxt)s)'\n\n            __slots__  = %(field_names)r\n\n            def __init__(self, %(argtxt)s):\n                %(inittxt)s\n\n            def __len__(self):\n                return %(numfields)d\n\n            def __iter__(self):\n                %(itertxt)s\n\n            def __getitem__(self, index):\n                return getattr(self, self.__slots__[index])\n\n            def __setitem__(self, index, value):\n                return setattr(self, self.__slots__[index], value)\n\n            def todict(self):\n                'Return a new dict which maps field names to their values'\n                return {%(dicttxt)s}\n\n            def __repr__(self):\n                return '%(typename)s(%(reprtxt)s)' %% %(tupletxt)s\n\n            def __eq__(self, other):\n                return isinstance(other, self.__class__) and %(eqtxt)s\n\n            def __ne__(self, other):\n                return not self==other\n\n            def __getstate__(self):\n                return %(tupletxt)s\n\n            def __setstate__(self, state):\n                %(tupletxt)s = state\n    ''') % locals()\n    # Execute the template string in a temporary namespace\n    namespace = {}\n    try:\n        exec(template, namespace)\n        if verbose: print(template)\n    except SyntaxError as e:\n        raise SyntaxError(e.message + ':\\n' + template)\n    cls = namespace[typename]\n    if sys.version_info.major == 3:\n        cls.__init__.__defaults__ = init_defaults\n    elif sys.version_info.major == 2:\n        cls.__init__.im_func.func_defaults = init_defaults\n    else:\n        #import pdb\n        #pdb.set_trace()\n        assert 0\n    # For pickling to work, the __module__ variable needs to be set to the frame\n    # where the named tuple is created.  Bypass this step in enviroments where\n    # sys._getframe is not defined (Jython for example).\n    if hasattr(sys, '_getframe') and sys.platform != 'cli':\n        cls.__module__ = sys._getframe(1).f_globals['__name__']\n    return cls", "response": "Returns a new class with named fields."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if a is a vector or scalar.", "response": "def isvector_or_scalar(a):\n    \"\"\"\n    one-dimensional arrays having shape [N],\n    row and column matrices having shape [1 N] and\n    [N 1] correspondingly, and their generalizations\n    having shape [1 1 ... N ... 1 1 1].\n    Scalars have shape [1 1 ... 1].\n    Empty arrays dont count\n    \"\"\"\n    try:\n        return a.size and a.ndim-a.shape.count(1) <= 1\n    except:\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef arange(start,stop,step=1,**kwargs):\n    expand_value = 1 if step > 0 else -1\n    return matlabarray(np.arange(start,\n                                 stop+expand_value,\n                                 step,\n                                 **kwargs).reshape(1,-1),**kwargs)", "response": "A matlab array of integers."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef size(a, b=0, nargout=1):\n    s = np.asarray(a).shape\n    if s is ():\n        return 1 if b else (1,)*nargout\n    # a is not a scalar\n    try:\n        if b:\n            return s[b-1]\n        else:\n            return matlabarray(s) if nargout <= 1 else s\n    except IndexError:\n        return 1", "response": "Return the size of the nargout element of a sequence of nargout elements."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_top(p):\n    if len(p) == 1:\n        p[0] = node.stmt_list()\n    else:\n        p[0] = p[1]\n        p[0].append(p[2])", "response": "top :\n        | top stmt"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_end_function(p):\n    p[0] = p[1]\n    p[0].append(node.return_stmt(ret=ret_expr))\n    p[0].append(node.comment_stmt(\"\\nif __name__ == '__main__':\\n    pass\"))", "response": "END_FUNCTION p [ 0 ] = top END_FUNCTION"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_arg1(p):\n    # a hack to support \"clear global\"\n    p[0] = node.string(value=str(p[1]), lineno=p.lineno(1), lexpos=p.lexpos(1))", "response": "arg1 : STRING\n         | NUMBER\n         | IDENT\n         | GLOBAL"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_case_list(p):\n    if len(p) == 1:\n        p[0] = node.stmt_list()\n    elif len(p) == 3:\n        assert isinstance(p[2], node.stmt_list)\n        p[0] = p[2]\n    elif len(p) == 6:\n        p[0] = node.if_stmt(\n            cond_expr=node.expr(\n                op=\"==\", args=node.expr_list([p[2]])),\n            then_stmt=p[4],\n            else_stmt=p[5])\n        p[0].cond_expr.args.append(\n            None)  # None will be replaced using backpatch()\n    else:\n        assert 0", "response": "CASE expr sep stmt_list_opt case_list\n | OTHERWISE stmt_list_opt case_list\n | CASE expr error stmt_list stmt_list_opt case_list stmt_list stmt_list_opt case_list\nSegement stmt_list stmt_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_cellarrayref(p):\n    args = node.expr_list() if len(p) == 4 else p[3]\n    assert isinstance(args, node.expr_list)\n    p[0] = node.cellarrayref(func_expr=p[1], args=args)", "response": "expr = expr_list RBRACE\n            = expr expr_list RBRACE"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_concat_list(p):\n    if p[1].__class__ == node.expr_list:\n        p[0] = node.concat_list([p[1], p[3]])\n    else:\n        p[0] = p[1]\n        p[0].append(p[3])", "response": "concat_list is a helper function to add a new entry to the list"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef p_elseif_stmt(p):\n    if len(p) == 1:\n        p[0] = node.stmt_list()\n    elif len(p) == 3:\n        p[0] = p[2]\n    elif len(p) == 6:\n        p[0] = node.if_stmt(cond_expr=p[2], then_stmt=p[4], else_stmt=p[5])\n    elif len(p) == 7:\n        p[0] = node.if_stmt(cond_expr=p[3], then_stmt=p[5], else_stmt=p[6])\n    else:\n        assert 0", "response": "parse an else - if - block"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef p_expr1(p):\n    p[0] = node.expr(op=p[1], args=node.expr_list([p[2]]))", "response": "expr1 : MINUS expr %prec UMINUS\n             | PLUS expr %prec UMINUS\n             | NEG expr\n             | HANDLE ident\n             | PLUSPLUS ident\n             | MINUSMINUS ident"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_expr2(p):\n    if p[2] == \"=\":\n        if p[1].__class__ is node.let:\n            raise_exception(SyntaxError,\n                            \"Not implemented assignment as expression\",\n                            new_lexer)\n        # The algorithm, which decides if an\n        # expression F(X)\n        # is arrayref or funcall, is implemented in\n        # resolve.py, except the following lines up\n        # to XXX. These lines handle the case where\n        # an undefined array is updated:\n        #    >>> clear a\n        #    >>> a[1:10]=123\n        # Though legal in matlab, these lines\n        # confuse the algorithm, which thinks that\n        # the undefined variable is a function name.\n        # To prevent the confusion, we mark these\n        # nodes arrayref as early as during the parse\n        # phase.\n        if p[1].__class__ is node.funcall:\n            # A(B) = C\n            p[1].__class__ = node.arrayref\n        elif p[1].__class__ is node.matrix:\n            # [A1(B1) A2(B2) ...] = C\n            for e in p[1].args:\n                if e.__class__ is node.funcall:\n                    e.__class__ = node.arrayref\n        # XXX\n\n        if isinstance(p[1], node.getfield):\n            # import pdb;pdb.set_trace()\n            # A.B=C  setfield(A,B,C)\n            p[0] = node.setfield(p[1].args[0], p[1].args[1], p[3])\n        else:\n            # assert len(p[1].args) > 0\n            ret = p[1].args if isinstance(p[1], node.matrix) else p[1]\n            p[0] = node.let(ret=ret,\n                            args=p[3],\n                            lineno=p.lineno(2),\n                            lexpos=p.lexpos(2))\n\n            if isinstance(p[1], node.matrix):\n                # TBD: mark idents as \"P\" - persistent\n                if p[3].__class__ not in (node.ident, node.funcall\n                                          ):  #, p[3].__class__\n                    raise_exception(SyntaxError,\n                                    \"multi-assignment\",\n                                    new_lexer)\n                if p[3].__class__ is node.ident:\n                    # [A1(B1) A2(B2) ...] = F     implied F()\n                    # import pdb; pdb.set_trace()\n                    p[3] = node.funcall(func_expr=p[3], args=node.expr_list())\n                # [A1(B1) A2(B2) ...] = F(X)\n                p[3].nargout = len(p[1].args[0])\n    elif p[2] == \"*\":\n        p[0] = node.funcall(\n            func_expr=node.ident(\"dot\"), args=node.expr_list([p[1], p[3]]))\n    elif p[2] == \".*\":\n        p[0] = node.funcall(\n            func_expr=node.ident(\"multiply\"),\n            args=node.expr_list([p[1], p[3]]))\n\n#    elif p[2] == \".\" and isinstance(p[3],node.expr) and p[3].op==\"parens\":\n#        p[0] = node.getfield(p[1],p[3].args[0])\n#        raise SyntaxError(p[3],p.lineno(3),p.lexpos(3))\n    elif p[2] == \":\" and isinstance(p[1], node.expr) and p[1].op == \":\":\n        # Colon expression means different things depending on the\n        # context.  As an array subscript, it is a slice; otherwise,\n        # it is a call to the \"range\" function, and the parser can't\n        # tell which is which.  So understanding of colon expressions\n        # is put off until after \"resolve\".\n        p[0] = p[1]\n        p[0].args.insert(1, p[3])\n    else:\n        p[0] = node.expr(op=p[2], args=node.expr_list([p[1], p[3]]))", "response": "Parse expression 2 - node expression."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_expr_stmt(p):\n    assert isinstance(p[1], node.expr_list)\n    p[0] = node.expr_stmt(expr=p[1])", "response": "A function to handle expr_list."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets string : STRING", "response": "def p_expr_string(p):\n    \"string : STRING\"\n    p[0] = node.string(p[1], lineno=p.lineno(1), lexpos=p.lexpos(1))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef p_exprs(p):\n    if len(p) == 2:\n        p[0] = node.expr_list([p[1]])\n    elif len(p) == 4:\n        p[0] = p[1]\n        p[0].append(p[3])\n    else:\n        assert (0)\n    assert isinstance(p[0], node.expr_list)", "response": "exprs : expr\n          | exprs COMMA expr"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_field_expr(p):\n    p[0] = node.expr(\n        op=\".\",\n        args=node.expr_list([\n            p[1], node.ident(\n                name=p[2], lineno=p.lineno(2), lexpos=p.lexpos(2))\n        ]))", "response": "A field expression is an expression that is a sequence of two elements."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef p_foo_stmt(p):\n    \"foo_stmt : expr OROR expr SEMI\"\n    expr1 = p[1][1][0]\n    expr2 = p[3][1][0]\n    ident = expr1.ret\n    args1 = expr1.args\n    args2 = expr2.args\n    p[0] = node.let(ret=ident,\n                    args=node.expr(\"or\", node.expr_list([args1, args2])))", "response": "A helper function for finding the next non - empty expression in the sequence."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_for_stmt(p):\n    if len(p) == 8:\n        if not isinstance(p[2], node.ident):\n            raise_exception(SyntaxError, \"Not implemented: for loop\", new_lexer)\n        p[2].props = \"I\"  # I= for-loop iteration variable\n        p[0] = node.for_stmt(ident=p[2], expr=p[4], stmt_list=p[6])", "response": "for_stmt : FOR ident EQ expr SEMI stmt_list END_STMT\n             | FOR LPAREN ident EQ expr RPAREN SEMI stmt_list END_STMT\n             | FOR matrix EQ expr SEMI stmt_list END_STMT"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_global_list(p):\n    if len(p) == 2:\n        p[0] = node.global_list([p[1]])\n    elif len(p) == 3:\n        p[0] = p[1]\n        p[0].append(p[2])", "response": "A function to parse the global_list parameter."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef p_if_stmt(p):\n    if len(p) == 7:\n        p[0] = node.if_stmt(cond_expr=p[2], then_stmt=p[4], else_stmt=p[5])\n    elif len(p) == 8:\n        p[0] = node.if_stmt(cond_expr=p[3], then_stmt=p[5], else_stmt=p[6])\n    else:\n        assert 0", "response": "P 6. 4. 6. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 1. 2. 2"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_ret(p):\n    if len(p) == 2:\n        p[0] = node.expr_list([p[1]])\n    elif len(p) == 3:\n        p[0] = node.expr_list([])\n    elif len(p) == 4:\n        assert isinstance(p[2], node.expr_list)\n        p[0] = p[2]\n    else:\n        assert 0\n    for ident in p[0]:\n        ident.props = \"F\"", "response": "parse the tree structure"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_switch_stmt(p):\n\n    def backpatch(expr, stmt):\n        if isinstance(stmt, node.if_stmt):\n            stmt.cond_expr.args[1] = expr\n            backpatch(expr, stmt.else_stmt)\n\n    backpatch(p[2], p[4])\n    p[0] = p[4]", "response": "switch_stmt : SWITCH expr semi_opt case_list END_STMT"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef p_try_catch(p):\n    ## | TRY stmt_list END_STMT\n    assert isinstance(p[2], node.stmt_list)\n    # assert isinstance(p[4],node.stmt_list)\n    p[0] = node.try_catch(\n        try_stmt=p[2],\n        catch_stmt=p[4],\n        finally_stmt=node.stmt_list())", "response": "A try catch statement is a special case for the try_catch statement."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef p_unwind(p):\n    p[0] = node.try_catch(\n        try_stmt=p[2], catch_stmt=node.expr_list(), finally_stmt=p[4])", "response": "unwind : UNWIND_PROTECT stmt_list UNWIND_PROTECT_CLEANUP stmt_list END_UNWIND_PROTECT"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_arrayref(u):\n    if u.__class__ is node.funcall:\n        try:\n            if u.func_expr.props in \"UR\": # upd,ref\n                u.__class__ = node.arrayref\n        except:\n            pass", "response": "Convert an arrayref node to a node."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef colon_subscripts(u):\n    if u.__class__ in (node.arrayref,node.cellarrayref):\n        for w in u.args:\n            if w.__class__ is node.expr and w.op == \":\":\n                w._replace(op=\"::\")", "response": "Look for colon subscripts foo 1 : 10 and colon expressions foo 1 : 10 look for the same thing."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef let_statement(u):\n    if u.__class__ is node.let:\n        if (u.ret.__class__ is node.ident and\n            u.args.__class__ is node.matrix):\n            u.args = node.funcall(func_expr=node.ident(\"matlabarray\"),\n                                  args=node.expr_list([u.args]))", "response": "A let statement is a special case where the LHS is a plain variable and the RHS is a matrix containing a matrix of array elements."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_win_streak(data):\n    #only do this if there are non-NANs in the column\n    if data['Streak'].count()>0:\n        data['Streak2'] = data['Streak'].str.len()\n        data.loc[data['Streak'].str[0]=='-','Streak2'] = -data['Streak2']\n        data['Streak'] = data['Streak2']\n        data = data.drop('Streak2',1)\n    return data", "response": "Convert win / loss streak column into a + / - integer column"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_request(start_dt, end_dt, player_id, url):\n    current_dt = datetime.datetime.strptime(start_dt, '%Y-%m-%d')\n    end_dt = datetime.datetime.strptime(end_dt, '%Y-%m-%d')\n    results = []  # list to hold data as it is returned\n    player_id = str(player_id)\n    print('Gathering Player Data')\n    # break query into multiple requests\n    while current_dt < end_dt:\n        remaining = end_dt - current_dt\n        # increment date ranges by at most 60 days\n        delta = min(remaining, datetime.timedelta(days=2190))\n        next_dt = current_dt + delta\n        start_str = current_dt.strftime('%Y-%m-%d')\n        end_str = next_dt.strftime('%Y-%m-%d')\n        # retrieve data\n        data = requests.get(url.format(start_str, end_str, player_id))\n        df = pd.read_csv(io.StringIO(data.text))\n        # add data to list and increment current dates\n        results.append(df)\n        current_dt = next_dt + datetime.timedelta(days=1)\n    return pd.concat(results)", "response": "Splits Statcast queries to avoid request timeouts"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a zip file from a URL", "response": "def get_zip_file(url):\n    \"\"\"\n    Get zip file from provided URL\n    \"\"\"\n    with requests.get(url, stream=True) as f:\n        z = zipfile.ZipFile(io.BytesIO(f.content))\n    return z"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef statcast_pitcher(start_dt=None, end_dt=None, player_id=None):\n    start_dt, end_dt, player_id = sanitize_input(start_dt, end_dt, player_id)\n\n    # inputs are valid if either both or zero dates are supplied. Not valid of only one given.\n    if start_dt and end_dt:\n        url = 'https://baseballsavant.mlb.com/statcast_search/csv?all=true&hfPT=&hfAB=&hfBBT=&hfPR=&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7CPO%7CS%7C=&hfSea=&hfSit=&player_type=pitcher&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=&game_date_gt={}&game_date_lt={}&pitchers_lookup%5B%5D={}&team=&position=&hfRO=&home_road=&hfFlag=&metric_1=&hfInn=&min_pitches=0&min_results=0&group_by=name&sort_col=pitches&player_event_sort=h_launch_speed&sort_order=desc&min_abs=0&type=details&'\n        df = split_request(start_dt, end_dt, player_id, url)\n        return df", "response": "Pulls statcast pitch - level data from Baseball Savant for a given pitcher."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting all batting stats for a given time range.", "response": "def batting_stats_range(start_dt=None, end_dt=None):\n    \"\"\"\n    Get all batting stats for a set time range. This can be the past week, the\n    month of August, anything. Just supply the start and end date in YYYY-MM-DD\n    format.\n    \"\"\"\n    # make sure date inputs are valid\n    start_dt, end_dt = sanitize_input(start_dt, end_dt)\n    if datetime.datetime.strptime(start_dt, \"%Y-%m-%d\").year < 2008:\n        raise ValueError(\"Year must be 2008 or later\")\n    if datetime.datetime.strptime(end_dt, \"%Y-%m-%d\").year < 2008:\n        raise ValueError(\"Year must be 2008 or later\")\n    # retrieve html from baseball reference\n    soup = get_soup(start_dt, end_dt)\n    table = get_table(soup)\n    table = table.dropna(how='all')  # drop if all columns are NA\n    # scraped data is initially in string format.\n    # convert the necessary columns to numeric.\n    for column in ['Age', '#days', 'G', 'PA', 'AB', 'R', 'H', '2B', '3B',\n                    'HR', 'RBI', 'BB', 'IBB', 'SO', 'HBP', 'SH', 'SF', 'GDP',\n                    'SB', 'CS', 'BA', 'OBP', 'SLG', 'OPS']:\n        #table[column] = table[column].astype('float')\n        table[column] = pd.to_numeric(table[column])\n        #table['column'] = table['column'].convert_objects(convert_numeric=True)\n    table = table.drop('', 1)\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all batting stats for a given season.", "response": "def batting_stats_bref(season=None):\n    \"\"\"\n    Get all batting stats for a set season. If no argument is supplied, gives\n    stats for current season to date.\n    \"\"\"\n    if season is None:\n        season = datetime.datetime.today().strftime(\"%Y\")\n    season = str(season)\n    start_dt = season + '-03-01' #opening day is always late march or early april\n    end_dt = season + '-11-01' #season is definitely over by November\n    return(batting_stats_range(start_dt, end_dt))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\npulling Retrosheet game logs for a given season", "response": "def season_game_logs(season):\n    \"\"\"\n    Pull Retrosheet game logs for a given season\n    \"\"\"\n    # validate input\n    max_year = int(datetime.now().year) - 1\n    if season > max_year or season < 1871:\n        raise ValueError('Season must be between 1871 and {}'.format(max_year))\n    file_name = 'GL{}.TXT'.format(season)\n    z = get_zip_file(gamelog_url.format(season))\n    data = pd.read_csv(z.open(file_name), header=None, sep=',', quotechar='\"')\n    data.columns = gamelog_columns\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npull Retrosheet All Star Game Logs", "response": "def all_star_game_logs():\n    \"\"\"\n    Pull Retrosheet All Star Game Logs\n    \"\"\"\n    file_name = 'GLAS.TXT'\n    z = get_zip_file(all_star_url)\n    data = pd.read_csv(z.open(file_name), header=None, sep=',', quotechar='\"')\n    data.columns = gamelog_columns\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef lcs_logs():\n    file_name = 'GLLC.TXT'\n    z = get_zip_file(lcs_url)\n    data = pd.read_csv(z.open(file_name), header=None, sep=',', quotechar='\"')\n    data.columns = gamelog_columns\n    return data", "response": "Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n    Pull Retrosheet LCS Game Logs\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef team_pitching(start_season, end_season=None, league='all', ind=1):\n    if start_season is None:\n        raise ValueError(\"You need to provide at least one season to collect data for. Try team_pitching(season) or team_pitching(start_season, end_season).\")\n    if end_season is None:\n        end_season = start_season\n    soup = get_soup(start_season=start_season, end_season=end_season, league=league, ind=ind)\n    table = get_table(soup, ind)\n    table = postprocessing(table)\n    return table", "response": "Get season - level pitching data aggregated by team."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef team_pitching_bref(team, start_season, end_season=None):\n    if start_season is None:\n        raise ValueError(\"You need to provide at least one season to collect data for. Try team_pitching_bref(season) or team_pitching_bref(start_season, end_season).\")\n    if end_season is None:\n        end_season = start_season\n\n    url = \"https://www.baseball-reference.com/teams/{}\".format(team)\n\n    data = []\n    headings = None\n    for season in range(start_season, end_season+1):\n        print(\"Getting Pitching Data: {} {}\".format(season, team))\n        stats_url = \"{}/{}.shtml\".format(url, season)\n        response = requests.get(stats_url)\n        soup = BeautifulSoup(response.content, 'html.parser')\n\n        table = soup.find_all('table', {'id': 'team_pitching'})[0]\n\n        if headings is None:\n            headings = [row.text.strip() for row in table.find_all('th')[1:34]]\n\n        rows = table.find_all('tr')\n        for row in rows:\n            cols = row.find_all('td')\n            cols = [ele.text.strip() for ele in cols]\n            cols = [col.replace('*', '').replace('#', '') for col in cols]  # Removes '*' and '#' from some names\n            cols = [col for col in cols if 'Totals' not in col and 'NL teams' not in col and 'AL teams' not in col]  # Removes Team Totals and other rows\n            cols.insert(2, season)\n            data.append([ele for ele in cols[0:]])\n\n    headings.insert(2, \"Year\")\n    data = pd.DataFrame(data=data, columns=headings) # [:-5]  # -5 to remove Team Totals and other rows (didn't work in multi-year queries)\n    data = data.dropna()  # Removes Row of All Nones\n\n    return data", "response": "Get season - level pitching statistics for specific team."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npulling statcast pitch - level data from Baseball Savant for a given batter.", "response": "def statcast_batter(start_dt=None, end_dt=None, player_id=None):\n    \"\"\"\n    Pulls statcast pitch-level data from Baseball Savant for a given batter.\n\n    ARGUMENTS\n    start_dt : YYYY-MM-DD : the first date for which you want a player's statcast data\n    end_dt : YYYY-MM-DD : the final date for which you want data\n    player_id : INT : the player's MLBAM ID. Find this by calling pybaseball.playerid_lookup(last_name, first_name), finding the correct player, and selecting their key_mlbam.\n    \"\"\"\n    start_dt, end_dt, player_id = sanitize_input(start_dt, end_dt, player_id)\n    # inputs are valid if either both or zero dates are supplied. Not valid of only one given.\n    if start_dt and end_dt:\n        url = 'https://baseballsavant.mlb.com/statcast_search/csv?all=true&hfPT=&hfAB=&hfBBT=&hfPR=&hfZ=&stadium=&hfBBL=&hfNewZones=&hfGT=R%7CPO%7CS%7C=&hfSea=&hfSit=&player_type=batter&hfOuts=&opponent=&pitcher_throws=&batter_stands=&hfSA=&game_date_gt={}&game_date_lt={}&batters_lookup%5B%5D={}&team=&position=&hfRO=&home_road=&hfFlag=&metric_1=&hfInn=&min_pitches=0&min_results=0&group_by=name&sort_col=pitches&player_event_sort=h_launch_speed&sort_order=desc&min_abs=0&type=details&'\n        df = split_request(start_dt, end_dt, player_id, url)\n        return df"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef playerid_reverse_lookup(player_ids, key_type=None):\n    key_types = ('mlbam', 'retro', 'bbref', 'fangraphs', )\n\n    if not key_type:\n        key_type = key_types[0]     # default is \"mlbam\" if key_type not provided\n    elif key_type not in key_types:\n        raise ValueError(\n            '[Key Type: {}] Invalid; Key Type must be one of \"{}\"'.format(key_type, '\", \"'.join(key_types))\n        )\n\n    table = get_lookup_table()\n    key = 'key_{}'.format(key_type)\n\n    results = table[table[key].isin(player_ids)]\n    results = results.reset_index().drop('index', 1)\n    return results", "response": "Retrieve a table of player information given a list of player ids"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget season - level pitching data from FanGraphs.", "response": "def pitching_stats(start_season, end_season=None, league='all', qual=1, ind=1):\n    \"\"\"\n    Get season-level pitching data from FanGraphs. \n\n    ARGUMENTS:\n    start_season : int : first season you want data for (or the only season if you do not specify an end_season)\n    end_season : int : final season you want data for \n    league : \"all\", \"nl\", or \"al\"\n    qual: minimum number of pitches thrown to be included in the data (integer). Use the string 'y' for fangraphs default.\n    ind : int : =1 if you want individual season-level data, =0 if you want a player's aggreagate data over all seasons in the query\n    \"\"\"\n    if start_season is None:\n        raise ValueError(\"You need to provide at least one season to collect data for. Try pitching_leaders(season) or pitching_leaders(start_season, end_season).\")\n    if end_season is None:\n        end_season = start_season\n    soup = get_soup(start_season=start_season, end_season=end_season, league=league, qual=qual, ind=ind)\n    table = get_table(soup, ind)\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef large_request(start_dt,end_dt,d1,d2,step,verbose):\n    error_counter = 0 # count failed requests. If > X, break\n    no_success_msg_flag = False # a flag for passing over the success message of requests are failing\n    print(\"This is a large query, it may take a moment to complete\")\n    dataframe_list = []\n    #step = 3 # number of days per mini-query (test this later to see how large I can make this without losing data)\n    d = d1 + datetime.timedelta(days=step)\n    while d <= d2: #while intermediate query end_dt <= global query end_dt, keep looping\n        # dates before 3/15 and after 11/15 will always be offseason\n        # if these dates are detected, check if the next season is within the user's query\n        # if yes, fast-forward to the next season to avoid empty requests\n        # if no, break the loop. all useful data has been pulled.\n        if ((d.month < 4 and d.day < 15) or (d1.month > 10 and d1.day > 14)):\n            if d2.year > d.year:\n                print('Skipping offseason dates')\n                d1 = d1.replace(month=3,day=15,year=d1.year+1)\n                d = d1 + datetime.timedelta(days=step+1)\n            else:\n                break\n\n        start_dt = d1.strftime('%Y-%m-%d')\n        intermediate_end_dt = d.strftime('%Y-%m-%d')\n        data = small_request(start_dt,intermediate_end_dt)\n        # append to list of dataframes if not empty or failed (failed requests have one row saying \"Error: Query Timeout\")\n        if data.shape[0] > 1:\n            dataframe_list.append(data)\n        # if it failed, retry up to three times\n        else:\n            success = 0\n            while success == 0:\n                data = small_request(start_dt,intermediate_end_dt)\n                if data.shape[0] > 1:\n                    dataframe_list.append(data)\n                    success = 1\n                else:\n                    error_counter += 1\n                if error_counter > 2:\n                    # this request is probably too large. Cut a day off of this request and make that its own separate request.\n                    # For each, append to dataframe list if successful, skip and print error message if failed\n                    tmp_end = d - datetime.timedelta(days=1)\n                    tmp_end = tmp_end.strftime('%Y-%m-%d')\n                    smaller_data_1 = small_request(start_dt, tmp_end)\n                    smaller_data_2 = small_request(intermediate_end_dt,intermediate_end_dt)\n                    if smaller_data_1.shape[0] > 1:\n                        dataframe_list.append(smaller_data_1)\n                        print(\"Completed sub-query from {} to {}\".format(start_dt,tmp_end))\n                    else:\n                        print(\"Query unsuccessful for data from {} to {}. Skipping these dates.\".format(start_dt,tmp_end))\n                    if smaller_data_2.shape[0] > 1:\n                        dataframe_list.append(smaller_data_2)\n                        print(\"Completed sub-query from {} to {}\".format(intermediate_end_dt,intermediate_end_dt))\n                    else:\n                        print(\"Query unsuccessful for data from {} to {}. Skipping these dates.\".format(intermediate_end_dt,intermediate_end_dt))\n\n                    no_success_msg_flag = True # flag for passing over the success message since this request failed\n                    error_counter = 0 # reset counter\n                    break\n\n\n        if verbose:\n            if no_success_msg_flag is False:\n                print(\"Completed sub-query from {} to {}\".format(start_dt,intermediate_end_dt))\n            else:\n                no_success_msg_flag = False # if failed, reset this flag so message will send again next iteration\n        # increment dates\n        d1 = d + datetime.timedelta(days=1)\n        d = d + datetime.timedelta(days=step+1)\n\n    # if start date > end date after being incremented, the loop captured each date's data\n    if d1 > d2:\n        pass\n    # if start date <= end date, then there are a few leftover dates to grab data for.\n    else:\n        # start_dt from the earlier loop will work, but instead of d we now want the original end_dt\n        start_dt = d1.strftime('%Y-%m-%d')\n        data = small_request(start_dt,end_dt)\n        dataframe_list.append(data)\n        if verbose:\n            print(\"Completed sub-query from {} to {}\".format(start_dt,end_dt))\n\n    # concatenate all dataframes into final result set\n    final_data = pd.concat(dataframe_list, axis=0)\n    return final_data", "response": "This function is used to make large queries for a single site."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef statcast(start_dt=None, end_dt=None, team=None, verbose=True):\n\n\n    start_dt, end_dt = sanitize_input(start_dt, end_dt)\n    # 3 days or less -> a quick one-shot request. Greater than 3 days -> break it into multiple smaller queries\n    small_query_threshold = 5\n    # inputs are valid if either both or zero dates are supplied. Not valid of only one given.\n\n\n    if start_dt and end_dt:\n        # how many days worth of data are needed?\n        date_format = \"%Y-%m-%d\"\n        d1 = datetime.datetime.strptime(start_dt, date_format)\n        d2 = datetime.datetime.strptime(end_dt, date_format)\n        days_in_query = (d2 - d1).days\n        if days_in_query <= small_query_threshold:\n            data = small_request(start_dt,end_dt)\n        else:\n            data = large_request(start_dt,end_dt,d1,d2,step=small_query_threshold,verbose=verbose)\n\n        data = postprocessing(data, team)\n        return data", "response": "Pulls statcast play - level data from Baseball Savant for a given date range."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves statcast play - level data for a single game.", "response": "def statcast_single_game(game_pk, team=None):\n    \"\"\"\n    Pulls statcast play-level data from Baseball Savant for a single game,\n    identified by its MLB game ID (game_pk in statcast data)\n\n    INPUTS:\n    game_pk : 6-digit integer MLB game ID to retrieve\n    \"\"\"\n    data = single_game_request(game_pk)\n    data = postprocessing(data, team)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting all pitching stats for a given time range.", "response": "def pitching_stats_range(start_dt=None, end_dt=None):\n    \"\"\"\n    Get all pitching stats for a set time range. This can be the past week, the \n    month of August, anything. Just supply the start and end date in YYYY-MM-DD \n    format. \n    \"\"\"\n    # ensure valid date strings, perform necessary processing for query\n    start_dt, end_dt = sanitize_input(start_dt, end_dt)\n    if datetime.datetime.strptime(start_dt, \"%Y-%m-%d\").year < 2008:\n        raise ValueError(\"Year must be 2008 or later\")\n    if datetime.datetime.strptime(end_dt, \"%Y-%m-%d\").year < 2008:\n        raise ValueError(\"Year must be 2008 or later\")\n    # retrieve html from baseball reference\n    soup = get_soup(start_dt, end_dt)\n    table = get_table(soup)\n    table = table.dropna(how='all') # drop if all columns are NA\n    #fix some strange formatting for percentage columns\n    table = table.replace('---%', np.nan)\n    #make sure these are all numeric\n    for column in ['Age', '#days', 'G', 'GS', 'W', 'L', 'SV', 'IP', 'H',\n                    'R', 'ER', 'BB', 'SO', 'HR', 'HBP', 'ERA', 'AB', '2B',\n                    '3B', 'IBB', 'GDP', 'SF', 'SB', 'CS', 'PO', 'BF', 'Pit',\n                    'WHIP', 'BAbip', 'SO9', 'SO/W']:\n        table[column] = pd.to_numeric(table[column])\n    #convert str(xx%) values to float(0.XX) decimal values\n    for column in ['Str', 'StL', 'StS', 'GB/FB', 'LD', 'PU']:\n        table[column] = table[column].replace('%','',regex=True).astype('float')/100\n\n    table = table.drop('',1)\n    return table"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pitching_stats_bref(season=None):\n    if season is None:\n        season = datetime.datetime.today().strftime(\"%Y\")\n    season = str(season)\n    start_dt = season + '-03-01' #opening day is always late march or early april\n    end_dt = season + '-11-01' #season is definitely over by November \n    return(pitching_stats_range(start_dt, end_dt))", "response": "Get all pitching stats for a set season."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets data from war_daily_pitch table. Returns WAR its components and a few other useful stats.", "response": "def bwar_pitch(return_all=False):\n    \"\"\"\n    Get data from war_daily_pitch table. Returns WAR, its components, and a few other useful stats. \n    To get all fields from this table, supply argument return_all=True.  \n    \"\"\"\n    url = \"http://www.baseball-reference.com/data/war_daily_pitch.txt\"\n    s = requests.get(url).content\n    c=pd.read_csv(io.StringIO(s.decode('utf-8')))\n    if return_all:\n        return c\n    else:\n        cols_to_keep = ['name_common', 'mlb_ID', 'player_ID', 'year_ID', 'team_ID', 'stint_ID', 'lg_ID',\n                        'G', 'GS', 'RA','xRA', 'BIP', 'BIP_perc','salary', 'ERA_plus', 'WAR_rep', 'WAA',\n                        'WAA_adj','WAR']\n        return c[cols_to_keep]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deprecated(version, *replacement):\n\n    def deprecated_wrapper(func):\n        replacement_message = 'Use {} instead'.format(', '.join(\n            [\"'{}'\".format(_get_func_fq_name(x))\n             for x in replacement]))\n        log_message = (\"'{}' is deprecated, {}\"\n                       \"\".format(_get_func_fq_name(func), replacement_message))\n        # func.__doc__ = replacement[0].__doc__\n\n        func_path = _get_func_path(func)\n        doc_replacement = []\n        for x in replacement:\n            if func_path == _get_func_path(x):\n                doc_replacement.append(':func:`{}`'.format(_func_name(x)))\n            else:\n                doc_replacement.append(\n                    ':func:`{}`'.format(_get_func_fq_name(x)))\n\n        func.__doc__ = \"\"\"\n            .. deprecated:: {}\n               Use {} instead\n               \n               {} \n            \"\"\".format(version,\n                       ', '.join(doc_replacement),\n                       func.__doc__ if func.__doc__ else '')\n\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            log.warning(log_message)\n            return func(*args, **kwargs)\n\n        return wrapper\n\n    return deprecated_wrapper", "response": "Decorator to mark a function as deprecated."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary in cloud format with the data of the current object.", "response": "def to_api_data(self, restrict_keys=None):\n        \"\"\" Returns a dictionary in cloud format\n\n        :param restrict_keys: a set of keys to restrict the returned data to.\n        \"\"\"\n        cc = self._cc  # alias\n\n        data = {\n            cc('displayName'): self.__display_name,\n            cc('givenName'): self.__name,\n            cc('surname'): self.__surname,\n            cc('title'): self.__title,\n            cc('jobTitle'): self.__job_title,\n            cc('companyName'): self.__company_name,\n            cc('department'): self.__department,\n            cc('officeLocation'): self.__office_location,\n            cc('businessPhones'): self.__business_phones,\n            cc('mobilePhone'): self.__mobile_phone,\n            cc('homePhones'): self.__home_phones,\n            cc('emailAddresses'): [{self._cc('name'): recipient.name or '',\n                                    self._cc('address'): recipient.address}\n                                   for recipient in self.emails],\n            cc('businessAddress'): self.__business_address,\n            cc('homesAddress'): self.__home_address,\n            cc('otherAddress'): self.__other_address,\n            cc('categories'): self.__categories\n        }\n\n        if restrict_keys:\n            restrict_keys.add(cc(\n                'givenName'))  # GivenName is required by the api all the time.\n            for key in list(data.keys()):\n                if key not in restrict_keys:\n                    del data[key]\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete this contact s root entry in the cloud", "response": "def delete(self):\n        \"\"\" Deletes this contact\n\n        :return: Success or Failure\n        :rtype: bool\n        :raises RuntimeError: if contact is not yet saved to cloud\n        \"\"\"\n        if not self.object_id:\n            raise RuntimeError('Attempting to delete an unsaved Contact')\n\n        url = self.build_url(\n            self._endpoints.get('root_contact').format(id=self.object_id))\n\n        response = self.con.delete(url)\n\n        return bool(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save(self):\n        if self.object_id:\n            # Update Contact\n            if not self._track_changes:\n                return True  # there's nothing to update\n            url = self.build_url(\n                self._endpoints.get('root_contact').format(id=self.object_id))\n            method = self.con.patch\n            data = self.to_api_data(restrict_keys=self._track_changes)\n        else:\n            # Save new Contact\n            if self.__folder_id:\n                url = self.build_url(\n                    self._endpoints.get('child_contact').format(\n                        folder_id=self.__folder_id))\n            else:\n                url = self.build_url(self._endpoints.get('contact'))\n            method = self.con.post\n            data = self.to_api_data(restrict_keys=self._track_changes)\n        response = method(url, data=data)\n\n        if not response:\n            return False\n\n        if not self.object_id:\n            # New Contact\n            contact = response.json()\n\n            self.object_id = contact.get(self._cc('id'), None)\n\n            self.__created = contact.get(self._cc('createdDateTime'), None)\n            self.__modified = contact.get(self._cc('lastModifiedDateTime'),\n                                          None)\n\n            local_tz = self.protocol.timezone\n            self.__created = parse(self.created).astimezone(\n                local_tz) if self.__created else None\n            self.__modified = parse(self.modified).astimezone(\n                local_tz) if self.__modified else None\n        else:\n            self.__modified = self.protocol.timezone.localize(dt.datetime.now())\n\n        return True", "response": "Saves this contact to the cloud"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef new_message(self, recipient=None, *, recipient_type=RecipientType.TO):\n        if self.main_resource == GAL_MAIN_RESOURCE:\n            # preventing the contact lookup to explode for big organizations..\n            raise RuntimeError('Sending a message to all users within an '\n                               'Organization is not allowed')\n\n        if isinstance(recipient_type, str):\n            recipient_type = RecipientType(recipient_type)\n\n        recipient = recipient or self.emails.get_first_recipient_with_address()\n        if not recipient:\n            return None\n\n        new_message = self.message_constructor(parent=self, is_draft=True)\n\n        target_recipients = getattr(new_message, str(recipient_type.value))\n        target_recipients.add(recipient)\n\n        return new_message", "response": "This method creates a new draft Message instance with the given recipient and recipient_type."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_contacts(self, limit=100, *, query=None, order_by=None, batch=None):\n\n        if self.main_resource == GAL_MAIN_RESOURCE:\n            # using Users endpoint to access the Global Address List\n            url = self.build_url(self._endpoints.get('gal'))\n        else:\n            if self.root:\n                url = self.build_url(self._endpoints.get('root_contacts'))\n            else:\n                url = self.build_url(\n                    self._endpoints.get('folder_contacts').format(\n                        id=self.folder_id))\n\n        if limit is None or limit > self.protocol.max_top_value:\n            batch = self.protocol.max_top_value\n\n        params = {'$top': batch if batch else limit}\n\n        if order_by:\n            params['$orderby'] = order_by\n\n        if query:\n            if isinstance(query, str):\n                params['$filter'] = query\n            else:\n                params.update(query.as_params())\n\n        response = self.con.get(url, params=params)\n        if not response:\n            return iter(())\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        contacts = (self.contact_constructor(parent=self,\n                                             **{self._cloud_data_key: contact})\n                    for contact in data.get('value', []))\n\n        next_link = data.get(NEXT_LINK_KEYWORD, None)\n\n        if batch and next_link:\n            return Pagination(parent=self, data=contacts,\n                              constructor=self.contact_constructor,\n                              next_link=next_link, limit=limit)\n        else:\n            return contacts", "response": "Gets a list of contacts from this address book."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Contact by it s email", "response": "def get_contact_by_email(self, email):\n        \"\"\" Returns a Contact by it's email\n\n        :param email: email to get contact for\n        :return: Contact for specified email\n        :rtype: Contact\n        \"\"\"\n        if not email:\n            return None\n\n        email = email.strip()\n        query = self.q().any(collection='email_addresses', attribute='address',\n                             word=email, operation='eq')\n        contacts = self.get_contacts(limit=1, query=query)\n        return contacts[0] if contacts else None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of child folders", "response": "def get_folders(self, limit=None, *, query=None, order_by=None):\n        \"\"\" Returns a list of child folders\n\n        :param int limit: max no. of folders to get. Over 999 uses batch.\n        :param query: applies a OData filter to the request\n        :type query: Query or str\n        :param order_by: orders the result set based on this condition\n        :type order_by: Query or str\n        :return: list of folders\n        :rtype: list[ContactFolder]\n        \"\"\"\n        if self.root:\n            url = self.build_url(self._endpoints.get('root_folders'))\n        else:\n            url = self.build_url(\n                self._endpoints.get('child_folders').format(id=self.folder_id))\n\n        params = {}\n\n        if limit:\n            params['$top'] = limit\n\n        if order_by:\n            params['$orderby'] = order_by\n\n        if query:\n            if isinstance(query, str):\n                params['$filter'] = query\n            else:\n                params.update(query.as_params())\n\n        response = self.con.get(url, params=params or None)\n        if not response:\n            return []\n\n        data = response.json()\n\n        return [self.__class__(parent=self, **{self._cloud_data_key: folder})\n                for folder in data.get('value', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a new child folder", "response": "def create_child_folder(self, folder_name):\n        \"\"\" Creates a new child folder\n\n        :param str folder_name: name of the new folder to create\n        :return: newly created folder\n        :rtype: ContactFolder or None\n        \"\"\"\n\n        if not folder_name:\n            return None\n\n        if self.root:\n            url = self.build_url(self._endpoints.get('root_folders'))\n        else:\n            url = self.build_url(\n                self._endpoints.get('child_folders').format(id=self.folder_id))\n\n        response = self.con.post(url,\n                                 data={self._cc('displayName'): folder_name})\n        if not response:\n            return None\n\n        folder = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.__class__(parent=self, **{self._cloud_data_key: folder})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_folder_name(self, name):\n        if self.root:\n            return False\n        if not name:\n            return False\n\n        url = self.build_url(\n            self._endpoints.get('get_folder').format(id=self.folder_id))\n\n        response = self.con.patch(url, data={self._cc('displayName'): name})\n        if not response:\n            return False\n\n        folder = response.json()\n\n        self.name = folder.get(self._cc('displayName'), '')\n        self.parent_id = folder.get(self._cc('parentFolderId'), None)\n\n        return True", "response": "Updates the name of the current folder"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove this entry into another folder", "response": "def move_folder(self, to_folder):\n        \"\"\" Change this folder name\n\n        :param to_folder: folder_id/ContactFolder to move into\n        :type to_folder: str or ContactFolder\n        :return: Moved or Not\n        :rtype: bool\n        \"\"\"\n        if self.root:\n            return False\n        if not to_folder:\n            return False\n\n        url = self.build_url(\n            self._endpoints.get('get_folder').format(id=self.folder_id))\n\n        if isinstance(to_folder, ContactFolder):\n            folder_id = to_folder.folder_id\n        elif isinstance(to_folder, str):\n            folder_id = to_folder\n        else:\n            return False\n\n        response = self.con.patch(url,\n                                  data={self._cc('parentFolderId'): folder_id})\n        if not response:\n            return False\n\n        folder = response.json()\n\n        self.name = folder.get(self._cc('displayName'), '')\n        self.parent_id = folder.get(self._cc('parentFolderId'), None)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a new contact to be saved into the parent folder", "response": "def new_contact(self):\n        \"\"\" Creates a new contact to be saved into it's parent folder\n\n        :return: newly created contact\n        :rtype: Contact\n        \"\"\"\n        contact = self.contact_constructor(parent=self)\n        if not self.root:\n            contact.__folder_id = self.folder_id\n        return contact"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_message(self, recipient_type=RecipientType.TO, *, query=None):\n\n        if isinstance(recipient_type, str):\n            recipient_type = RecipientType(recipient_type)\n\n        recipients = [contact.emails[0]\n                      for contact in self.get_contacts(limit=None, query=query)\n                      if contact.emails and contact.emails[0].address]\n\n        if not recipients:\n            return None\n\n        new_message = self.message_constructor(parent=self, is_draft=True)\n        target_recipients = getattr(new_message, str(recipient_type.value))\n        target_recipients.add(recipients)\n\n        return new_message", "response": "This method creates a new draft Message instance with all the the\n        contacts first email as a recipient"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_contact_by_email(self, email):\n        if not email:\n            return None\n\n        email = email.strip()\n\n        url = self.build_url('{}/{}'.format(self._endpoints.get('gal'), email))\n\n        response = self.con.get(url)\n        if not response:\n            return []\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.contact_constructor(parent=self,\n                                        **{self._cloud_data_key: data})", "response": "Returns a Contact by it s email"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_flagged(self, *, start_date=None, due_date=None):\n        self.__status = Flag.Flagged\n        start_date = start_date or dt.datetime.now()\n        due_date = due_date or dt.datetime.now()\n        if start_date.tzinfo is None:\n            start_date = self.protocol.timezone.localize(start_date)\n        if due_date.tzinfo is None:\n            due_date = self.protocol.timezone.localize(due_date)\n        self.__start = start_date\n        self.__due_date = due_date\n        self._track_changes()", "response": "Sets this message as flagged."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsetting this message flag as completed.", "response": "def set_completed(self, *, completition_date=None):\n        \"\"\" Sets this message flag as completed\n        :param completition_date: the datetime this followUp was completed\n        \"\"\"\n        self.__status = Flag.Complete\n        completition_date = completition_date or dt.datetime.now()\n        if completition_date.tzinfo is None:\n            completition_date = self.protocol.timezone.localize(completition_date)\n        self.__completed = completition_date\n        self._track_changes()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_flag(self):\n        self.__status = Flag.NotFlagged\n        self.__start = None\n        self.__due_date = None\n        self.__completed = None\n        self._track_changes()", "response": "Sets this message as un flagged"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_api_data(self):\n        data = {\n            self._cc('flagStatus'): self._cc(self.__status.value)\n        }\n        if self.__status is Flag.Flagged:\n            data[self._cc('startDateTime')] = self._build_date_time_time_zone(self.__start)\n            data[self._cc('dueDateTime')] = self._build_date_time_time_zone(self.__due_date)\n\n        if self.__status is Flag.Complete:\n            data[self._cc('completedDateTime')] = self._build_date_time_time_zone(self.__completed)\n\n        return data", "response": "Returns this data as a dict to be sent to the server"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting the sender property of the object", "response": "def sender(self, value):\n        \"\"\" sender is a property to force to be always a Recipient class \"\"\"\n        if isinstance(value, Recipient):\n            if value._parent is None:\n                value._parent = self\n                value._field = 'from'\n            self.__sender = value\n        elif isinstance(value, str):\n            self.__sender.address = value\n            self.__sender.name = ''\n        else:\n            raise ValueError(\n                'sender must be an address string or a Recipient object')\n        self._track_changes.add('from')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nconvert the object to the cloud based format.", "response": "def to_api_data(self, restrict_keys=None):\n        \"\"\" Returns a dict representation of this message prepared to be send\n        to the cloud\n\n        :param restrict_keys: a set of keys to restrict the returned\n         data to\n        :type restrict_keys: dict or set\n        :return: converted to cloud based keys\n        :rtype: dict\n        \"\"\"\n\n        cc = self._cc  # alias to shorten the code\n\n        message = {\n            cc('subject'): self.subject,\n            cc('body'): {\n                cc('contentType'): self.body_type,\n                cc('content'): self.body},\n            cc('importance'): cc(self.importance.value),\n            cc('flag'): self.flag.to_api_data(),\n            cc('isReadReceiptRequested'): self.is_read_receipt_requested,\n            cc('isDeliveryReceiptRequested'): self.is_delivery_receipt_requested,\n        }\n\n        if self.to:\n            message[cc('toRecipients')] = [self._recipient_to_cloud(recipient)\n                                           for recipient in self.to]\n        if self.cc:\n            message[cc('ccRecipients')] = [self._recipient_to_cloud(recipient)\n                                           for recipient in self.cc]\n        if self.bcc:\n            message[cc('bccRecipients')] = [self._recipient_to_cloud(recipient)\n                                            for recipient in self.bcc]\n        if self.reply_to:\n            message[cc('replyTo')] = [self._recipient_to_cloud(recipient) for\n                                      recipient in self.reply_to]\n        if self.attachments:\n            message[cc('attachments')] = self.attachments.to_api_data()\n        if self.sender and self.sender.address:\n            message[cc('from')] = self._recipient_to_cloud(self.sender)\n\n        if self.categories or 'categories' in (restrict_keys or {}):\n            message[cc('categories')] = self.categories\n\n        if self.object_id and not self.__is_draft:\n            # return the whole signature of this message\n\n            message[cc('id')] = self.object_id\n            if self.created:\n                message[cc('createdDateTime')] = self.created.astimezone(\n                    pytz.utc).isoformat()\n            if self.received:\n                message[cc('receivedDateTime')] = self.received.astimezone(\n                    pytz.utc).isoformat()\n            if self.sent:\n                message[cc('sentDateTime')] = self.sent.astimezone(\n                    pytz.utc).isoformat()\n            message[cc('hasAttachments')] = bool(self.attachments)\n            message[cc('isRead')] = self.is_read\n            message[cc('isDraft')] = self.__is_draft\n            message[cc('conversationId')] = self.conversation_id\n            # this property does not form part of the message itself\n            message[cc('parentFolderId')] = self.folder_id\n\n        if restrict_keys:\n            for key in list(message.keys()):\n                if key not in restrict_keys:\n                    del message[key]\n\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending this message to the specified object.", "response": "def send(self, save_to_sent_folder=True):\n        \"\"\" Sends this message\n\n        :param bool save_to_sent_folder: whether or not to save it to\n         sent folder\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        if self.object_id and not self.__is_draft:\n            return RuntimeError('Not possible to send a message that is not '\n                                'new or a draft. Use Reply or Forward instead.')\n\n        if self.__is_draft and self.object_id:\n            url = self.build_url(\n                self._endpoints.get('send_draft').format(id=self.object_id))\n            if self._track_changes:\n                # there are pending changes to be committed\n                self.save_draft()\n            data = None\n\n        else:\n            url = self.build_url(self._endpoints.get('send_mail'))\n            data = {self._cc('message'): self.to_api_data()}\n            if save_to_sent_folder is False:\n                data[self._cc('saveToSentItems')] = False\n\n        response = self.con.post(url, data=data)\n        # response evaluates to false if 4XX or 5XX status codes are returned\n        if not response:\n            return False\n\n        self.object_id = 'sent_message' if not self.object_id \\\n            else self.object_id\n        self.__is_draft = False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreply to the current message", "response": "def reply(self, to_all=True):\n        \"\"\"  Creates a new message that is a reply to this message\n\n        :param bool to_all: whether or not to replies to all the recipients\n         instead to just the sender\n        :return: new message\n        :rtype: Message\n        \"\"\"\n        if not self.object_id or self.__is_draft:\n            raise RuntimeError(\"Can't reply to this message\")\n\n        if to_all:\n            url = self.build_url(self._endpoints.get('create_reply_all').format(\n                id=self.object_id))\n        else:\n            url = self.build_url(\n                self._endpoints.get('create_reply').format(id=self.object_id))\n\n        response = self.con.post(url)\n        if not response:\n            return None\n\n        message = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.__class__(parent=self, **{self._cloud_data_key: message})"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a new message that is a forward this message", "response": "def forward(self):\n        \"\"\"  Creates a new message that is a forward this message\n\n        :return: new message\n        :rtype: Message\n        \"\"\"\n        if not self.object_id or self.__is_draft:\n            raise RuntimeError(\"Can't forward this message\")\n\n        url = self.build_url(\n            self._endpoints.get('forward_message').format(id=self.object_id))\n\n        response = self.con.post(url)\n        if not response:\n            return None\n\n        message = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.__class__(parent=self, **{self._cloud_data_key: message})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmark this message as read in the cloud", "response": "def mark_as_read(self):\n        \"\"\" Marks this message as read in the cloud\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        if self.object_id is None or self.__is_draft:\n            raise RuntimeError('Attempting to mark as read an unsaved Message')\n\n        data = {self._cc('isRead'): True}\n\n        url = self.build_url(\n            self._endpoints.get('get_message').format(id=self.object_id))\n\n        response = self.con.patch(url, data=data)\n        if not response:\n            return False\n\n        self.__is_read = True\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef move(self, folder):\n        if self.object_id is None:\n            raise RuntimeError('Attempting to move an unsaved Message')\n\n        url = self.build_url(\n            self._endpoints.get('move_message').format(id=self.object_id))\n\n        if isinstance(folder, str):\n            folder_id = folder\n        else:\n            folder_id = getattr(folder, 'folder_id', None)\n\n        if not folder_id:\n            raise RuntimeError('Must Provide a valid folder_id')\n\n        data = {self._cc('destinationId'): folder_id}\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return False\n\n        self.folder_id = folder_id\n\n        return True", "response": "Move the message to a given folder"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncopy the message to a given folder", "response": "def copy(self, folder):\n        \"\"\" Copy the message to a given folder\n\n        :param folder: Folder object or Folder id or Well-known name to\n         copy this message to\n        :type folder: str or mailbox.Folder\n        :returns: the copied message\n        :rtype: Message\n        \"\"\"\n        if self.object_id is None:\n            raise RuntimeError('Attempting to move an unsaved Message')\n\n        url = self.build_url(\n            self._endpoints.get('copy_message').format(id=self.object_id))\n\n        if isinstance(folder, str):\n            folder_id = folder\n        else:\n            folder_id = getattr(folder, 'folder_id', None)\n\n        if not folder_id:\n            raise RuntimeError('Must Provide a valid folder_id')\n\n        data = {self._cc('destinationId'): folder_id}\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        message = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.__class__(parent=self, **{self._cloud_data_key: message})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave changes to a message.", "response": "def save_message(self):\n        \"\"\" Saves changes to a message.\n        If the message is a new or saved draft it will call 'save_draft' otherwise\n        this will save only properties of a message that are draft-independent such as:\n            - is_read\n            - category\n            - flag\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        if self.object_id and not self.__is_draft:\n            # we are only allowed to save some properties:\n            allowed_changes = {self._cc('isRead'), self._cc('categories'), self._cc('flag')}  # allowed changes to be saved by this method\n            changes = {tc for tc in self._track_changes if tc in allowed_changes}\n\n            if not changes:\n                return True  # there's nothing to update\n\n            url = self.build_url(self._endpoints.get('get_message').format(id=self.object_id))\n\n            data = self.to_api_data(restrict_keys=changes)\n\n            response = self.con.patch(url, data=data)\n\n            if not response:\n                return False\n\n            self._track_changes.clear()  # reset the tracked changes as they are all saved\n            self.__modified = self.protocol.timezone.localize(dt.datetime.now())\n\n            return True\n        else:\n            # fallback to save_draft\n            return self.save_draft()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave this message as a draft on the cloud.", "response": "def save_draft(self, target_folder=OutlookWellKnowFolderNames.DRAFTS):\n        \"\"\" Save this message as a draft on the cloud\n\n        :param target_folder: name of the drafts folder\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        if self.object_id:\n            # update message. Attachments are NOT included nor saved.\n            if not self.__is_draft:\n                raise RuntimeError('Only draft messages can be updated')\n            if not self._track_changes:\n                return True  # there's nothing to update\n            url = self.build_url(\n                self._endpoints.get('get_message').format(id=self.object_id))\n            method = self.con.patch\n            data = self.to_api_data(restrict_keys=self._track_changes)\n\n            data.pop(self._cc('attachments'),\n                     None)  # attachments are handled by the next method call\n            # noinspection PyProtectedMember\n            self.attachments._update_attachments_to_cloud()\n        else:\n            # new message. Attachments are included and saved.\n            if not self.__is_draft:\n                raise RuntimeError('Only draft messages can be saved as drafts')\n\n            target_folder = target_folder or OutlookWellKnowFolderNames.DRAFTS\n            if isinstance(target_folder, OutlookWellKnowFolderNames):\n                target_folder = target_folder.value\n            elif not isinstance(target_folder, str):\n                # a Folder instance\n                target_folder = getattr(target_folder, 'folder_id',\n                                        OutlookWellKnowFolderNames.DRAFTS.value)\n\n            url = self.build_url(\n                self._endpoints.get('create_draft_folder').format(\n                    id=target_folder))\n            method = self.con.post\n            data = self.to_api_data()\n\n        if not data:\n            return True\n\n        response = method(url, data=data)\n        if not response:\n            return False\n\n        self._track_changes.clear()  # reset the tracked changes as they are all saved\n\n        if not self.object_id:\n            # new message\n            message = response.json()\n\n            self.object_id = message.get(self._cc('id'), None)\n            self.folder_id = message.get(self._cc('parentFolderId'), None)\n\n            # fallback to office365 v1.0\n            self.__created = message.get(self._cc('createdDateTime'),\n                                         message.get(\n                                             self._cc('dateTimeCreated'),\n                                             None))\n            # fallback to office365 v1.0\n            self.__modified = message.get(self._cc('lastModifiedDateTime'),\n                                          message.get(\n                                              self._cc('dateTimeModified'),\n                                              None))\n\n            self.__created = parse(self.__created).astimezone(\n                self.protocol.timezone) if self.__created else None\n            self.__modified = parse(self.__modified).astimezone(\n                self.protocol.timezone) if self.__modified else None\n\n        else:\n            self.__modified = self.protocol.timezone.localize(dt.datetime.now())\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the related Event", "response": "def get_event(self):\n        \"\"\" If this is a EventMessage it should return the related Event\"\"\"\n\n        if not self.is_event_message:\n            return None\n\n        # select a dummy field (eg. subject) to avoid pull unneccesary data\n        query = self.q().select('subject').expand('event')\n\n        url = self.build_url(self._endpoints.get('get_message').format(id=self.object_id))\n\n        response = self.con.get(url, params=query.as_params())\n\n        if not response:\n            return None\n\n        data = response.json()\n        event_data = data.get(self._cc('event'))\n\n        return Event(parent=self, **{self._cloud_data_key: event_data})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef build(force):\n    dist_path = Path(DIST_PATH)\n    if dist_path.exists() and list(dist_path.glob('*')):\n        if force or click.confirm('{} is not empty - delete contents?'.format(dist_path)):\n            dist_path.rename(DIST_PATH_DELETE)\n            shutil.rmtree(Path(DIST_PATH_DELETE))\n            dist_path.mkdir()\n        else:\n            click.echo('Aborting')\n            sys.exit(1)\n\n    subprocess.check_call(['python', 'setup.py', 'bdist_wheel'])\n    subprocess.check_call(['python', 'setup.py', 'sdist',\n                           '--formats=gztar'])", "response": "Builds the distribution files."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload distribuition files to pypi or pypitest.", "response": "def upload(ctx, release, rebuild):\n    \"\"\" Uploads distribuition files to pypi or pypitest. \"\"\"\n    dist_path = Path(DIST_PATH)\n    if rebuild is False:\n        if not dist_path.exists() or not list(dist_path.glob('*')):\n            print(\"No distribution files found. Please run 'build' command first\")\n            return\n    else:\n        ctx.invoke(build, force=True)\n\n    if release:\n        args = ['twine', 'upload', 'dist/*']\n    else:\n        repository = 'https://test.pypi.org/legacy/'\n        args = ['twine', 'upload', '--repository-url', repository, 'dist/*']\n\n    env = os.environ.copy()\n\n    p = subprocess.Popen(args, env=env)\n    p.wait()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check():\n    dist_path = Path(DIST_PATH)\n    if not dist_path.exists() or not list(dist_path.glob('*')):\n        print(\"No distribution files found. Please run 'build' command first\")\n        return\n\n    subprocess.check_call(['twine', 'check', 'dist/*'])", "response": "Checks the long description."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists all releases published on pypi.", "response": "def list_releases():\n    \"\"\" Lists all releases published on pypi. \"\"\"\n    response = requests.get(PYPI_URL.format(package=PYPI_PACKAGE_NAME))\n    if response:\n        data = response.json()\n\n        releases_dict = data.get('releases', {})\n\n        if releases_dict:\n            for version, release in releases_dict.items():\n                release_formats = []\n                published_on_date = None\n                for fmt in release:\n                    release_formats.append(fmt.get('packagetype'))\n                    published_on_date = fmt.get('upload_time')\n\n                release_formats = ' | '.join(release_formats)\n                print('{:<10}{:>15}{:>25}'.format(version, published_on_date, release_formats))\n        else:\n            print('No releases found for {}'.format(PYPI_PACKAGE_NAME))\n    else:\n        print('Package \"{}\" not found on Pypi.org'.format(PYPI_PACKAGE_NAME))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndisplay a table of the contributors and to what extent we have them to thank.", "response": "def contribution_breakdown():\n    \"\"\" Displays a table of the contributors and to what extent we have them to thank.\"\"\"\n    args = ['git', 'blame']\n    counts = {}\n    line_format = '{0:30}\\t{1:>10}\\t{2:>10}%'\n    files = subprocess.check_output(['git', 'ls-files']).decode(\"utf-8\").split('\\n')\n\n    for f in files[:-1]:\n        if 'docs/latest' in f or '_themes' in f:\n            continue  # skip generated stuff\n        lines = subprocess.check_output(args + [f]).decode('utf-8')\n        blames = [get_line_blame(line) for line in lines.split('\\n')]\n        for blame in blames:\n            counts[blame] = counts.get(blame, 0) + 1\n\n    total = sum([counts[count] for count in counts])\n    contribs = [(user, counts[user]) for user in counts]\n    contribs.sort(key=lambda x: x[1], reverse=True)\n\n    print(line_format.format('User', 'Lines', 'Line '))\n\n    for user in contribs:\n        percent = floor(100.0 * user[1] / total)\n        if percent == 0: percent = '>1'\n        print(line_format.format(user[0], user[1], percent))\n\n    print(line_format.format('Total', total, 100))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of scopes needed for each of the user provided scopes.", "response": "def get_scopes_for(self, user_provided_scopes):\n        \"\"\" Returns a list of scopes needed for each of the\n        scope_helpers provided, by adding the prefix to them if required\n\n        :param user_provided_scopes: a list of scopes or scope helpers\n        :type user_provided_scopes: list or tuple or str\n        :return: scopes with url prefix added\n        :rtype: list\n        :raises ValueError: if unexpected datatype of scopes are passed\n        \"\"\"\n        if user_provided_scopes is None:\n            # return all available scopes\n            user_provided_scopes = [app_part for app_part in self._oauth_scopes]\n        elif isinstance(user_provided_scopes, str):\n            user_provided_scopes = [user_provided_scopes]\n\n        if not isinstance(user_provided_scopes, (list, tuple)):\n            raise ValueError(\n                \"'user_provided_scopes' must be a list or a tuple of strings\")\n\n        scopes = set()\n        for app_part in user_provided_scopes:\n            for scope in self._oauth_scopes.get(app_part, [(app_part,)]):\n                scopes.add(self._prefix_scope(scope))\n\n        return list(scopes)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd the protocol scope prefix if required", "response": "def _prefix_scope(self, scope):\n        \"\"\" Inserts the protocol scope prefix if required\"\"\"\n        if self.protocol_scope_prefix:\n            if isinstance(scope, tuple):\n                return scope[0]\n            elif scope.startswith(self.protocol_scope_prefix):\n                return scope\n            else:\n                return '{}{}'.format(self.protocol_scope_prefix, scope)\n        else:\n            if isinstance(scope, tuple):\n                return scope[0]\n            else:\n                return scope"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsetting a proxy on the Session", "response": "def set_proxy(self, proxy_server, proxy_port, proxy_username,\n                  proxy_password):\n        \"\"\" Sets a proxy on the Session\n\n        :param str proxy_server: the proxy server\n        :param int proxy_port: the proxy port, defaults to 8080\n        :param str proxy_username: the proxy username\n        :param str proxy_password: the proxy password\n        \"\"\"\n        if proxy_server and proxy_port:\n            if proxy_username and proxy_password:\n                self.proxy = {\n                    \"http\": \"http://{}:{}@{}:{}\".format(proxy_username,\n                                                        proxy_password,\n                                                        proxy_server,\n                                                        proxy_port),\n                    \"https\": \"https://{}:{}@{}:{}\".format(proxy_username,\n                                                          proxy_password,\n                                                          proxy_server,\n                                                          proxy_port),\n                }\n            else:\n                self.proxy = {\n                    \"http\": \"http://{}:{}\".format(proxy_server, proxy_port),\n                    \"https\": \"https://{}:{}\".format(proxy_server, proxy_port),\n                }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_token_file(self):\n        # TODO: remove this method in a future release\n        warnings.warn('This method will be removed in future versions',\n                      DeprecationWarning)\n        return self.token_backend.check_token() if hasattr(self.token_backend, 'check_token') else None", "response": "Checks if the token file exists at the given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ninitializes the oauth flow and returns the authorization url that the user must approve.", "response": "def get_authorization_url(self, requested_scopes=None,\n                              redirect_uri=OAUTH_REDIRECT_URL, **kwargs):\n        \"\"\" Initializes the oauth authorization flow, getting the\n        authorization url that the user must approve.\n\n        :param list[str] requested_scopes: list of scopes to request access for\n        :param str redirect_uri: redirect url configured in registered app\n        :param kwargs: allow to pass unused params in conjunction with Connection\n        :return: authorization url\n        :rtype: str\n        \"\"\"\n\n        # TODO: remove this warning in future releases\n        if redirect_uri == OAUTH_REDIRECT_URL:\n            warnings.warn('The default redirect uri was changed in version 1.1.4. to'\n                          ' \"https://login.microsoftonline.com/common/oauth2/nativeclient\".'\n                          ' You may have to change the registered app \"redirect uri\" or pass here the old \"redirect_uri\"',\n                          DeprecationWarning)\n\n        client_id, client_secret = self.auth\n\n        if requested_scopes:\n            scopes = requested_scopes\n        elif self.scopes is not None:\n            scopes = self.scopes\n        else:\n            raise ValueError('Must provide at least one scope')\n\n        self.session = oauth = OAuth2Session(client_id=client_id,\n                                             redirect_uri=redirect_uri,\n                                             scope=scopes)\n        self.session.proxies = self.proxy\n        if self.request_retries:\n            retry = Retry(total=self.request_retries, read=self.request_retries,\n                          connect=self.request_retries,\n                          backoff_factor=RETRIES_BACKOFF_FACTOR,\n                          status_forcelist=RETRIES_STATUS_LIST)\n            adapter = HTTPAdapter(max_retries=retry)\n            self.session.mount('http://', adapter)\n            self.session.mount('https://', adapter)\n\n        # TODO: access_type='offline' has no effect according to documentation\n        #  This is done through scope 'offline_access'.\n        auth_url, state = oauth.authorization_url(\n            url=self._oauth2_authorize_url, access_type='offline')\n\n        return auth_url"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef request_token(self, authorization_url, store_token=True,\n                      token_path=None, **kwargs):\n        \"\"\" Authenticates for the specified url and gets the token, save the\n        token for future based if requested\n\n        :param str authorization_url: url given by the authorization flow\n        :param bool store_token: whether or not to store the token,\n         so u don't have to keep opening the auth link and\n         authenticating every time\n        :param Path token_path: full path to where the token should be saved to\n        :param kwargs: allow to pass unused params in conjunction with Connection\n        :return: Success/Failure\n        :rtype: bool\n        \"\"\"\n\n        if self.session is None:\n            raise RuntimeError(\"Fist call 'get_authorization_url' to \"\n                               \"generate a valid oauth object\")\n\n        # TODO: remove token_path in future versions\n        if token_path is not None:\n            warnings.warn('\"token_path\" param will be removed in future versions.'\n                          ' Use a TokenBackend instead', DeprecationWarning)\n        _, client_secret = self.auth\n\n        # Allow token scope to not match requested scope.\n        # (Other auth libraries allow this, but Requests-OAuthlib\n        # raises exception on scope mismatch by default.)\n        os.environ['OAUTHLIB_RELAX_TOKEN_SCOPE'] = '1'\n        os.environ['OAUTHLIB_IGNORE_SCOPE_CHANGE'] = '1'\n\n        try:\n            self.token_backend.token = Token(self.session.fetch_token(\n                token_url=self._oauth2_token_url,\n                authorization_response=authorization_url,\n                include_client_id=True,\n                client_secret=client_secret))\n        except Exception as e:\n            log.error('Unable to fetch auth token. Error: {}'.format(str(e)))\n            return False\n\n        if store_token:\n            self.token_backend.save_token()\n        return True", "response": "Request a token from the specified url and store it in the session object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_session(self, token_path=None):\n        # TODO: remove token_path in future versions\n        if token_path is not None:\n            warnings.warn('\"token_path\" param will be removed in future versions.'\n                          ' Use a TokenBackend instead.', DeprecationWarning)\n\n        # gets a fresh token from the store\n        token = self.token_backend.get_token()\n\n        if token:\n            client_id, _ = self.auth\n            session = OAuth2Session(client_id=client_id, token=token)\n        else:\n            raise RuntimeError(\n                'No auth token found. Authentication Flow needed')\n\n        session.proxies = self.proxy\n\n        if self.request_retries:\n            retry = Retry(total=self.request_retries, read=self.request_retries,\n                          connect=self.request_retries,\n                          backoff_factor=RETRIES_BACKOFF_FACTOR,\n                          status_forcelist=RETRIES_STATUS_LIST)\n            adapter = HTTPAdapter(max_retries=retry)\n            session.mount('http://', adapter)\n            session.mount('https://', adapter)\n\n        return session", "response": "Create a requests Session object from the token_path."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nrefreshes the OAuth authorization token.", "response": "def refresh_token(self):\n        \"\"\"\n        Refresh the OAuth authorization token.\n        This will be called automatically when the access token\n         expires, however, you can manually call this method to\n         request a new refresh token.\n        :return bool: Success / Failure\n        \"\"\"\n        if self.session is None:\n            self.session = self.get_session()\n\n        token = self.token_backend.token\n        if token and token.is_long_lived:\n            client_id, client_secret = self.auth\n            token = Token(self.session.refresh_token(\n                self._oauth2_token_url,\n                client_id=client_id,\n                client_secret=client_secret))\n        else:\n            log.error('You can not refresh an access token that has no \"refreh_token\" available.'\n                      'Include \"offline_access\" scope when authentication to get a \"refresh_token\"')\n            return False\n\n        self.token_backend.token = token\n\n        if self.store_token:\n            self.token_backend.save_token()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _check_delay(self):\n        if self._previous_request_at:\n            dif = round(time.time() - self._previous_request_at,\n                        2) * 1000  # difference in miliseconds\n            if dif < self.requests_delay:\n                time.sleep(\n                    (self.requests_delay - dif) / 1000)  # sleep needs seconds\n        self._previous_request_at = time.time()", "response": "Checks if a delay is needed between requests and sleeps"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef naive_request(self, url, method, **kwargs):\n        return self._internal_request(self.naive_session, url, method, **kwargs)", "response": "Makes a request to url using an oauth authorization session and a normal session"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking a request to url using an oauth session", "response": "def oauth_request(self, url, method, **kwargs):\n        \"\"\" Makes a request to url using an oauth session\n\n        :param str url: url to send request to\n        :param str method: type of request (get/put/post/patch/delete)\n        :param kwargs: extra params to send to the request api\n        :return: Response of the request\n        :rtype: requests.Response\n        \"\"\"\n        # oauth authentication\n        if self.session is None:\n            self.session = self.get_session()\n\n        return self._internal_request(self.session, url, method, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, url, params=None, **kwargs):\n        return self.oauth_request(url, 'get', params=params, **kwargs)", "response": "Shorthand for self. oauth_request"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending an oauth put request to the specified url and returns the response", "response": "def put(self, url, data=None, **kwargs):\n        \"\"\" Shorthand for self.oauth_request(url, 'put')\n\n        :param str url: url to send put oauth request to\n        :param dict data: put data to update the service\n        :param kwargs: extra params to send to request api\n        :return: Response of the request\n        :rtype: requests.Response\n        \"\"\"\n        return self.oauth_request(url, 'put', data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef attachment_name(self):\n        if self.__attachment_name is not None:\n            return self.__attachment_name\n        if self.__attachment_name_property:\n            return getattr(self, self.__attachment_name_property, '')\n        else:\n            # property order resolution:\n            # 1) try property 'subject'\n            # 2) try property 'name'\n            try:\n                attachment_name = getattr(self, 'subject')\n            except AttributeError:\n                attachment_name = getattr(self, 'name', '')\n            return attachment_name", "response": "Returns the name of the attachment in the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict to communicate with the server", "response": "def to_api_data(self):\n        \"\"\" Returns a dict to communicate with the server\n\n        :rtype: dict\n        \"\"\"\n        data = {'@odata.type': self._gk(\n            '{}_attachment_type'.format(self.attachment_type)),\n            self._cc('name'): self.name}\n\n        if self.attachment_type == 'file':\n            data[self._cc('contentBytes')] = self.content\n        else:\n            data[self._cc('item')] = self.content\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsave the attachment locally to disk.", "response": "def save(self, location=None, custom_name=None):\n        \"\"\"  Save the attachment locally to disk\n\n        :param str location: path string to where the file is to be saved.\n        :param str custom_name: a custom name to be saved as\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        if not self.content:\n            return False\n\n        location = Path(location or '')\n        if not location.exists():\n            log.debug('the location provided does not exist')\n            return False\n\n        name = custom_name or self.name\n        name = name.replace('/', '-').replace('\\\\', '')\n        try:\n            path = location / name\n            with path.open('wb') as file:\n                file.write(base64.b64decode(self.content))\n            self.attachment = path\n            self.on_disk = True\n            log.debug('file saved locally.')\n        except Exception as e:\n            log.error('file failed to be saved: %s', str(e))\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nattach this attachment to an existing BaseAttachment object.", "response": "def attach(self, api_object, on_cloud=False):\n        \"\"\" Attach this attachment to an existing api_object. This\n        BaseAttachment object must be an orphan BaseAttachment created for the\n        sole purpose of attach it to something and therefore run this method.\n\n        :param api_object: object to attach to\n        :param on_cloud: if the attachment is on cloud or not\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        if self.on_cloud:\n            # item is already saved on the cloud.\n            return True\n\n        # api_object must exist and if implements attachments\n        # then we can attach to it.\n        if api_object and getattr(api_object, 'attachments', None):\n            if on_cloud:\n                if not api_object.object_id:\n                    raise RuntimeError(\n                        'A valid object id is needed in order to attach a file')\n                # api_object builds its own url using its\n                # resource and main configuration\n                url = api_object.build_url(self._endpoints.get('attach').format(\n                    id=api_object.object_id))\n\n                response = api_object.con.post(url, data=self.to_api_data())\n\n                return bool(response)\n            else:\n                if self.attachment_type == 'file':\n                    api_object.attachments.add([{\n                        'attachment_id': self.attachment_id,\n                        # TODO: copy attachment id? or set to None?\n                        'path': str(\n                            self.attachment) if self.attachment else None,\n                        'name': self.name,\n                        'content': self.content,\n                        'on_disk': self.on_disk\n                    }])\n                else:\n                    raise RuntimeError('Only file attachments can be attached')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict to communicate with the server", "response": "def to_api_data(self):\n        \"\"\" Returns a dict to communicate with the server\n\n        :rtype: dict\n        \"\"\"\n        return [attachment.to_api_data() for attachment in self.__attachments if\n                attachment.on_cloud is False]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _update_parent_attachments(self):\n        try:\n            self._parent.has_attachments = bool(len(self.__attachments))\n        except AttributeError:\n            pass", "response": "Update the parent s has_attachments property."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add(self, attachments):\n        if attachments:\n            if isinstance(attachments, (str, Path)):\n                attachments = [attachments]\n            if isinstance(attachments, (list, tuple, set)):\n                # User provided attachments\n                attachments_temp = [\n                    self._attachment_constructor(attachment, parent=self)\n                    for attachment in attachments]\n            elif isinstance(attachments,\n                            dict) and self._cloud_data_key in attachments:\n                # Cloud downloaded attachments. We pass on_cloud=True\n                # to track if this attachment is saved on the server\n                attachments_temp = [self._attachment_constructor(\n                    {self._cloud_data_key: attachment}, parent=self,\n                    on_cloud=True)\n                    for attachment in\n                    attachments.get(self._cloud_data_key, [])]\n            else:\n                raise ValueError('Attachments must be a str or Path or a '\n                                 'list, tuple or set of the former')\n\n            self.__attachments.extend(attachments_temp)\n            self._update_parent_attachments()\n            self._track_changes()", "response": "Add more attachments to the current object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove the specified attachments from the current object.", "response": "def remove(self, attachments):\n        \"\"\" Remove the specified attachments\n\n        :param attachments: list of attachments\n        :type attachments: list[str] or list[Path] or str or Path or dict\n        \"\"\"\n        if isinstance(attachments, (list, tuple)):\n            attachments = ({attachment.name\n                            if isinstance(attachment, BaseAttachment)\n                            else attachment for attachment in attachments})\n        elif isinstance(attachments, str):\n            attachments = {attachments}\n        elif isinstance(attachments, BaseAttachment):\n            attachments = {attachments.name}\n        else:\n            raise ValueError('Incorrect parameter type for attachments')\n\n        new_attachments = []\n        for attachment in self.__attachments:\n            if attachment.name not in attachments:\n                new_attachments.append(attachment)\n            else:\n                if attachment.on_cloud:\n                    # add to removed_attachments so later we can delete them\n                    self.__removed_attachments.append(\n                        attachment)\n        self.__attachments = new_attachments\n        self._update_parent_attachments()\n        self._track_changes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads this message s attachments into memory.", "response": "def download_attachments(self):\n        \"\"\" Downloads this message attachments into memory.\n        Need a call to 'attachment.save' to save them on disk.\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        if not self._parent.has_attachments:\n            log.debug(\n                'Parent {} has no attachments, skipping out early.'.format(\n                    self._parent.__class__.__name__))\n            return False\n\n        if not self._parent.object_id:\n            raise RuntimeError(\n                'Attempted to download attachments of an unsaved {}'.format(\n                    self._parent.__class__.__name__))\n\n        url = self.build_url(self._endpoints.get('attachments').format(\n            id=self._parent.object_id))\n\n        response = self._parent.con.get(url)\n        if not response:\n            return False\n\n        attachments = response.json().get('value', [])\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        self.untrack = True\n        self.add({self._cloud_data_key: attachments})\n        self.untrack = False\n\n        # TODO: when it's a item attachment the attachment itself\n        # is not downloaded. We must download it...\n        # TODO: idea: retrieve the attachments ids' only with\n        # select and then download one by one.\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _update_attachments_to_cloud(self):\n        url = self.build_url(self._endpoints.get('attachments').format(\n            id=self._parent.object_id))\n\n        # ! potentially several api requests can be made by this method.\n\n        for attachment in self.__attachments:\n            if attachment.on_cloud is False:\n                # upload attachment:\n                response = self._parent.con.post(url,\n                                                 data=attachment.to_api_data())\n                if not response:\n                    return False\n\n                data = response.json()\n\n                # update attachment data\n                attachment.attachment_id = data.get('id')\n                attachment.content = data.get(self._cc('contentBytes'), None)\n                attachment.on_cloud = True\n\n        for attachment in self.__removed_attachments:\n            if attachment.on_cloud and attachment.attachment_id is not None:\n                # delete attachment\n                url = self.build_url(self._endpoints.get('attachment').format(\n                    id=self._parent.object_id, ida=attachment.attachment_id))\n\n                response = self._parent.con.delete(url)\n                if not response:\n                    return False\n\n        self.__removed_attachments = []  # reset the removed attachments\n\n        log.debug('Successfully updated attachments on {}'.format(\n            self._parent.object_id))\n\n        return True", "response": "Push new unsaved attachments to the cloud and remove removed attachments. This method should not be called for draft messages."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating the track_changes on the parent to reflect this field needed update on the a", "response": "def _track_changes(self):\n        \"\"\" Update the track_changes on the parent to reflect a\n        needed update on this field \"\"\"\n        if self._field and getattr(self._parent, '_track_changes',\n                                   None) is not None:\n            self._parent._track_changes.add(self._field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _track_changes(self):\n        if self._field and getattr(self._parent, '_track_changes',\n                                   None) is not None and self.untrack is False:\n            self._parent._track_changes.add(self._field)", "response": "Update the track_changes on the parent to reflect this field needed update on the a\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds the supplied recipients to the exiting list getWorkspace", "response": "def add(self, recipients):\n        \"\"\" Add the supplied recipients to the exiting list\n\n        :param recipients: list of either address strings or\n         tuples (name, address) or dictionary elements\n        :type recipients: list[str] or list[tuple] or list[dict]\n        \"\"\"\n\n        if recipients:\n            if isinstance(recipients, str):\n                self._recipients.append(\n                    Recipient(address=recipients, parent=self._parent,\n                              field=self._field))\n            elif isinstance(recipients, Recipient):\n                self._recipients.append(recipients)\n            elif isinstance(recipients, tuple):\n                name, address = recipients\n                if address:\n                    self._recipients.append(\n                        Recipient(address=address, name=name,\n                                  parent=self._parent, field=self._field))\n            elif isinstance(recipients, list):\n                for recipient in recipients:\n                    self.add(recipient)\n            else:\n                raise ValueError('Recipients must be an address string, a '\n                                 'Recipient instance, a (name, address) '\n                                 'tuple or a list')\n            self._track_changes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove(self, address):\n        recipients = []\n        if isinstance(address, str):\n            address = {address}  # set\n        elif isinstance(address, (list, tuple)):\n            address = set(address)\n\n        for recipient in self._recipients:\n            if recipient.address not in address:\n                recipients.append(recipient)\n        if len(recipients) != len(self._recipients):\n            self._track_changes()\n        self._recipients = recipients", "response": "Removes an address or multiple addresses\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the first recipient found with a non blank address", "response": "def get_first_recipient_with_address(self):\n        \"\"\" Returns the first recipient found with a non blank address\n\n        :return: First Recipient\n        :rtype: Recipient\n        \"\"\"\n        recipients_with_address = [recipient for recipient in self._recipients\n                                   if recipient.address]\n        if recipients_with_address:\n            return recipients_with_address[0]\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ntransforms a list of recipients from cloud data to object data", "response": "def _recipients_from_cloud(self, recipients, field=None):\n        \"\"\" Transform a recipient from cloud data to object data \"\"\"\n        recipients_data = []\n        for recipient in recipients:\n            recipients_data.append(\n                self._recipient_from_cloud(recipient, field=field))\n        return Recipients(recipients_data, parent=self, field=field)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms a recipient from cloud data to object data", "response": "def _recipient_from_cloud(self, recipient, field=None):\n        \"\"\" Transform a recipient from cloud data to object data \"\"\"\n\n        if recipient:\n            recipient = recipient.get(self._cc('emailAddress'),\n                                      recipient if isinstance(recipient,\n                                                              dict) else {})\n            address = recipient.get(self._cc('address'), '')\n            name = recipient.get(self._cc('name'), '')\n            return Recipient(address=address, name=name, parent=self,\n                             field=field)\n        else:\n            return Recipient()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntransforms a Recipient object to a cloud dict", "response": "def _recipient_to_cloud(self, recipient):\n        \"\"\" Transforms a Recipient object to a cloud dict \"\"\"\n        data = None\n        if recipient:\n            data = {self._cc('emailAddress'): {\n                self._cc('address'): recipient.address}}\n            if recipient.name:\n                data[self._cc('emailAddress')][\n                    self._cc('name')] = recipient.name\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _parse_resource(resource):\n        resource = resource.strip() if resource else resource\n        if resource in {ME_RESOURCE, USERS_RESOURCE}:\n            return resource\n        elif '@' in resource and not resource.startswith(USERS_RESOURCE):\n            # when for example accessing a shared mailbox the\n            # resource is set to the email address. we have to prefix\n            # the email with the resource 'users/' so --> 'users/email_address'\n            return '{}/{}'.format(USERS_RESOURCE, resource)\n        else:\n            return resource", "response": "Parses and completes resource information"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_date_time_time_zone(self, date_time_time_zone):\n        if date_time_time_zone is None:\n            return None\n\n        local_tz = self.protocol.timezone\n        if isinstance(date_time_time_zone, dict):\n            try:\n                timezone = pytz.timezone(\n                    get_iana_tz(date_time_time_zone.get(self._cc('timeZone'), 'UTC')))\n            except pytz.UnknownTimeZoneError:\n                timezone = local_tz\n            date_time = date_time_time_zone.get(self._cc('dateTime'), None)\n            try:\n                date_time = timezone.localize(parse(date_time)) if date_time else None\n            except OverflowError as e:\n                log.debug('Could not parse dateTimeTimeZone: {}. Error: {}'.format(date_time_time_zone, str(e)))\n                date_time = None\n\n            if date_time and timezone != local_tz:\n                date_time = date_time.astimezone(local_tz)\n        else:\n            # Outlook v1.0 api compatibility (fallback to datetime string)\n            try:\n                date_time = local_tz.localize(parse(date_time_time_zone)) if date_time_time_zone else None\n            except Exception as e:\n                log.debug('Could not parse dateTimeTimeZone: {}. Error: {}'.format(date_time_time_zone, str(e)))\n                date_time = None\n\n        return date_time", "response": "Parses and converts a dateTimeTimeZone resource into a datetime object"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _build_date_time_time_zone(self, date_time):\n        timezone = date_time.tzinfo.zone if date_time.tzinfo is not None else None\n        return {\n            self._cc('dateTime'): date_time.strftime('%Y-%m-%dT%H:%M:%S'),\n            self._cc('timeZone'): get_windows_tz(timezone or self.protocol.timezone)\n        }", "response": "Converts a datetime to a dateTimeTimeZone resource"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef select(self, *attributes):\n        if attributes:\n            for attribute in attributes:\n                attribute = self.protocol.convert_case(\n                    attribute) if attribute and isinstance(attribute,\n                                                           str) else None\n                if attribute:\n                    if '/' in attribute:\n                        # only parent attribute can be selected\n                        attribute = attribute.split('/')[0]\n                    self._selects.add(attribute)\n        else:\n            if self._attribute:\n                self._selects.add(self._attribute)\n\n        return self", "response": "Adds the attribute to the select list"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd the relationships that should be expanded with the $expand parameter.", "response": "def expand(self, *relationships):\n        \"\"\" Adds the relationships (e.g. \"event\" or \"attachments\")\n        that should be expanded with the $expand parameter\n        Important: The ApiComponent using this should know how to handle this relationships.\n            eg: Message knows how to handle attachments, and event (if it's an EventMessage).\n        Important: When using expand on multi-value relationships a max of 20 items will be returned.\n        :param str relationships: the relationships tuple to expand.\n        :rtype: Query\n        \"\"\"\n\n        for relationship in relationships:\n            if relationship == 'event':\n                relationship = '{}/event'.format(self.protocol.get_service_keyword('event_message_type'))\n            self._expands.add(relationship)\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef search(self, text):\n        if text is None:\n            self._search = None\n        else:\n            # filters an order are not allowed\n            self.clear_filters()\n            self.clear_order()\n            self._search = '\"{}\"'.format(text)\n\n        return self", "response": "Perform a search on the resource."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_filters(self):\n        if self._filters:\n            filters_list = self._filters\n            if isinstance(filters_list[-1], Enum):\n                filters_list = filters_list[:-1]\n            return ' '.join(\n                [fs.value if isinstance(fs, Enum) else fs[1] for fs in\n                 filters_list]).strip()\n        else:\n            return None", "response": "Returns the result of the get_filters method"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the order of the attributes in the order_by clause of the entries in the resource.", "response": "def get_order(self):\n        \"\"\" Returns the result order by clauses\n\n        :rtype: str or None\n        \"\"\"\n        # first get the filtered attributes in order as they must appear\n        # in the order_by first\n        if not self.has_order:\n            return None\n        filter_order_clauses = OrderedDict([(filter_attr[0], None)\n                                            for filter_attr in self._filters\n                                            if isinstance(filter_attr, tuple)])\n\n        # any order_by attribute that appears in the filters is ignored\n        order_by_dict = self._order_by.copy()\n        for filter_oc in filter_order_clauses.keys():\n            direction = order_by_dict.pop(filter_oc, None)\n            filter_order_clauses[filter_oc] = direction\n\n        filter_order_clauses.update(\n            order_by_dict)  # append any remaining order_by clause\n\n        if filter_order_clauses:\n            return ','.join(['{} {}'.format(attribute,\n                                            direction if direction else '')\n                            .strip()\n                             for attribute, direction in\n                             filter_order_clauses.items()])\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new(self, attribute, operation=ChainOperator.AND):\n        if isinstance(operation, str):\n            operation = ChainOperator(operation)\n        self._chain = operation\n        self._attribute = self._get_mapping(attribute) if attribute else None\n        self._negation = False\n        return self", "response": "Combine with a new query\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clear(self):\n        self._filters = []\n        self._order_by = OrderedDict()\n        self._selects = set()\n        self._negation = False\n        self._attribute = None\n        self._chain = None\n        self._search = None\n\n        return self", "response": "Clear all the items in the cache."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef chain(self, operation=ChainOperator.AND):\n        if isinstance(operation, str):\n            operation = ChainOperator(operation)\n        self._chain = operation\n        return self", "response": "Start a chain operation\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove_filter(self, filter_attr):\n        filter_attr = self._get_mapping(filter_attr)\n        new_filters = []\n        remove_chain = False\n\n        for flt in self._filters:\n            if isinstance(flt, tuple):\n                if flt[0] == filter_attr:\n                    remove_chain = True\n                else:\n                    new_filters.append(flt)\n            else:\n                # this is a ChainOperator\n                if remove_chain is False:\n                    new_filters.append(flt)\n                else:\n                    remove_chain = False\n\n        self._filters = new_filters", "response": "Removes a filter given the attribute name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_filter_word(self, word):\n        if isinstance(word, str):\n            word = \"'{}'\".format(word)\n        elif isinstance(word, dt.date):\n            if isinstance(word, dt.datetime):\n                if word.tzinfo is None:\n                    # if it's a naive datetime, localize the datetime.\n                    word = self.protocol.timezone.localize(\n                        word)  # localize datetime into local tz\n                if word.tzinfo != pytz.utc:\n                    word = word.astimezone(\n                        pytz.utc)  # transform local datetime to utc\n            if '/' in self._attribute:\n                # TODO: this is a fix for the case when the parameter\n                #  filtered is a string instead a dateTimeOffset\n                #  but checking the '/' is not correct, but it will\n                #  differentiate for now the case on events:\n                #  start/dateTime (date is a string here) from\n                #  the case on other dates such as\n                #  receivedDateTime (date is a dateTimeOffset)\n                word = \"'{}'\".format(\n                    word.isoformat())  # convert datetime to isoformat.\n            else:\n                word = \"{}\".format(\n                    word.isoformat())  # convert datetime to isoformat\n        elif isinstance(word, bool):\n            word = str(word).lower()\n        return word", "response": "Converts the word parameter into the correct format"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef logical_operator(self, operation, word):\n        word = self._parse_filter_word(word)\n        self._add_filter(\n            *self._prepare_sentence(self._attribute, operation, word,\n                                    self._negation))\n        return self", "response": "Apply a logical operator to a resource item."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef function(self, function_name, word):\n        word = self._parse_filter_word(word)\n\n        self._add_filter(\n            *self._prepare_function(function_name, self._attribute, word,\n                                    self._negation))\n        return self", "response": "Apply a function on given word"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\napplying a function or a logical operation to the attribute of the current object", "response": "def iterable(self, iterable_name, *, collection, attribute, word, func=None,\n                 operation=None):\n        \"\"\" Performs a filter with the OData 'iterable_name' keyword\n        on the collection\n\n        For example:\n        q.iterable('any', collection='email_addresses', attribute='address',\n        operation='eq', word='george@best.com')\n\n        will transform to a filter such as:\n        emailAddresses/any(a:a/address eq 'george@best.com')\n\n        :param str iterable_name: the OData name of the iterable\n        :param str collection: the collection to apply the any keyword on\n        :param str attribute: the attribute of the collection to check\n        :param str word: the word to check\n        :param str func: the logical function to apply to the attribute inside\n         the collection\n        :param str operation: the logical operation to apply to the attribute\n         inside the collection\n        :rtype: Query\n        \"\"\"\n\n        if func is None and operation is None:\n            raise ValueError('Provide a function or an operation to apply')\n        elif func is not None and operation is not None:\n            raise ValueError(\n                'Provide either a function or an operation but not both')\n\n        current_att = self._attribute\n        self._attribute = iterable_name\n\n        word = self._parse_filter_word(word)\n        collection = self._get_mapping(collection)\n        attribute = self._get_mapping(attribute)\n\n        if func is not None:\n            sentence = self._prepare_function(func, attribute, word)\n        else:\n            sentence = self._prepare_sentence(attribute, operation, word)\n\n        filter_str, attrs = sentence\n\n        filter_data = '{}/{}(a:a/{})'.format(collection, iterable_name, filter_str), attrs\n        self._add_filter(*filter_data)\n\n        self._attribute = current_att\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef any(self, *, collection, attribute, word, func=None, operation=None):\n\n        return self.iterable('any', collection=collection, attribute=attribute,\n                             word=word, func=func, operation=operation)", "response": "Returns a QuerySet containing only the elements that match the OData any keyword."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a QuerySet containing only the items in the specified collection that match the specified criteria.", "response": "def all(self, *, collection, attribute, word, func=None, operation=None):\n        \"\"\" Performs a filter with the OData 'all' keyword on the collection\n\n        For example:\n        q.any(collection='email_addresses', attribute='address',\n        operation='eq', word='george@best.com')\n\n        will transform to a filter such as:\n\n        emailAddresses/all(a:a/address eq 'george@best.com')\n\n        :param str collection: the collection to apply the any keyword on\n        :param str attribute: the attribute of the collection to check\n        :param str word: the word to check\n        :param str func: the logical function to apply to the attribute\n         inside the collection\n        :param str operation: the logical operation to apply to the\n         attribute inside the collection\n        :rtype: Query\n        \"\"\"\n\n        return self.iterable('all', collection=collection, attribute=attribute,\n                             word=word, func=func, operation=operation)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\napply a order_by clause to the current Query object.", "response": "def order_by(self, attribute=None, *, ascending=True):\n        \"\"\" Applies a order_by clause\n\n        :param str attribute: attribute to apply on\n        :param bool ascending: should it apply ascending order or descending\n        :rtype: Query\n        \"\"\"\n        attribute = self._get_mapping(attribute) or self._attribute\n        if attribute:\n            self._order_by[attribute] = None if ascending else 'desc'\n        else:\n            raise ValueError(\n                'Attribute property needed. call on_attribute(attribute) '\n                'or new(attribute)')\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_my_tasks(self, *args):\n\n        url = self.build_url(self._endpoints.get('get_my_tasks'))\n\n        response = self.con.get(url)\n\n        if not response:\n            return None\n\n        data = response.json()\n\n        return [\n            self.task_constructor(parent=self, **{self._cloud_data_key: site})\n            for site in data.get('value', [])]", "response": "Returns a list of open planner tasks assigned to me\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a valid pytz TimeZone from a given Windows TimeZone", "response": "def get_iana_tz(windows_tz):\n    \"\"\" Returns a valid pytz TimeZone (Iana/Olson Timezones) from a given\n    windows TimeZone\n\n    :param windows_tz: windows format timezone usually returned by\n     microsoft api response\n    :return:\n    :rtype:\n    \"\"\"\n    timezone = WIN_TO_IANA.get(windows_tz)\n    if timezone is None:\n        # Nope, that didn't work. Try adding \"Standard Time\",\n        # it seems to work a lot of times:\n        timezone = WIN_TO_IANA.get(windows_tz + ' Standard Time')\n\n    # Return what we have.\n    if timezone is None:\n        raise pytz.UnknownTimeZoneError(\n            \"Can't find Windows TimeZone \" + windows_tz)\n\n    return timezone"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a valid windows TimeZone from a given pytz object", "response": "def get_windows_tz(iana_tz):\n    \"\"\" Returns a valid windows TimeZone from a given pytz TimeZone\n    (Iana/Olson Timezones)\n    Note: Windows Timezones are SHIT!... no ... really THEY ARE\n    HOLY FUCKING SHIT!.\n    \"\"\"\n    timezone = IANA_TO_WIN.get(\n        iana_tz.zone if isinstance(iana_tz, tzinfo) else iana_tz)\n    if timezone is None:\n        raise pytz.UnknownTimeZoneError(\n            \"Can't find Iana TimeZone \" + iana_tz.zone)\n\n    return timezone"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update_fields(self, updates):\n\n        for field in updates:\n            if self._valid_field(field):\n                self._track_changes.add(field)\n            else:\n                raise ValueError('\"{}\" is not a valid internal field name'.format(field))\n\n        # Update existing instance of fields, or create a fields instance if needed\n        if self.fields:\n            self.fields.update(updates)\n        else:\n            self.fields = updates", "response": "Update the value for a field in the listitem\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the updated fields to the cloud", "response": "def save_updates(self):\n        \"\"\"Save the updated fields to the cloud\"\"\"\n\n        if not self._track_changes:\n            return True  # there's nothing to update\n\n        url = self.build_url(self._endpoints.get('update_list_item').format(item_id=self.object_id))\n        update = {field: value for field, value in self.fields.items()\n                  if self._cc(field) in self._track_changes}\n\n        response = self.con.patch(url, update)\n        if not response:\n            return False\n        self._clear_tracker()\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a collection of Sharepoint Items with the specified attributes", "response": "def get_items(self):\n        \"\"\" Returns a collection of Sharepoint Items\n\n        :rtype: list[SharepointListItem]\n        \"\"\"\n        url = self.build_url(self._endpoints.get('get_items'))\n\n        response = self.con.get(url)\n\n        if not response:\n            return []\n\n        data = response.json()\n\n        return [self.list_item_constructor(parent=self,\n                                           **{self._cloud_data_key: item})\n                for item in data.get('value', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list item based on the id", "response": "def get_item_by_id(self, item_id):\n        \"\"\" Returns a sharepoint list item based on id\"\"\"\n\n        url = self.build_url(self._endpoints.get('get_item_by_id').format(item_id=item_id))\n\n        response = self.con.get(url)\n\n        if not response:\n            return []\n\n        data = response.json()\n\n        return self.list_item_constructor(parent=self, **{self._cloud_data_key: data})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the list of sharepoint list columns", "response": "def get_list_columns(self):\n        \"\"\" Returns the sharepoint list columns \"\"\"\n\n        url = self.build_url(self._endpoints.get('get_list_columns'))\n\n        response = self.con.get(url)\n\n        if not response:\n            return []\n\n        data = response.json()\n\n        return [self.list_column_constructor(parent=self, **{self._cloud_data_key: column})\n                for column in data.get('value', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_list_item(self, new_data):\n\n        url = self.build_url(self._endpoints.get('get_items'))\n\n        response = self.con.post(url, {'fields': new_data})\n        if not response:\n            return False\n\n        data = response.json()\n\n        return self.list_item_constructor(parent=self, **{self._cloud_data_key: data})", "response": "Creates a new list item with the given data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_list_item(self, item_id):\n\n        url = self.build_url(self._endpoints.get('get_item_by_id').format(item_id=item_id))\n\n        response = self.con.delete(url)\n\n        return bool(response)", "response": "Deletes an existing list item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a collection of document libraries for this site.", "response": "def list_document_libraries(self, limit=None, *, query=None, order_by=None,\n                                batch=None):\n        \"\"\" Returns a collection of document libraries for this site\n        (a collection of Drive instances)\n\n        :param int limit: max no. of items to get. Over 999 uses batch.\n        :param query: applies a OData filter to the request\n        :type query: Query or str\n        :param order_by: orders the result set based on this condition\n        :type order_by: Query or str\n        :param int batch: batch size, retrieves items in\n         batches allowing to retrieve more items than the limit.\n        :return: list of items in this folder\n        :rtype: list[Drive] or Pagination\n        \"\"\"\n        return self.site_storage.get_drives(limit=limit, query=query,\n                                            order_by=order_by, batch=batch)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_subsites(self):\n        url = self.build_url(\n            self._endpoints.get('get_subsites').format(id=self.object_id))\n\n        response = self.con.get(url)\n        if not response:\n            return []\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return [self.__class__(parent=self, **{self._cloud_data_key: site}) for\n                site in data.get('value', [])]", "response": "Returns a list of subsites defined for this site"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_lists(self):\n        url = self.build_url(self._endpoints.get('get_lists'))\n\n        response = self.con.get(url)\n        if not response:\n            return []\n\n        data = response.json()\n\n        return [self.list_constructor(parent=self, **{self._cloud_data_key: lst}) for lst in data.get('value', [])]", "response": "Returns a collection of lists within this site"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_list_by_name(self, display_name):\n\n        if not display_name:\n            raise ValueError('Must provide a valid list display name')\n\n        url = self.build_url(self._endpoints.get('get_list_by_name').format(display_name=display_name))\n\n        response = self.con.get(url)\n        if not response:\n            return []\n\n        data = response.json()\n\n        return self.list_constructor(parent=self, **{self._cloud_data_key: data})", "response": "Returns a list of sharepoint items based on the display name of the list"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch a sharepoint host for sites with the provided keyword.", "response": "def search_site(self, keyword):\n        \"\"\" Search a sharepoint host for sites with the provided keyword\n\n        :param keyword: a keyword to search sites\n        :rtype: list[Site]\n        \"\"\"\n        if not keyword:\n            raise ValueError('Must provide a valid keyword')\n\n        url = self.build_url(\n            self._endpoints.get('search').format(keyword=keyword))\n\n        response = self.con.get(url)\n        if not response:\n            return []\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return [\n            self.site_constructor(parent=self, **{self._cloud_data_key: site})\n            for site in data.get('value', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new Site object", "response": "def get_site(self, *args):\n        \"\"\" Returns a sharepoint site\n\n        :param args: It accepts multiple ways of retrieving a site:\n\n         get_site(host_name): the host_name: host_name ej.\n         'contoso.sharepoint.com' or 'root'\n\n         get_site(site_id): the site_id: a comma separated string of\n         (host_name, site_collection_id, site_id)\n\n         get_site(host_name, path_to_site): host_name ej. 'contoso.\n         sharepoint.com', path_to_site: a url path (with a leading slash)\n\n         get_site(host_name, site_collection_id, site_id):\n         host_name ej. 'contoso.sharepoint.com'\n        :rtype: Site\n        \"\"\"\n        num_args = len(args)\n        if num_args == 1:\n            site = args[0]\n        elif num_args == 2:\n            host_name, path_to_site = args\n            path_to_site = '/' + path_to_site if not path_to_site.startswith(\n                '/') else path_to_site\n            site = '{}:{}:'.format(host_name, path_to_site)\n        elif num_args == 3:\n            site = ','.join(args)\n        else:\n            raise ValueError('Incorrect number of arguments')\n\n        url = self.build_url(self._endpoints.get('get_site').format(id=site))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return self.site_constructor(parent=self,\n                                     **{self._cloud_data_key: data})"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of child folders matching the query", "response": "def get_folders(self, limit=None, *, query=None, order_by=None, batch=None):\n        \"\"\" Returns a list of child folders matching the query\n\n        :param int limit: max no. of folders to get. Over 999 uses batch.\n        :param query: applies a filter to the request such as\n         \"displayName eq 'HelloFolder'\"\n        :type query: Query or str\n        :param order_by: orders the result set based on this condition\n        :type order_by: Query or str\n        :param int batch: batch size, retrieves items in\n         batches allowing to retrieve more items than the limit.\n        :return: list of folders\n        :rtype: list[mailbox.Folder] or Pagination\n        \"\"\"\n\n        if self.root:\n            url = self.build_url(self._endpoints.get('root_folders'))\n        else:\n            url = self.build_url(\n                self._endpoints.get('child_folders').format(id=self.folder_id))\n\n        if limit is None or limit > self.protocol.max_top_value:\n            batch = self.protocol.max_top_value\n\n        params = {'$top': batch if batch else limit}\n\n        if order_by:\n            params['$orderby'] = order_by\n\n        if query:\n            if isinstance(query, str):\n                params['$filter'] = query\n            else:\n                params.update(query.as_params())\n\n        response = self.con.get(url, params=params)\n        if not response:\n            return []\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        self_class = getattr(self, 'folder_constructor', type(self))\n        folders = [self_class(parent=self, **{self._cloud_data_key: folder}) for\n                   folder in data.get('value', [])]\n        next_link = data.get(NEXT_LINK_KEYWORD, None)\n        if batch and next_link:\n            return Pagination(parent=self, data=folders, constructor=self_class,\n                              next_link=next_link, limit=limit)\n        else:\n            return folders"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a single Message object from the query result.", "response": "def get_message(self, object_id=None, query=None, *, download_attachments=False):\n        \"\"\" Get one message from the query result.\n         A shortcut to get_messages with limit=1\n        :param object_id: the message id to be retrieved.\n        :param query: applies a filter to the request such as\n         \"displayName eq 'HelloFolder'\"\n        :type query: Query or str\n        :param bool download_attachments: whether or not to download attachments\n        :return: one Message\n        :rtype: Message or None\n        \"\"\"\n        if object_id is not None and query is not None:\n            raise ValueError('Must provide object id or query but not both.')\n\n        if object_id is not None:\n            url = self.build_url(self._endpoints.get('message').format(id=object_id))\n            response = self.con.get(url)\n            if not response:\n                return None\n\n            message = response.json()\n\n            return self.message_constructor(parent=self,\n                                            download_attachments=download_attachments,\n                                            **{self._cloud_data_key: message})\n\n        else:\n            messages = list(self.get_messages(limit=1, query=query,\n                                              download_attachments=download_attachments))\n\n            return messages[0] if messages else None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndownloading all messages from this folder.", "response": "def get_messages(self, limit=25, *, query=None, order_by=None, batch=None,\n                     download_attachments=False):\n        \"\"\"\n        Downloads messages from this folder\n\n        :param int limit: limits the result set. Over 999 uses batch.\n        :param query: applies a filter to the request such as\n         \"displayName eq 'HelloFolder'\"\n        :type query: Query or str\n        :param order_by: orders the result set based on this condition\n        :type order_by: Query or str\n        :param int batch: batch size, retrieves items in\n         batches allowing to retrieve more items than the limit.\n        :param bool download_attachments: whether or not to download attachments\n        :return: list of messages\n        :rtype: list[Message] or Pagination\n        \"\"\"\n\n        if self.root:\n            url = self.build_url(self._endpoints.get('root_messages'))\n        else:\n            url = self.build_url(self._endpoints.get('folder_messages').format(\n                id=self.folder_id))\n\n        if limit is None or limit > self.protocol.max_top_value:\n            batch = self.protocol.max_top_value\n\n        params = {'$top': batch if batch else limit}\n\n        if order_by:\n            params['$orderby'] = order_by\n\n        if query:\n            if isinstance(query, str):\n                params['$filter'] = query\n            else:\n                params.update(query.as_params())\n\n        response = self.con.get(url, params=params)\n        if not response:\n            return iter(())\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        messages = (self.message_constructor(\n            parent=self,\n            download_attachments=download_attachments,\n            **{self._cloud_data_key: message})\n            for message in data.get('value', []))\n\n        next_link = data.get(NEXT_LINK_KEYWORD, None)\n        if batch and next_link:\n            return Pagination(parent=self, data=messages,\n                              constructor=self.message_constructor,\n                              next_link=next_link, limit=limit,\n                              download_attachments=download_attachments)\n        else:\n            return messages"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting a single folder by it s id or name.", "response": "def get_folder(self, *, folder_id=None, folder_name=None):\n        \"\"\" Get a folder by it's id or name\n\n        :param str folder_id: the folder_id to be retrieved.\n         Can be any folder Id (child or not)\n        :param str folder_name: the folder name to be retrieved.\n         Must be a child of this folder.\n        :return: a single folder\n        :rtype: mailbox.Folder or None\n        \"\"\"\n        if folder_id and folder_name:\n            raise RuntimeError('Provide only one of the options')\n\n        if not folder_id and not folder_name:\n            raise RuntimeError('Provide one of the options')\n\n        if folder_id:\n            # get folder by it's id, independent of the parent of this folder_id\n            url = self.build_url(\n                self._endpoints.get('get_folder').format(id=folder_id))\n            params = None\n        else:\n            # get folder by name. Only looks up in child folders.\n            if self.root:\n                url = self.build_url(self._endpoints.get('root_folders'))\n            else:\n                url = self.build_url(\n                    self._endpoints.get('child_folders').format(\n                        id=self.folder_id))\n            params = {'$filter': \"{} eq '{}'\".format(self._cc('displayName'),\n                                                     folder_name), '$top': 1}\n\n        response = self.con.get(url, params=params)\n        if not response:\n            return None\n\n        if folder_id:\n            folder = response.json()\n        else:\n            folder = response.json().get('value')\n            folder = folder[0] if folder else None\n            if folder is None:\n                return None\n\n        self_class = getattr(self, 'folder_constructor', type(self))\n        # Everything received from cloud must be passed as self._cloud_data_key\n        # We don't pass parent, as this folder may not be a child of self.\n        return self_class(con=self.con, protocol=self.protocol,\n                          main_resource=self.main_resource,\n                          **{self._cloud_data_key: folder})"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef refresh_folder(self, update_parent_if_changed=False):\n        folder_id = getattr(self, 'folder_id', None)\n        if self.root or folder_id is None:\n            return False\n\n        folder = self.get_folder(folder_id=folder_id)\n        if folder is None:\n            return False\n\n        self.name = folder.name\n        if folder.parent_id and self.parent_id:\n            if folder.parent_id != self.parent_id:\n                self.parent_id = folder.parent_id\n                self.parent = (self.get_parent_folder()\n                               if update_parent_if_changed else None)\n        self.child_folders_count = folder.child_folders_count\n        self.unread_items_count = folder.unread_items_count\n        self.total_items_count = folder.total_items_count\n        self.updated_at = folder.updated_at\n\n        return True", "response": "Re - download folder data\nApplicationPropertyOf Inbox Folder will be unable to download its own data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the parent folder from the attribute self. parent or getting it from the cloud", "response": "def get_parent_folder(self):\n        \"\"\" Get the parent folder from attribute self.parent or\n        getting it from the cloud\n\n        :return: Parent Folder\n        :rtype: mailbox.Folder or None\n        \"\"\"\n        if self.root:\n            return None\n        if self.parent:\n            return self.parent\n\n        if self.parent_id:\n            self.parent = self.get_folder(folder_id=self.parent_id)\n        return self.parent"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the name of the current folder", "response": "def update_folder_name(self, name, update_folder_data=True):\n        \"\"\" Change this folder name\n\n        :param str name: new name to change to\n        :param bool update_folder_data: whether or not to re-fetch the data\n        :return: Updated or Not\n        :rtype: bool\n        \"\"\"\n        if self.root:\n            return False\n        if not name:\n            return False\n\n        url = self.build_url(\n            self._endpoints.get('get_folder').format(id=self.folder_id))\n\n        response = self.con.patch(url, data={self._cc('displayName'): name})\n        if not response:\n            return False\n\n        self.name = name\n        if not update_folder_data:\n            return True\n\n        folder = response.json()\n\n        self.name = folder.get(self._cc('displayName'), '')\n        self.parent_id = folder.get(self._cc('parentFolderId'), None)\n        self.child_folders_count = folder.get(self._cc('childFolderCount'), 0)\n        self.unread_items_count = folder.get(self._cc('unreadItemCount'), 0)\n        self.total_items_count = folder.get(self._cc('totalItemCount'), 0)\n        self.updated_at = dt.datetime.now()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncopies this folder and its contents to another folder", "response": "def copy_folder(self, to_folder):\n        \"\"\" Copy this folder and it's contents to into another folder\n\n        :param to_folder: the destination Folder/folder_id to copy into\n        :type to_folder: mailbox.Folder or str\n        :return: The new folder after copying\n        :rtype: mailbox.Folder or None\n        \"\"\"\n        to_folder_id = to_folder.folder_id if isinstance(to_folder,\n                                                         Folder) else to_folder\n\n        if self.root or not self.folder_id or not to_folder_id:\n            return None\n\n        url = self.build_url(\n            self._endpoints.get('copy_folder').format(id=self.folder_id))\n\n        response = self.con.post(url,\n                                 data={self._cc('destinationId'): to_folder_id})\n        if not response:\n            return None\n\n        folder = response.json()\n\n        self_class = getattr(self, 'folder_constructor', type(self))\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self_class(con=self.con, main_resource=self.main_resource,\n                          **{self._cloud_data_key: folder})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef move_folder(self, to_folder, *, update_parent_if_changed=True):\n        to_folder_id = to_folder.folder_id if isinstance(to_folder,\n                                                         Folder) else to_folder\n\n        if self.root or not self.folder_id or not to_folder_id:\n            return False\n\n        url = self.build_url(\n            self._endpoints.get('move_folder').format(id=self.folder_id))\n\n        response = self.con.post(url,\n                                 data={self._cc('destinationId'): to_folder_id})\n        if not response:\n            return False\n\n        folder = response.json()\n\n        parent_id = folder.get(self._cc('parentFolderId'), None)\n\n        if parent_id and self.parent_id:\n            if parent_id != self.parent_id:\n                self.parent_id = parent_id\n                self.parent = (self.get_parent_folder()\n                               if update_parent_if_changed else None)\n\n        return True", "response": "Moves this folder to another folder"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef new_message(self):\n\n        draft_message = self.message_constructor(parent=self, is_draft=True)\n\n        if self.root:\n            draft_message.folder_id = OutlookWellKnowFolderNames.DRAFTS.value\n        else:\n            draft_message.folder_id = self.folder_id\n\n        return draft_message", "response": "Creates a new draft message under this folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a stored message in the store", "response": "def delete_message(self, message):\n        \"\"\" Deletes a stored message\n\n        :param message: message/message_id to delete\n        :type message: Message or str\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        message_id = message.object_id if isinstance(message,\n                                                     Message) else message\n\n        if message_id is None:\n            raise RuntimeError('Provide a valid Message or a message id')\n\n        url = self.build_url(\n            self._endpoints.get('message').format(id=message_id))\n\n        response = self.con.delete(url)\n\n        return bool(response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef inbox_folder(self):\n        return self.folder_constructor(parent=self, name='Inbox',\n                                       folder_id=OutlookWellKnowFolderNames\n                                       .INBOX.value)", "response": "Shortcut to get Inbox Folder instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef junk_folder(self):\n        return self.folder_constructor(parent=self, name='Junk',\n                                       folder_id=OutlookWellKnowFolderNames\n                                       .JUNK.value)", "response": "Shortcut to get Junk Folder instance"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef deleted_folder(self):\n        return self.folder_constructor(parent=self, name='DeletedItems',\n                                       folder_id=OutlookWellKnowFolderNames\n                                       .DELETED.value)", "response": "Shortcut to get DeletedItems Folder instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef drafts_folder(self):\n        return self.folder_constructor(parent=self, name='Drafts',\n                                       folder_id=OutlookWellKnowFolderNames\n                                       .DRAFTS.value)", "response": "Shortcut to get Drafts Folder instance"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_session(self):\n\n        url = self.build_url(self._endpoints.get('create_session'))\n        response = self.con.post(url, data={'persistChanges': self.persist})\n        if not response:\n            raise RuntimeError('Could not create session as requested by the user.')\n        data = response.json()\n        self.session_id = data.get('id')\n\n        return True", "response": "Request a new session id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close_session(self):\n\n        if self.session_id:\n            url = self.build_url(self._endpoints.get('close_session'))\n            response = self.con.post(url, headers={'workbook-session-id': self.session_id})\n            return bool(response)\n        return False", "response": "Closes the current session"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare_request(self, kwargs):\n        if self.session_id is not None:\n            actual = dt.datetime.now()\n\n            if (self.last_activity + self.inactivity_limit) < actual:\n                # session expired\n                if self.persist:\n                    # request new session\n                    self.create_session()\n                    actual = dt.datetime.now()\n                else:\n                    # raise error and recommend to manualy refresh session\n                    raise RuntimeError('A non Persistent Session is expired. '\n                                       'For consistency reasons this exception is raised. '\n                                       'Please try again with manual refresh of the session ')\n            self.last_activity = actual\n\n            headers = kwargs.get('headers')\n            if headers is None:\n                kwargs['headers'] = headers = {}\n            headers['workbook-session-id'] = self.session_id", "response": "Prepares the request headers and inactivity checks if the session is expired and if not raises a RuntimeError"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _load_data(self):\n        url = self.parent.build_url(self.parent._endpoints.get('format'))\n        response = self.parent.session.get(url)\n        if not response:\n            return False\n        data = response.json()\n\n        self._bold = data.get('bold', False)\n        self._color = data.get('color', '#000000')  # default black\n        self._italic = data.get('italic', False)\n        self._name = data.get('name', 'Calibri')  # default Calibri\n        self._size = data.get('size', 10)  # default 10\n        self._underline = data.get('underline', 'None')\n\n        self._loaded = True\n        return True", "response": "Loads the data into the instance attributes."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a dict to communicate with the server", "response": "def to_api_data(self, restrict_keys=None):\n        \"\"\" Returns a dict to communicate with the server\n\n        :param restrict_keys: a set of keys to restrict the returned data to\n        :rtype: dict\n        \"\"\"\n        cc = self.parent._cc  # alias\n        data = {\n            cc('bold'): self._bold,\n            cc('color'): self._color,\n            cc('italic'): self._italic,\n            cc('name'): self._name,\n            cc('size'): self._size,\n            cc('underline'): self._underline\n        }\n\n        if restrict_keys:\n            for key in list(data.keys()):\n                if key not in restrict_keys:\n                    del data[key]\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_api_data(self, restrict_keys=None):\n        cc = self._cc  # alias\n        data = {\n            cc('column_width'): self._column_width,\n            cc('horizontal_alignment'): self._horizontal_alignment,\n            cc('row_height'): self._row_height,\n            cc('vertical_alignment'): self._vertical_alignment,\n            cc('wrap_text'): self._wrap_text,\n        }\n\n        if restrict_keys:\n            for key in list(data.keys()):\n                if key not in restrict_keys:\n                    del data[key]\n        return data", "response": "Returns a dict to communicate with the server"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update(self):\n        if self._track_changes:\n            data = self.to_api_data(restrict_keys=self._track_changes)\n            if data:\n                response = self.session.patch(self.build_url(''), data=data)\n                if not response:\n                    return False\n                self._track_changes.clear()\n        if self._font._track_changes:\n            data = self._font.to_api_data(restrict_keys=self._font._track_changes)\n            if data:\n                response = self.session.patch(self.build_url(self._endpoints.get('font')), data=data)\n                if not response:\n                    return False\n                self._font._track_changes.clear()\n        if self._track_background_color:\n            if self._background_color is None:\n                url = self.build_url(self._endpoints.get('clear_fill'))\n                response = self.session.post(url)\n            else:\n                data = {'color': self._background_color}\n                url = self.build_url(self._endpoints.get('fill'))\n                response = self.session.patch(url, data=data)\n            if not response:\n                return False\n            self._track_background_color = False\n\n        return True", "response": "Updates this range format"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the background color from the fill color endpoint", "response": "def _load_background_color(self):\n        \"\"\" Loads the data related to the fill color \"\"\"\n        url = self.build_url(self._endpoints.get('fill'))\n        response = self.session.get(url)\n        if not response:\n            return None\n        data = response.json()\n        self._background_color = data.get('color', None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchanging the width of the columns of the current range to achieve the best fit based on the current data in the columns", "response": "def auto_fit_columns(self):\n        \"\"\" Changes the width of the columns of the current range\n         to achieve the best fit, based on the current data in the columns\n        \"\"\"\n        url = self.build_url(self._endpoints.get('auto_fit_columns'))\n        return bool(self.session.post(url))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a dict that can be used to communicate with the server", "response": "def to_api_data(self, restrict_keys=None):\n        \"\"\" Returns a dict to communicate with the server\n\n        :param restrict_keys: a set of keys to restrict the returned data to\n        :rtype: dict\n        \"\"\"\n        cc = self._cc  # alias\n        data = {\n            cc('column_hidden'): self._column_hidden,\n            cc('row_hidden'): self._row_hidden,\n            cc('formulas'): self._formulas,\n            cc('formulas_local'): self._formulas_local,\n            cc('formulas_r1_c1'): self._formulas_r1_c1,\n            cc('number_format'): self._number_format,\n            cc('values'): self._values,\n        }\n\n        if restrict_keys:\n            for key in list(data.keys()):\n                if key not in restrict_keys:\n                    del data[key]\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_range(self, endpoint, *args, method='GET', **kwargs):\n        if args:\n            url = self.build_url(self._endpoints.get(endpoint).format(*args))\n        else:\n            url = self.build_url(self._endpoints.get(endpoint))\n        if not kwargs:\n            kwargs = None\n        if method == 'GET':\n            response = self.session.get(url, params=kwargs)\n        elif method == 'POST':\n            response = self.session.post(url, data=kwargs)\n        if not response:\n            return None\n        return self.__class__(parent=self, **{self._cloud_data_key: response.json()})", "response": "Helper that returns another range"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_offset_range(self, row_offset, column_offset):\n        return self._get_range('offset_range', rowOffset=row_offset, columnOffset=column_offset)", "response": "Gets an object which represents a range that s offset from the specified range."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nclears the current set of items from the cache.", "response": "def clear(self, apply_to='all'):\n        \"\"\"\n        Clear range values, format, fill, border, etc.\n        :param str apply_to: Optional. Determines the type of clear action.\n         The possible values are: all, formats, contents.\n        \"\"\"\n        url = self.build_url(self._endpoints.get('clear_range'))\n        return bool(self.session.post(url, data={'applyTo': apply_to.capitalize()}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the cells associated with the range.", "response": "def delete(self, shift='up'):\n        \"\"\"\n        Deletes the cells associated with the range.\n        :param str shift: Optional. Specifies which way to shift the cells.\n         The possible values are: up, left.\n        \"\"\"\n        url = self.build_url(self._endpoints.get('delete_range'))\n        return bool(self.session.post(url, data={'shift': shift.capitalize()}))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmerges the range cells into one region in the worksheet.", "response": "def merge(self, across=False):\n        \"\"\"\n        Merge the range cells into one region in the worksheet.\n        :param bool across: Optional. Set True to merge cells in each row of the\n         specified range as separate merged cells.\n        \"\"\"\n        url = self.build_url(self._endpoints.get('merge_range'))\n        return bool(self.session.post(url, data={'across': across}))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates the range of the current object.", "response": "def update(self):\n        \"\"\" Update this range \"\"\"\n\n        if not self._track_changes:\n            return True  # there's nothing to update\n\n        data = self.to_api_data(restrict_keys=self._track_changes)\n        response = self.session.patch(self.build_url(''), data=data)\n        if not response:\n            return False\n\n        data = response.json()\n\n        for field in self._track_changes:\n            setattr(self, snakecase(field), data.get(field))\n        self._track_changes.clear()\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_format(self):\n        url = self.build_url(self._endpoints.get('get_format'))\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.range_format_constructor(parent=self, **{self._cloud_data_key: response.json()})", "response": "Returns a RangeFormat instance with the format of this range"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupdate the named range with the specified parameters.", "response": "def update(self, *, visible=None, comment=None):\n        \"\"\"\n        Updates this named range\n        :param bool visible: Specifies whether the object is visible or not\n        :param str comment: Represents the comment associated with this name\n        :return: Success or Failure\n        \"\"\"\n        if visible is None and comment is None:\n            raise ValueError('Provide \"visible\" or \"comment\" to update.')\n        data = {}\n        if visible is not None:\n            data['visible'] = visible\n        if comment is not None:\n            data['comment'] = comment\n        data = None if not data else data\n        response = self.session.patch(self.build_url(''), data=data)\n        if not response:\n            return False\n        data = response.json()\n\n        self.visible = data.get('visible', self.visible)\n        self.comment = data.get('comment', self.comment)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the row with the values provided.", "response": "def update(self, values):\n        \"\"\" Updates this row \"\"\"\n        response = self.session.patch(self.build_url(''), data={'values': values})\n        if not response:\n            return False\n        data = response.json()\n        self.values = data.get('values', self.values)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\napplies the given filter criteria on the given column.", "response": "def apply_filter(self, criteria):\n        \"\"\"\n        Apply the given filter criteria on the given column.\n        :param str criteria: the criteria to apply\n        criteria example:\n        {\n          \"color\": \"string\",\n          \"criterion1\": \"string\",\n          \"criterion2\": \"string\",\n          \"dynamicCriteria\": \"string\",\n          \"filterOn\": \"string\",\n          \"icon\": {\"@odata.type\": \"microsoft.graph.workbookIcon\"},\n          \"values\": {\"@odata.type\": \"microsoft.graph.Json\"}\n        }\n        \"\"\"\n        url = self.build_url(self._endpoints.get('apply_filter'))\n        return bool(self.session.post(url, data={'criteria': criteria}))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_filter(self):\n        q = self.q().select('name').expand('filter')\n        response = self.session.get(self.build_url(''), params=q.as_params())\n        if not response:\n            return None\n        data = response.json()\n        return data.get('criteria', None)", "response": "Returns the filter applie to this column"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the columns of this table.", "response": "def get_columns(self, *, top=None, skip=None):\n        \"\"\"\n        Return the columns of this table\n        :param int top: specify n columns to retrieve\n        :param int skip: specify n columns to skip\n        \"\"\"\n        url = self.build_url(self._endpoints.get('get_columns'))\n\n        params = {}\n        if top is not None:\n            params['$top'] = top\n        if skip is not None:\n            params['$skip'] = skip\n        params = None if not params else params\n        response = self.session.get(url, params=params)\n\n        if not response:\n            return iter(())\n\n        data = response.json()\n\n        return (self.column_constructor(parent=self, **{self._cloud_data_key: column})\n                for column in data.get('value', []))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_column(self, id_or_name):\n        url = self.build_url(self._endpoints.get('get_column').format(quote(id_or_name)))\n        response = self.session.get(url)\n\n        if not response:\n            return None\n\n        data = response.json()\n\n        return self.column_constructor(parent=self, **{self._cloud_data_key: data})", "response": "Gets a column from this table by id or name"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a table column by its index", "response": "def get_column_at_index(self, index):\n        \"\"\"\n        Returns a table column by it's index\n        :param int index: the zero-indexed position of the column in the table\n        \"\"\"\n        if index is None:\n            return None\n\n        url = self.build_url(self._endpoints.get('get_column_index'))\n        response = self.session.post(url, data={'index': index})\n\n        if not response:\n            return None\n\n        return self.column_constructor(parent=self, **{self._cloud_data_key: response.json()})"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef delete_column(self, id_or_name):\n        url = self.build_url(self._endpoints.get('delete_column').format(id=quote(id_or_name)))\n        return bool(self.session.post(url))", "response": "Deletes a Column by its id or name"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a column to the table.", "response": "def add_column(self, name, *, index=0, values=None):\n        \"\"\"\n        Adds a column to the table\n        :param str name: the name of the column\n        :param int index: the index at which the column should be added. Defaults to 0.\n        :param list values: a two dimension array of values to add to the column\n        \"\"\"\n        if name is None:\n            return None\n\n        params = {\n            'name': name,\n            'index': index\n        }\n        if values is not None:\n            params['values'] = values\n\n        url = self.build_url(self._endpoints.get('add_column'))\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return self.column_constructor(parent=self, **{self._cloud_data_key: data})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_rows(self, *, top=None, skip=None):\n        url = self.build_url(self._endpoints.get('get_rows'))\n\n        params = {}\n        if top is not None:\n            params['$top'] = top\n        if skip is not None:\n            params['$skip'] = skip\n        params = None if not params else params\n        response = self.session.get(url, params=params)\n\n        if not response:\n            return iter(())\n\n        data = response.json()\n\n        return (self.row_constructor(parent=self, **{self._cloud_data_key: row})\n                for row in data.get('value', []))", "response": "Returns a generator that yields all the rows of this table."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Row instance at an index", "response": "def get_row(self, index):\n        \"\"\" Returns a Row instance at an index \"\"\"\n        url = self.build_url(self._endpoints.get('get_row').format(id=index))\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.row_constructor(parent=self, **{self._cloud_data_key: response.json()})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_row_at_index(self, index):\n        if index is None:\n            return None\n\n        url = self.build_url(self._endpoints.get('get_row_index'))\n        response = self.session.post(url, data={'index': index})\n\n        if not response:\n            return None\n\n        return self.row_constructor(parent=self, **{self._cloud_data_key: response.json()})", "response": "Returns a table row by it s index"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_row(self, index):\n        url = self.build_url(self._endpoints.get('delete_row').format(id=index))\n        return bool(self.session.post(url))", "response": "Deletes a Row by it s index"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nadd rows to this table.", "response": "def add_rows(self, values=None, index=None):\n        \"\"\"\n        Add rows to this table.\n        Multiple rows can be added at once.\n\n        This request might occasionally receive a 504 HTTP error.\n         The appropriate response to this error is to repeat the request.\n        :param list values: Optional. a 1 or 2 dimensional array of values to add\n        :param int index: Optional. Specifies the relative position of the new row.\n         If null, the addition happens at the end.\n        :return:\n        \"\"\"\n        params = {}\n        if values is not None:\n            if values and not isinstance(values[0], list):\n                # this is a single row\n                values = [values]\n            params['values'] = values\n        if index is not None:\n            params['index'] = index\n\n        params = params if params else None\n\n        url = self.build_url(self._endpoints.get('add_rows'))\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n        return self.row_constructor(parent=self, **{self._cloud_data_key: response.json()})"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nupdating the properties of the table with the given parameters.", "response": "def update(self, *, name=None, show_headers=None, show_totals=None, style=None):\n        \"\"\"\n        Updates this table\n        :param str name: the name of the table\n        :param bool show_headers: whether or not to show the headers\n        :param bool show_totals: whether or not to show the totals\n        :param str style: the style of the table\n        :return: Success or Failure\n        \"\"\"\n        if name is None and show_headers is None and show_totals is None and style is None:\n            raise ValueError('Provide at least one parameter to update')\n        data = {}\n        if name:\n            data['name'] = name\n        if show_headers:\n            data['showHeaders'] = show_headers\n        if show_totals:\n            data['showTotals'] = show_totals\n        if style:\n            data['style'] = style\n\n        response = self.session.patch(self.build_url(''), data=data)\n        if not response:\n            return False\n\n        data = response.json()\n        self.name = data.get('name', self.name)\n        self.show_headers = data.get('showHeaders', self.show_headers)\n        self.show_totals = data.get('showTotals', self.show_totals)\n        self.style = data.get('style', self.style)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a Range based on the endpoint name", "response": "def _get_range(self, endpoint_name):\n        \"\"\" Returns a Range based on the endpoint name \"\"\"\n\n        url = self.build_url(self._endpoints.get(endpoint_name))\n        response = self.session.get(url)\n        if not response:\n            return None\n        data = response.json()\n        return self.range_constructor(parent=self, **{self._cloud_data_key: data})"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns this table worksheet", "response": "def get_worksheet(self):\n        \"\"\" Returns this table worksheet \"\"\"\n        url = self.build_url('')\n        q = self.q().select('name').expand('worksheet')\n        response = self.session.get(url, params=q.as_params())\n        if not response:\n            return None\n        data = response.json()\n\n        ws = data.get('worksheet')\n        if ws is None:\n            return None\n        return WorkSheet(parent=self.parent, **{self._cloud_data_key: ws})"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef update(self, *, name=None, position=None, visibility=None):\n\n        if name is None and position is None and visibility is None:\n            raise ValueError('Provide at least one parameter to update')\n        data = {}\n        if name:\n            data['name'] = name\n        if position:\n            data['position'] = position\n        if visibility:\n            data['visibility'] = visibility\n\n        response = self.session.patch(self.build_url(''), data=data)\n        if not response:\n            return False\n\n        data = response.json()\n        self.name = data.get('name', self.name)\n        self.position = data.get('position', self.position)\n        self.visibility = data.get('visibility', self.visibility)\n\n        return True", "response": "Updates the name position or visibility of this worksheet"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tables(self):\n\n        url = self.build_url(self._endpoints.get('get_tables'))\n        response = self.session.get(url)\n\n        if not response:\n            return []\n\n        data = response.json()\n\n        return [self.table_constructor(parent=self, **{self._cloud_data_key: table})\n                for table in data.get('value', [])]", "response": "Returns a collection of this worksheet tables"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_table(self, id_or_name):\n        url = self.build_url(self._endpoints.get('get_table').format(id=id_or_name))\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.table_constructor(parent=self, **{self._cloud_data_key: response.json()})", "response": "Returns a Table instance by id or name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a table to this worksheet", "response": "def add_table(self, address, has_headers):\n        \"\"\"\n        Adds a table to this worksheet\n        :param str address: a range address eg: 'A1:D4'\n        :param bool has_headers: if the range address includes headers or not\n        :return: a Table instance\n        \"\"\"\n        if address is None:\n            return None\n        params = {\n            'address': address,\n            'hasHeaders': has_headers\n        }\n        url = self.build_url(self._endpoints.get('add_table'))\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n        return self.table_constructor(parent=self, **{self._cloud_data_key: response.json()})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a Range instance from whitin this worksheet", "response": "def get_range(self, address=None):\n        \"\"\"\n        Returns a Range instance from whitin this worksheet\n        :param str address: Optional, the range address you want\n        :return: a Range instance\n        \"\"\"\n        url = self.build_url(self._endpoints.get('get_range'))\n        if address is not None:\n            url = \"{}(address='{}')\".format(url, address)\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.range_constructor(parent=self, **{self._cloud_data_key: response.json()})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the range object containing the single cell based on row and column numbers.", "response": "def get_cell(self, row, column):\n        \"\"\" Gets the range object containing the single cell based on row and column numbers. \"\"\"\n        url = self.build_url(self._endpoints.get('get_cell').format(row=row, column=column))\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.range_constructor(parent=self, **{self._cloud_data_key: response.json()})"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_named_range(self, name, reference, comment='', is_formula=False):\n        if is_formula:\n            url = self.build_url(self._endpoints.get('add_named_range_f'))\n        else:\n            url = self.build_url(self._endpoints.get('add_named_range'))\n        params = {\n            'name': name,\n            'reference': reference,\n            'comment': comment\n        }\n        response = self.session.post(url, data=params)\n        if not response:\n            return None\n        return self.named_range_constructor(parent=self, **{self._cloud_data_key: response.json()})", "response": "Adds a new name to the collection of the given scope using the user s locale for the formula."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a Named range by it s name", "response": "def get_named_range(self, name):\n        \"\"\" Retrieves a Named range by it's name \"\"\"\n        url = self.build_url(self._endpoints.get('get_named_range').format(name=name))\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.named_range_constructor(parent=self, **{self._cloud_data_key: response.json()})"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a collection of this workbook worksheets", "response": "def get_worksheets(self):\n        \"\"\" Returns a collection of this workbook worksheets\"\"\"\n\n        url = self.build_url(self._endpoints.get('get_worksheets'))\n        response = self.session.get(url)\n\n        if not response:\n            return []\n\n        data = response.json()\n\n        return [self.worksheet_constructor(parent=self, **{self._cloud_data_key: ws})\n                for ws in data.get('value', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_worksheet(self, id_or_name):\n        url = self.build_url(self._endpoints.get('get_worksheet').format(id=quote(id_or_name)))\n        response = self.session.get(url)\n        if not response:\n            return None\n        return self.worksheet_constructor(parent=self, **{self._cloud_data_key: response.json()})", "response": "Gets a specific worksheet by id or name"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_worksheet(self, name=None):\n        url = self.build_url(self._endpoints.get('get_worksheets'))\n        response = self.session.post(url, data={'name': name} if name else None)\n        if not response:\n            return None\n        data = response.json()\n        return self.worksheet_constructor(parent=self, **{self._cloud_data_key: data})", "response": "Adds a new worksheet"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a worksheet by it s id", "response": "def delete_worksheet(self, worksheet_id):\n        \"\"\" Deletes a worksheet by it's id \"\"\"\n        url = self.build_url(self._endpoints.get('get_worksheet').format(id=quote(worksheet_id)))\n        return bool(self.session.delete(url))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef invoke_function(self, function_name, **function_params):\n        url = self.build_url(self._endpoints.get('function').format(function_name))\n        response = self.session.post(url, data=function_params)\n        if not response:\n            return None\n        data = response.json()\n\n        error = data.get('error')\n        if error is None:\n            return data.get('value')\n        else:\n            raise FunctionException(error)", "response": "Invoke an Excel Function"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_named_ranges(self):\n\n        url = self.build_url(self._endpoints.get('get_names'))\n        response = self.session.get(url)\n        if not response:\n            return []\n        data = response.json()\n        return [self.named_range_constructor(parent=self, **{self._cloud_data_key: nr})\n                for nr in data.get('value', [])]", "response": "Returns the list of named ranges for this Workbook"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_api_data(self):\n        data = {}\n        # recurrence pattern\n        if self.__interval and isinstance(self.__interval, int):\n            recurrence_pattern = data[self._cc('pattern')] = {}\n            recurrence_pattern[self._cc('type')] = 'daily'\n            recurrence_pattern[self._cc('interval')] = self.__interval\n            if self.__days_of_week and isinstance(self.__days_of_week,\n                                                  (list, tuple, set)):\n                recurrence_pattern[self._cc('type')] = 'relativeMonthly'\n                recurrence_pattern[self._cc('daysOfWeek')] = list(\n                    self.__days_of_week)\n                if self.__first_day_of_week:\n                    recurrence_pattern[self._cc('type')] = 'weekly'\n                    recurrence_pattern[\n                        self._cc('firstDayOfWeek')] = self.__first_day_of_week\n                elif self.__month and isinstance(self.__month, int):\n                    recurrence_pattern[self._cc('type')] = 'relativeYearly'\n                    recurrence_pattern[self._cc('month')] = self.__month\n                    if self.__index:\n                        recurrence_pattern[self._cc('index')] = self.__index\n                else:\n                    if self.__index:\n                        recurrence_pattern[self._cc('index')] = self.__index\n\n            elif self.__day_of_month and isinstance(self.__day_of_month, int):\n                recurrence_pattern[self._cc('type')] = 'absoluteMonthly'\n                recurrence_pattern[self._cc('dayOfMonth')] = self.__day_of_month\n                if self.__month and isinstance(self.__month, int):\n                    recurrence_pattern[self._cc('type')] = 'absoluteYearly'\n                    recurrence_pattern[self._cc('month')] = self.__month\n\n        # recurrence range\n        if self.__start_date:\n            recurrence_range = data[self._cc('range')] = {}\n            recurrence_range[self._cc('type')] = 'noEnd'\n            recurrence_range[\n                self._cc('startDate')] = self.__start_date.isoformat()\n            recurrence_range[\n                self._cc('recurrenceTimeZone')] = self.__recurrence_time_zone\n\n            if self.__end_date:\n                recurrence_range[self._cc('type')] = 'endDate'\n                recurrence_range[\n                    self._cc('endDate')] = self.__end_date.isoformat()\n            elif self.__occurrences is not None and isinstance(\n                    self.__occurrences,\n                    int):\n                recurrence_range[self._cc('type')] = 'numbered'\n                recurrence_range[\n                    self._cc('numberOfOccurrences')] = self.__occurrences\n\n        return data", "response": "Returns a dict that contains the internal data of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _clear_pattern(self):\n        # pattern group\n        self.__interval = None\n        self.__days_of_week = set()\n        self.__first_day_of_week = None\n        self.__day_of_month = None\n        self.__month = None\n        self.__index = 'first'\n        # range group\n        self.__start_date = None\n        self.__end_date = None\n        self.__occurrences = None", "response": "Clears the recurrence pattern"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the range of recurrence", "response": "def set_range(self, start=None, end=None, occurrences=None):\n        \"\"\" Set the range of recurrence\n\n        :param date start: Start date of repetition\n        :param date end: End date of repetition\n        :param int occurrences: no of occurrences\n        \"\"\"\n        if start is None:\n            if self.__start_date is None:\n                self.__start_date = dt.date.today()\n        else:\n            self.start_date = start\n\n        if end:\n            self.end_date = end\n        elif occurrences:\n            self.__occurrences = occurrences\n        self._track_changes()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_daily(self, interval, **kwargs):\n        self._clear_pattern()\n        self.__interval = interval\n        self.set_range(**kwargs)", "response": "Set to repeat every x no. of days at a specific interval"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets to repeat every week on specified days", "response": "def set_weekly(self, interval, *, days_of_week, first_day_of_week,\n                   **kwargs):\n        \"\"\" Set to repeat every week on specified days for every x no. of days\n\n        :param int interval: no. of days to repeat at\n        :param str first_day_of_week: starting day for a week\n        :param list[str] days_of_week: list of days of the week to repeat\n        :keyword date start: Start date of repetition (kwargs)\n        :keyword date end: End date of repetition (kwargs)\n        :keyword int occurrences: no of occurrences (kwargs)\n        \"\"\"\n        self.set_daily(interval, **kwargs)\n        self.__days_of_week = set(days_of_week)\n        self.__first_day_of_week = first_day_of_week"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset to repeat every month on specified days for every month.", "response": "def set_monthly(self, interval, *, day_of_month=None, days_of_week=None,\n                    index=None, **kwargs):\n        \"\"\" Set to repeat every month on specified days for every x no. of days\n\n        :param int interval: no. of days to repeat at\n        :param int day_of_month: repeat day of a month\n        :param list[str] days_of_week: list of days of the week to repeat\n        :param index: index\n        :keyword date start: Start date of repetition (kwargs)\n        :keyword date end: End date of repetition (kwargs)\n        :keyword int occurrences: no of occurrences (kwargs)\n        \"\"\"\n        if not day_of_month and not days_of_week:\n            raise ValueError('Must provide day_of_month or days_of_week values')\n        if day_of_month and days_of_week:\n            raise ValueError('Must provide only one of the two options')\n        self.set_daily(interval, **kwargs)\n        if day_of_month:\n            self.__day_of_month = day_of_month\n        elif days_of_week:\n            self.__days_of_week = set(days_of_week)\n            if index:\n                self.__index = index"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nset to repeat every month on specified days for every month on specified days for every week.", "response": "def set_yearly(self, interval, month, *, day_of_month=None,\n                   days_of_week=None, index=None, **kwargs):\n        \"\"\" Set to repeat every month on specified days for every x no. of days\n\n        :param int interval: no. of days to repeat at\n        :param int month: month to repeat\n        :param int day_of_month: repeat day of a month\n        :param list[str] days_of_week: list of days of the week to repeat\n        :param index: index\n        :keyword date start: Start date of repetition (kwargs)\n        :keyword date end: End date of repetition (kwargs)\n        :keyword int occurrences: no of occurrences (kwargs)\n        \"\"\"\n        self.set_monthly(interval, day_of_month=day_of_month,\n                         days_of_week=days_of_week, index=index, **kwargs)\n        self.__month = month"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding attendees to the event object.", "response": "def add(self, attendees):\n        \"\"\" Add attendees to the parent event\n\n        :param attendees: list of attendees to add\n        :type attendees: str or tuple(str, str) or Attendee or list[str] or\n         list[tuple(str,str)] or list[Attendee]\n        \"\"\"\n        if attendees:\n            if isinstance(attendees, str):\n                self.__attendees.append(\n                    Attendee(address=attendees, event=self._event))\n                self._track_changes()\n            elif isinstance(attendees, Attendee):\n                self.__attendees.append(attendees)\n                self._track_changes()\n            elif isinstance(attendees, tuple):\n                name, address = attendees\n                if address:\n                    self.__attendees.append(\n                        Attendee(address=address, name=name, event=self._event))\n                    self._track_changes()\n            elif isinstance(attendees, list):\n                for attendee in attendees:\n                    self.add(attendee)\n            elif isinstance(attendees,\n                            dict) and self._cloud_data_key in attendees:\n                attendees = attendees.get(self._cloud_data_key)\n                for attendee in attendees:\n                    email = attendee.get(self._cc('emailAddress'), {})\n                    address = email.get(self._cc('address'), None)\n                    if address:\n                        name = email.get(self._cc('name'), None)\n                        # default value\n                        attendee_type = attendee.get(self._cc('type'),\n                                                     'required')\n                        self.__attendees.append(\n                            Attendee(address=address, name=name,\n                                     attendee_type=attendee_type,\n                                     event=self._event,\n                                     response_status=\n                                     ResponseStatus(parent=self,\n                                                    response_status=\n                                                    attendee.get(\n                                                        self._cc('status'),\n                                                        {}))))\n            else:\n                raise ValueError('Attendees must be an address string, an '\n                                 'Attendee instance, a (name, address) '\n                                 'tuple or a list')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef remove(self, attendees):\n        if isinstance(attendees, (list, tuple)):\n            attendees = {\n                attendee.address if isinstance(attendee, Attendee) else attendee\n                for\n                attendee in attendees}\n        elif isinstance(attendees, str):\n            attendees = {attendees}\n        elif isinstance(attendees, Attendee):\n            attendees = {attendees.address}\n        else:\n            raise ValueError('Incorrect parameter type for attendees')\n\n        new_attendees = []\n        for attendee in self.__attendees:\n            if attendee.address not in attendees:\n                new_attendees.append(attendee)\n        self.__attendees = new_attendees\n        self._track_changes()", "response": "Removes the provided attendees from the event\nAttributeNames list."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef to_api_data(self):\n        data = []\n        for attendee in self.__attendees:\n            if attendee.address:\n                att_data = {\n                    self._cc('emailAddress'): {\n                        self._cc('address'): attendee.address,\n                        self._cc('name'): attendee.name\n                    },\n                    self._cc('type'): self._cc(attendee.attendee_type.value)\n                }\n                data.append(att_data)\n        return data", "response": "Returns a list of data to communicate with the server"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a dict of all the properties of the object in the API format.", "response": "def to_api_data(self, restrict_keys=None):\n        \"\"\" Returns a dict to communicate with the server\n\n        :param restrict_keys: a set of keys to restrict the returned data to\n        :rtype: dict\n        \"\"\"\n        cc = self._cc  # alias\n        data = {\n            cc('subject'): self.__subject,\n            cc('body'): {\n                cc('contentType'): self.body_type,\n                cc('content'): self.__body},\n            cc('start'): self._build_date_time_time_zone(self.__start),\n            cc('end'): self._build_date_time_time_zone(self.__end),\n            cc('attendees'): self.__attendees.to_api_data(),\n            cc('location'): {cc('displayName'): self.__location},\n            cc('categories'): self.__categories,\n            cc('isAllDay'): self.__is_all_day,\n            cc('importance'): cc(self.__importance.value),\n            cc('isReminderOn'): self.__is_reminder_on,\n            cc('reminderMinutesBeforeStart'): self.__remind_before_minutes,\n            cc('responseRequested'): self.__response_requested,\n            cc('sensitivity'): cc(self.__sensitivity.value),\n            cc('showAs'): cc(self.__show_as.value),\n        }\n\n        if self.__recurrence:\n            data[cc('recurrence')] = self.__recurrence.to_api_data()\n\n        if self.has_attachments:\n            data[cc('attachments')] = self.__attachments.to_api_data()\n\n        if restrict_keys:\n            for key in list(data.keys()):\n                if key not in restrict_keys:\n                    del data[key]\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_occurrences(self, start, end, *, limit=None, query=None, order_by=None, batch=None):\n        if self.event_type != EventType.SeriesMaster:\n            # you can only get occurrences if its a seriesMaster\n            return []\n\n        url = self.build_url(\n            self._endpoints.get('occurrences').format(id=self.object_id))\n\n        if limit is None or limit > self.protocol.max_top_value:\n            batch = self.protocol.max_top_value\n\n        params = {'$top': batch if batch else limit}\n\n        if order_by:\n            params['$orderby'] = order_by\n\n        if query:\n            if isinstance(query, str):\n                params['$filter'] = query\n            else:\n                params.update(query.as_params())\n\n        if start.tzinfo is None:\n            # if it's a naive datetime, localize the datetime.\n            start = self.protocol.timezone.localize(start)  # localize datetime into local tz\n        if start.tzinfo != pytz.utc:\n            start = start.astimezone(pytz.utc)  # transform local datetime to utc\n\n        if end.tzinfo is None:\n            # if it's a naive datetime, localize the datetime.\n            end = self.protocol.timezone.localize(end)  # localize datetime into local tz\n        if end.tzinfo != pytz.utc:\n            end = end.astimezone(pytz.utc)  # transform local datetime to utc\n\n        params[self._cc('startDateTime')] = start.isoformat()\n        params[self._cc('endDateTime')] = end.isoformat()\n\n        response = self.con.get(url, params=params,\n                                headers={'Prefer': 'outlook.timezone=\"UTC\"'})\n        if not response:\n            return iter(())\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        events = (self.__class__(parent=self, **{self._cloud_data_key: event})\n                  for event in data.get('value', []))\n        next_link = data.get(NEXT_LINK_KEYWORD, None)\n        if batch and next_link:\n            return Pagination(parent=self, data=events,\n                              constructor=self.__class__,\n                              next_link=next_link, limit=limit)\n        else:\n            return events", "response": "Returns all the occurrences of a seriesMaster event for a specified time range."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncreates a new event or update an existing one", "response": "def save(self):\n        \"\"\" Create a new event or update an existing one by checking what\n        values have changed and update them on the server\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        if self.object_id:\n            # update event\n            if not self._track_changes:\n                return True  # there's nothing to update\n            url = self.build_url(\n                self._endpoints.get('event').format(id=self.object_id))\n            method = self.con.patch\n            data = self.to_api_data(restrict_keys=self._track_changes)\n        else:\n            # new event\n            if self.calendar_id:\n                url = self.build_url(\n                    self._endpoints.get('event_calendar').format(\n                        id=self.calendar_id))\n            else:\n                url = self.build_url(self._endpoints.get('event_default'))\n            method = self.con.post\n            data = self.to_api_data()\n\n        response = method(url, data=data)\n        if not response:\n            return False\n\n        self._track_changes.clear()  # clear the tracked changes\n\n        if not self.object_id:\n            # new event\n            event = response.json()\n\n            self.object_id = event.get(self._cc('id'), None)\n\n            self.__created = event.get(self._cc('createdDateTime'), None)\n            self.__modified = event.get(self._cc('lastModifiedDateTime'), None)\n\n            self.__created = parse(self.__created).astimezone(\n                self.protocol.timezone) if self.__created else None\n            self.__modified = parse(self.__modified).astimezone(\n                self.protocol.timezone) if self.__modified else None\n        else:\n            self.__modified = self.protocol.timezone.localize(dt.datetime.now())\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef decline_event(self, comment=None, *, send_response=True):\n        if not self.object_id:\n            raise RuntimeError(\"Can't accept event that doesn't exist\")\n\n        url = self.build_url(\n            self._endpoints.get('event').format(id=self.object_id))\n        url = url + '/decline'\n\n        data = {}\n        if comment and isinstance(comment, str):\n            data[self._cc('comment')] = comment\n        if send_response is False:\n            data[self._cc('sendResponse')] = send_response\n\n        response = self.con.post(url, data=data or None)\n\n        return bool(response)", "response": "Decline the event that has been created"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses the body and return the body text using bs4", "response": "def get_body_text(self):\n        \"\"\" Parse the body html and returns the body text using bs4\n\n        :return: body text\n        :rtype: str\n        \"\"\"\n        if self.body_type != 'HTML':\n            return self.body\n\n        try:\n            soup = bs(self.body, 'html.parser')\n        except RuntimeError:\n            return self.body\n        else:\n            return soup.body.text"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate this calendar. Only name and color can be changed.", "response": "def update(self):\n        \"\"\" Updates this calendar. Only name and color can be changed.\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        if not self.calendar_id:\n            return False\n\n        url = self.build_url(self._endpoints.get('calendar'))\n\n        data = {\n            self._cc('name'): self.name,\n            self._cc('color'): self._cc(self.color.value\n                                        if isinstance(self.color, CalendarColor)\n                                        else self.color)\n        }\n\n        response = self.con.patch(url, data=data)\n\n        return bool(response)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_events(self, limit=25, *, query=None, order_by=None, batch=None,\n                   download_attachments=False, include_recurring=True):\n        \"\"\" Get events from the this Calendar\n\n        :param int limit: max no. of events to get. Over 999 uses batch.\n        :param query: applies a OData filter to the request\n        :type query: Query or str\n        :param order_by: orders the result set based on this condition\n        :type order_by: Query or str\n        :param int batch: batch size, retrieves items in\n         batches allowing to retrieve more items than the limit.\n        :param download_attachments: downloads event attachments\n        :param bool include_recurring: whether to include recurring events or not\n        :return: list of events in this calendar\n        :rtype: list[Event] or Pagination\n        \"\"\"\n\n        if self.calendar_id is None:\n            # I'm the default calendar\n            if include_recurring:\n                url = self.build_url(self._endpoints.get('default_events_view'))\n            else:\n                url = self.build_url(self._endpoints.get('default_events'))\n        else:\n            if include_recurring:\n                url = self.build_url(\n                    self._endpoints.get('events_view').format(id=self.calendar_id))\n            else:\n                url = self.build_url(\n                    self._endpoints.get('get_events').format(id=self.calendar_id))\n\n        if limit is None or limit > self.protocol.max_top_value:\n            batch = self.protocol.max_top_value\n\n        if batch:\n            download_attachments = False\n\n        params = {'$top': batch if batch else limit}\n\n        if include_recurring:\n            start = None\n            end = None\n            if query and not isinstance(query, str):\n                # extract start and end from query because\n                # those are required by a calendarView\n                for query_data in query._filters:\n                    if not isinstance(query_data, tuple):\n                        continue\n                    attribute = query_data[0]\n                    # the 2nd position contains the filter data\n                    # and the 3rd position in filter_data contains the value\n                    word = query_data[2][3]\n\n                    if attribute.lower().startswith('start/'):\n                        start = word.replace(\"'\", '')  # remove the quotes\n                        query.remove_filter('start')\n                    if attribute.lower().startswith('end/'):\n                        end = word.replace(\"'\", '')  # remove the quotes\n                        query.remove_filter('end')\n\n            if start is None or end is None:\n                raise ValueError(\"When 'include_recurring' is True you must provide a 'start' and 'end' datetimes inside a Query instance.\")\n\n            if end < start:\n                raise ValueError('When using \"include_recurring=True\", the date asigned to the \"end\" datetime'\n                                 ' should be greater or equal than the date asigned to the \"start\" datetime.')\n\n            params[self._cc('startDateTime')] = start\n            params[self._cc('endDateTime')] = end\n\n        if order_by:\n            params['$orderby'] = order_by\n\n        if query:\n            if isinstance(query, str):\n                params['$filter'] = query\n            else:\n                params.update(query.as_params())\n\n        response = self.con.get(url, params=params,\n                                headers={'Prefer': 'outlook.timezone=\"UTC\"'})\n        if not response:\n            return iter(())\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        events = (self.event_constructor(parent=self,\n                                         download_attachments=\n                                         download_attachments,\n                                         **{self._cloud_data_key: event})\n                  for event in data.get('value', []))\n        next_link = data.get(NEXT_LINK_KEYWORD, None)\n        if batch and next_link:\n            return Pagination(parent=self, data=events,\n                              constructor=self.event_constructor,\n                              next_link=next_link, limit=limit)\n        else:\n            return events", "response": "Get events from the calendar."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new_event(self, subject=None):\n        return self.event_constructor(parent=self, subject=subject,\n                                      calendar_id=self.calendar_id)", "response": "Returns a new Event object with the given subject and calendar_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_event(self, param):\n\n        if param is None:\n            return None\n        if isinstance(param, str):\n            url = self.build_url(\n                self._endpoints.get('get_event').format(id=self.calendar_id,\n                                                        ide=param))\n            params = None\n            by_id = True\n        else:\n            url = self.build_url(\n                self._endpoints.get('get_events').format(id=self.calendar_id))\n            params = {'$top': 1}\n            params.update(param.as_params())\n            by_id = False\n\n        response = self.con.get(url, params=params,\n                                headers={'Prefer': 'outlook.timezone=\"UTC\"'})\n        if not response:\n            return None\n\n        if by_id:\n            event = response.json()\n        else:\n            event = response.json().get('value', [])\n            if event:\n                event = event[0]\n            else:\n                return None\n        return self.event_constructor(parent=self,\n                                      **{self._cloud_data_key: event})", "response": "Returns an Event instance by it s id"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets a list of calendars for the current user", "response": "def list_calendars(self, limit=None, *, query=None, order_by=None):\n        \"\"\" Gets a list of calendars\n\n        To use query an order_by check the OData specification here:\n        http://docs.oasis-open.org/odata/odata/v4.0/errata03/os/complete/\n        part2-url-conventions/odata-v4.0-errata03-os-part2-url-conventions\n        -complete.html\n\n        :param int limit: max no. of calendars to get. Over 999 uses batch.\n        :param query: applies a OData filter to the request\n        :type query: Query or str\n        :param order_by: orders the result set based on this condition\n        :type order_by: Query or str\n        :return: list of calendars\n        :rtype: list[Calendar]\n\n        \"\"\"\n        url = self.build_url(self._endpoints.get('root_calendars'))\n\n        params = {}\n        if limit:\n            params['$top'] = limit\n        if query:\n            params['$filter'] = str(query)\n        if order_by:\n            params['$orderby'] = order_by\n\n        response = self.con.get(url, params=params or None)\n        if not response:\n            return []\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        contacts = [self.calendar_constructor(parent=self, **{\n            self._cloud_data_key: x}) for x in data.get('value', [])]\n\n        return contacts"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new calendar instance with the specified name", "response": "def new_calendar(self, calendar_name):\n        \"\"\" Creates a new calendar\n\n        :param str calendar_name: name of the new calendar\n        :return: a new Calendar instance\n        :rtype: Calendar\n        \"\"\"\n        if not calendar_name:\n            return None\n\n        url = self.build_url(self._endpoints.get('root_calendars'))\n\n        response = self.con.post(url, data={self._cc('name'): calendar_name})\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.calendar_constructor(parent=self,\n                                         **{self._cloud_data_key: data})"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_calendar(self, calendar_id=None, calendar_name=None):\n        if calendar_id and calendar_name:\n            raise RuntimeError('Provide only one of the options')\n\n        if not calendar_id and not calendar_name:\n            raise RuntimeError('Provide one of the options')\n\n        if calendar_id:\n            # get calendar by it's id\n            url = self.build_url(\n                self._endpoints.get('get_calendar').format(id=calendar_id))\n            params = None\n        else:\n            # get calendar by name\n            url = self.build_url(self._endpoints.get('root_calendars'))\n            params = {\n                '$filter': \"{} eq '{}'\".format(self._cc('name'), calendar_name),\n                '$top': 1}\n\n        response = self.con.get(url, params=params)\n        if not response:\n            return None\n\n        if calendar_id:\n            data = response.json()\n        else:\n            data = response.json().get('value')\n            data = data[0] if data else None\n            if data is None:\n                return None\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.calendar_constructor(parent=self,\n                                         **{self._cloud_data_key: data})", "response": "Returns a calendar by it s id or name"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_default_calendar(self):\n\n        url = self.build_url(self._endpoints.get('default_calendar'))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.calendar_constructor(parent=self,\n                                         **{self._cloud_data_key: data})", "response": "Returns the default calendar for the current user"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_events(self, limit=25, *, query=None, order_by=None, batch=None,\n                   download_attachments=False, include_recurring=True):\n        \"\"\" Get events from the default Calendar\n\n        :param int limit: max no. of events to get. Over 999 uses batch.\n        :param query: applies a OData filter to the request\n        :type query: Query or str\n        :param order_by: orders the result set based on this condition\n        :type order_by: Query or str\n        :param int batch: batch size, retrieves items in\n         batches allowing to retrieve more items than the limit.\n        :param bool download_attachments: downloads event attachments\n        :param bool include_recurring: whether to include recurring events or not\n        :return: list of items in this folder\n        :rtype: list[Event] or Pagination\n        \"\"\"\n\n        default_calendar = self.calendar_constructor(parent=self)\n\n        return default_calendar.get_events(limit=limit, query=query,\n                                           order_by=order_by, batch=batch,\n                                           download_attachments=download_attachments,\n                                           include_recurring=include_recurring)", "response": "Get events from the default Calendar object"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the expiration datetime of the token", "response": "def expiration_datetime(self):\n        \"\"\"\n        Returns the expiration datetime\n        :return datetime: The datetime this token expires\n        \"\"\"\n        expires_at = self.get('expires_at')\n        if expires_at is None:\n            # consider it is expired\n            return dt.datetime.now() - dt.timedelta(seconds=10)\n        expires_on = dt.datetime.fromtimestamp(expires_at) - dt.timedelta(seconds=EXPIRES_ON_THRESHOLD)\n        if self.is_long_lived:\n            expires_on = expires_on + dt.timedelta(days=90)\n        return expires_on"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve the token from the File System", "response": "def get_token(self):\n        \"\"\"\n        Retrieves the token from the File System\n        :return dict or None: The token if exists, None otherwise\n        \"\"\"\n        token = None\n        if self.token_path.exists():\n            with self.token_path.open('r') as token_file:\n                token = self.token_constructor(self.serializer.load(token_file))\n        self.token = token\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving the token dict in the specified file.", "response": "def save_token(self):\n        \"\"\"\n        Saves the token dict in the specified file\n        :return bool: Success / Failure\n        \"\"\"\n        if self.token is None:\n            raise ValueError('You have to set the \"token\" first.')\n\n        try:\n            if not self.token_path.parent.exists():\n                self.token_path.parent.mkdir(parents=True)\n        except Exception as e:\n            log.error('Token could not be saved: {}'.format(str(e)))\n            return False\n\n        with self.token_path.open('w') as token_file:\n            # 'indent = True' will make the file human readable\n            self.serializer.dump(self.token, token_file, indent=True)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndelete the token file", "response": "def delete_token(self):\n        \"\"\"\n        Deletes the token file\n        :return bool: Success / Failure\n        \"\"\"\n        if self.token_path.exists():\n            self.token_path.unlink()\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nretrieve the token from the store.", "response": "def get_token(self):\n        \"\"\"\n        Retrieves the token from the store\n        :return dict or None: The token if exists, None otherwise\n        \"\"\"\n        token = None\n        try:\n            doc = self.doc_ref.get()\n        except Exception as e:\n            log.error('Token (collection: {}, doc_id: {}) '\n                      'could not be retrieved from the backend: {}'\n                      .format(self.collection, self.doc_id, str(e)))\n            doc = None\n        if doc and doc.exists:\n            token_str = doc.get(self.field_name)\n            if token_str:\n                token = self.token_constructor(self.serializer.loads(token_str))\n        self.token = token\n        return token"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef save_token(self):\n        if self.token is None:\n            raise ValueError('You have to set the \"token\" first.')\n\n        try:\n            # set token will overwrite previous data\n            self.doc_ref.set({\n                self.field_name: self.serializer.dumps(self.token)\n            })\n        except Exception as e:\n            log.error('Token could not be saved: {}'.format(str(e)))\n            return False\n\n        return True", "response": "Saves the token dict in the store."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete_token(self):\n        try:\n            self.doc_ref.delete()\n        except Exception as e:\n            log.error('Could not delete the token (key: {}): {}'.format(self.doc_id, str(e)))\n            return False\n        return True", "response": "Deletes the token from the store"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck if the token exists on the store.", "response": "def check_token(self):\n        \"\"\"\n        Checks if the token exists\n        :return bool: True if it exists on the store\n        \"\"\"\n        try:\n            doc = self.doc_ref.get()\n        except Exception as e:\n            log.error('Token (collection: {}, doc_id: {}) '\n                      'could not be retrieved from the backend: {}'\n                      .format(self.collection, self.doc_id, str(e)))\n            doc = None\n        return doc and doc.exists"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_authenticated(self):\n        token = self.con.token_backend.token\n        if not token:\n            token = self.con.token_backend.get_token()\n\n        return token is not None and not token.is_expired", "response": "Checks whether the library has the authentication and that is not expired"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef authenticate(self, *, scopes, **kwargs):\n        kwargs.setdefault('token_backend', self.con.token_backend)\n        return oauth_authentication_flow(*self.con.auth, scopes=scopes,\n                                         protocol=self.protocol, **kwargs)", "response": "This method performs the oauth authentication flow resulting in a stored token\n       "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef new_message(self, resource=None):\n        return Message(parent=self, main_resource=resource, is_draft=True)", "response": "Creates a new empty message with the given resource"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef address_book(self, *, resource=None, address_book='personal'):\n        if address_book.lower() == 'personal':\n            return AddressBook(parent=self, main_resource=resource,\n                               name='Personal Address Book')\n        elif address_book.lower() == 'gal':\n            return GlobalAddressList(parent=self)\n        else:\n            raise RuntimeError(\n                'address_book must be either \"personal\" '\n                '(resource address book) or \"gal\" (Global Address List)')", "response": "Returns an instance of the specified address book or GlobalAddressList if invalid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an instance of Storage class for the specified account resource.", "response": "def storage(self, *, resource=None):\n        \"\"\" Get an instance to handle file storage (OneDrive / Sharepoint)\n        for the specified account resource\n\n        :param str resource: Custom resource to be used in this drive object\n         (Defaults to parent main_resource)\n        :return: a representation of OneDrive File Storage\n        :rtype: Storage\n        :raises RuntimeError: if protocol doesn't support the feature\n        \"\"\"\n        if not isinstance(self.protocol, MSGraphProtocol):\n            # TODO: Custom protocol accessing OneDrive/Sharepoint Api fails here\n            raise RuntimeError(\n                'Drive options only works on Microsoft Graph API')\n\n        return Storage(parent=self, main_resource=resource)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sharepoint(self, *, resource=''):\n\n        if not isinstance(self.protocol, MSGraphProtocol):\n            # TODO: Custom protocol accessing OneDrive/Sharepoint Api fails here\n            raise RuntimeError(\n                'Sharepoint api only works on Microsoft Graph API')\n\n        return Sharepoint(parent=self, main_resource=resource)", "response": "Returns an instance of Sharepoint object for the specified account resource"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef planner(self, *, resource=''):\n\n        if not isinstance(self.protocol, MSGraphProtocol):\n            # TODO: Custom protocol accessing OneDrive/Sharepoint Api fails here\n            raise RuntimeError(\n                'planner api only works on Microsoft Graph API')\n\n        return Planner(parent=self, main_resource=resource)", "response": "Get an instance to read information from Microsoft Planner API"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndownload the file from the local drive.", "response": "def download(self, to_path=None, name=None, chunk_size='auto',\n                 convert_to_pdf=False):\n        \"\"\" Downloads this file to the local drive. Can download the\n        file in chunks with multiple requests to the server.\n\n        :param to_path: a path to store the downloaded file\n        :type to_path: str or Path\n        :param str name: the name you want the stored file to have.\n        :param int chunk_size: number of bytes to retrieve from\n         each api call to the server. if auto, files bigger than\n         SIZE_THERSHOLD will be chunked (into memory, will be\n         however only 1 request)\n        :param bool convert_to_pdf: will try to download the converted pdf\n         if file extension in ALLOWED_PDF_EXTENSIONS\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        # TODO: Add download with more than one request (chunk_requests) with\n        # header 'Range'. For example: 'Range': 'bytes=0-1024'\n\n        if to_path is None:\n            to_path = Path()\n        else:\n            if not isinstance(to_path, Path):\n                to_path = Path(to_path)\n\n        if not to_path.exists():\n            raise FileNotFoundError('{} does not exist'.format(to_path))\n\n        if name and not Path(name).suffix and self.name:\n            name = name + Path(self.name).suffix\n\n        name = name or self.name\n        to_path = to_path / name\n\n        url = self.build_url(\n            self._endpoints.get('download').format(id=self.object_id))\n\n        try:\n            if chunk_size is None:\n                stream = False\n            elif chunk_size == 'auto':\n                if self.size and self.size > SIZE_THERSHOLD:\n                    stream = True\n                else:\n                    stream = False\n            elif isinstance(chunk_size, int):\n                stream = True\n            else:\n                raise ValueError(\"Argument chunk_size must be either 'auto' \"\n                                 \"or any integer number representing bytes\")\n\n            params = {}\n            if convert_to_pdf and Path(name).suffix in ALLOWED_PDF_EXTENSIONS:\n                params['format'] = 'pdf'\n\n            with self.con.get(url, stream=stream, params=params) as response:\n                if not response:\n                    log.debug('Downloading driveitem Request failed: {}'.format(\n                        response.reason))\n                    return False\n                with to_path.open(mode='wb') as output:\n                    if stream:\n                        for chunk in response.iter_content(\n                                chunk_size=chunk_size):\n                            if chunk:\n                                output.write(chunk)\n                    else:\n                        output.write(response.content)\n        except Exception as e:\n            log.error(\n                'Error downloading driveitem {}. Error: {}'.format(self.name,\n                                                                   str(e)))\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks the api endpoint to check if the async job is in progress", "response": "def _request_status(self):\n        \"\"\" Checks the api endpoint to check if the async job progress \"\"\"\n        if self.item_id:\n            return True\n\n        response = self.con.get(self.monitor_url)\n        if not response:\n            return False\n\n        data = response.json()\n\n        self.status = data.get('status', 'inProgress')\n        self.completion_percentage = data.get(self._cc('percentageComplete'),\n                                              0)\n        self.item_id = data.get(self._cc('resourceId'), None)\n\n        return self.item_id is not None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks the status of the item in a loop.", "response": "def check_status(self, delay=0):\n        \"\"\" Checks the api endpoint in a loop\n\n        :param delay: number of seconds to wait between api calls.\n         Note Connection 'requests_delay' also apply.\n        :return: tuple of status and percentage complete\n        :rtype: tuple(str, float)\n        \"\"\"\n        if not self.item_id:\n            while not self._request_status():\n                # wait until _request_status returns True\n                yield self.status, self.completion_percentage\n                if self.item_id is None:\n                    sleep(delay)\n        else:\n            yield self.status, self.completion_percentage"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrestore this DriveItem Version. You can not restore the current version.", "response": "def restore(self):\n        \"\"\" Restores this DriveItem Version.\n        You can not restore the current version (last one).\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        url = self.build_url(\n            self._endpoints.get('restore').format(id=self.object_id))\n\n        response = self.con.post(url)\n\n        return bool(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download(self, to_path=None, name=None, chunk_size='auto',\n                 convert_to_pdf=False):\n        \"\"\" Downloads this version.\n        You can not download the current version (last one).\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n        return super().download(to_path=to_path, name=name,\n                                chunk_size=chunk_size,\n                                convert_to_pdf=convert_to_pdf)", "response": "Downloads the current version of the current version."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef update_roles(self, roles='view'):\n        if not self.object_id:\n            return False\n\n        url = self.build_url(self._endpoints.get('permission').format(\n            driveitem_id=self.driveitem_id, id=self.object_id))\n\n        if roles in {'view', 'read'}:\n            data = {'roles': ['read']}\n        elif roles == {'edit', 'write'}:\n            data = {'roles': ['write']}\n        else:\n            raise ValueError('\"{}\" is not a valid share_type'.format(roles))\n\n        response = self.con.patch(url, data=data)\n        if not response:\n            return False\n\n        self.roles = data.get('roles', [])\n        return True", "response": "Updates the roles of this permission."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_parent(self):\n        if self._parent and self._parent.object_id == self.parent_id:\n            return self._parent\n        else:\n            if self.parent_id:\n                return self.drive.get_item(self.parent_id)\n            else:\n                # return the drive\n                return self.drive", "response": "the parent of this item"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn this Item Thumbnails. Thumbnails are not supported on SharePoint Server 2016.", "response": "def get_thumbnails(self, size=None):\n        \"\"\" Returns this Item Thumbnails. Thumbnails are not supported on\n        SharePoint Server 2016.\n\n        :param size: request only the specified size: ej: \"small\",\n         Custom 300x400 px: \"c300x400\", Crop: \"c300x400_Crop\"\n        :return: Thumbnail Data\n        :rtype: dict\n        \"\"\"\n        if not self.object_id:\n            return []\n\n        url = self.build_url(\n            self._endpoints.get('thumbnails').format(id=self.object_id))\n\n        params = {}\n        if size is not None:\n            params['select'] = size\n\n        response = self.con.get(url, params=params)\n        if not response:\n            return []\n\n        data = response.json()\n\n        if not self.thumbnails or size is None:\n            self.thumbnails = data\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, **kwargs):\n        if not self.object_id:\n            return False\n\n        url = self.build_url(\n            self._endpoints.get('item').format(id=self.object_id))\n\n        data = {self._cc(key): value for key, value in kwargs.items() if\n                key in {'name',\n                        'description'}}  # convert keys to protocol casing\n        if not data:\n            return False\n\n        response = self.con.patch(url, data=data)\n        if not response:\n            return False\n\n        new_data = response.json()\n\n        for key in data:\n            value = new_data.get(key, None)\n            if value:\n                setattr(self, self.protocol.to_api_case(key), value)\n\n        return True", "response": "Updates the item s attributes with the specified values."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(self):\n\n        if not self.object_id:\n            return False\n\n        url = self.build_url(\n            self._endpoints.get('item').format(id=self.object_id))\n\n        response = self.con.delete(url)\n        if not response:\n            return False\n\n        self.object_id = None\n\n        return True", "response": "Moves this item to the Recycle Bin"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef move(self, target):\n\n        if isinstance(target, Folder):\n            target_id = target.object_id\n        elif isinstance(target, Drive):\n            # we need the root folder id\n            root_folder = target.get_root_folder()\n            if not root_folder:\n                return False\n            target_id = root_folder.object_id\n        elif isinstance(target, str):\n            target_id = target\n        else:\n            raise ValueError('Target must be a Folder or Drive')\n\n        if not self.object_id or not target_id:\n            raise ValueError(\n                'Both self, and target must have a valid object_id.')\n\n        if target_id == 'root':\n            raise ValueError(\"When moving, target id can't be 'root'\")\n\n        url = self.build_url(\n            self._endpoints.get('item').format(id=self.object_id))\n\n        data = {'parentReference': {'id': target_id}}\n\n        response = self.con.patch(url, data=data)\n        if not response:\n            return False\n\n        self.parent_id = target_id\n\n        return True", "response": "Moves this DriveItem to another Folder or DriveItem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef copy(self, target=None, name=None):\n        if target is None and name is None:\n            raise ValueError('Must provide a target or a name (or both)')\n\n        if isinstance(target, Folder):\n            target_id = target.object_id\n            drive_id = target.drive_id\n        elif isinstance(target, Drive):\n            # we need the root folder\n            root_folder = target.get_root_folder()\n            if not root_folder:\n                return None\n            target_id = root_folder.object_id\n            drive_id = root_folder.drive_id\n        elif target is None:\n            target_id = None\n            drive_id = None\n        else:\n            raise ValueError('Target, if provided, must be a Folder or Drive')\n\n        if not self.object_id:\n            return None\n\n        if target_id == 'root':\n            raise ValueError(\"When copying, target id can't be 'root'\")\n\n        url = self.build_url(\n            self._endpoints.get('copy').format(id=self.object_id))\n\n        if target_id and drive_id:\n            data = {'parentReference': {'id': target_id, 'driveId': drive_id}}\n        else:\n            data = {}\n        if name:\n            # incorporate the extension if the name provided has none.\n            if not Path(name).suffix and self.name:\n                name = name + Path(self.name).suffix\n            data['name'] = name\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        # Find out if the server has run a Sync or Async operation\n        location = response.headers.get('Location', None)\n\n        if 'monitor' in location:\n            # Async operation\n            return CopyOperation(parent=self.drive, monitor_url=location)\n        else:\n            # Sync operation. Item is ready to be retrieved\n            path = urlparse(location).path\n            item_id = path.split('/')[-1]\n            return CopyOperation(parent=self.drive, item_id=item_id)", "response": "Asynchronously creates a copy of this DriveItem and all its children."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of available versions for this item", "response": "def get_versions(self):\n        \"\"\" Returns a list of available versions for this item\n\n        :return: list of versions\n        :rtype: list[DriveItemVersion]\n        \"\"\"\n\n        if not self.object_id:\n            return []\n        url = self.build_url(\n            self._endpoints.get('versions').format(id=self.object_id))\n\n        response = self.con.get(url)\n        if not response:\n            return []\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return [DriveItemVersion(parent=self, **{self._cloud_data_key: item})\n                for item in data.get('value', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_version(self, version_id):\n        if not self.object_id:\n            return None\n\n        url = self.build_url(\n            self._endpoints.get('version').format(id=self.object_id,\n                                                  version_id=version_id))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return DriveItemVersion(parent=self, **{self._cloud_data_key: data})", "response": "Returns a version object for the specified id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef share_with_link(self, share_type='view', share_scope='anonymous'):\n\n        if not self.object_id:\n            return None\n\n        url = self.build_url(\n            self._endpoints.get('share_link').format(id=self.object_id))\n\n        data = {\n            'type': share_type,\n            'scope': share_scope\n        }\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # return data.get('link', {}).get('webUrl')\n        return DriveItemPermission(parent=self, **{self._cloud_data_key: data})", "response": "Creates or returns a link you can share with others"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending an invitation to access or edit this DriveItem", "response": "def share_with_invite(self, recipients, require_sign_in=True,\n                          send_email=True, message=None, share_type='view'):\n        \"\"\" Sends an invitation to access or edit this DriveItem\n\n        :param recipients: a string or Contact or a list of the former\n         representing recipients of this invitation\n        :type recipients: list[str] or list[Contact] or str or Contact\n        :param bool require_sign_in: if True the recipients\n         invited will need to log in to view the contents\n        :param bool send_email: if True an email will be send to the recipients\n        :param str message: the body text of the message emailed\n        :param str share_type: 'view': will allow to read the contents.\n         'edit' will allow to modify the contents\n        :return: link to share\n        :rtype: DriveItemPermission\n        \"\"\"\n        if not self.object_id:\n            return None\n\n        to = []\n        if recipients is None:\n            raise ValueError('Provide a valid to parameter')\n        elif isinstance(recipients, (list, tuple)):\n            for x in recipients:\n                if isinstance(x, str):\n                    to.append({'email': x})\n                elif isinstance(x, Contact):\n                    to.append({'email': x.main_email})\n                else:\n                    raise ValueError(\n                        'All the recipients must be either strings or Contacts')\n        elif isinstance(recipients, str):\n            to.append({'email': recipients})\n        elif isinstance(recipients, Contact):\n            to.append({'email': recipients.main_email})\n        else:\n            raise ValueError(\n                'All the recipients must be either strings or Contacts')\n\n        url = self.build_url(\n            self._endpoints.get('share_invite').format(id=self.object_id))\n\n        data = {\n            'recipients': to,\n            self._cc('requireSignIn'): require_sign_in,\n            self._cc('sendInvitation'): send_email,\n        }\n        if share_type in {'view', 'read'}:\n            data['roles'] = ['read']\n        elif share_type == {'edit', 'write'}:\n            data['roles'] = ['write']\n        else:\n            raise ValueError(\n                '\"{}\" is not a valid share_type'.format(share_type))\n        if send_email and message:\n            data['message'] = message\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        data = response.json()\n\n        return DriveItemPermission(parent=self, **{self._cloud_data_key: data})"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_permissions(self):\n        if not self.object_id:\n            return []\n\n        url = self.build_url(\n            self._endpoints.get('permissions').format(id=self.object_id))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return [DriveItemPermission(parent=self, **{self._cloud_data_key: item})\n                for item in data.get('value', [])]", "response": "Returns a list of DriveItemPermissions with the\n        permissions granted for this DriveItem."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_child_folder(self, name, description=None):\n\n        if not self.object_id:\n            return None\n\n        url = self.build_url(\n            self._endpoints.get('list_items').format(id=self.object_id))\n\n        data = {'name': name, 'folder': {}}\n        if description:\n            data['description'] = description\n\n        response = self.con.post(url, data=data)\n        if not response:\n            return None\n\n        folder = response.json()\n\n        return self._classifier(folder)(parent=self,\n                                        **{self._cloud_data_key: folder})", "response": "Creates a Child Folder with the specified name and description."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef upload_file(self, item, chunk_size=DEFAULT_UPLOAD_CHUNK_SIZE):\n\n        if item is None:\n            raise ValueError('Item must be a valid path to file')\n        item = Path(item) if not isinstance(item, Path) else item\n\n        if not item.exists():\n            raise ValueError('Item must exist')\n        if not item.is_file():\n            raise ValueError('Item must be a file')\n\n        file_size = item.stat().st_size\n\n        if file_size <= UPLOAD_SIZE_LIMIT_SIMPLE:\n            # Simple Upload\n            url = self.build_url(\n                self._endpoints.get('simple_upload').format(id=self.object_id,\n                                                            filename=item.name))\n            # headers = {'Content-type': 'text/plain'}\n            headers = {'Content-type': 'application/octet-stream'}\n            # headers = None\n            with item.open(mode='rb') as file:\n                data = file.read()\n\n            response = self.con.put(url, headers=headers, data=data)\n            if not response:\n                return None\n\n            data = response.json()\n\n            return self._classifier(data)(parent=self,\n                                          **{self._cloud_data_key: data})\n        else:\n            # Resumable Upload\n            url = self.build_url(\n                self._endpoints.get('create_upload_session').format(\n                    id=self.object_id, filename=item.name))\n\n            response = self.con.post(url)\n            if not response:\n                return None\n\n            data = response.json()\n\n            upload_url = data.get(self._cc('uploadUrl'), None)\n            if upload_url is None:\n                log.error('Create upload session response without '\n                          'upload_url for file {}'.format(item.name))\n                return None\n\n            current_bytes = 0\n            with item.open(mode='rb') as file:\n                while True:\n                    data = file.read(chunk_size)\n                    if not data:\n                        break\n                    transfer_bytes = len(data)\n                    headers = {\n                        'Content-type': 'application/octet-stream',\n                        'Content-Length': str(len(data)),\n                        'Content-Range': 'bytes {}-{}/{}'\n                                         ''.format(current_bytes,\n                                                   current_bytes +\n                                                   transfer_bytes - 1,\n                                                   file_size)\n                    }\n                    current_bytes += transfer_bytes\n\n                    # this request mut NOT send the authorization header.\n                    # so we use a naive simple request.\n                    response = self.con.naive_request(upload_url, 'PUT',\n                                                      data=data,\n                                                      headers=headers)\n                    if not response:\n                        return None\n\n                    if response.status_code != 202:\n                        # file is completed\n                        data = response.json()\n                        return self._classifier(data)(parent=self, **{\n                            self._cloud_data_key: data})", "response": "Uploads a file to the archive"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a collection of drive items from the API.", "response": "def _base_get_list(self, url, limit=None, *, query=None, order_by=None,\n                       batch=None):\n        \"\"\" Returns a collection of drive items \"\"\"\n\n        if limit is None or limit > self.protocol.max_top_value:\n            batch = self.protocol.max_top_value\n\n        params = {'$top': batch if batch else limit}\n\n        if order_by:\n            params['$orderby'] = order_by\n\n        if query:\n            if query.has_filters:\n                warnings.warn(\n                    'Filters are not allowed by the Api Provider '\n                    'in this method')\n                query.clear_filters()\n            if isinstance(query, str):\n                params['$filter'] = query\n            else:\n                params.update(query.as_params())\n\n        response = self.con.get(url, params=params)\n        if not response:\n            return iter(())\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        items = (\n            self._classifier(item)(parent=self, **{self._cloud_data_key: item})\n            for item in data.get('value', []))\n        next_link = data.get(NEXT_LINK_KEYWORD, None)\n        if batch and next_link:\n            return Pagination(parent=self, data=items,\n                              constructor=self._classifier,\n                              next_link=next_link, limit=limit)\n        else:\n            return items"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_items(self, limit=None, *, query=None, order_by=None, batch=None):\n\n        if self.object_id:\n            # reference the current drive_id\n            url = self.build_url(\n                self._endpoints.get('list_items').format(id=self.object_id))\n        else:\n            # we don't know the drive_id so go to the default\n            url = self.build_url(self._endpoints.get('list_items_default'))\n\n        return self._base_get_list(url, limit=limit, query=query,\n                                   order_by=order_by, batch=batch)", "response": "Returns a collection of drive items in this folder."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_item(self, item_id):\n        if self.object_id:\n            # reference the current drive_id\n            url = self.build_url(\n                self._endpoints.get('get_item').format(id=self.object_id,\n                                                       item_id=item_id))\n        else:\n            # we don't know the drive_id so go to the default drive\n            url = self.build_url(\n                self._endpoints.get('get_item_default').format(item_id=item_id))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self._classifier(data)(parent=self,\n                                      **{self._cloud_data_key: data})", "response": "Returns a DriveItem by it s Id\n\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the specified Special Folder object", "response": "def get_special_folder(self, name):\n        \"\"\" Returns the specified Special Folder\n\n        :return: a special Folder\n        :rtype: drive.Folder\n        \"\"\"\n\n        name = name if \\\n            isinstance(name, OneDriveWellKnowFolderNames) \\\n            else OneDriveWellKnowFolderNames(name.lower())\n        name = name.value\n\n        if self.object_id:\n            # reference the current drive_id\n            url = self.build_url(\n                self._endpoints.get('get_special').format(id=self.object_id,\n                                                          name=name))\n        else:\n            # we don't know the drive_id so go to the default\n            url = self.build_url(\n                self._endpoints.get('get_special_default').format(name=name))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self._classifier(data)(parent=self,\n                                      **{self._cloud_data_key: data})"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates this drive with data from the server", "response": "def refresh(self):\n        \"\"\" Updates this drive with data from the server\n\n        :return: Success / Failure\n        :rtype: bool\n        \"\"\"\n\n        if self.object_id is None:\n            url = self.build_url(self._endpoints.get('default_drive'))\n        else:\n            url = self.build_url(\n                self._endpoints.get('get_drive').format(id=self.object_id))\n\n        response = self.con.get(url)\n        if not response:\n            return False\n\n        drive = response.json()\n\n        self._update_data({self._cloud_data_key: drive})\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Drive instance with the default drive data", "response": "def get_default_drive(self, request_drive=False):\n        \"\"\" Returns a Drive instance\n\n        :param request_drive: True will make an api call to retrieve the drive\n         data\n        :return: default One Drive\n        :rtype: Drive\n        \"\"\"\n        if request_drive is False:\n            return Drive(con=self.con, protocol=self.protocol,\n                         main_resource=self.main_resource, name='Default Drive')\n\n        url = self.build_url(self._endpoints.get('default_drive'))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        drive = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.drive_constructor(con=self.con, protocol=self.protocol,\n                                      main_resource=self.main_resource,\n                                      **{self._cloud_data_key: drive})"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a Drive instance for the given id", "response": "def get_drive(self, drive_id):\n        \"\"\" Returns a Drive instance\n\n        :param drive_id: the drive_id to be retrieved\n        :return: Drive for the id\n        :rtype: Drive\n        \"\"\"\n        if not drive_id:\n            return None\n\n        url = self.build_url(\n            self._endpoints.get('get_drive').format(id=drive_id))\n\n        response = self.con.get(url)\n        if not response:\n            return None\n\n        drive = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return self.drive_constructor(con=self.con, protocol=self.protocol,\n                                      main_resource=self.main_resource,\n                                      **{self._cloud_data_key: drive})"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a collection of all the drives in this instance", "response": "def get_drives(self):\n        \"\"\" Returns a collection of drives\"\"\"\n\n        url = self.build_url(self._endpoints.get('list_drives'))\n\n        response = self.con.get(url)\n        if not response:\n            return []\n\n        data = response.json()\n\n        # Everything received from cloud must be passed as self._cloud_data_key\n        return [self.drive_constructor(parent=self, **{self._cloud_data_key: drive})\n                for drive in data.get('value', [])]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a ParsedNodePatch add the new information to the node.", "response": "def patch(self, patch):\n        \"\"\"Given a ParsedNodePatch, add the new information to the node.\"\"\"\n        # explicitly pick out the parts to update so we don't inadvertently\n        # step on the model name or anything\n        self._contents.update({\n            'patch_path': patch.original_file_path,\n            'description': patch.description,\n            'columns': patch.columns,\n            'docrefs': patch.docrefs,\n        })\n        # patches always trigger re-validation\n        self.validate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the forward and backward edges on the given list of ParsedNodes and return them as two separate dictionaries each mapping unique IDs to their lists of edges.", "response": "def build_edges(nodes):\n    \"\"\"Build the forward and backward edges on the given list of ParsedNodes\n    and return them as two separate dictionaries, each mapping unique IDs to\n    lists of edges.\n    \"\"\"\n    backward_edges = {}\n    # pre-populate the forward edge dict for simplicity\n    forward_edges = {node.unique_id: [] for node in nodes}\n    for node in nodes:\n        backward_edges[node.unique_id] = node.depends_on_nodes[:]\n        for unique_id in node.depends_on_nodes:\n            forward_edges[unique_id].append(node.unique_id)\n    return _sort_values(forward_edges), _sort_values(backward_edges)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert the parsed manifest to a nested dict structure that we can safely serialize to JSON.", "response": "def serialize(self):\n        \"\"\"Convert the parsed manifest to a nested dict structure that we can\n        safely serialize to JSON.\n        \"\"\"\n        forward_edges, backward_edges = build_edges(self.nodes.values())\n\n        return {\n            'nodes': {k: v.serialize() for k, v in self.nodes.items()},\n            'macros': {k: v.serialize() for k, v in self.macros.items()},\n            'docs': {k: v.serialize() for k, v in self.docs.items()},\n            'parent_map': backward_edges,\n            'child_map': forward_edges,\n            'generated_at': self.generated_at,\n            'metadata': self.metadata,\n            'disabled': [v.serialize() for v in self.disabled],\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef find_macro_by_name(self, name, package):\n        return self._find_by_name(name, package, 'macros', [NodeType.Macro])", "response": "Find a macro in the graph by its name and package name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds any valid target for ref (", "response": "def find_refable_by_name(self, name, package):\n        \"\"\"Find any valid target for \"ref()\" in the graph by its name and\n        package name, or None for any package.\n        \"\"\"\n        return self._find_by_name(name, package, 'nodes', NodeType.refable())"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_source_by_name(self, source_name, table_name, package):\n        name = '{}.{}'.format(source_name, table_name)\n        return self._find_by_name(name, package, 'nodes', [NodeType.Source])", "response": "Find any valid source node for source_name and table_name and package name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _filter_subgraph(self, subgraph, predicate):\n        to_return = []\n\n        for unique_id, item in subgraph.items():\n            if predicate(item):\n                to_return.append(item)\n\n        return to_return", "response": "Filter a subgraph by a predicate."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_unique_ids_for_schema_and_table(self, schema, table):\n        def predicate(model):\n            return self._model_matches_schema_and_table(schema, table, model)\n\n        matching = list(self._filter_subgraph(self.nodes, predicate))\n        return [match.get('unique_id') for match in matching]", "response": "Given a schema and table find matching models and return their unique_ids."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_nodes(self, new_nodes):\n        for unique_id, node in new_nodes.items():\n            if unique_id in self.nodes:\n                raise_duplicate_resource_name(node, self.nodes[unique_id])\n            self.nodes[unique_id] = node", "response": "Add the given dict of new nodes to the manifest."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\npatches nodes with the given dict of patches. Note that this consumes the input!", "response": "def patch_nodes(self, patches):\n        \"\"\"Patch nodes with the given dict of patches. Note that this consumes\n        the input!\n        \"\"\"\n        # because we don't have any mapping from node _names_ to nodes, and we\n        # only have the node name in the patch, we have to iterate over all the\n        # nodes looking for matching names. We could use _find_by_name if we\n        # were ok with doing an O(n*m) search (one nodes scan per patch)\n        for node in self.nodes.values():\n            if node.resource_type != NodeType.Model:\n                continue\n            patch = patches.pop(node.name, None)\n            if not patch:\n                continue\n            node.patch(patch)\n\n        # log debug-level warning about nodes we couldn't find\n        if patches:\n            for patch in patches.values():\n                # since patches aren't nodes, we can't use the existing\n                # target_not_found warning\n                logger.debug((\n                    'WARNING: Found documentation for model \"{}\" which was '\n                    'not found or is disabled').format(patch.name)\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef to_flat_graph(self):\n        return {\n            'nodes': {k: v.to_shallow_dict() for k, v in self.nodes.items()},\n            'macros': self.macros,\n        }", "response": "Convert the parsed manifest to a flat graph that the compiler\n        expects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting list of dictionaries into an Agate table", "response": "def table_from_data(data, column_names):\n    \"Convert list of dictionaries into an Agate table\"\n\n    # The agate table is generated from a list of dicts, so the column order\n    # from `data` is not preserved. We can use `select` to reorder the columns\n    #\n    # If there is no data, create an empty table with the specified columns\n\n    if len(data) == 0:\n        return agate.Table([], column_names=column_names)\n    else:\n        table = agate.Table.from_object(data, column_types=DEFAULT_TYPE_TESTER)\n        return table.select(column_names)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_schema_func(self):\n        if self._get_schema_func is not None:\n            return self._get_schema_func\n\n        get_schema_macro = self.macro_manifest.find_macro_by_name(\n            'generate_schema_name',\n            self.root_project_config.project_name\n        )\n        if get_schema_macro is None:\n            get_schema_macro = self.macro_manifest.find_macro_by_name(\n                'generate_schema_name',\n                GLOBAL_PROJECT_NAME\n            )\n        if get_schema_macro is None:\n            def get_schema(_):\n                return self.default_schema\n        else:\n            root_context = dbt.context.parser.generate_macro(\n                get_schema_macro, self.root_project_config,\n                self.macro_manifest\n            )\n            get_schema = get_schema_macro.generator(root_context)\n\n        self._get_schema_func = get_schema\n        return self._get_schema_func", "response": "This method returns the function that returns the schema for the current version of the project."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _build_intermediate_node_dict(self, config, node_dict, node_path,\n                                      package_project_config, tags, fqn,\n                                      agate_table, archive_config,\n                                      column_name):\n        \"\"\"Update the unparsed node dictionary and build the basis for an\n        intermediate ParsedNode that will be passed into the renderer\n        \"\"\"\n        # because this takes and returns dicts, subclasses can safely override\n        # this and mutate its results using super() both before and after.\n        if agate_table is not None:\n            node_dict['agate_table'] = agate_table\n\n        # Set this temporarily. Not the full config yet (as config() hasn't\n        # been called from jinja yet). But the Var() call below needs info\n        # about project level configs b/c they might contain refs.\n        # TODO: Restructure this?\n        config_dict = coalesce(archive_config, {})\n        config_dict.update(config.config)\n\n        empty = (\n            'raw_sql' in node_dict and len(node_dict['raw_sql'].strip()) == 0\n        )\n\n        node_dict.update({\n            'refs': [],\n            'sources': [],\n            'depends_on': {\n                'nodes': [],\n                'macros': [],\n            },\n            'unique_id': node_path,\n            'empty': empty,\n            'fqn': fqn,\n            'tags': tags,\n            'config': config_dict,\n            # Set these temporarily so get_rendered() has access to a schema,\n            # database, and alias.\n            'schema': self.default_schema,\n            'database': self.default_database,\n            'alias': node_dict.get('name'),\n        })\n\n        # if there's a column, it should end up part of the ParsedNode\n        if column_name is not None:\n            node_dict['column_name'] = column_name\n\n        return node_dict", "response": "Update the unparsed node dictionary and build the basis for an intermediate ParsedNode that will be passed into the renderer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrendering the node s sql with the given config.", "response": "def _render_with_context(self, parsed_node, config):\n        \"\"\"Given the parsed node and a SourceConfig to use during parsing,\n        render the node's sql wtih macro capture enabled.\n\n        Note: this mutates the config object when config() calls are rendered.\n        \"\"\"\n        context = dbt.context.parser.generate(\n            parsed_node,\n            self.root_project_config,\n            self.macro_manifest,\n            config)\n\n        dbt.clients.jinja.get_rendered(\n            parsed_node.raw_sql, context, parsed_node.to_shallow_dict(),\n            capture_macros=True)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _update_parsed_node_info(self, parsed_node, config):\n        # Special macro defined in the global project. Use the root project's\n        # definition, not the current package\n        schema_override = config.config.get('schema')\n        get_schema = self.get_schema_func()\n        parsed_node.schema = get_schema(schema_override).strip()\n\n        alias_override = config.config.get('alias')\n        get_alias = self.get_alias_func()\n        parsed_node.alias = get_alias(parsed_node, alias_override).strip()\n\n        parsed_node.database = config.config.get(\n            'database', self.default_database\n        ).strip()\n\n        # Set tags on node provided in config blocks\n        model_tags = config.config.get('tags', [])\n        parsed_node.tags.extend(model_tags)\n\n        # Overwrite node config\n        config_dict = parsed_node.get('config', {})\n        config_dict.update(config.config)\n        parsed_node.config = config_dict\n\n        for hook_type in dbt.hooks.ModelHookType.Both:\n            parsed_node.config[hook_type] = dbt.hooks.get_hooks(parsed_node,\n                                                                hook_type)", "response": "Update the parsed node with the information from the source config."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef parse_node(self, node, node_path, package_project_config, tags=None,\n                   fqn_extra=None, fqn=None, agate_table=None,\n                   archive_config=None, column_name=None):\n        \"\"\"Parse a node, given an UnparsedNode and any other required information.\n\n        agate_table should be set if the node came from a seed file.\n        archive_config should be set if the node is an Archive node.\n        column_name should be set if the node is a Test node associated with a\n        particular column.\n        \"\"\"\n        logger.debug(\"Parsing {}\".format(node_path))\n\n        tags = coalesce(tags, [])\n        fqn_extra = coalesce(fqn_extra, [])\n\n        if fqn is None:\n            fqn = self.get_fqn(node, package_project_config, fqn_extra)\n\n        config = SourceConfig(\n            self.root_project_config,\n            package_project_config,\n            fqn,\n            node.resource_type)\n\n        parsed_dict = self._build_intermediate_node_dict(\n            config, node.serialize(), node_path, config, tags, fqn,\n            agate_table, archive_config, column_name\n        )\n        parsed_node = ParsedNode(**parsed_dict)\n\n        self._render_with_context(parsed_node, config)\n        self._update_parsed_node_info(parsed_node, config)\n\n        parsed_node.validate()\n\n        return parsed_node", "response": "Parse a node and return a ParsedNode object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef check_block_parsing(self, name, path, contents):\n        if not dbt.flags.TEST_NEW_PARSER:\n            return True\n        try:\n            dbt.clients.jinja.extract_toplevel_blocks(contents)\n        except Exception:\n            return False\n        return True", "response": "Check if we were able to extract toplevel blocks from the given contents. Return True if we were able to extract toplevel blocks False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef column_to_bq_schema(self):\n        kwargs = {}\n        if len(self.fields) > 0:\n            fields = [field.column_to_bq_schema() for field in self.fields]\n            kwargs = {\"fields\": fields}\n\n        return google.cloud.bigquery.SchemaField(self.name, self.dtype,\n                                                 self.mode, **kwargs)", "response": "Convert a column to a bigquery schema object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef runtime_cleanup(self, selected_uids):\n        self.run_count = 0\n        self.num_nodes = len(selected_uids)\n        self.node_results = []\n        self._skipped_children = {}\n        self._skipped_children = {}\n        self._raise_next_tick = None", "response": "Do some pre - run cleanup that is usually performed in Task. __init__."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef parse_seed_file(cls, file_match, root_dir, package_name, should_parse):\n        abspath = file_match['absolute_path']\n        logger.debug(\"Parsing {}\".format(abspath))\n        table_name = os.path.basename(abspath)[:-4]\n        node = UnparsedNode(\n            path=file_match['relative_path'],\n            name=table_name,\n            root_path=root_dir,\n            resource_type=NodeType.Seed,\n            # Give this raw_sql so it conforms to the node spec,\n            # use dummy text so it doesn't look like an empty node\n            raw_sql='-- csv --',\n            package_name=package_name,\n            original_file_path=os.path.join(file_match.get('searched_path'),\n                                            file_match.get('relative_path')),\n        )\n        if should_parse:\n            try:\n                table = dbt.clients.agate_helper.from_csv(abspath)\n            except ValueError as e:\n                dbt.exceptions.raise_compiler_error(str(e), node)\n        else:\n            table = dbt.clients.agate_helper.empty_table()\n        table.original_abspath = abspath\n        return node, table", "response": "Parse the given seed file and return an UnparsedNode and the agate\n            table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_and_parse(self, package_name, root_dir, relative_dirs, tags=None):\n\n        extension = \"[!.#~]*.csv\"\n        if dbt.flags.STRICT_MODE:\n            dbt.contracts.project.ProjectList(**self.all_projects)\n\n        file_matches = dbt.clients.system.find_matching(\n            root_dir,\n            relative_dirs,\n            extension)\n\n        # we only want to parse seeds if we're inside 'dbt seed'\n        should_parse = self.root_project_config.args.which == 'seed'\n\n        result = {}\n        for file_match in file_matches:\n            node, agate_table = self.parse_seed_file(file_match, root_dir,\n                                                     package_name,\n                                                     should_parse)\n            node_path = self.get_path(NodeType.Seed, package_name, node.name)\n            parsed = self.parse_node(node, node_path,\n                                     self.all_projects.get(package_name),\n                                     tags=tags, agate_table=agate_table)\n            result[node_path] = parsed\n\n        return result", "response": "Load and parse seed files in a list of directories. Returns a dict\n           that maps unique ids onto ParsedNodes"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngo through source looking for every key value pair where the key starts with the given prefix.", "response": "def get_stripped_prefix(source, prefix):\n    \"\"\"Go through source, extracting every key/value pair where the key starts\n    with the given prefix.\n    \"\"\"\n    cut = len(prefix)\n    return {\n        k[cut:]: v for k, v in source.items()\n        if k.startswith(prefix)\n    }"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef format_stats(stats):\n    stats_collector = {}\n    for stat_key, stat_value in stats.items():\n        stat_id, stat_field = stat_key.split(\":\")\n\n        stats_collector.setdefault(stat_id, {\"id\": stat_id})\n        stats_collector[stat_id][stat_field] = stat_value\n\n    # strip out all the stats we don't want\n    stats_collector = {\n        stat_id: stats\n        for stat_id, stats in stats_collector.items()\n        if stats.get('include', False)\n    }\n\n    # we always have a 'has_stats' field, it's never included\n    has_stats = {\n        'id': 'has_stats',\n        'label': 'Has Stats?',\n        'value': len(stats_collector) > 0,\n        'description': 'Indicates whether there are statistics for this table',\n        'include': False,\n    }\n    stats_collector['has_stats'] = has_stats\n    return stats_collector", "response": "This function will convert a dictionary into a structure that can be used to create a new stats page."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a list of dictionaries representing column dictionaries return a nested structure with the keys that are not present in the table metadata and the values that are present in the table stats.", "response": "def unflatten(columns):\n    \"\"\"Given a list of column dictionaries following this layout:\n\n        [{\n            'column_comment': None,\n            'column_index': Decimal('1'),\n            'column_name': 'id',\n            'column_type': 'integer',\n            'table_comment': None,\n            'table_name': 'test_table',\n            'table_schema': 'test_schema',\n            'table_type': 'BASE TABLE'\n        }]\n\n    unflatten will convert them into a dict with this nested structure:\n\n        {\n            'test_schema': {\n                'test_table': {\n                    'metadata': {\n                        'comment': None,\n                        'name': 'test_table',\n                        'type': 'BASE TABLE',\n                        'schema': 'test_schema',\n                    },\n                    'columns': {\n                        \"id\": {\n                            'type': 'integer',\n                            'comment': None,\n                            'index': bigint(1),\n                            'name': 'id'\n                        }\n                    }\n                }\n            }\n        }\n\n    Required keys in each column: table_schema, table_name, column_index\n\n    Keys prefixed with 'column_' end up in per-column data and keys prefixed\n    with 'table_' end up in table metadata. Keys without either prefix are\n    ignored.\n    \"\"\"\n    structured = {}\n    for entry in columns:\n        schema_name = entry['table_schema']\n        table_name = entry['table_name']\n\n        if schema_name not in structured:\n            structured[schema_name] = {}\n        schema = structured[schema_name]\n\n        if table_name not in schema:\n            metadata = get_stripped_prefix(entry, 'table_')\n            stats = get_stripped_prefix(entry, 'stats:')\n            stats_dict = format_stats(stats)\n\n            schema[table_name] = {\n                'metadata': metadata,\n                'stats': stats_dict,\n                'columns': {}\n            }\n\n        table = schema[table_name]\n\n        column = get_stripped_prefix(entry, 'column_')\n\n        # the index should really never be that big so it's ok to end up\n        # serializing this to JSON (2^53 is the max safe value there)\n        column['index'] = bigint(column['index'])\n        table['columns'][column['name']] = column\n    return structured"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nupdates an existing context in - place adding the given macro map to the appropriate package namespace.", "response": "def _add_macro_map(context, package_name, macro_map):\n    \"\"\"Update an existing context in-place, adding the given macro map to the\n    appropriate package namespace. Adapter packages get inserted into the\n    global namespace.\n    \"\"\"\n    key = package_name\n    if package_name in PACKAGES:\n        key = GLOBAL_PROJECT_NAME\n    if key not in context:\n        context[key] = {}\n\n    context[key].update(macro_map)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate the common aspects of the config dict.", "response": "def generate_base(model, model_dict, config, manifest, source_config,\n                  provider, adapter=None):\n    \"\"\"Generate the common aspects of the config dict.\"\"\"\n    if provider is None:\n        raise dbt.exceptions.InternalException(\n            \"Invalid provider given to context: {}\".format(provider))\n\n    target_name = config.target_name\n    target = config.to_profile_info()\n    del target['credentials']\n    target.update(config.credentials.serialize(with_aliases=True))\n    target['type'] = config.credentials.type\n    target.pop('pass', None)\n    target['name'] = target_name\n\n    adapter = get_adapter(config)\n\n    context = {'env': target}\n\n    pre_hooks = None\n    post_hooks = None\n\n    db_wrapper = DatabaseWrapper(adapter)\n\n    context = dbt.utils.merge(context, {\n        \"adapter\": db_wrapper,\n        \"api\": {\n            \"Relation\": db_wrapper.Relation,\n            \"Column\": adapter.Column,\n        },\n        \"column\": adapter.Column,\n        \"config\": provider.Config(model_dict, source_config),\n        \"database\": config.credentials.database,\n        \"env_var\": env_var,\n        \"exceptions\": dbt.exceptions.wrapped_exports(model),\n        \"execute\": provider.execute,\n        \"flags\": dbt.flags,\n        # TODO: Do we have to leave this in?\n        \"graph\": manifest.to_flat_graph(),\n        \"log\": log,\n        \"model\": model_dict,\n        \"modules\": {\n            \"pytz\": get_pytz_module_context(),\n            \"datetime\": get_datetime_module_context(),\n        },\n        \"post_hooks\": post_hooks,\n        \"pre_hooks\": pre_hooks,\n        \"ref\": provider.ref(db_wrapper, model, config, manifest),\n        \"return\": _return,\n        \"schema\": config.credentials.schema,\n        \"sql\": None,\n        \"sql_now\": adapter.date_function(),\n        \"source\": provider.source(db_wrapper, model, config, manifest),\n        \"fromjson\": fromjson,\n        \"tojson\": tojson,\n        \"target\": target,\n        \"try_or_compiler_error\": try_or_compiler_error(model)\n    })\n\n    return context"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate a new version of the current language.", "response": "def generate(model, config, manifest, source_config=None, provider=None):\n    \"\"\"\n    Not meant to be called directly. Call with either:\n        dbt.context.parser.generate\n    or\n        dbt.context.runtime.generate\n    \"\"\"\n    return generate_model(model, config, manifest, source_config, provider)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new object with the desired output schema and write it.", "response": "def write(self, path):\n        \"\"\"Create a new object with the desired output schema and write it.\"\"\"\n        meta = {\n            'generated_at': self.generated_at,\n            'elapsed_time': self.elapsed_time,\n        }\n        sources = {}\n        for result in self.results:\n            unique_id = result.node.unique_id\n            if result.error is not None:\n                result_dict = {\n                    'error': result.error,\n                    'state': 'runtime error'\n                }\n            else:\n                result_dict = {\n                    'max_loaded_at': result.max_loaded_at,\n                    'snapshotted_at': result.snapshotted_at,\n                    'max_loaded_at_time_ago_in_s': result.age,\n                    'state': result.status,\n                    'criteria': result.node.freshness,\n                }\n            sources[unique_id] = result_dict\n        output = FreshnessRunOutput(meta=meta, sources=sources)\n        output.write(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _make_key(relation):\n    return _ReferenceKey(_lower(relation.database),\n                         _lower(relation.schema),\n                         _lower(relation.identifier))", "response": "Make a _ReferenceKey with lowercase values for the cache so we don t have quotes\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef collect_consequences(self):\n        consequences = {self.key()}\n        for relation in self.referenced_by.values():\n            consequences.update(relation.collect_consequences())\n        return consequences", "response": "Recursively collect a set of _ReferenceKeys that would\n        consequentially get dropped if this were dropped via\n        drop... cascade."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef rename(self, new_relation):\n        # Relations store this stuff inside their `path` dict. But they\n        # also store a table_name, and usually use it in their  .render(),\n        # so we need to update that as well. It doesn't appear that\n        # table_name is ever anything but the identifier (via .create())\n        self.inner = self.inner.incorporate(\n            path={\n                'database': new_relation.inner.database,\n                'schema': new_relation.inner.schema,\n                'identifier': new_relation.inner.identifier\n            },\n            table_name=new_relation.inner.identifier\n        )", "response": "Rename this cached relation to new_relation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nrenames a reference that may or may not exist. Only handles the old_key and new_key.", "response": "def rename_key(self, old_key, new_key):\n        \"\"\"Rename a reference that may or may not exist. Only handles the\n        reference itself, so this is the other half of what `rename` does.\n\n        If old_key is not in referenced_by, this is a no-op.\n\n        :param _ReferenceKey old_key: The old key to be renamed.\n        :param _ReferenceKey new_key: The new key to rename to.\n        :raises InternalError: If the new key already exists.\n        \"\"\"\n        if new_key in self.referenced_by:\n            dbt.exceptions.raise_cache_inconsistent(\n                'in rename of \"{}\" -> \"{}\", new name is in the cache already'\n                .format(old_key, new_key)\n            )\n\n        if old_key not in self.referenced_by:\n            return\n        value = self.referenced_by.pop(old_key)\n        self.referenced_by[new_key] = value"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_schema(self, database, schema):\n        self.schemas.add((_lower(database), _lower(schema)))", "response": "Add a schema to the set of known schemas ( case - insensitive"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nremove a schema from the set of known schemas.", "response": "def remove_schema(self, database, schema):\n        \"\"\"Remove a schema from the set of known schemas (case-insensitive)\n\n        If the schema does not exist, it will be ignored - it could just be a\n        temporary table.\n\n        :param str database: The database name to remove.\n        :param str schema: The schema name to remove.\n        \"\"\"\n        self.schemas.discard((_lower(database), _lower(schema)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds multiple schemas to the set of known schemas ( case - insensitive", "response": "def update_schemas(self, schemas):\n        \"\"\"Add multiple schemas to the set of known schemas (case-insensitive)\n\n        :param Iterable[str] schemas: An iterable of the schema names to add.\n        \"\"\"\n        self.schemas.update((_lower(d), _lower(s)) for (d, s) in schemas)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a relation to the cache or return it if it already exists.", "response": "def _setdefault(self, relation):\n        \"\"\"Add a relation to the cache, or return it if it already exists.\n\n        :param _CachedRelation relation: The relation to set or get.\n        :return _CachedRelation: The relation stored under the given relation's\n            key\n        \"\"\"\n        self.add_schema(relation.database, relation.schema)\n        key = relation.key()\n        return self.relations.setdefault(key, relation)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a link between two related entries to the database. Both the old and the new entries must alraedy exist in the database.", "response": "def _add_link(self, referenced_key, dependent_key):\n        \"\"\"Add a link between two relations to the database. Both the old and\n        new entries must alraedy exist in the database.\n\n        :param _ReferenceKey referenced_key: The key identifying the referenced\n            model (the one that if dropped will drop the dependent model).\n        :param _ReferenceKey dependent_key: The key identifying the dependent\n            model.\n        :raises InternalError: If either entry does not exist.\n        \"\"\"\n        referenced = self.relations.get(referenced_key)\n        if referenced is None:\n            dbt.exceptions.raise_cache_inconsistent(\n                'in add_link, referenced link key {} not in cache!'\n                .format(referenced_key)\n            )\n\n        dependent = self.relations.get(dependent_key)\n        if dependent is None:\n            dbt.exceptions.raise_cache_inconsistent(\n                'in add_link, dependent link key {} not in cache!'\n                .format(dependent_key)\n            )\n\n        referenced.add_reference(dependent)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a link between two relations to the database. Both the old and the new entries must already exist in the database.", "response": "def add_link(self, referenced, dependent):\n        \"\"\"Add a link between two relations to the database. Both the old and\n        new entries must already exist in the database.\n\n        The dependent model refers _to_ the referenced model. So, given\n        arguments of (jake_test, bar, jake_test, foo):\n        both values are in the schema jake_test and foo is a view that refers\n        to bar, so \"drop bar cascade\" will drop foo and all of foo's\n        dependents.\n\n        :param BaseRelation referenced: The referenced model.\n        :param BaseRelation dependent: The dependent model.\n        :raises InternalError: If either entry does not exist.\n        \"\"\"\n        referenced = _make_key(referenced)\n        if (referenced.database, referenced.schema) not in self:\n            # if we have not cached the referenced schema at all, we must be\n            # referring to a table outside our control. There's no need to make\n            # a link - we will never drop the referenced relation during a run.\n            logger.debug(\n                '{dep!s} references {ref!s} but {ref.database}.{ref.schema} '\n                'is not in the cache, skipping assumed external relation'\n                .format(dep=dependent, ref=referenced)\n            )\n            return\n        dependent = _make_key(dependent)\n        logger.debug(\n            'adding link, {!s} references {!s}'.format(dependent, referenced)\n        )\n        with self.lock:\n            self._add_link(referenced, dependent)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, relation):\n        cached = _CachedRelation(relation)\n        logger.debug('Adding relation: {!s}'.format(cached))\n        logger.debug('before adding: {}'.format(\n            pprint.pformat(self.dump_graph()))\n        )\n        with self.lock:\n            self._setdefault(cached)\n        logger.debug('after adding: {}'.format(\n            pprint.pformat(self.dump_graph()))\n        )", "response": "Add the relation to the cache under the schema and the identifier identifier."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _remove_refs(self, keys):\n        # remove direct refs\n        for key in keys:\n            del self.relations[key]\n        # then remove all entries from each child\n        for cached in self.relations.values():\n            cached.release_references(keys)", "response": "Removes all references to all entries in keys. This does not cascade!"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _drop_cascade_relation(self, dropped):\n        if dropped not in self.relations:\n            logger.debug('dropped a nonexistent relationship: {!s}'\n                         .format(dropped))\n            return\n        consequences = self.relations[dropped].collect_consequences()\n        logger.debug(\n            'drop {} is cascading to {}'.format(dropped, consequences)\n        )\n        self._remove_refs(consequences)", "response": "Drop the given relation and cascade it appropriately to all\nAttributeNames dependent relations."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef drop(self, relation):\n        dropped = _make_key(relation)\n        logger.debug('Dropping relation: {!s}'.format(dropped))\n        with self.lock:\n            self._drop_cascade_relation(dropped)", "response": "Drop the named relation and cascade it appropriately to all\n        dependent relations."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rename_relation(self, old_key, new_relation):\n        # On the database level, a rename updates all values that were\n        # previously referenced by old_name to be referenced by new_name.\n        # basically, the name changes but some underlying ID moves. Kind of\n        # like an object reference!\n        relation = self.relations.pop(old_key)\n        new_key = new_relation.key()\n\n        # relaton has to rename its innards, so it needs the _CachedRelation.\n        relation.rename(new_relation)\n        # update all the relations that refer to it\n        for cached in self.relations.values():\n            if cached.is_referenced_by(old_key):\n                logger.debug(\n                    'updated reference from {0} -> {2} to {1} -> {2}'\n                    .format(old_key, new_key, cached.key())\n                )\n                cached.rename_key(old_key, new_key)\n\n        self.relations[new_key] = relation\n        # also fixup the schemas!\n        self.remove_schema(old_key.database, old_key.schema)\n        self.add_schema(new_key.database, new_key.schema)\n\n        return True", "response": "Rename a relation named old_key to new_key updating references."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck the rename constraints and returns whether or not the rename can proceed.", "response": "def _check_rename_constraints(self, old_key, new_key):\n        \"\"\"Check the rename constraints, and return whether or not the rename\n        can proceed.\n\n        If the new key is already present, that is an error.\n        If the old key is absent, we debug log and return False, assuming it's\n        a temp table being renamed.\n\n        :param _ReferenceKey old_key: The existing key, to rename from.\n        :param _ReferenceKey new_key: The new key, to rename to.\n        :return bool: If the old relation exists for renaming.\n        :raises InternalError: If the new key is already present.\n        \"\"\"\n        if new_key in self.relations:\n            dbt.exceptions.raise_cache_inconsistent(\n                'in rename, new key {} already in cache: {}'\n                .format(new_key, list(self.relations.keys()))\n            )\n\n        if old_key not in self.relations:\n            logger.debug(\n                'old key {} not found in self.relations, assuming temporary'\n                .format(old_key)\n            )\n            return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrenaming the old schema / identifier to the new schema / identifier and update references.", "response": "def rename(self, old, new):\n        \"\"\"Rename the old schema/identifier to the new schema/identifier and\n        update references.\n\n        If the new schema/identifier is already present, that is an error.\n        If the schema/identifier key is absent, we only debug log and return,\n        assuming it's a temp table being renamed.\n\n        :param BaseRelation old: The existing relation name information.\n        :param BaseRelation new: The new relation name information.\n        :raises InternalError: If the new key is already present.\n        \"\"\"\n        old_key = _make_key(old)\n        new_key = _make_key(new)\n        logger.debug('Renaming relation {!s} to {!s}'.format(\n            old_key, new_key)\n        )\n        logger.debug('before rename: {}'.format(\n            pprint.pformat(self.dump_graph()))\n        )\n\n        with self.lock:\n            if self._check_rename_constraints(old_key, new_key):\n                self._rename_relation(old_key, _CachedRelation(new))\n            else:\n                self._setdefault(_CachedRelation(new))\n\n        logger.debug('after rename: {}'.format(\n            pprint.pformat(self.dump_graph()))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_relations(self, database, schema):\n        schema = _lower(schema)\n        with self.lock:\n            results = [\n                r.inner for r in self.relations.values()\n                if (r.schema == _lower(schema) and\n                    r.database == _lower(database))\n            ]\n\n        if None in results:\n            dbt.exceptions.raise_cache_inconsistent(\n                'in get_relations, a None relation was found in the cache!'\n            )\n        return results", "response": "Case - insensitively yield all relations matching the given schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef clear(self):\n        with self.lock:\n            self.relations.clear()\n            self.schemas.clear()", "response": "Clear the cache of the related and schema."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits sql statements at semicolons into discrete queries", "response": "def _split_queries(cls, sql):\n        \"Splits sql statements at semicolons into discrete queries\"\n\n        sql_s = dbt.compat.to_string(sql)\n        sql_buf = StringIO(sql_s)\n        split_query = snowflake.connector.util_text.split_statements(sql_buf)\n        return [part[0] for part in split_query]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget Snowflake private key by path or None.", "response": "def _get_private_key(cls, private_key_path, private_key_passphrase):\n        \"\"\"Get Snowflake private key by path or None.\"\"\"\n        if private_key_path is None or private_key_passphrase is None:\n            return None\n\n        with open(private_key_path, 'rb') as key:\n            p_key = serialization.load_pem_private_key(\n                key.read(),\n                password=private_key_passphrase.encode(),\n                backend=default_backend())\n\n        return p_key.private_bytes(\n            encoding=serialization.Encoding.DER,\n            format=serialization.PrivateFormat.PKCS8,\n            encryption_algorithm=serialization.NoEncryption())"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _rollback_handle(cls, connection):\n        try:\n            connection.handle.rollback()\n        except snowflake.connector.errors.ProgrammingError as e:\n            msg = dbt.compat.to_string(e)\n            if 'Session no longer exists' not in msg:\n                raise", "response": "On snowflake rolling back the handle of an aborted session raises\n            an exception."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_all(self, models, path, package_name, root_dir):\n        filtered = _filter_validate(path, 'models', models, UnparsedNodeUpdate)\n        nodes = itertools.chain.from_iterable(\n            self.parse_models_entry(model, path, package_name, root_dir)\n            for model in filtered\n        )\n        for node_type, node in nodes:\n            yield node_type, node", "response": "Parse all the model dictionaries in the models section of the schema. yml file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nparse all the model dictionaries in sources.", "response": "def parse_all(self, sources, path, package_name, root_dir):\n        \"\"\"Parse all the model dictionaries in sources.\n\n        :param List[dict] sources: The `sources` section of the schema.yml, as\n            a list of dicts.\n        :param str path: The path to the schema.yml file\n        :param str package_name: The name of the current package\n        :param str root_dir: The root directory of the search\n        \"\"\"\n        filtered = _filter_validate(path, 'sources', sources,\n                                    self._sources_validate)\n        nodes = itertools.chain.from_iterable(\n            self.parse_source_entry(source, path, package_name, root_dir)\n            for source in filtered\n        )\n\n        for node_type, node in nodes:\n            yield node_type, node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_schema_yml(cls, package_name, root_dir, relative_dirs):\n        extension = \"[!.#~]*.yml\"\n\n        file_matches = dbt.clients.system.find_matching(\n            root_dir,\n            relative_dirs,\n            extension)\n\n        for file_match in file_matches:\n            file_contents = dbt.clients.system.load_file_contents(\n                file_match.get('absolute_path'), strip=False)\n            test_path = file_match.get('relative_path', '')\n\n            original_file_path = os.path.join(file_match.get('searched_path'),\n                                              test_path)\n\n            try:\n                test_yml = dbt.clients.yaml_helper.load_yaml_text(\n                    file_contents\n                )\n            except dbt.exceptions.ValidationException as e:\n                test_yml = None\n                logger.info(\"Error reading {}:{} - Skipping\\n{}\".format(\n                            package_name, test_path, e))\n\n            if test_yml is None:\n                continue\n\n            yield original_file_path, test_yml", "response": "This function finds the schema. yml files in the specified directory and returns a generator that yields the original filepath and loaded yaml content."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _subset_graph(graph, include_nodes):\n    new_graph = nx.algorithms.transitive_closure(graph)\n\n    include_nodes = set(include_nodes)\n\n    for node in graph.nodes():\n        if node not in include_nodes:\n            new_graph.remove_node(node)\n\n    for node in include_nodes:\n        if node not in new_graph:\n            raise RuntimeError(\n                \"Couldn't find model '{}' -- does it exist or is \"\n                \"it disabled?\".format(node)\n            )\n    return new_graph", "response": "Create and return a shallow copy of graph but with only the nodes in include_nodes."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calculate_scores(self):\n        scores = {}\n        for node in self.graph.nodes():\n            score = -1 * len([\n                d for d in nx.descendants(self.graph, node)\n                if self._include_in_cost(d)\n            ])\n            scores[node] = score\n        return scores", "response": "Calculate the score of each node in the priority queue based on how many blocking descendants it has."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a node off the inner priority queue.", "response": "def get(self, block=True, timeout=None):\n        \"\"\"Get a node off the inner priority queue. By default, this blocks.\n\n        This takes the lock, but only for part of it.\n\n        :param bool block: If True, block until the inner queue has data\n        :param Optional[float] timeout: If set, block for timeout seconds\n            waiting for data.\n        :return ParsedNode: The node as present in the manifest.\n\n        See `queue.PriorityQueue` for more information on `get()` behavior and\n        exceptions.\n        \"\"\"\n        _, node_id = self.inner.get(block=block, timeout=timeout)\n        with self.lock:\n            self._mark_in_progress(node_id)\n        return self.get_node(node_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _find_new_additions(self):\n        for node, in_degree in self.graph.in_degree_iter():\n            if not self._already_known(node) and in_degree == 0:\n                self.inner.put((self._scores[node], node))\n                self.queued.add(node)", "response": "Find any nodes that need to be added to the internal\n        queue and add them to the internal\n        queue."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngiving a node's unique ID, mark it as done. This method takes the lock. :param str node_id: The node ID to mark as complete.", "response": "def mark_done(self, node_id):\n        \"\"\"Given a node's unique ID, mark it as done.\n\n        This method takes the lock.\n\n        :param str node_id: The node ID to mark as complete.\n        \"\"\"\n        with self.lock:\n            self.in_progress.remove(node_id)\n            self.graph.remove_node(node_id)\n            self._find_new_additions()\n            self.inner.task_done()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmark the node as in progress.", "response": "def _mark_in_progress(self, node_id):\n        \"\"\"Mark the node as 'in progress'.\n\n        Callers must hold the lock.\n\n        :param str node_id: The node ID to mark as in progress.\n        \"\"\"\n        self.queued.remove(node_id)\n        self.in_progress.add(node_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a queue over nodes in the graph that tracks progress of the current node.", "response": "def as_graph_queue(self, manifest, limit_to=None):\n        \"\"\"Returns a queue over nodes in the graph that tracks progress of\n        dependecies.\n        \"\"\"\n        if limit_to is None:\n            graph_nodes = self.graph.nodes()\n        else:\n            graph_nodes = limit_to\n\n        new_graph = _subset_graph(self.graph, graph_nodes)\n        return GraphQueue(new_graph, manifest)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nindicating that node1 depends on node2", "response": "def dependency(self, node1, node2):\n        \"indicate that node1 depends on node2\"\n        self.graph.add_node(node1)\n        self.graph.add_node(node2)\n        self.graph.add_edge(node2, node1)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nwrite the graph to a gpickle file.", "response": "def write_graph(self, outfile, manifest):\n        \"\"\"Write the graph to a gpickle file. Before doing so, serialize and\n        include all nodes in their corresponding graph entries.\n        \"\"\"\n        out_graph = _updated_graph(self.graph, manifest)\n        nx.write_gpickle(out_graph, outfile)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize_config_values(parsed):\n    try:\n        cfg = UserConfig.from_directory(parsed.profiles_dir)\n    except RuntimeException:\n        cfg = UserConfig.from_dict(None)\n\n    if cfg.send_anonymous_usage_stats:\n        dbt.tracking.initialize_tracking(parsed.profiles_dir)\n    else:\n        dbt.tracking.do_not_track()\n\n    if cfg.use_colors:\n        dbt.ui.printer.use_colors()", "response": "Given the parsed args initialize the dbt tracking code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef find_matching(root_path,\n                  relative_paths_to_search,\n                  file_pattern):\n    \"\"\"\n    Given an absolute `root_path`, a list of relative paths to that\n    absolute root path (`relative_paths_to_search`), and a `file_pattern`\n    like '*.sql', returns information about the files. For example:\n\n    > find_matching('/root/path', 'models', '*.sql')\n\n      [ { 'absolute_path': '/root/path/models/model_one.sql',\n          'relative_path': 'models/model_one.sql',\n          'searched_path': 'models' },\n        { 'absolute_path': '/root/path/models/subdirectory/model_two.sql',\n          'relative_path': 'models/subdirectory/model_two.sql',\n          'searched_path': 'models' } ]\n    \"\"\"\n    matching = []\n    root_path = os.path.normpath(root_path)\n\n    for relative_path_to_search in relative_paths_to_search:\n        absolute_path_to_search = os.path.join(\n            root_path, relative_path_to_search)\n        walk_results = os.walk(absolute_path_to_search)\n\n        for current_path, subdirectories, local_files in walk_results:\n            for local_file in local_files:\n                absolute_path = os.path.join(current_path, local_file)\n                relative_path = os.path.relpath(\n                    absolute_path, absolute_path_to_search)\n\n                if fnmatch.fnmatch(local_file, file_pattern):\n                    matching.append({\n                        'searched_path': relative_path_to_search,\n                        'absolute_path': absolute_path,\n                        'relative_path': relative_path,\n                    })\n\n    return matching", "response": "Given an absolute path a list of relative paths to that absolute path and a file pattern returns information about the files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake a directory and any intermediate directories that don t already exist.", "response": "def make_directory(path):\n    \"\"\"\n    Make a directory and any intermediate directories that don't already\n    exist. This function handles the case where two threads try to create\n    a directory at once.\n    \"\"\"\n    if not os.path.exists(path):\n        # concurrent writes that try to create the same dir can fail\n        try:\n            os.makedirs(path)\n\n        except OSError as e:\n            if e.errno == errno.EEXIST:\n                pass\n            else:\n                raise e"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef make_file(path, contents='', overwrite=False):\n    if overwrite or not os.path.exists(path):\n        with open(path, 'w') as fh:\n            fh.write(contents)\n        return True\n\n    return False", "response": "Make a file at path assuming that the directory it resides in already exists."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a symlink at link_path pointing to source.", "response": "def make_symlink(source, link_path):\n    \"\"\"\n    Create a symlink at `link_path` referring to `source`.\n    \"\"\"\n    if not supports_symlinks():\n        dbt.exceptions.system_error('create a symbolic link')\n\n    return os.symlink(source, link_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef resolve_path_from_base(path_to_resolve, base_path):\n    return os.path.abspath(\n        os.path.join(\n            base_path,\n            os.path.expanduser(path_to_resolve)))", "response": "Resolve a path to a base path."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef rmdir(path):\n    logger.debug(\"DEBUG** Window rmdir sys.platform: {}\".format(sys.platform))\n    if sys.platform == 'win32':\n        onerror = _windows_rmdir_readonly\n    else:\n        onerror = None\n\n    return shutil.rmtree(path, onerror=onerror)", "response": "Recursively deletes a directory."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle an error in a set of posix systems.", "response": "def _handle_posix_error(exc, cwd, cmd):\n    \"\"\"OSError handling for posix systems.\n\n    Some things that could happen to trigger an OSError:\n        - cwd could not exist\n            - exc.errno == ENOENT\n            - exc.filename == cwd\n        - cwd could have permissions that prevent the current user moving to it\n            - exc.errno == EACCES\n            - exc.filename == cwd\n        - cwd could exist but not be a directory\n            - exc.errno == ENOTDIR\n            - exc.filename == cwd\n        - cmd[0] could not exist\n            - exc.errno == ENOENT\n            - exc.filename == None(?)\n        - cmd[0] could exist but have permissions that prevents the current\n            user from executing it (executable bit not set for the user)\n            - exc.errno == EACCES\n            - exc.filename == None(?)\n    \"\"\"\n    if getattr(exc, 'filename', None) == cwd:\n        _handle_posix_cwd_error(exc, cwd, cmd)\n    else:\n        _handle_posix_cmd_error(exc, cwd, cmd)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ninterpret an OSError exc and raise the appropriate dbt exception.", "response": "def _interpret_oserror(exc, cwd, cmd):\n    \"\"\"Interpret an OSError exc and raise the appropriate dbt exception.\n\n    \"\"\"\n    if len(cmd) == 0:\n        raise dbt.exceptions.CommandError(cwd, cmd)\n\n    # all of these functions raise unconditionally\n    if os.name == 'nt':\n        _handle_windows_error(exc, cwd, cmd)\n    else:\n        _handle_posix_error(exc, cwd, cmd)\n\n    # this should not be reachable, raise _something_ at least!\n    raise dbt.exceptions.InternalException(\n        'Unhandled exception in _interpret_oserror: {}'.format(exc)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef chmod_and_retry(func, path, exc_info):\n    if func is os.listdir or os.name != 'nt':\n        raise\n    os.chmod(path, stat.S_IREAD | stat.S_IWRITE)\n    # on error,this will raise.\n    func(path)", "response": "This function will be used to set the permissions of a file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an ordered iterator of key - value pairs for pretty - printing.", "response": "def connection_info(self):\n        \"\"\"Return an ordered iterator of key/value pairs for pretty-printing.\n        \"\"\"\n        for key in self._connection_keys():\n            if key in self._contents:\n                yield key, self._contents[key]"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a function that takes a row and decides if the row should be included in the catalog output.", "response": "def _catalog_filter_schemas(manifest):\n    \"\"\"Return a function that takes a row and decides if the row should be\n    included in the catalog output.\n    \"\"\"\n    schemas = frozenset((d.lower(), s.lower())\n                        for d, s in manifest.get_used_schemas())\n\n    def test(row):\n        table_database = _expect_row_value('table_database', row)\n        table_schema = _expect_row_value('table_schema', row)\n        # the schema may be present but None, which is not an error and should\n        # be filtered out\n        if table_schema is None:\n            return False\n        return (table_database.lower(), table_schema.lower()) in schemas\n    return test"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a datetime that s in UTC.", "response": "def _utc(dt, source, field_name):\n    \"\"\"If dt has a timezone, return a new datetime that's in UTC. Otherwise,\n    assume the datetime is already for UTC and add the timezone.\n    \"\"\"\n    if dt is None:\n        raise dbt.exceptions.raise_database_error(\n            \"Expected a non-null value when querying field '{}' of table \"\n            \" {} but received value 'null' instead\".format(\n                field_name,\n                source))\n\n    elif not hasattr(dt, 'tzinfo'):\n        raise dbt.exceptions.raise_database_error(\n            \"Expected a timestamp value when querying field '{}' of table \"\n            \"{} but received value of type '{}' instead\".format(\n                field_name,\n                source,\n                type(dt).__name__))\n\n    elif dt.tzinfo:\n        return dt.astimezone(pytz.UTC)\n    else:\n        return dt.replace(tzinfo=pytz.UTC)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\noverrides jinja s compilation to stash the rendered source inside the python linecache for debugging.", "response": "def _compile(self, source, filename):\n        \"\"\"Override jinja's compilation to stash the rendered source inside\n        the python linecache for debugging.\n        \"\"\"\n        if filename == '<template>':\n            # make a better filename\n            filename = 'dbt-{}'.format(\n                codecs.encode(os.urandom(12), 'hex').decode('ascii')\n            )\n            # encode, though I don't think this matters\n            filename = jinja2._compat.encode_filename(filename)\n            # put ourselves in the cache\n            linecache.cache[filename] = (\n                len(source),\n                None,\n                [line + '\\n' for line in source.splitlines()],\n                filename\n            )\n\n        return super(MacroFuzzEnvironment, self)._compile(source, filename)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self):\n        validator = Draft7Validator(self.SCHEMA)\n\n        errors = set()  # make errors a set to avoid duplicates\n\n        for error in validator.iter_errors(self.serialize()):\n            errors.add('.'.join(\n                list(map(str, error.path)) + [error.message]\n            ))\n\n        if errors:\n            raise JSONValidationException(type(self).__name__, errors)", "response": "Validate the attributes of the object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _safe_release_connection(self):\n        try:\n            self.adapter.release_connection()\n        except Exception as exc:\n            logger.debug(\n                'Error releasing connection for node {}: {!s}\\n{}'\n                .format(self.node.name, exc, traceback.format_exc())\n            )\n            return dbt.compat.to_string(exc)\n\n        return None", "response": "Try to release a connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calculate_status(self, target_freshness, freshness):\n        # if freshness > warn_after > error_after, you'll get an error, not a\n        # warning\n        for key in ('error', 'warn'):\n            fullkey = '{}_after'.format(key)\n            if fullkey not in target_freshness:\n                continue\n\n            target = target_freshness[fullkey]\n            kwname = target['period'] + 's'\n            kwargs = {kwname: target['count']}\n            if freshness > timedelta(**kwargs).total_seconds():\n                return key\n        return 'pass'", "response": "Calculate the status of a run."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef new_project(self, project_root):\n        # copy profile\n        profile = Profile(**self.to_profile_info())\n        profile.validate()\n        # load the new project and its packages. Don't pass cli variables.\n        project = Project.from_project_root(project_root, {})\n\n        cfg = self.from_parts(\n            project=project,\n            profile=profile,\n            args=deepcopy(self.args),\n        )\n        # force our quoting back onto the new project.\n        cfg.quoting = deepcopy(self.quoting)\n        return cfg", "response": "Read in the project dictionary supply the\n        existing project s profile info and create a new project file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize(self):\n        result = self.to_project_config(with_packages=True)\n        result.update(self.to_profile_info(serialize_credentials=True))\n        result['cli_vars'] = deepcopy(self.cli_vars)\n        return result", "response": "Serialize the full configuration to a single dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef validate(self):\n        try:\n            Configuration(**self.serialize())\n        except ValidationException as e:\n            raise DbtProjectError(str(e))\n\n        if getattr(self.args, 'version_check', False):\n            self.validate_version()", "response": "Validate the configuration against its contract."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngives arguments read in dbt_project. yml from the current directory profile. yml and use them to find the profile to load.", "response": "def from_args(cls, args):\n        \"\"\"Given arguments, read in dbt_project.yml from the current directory,\n        read in packages.yml if it exists, and use them to find the profile to\n        load.\n\n        :param args argparse.Namespace: The arguments as parsed from the cli.\n        :raises DbtProjectError: If the project is invalid or missing.\n        :raises DbtProfileError: If the profile is invalid or missing.\n        :raises ValidationException: If the cli variables are invalid.\n        \"\"\"\n        # build the project and read in packages.yml\n        project = Project.from_args(args)\n\n        # build the profile\n        profile = Profile.from_args(\n            args=args,\n            project_profile_name=project.profile_name\n        )\n\n        return cls.from_parts(\n            project=project,\n            profile=profile,\n            args=args\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndetermine if a qualfied name matches an fqn.", "response": "def _node_is_match(qualified_name, package_names, fqn):\n    \"\"\"Determine if a qualfied name matches an fqn, given the set of package\n    names in the graph.\n\n    :param List[str] qualified_name: The components of the selector or node\n        name, split on '.'.\n    :param Set[str] package_names: The set of pacakge names in the graph.\n    :param List[str] fqn: The node's fully qualified name in the graph.\n    \"\"\"\n    if len(qualified_name) == 1 and fqn[-1] == qualified_name[0]:\n        return True\n\n    if qualified_name[0] in package_names:\n        if is_selected_node(fqn, qualified_name):\n            return True\n\n    for package_name in package_names:\n        local_qualified_node_name = [package_name] + qualified_name\n        if is_selected_node(fqn, local_qualified_node_name):\n            return True\n\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_nodes_by_qualified_name(self, graph, qualified_name_selector):\n        qualified_name = qualified_name_selector.split(\".\")\n        package_names = get_package_names(graph)\n        for node, real_node in self.parsed_nodes(graph):\n            if _node_is_match(qualified_name, package_names, real_node.fqn):\n                yield node", "response": "Yields all nodes in the graph that match the qualified_name_selector."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nyields nodes that have the specified tag", "response": "def get_nodes_by_tag(self, graph, tag_name):\n        \"\"\" yields nodes from graph that have the specified tag \"\"\"\n        for node, real_node in self.parsed_nodes(graph):\n            if tag_name in real_node.tags:\n                yield node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_nodes_by_source(self, graph, source_full_name):\n        parts = source_full_name.split('.')\n        if len(parts) == 1:\n            target_source, target_table = parts[0], None\n        elif len(parts) == 2:\n            target_source, target_table = parts\n        else:  # len(parts) > 2 or len(parts) == 0\n            msg = (\n                'Invalid source selector value \"{}\". Sources must be of the '\n                'form `${{source_name}}` or '\n                '`${{source_name}}.${{target_name}}`'\n            ).format(source_full_name)\n            raise dbt.exceptions.RuntimeException(msg)\n\n        for node, real_node in self.source_nodes(graph):\n            if target_source not in (real_node.source_name, SELECTOR_GLOB):\n                continue\n            if target_table in (None, real_node.name, SELECTOR_GLOB):\n                yield node", "response": "yields nodes from graph are the specified source."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_query(self, sql, auto_begin=True, bindings=None,\n                  abridge_sql_log=False):\n        \"\"\"Add a query to the current transaction. A thin wrapper around\n        ConnectionManager.add_query.\n\n        :param str sql: The SQL query to add\n        :param bool auto_begin: If set and there is no transaction in progress,\n            begin a new one.\n        :param Optional[List[object]]: An optional list of bindings for the\n            query.\n        :param bool abridge_sql_log: If set, limit the raw sql logged to 512\n            characters\n        \"\"\"\n        return self.connections.add_query(sql, auto_begin, bindings,\n                                          abridge_sql_log)", "response": "A thin wrapper around ConnectionManager. add_query."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchanging the column type of the entry in the database.", "response": "def alter_column_type(self, relation, column_name, new_column_type):\n        \"\"\"\n        1. Create a new column (w/ temp name and correct type)\n        2. Copy data over to it\n        3. Drop the existing column (cascade!)\n        4. Rename the new column to existing column\n        \"\"\"\n        kwargs = {\n            'relation': relation,\n            'column_name': column_name,\n            'new_column_type': new_column_type,\n        }\n        self.execute_macro(\n            ALTER_COLUMN_TYPE_MACRO_NAME,\n            kwargs=kwargs\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef id_matches(unique_id, target_name, target_package, nodetypes, model):\n    node_type = model.get('resource_type', 'node')\n    node_parts = unique_id.split('.', 2)\n    if len(node_parts) != 3:\n        msg = \"unique_id {} is malformed\".format(unique_id)\n        dbt.exceptions.raise_compiler_error(msg, model)\n\n    resource_type, package_name, node_name = node_parts\n    if node_type == NodeType.Source:\n        if node_name.count('.') != 1:\n            msg = \"{} names must contain exactly 1 '.' character\"\\\n                .format(node_type)\n            dbt.exceptions.raise_compiler_error(msg, model)\n    else:\n        if '.' in node_name:\n            msg = \"{} names cannot contain '.' characters\".format(node_type)\n            dbt.exceptions.raise_compiler_error(msg, model)\n\n    if resource_type not in nodetypes:\n        return False\n\n    if target_name != node_name:\n        return False\n\n    return target_package is None or target_package == package_name", "response": "Return True if the given unique ID matches the given name package and type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef find_in_subgraph_by_name(subgraph, target_name, target_package, nodetype):\n    for name, model in subgraph.items():\n        if id_matches(name, target_name, target_package, nodetype, model):\n            return model\n\n    return None", "response": "Find an entry in a subgraph by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds an entry in the given list by name.", "response": "def find_in_list_by_name(haystack, target_name, target_package, nodetype):\n    \"\"\"Find an entry in the given list by name.\"\"\"\n    for model in haystack:\n        name = model.get('unique_id')\n        if id_matches(name, target_name, target_package, nodetype, model):\n            return model\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap the function func onto each non - container value in value recursively returning a new value.", "response": "def deep_map(func, value):\n    \"\"\"map the function func() onto each non-container value in 'value'\n    recursively, returning a new value. As long as func does not manipulate\n    value, then deep_map will also not manipulate it.\n\n    value should be a value returned by `yaml.safe_load` or `json.load` - the\n    only expected types are list, dict, native python number, str, NoneType,\n    and bool.\n\n    func() will be called on numbers, strings, Nones, and booleans. Its first\n    parameter will be the value, and the second will be its keypath, an\n    iterable over the __getitem__ keys needed to get to it.\n\n    :raises: If there are cycles in the value, raises a\n        dbt.exceptions.RecursionException\n    \"\"\"\n    try:\n        return _deep_map(func, value, ())\n    except RuntimeError as exc:\n        if 'maximum recursion depth exceeded' in str(exc):\n            raise dbt.exceptions.RecursionException(\n                'Cycle detected in deep_map'\n            )\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a dict of keyword arguments and a dict mapping aliases to their articles canonical keys canonicalize the keys in the kwargs dict.", "response": "def translate_aliases(kwargs, aliases):\n    \"\"\"Given a dict of keyword arguments and a dict mapping aliases to their\n    canonical values, canonicalize the keys in the kwargs dict.\n\n    :return: A dict continaing all the values in kwargs referenced by their\n        canonical key.\n    :raises: `AliasException`, if a canonical key is defined more than once.\n    \"\"\"\n    result = {}\n\n    for given_key, value in kwargs.items():\n        canonical_key = aliases.get(given_key, given_key)\n        if canonical_key in result:\n            # dupe found: go through the dict so we can have a nice-ish error\n            key_names = ', '.join(\"{}\".format(k) for k in kwargs if\n                                  aliases.get(k) == canonical_key)\n\n            raise dbt.exceptions.AliasException(\n                'Got duplicate keys: ({}) all map to \"{}\"'\n                .format(key_names, canonical_key)\n            )\n\n        result[canonical_key] = value\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _checkout(self, project):\n        if len(self.version) != 1:\n            dbt.exceptions.raise_dependency_error(\n                'Cannot checkout repository until the version is pinned.')\n        try:\n            dir_ = dbt.clients.git.clone_and_checkout(\n                self.git, DOWNLOADS_PATH, branch=self.version[0],\n                dirname=self._checkout_name)\n        except dbt.exceptions.ExecutableError as exc:\n            if exc.cmd and exc.cmd[0] == 'git':\n                logger.error(\n                    'Make sure git is installed on your machine. More '\n                    'information: '\n                    'https://docs.getdbt.com/docs/package-management'\n                )\n            raise\n        return os.path.join(DOWNLOADS_PATH, dir_)", "response": "Performs a shallow clone of the repository into the downloads\n        directory."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndrop a relation from the database.", "response": "def drop_relation(self, relation):\n        \"\"\"\n        In Redshift, DROP TABLE ... CASCADE should not be used\n        inside a transaction. Redshift doesn't prevent the CASCADE\n        part from conflicting with concurrent transactions. If we do\n        attempt to drop two tables with CASCADE at once, we'll often\n        get the dreaded:\n\n            table was dropped by a concurrent transaction\n\n        So, we need to lock around calls to the underlying\n        drop_relation() function.\n\n        https://docs.aws.amazon.com/redshift/latest/dg/r_DROP_TABLE.html\n        \"\"\"\n        with self.connections.fresh_transaction():\n            parent = super(RedshiftAdapter, self)\n            return parent.drop_relation(relation)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads and parse documentation in a list of projects. Returns a list", "response": "def load_file(cls, package_name, root_dir, relative_dirs):\n        \"\"\"Load and parse documentation in a list of projects. Returns a list\n        of ParsedNodes.\n        \"\"\"\n        extension = \"[!.#~]*.md\"\n\n        file_matches = dbt.clients.system.find_matching(\n            root_dir,\n            relative_dirs,\n            extension)\n\n        for file_match in file_matches:\n            file_contents = dbt.clients.system.load_file_contents(\n                file_match.get('absolute_path'), strip=False)\n\n            parts = dbt.utils.split_path(file_match.get('relative_path', ''))\n            name, _ = os.path.splitext(parts[-1])\n\n            path = file_match.get('relative_path')\n            original_file_path = os.path.join(\n                file_match.get('searched_path'),\n                path)\n\n            yield UnparsedDocumentationFile(\n                root_path=root_dir,\n                resource_type=NodeType.Documentation,\n                path=path,\n                original_file_path=original_file_path,\n                package_name=package_name,\n                file_contents=file_contents\n            )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef config(self):\n\n        defaults = {\"enabled\": True, \"materialized\": \"view\"}\n\n        if self.node_type == NodeType.Seed:\n            defaults['materialized'] = 'seed'\n        elif self.node_type == NodeType.Archive:\n            defaults['materialized'] = 'archive'\n\n        active_config = self.load_config_from_active_project()\n\n        if self.active_project.project_name == self.own_project.project_name:\n            cfg = self._merge(defaults, active_config,\n                              self.in_model_config)\n        else:\n            own_config = self.load_config_from_own_project()\n\n            cfg = self._merge(\n                defaults, own_config, self.in_model_config, active_config\n            )\n\n        return cfg", "response": "Load the configuration of the node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexpecting a comment end and return the match object.", "response": "def expect_comment_end(self):\n        \"\"\"Expect a comment end and return the match object.\n        \"\"\"\n        match = self._expect_match('#}', COMMENT_END_PATTERN)\n        self.advance(match.end())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nhandle a block. The current state of the parser should be after the open block is completed: {% blk foo %}my data {% endblk %} ^ right here", "response": "def handle_block(self, match, block_start=None):\n        \"\"\"Handle a block. The current state of the parser should be after the\n        open block is completed:\n            {% blk foo %}my data {% endblk %}\n                         ^ right here\n        \"\"\"\n        # we have to handle comments inside blocks because you could do this:\n        # {% blk foo %}asdf {# {% endblk %} #} {%endblk%}\n        # they still end up in the data/raw_data of the block itself, but we\n        # have to know to ignore stuff until the end comment marker!\n        found = BlockTag(**match.groupdict())\n        # the full block started at the given match start, which may include\n        # prefixed whitespace! we'll strip it later\n        if block_start is None:\n            block_start = match.start()\n\n        self._block_contents = ''\n\n        # you can have as many comments in your block as you'd like!\n        while True:\n            match = self._expect_match(\n                '\"{}\"'.format(found.end_block_type_name),\n                found.end_pat(), COMMENT_START_PATTERN, RAW_START_PATTERN,\n                regex('''(?P<quote>(['\"]))''')\n            )\n            groups = match.groupdict()\n            if groups.get('endblock') is not None:\n                break\n\n            self.advance(match.end())\n\n            if groups.get('comment_start') is not None:\n                self.expect_comment_end()\n            elif groups.get('raw_start') is not None:\n                self.expect_raw_end()\n            elif groups.get('quote') is not None:\n                self.rewind()\n                match = self._expect_match('any string', STRING_PATTERN)\n                self.advance(match.end())\n            else:\n                raise dbt.exceptions.InternalException(\n                    'unhandled regex in handle_block, no match: {}'\n                    .format(groups)\n                )\n\n        # we want to advance to just the end tag at first, to extract the\n        # contents\n        self.advance(match.start())\n        found.contents = self._block_contents\n        self._block_contents = None\n        # now advance to the end\n        self.advance(match.end())\n        found.full_block = self.data[block_start:self.pos]\n        return found"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprocess the rval of an assignment statement or a do - block - component statement.", "response": "def _process_rval_components(self):\n        \"\"\"This is suspiciously similar to _process_macro_default_arg, probably\n        want to figure out how to merge the two.\n\n        Process the rval of an assignment statement or a do-block\n        \"\"\"\n        while True:\n            match = self._expect_match(\n                'do block component',\n                # you could have a string, though that would be weird\n                STRING_PATTERN,\n                # a quote or an open/close parenthesis\n                NON_STRING_DO_BLOCK_MEMBER_PATTERN,\n                # a tag close\n                TAG_CLOSE_PATTERN\n            )\n            matchgroups = match.groupdict()\n            self.advance(match.end())\n            if matchgroups.get('string') is not None:\n                continue\n            elif matchgroups.get('quote') is not None:\n                self.rewind()\n                # now look for a string\n                match = self._expect_match('any string', STRING_PATTERN)\n                self.advance(match.end())\n            elif matchgroups.get('open'):\n                self._parenthesis_stack.append(True)\n            elif matchgroups.get('close'):\n                self._parenthesis_stack.pop()\n            elif matchgroups.get('tag_close'):\n                if self._parenthesis_stack:\n                    msg = ('Found \"%}\", expected \")\"')\n                    dbt.exceptions.raise_compiler_error(msg)\n                return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprocessing the bit after an '=' in a macro default argument.", "response": "def _process_macro_default_arg(self):\n        \"\"\"Handle the bit after an '=' in a macro default argument. This is\n        probably the trickiest thing. The goal here is to accept all strings\n        jinja would accept and always handle block start/end correctly: It's\n        fine to have false positives, jinja can fail later.\n\n        Return True if there are more arguments expected.\n        \"\"\"\n        while self._parenthesis_stack:\n            match = self._expect_match(\n                'macro argument',\n                # you could have a string\n                STRING_PATTERN,\n                # a quote, a comma, or a open/close parenthesis\n                NON_STRING_MACRO_ARGS_PATTERN,\n                # we want to \"match\", not \"search\"\n                method='match'\n            )\n            matchgroups = match.groupdict()\n            self.advance(match.end())\n            if matchgroups.get('string') is not None:\n                # we got a string value. There could be more data.\n                continue\n            elif matchgroups.get('quote') is not None:\n                # we got a bunch of data and then a string opening value.\n                # put the quote back on the menu\n                self.rewind()\n                # now look for a string\n                match = self._expect_match('any string', STRING_PATTERN)\n                self.advance(match.end())\n            elif matchgroups.get('comma') is not None:\n                # small hack: if we hit a comma and there is one parenthesis\n                # left, return to look for a new name. otherwise we're still\n                # looking for the parameter close.\n                if len(self._parenthesis_stack) == 1:\n                    return\n            elif matchgroups.get('open'):\n                self._parenthesis_stack.append(True)\n            elif matchgroups.get('close'):\n                self._parenthesis_stack.pop()\n            else:\n                raise dbt.exceptions.InternalException(\n                    'unhandled regex in _process_macro_default_arg(), no match'\n                    ': {}'.format(matchgroups)\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _render_project_entry(self, value, keypath):\n        # hooks should be treated as raw sql, they'll get rendered later.\n        # Same goes for 'vars' declarations inside 'models'/'seeds'.\n        if self._is_hook_or_model_vars_path(keypath):\n            return value\n\n        return self.render_value(value)", "response": "Render an entry in case it s jinja."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef render_project(self, as_parsed):\n        try:\n            return deep_map(self._render_project_entry, as_parsed)\n        except RecursionException:\n            raise DbtProjectError(\n                'Cycle detected: Project input has a reference to itself',\n                project=as_parsed\n            )", "response": "Render the parsed data returning a new dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef render_profile_data(self, as_parsed):\n        try:\n            return deep_map(self._render_profile_data, as_parsed)\n        except RecursionException:\n            raise DbtProfileError(\n                'Cycle detected: Profile input has a reference to itself',\n                project=as_parsed\n            )", "response": "Render the chosen profile entry as it was parsed."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nloads and parse archives in a list of projects. Returns a dict that maps unique ids onto ParsedNodes", "response": "def load_and_parse(self):\n        \"\"\"Load and parse archives in a list of projects. Returns a dict\n           that maps unique ids onto ParsedNodes\"\"\"\n\n        archives = []\n        to_return = {}\n\n        for name, project in self.all_projects.items():\n            archives = archives + self.parse_archives_from_project(project)\n\n        # We're going to have a similar issue with parsed nodes, if we want to\n        # make parse_node return those.\n        for a in archives:\n            # archives have a config, but that would make for an invalid\n            # UnparsedNode, so remove it and pass it along to parse_node as an\n            # argument.\n            archive_config = a.pop('config')\n            archive = UnparsedNode(**a)\n            node_path = self.get_path(archive.resource_type,\n                                      archive.package_name,\n                                      archive.name)\n\n            to_return[node_path] = self.parse_node(\n                archive,\n                node_path,\n                self.all_projects.get(archive.package_name),\n                archive_config=archive_config)\n\n        return to_return"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef to_profile_info(self, serialize_credentials=False):\n        result = {\n            'profile_name': self.profile_name,\n            'target_name': self.target_name,\n            'config': self.config.to_dict(),\n            'threads': self.threads,\n            'credentials': self.credentials.incorporate(),\n        }\n        if serialize_credentials:\n            result['credentials'] = result['credentials'].serialize()\n        return result", "response": "This method returns a dict that represents the profile."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_credentials(cls, credentials, threads, profile_name, target_name,\n                         user_cfg=None):\n        \"\"\"Create a profile from an existing set of Credentials and the\n        remaining information.\n\n        :param credentials dict: The credentials dict for this profile.\n        :param threads int: The number of threads to use for connections.\n        :param profile_name str: The profile name used for this profile.\n        :param target_name str: The target name used for this profile.\n        :param user_cfg Optional[dict]: The user-level config block from the\n            raw profiles, if specified.\n        :raises DbtProfileError: If the profile is invalid.\n        :returns Profile: The new Profile object.\n        \"\"\"\n        config = UserConfig.from_dict(user_cfg)\n        profile = cls(\n            profile_name=profile_name,\n            target_name=target_name,\n            config=config,\n            threads=threads,\n            credentials=credentials\n        )\n        profile.validate()\n        return profile", "response": "Create a profile from an existing set of Credentials and the\n            remaining information."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef render_profile(cls, raw_profile, profile_name, target_override,\n                       cli_vars):\n        \"\"\"This is a containment zone for the hateful way we're rendering\n        profiles.\n        \"\"\"\n        renderer = ConfigRenderer(cli_vars=cli_vars)\n\n        # rendering profiles is a bit complex. Two constraints cause trouble:\n        # 1) users should be able to use environment/cli variables to specify\n        #    the target in their profile.\n        # 2) Missing environment/cli variables in profiles/targets that don't\n        #    end up getting selected should not cause errors.\n        # so first we'll just render the target name, then we use that rendered\n        # name to extract a profile that we can render.\n        if target_override is not None:\n            target_name = target_override\n        elif 'target' in raw_profile:\n            # render the target if it was parsed from yaml\n            target_name = renderer.render_value(raw_profile['target'])\n        else:\n            target_name = 'default'\n            logger.debug(\n                \"target not specified in profile '{}', using '{}'\"\n                .format(profile_name, target_name)\n            )\n\n        raw_profile_data = cls._get_profile_data(\n            raw_profile, profile_name, target_name\n        )\n\n        profile_data = renderer.render_profile_data(raw_profile_data)\n        return target_name, profile_data", "response": "Render a profile and return the target name and profile data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a Profile object from its raw profile information.", "response": "def from_raw_profile_info(cls, raw_profile, profile_name, cli_vars,\n                              user_cfg=None, target_override=None,\n                              threads_override=None):\n        \"\"\"Create a profile from its raw profile information.\n\n         (this is an intermediate step, mostly useful for unit testing)\n\n        :param raw_profile dict: The profile data for a single profile, from\n            disk as yaml and its values rendered with jinja.\n        :param profile_name str: The profile name used.\n        :param cli_vars dict: The command-line variables passed as arguments,\n            as a dict.\n        :param user_cfg Optional[dict]: The global config for the user, if it\n            was present.\n        :param target_override Optional[str]: The target to use, if provided on\n            the command line.\n        :param threads_override Optional[str]: The thread count to use, if\n            provided on the command line.\n        :raises DbtProfileError: If the profile is invalid or missing, or the\n            target could not be found\n        :returns Profile: The new Profile object.\n        \"\"\"\n        # user_cfg is not rendered since it only contains booleans.\n        # TODO: should it be, and the values coerced to bool?\n        target_name, profile_data = cls.render_profile(\n            raw_profile, profile_name, target_override, cli_vars\n        )\n\n        # valid connections never include the number of threads, but it's\n        # stored on a per-connection level in the raw configs\n        threads = profile_data.pop('threads', DEFAULT_THREADS)\n        if threads_override is not None:\n            threads = threads_override\n\n        credentials = cls._credentials_from_profile(\n            profile_data, profile_name, target_name\n        )\n\n        return cls.from_credentials(\n            credentials=credentials,\n            profile_name=profile_name,\n            target_name=target_name,\n            threads=threads,\n            user_cfg=user_cfg\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_raw_profiles(cls, raw_profiles, profile_name, cli_vars,\n                          target_override=None, threads_override=None):\n        \"\"\"\n        :param raw_profiles dict: The profile data, from disk as yaml.\n        :param profile_name str: The profile name to use.\n        :param cli_vars dict: The command-line variables passed as arguments,\n            as a dict.\n        :param target_override Optional[str]: The target to use, if provided on\n            the command line.\n        :param threads_override Optional[str]: The thread count to use, if\n            provided on the command line.\n        :raises DbtProjectError: If there is no profile name specified in the\n            project or the command line arguments\n        :raises DbtProfileError: If the profile is invalid or missing, or the\n            target could not be found\n        :returns Profile: The new Profile object.\n        \"\"\"\n        if profile_name not in raw_profiles:\n            raise DbtProjectError(\n                \"Could not find profile named '{}'\".format(profile_name)\n            )\n\n        # First, we've already got our final decision on profile name, and we\n        # don't render keys, so we can pluck that out\n        raw_profile = raw_profiles[profile_name]\n\n        user_cfg = raw_profiles.get('config')\n\n        return cls.from_raw_profile_info(\n            raw_profile=raw_profile,\n            profile_name=profile_name,\n            cli_vars=cli_vars,\n            user_cfg=user_cfg,\n            target_override=target_override,\n            threads_override=threads_override,\n        )", "response": "Creates a new Profile object from a dictionary of raw profiles."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_args(cls, args, project_profile_name=None):\n        cli_vars = parse_cli_vars(getattr(args, 'vars', '{}'))\n        threads_override = getattr(args, 'threads', None)\n        target_override = getattr(args, 'target', None)\n        raw_profiles = read_profile(args.profiles_dir)\n        profile_name = cls.pick_profile_name(args.profile,\n                                             project_profile_name)\n\n        return cls.from_raw_profiles(\n            raw_profiles=raw_profiles,\n            profile_name=profile_name,\n            cli_vars=cli_vars,\n            target_override=target_override,\n            threads_override=threads_override\n        )", "response": "Returns a new instance of the class from the given command line arguments."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef docs(node, manifest, config, column_name=None):\n    current_project = config.project_name\n\n    def do_docs(*args):\n        if len(args) == 1:\n            doc_package_name = None\n            doc_name = args[0]\n        elif len(args) == 2:\n            doc_package_name, doc_name = args\n        else:\n            dbt.exceptions.doc_invalid_args(node, args)\n\n        target_doc = ParserUtils.resolve_doc(\n            manifest, doc_name, doc_package_name, current_project,\n            node.package_name\n        )\n\n        if target_doc is None:\n            dbt.exceptions.doc_target_not_found(node, doc_name,\n                                                doc_package_name)\n\n        return target_doc.block_contents\n\n    return do_docs", "response": "Return a function that will process doc references in jinja look\n    them up in the manifest and return the appropriate block contents."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves the given documentation.", "response": "def resolve_doc(cls, manifest, target_doc_name, target_doc_package,\n                    current_project, node_package):\n        \"\"\"Resolve the given documentation. This follows the same algorithm as\n        resolve_ref except the is_enabled checks are unnecessary as docs are\n        always enabled.\n        \"\"\"\n        if target_doc_package is not None:\n            return manifest.find_docs_by_name(target_doc_name,\n                                              target_doc_package)\n\n        candidate_targets = [current_project, node_package, None]\n        target_doc = None\n        for candidate in candidate_targets:\n            target_doc = manifest.find_docs_by_name(target_doc_name, candidate)\n            if target_doc is not None:\n                break\n        return target_doc"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_node_column(cls, node, column_name):\n        if not hasattr(node, 'columns'):\n            node.set('columns', {})\n\n        if column_name in node.columns:\n            column = node.columns[column_name]\n        else:\n            column = {'name': column_name, 'description': ''}\n            node.columns[column_name] = column\n\n        return column", "response": "Given a ParsedNode add some fields that might be missing. Return a dictionary that refers to the given column."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a manifest and a node in that manifest process its refs", "response": "def process_refs_for_node(cls, manifest, current_project, node):\n        \"\"\"Given a manifest and a node in that manifest, process its refs\"\"\"\n        target_model = None\n        target_model_name = None\n        target_model_package = None\n\n        for ref in node.refs:\n            if len(ref) == 1:\n                target_model_name = ref[0]\n            elif len(ref) == 2:\n                target_model_package, target_model_name = ref\n\n            target_model = cls.resolve_ref(\n                manifest,\n                target_model_name,\n                target_model_package,\n                current_project,\n                node.get('package_name'))\n\n            if target_model is None or target_model is cls.DISABLED:\n                # This may raise. Even if it doesn't, we don't want to add\n                # this node to the graph b/c there is no destination node\n                node.config['enabled'] = False\n                dbt.utils.invalid_ref_fail_unless_test(\n                    node, target_model_name, target_model_package,\n                    disabled=(target_model is cls.DISABLED)\n                )\n\n                continue\n\n            target_model_id = target_model.get('unique_id')\n\n            node.depends_on['nodes'].append(target_model_id)\n            manifest.nodes[node['unique_id']] = node"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a new node copy the manifest and insert the new node into the manifest and return the new manifest.", "response": "def add_new_refs(cls, manifest, current_project, node, macros):\n        \"\"\"Given a new node that is not in the manifest, copy the manifest and\n        insert the new node into it as if it were part of regular ref\n        processing\n        \"\"\"\n        manifest = manifest.deepcopy(config=current_project)\n        # it's ok for macros to silently override a local project macro name\n        manifest.macros.update(macros)\n\n        if node.unique_id in manifest.nodes:\n            # this should be _impossible_ due to the fact that rpc calls get\n            # a unique ID that starts with 'rpc'!\n            raise dbt.exceptions.raise_duplicate_resource_name(\n                manifest.nodes[node.unique_id], node\n            )\n        manifest.nodes[node.unique_id] = node\n        cls.process_sources_for_node(manifest, current_project, node)\n        cls.process_refs_for_node(manifest, current_project, node)\n        cls.process_docs_for_node(manifest, current_project, node)\n        return manifest"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses multiple versions as read from disk.", "response": "def _parse_versions(versions):\n    \"\"\"Parse multiple versions as read from disk. The versions value may be any\n    one of:\n        - a single version string ('>0.12.1')\n        - a single string specifying multiple comma-separated versions\n            ('>0.11.1,<=0.12.2')\n        - an array of single-version strings (['>0.11.1', '<=0.12.2'])\n\n    Regardless, this will return a list of VersionSpecifiers\n    \"\"\"\n    if isinstance(versions, compat.basestring):\n        versions = versions.split(',')\n    return [VersionSpecifier.from_version_string(v) for v in versions]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _preprocess(project_dict):\n        handlers = {\n            ('archive',): _list_if_none,\n            ('on-run-start',): _list_if_none_or_string,\n            ('on-run-end',): _list_if_none_or_string,\n        }\n\n        for k in ('models', 'seeds'):\n            handlers[(k,)] = _dict_if_none\n            handlers[(k, 'vars')] = _dict_if_none\n            handlers[(k, 'pre-hook')] = _list_if_none_or_string\n            handlers[(k, 'post-hook')] = _list_if_none_or_string\n        handlers[('seeds', 'column_types')] = _dict_if_none\n\n        def converter(value, keypath):\n            if keypath in handlers:\n                handler = handlers[keypath]\n                return handler(value)\n            else:\n                return value\n\n        return deep_map(converter, project_dict)", "response": "Pre - process certain special keys to convert them from None values\n        into empty containers and to turn strings into arrays of strings."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_project_config(cls, project_dict, packages_dict=None):\n        try:\n            project_dict = cls._preprocess(project_dict)\n        except RecursionException:\n            raise DbtProjectError(\n                'Cycle detected: Project input has a reference to itself',\n                project=project_dict\n            )\n        # just for validation.\n        try:\n            ProjectContract(**project_dict)\n        except ValidationException as e:\n            raise DbtProjectError(str(e))\n\n        # name/version are required in the Project definition, so we can assume\n        # they are present\n        name = project_dict['name']\n        version = project_dict['version']\n        # this is added at project_dict parse time and should always be here\n        # once we see it.\n        project_root = project_dict['project-root']\n        # this is only optional in the sense that if it's not present, it needs\n        # to have been a cli argument.\n        profile_name = project_dict.get('profile')\n        # these are optional\n        source_paths = project_dict.get('source-paths', ['models'])\n        macro_paths = project_dict.get('macro-paths', ['macros'])\n        data_paths = project_dict.get('data-paths', ['data'])\n        test_paths = project_dict.get('test-paths', ['test'])\n        analysis_paths = project_dict.get('analysis-paths', [])\n        docs_paths = project_dict.get('docs-paths', source_paths[:])\n        target_path = project_dict.get('target-path', 'target')\n        archive_paths = project_dict.get('archive-paths', ['archives'])\n        # should this also include the modules path by default?\n        clean_targets = project_dict.get('clean-targets', [target_path])\n        log_path = project_dict.get('log-path', 'logs')\n        modules_path = project_dict.get('modules-path', 'dbt_modules')\n        # in the default case we'll populate this once we know the adapter type\n        quoting = project_dict.get('quoting', {})\n\n        models = project_dict.get('models', {})\n        on_run_start = project_dict.get('on-run-start', [])\n        on_run_end = project_dict.get('on-run-end', [])\n        archive = project_dict.get('archive', [])\n        seeds = project_dict.get('seeds', {})\n        dbt_raw_version = project_dict.get('require-dbt-version', '>=0.0.0')\n\n        try:\n            dbt_version = _parse_versions(dbt_raw_version)\n        except SemverException as e:\n            raise DbtProjectError(str(e))\n\n        packages = package_config_from_data(packages_dict)\n\n        project = cls(\n            project_name=name,\n            version=version,\n            project_root=project_root,\n            profile_name=profile_name,\n            source_paths=source_paths,\n            macro_paths=macro_paths,\n            data_paths=data_paths,\n            test_paths=test_paths,\n            analysis_paths=analysis_paths,\n            docs_paths=docs_paths,\n            target_path=target_path,\n            archive_paths=archive_paths,\n            clean_targets=clean_targets,\n            log_path=log_path,\n            modules_path=modules_path,\n            quoting=quoting,\n            models=models,\n            on_run_start=on_run_start,\n            on_run_end=on_run_end,\n            archive=archive,\n            seeds=seeds,\n            dbt_version=dbt_version,\n            packages=packages\n        )\n        # sanity check - this means an internal issue\n        project.validate()\n        return project", "response": "Creates a new project from a project and package configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_project_config(self, with_packages=False):\n        result = deepcopy({\n            'name': self.project_name,\n            'version': self.version,\n            'project-root': self.project_root,\n            'profile': self.profile_name,\n            'source-paths': self.source_paths,\n            'macro-paths': self.macro_paths,\n            'data-paths': self.data_paths,\n            'test-paths': self.test_paths,\n            'analysis-paths': self.analysis_paths,\n            'docs-paths': self.docs_paths,\n            'target-path': self.target_path,\n            'archive-paths': self.archive_paths,\n            'clean-targets': self.clean_targets,\n            'log-path': self.log_path,\n            'quoting': self.quoting,\n            'models': self.models,\n            'on-run-start': self.on_run_start,\n            'on-run-end': self.on_run_end,\n            'archive': self.archive,\n            'seeds': self.seeds,\n            'require-dbt-version': [\n                v.to_version_string() for v in self.dbt_version\n            ],\n        })\n        if with_packages:\n            result.update(self.packages.serialize())\n        return result", "response": "Returns a dict representation of the profile that could be written to the project config disk with yaml. safe_dump to get this configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new project from a project root.", "response": "def from_project_root(cls, project_root, cli_vars):\n        \"\"\"Create a project from a root directory. Reads in dbt_project.yml and\n        packages.yml, if it exists.\n\n        :param project_root str: The path to the project root to load.\n        :raises DbtProjectError: If the project is missing or invalid, or if\n            the packages file exists and is invalid.\n        :returns Project: The project, with defaults populated.\n        \"\"\"\n        project_root = os.path.normpath(project_root)\n        project_yaml_filepath = os.path.join(project_root, 'dbt_project.yml')\n\n        # get the project.yml contents\n        if not path_exists(project_yaml_filepath):\n            raise DbtProjectError(\n                'no dbt_project.yml found at expected path {}'\n                .format(project_yaml_filepath)\n            )\n\n        if isinstance(cli_vars, compat.basestring):\n            cli_vars = parse_cli_vars(cli_vars)\n        renderer = ConfigRenderer(cli_vars)\n\n        project_dict = _load_yaml(project_yaml_filepath)\n        rendered_project = renderer.render_project(project_dict)\n        rendered_project['project-root'] = project_root\n        packages_dict = package_data_from_root(project_root)\n        return cls.from_project_config(rendered_project, packages_dict)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of lists of strings where each list of strings represents a type + FQN path of a resource configuration that is not used.", "response": "def get_unused_resource_config_paths(self, resource_fqns, disabled):\n        \"\"\"Return a list of lists of strings, where each inner list of strings\n        represents a type + FQN path of a resource configuration that is not\n        used.\n        \"\"\"\n        disabled_fqns = frozenset(tuple(fqn) for fqn in disabled)\n        resource_config_paths = self.get_resource_config_paths()\n        unused_resource_config_paths = []\n        for resource_type, config_paths in resource_config_paths.items():\n            used_fqns = resource_fqns.get(resource_type, frozenset())\n            fqns = used_fqns | disabled_fqns\n\n            for config_path in config_paths:\n                if not _is_config_used(config_path, fqns):\n                    unused_resource_config_paths.append(\n                        (resource_type,) + config_path\n                    )\n        return unused_resource_config_paths"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_version(self):\n        installed = get_installed_version()\n        if not versions_compatible(*self.dbt_version):\n            msg = IMPOSSIBLE_VERSION_ERROR.format(\n                package=self.project_name,\n                version_spec=[\n                    x.to_version_string() for x in self.dbt_version\n                ]\n            )\n            raise DbtProjectError(msg)\n\n        if not versions_compatible(installed, *self.dbt_version):\n            msg = INVALID_VERSION_ERROR.format(\n                package=self.project_name,\n                installed=installed.to_version_string(),\n                version_spec=[\n                    x.to_version_string() for x in self.dbt_version\n                ]\n            )\n            raise DbtProjectError(msg)", "response": "Ensure this package works with the installed version of dbt."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef can_expand_to(self, other_column):\n        if not self.is_string() or not other_column.is_string():\n            return False\n\n        return other_column.string_size() > self.string_size()", "response": "returns True if this column can be expanded to the size of the other column"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fetch_cluster_credentials(cls, db_user, db_name, cluster_id,\n                                  duration_s):\n        \"\"\"Fetches temporary login credentials from AWS. The specified user\n        must already exist in the database, or else an error will occur\"\"\"\n        boto_client = boto3.client('redshift')\n\n        try:\n            return boto_client.get_cluster_credentials(\n                DbUser=db_user,\n                DbName=db_name,\n                ClusterIdentifier=cluster_id,\n                DurationSeconds=duration_s,\n                AutoCreate=False)\n\n        except boto_client.exceptions.ClientError as e:\n            raise dbt.exceptions.FailedToConnectException(\n                \"Unable to get temporary Redshift cluster credentials: {}\"\n                .format(e))", "response": "Fetches temporary login credentials from AWS."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload and parse models in a list of directories. Returns a dict that maps unique ids onto ParsedNodes", "response": "def load_and_parse(self, package_name, root_dir, relative_dirs,\n                       resource_type, tags=None):\n        \"\"\"Load and parse models in a list of directories. Returns a dict\n           that maps unique ids onto ParsedNodes\"\"\"\n\n        extension = \"[!.#~]*.sql\"\n\n        if tags is None:\n            tags = []\n\n        if dbt.flags.STRICT_MODE:\n            dbt.contracts.project.ProjectList(**self.all_projects)\n\n        file_matches = dbt.clients.system.find_matching(\n            root_dir,\n            relative_dirs,\n            extension)\n\n        result = []\n\n        for file_match in file_matches:\n            file_contents = dbt.clients.system.load_file_contents(\n                file_match.get('absolute_path'))\n\n            parts = dbt.utils.split_path(file_match.get('relative_path', ''))\n            name, _ = os.path.splitext(parts[-1])\n\n            path = self.get_compiled_path(name,\n                                          file_match.get('relative_path'))\n\n            original_file_path = os.path.join(\n                file_match.get('searched_path'),\n                path)\n\n            result.append({\n                'name': name,\n                'root_path': root_dir,\n                'resource_type': resource_type,\n                'path': path,\n                'original_file_path': original_file_path,\n                'package_name': package_name,\n                'raw_sql': file_contents\n            })\n\n        return self.parse_sql_nodes(result, tags)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nwaiting for results off the queue.", "response": "def _wait_for_results(self):\n        \"\"\"Wait for results off the queue. If there is a timeout set, and it is\n        exceeded, raise an RPCTimeoutException.\n        \"\"\"\n        while True:\n            get_timeout = self._next_timeout()\n            try:\n                msgtype, value = self.queue.get(timeout=get_timeout)\n            except QueueEmpty:\n                raise dbt.exceptions.RPCTimeoutException(self.timeout)\n\n            if msgtype == QueueMessageType.Log:\n                self.logs.append(value)\n            elif msgtype in QueueMessageType.terminating():\n                return msgtype, value\n            else:\n                raise dbt.exceptions.InternalException(\n                    'Got invalid queue message type {}'.format(msgtype)\n                )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a queue log handler to the global logger.", "response": "def add_queue_handler(queue):\n    \"\"\"Add a queue log handler to the global logger.\"\"\"\n    handler = QueueLogHandler(queue)\n    handler.setFormatter(QueueFormatter())\n    handler.setLevel(DEBUG)\n    GLOBAL_LOGGER.addHandler(handler)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef invalid_type_error(method_name, arg_name, got_value, expected_type,\n                       version='0.13.0'):\n    \"\"\"Raise a CompilationException when an adapter method available to macros\n    has changed.\n    \"\"\"\n    got_type = type(got_value)\n    msg = (\"As of {version}, 'adapter.{method_name}' expects argument \"\n           \"'{arg_name}' to be of type '{expected_type}', instead got \"\n           \"{got_value} ({got_type})\")\n    raise_compiler_error(msg.format(version=version, method_name=method_name,\n                         arg_name=arg_name, expected_type=expected_type,\n                         got_value=got_value, got_type=got_type))", "response": "Raise a CompilationException when an adapter method available to macros\n    has changed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _inject_ctes_into_sql(sql, ctes):\n    if len(ctes) == 0:\n        return sql\n\n    parsed_stmts = sqlparse.parse(sql)\n    parsed = parsed_stmts[0]\n\n    with_stmt = None\n    for token in parsed.tokens:\n        if token.is_keyword and token.normalized == 'WITH':\n            with_stmt = token\n            break\n\n    if with_stmt is None:\n        # no with stmt, add one, and inject CTEs right at the beginning\n        first_token = parsed.token_first()\n        with_stmt = sqlparse.sql.Token(sqlparse.tokens.Keyword, 'with')\n        parsed.insert_before(first_token, with_stmt)\n    else:\n        # stmt exists, add a comma (which will come after injected CTEs)\n        trailing_comma = sqlparse.sql.Token(sqlparse.tokens.Punctuation, ',')\n        parsed.insert_after(with_stmt, trailing_comma)\n\n    token = sqlparse.sql.Token(\n        sqlparse.tokens.Keyword,\n        \", \".join(c['sql'] for c in ctes)\n    )\n    parsed.insert_after(with_stmt, token)\n\n    return dbt.compat.to_string(parsed)", "response": "Injects CTEs into the SQL."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nbuild out the directory skeleton and python namespace files", "response": "def build_namespace(self):\n        \"\"\"Build out the directory skeleton and python namespace files:\n\n            dbt/\n                __init__.py\n                adapters/\n                    ${adapter_name}\n                    __init__.py\n                include/\n                    ${adapter_name}\n                    __init__.py\n        \"\"\"\n        os.makedirs(self.adapters_path)\n        os.makedirs(pj(self.include_path, 'macros'))\n        with open(pj(self.dbt_dir, '__init__.py'), 'w') as fp:\n            fp.write(NAMESPACE_INIT_TEMPLATE)\n        with open(pj(self.dbt_dir, 'adapters', '__init__.py'), 'w') as fp:\n            fp.write(NAMESPACE_INIT_TEMPLATE)\n        with open(pj(self.dbt_dir, 'include', '__init__.py'), 'w') as fp:\n            fp.write(NAMESPACE_INIT_TEMPLATE)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntranslates BQ SchemaField dicts into dbt BigQueryColumn objects", "response": "def _get_dbt_columns_from_bq_table(self, table):\n        \"Translates BQ SchemaField dicts into dbt BigQueryColumn objects\"\n\n        columns = []\n        for col in table.schema:\n            # BigQuery returns type labels that are not valid type specifiers\n            dtype = self.Column.translate_type(col.field_type)\n            column = self.Column(\n                col.name, dtype, col.fields, col.mode)\n            columns.append(column)\n\n        return columns"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _agate_to_schema(self, agate_table, column_override):\n        bq_schema = []\n        for idx, col_name in enumerate(agate_table.column_names):\n            inferred_type = self.convert_agate_type(agate_table, idx)\n            type_ = column_override.get(col_name, inferred_type)\n            bq_schema.append(\n                google.cloud.bigquery.SchemaField(col_name, type_)\n            )\n        return bq_schema", "response": "Convert agate. Table with column names to a list of bigquery schemas."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_stats_column_names(cls):\n        columns = []\n        stats = ('num_bytes', 'num_rows', 'location', 'partitioning_type',\n                 'clustering_fields')\n        stat_components = ('label', 'value', 'description', 'include')\n        for stat_id in stats:\n            for stat_component in stat_components:\n                columns.append('stats:{}:{}'.format(stat_id, stat_component))\n        return tuple(columns)", "response": "Construct a tuple of the column names for stats. Each stat has 4\n        columns of data."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a table return an iterator of key - value pairs for stats column names and values.", "response": "def _get_stats_columns(cls, table, relation_type):\n        \"\"\"Given a table, return an iterator of key/value pairs for stats\n        column names/values.\n        \"\"\"\n        column_names = cls._get_stats_column_names()\n\n        # agate does not handle the array of column names gracefully\n        clustering_value = None\n        if table.clustering_fields is not None:\n            clustering_value = ','.join(table.clustering_fields)\n        # cast num_bytes/num_rows to str before they get to agate, or else\n        # agate will incorrectly decide they are booleans.\n        column_values = (\n            'Number of bytes',\n            str(table.num_bytes),\n            'The number of bytes this table consumes',\n            relation_type == 'table',\n\n            'Number of rows',\n            str(table.num_rows),\n            'The number of rows in this table',\n            relation_type == 'table',\n\n            'Location',\n            table.location,\n            'The geographic location of this table',\n            True,\n\n            'Partitioning Type',\n            table.partitioning_type,\n            'The partitioning type used for this table',\n            relation_type == 'table',\n\n            'Clustering Fields',\n            clustering_value,\n            'The clustering fields for this table',\n            relation_type == 'table',\n        )\n        return zip(column_names, column_values)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _submit(self, pool, args, callback):\n        if self.config.args.single_threaded:\n            callback(self.call_runner(*args))\n        else:\n            pool.apply_async(self.call_runner, args=args, callback=callback)", "response": "Submit the call to the specified pool."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_queue(self, pool):\n        def callback(result):\n            \"\"\"Note: mark_done, at a minimum, must happen here or dbt will\n            deadlock during ephemeral result error handling!\n            \"\"\"\n            self._handle_result(result)\n            self.job_queue.mark_done(result.node.unique_id)\n\n        while not self.job_queue.empty():\n            node = self.job_queue.get()\n            self._raise_set_error()\n            runner = self.get_runner(node)\n            # we finally know what we're running! Make sure we haven't decided\n            # to skip it due to upstream failures\n            if runner.node.unique_id in self._skipped_children:\n                cause = self._skipped_children.pop(runner.node.unique_id)\n                runner.do_skip(cause=cause)\n            args = (runner,)\n            self._submit(pool, args, callback)\n\n        # block on completion\n        self.job_queue.join()\n        # if an error got set during join(), raise it.\n        self._raise_set_error()\n\n        return", "response": "Given a pool submit jobs from the queue to the pool."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_result(self, result):\n        is_ephemeral = result.node.is_ephemeral_model\n        if not is_ephemeral:\n            self.node_results.append(result)\n\n        node = CompileResultNode(**result.node)\n        node_id = node.unique_id\n        self.manifest.nodes[node_id] = node\n\n        if result.error is not None:\n            if is_ephemeral:\n                cause = result\n            else:\n                cause = None\n            self._mark_dependent_errors(node_id, result, cause)", "response": "Mark the result as completed, insert the `CompiledResultNode` into\n        the manifest, and mark any descendants (potentially with a 'cause' if\n        the result was an ephemeral model) as skipped."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the dbt query based on the graph.", "response": "def run(self):\n        \"\"\"\n        Run dbt for the query, based on the graph.\n        \"\"\"\n        self._runtime_initialize()\n\n        if len(self._flattened_nodes) == 0:\n            logger.warning(\"WARNING: Nothing to do. Try checking your model \"\n                           \"configs and model specification args\")\n            return []\n        else:\n            logger.info(\"\")\n\n        selected_uids = frozenset(n.unique_id for n in self._flattened_nodes)\n        result = self.execute_with_hooks(selected_uids)\n\n        result.write(self.result_path())\n\n        self.task_end_messages(result.results)\n        return result.results"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_bigquery_table(self, database, schema, table_name, callback,\n                              sql):\n        \"\"\"Create a bigquery table. The caller must supply a callback\n        that takes one argument, a `google.cloud.bigquery.Table`, and mutates\n        it.\n        \"\"\"\n        conn = self.get_thread_connection()\n        client = conn.handle\n\n        view_ref = self.table_ref(database, schema, table_name, conn)\n        view = google.cloud.bigquery.Table(view_ref)\n        callback(view)\n\n        with self.exception_handler(sql):\n            client.create_table(view)", "response": "Create a bigquery table."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_bq_table(self, database, schema, identifier):\n        conn = self.get_thread_connection()\n        table_ref = self.table_ref(database, schema, identifier, conn)\n        return conn.handle.get_table(table_ref)", "response": "Get a bigquery table for a schema and model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a task that calls ``compute fn`` (along with additional arguments ``*compute_args`` and ``**compute_kwargs``), then applies the returned value to :meth:`.Bot.answerInlineQuery` to answer the inline query. If a preceding task is already working for a user, that task is cancelled, thus ensuring at most one active task per user id. :param inline_query: The inline query to be processed. The originating user is inferred from ``msg['from']['id']``. :param compute_fn: A function whose returned value is given to :meth:`.Bot.answerInlineQuery` to send. May return: - a *list* of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_ - a *tuple* whose first element is a list of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_, followed by positional arguments to be supplied to :meth:`.Bot.answerInlineQuery` - a *dictionary* representing keyword arguments to be supplied to :meth:`.Bot.answerInlineQuery` :param \\*compute_args: positional arguments to ``compute_fn`` :param \\*\\*compute_kwargs: keyword arguments to ``compute_fn``", "response": "def answer(self, inline_query, compute_fn, *compute_args, **compute_kwargs):\n        \"\"\"\n        Create a task that calls ``compute fn`` (along with additional arguments\n        ``*compute_args`` and ``**compute_kwargs``), then applies the returned value to\n        :meth:`.Bot.answerInlineQuery` to answer the inline query.\n        If a preceding task is already working for a user, that task is cancelled,\n        thus ensuring at most one active task per user id.\n\n        :param inline_query:\n            The inline query to be processed. The originating user is inferred from ``msg['from']['id']``.\n\n        :param compute_fn:\n            A function whose returned value is given to :meth:`.Bot.answerInlineQuery` to send.\n            May return:\n\n            - a *list* of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_\n            - a *tuple* whose first element is a list of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_,\n              followed by positional arguments to be supplied to :meth:`.Bot.answerInlineQuery`\n            - a *dictionary* representing keyword arguments to be supplied to :meth:`.Bot.answerInlineQuery`\n\n        :param \\*compute_args: positional arguments to ``compute_fn``\n        :param \\*\\*compute_kwargs: keyword arguments to ``compute_fn``\n        \"\"\"\n\n        from_id = inline_query['from']['id']\n\n        async def compute_and_answer():\n            try:\n                query_id = inline_query['id']\n\n                ans = await _invoke(compute_fn, *compute_args, **compute_kwargs)\n\n                if isinstance(ans, list):\n                    await self._bot.answerInlineQuery(query_id, ans)\n                elif isinstance(ans, tuple):\n                    await self._bot.answerInlineQuery(query_id, *ans)\n                elif isinstance(ans, dict):\n                    await self._bot.answerInlineQuery(query_id, **ans)\n                else:\n                    raise ValueError('Invalid answer format')\n            except CancelledError:\n                # Cancelled. Record has been occupied by new task. Don't touch.\n                raise\n            except:\n                # Die accidentally. Remove myself from record.\n                del self._working_tasks[from_id]\n                raise\n            else:\n                # Die naturally. Remove myself from record.\n                del self._working_tasks[from_id]\n\n        if from_id in self._working_tasks:\n            self._working_tasks[from_id].cancel()\n\n        t = self._loop.create_task(compute_and_answer())\n        self._working_tasks[from_id] = t"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def run_forever(self, relax=0.1, offset=None, timeout=20, allowed_updates=None):\n        while 1:\n            try:\n                result = await self._bot.getUpdates(offset=offset,\n                                                    timeout=timeout,\n                                                    allowed_updates=allowed_updates)\n\n                # Once passed, this parameter is no longer needed.\n                allowed_updates = None\n\n                # No sort. Trust server to give messages in correct order.\n                for update in result:\n                    self._update_handler(update)\n                    offset = update['update_id'] + 1\n\n            except CancelledError:\n                break\n            except exception.BadHTTPResponse as e:\n                traceback.print_exc()\n\n                # Servers probably down. Wait longer.\n                if e.status == 502:\n                    await asyncio.sleep(30)\n            except:\n                traceback.print_exc()\n                await asyncio.sleep(relax)\n            else:\n                await asyncio.sleep(relax)", "response": "Loop forever and process new updates in infinity loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the message s flavor.", "response": "def flavor(msg):\n    \"\"\"\n    Return flavor of message or event.\n\n    A message's flavor may be one of these:\n\n    - ``chat``\n    - ``callback_query``\n    - ``inline_query``\n    - ``chosen_inline_result``\n    - ``shipping_query``\n    - ``pre_checkout_query``\n\n    An event's flavor is determined by the single top-level key.\n    \"\"\"\n    if 'message_id' in msg:\n        return 'chat'\n    elif 'id' in msg and 'chat_instance' in msg:\n        return 'callback_query'\n    elif 'id' in msg and 'query' in msg:\n        return 'inline_query'\n    elif 'result_id' in msg:\n        return 'chosen_inline_result'\n    elif 'id' in msg and 'shipping_address' in msg:\n        return 'shipping_query'\n    elif 'id' in msg and 'total_amount' in msg:\n        return 'pre_checkout_query'\n    else:\n        top_keys = list(msg.keys())\n        if len(top_keys) == 1:\n            return top_keys[0]\n\n        raise exception.BadFlavor(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a list of headlines for a message.", "response": "def glance(msg, flavor='chat', long=False):\n    \"\"\"\n    Extract \"headline\" info about a message.\n    Use parameter ``long`` to control whether a short or long tuple is returned.\n\n    When ``flavor`` is ``chat``\n    (``msg`` being a `Message <https://core.telegram.org/bots/api#message>`_ object):\n\n    - short: (content_type, ``msg['chat']['type']``, ``msg['chat']['id']``)\n    - long: (content_type, ``msg['chat']['type']``, ``msg['chat']['id']``, ``msg['date']``, ``msg['message_id']``)\n\n    *content_type* can be: ``text``, ``audio``, ``document``, ``game``, ``photo``, ``sticker``, ``video``, ``voice``,\n    ``video_note``, ``contact``, ``location``, ``venue``, ``new_chat_member``, ``left_chat_member``, ``new_chat_title``,\n    ``new_chat_photo``, ``delete_chat_photo``, ``group_chat_created``, ``supergroup_chat_created``,\n    ``channel_chat_created``, ``migrate_to_chat_id``, ``migrate_from_chat_id``, ``pinned_message``,\n    ``new_chat_members``, ``invoice``, ``successful_payment``.\n\n    When ``flavor`` is ``callback_query``\n    (``msg`` being a `CallbackQuery <https://core.telegram.org/bots/api#callbackquery>`_ object):\n\n    - regardless: (``msg['id']``, ``msg['from']['id']``, ``msg['data']``)\n\n    When ``flavor`` is ``inline_query``\n    (``msg`` being a `InlineQuery <https://core.telegram.org/bots/api#inlinequery>`_ object):\n\n    - short: (``msg['id']``, ``msg['from']['id']``, ``msg['query']``)\n    - long: (``msg['id']``, ``msg['from']['id']``, ``msg['query']``, ``msg['offset']``)\n\n    When ``flavor`` is ``chosen_inline_result``\n    (``msg`` being a `ChosenInlineResult <https://core.telegram.org/bots/api#choseninlineresult>`_ object):\n\n    - regardless: (``msg['result_id']``, ``msg['from']['id']``, ``msg['query']``)\n\n    When ``flavor`` is ``shipping_query``\n    (``msg`` being a `ShippingQuery <https://core.telegram.org/bots/api#shippingquery>`_ object):\n\n    - regardless: (``msg['id']``, ``msg['from']['id']``, ``msg['invoice_payload']``)\n\n    When ``flavor`` is ``pre_checkout_query``\n    (``msg`` being a `PreCheckoutQuery <https://core.telegram.org/bots/api#precheckoutquery>`_ object):\n\n    - short: (``msg['id']``, ``msg['from']['id']``, ``msg['invoice_payload']``)\n    - long: (``msg['id']``, ``msg['from']['id']``, ``msg['invoice_payload']``, ``msg['currency']``, ``msg['total_amount']``)\n    \"\"\"\n    def gl_chat():\n        content_type = _find_first_key(msg, all_content_types)\n\n        if long:\n            return content_type, msg['chat']['type'], msg['chat']['id'], msg['date'], msg['message_id']\n        else:\n            return content_type, msg['chat']['type'], msg['chat']['id']\n\n    def gl_callback_query():\n        return msg['id'], msg['from']['id'], msg['data']\n\n    def gl_inline_query():\n        if long:\n            return msg['id'], msg['from']['id'], msg['query'], msg['offset']\n        else:\n            return msg['id'], msg['from']['id'], msg['query']\n\n    def gl_chosen_inline_result():\n        return msg['result_id'], msg['from']['id'], msg['query']\n\n    def gl_shipping_query():\n        return msg['id'], msg['from']['id'], msg['invoice_payload']\n\n    def gl_pre_checkout_query():\n        if long:\n            return msg['id'], msg['from']['id'], msg['invoice_payload'], msg['currency'], msg['total_amount']\n        else:\n            return msg['id'], msg['from']['id'], msg['invoice_payload']\n\n    try:\n        fn = {'chat': gl_chat,\n              'callback_query': gl_callback_query,\n              'inline_query': gl_inline_query,\n              'chosen_inline_result': gl_chosen_inline_result,\n              'shipping_query': gl_shipping_query,\n              'pre_checkout_query': gl_pre_checkout_query}[flavor]\n    except KeyError:\n        raise exception.BadFlavor(flavor)\n\n    return fn()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flance(msg, long=False):\n    f = flavor(msg)\n    g = glance(msg, flavor=f, long=long)\n    return f,g", "response": "Returns a 2 - tuple of the current message s flavor and glance."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message to a chat.", "response": "def sendMessage(self, chat_id, text,\n                    parse_mode=None,\n                    disable_web_page_preview=None,\n                    disable_notification=None,\n                    reply_to_message_id=None,\n                    reply_markup=None):\n        \"\"\" See: https://core.telegram.org/bots/api#sendmessage \"\"\"\n        p = _strip(locals())\n        return self._api_request('sendMessage', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nforwarding a message to a chat.", "response": "def forwardMessage(self, chat_id, from_chat_id, message_id,\n                       disable_notification=None):\n        \"\"\" See: https://core.telegram.org/bots/api#forwardmessage \"\"\"\n        p = _strip(locals())\n        return self._api_request('forwardMessage', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sendVoice(self, chat_id, voice,\n                  caption=None,\n                  parse_mode=None,\n                  duration=None,\n                  disable_notification=None,\n                  reply_to_message_id=None,\n                  reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#sendvoice\n\n        :param voice: Same as ``photo`` in :meth:`telepot.Bot.sendPhoto`\n        \"\"\"\n        p = _strip(locals(), more=['voice'])\n        return self._api_request_with_file('sendVoice', _rectify(p), 'voice', voice)", "response": "Send a voice to a chat."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nusing this method to send a video note.", "response": "def sendVideoNote(self, chat_id, video_note,\n                      duration=None,\n                      length=None,\n                      disable_notification=None,\n                      reply_to_message_id=None,\n                      reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#sendvideonote\n\n        :param video_note: Same as ``photo`` in :meth:`telepot.Bot.sendPhoto`\n\n        :param length:\n            Although marked as optional, this method does not seem to work without\n            it being specified. Supply any integer you want. It seems to have no effect\n            on the video note's display size.\n        \"\"\"\n        p = _strip(locals(), more=['video_note'])\n        return self._api_request_with_file('sendVideoNote', _rectify(p), 'video_note', video_note)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sendMediaGroup(self, chat_id, media,\n                       disable_notification=None,\n                       reply_to_message_id=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#sendmediagroup\n\n        :type media: array of `InputMedia <https://core.telegram.org/bots/api#inputmedia>`_ objects\n        :param media:\n            To indicate media locations, each InputMedia object's ``media`` field\n            should be one of these:\n\n            - string: ``file_id`` for a file existing on Telegram servers\n            - string: HTTP URL of a file from the Internet\n            - file-like object: obtained by ``open(path, 'rb')``\n            - tuple: (form-data name, file-like object)\n            - tuple: (form-data name, (filename, file-like object))\n\n            In case of uploading, you may supply customized multipart/form-data\n            names for each uploaded file (as in last 2 options above). Otherwise,\n            telepot assigns unique names to each uploaded file. Names assigned by\n            telepot will not collide with user-supplied names, if any.\n        \"\"\"\n        p = _strip(locals(), more=['media'])\n        legal_media, files_to_attach = _split_input_media_array(media)\n\n        p['media'] = legal_media\n        return self._api_request('sendMediaGroup', _rectify(p), files_to_attach)", "response": "Send a group of media to Telegram servers."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sendLocation(self, chat_id, latitude, longitude,\n                     live_period=None,\n                     disable_notification=None,\n                     reply_to_message_id=None,\n                     reply_markup=None):\n        \"\"\" See: https://core.telegram.org/bots/api#sendlocation \"\"\"\n        p = _strip(locals())\n        return self._api_request('sendLocation', _rectify(p))", "response": "Send a location to a specific chat."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsend a game to a specific chat.", "response": "def sendGame(self, chat_id, game_short_name,\n                 disable_notification=None,\n                 reply_to_message_id=None,\n                 reply_markup=None):\n        \"\"\" See: https://core.telegram.org/bots/api#sendgame \"\"\"\n        p = _strip(locals())\n        return self._api_request('sendGame', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending an invoice to a specific channel.", "response": "def sendInvoice(self, chat_id, title, description, payload,\n                    provider_token, start_parameter, currency, prices,\n                    provider_data=None,\n                    photo_url=None,\n                    photo_size=None,\n                    photo_width=None,\n                    photo_height=None,\n                    need_name=None,\n                    need_phone_number=None,\n                    need_email=None,\n                    need_shipping_address=None,\n                    is_flexible=None,\n                    disable_notification=None,\n                    reply_to_message_id=None,\n                    reply_markup=None):\n        \"\"\" See: https://core.telegram.org/bots/api#sendinvoice \"\"\"\n        p = _strip(locals())\n        return self._api_request('sendInvoice', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sendChatAction(self, chat_id, action):\n        p = _strip(locals())\n        return self._api_request('sendChatAction', _rectify(p))", "response": "Send a chat action"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef getUserProfilePhotos(self, user_id,\n                             offset=None,\n                             limit=None):\n        \"\"\" See: https://core.telegram.org/bots/api#getuserprofilephotos \"\"\"\n        p = _strip(locals())\n        return self._api_request('getUserProfilePhotos', _rectify(p))", "response": "Get user profile photos"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\npromote a user to a chat.", "response": "def promoteChatMember(self, chat_id, user_id,\n                          can_change_info=None,\n                          can_post_messages=None,\n                          can_edit_messages=None,\n                          can_delete_messages=None,\n                          can_invite_users=None,\n                          can_restrict_members=None,\n                          can_pin_messages=None,\n                          can_promote_members=None):\n        \"\"\" See: https://core.telegram.org/bots/api#promotechatmember \"\"\"\n        p = _strip(locals())\n        return self._api_request('promoteChatMember', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexport a chat invite link", "response": "def exportChatInviteLink(self, chat_id):\n        \"\"\" See: https://core.telegram.org/bots/api#exportchatinvitelink \"\"\"\n        p = _strip(locals())\n        return self._api_request('exportChatInviteLink', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets the chat photo", "response": "def setChatPhoto(self, chat_id, photo):\n        \"\"\" See: https://core.telegram.org/bots/api#setchatphoto \"\"\"\n        p = _strip(locals(), more=['photo'])\n        return self._api_request_with_file('setChatPhoto', _rectify(p), 'photo', photo)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef deleteChatPhoto(self, chat_id):\n        p = _strip(locals())\n        return self._api_request('deleteChatPhoto', _rectify(p))", "response": "Delete a chat photo"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets a chat by id", "response": "def getChat(self, chat_id):\n        \"\"\" See: https://core.telegram.org/bots/api#getchat \"\"\"\n        p = _strip(locals())\n        return self._api_request('getChat', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting a list of administrators for a chat", "response": "def getChatAdministrators(self, chat_id):\n        \"\"\" See: https://core.telegram.org/bots/api#getchatadministrators \"\"\"\n        p = _strip(locals())\n        return self._api_request('getChatAdministrators', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef getChatMembersCount(self, chat_id):\n        p = _strip(locals())\n        return self._api_request('getChatMembersCount', _rectify(p))", "response": "Get the number of members of a chat."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef answerShippingQuery(self, shipping_query_id, ok,\n                            shipping_options=None,\n                            error_message=None):\n        \"\"\" See: https://core.telegram.org/bots/api#answershippingquery \"\"\"\n        p = _strip(locals())\n        return self._api_request('answerShippingQuery', _rectify(p))", "response": "Send an answer to a shipping query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef answerPreCheckoutQuery(self, pre_checkout_query_id, ok,\n                               error_message=None):\n        \"\"\" See: https://core.telegram.org/bots/api#answerprecheckoutquery \"\"\"\n        p = _strip(locals())\n        return self._api_request('answerPreCheckoutQuery', _rectify(p))", "response": "Send an answer to a pre - checkout query."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef editMessageText(self, msg_identifier, text,\n                        parse_mode=None,\n                        disable_web_page_preview=None,\n                        reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#editmessagetext\n\n        :param msg_identifier:\n            a 2-tuple (``chat_id``, ``message_id``),\n            a 1-tuple (``inline_message_id``),\n            or simply ``inline_message_id``.\n            You may extract this value easily with :meth:`telepot.message_identifier`\n        \"\"\"\n        p = _strip(locals(), more=['msg_identifier'])\n        p.update(_dismantle_message_identifier(msg_identifier))\n        return self._api_request('editMessageText', _rectify(p))", "response": "Edit a message s text."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nedit the caption of a message.", "response": "def editMessageCaption(self, msg_identifier,\n                           caption=None,\n                           parse_mode=None,\n                           reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#editmessagecaption\n\n        :param msg_identifier: Same as ``msg_identifier`` in :meth:`telepot.Bot.editMessageText`\n        \"\"\"\n        p = _strip(locals(), more=['msg_identifier'])\n        p.update(_dismantle_message_identifier(msg_identifier))\n        return self._api_request('editMessageCaption', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef uploadStickerFile(self, user_id, png_sticker):\n        p = _strip(locals(), more=['png_sticker'])\n        return self._api_request_with_file('uploadStickerFile', _rectify(p), 'png_sticker', png_sticker)", "response": "Upload a sticker file to a user"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef createNewStickerSet(self, user_id, name, title, png_sticker, emojis,\n                            contains_masks=None,\n                            mask_position=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#createnewstickerset\n        \"\"\"\n        p = _strip(locals(), more=['png_sticker'])\n        return self._api_request_with_file('createNewStickerSet', _rectify(p), 'png_sticker', png_sticker)", "response": "Create a new sticker set."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef addStickerToSet(self, user_id, name, png_sticker, emojis,\n                        mask_position=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#addstickertoset\n        \"\"\"\n        p = _strip(locals(), more=['png_sticker'])\n        return self._api_request_with_file('addStickerToSet', _rectify(p), 'png_sticker', png_sticker)", "response": "Add a new sticker to a user."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef setStickerPositionInSet(self, sticker, position):\n        p = _strip(locals())\n        return self._api_request('setStickerPositionInSet', _rectify(p))", "response": "Set the sticker s position in the set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setGameScore(self, user_id, score, game_message_identifier,\n                     force=None,\n                     disable_edit_message=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#setgamescore\n\n        :param game_message_identifier: Same as ``msg_identifier`` in :meth:`telepot.Bot.editMessageText`\n        \"\"\"\n        p = _strip(locals(), more=['game_message_identifier'])\n        p.update(_dismantle_message_identifier(game_message_identifier))\n        return self._api_request('setGameScore', _rectify(p))", "response": "Set the game score for a user."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the high scores of a game message.", "response": "def getGameHighScores(self, user_id, game_message_identifier):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#getgamehighscores\n\n        :param game_message_identifier: Same as ``msg_identifier`` in :meth:`telepot.Bot.editMessageText`\n        \"\"\"\n        p = _strip(locals(), more=['game_message_identifier'])\n        p.update(_dismantle_message_identifier(game_message_identifier))\n        return self._api_request('getGameHighScores', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef download_file(self, file_id, dest):\n        f = self.getFile(file_id)\n        try:\n            d = dest if _isfile(dest) else open(dest, 'wb')\n\n            r = api.download((self._token, f['file_path']), preload_content=False)\n\n            while 1:\n                data = r.read(self._file_chunk_size)\n                if not data:\n                    break\n                d.write(data)\n        finally:\n            if not _isfile(dest) and 'd' in locals():\n                d.close()\n\n            if 'r' in locals():\n                r.release_conn()", "response": "Download a file to local disk."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef per_chat_id_in(s, types='all'):\n    return _wrap_none(lambda msg:\n                          msg['chat']['id']\n                          if (types == 'all' or msg['chat']['type'] in types) and msg['chat']['id'] in s\n                          else None)", "response": "Returns a seeder function that returns the chat id only if the chat id is in s and chat type is in types."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a seeder function that returns the chat id only if the chat id is not in s and chat type is in types.", "response": "def per_chat_id_except(s, types='all'):\n    \"\"\"\n    :param s:\n        a list or set of chat id\n\n    :param types:\n        ``all`` or a list of chat types (``private``, ``group``, ``channel``)\n\n    :return:\n        a seeder function that returns the chat id only if the chat id is *not* in ``s``\n        and chat type is in ``types``.\n    \"\"\"\n    return _wrap_none(lambda msg:\n                          msg['chat']['id']\n                          if (types == 'all' or msg['chat']['type'] in types) and msg['chat']['id'] not in s\n                          else None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a seeder function that returns the from id only if the message has a from id.", "response": "def per_from_id(flavors=chat_flavors+inline_flavors):\n    \"\"\"\n    :param flavors:\n        ``all`` or a list of flavors\n\n    :return:\n        a seeder function that returns the from id only if the message flavor is\n        in ``flavors``.\n    \"\"\"\n    return _wrap_none(lambda msg:\n                          msg['from']['id']\n                          if flavors == 'all' or flavor(msg) in flavors\n                          else None)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef per_from_id_in(s, flavors=chat_flavors+inline_flavors):\n    return _wrap_none(lambda msg:\n                          msg['from']['id']\n                          if (flavors == 'all' or flavor(msg) in flavors) and msg['from']['id'] in s\n                          else None)", "response": "Returns a seeder function that returns the from id only if the from id is in s."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef per_from_id_except(s, flavors=chat_flavors+inline_flavors):\n    return _wrap_none(lambda msg:\n                          msg['from']['id']\n                          if (flavors == 'all' or flavor(msg) in flavors) and msg['from']['id'] not in s\n                          else None)", "response": "Returns a seeder function that returns the from id only if the from id is not in s."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef per_callback_query_chat_id(types='all'):\n    def f(msg):\n        if (flavor(msg) == 'callback_query' and 'message' in msg\n            and (types == 'all' or msg['message']['chat']['type'] in types)):\n            return msg['message']['chat']['id']\n        else:\n            return None\n    return f", "response": "Returns a seeder function that returns a callback query s originating chat id if the chat type is in types."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a seeder function that returns a callback query s origin identifier if the origin type is in origins.", "response": "def per_callback_query_origin(origins='all'):\n    \"\"\"\n    :param origins:\n        ``all`` or a list of origin types (``chat``, ``inline``)\n\n    :return:\n        a seeder function that returns a callback query's origin identifier if\n        that origin type is in ``origins``. The origin identifier is guaranteed\n        to be a tuple.\n    \"\"\"\n    def f(msg):\n        def origin_type_ok():\n            return (origins == 'all'\n                or ('chat' in origins and 'message' in msg)\n                or ('inline' in origins and 'inline_message_id' in msg))\n\n        if flavor(msg) == 'callback_query' and origin_type_ok():\n            if 'inline_message_id' in msg:\n                return msg['inline_message_id'],\n            else:\n                return msg['message']['chat']['id'], msg['message']['message_id']\n        else:\n            return None\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef call(func, *args, **kwargs):\n    def f(seed_tuple):\n        return func, (seed_tuple,)+args, kwargs\n    return f", "response": "A function that returns a tuple that returns a seed tuple and args and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_run(cls, *args, **kwargs):\n    def f(seed_tuple):\n        j = cls(seed_tuple, *args, **kwargs)\n        return j.run\n    return f", "response": "Creates a delegator function that calls the cls constructor followed by supplied args and kwargs and returns\n        the object s run method."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a new object that opens a new message and returns the new object.", "response": "def create_open(cls, *args, **kwargs):\n    \"\"\"\n    :return:\n        a delegator function that calls the ``cls`` constructor whose arguments being\n        a seed tuple followed by supplied ``*args`` and ``**kwargs``, then returns\n        a looping function that uses the object's ``listener`` to wait for messages\n        and invokes instance method ``open``, ``on_message``, and ``on_close`` accordingly.\n        By default, a thread wrapping that looping function is spawned.\n    \"\"\"\n    def f(seed_tuple):\n        j = cls(seed_tuple, *args, **kwargs)\n\n        def wait_loop():\n            bot, msg, seed = seed_tuple\n            try:\n                handled = j.open(msg, seed)\n                if not handled:\n                    j.on_message(msg)\n\n                while 1:\n                    msg = j.listener.wait()\n                    j.on_message(msg)\n\n            # These exceptions are \"normal\" exits.\n            except (exception.IdleTerminate, exception.StopListening) as e:\n                j.on_close(e)\n\n            # Any other exceptions are accidents. **Print it out.**\n            # This is to prevent swallowing exceptions in the case that on_close()\n            # gets overridden but fails to account for unexpected exceptions.\n            except Exception as e:\n                traceback.print_exc()\n                j.on_close(e)\n\n        return wait_loop\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a function that returns the first seed that satisfies a condition.", "response": "def until(condition, fns):\n    \"\"\"\n    Try a list of seeder functions until a condition is met.\n\n    :param condition:\n        a function that takes one argument - a seed - and returns ``True``\n        or ``False``\n\n    :param fns:\n        a list of seeder functions\n\n    :return:\n        a \"composite\" seeder function that calls each supplied function in turn,\n        and returns the first seed where the condition is met. If the condition\n        is never met, it returns ``None``.\n    \"\"\"\n    def f(msg):\n        for fn in fns:\n            seed = fn(msg)\n            if condition(seed):\n                return seed\n        return None\n    return f"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pair(seeders, delegator_factory, *args, **kwargs):\n    return (chain(*seeders) if len(seeders) > 1 else seeders[0],\n            delegator_factory(*args, **kwargs))", "response": "A basic pair producer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef intercept_callback_query_origin(fn=pair, origins='all'):\n    origin_map = helper.SafeDict()\n\n    # For key functions that returns a tuple as key (e.g. per_callback_query_origin()),\n    # wrap the key in another tuple to prevent router from mistaking it as\n    # a key followed by some arguments.\n    def tuplize(fn):\n        def tp(msg):\n            return (fn(msg),)\n        return tp\n\n    router = helper.Router(tuplize(per_callback_query_origin(origins=origins)),\n                           origin_map)\n\n    def modify_origin_map(origin, dest, set):\n        if set:\n            origin_map[origin] = dest\n        else:\n            try:\n                del origin_map[origin]\n            except KeyError:\n                pass\n\n    if origins == 'all':\n        intercept = modify_origin_map\n    else:\n        intercept = (modify_origin_map if 'chat' in origins else False,\n                     modify_origin_map if 'inline' in origins else False)\n\n    @_ensure_seeders_list\n    def p(seeders, delegator_factory, *args, **kwargs):\n        return fn(seeders + [_wrap_none(router.map)],\n                  delegator_factory, *args, intercept_callback_query=intercept, **kwargs)\n    return p", "response": "Decorator that intercepts callback query origin mapping across seeder and delegator."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the proxy for the base class of the base class.", "response": "def set_proxy(url, basic_auth=None):\n    \"\"\"\n    Access Bot API through a proxy.\n\n    :param url: proxy URL\n    :param basic_auth: 2-tuple ``('username', 'password')``\n    \"\"\"\n    global _pools, _onetime_pool_spec\n    if not url:\n        _pools['default'] = urllib3.PoolManager(**_default_pool_params)\n        _onetime_pool_spec = (urllib3.PoolManager, _onetime_pool_params)\n    elif basic_auth:\n        h = urllib3.make_headers(proxy_basic_auth=':'.join(basic_auth))\n        _pools['default'] = urllib3.ProxyManager(url, proxy_headers=h, **_default_pool_params)\n        _onetime_pool_spec = (urllib3.ProxyManager, dict(proxy_url=url, proxy_headers=h, **_onetime_pool_params))\n    else:\n        _pools['default'] = urllib3.ProxyManager(url, **_default_pool_params)\n        _onetime_pool_spec = (urllib3.ProxyManager, dict(proxy_url=url, **_onetime_pool_params))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_forever(self, *args, **kwargs):\n        collectloop = CollectLoop(self._handle)\n        updatesloop = GetUpdatesLoop(self._bot,\n                                     lambda update:\n                                         collectloop.input_queue.put(_extract_message(update)[1]))\n                                         # feed messages to collect loop\n        # feed events to collect loop\n        self._bot.scheduler.on_event(collectloop.input_queue.put)\n        self._bot.scheduler.run_as_thread()\n\n        updatesloop.run_as_thread(*args, **kwargs)\n        collectloop.run_forever()", "response": "This method runs the main loop for the getUpdates method."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_forever(self, *args, **kwargs):\n        # feed events to collect loop\n        self._bot.scheduler.on_event(self._collectloop.input_queue.put)\n        self._bot.scheduler.run_as_thread()\n\n        self._orderer.run_as_thread(*args, **kwargs)\n        self._collectloop.run_forever()", "response": "This method is called by the bot to run the loop forever."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef feed(self, data):\n        update = _dictify(data)\n        self._orderer.input_queue.put(update)", "response": "Feeds data into the internal queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a routing table for the given object and a list of keys.", "response": "def make_routing_table(obj, keys, prefix='on_'):\n    \"\"\"\n    :return:\n        a dictionary roughly equivalent to ``{'key1': obj.on_key1, 'key2': obj.on_key2, ...}``,\n        but ``obj`` does not have to define all methods. It may define the needed ones only.\n\n    :param obj: the object\n\n    :param keys: a list of keys\n\n    :param prefix: a string to be prepended to keys to make method names\n    \"\"\"\n    def maptuple(k):\n        if isinstance(k, tuple):\n            if len(k) == 2:\n                return k\n            elif len(k) == 1:\n                return k[0], _create_invoker(obj, prefix+k[0])\n            else:\n                raise ValueError()\n        else:\n            return k, _create_invoker(obj, prefix+k)\n\n    return dict([maptuple(k) for k in keys])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef by_command(extractor, prefix=('/',), separator=' ', pass_args=False):\n    if not isinstance(prefix, (tuple, list)):\n        prefix = (prefix,)\n\n    def f(msg):\n        text = extractor(msg)\n        for px in prefix:\n            if text.startswith(px):\n                chunks = text[len(px):].split(separator)\n                return chunks[0], (chunks[1:],) if pass_args else ()\n        return (None,),  # to distinguish with `None`\n    return f", "response": "A key function that returns a portion of a chat message that is used to interpret a specific part of a chat message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a key function that interprets a chat message s text and returns a 1 - tuple containing the key of the embedded command.", "response": "def by_chat_command(prefix=('/',), separator=' ', pass_args=False):\n    \"\"\"\n    :param prefix:\n        a list of special characters expected to indicate the head of a command.\n\n    :param separator:\n        a command may be followed by arguments separated by ``separator``.\n\n    :type pass_args: bool\n    :param pass_args:\n        If ``True``, arguments following a command will be passed to the handler\n        function.\n\n    :return:\n        a key function that interprets a chat message's text and returns\n        the embedded command, optionally followed by arguments. If the text is\n        not preceded by any of the specified ``prefix``, it returns a 1-tuple\n        ``(None,)`` as the key. This is to distinguish with the special\n        ``None`` key in routing table.\n    \"\"\"\n    return by_command(lambda msg: msg['text'], prefix, separator, pass_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef by_regex(extractor, regex, key=1):\n    if _isstring(regex):\n        regex = re.compile(regex)\n\n    def f(msg):\n        text = extractor(msg)\n        match = regex.search(text)\n        if match:\n            index = key if isinstance(key, tuple) else (key,)\n            return match.group(*index), (match,)\n        else:\n            return (None,),  # to distinguish with `None`\n    return f", "response": "Returns a function that returns a portion of the message that matches the given regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef lower_key(fn):\n    def lower(key):\n        try:\n            return key.lower()\n        except AttributeError:\n            return key\n    return process_key(lower, fn)", "response": "A function that wraps around the supplied key function to ensure that the key is in lowercase."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef wait(self):\n        if not self._patterns:\n            raise RuntimeError('Listener has nothing to capture')\n\n        while 1:\n            msg = self._queue.get(block=True)\n\n            if any(map(lambda p: filtering.match_all(msg, p), self._patterns)):\n                return msg", "response": "Block until a message appears in the queue."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nspawns a thread that calls ``compute fn`` (along with additional arguments ``*compute_args`` and ``**compute_kwargs``), then applies the returned value to :meth:`.Bot.answerInlineQuery` to answer the inline query. If a preceding thread is already working for a user, that thread is cancelled, thus ensuring at most one active thread per user id. :param inline_query: The inline query to be processed. The originating user is inferred from ``msg['from']['id']``. :param compute_fn: A **thread-safe** function whose returned value is given to :meth:`.Bot.answerInlineQuery` to send. May return: - a *list* of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_ - a *tuple* whose first element is a list of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_, followed by positional arguments to be supplied to :meth:`.Bot.answerInlineQuery` - a *dictionary* representing keyword arguments to be supplied to :meth:`.Bot.answerInlineQuery` :param \\*compute_args: positional arguments to ``compute_fn`` :param \\*\\*compute_kwargs: keyword arguments to ``compute_fn``", "response": "def answer(outerself, inline_query, compute_fn, *compute_args, **compute_kwargs):\n        \"\"\"\n        Spawns a thread that calls ``compute fn`` (along with additional arguments\n        ``*compute_args`` and ``**compute_kwargs``), then applies the returned value to\n        :meth:`.Bot.answerInlineQuery` to answer the inline query.\n        If a preceding thread is already working for a user, that thread is cancelled,\n        thus ensuring at most one active thread per user id.\n\n        :param inline_query:\n            The inline query to be processed. The originating user is inferred from ``msg['from']['id']``.\n\n        :param compute_fn:\n            A **thread-safe** function whose returned value is given to :meth:`.Bot.answerInlineQuery` to send.\n            May return:\n\n            - a *list* of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_\n            - a *tuple* whose first element is a list of `InlineQueryResult <https://core.telegram.org/bots/api#inlinequeryresult>`_,\n              followed by positional arguments to be supplied to :meth:`.Bot.answerInlineQuery`\n            - a *dictionary* representing keyword arguments to be supplied to :meth:`.Bot.answerInlineQuery`\n\n        :param \\*compute_args: positional arguments to ``compute_fn``\n        :param \\*\\*compute_kwargs: keyword arguments to ``compute_fn``\n        \"\"\"\n\n        from_id = inline_query['from']['id']\n\n        class Worker(threading.Thread):\n            def __init__(innerself):\n                super(Worker, innerself).__init__()\n                innerself._cancelled = False\n\n            def cancel(innerself):\n                innerself._cancelled = True\n\n            def run(innerself):\n                try:\n                    query_id = inline_query['id']\n\n                    if innerself._cancelled:\n                        return\n\n                    # Important: compute function must be thread-safe.\n                    ans = compute_fn(*compute_args, **compute_kwargs)\n\n                    if innerself._cancelled:\n                        return\n\n                    if isinstance(ans, list):\n                        outerself._bot.answerInlineQuery(query_id, ans)\n                    elif isinstance(ans, tuple):\n                        outerself._bot.answerInlineQuery(query_id, *ans)\n                    elif isinstance(ans, dict):\n                        outerself._bot.answerInlineQuery(query_id, **ans)\n                    else:\n                        raise ValueError('Invalid answer format')\n                finally:\n                    with outerself._lock:\n                        # Delete only if I have NOT been cancelled.\n                        if not innerself._cancelled:\n                            del outerself._workers[from_id]\n\n                        # If I have been cancelled, that position in `outerself._workers`\n                        # no longer belongs to me. I should not delete that key.\n\n        # Several threads may access `outerself._workers`. Use `outerself._lock` to protect.\n        with outerself._lock:\n            if from_id in outerself._workers:\n                outerself._workers[from_id].cancel()\n\n            outerself._workers[from_id] = Worker()\n            outerself._workers[from_id].start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nconfigures a : class :. Listener to capture callback query", "response": "def configure(self, listener):\n        \"\"\"\n        Configure a :class:`.Listener` to capture callback query\n        \"\"\"\n        listener.capture([\n            lambda msg: flavor(msg) == 'callback_query',\n            {'message': self._chat_origin_included}\n        ])\n\n        listener.capture([\n            lambda msg: flavor(msg) == 'callback_query',\n            {'inline_message_id': self._inline_origin_included}\n        ])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef augment_send(self, send_func):\n        def augmented(*aa, **kw):\n            sent = send_func(*aa, **kw)\n\n            if self._enable_chat and self._contains_callback_data(kw):\n                self.capture_origin(message_identifier(sent))\n\n            return sent\n        return augmented", "response": "Augment a send function to capture originating from the sent message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef augment_delete(self, delete_func):\n        def augmented(msg_identifier, *aa, **kw):\n            deleted = delete_func(msg_identifier, *aa, **kw)\n\n            if deleted is True:\n                self.uncapture_origin(msg_identifier)\n\n            return deleted\n        return augmented", "response": "Augment a delete function to stop capturing the message originating from that message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef augment_bot(self, bot):\n        # Because a plain object cannot be set attributes, we need a class.\n        class BotProxy(object):\n            pass\n\n        proxy = BotProxy()\n\n        send_methods = ['sendMessage',\n                        'forwardMessage',\n                        'sendPhoto',\n                        'sendAudio',\n                        'sendDocument',\n                        'sendSticker',\n                        'sendVideo',\n                        'sendVoice',\n                        'sendVideoNote',\n                        'sendLocation',\n                        'sendVenue',\n                        'sendContact',\n                        'sendGame',\n                        'sendInvoice',\n                        'sendChatAction',]\n\n        for method in send_methods:\n            setattr(proxy, method, self.augment_send(getattr(bot, method)))\n\n        edit_methods = ['editMessageText',\n                        'editMessageCaption',\n                        'editMessageReplyMarkup',]\n\n        for method in edit_methods:\n            setattr(proxy, method, self.augment_edit(getattr(bot, method)))\n\n        delete_methods = ['deleteMessage']\n\n        for method in delete_methods:\n            setattr(proxy, method, self.augment_delete(getattr(bot, method)))\n\n        def public_untouched(nv):\n            name, value = nv\n            return (not name.startswith('_')\n                    and name not in send_methods + edit_methods + delete_methods)\n\n        for name, value in filter(public_untouched, inspect.getmembers(bot)):\n            setattr(proxy, name, value)\n\n        return proxy", "response": "Augment bot with all the methods that are set in the bot."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh(self):\n        try:\n            if self._timeout_event:\n                self._scheduler.cancel(self._timeout_event)\n\n        # Timeout event has been popped from queue prematurely\n        except exception.EventNotFound:\n            pass\n\n        # Ensure a new event is scheduled always\n        finally:\n            self._timeout_event = self._scheduler.event_later(\n                                      self._timeout_seconds,\n                                      ('_idle', {'seconds': self._timeout_seconds}))", "response": "Refresh the timeout timer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\naugmenting a function to be called when a message is received from the queue.", "response": "def augment_on_message(self, handler):\n        \"\"\"\n        :return:\n            a function wrapping ``handler`` to refresh timer for every\n            non-event message\n        \"\"\"\n        def augmented(msg):\n            # Reset timer if this is an external message\n            is_event(msg) or self.refresh()\n\n            # Ignore timeout event that have been popped from queue prematurely\n            if flavor(msg) == '_idle' and msg is not self._timeout_event.data:\n                return\n\n            return handler(msg)\n        return augmented"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef augment_on_close(self, handler):\n        def augmented(ex):\n            try:\n                if self._timeout_event:\n                    self._scheduler.cancel(self._timeout_event)\n                    self._timeout_event = None\n            # This closing may have been caused by my own timeout, in which case\n            # the timeout event can no longer be found in the scheduler.\n            except exception.EventNotFound:\n                self._timeout_event = None\n            return handler(ex)\n        return augmented", "response": "Augment the handler to cancel the timeout event if it is found in the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef configure(self, listener):\n        listener.capture([{re.compile('^_.+'): {'source': {'space': self._event_space, 'id': self._source_id}}}])", "response": "Configure a : class :. Listener to capture events with this object s\n        event space and source id."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert the data into a standard event.", "response": "def make_event_data(self, flavor, data):\n        \"\"\"\n        Marshall ``flavor`` and ``data`` into a standard event.\n        \"\"\"\n        if not flavor.startswith('_'):\n            raise ValueError('Event flavor must start with _underscore')\n\n        d = {'source': {'space': self._event_space, 'id': self._source_id}}\n        d.update(data)\n        return {flavor: d}"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nschedule an event to be emitted at a certain time.", "response": "def event_at(self, when, data_tuple):\n        \"\"\"\n        Schedule an event to be emitted at a certain time.\n\n        :param when: an absolute timestamp\n        :param data_tuple: a 2-tuple (flavor, data)\n        :return: an event object, useful for cancelling.\n        \"\"\"\n        return self._base.event_at(when, self.make_event_data(*data_tuple))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef event_later(self, delay, data_tuple):\n        return self._base.event_later(delay, self.make_event_data(*data_tuple))", "response": "Schedule an event to be emitted after a delay."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef map(self, msg):\n        k = self.key_function(msg)\n        key = k[0] if isinstance(k, (tuple, list)) else k\n        return self.routing_table[key]", "response": "Apply key function to msg to obtain a key. Return the routing table entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef route(self, msg, *aa, **kw):\n        k = self.key_function(msg)\n\n        if isinstance(k, (tuple, list)):\n            key, args, kwargs = {1: tuple(k) + ((),{}),\n                                 2: tuple(k) + ({},),\n                                 3: tuple(k),}[len(k)]\n        else:\n            key, args, kwargs = k, (), {}\n\n        try:\n            fn = self.routing_table[key]\n        except KeyError as e:\n            # Check for default handler, key=None\n            if None in self.routing_table:\n                fn = self.routing_table[None]\n            else:\n                raise RuntimeError('No handler for key: %s, and default handler not defined' % str(e.args))\n\n        return fn(msg, *args, **kwargs)", "response": "Route a message to a specific handler function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef apply_entities_as_html(text, entities):\n    escapes = {'<': '&lt;',\n               '>': '&gt;',\n               '&': '&amp;',}\n\n    formatters = {'bold':         lambda s,e: '<b>'+s+'</b>',\n                  'italic':       lambda s,e: '<i>'+s+'</i>',\n                  'text_link':    lambda s,e: '<a href=\"'+e['url']+'\">'+s+'</a>',\n                  'text_mention': lambda s,e: '<a href=\"tg://user?id='+str(e['user']['id'])+'\">'+s+'</a>',\n                  'code':         lambda s,e: '<code>'+s+'</code>',\n                  'pre':          lambda s,e: '<pre>'+s+'</pre>'}\n\n    return _apply_entities(text, entities, escapes, formatters)", "response": "Formats text as HTML."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a photo to an existing Internet .", "response": "async def sendPhoto(self, chat_id, photo,\n                        caption=None,\n                        parse_mode=None,\n                        disable_notification=None,\n                        reply_to_message_id=None,\n                        reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#sendphoto\n\n        :param photo:\n            - string: ``file_id`` for a photo existing on Telegram servers\n            - string: HTTP URL of a photo from the Internet\n            - file-like object: obtained by ``open(path, 'rb')``\n            - tuple: (filename, file-like object). If the filename contains\n              non-ASCII characters and you are using Python 2.7, make sure the\n              filename is a unicode string.\n        \"\"\"\n        p = _strip(locals(), more=['photo'])\n        return await self._api_request_with_file('sendPhoto', _rectify(p), 'photo', photo)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def sendAudio(self, chat_id, audio,\n                        caption=None,\n                        parse_mode=None,\n                        duration=None,\n                        performer=None,\n                        title=None,\n                        disable_notification=None,\n                        reply_to_message_id=None,\n                        reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#sendaudio\n\n        :param audio: Same as ``photo`` in :meth:`telepot.aio.Bot.sendPhoto`\n        \"\"\"\n        p = _strip(locals(), more=['audio'])\n        return await self._api_request_with_file('sendAudio', _rectify(p), 'audio', audio)", "response": "Send an audio file to the chat."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def editMessageLiveLocation(self, msg_identifier, latitude, longitude,\n                                      reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#editmessagelivelocation\n\n        :param msg_identifier: Same as in :meth:`.Bot.editMessageText`\n        \"\"\"\n        p = _strip(locals(), more=['msg_identifier'])\n        p.update(_dismantle_message_identifier(msg_identifier))\n        return await self._api_request('editMessageLiveLocation', _rectify(p))", "response": "Edit the live location of a message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget a file from the server", "response": "async def getFile(self, file_id):\n        \"\"\" See: https://core.telegram.org/bots/api#getfile \"\"\"\n        p = _strip(locals())\n        return await self._api_request('getFile', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\nasync def kickChatMember(self, chat_id, user_id,\n                             until_date=None):\n        \"\"\" See: https://core.telegram.org/bots/api#kickchatmember \"\"\"\n        p = _strip(locals())\n        return await self._api_request('kickChatMember', _rectify(p))", "response": "Kick a chat member."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def restrictChatMember(self, chat_id, user_id,\n                                 until_date=None,\n                                 can_send_messages=None,\n                                 can_send_media_messages=None,\n                                 can_send_other_messages=None,\n                                 can_add_web_page_previews=None):\n        \"\"\" See: https://core.telegram.org/bots/api#restrictchatmember \"\"\"\n        p = _strip(locals())\n        return await self._api_request('restrictChatMember', _rectify(p))", "response": "Use this method to restrict a user to a chat."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the chat s description.", "response": "async def setChatDescription(self, chat_id,\n                                 description=None):\n        \"\"\" See: https://core.telegram.org/bots/api#setchatdescription \"\"\"\n        p = _strip(locals())\n        return await self._api_request('setChatDescription', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def pinChatMessage(self, chat_id, message_id,\n                             disable_notification=None):\n        \"\"\" See: https://core.telegram.org/bots/api#pinchatmessage \"\"\"\n        p = _strip(locals())\n        return await self._api_request('pinChatMessage', _rectify(p))", "response": "Pin a message from chat to chat_id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def unpinChatMessage(self, chat_id):\n        p = _strip(locals())\n        return await self._api_request('unpinChatMessage', _rectify(p))", "response": "Unpin a chat message."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def getChatMember(self, chat_id, user_id):\n        p = _strip(locals())\n        return await self._api_request('getChatMember', _rectify(p))", "response": "Get a specific chat member"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse this method to change the chat s sticker set.", "response": "async def setChatStickerSet(self, chat_id, sticker_set_name):\n        \"\"\" See: https://core.telegram.org/bots/api#setchatstickerset \"\"\"\n        p = _strip(locals())\n        return await self._api_request('setChatStickerSet', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete a chat sticker set.", "response": "async def deleteChatStickerSet(self, chat_id):\n        \"\"\" See: https://core.telegram.org/bots/api#deletechatstickerset \"\"\"\n        p = _strip(locals())\n        return await self._api_request('deleteChatStickerSet', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsend an answer to a callback query.", "response": "async def answerCallbackQuery(self, callback_query_id,\n                                  text=None,\n                                  show_alert=None,\n                                  url=None,\n                                  cache_time=None):\n        \"\"\" See: https://core.telegram.org/bots/api#answercallbackquery \"\"\"\n        p = _strip(locals())\n        return await self._api_request('answerCallbackQuery', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes a message from the Telegram.", "response": "async def deleteMessage(self, msg_identifier):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#deletemessage\n\n        :param msg_identifier:\n            Same as ``msg_identifier`` in :meth:`telepot.aio.Bot.editMessageText`,\n            except this method does not work on inline messages.\n        \"\"\"\n        p = _strip(locals(), more=['msg_identifier'])\n        p.update(_dismantle_message_identifier(msg_identifier))\n        return await self._api_request('deleteMessage', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a sticker to a chat.", "response": "async def sendSticker(self, chat_id, sticker,\n                          disable_notification=None,\n                          reply_to_message_id=None,\n                          reply_markup=None):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#sendsticker\n\n        :param sticker: Same as ``photo`` in :meth:`telepot.aio.Bot.sendPhoto`\n        \"\"\"\n        p = _strip(locals(), more=['sticker'])\n        return await self._api_request_with_file('sendSticker', _rectify(p), 'sticker', sticker)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def getStickerSet(self, name):\n        p = _strip(locals())\n        return await self._api_request('getStickerSet', _rectify(p))", "response": "Get a specific sticker set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a sticker from a set.", "response": "async def deleteStickerFromSet(self, sticker):\n        \"\"\"\n        See: https://core.telegram.org/bots/api#deletestickerfromset\n        \"\"\"\n        p = _strip(locals())\n        return await self._api_request('deleteStickerFromSet', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending an answer to an inline query.", "response": "async def answerInlineQuery(self, inline_query_id, results,\n                                cache_time=None,\n                                is_personal=None,\n                                next_offset=None,\n                                switch_pm_text=None,\n                                switch_pm_parameter=None):\n        \"\"\" See: https://core.telegram.org/bots/api#answerinlinequery \"\"\"\n        p = _strip(locals())\n        return await self._api_request('answerInlineQuery', _rectify(p))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def getUpdates(self,\n                         offset=None,\n                         limit=None,\n                         timeout=None,\n                         allowed_updates=None):\n        \"\"\" See: https://core.telegram.org/bots/api#getupdates \"\"\"\n        p = _strip(locals())\n        return await self._api_request('getUpdates', _rectify(p))", "response": "Get updates for a specific locale."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def setWebhook(self,\n                         url=None,\n                         certificate=None,\n                         max_connections=None,\n                         allowed_updates=None):\n        \"\"\" See: https://core.telegram.org/bots/api#setwebhook \"\"\"\n        p = _strip(locals(), more=['certificate'])\n\n        if certificate:\n            files = {'certificate': certificate}\n            return await self._api_request('setWebhook', _rectify(p), files)\n        else:\n            return await self._api_request('setWebhook', _rectify(p))", "response": "Set the webhook for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndownloads a file to local disk.", "response": "async def download_file(self, file_id, dest):\n        \"\"\"\n        Download a file to local disk.\n\n        :param dest: a path or a ``file`` object\n        \"\"\"\n        f = await self.getFile(file_id)\n\n        try:\n            d = dest if isinstance(dest, io.IOBase) else open(dest, 'wb')\n\n            session, request = api.download((self._token, f['file_path']))\n\n            async with session:\n                async with request as r:\n                    while 1:\n                        chunk = await r.content.read(self._file_chunk_size)\n                        if not chunk:\n                            break\n                        d.write(chunk)\n                        d.flush()\n        finally:\n            if not isinstance(dest, io.IOBase) and 'd' in locals():\n                d.close()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a task to constantly ``getUpdates`` or pull updates from a queue. Apply ``handler`` to every message received. :param handler: a function that takes one argument (the message), or a routing table. If ``None``, the bot's ``handle`` method is used. A *routing table* is a dictionary of ``{flavor: function}``, mapping messages to appropriate handler functions according to their flavors. It allows you to define functions specifically to handle one flavor of messages. It usually looks like this: ``{'chat': fn1, 'callback_query': fn2, 'inline_query': fn3, ...}``. Each handler function should take one argument (the message). :param source: Source of updates. If ``None``, ``getUpdates`` is used to obtain new messages from Telegram servers. If it is a ``asyncio.Queue``, new messages are pulled from the queue. A web application implementing a webhook can dump updates into the queue, while the bot pulls from it. This is how telepot can be integrated with webhooks. Acceptable contents in queue: - ``str`` or ``bytes`` (decoded using UTF-8) representing a JSON-serialized `Update <https://core.telegram.org/bots/api#update>`_ object. - a ``dict`` representing an Update object. When ``source`` is a queue, these parameters are meaningful: :type ordered: bool :param ordered: If ``True``, ensure in-order delivery of messages to ``handler`` (i.e. updates with a smaller ``update_id`` always come before those with a larger ``update_id``). If ``False``, no re-ordering is done. ``handler`` is applied to messages as soon as they are pulled from queue. :type maxhold: float :param maxhold: Applied only when ``ordered`` is ``True``. The maximum number of seconds an update is held waiting for a not-yet-arrived smaller ``update_id``. When this number of seconds is up, the update is delivered to ``handler`` even if some smaller ``update_id``\\s have not yet arrived. If those smaller ``update_id``\\s arrive at some later time, they are discarded. :type timeout: int :param timeout: ``timeout`` parameter supplied to :meth:`telepot.aio.Bot.getUpdates`, controlling how long to poll in seconds. :type allowed_updates: array of string :param allowed_updates: ``allowed_updates`` parameter supplied to :meth:`telepot.aio.Bot.getUpdates`, controlling which types of updates to receive.", "response": "async def message_loop(self, handler=None, relax=0.1,\n                           timeout=20, allowed_updates=None,\n                           source=None, ordered=True, maxhold=3):\n        \"\"\"\n        Return a task to constantly ``getUpdates`` or pull updates from a queue.\n        Apply ``handler`` to every message received.\n\n        :param handler:\n            a function that takes one argument (the message), or a routing table.\n            If ``None``, the bot's ``handle`` method is used.\n\n        A *routing table* is a dictionary of ``{flavor: function}``, mapping messages to appropriate\n        handler functions according to their flavors. It allows you to define functions specifically\n        to handle one flavor of messages. It usually looks like this: ``{'chat': fn1,\n        'callback_query': fn2, 'inline_query': fn3, ...}``. Each handler function should take\n        one argument (the message).\n\n        :param source:\n            Source of updates.\n            If ``None``, ``getUpdates`` is used to obtain new messages from Telegram servers.\n            If it is a ``asyncio.Queue``, new messages are pulled from the queue.\n            A web application implementing a webhook can dump updates into the queue,\n            while the bot pulls from it. This is how telepot can be integrated with webhooks.\n\n        Acceptable contents in queue:\n\n        - ``str`` or ``bytes`` (decoded using UTF-8)\n          representing a JSON-serialized `Update <https://core.telegram.org/bots/api#update>`_ object.\n        - a ``dict`` representing an Update object.\n\n        When ``source`` is a queue, these parameters are meaningful:\n\n        :type ordered: bool\n        :param ordered:\n            If ``True``, ensure in-order delivery of messages to ``handler``\n            (i.e. updates with a smaller ``update_id`` always come before those with\n            a larger ``update_id``).\n            If ``False``, no re-ordering is done. ``handler`` is applied to messages\n            as soon as they are pulled from queue.\n\n        :type maxhold: float\n        :param maxhold:\n            Applied only when ``ordered`` is ``True``. The maximum number of seconds\n            an update is held waiting for a not-yet-arrived smaller ``update_id``.\n            When this number of seconds is up, the update is delivered to ``handler``\n            even if some smaller ``update_id``\\s have not yet arrived. If those smaller\n            ``update_id``\\s arrive at some later time, they are discarded.\n\n        :type timeout: int\n        :param timeout:\n            ``timeout`` parameter supplied to :meth:`telepot.aio.Bot.getUpdates`,\n            controlling how long to poll in seconds.\n\n        :type allowed_updates: array of string\n        :param allowed_updates:\n            ``allowed_updates`` parameter supplied to :meth:`telepot.aio.Bot.getUpdates`,\n            controlling which types of updates to receive.\n        \"\"\"\n        if handler is None:\n            handler = self.handle\n        elif isinstance(handler, dict):\n            handler = flavor_router(handler)\n\n        def create_task_for(msg):\n            self.loop.create_task(handler(msg))\n\n        if asyncio.iscoroutinefunction(handler):\n            callback = create_task_for\n        else:\n            callback = handler\n\n        def handle(update):\n            try:\n                key = _find_first_key(update, ['message',\n                                               'edited_message',\n                                               'channel_post',\n                                               'edited_channel_post',\n                                               'callback_query',\n                                               'inline_query',\n                                               'chosen_inline_result',\n                                               'shipping_query',\n                                               'pre_checkout_query'])\n\n                callback(update[key])\n            except:\n                # Localize the error so message thread can keep going.\n                traceback.print_exc()\n            finally:\n                return update['update_id']\n\n        async def get_from_telegram_server():\n            offset = None  # running offset\n            allowed_upd = allowed_updates\n            while 1:\n                try:\n                    result = await self.getUpdates(offset=offset,\n                                                   timeout=timeout,\n                                                   allowed_updates=allowed_upd)\n\n                    # Once passed, this parameter is no longer needed.\n                    allowed_upd = None\n\n                    if len(result) > 0:\n                        # No sort. Trust server to give messages in correct order.\n                        # Update offset to max(update_id) + 1\n                        offset = max([handle(update) for update in result]) + 1\n                except CancelledError:\n                    raise\n                except exception.BadHTTPResponse as e:\n                    traceback.print_exc()\n\n                    # Servers probably down. Wait longer.\n                    if e.status == 502:\n                        await asyncio.sleep(30)\n                except:\n                    traceback.print_exc()\n                    await asyncio.sleep(relax)\n                else:\n                    await asyncio.sleep(relax)\n\n        def dictify(data):\n            if type(data) is bytes:\n                return json.loads(data.decode('utf-8'))\n            elif type(data) is str:\n                return json.loads(data)\n            elif type(data) is dict:\n                return data\n            else:\n                raise ValueError()\n\n        async def get_from_queue_unordered(qu):\n            while 1:\n                try:\n                    data = await qu.get()\n                    update = dictify(data)\n                    handle(update)\n                except:\n                    traceback.print_exc()\n\n        async def get_from_queue(qu):\n            # Here is the re-ordering mechanism, ensuring in-order delivery of updates.\n            max_id = None                 # max update_id passed to callback\n            buffer = collections.deque()  # keep those updates which skip some update_id\n            qwait = None                  # how long to wait for updates,\n                                          # because buffer's content has to be returned in time.\n\n            while 1:\n                try:\n                    data = await asyncio.wait_for(qu.get(), qwait)\n                    update = dictify(data)\n\n                    if max_id is None:\n                        # First message received, handle regardless.\n                        max_id = handle(update)\n\n                    elif update['update_id'] == max_id + 1:\n                        # No update_id skipped, handle naturally.\n                        max_id = handle(update)\n\n                        # clear contagious updates in buffer\n                        if len(buffer) > 0:\n                            buffer.popleft()  # first element belongs to update just received, useless now.\n                            while 1:\n                                try:\n                                    if type(buffer[0]) is dict:\n                                        max_id = handle(buffer.popleft())  # updates that arrived earlier, handle them.\n                                    else:\n                                        break  # gap, no more contagious updates\n                                except IndexError:\n                                    break  # buffer empty\n\n                    elif update['update_id'] > max_id + 1:\n                        # Update arrives pre-maturely, insert to buffer.\n                        nbuf = len(buffer)\n                        if update['update_id'] <= max_id + nbuf:\n                            # buffer long enough, put update at position\n                            buffer[update['update_id'] - max_id - 1] = update\n                        else:\n                            # buffer too short, lengthen it\n                            expire = time.time() + maxhold\n                            for a in range(nbuf, update['update_id']-max_id-1):\n                                buffer.append(expire)  # put expiry time in gaps\n                            buffer.append(update)\n\n                    else:\n                        pass  # discard\n\n                except asyncio.TimeoutError:\n                    # debug message\n                    # print('Timeout')\n\n                    # some buffer contents have to be handled\n                    # flush buffer until a non-expired time is encountered\n                    while 1:\n                        try:\n                            if type(buffer[0]) is dict:\n                                max_id = handle(buffer.popleft())\n                            else:\n                                expire = buffer[0]\n                                if expire <= time.time():\n                                    max_id += 1\n                                    buffer.popleft()\n                                else:\n                                    break  # non-expired\n                        except IndexError:\n                            break  # buffer empty\n                except:\n                    traceback.print_exc()\n                finally:\n                    try:\n                        # don't wait longer than next expiry time\n                        qwait = buffer[0] - time.time()\n                        if qwait < 0:\n                            qwait = 0\n                    except IndexError:\n                        # buffer empty, can wait forever\n                        qwait = None\n\n                    # debug message\n                    # print ('Buffer:', str(buffer), ', To Wait:', qwait, ', Max ID:', max_id)\n\n        self._scheduler._callback = callback\n\n        if source is None:\n            await get_from_telegram_server()\n        elif isinstance(source, asyncio.Queue):\n            if ordered:\n                await get_from_queue(source)\n            else:\n                await get_from_queue_unordered(source)\n        else:\n            raise ValueError('Invalid source')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def grab(self, *, countries=None, limit=0):\n        self._countries = countries\n        self._limit = limit\n        task = asyncio.ensure_future(self._grab(check=False))\n        self._all_tasks.append(task)", "response": "Gather proxies from the providers without checking."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def find(\n        self,\n        *,\n        types=None,\n        data=None,\n        countries=None,\n        post=False,\n        strict=False,\n        dnsbl=None,\n        limit=0,\n        **kwargs\n    ):\n        \"\"\"Gather and check proxies from providers or from a passed data.\n\n        :ref:`Example of usage <proxybroker-examples-find>`.\n\n        :param list types:\n            Types (protocols) that need to be check on support by proxy.\n            Supported: HTTP, HTTPS, SOCKS4, SOCKS5, CONNECT:80, CONNECT:25\n            And levels of anonymity (HTTP only): Transparent, Anonymous, High\n        :param data:\n            (optional) String or list with proxies. Also can be a file-like\n            object supports `read()` method. Used instead of providers\n        :param list countries:\n            (optional) List of ISO country codes where should be located\n            proxies\n        :param bool post:\n            (optional) Flag indicating use POST instead of GET for requests\n            when checking proxies\n        :param bool strict:\n            (optional) Flag indicating that anonymity levels of types\n            (protocols) supported by a proxy must be equal to the requested\n            types and levels of anonymity. By default, strict mode is off and\n            for a successful check is enough to satisfy any one of the\n            requested types\n        :param list dnsbl:\n            (optional) Spam databases for proxy checking.\n            `Wiki <https://en.wikipedia.org/wiki/DNSBL>`_\n        :param int limit: (optional) The maximum number of proxies\n\n        :raises ValueError:\n            If :attr:`types` not given.\n\n        .. versionchanged:: 0.2.0\n            Added: :attr:`post`, :attr:`strict`, :attr:`dnsbl`.\n            Changed: :attr:`types` is required.\n        \"\"\"\n        ip = await self._resolver.get_real_ext_ip()\n        types = _update_types(types)\n\n        if not types:\n            raise ValueError('`types` is required')\n\n        self._checker = Checker(\n            judges=self._judges,\n            timeout=self._timeout,\n            verify_ssl=self._verify_ssl,\n            max_tries=self._max_tries,\n            real_ext_ip=ip,\n            types=types,\n            post=post,\n            strict=strict,\n            dnsbl=dnsbl,\n            loop=self._loop,\n        )\n        self._countries = countries\n        self._limit = limit\n\n        tasks = [asyncio.ensure_future(self._checker.check_judges())]\n        if data:\n            task = asyncio.ensure_future(self._load(data, check=True))\n        else:\n            task = asyncio.ensure_future(self._grab(types, check=True))\n        tasks.append(task)\n        self._all_tasks.extend(tasks)", "response": "Gather and check proxies from providers or from a passed data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serve(self, host='127.0.0.1', port=8888, limit=100, **kwargs):\n\n        if limit <= 0:\n            raise ValueError(\n                'In serve mode value of the limit cannot be less than or '\n                'equal to zero. Otherwise, a parsing of providers will be '\n                'endless'\n            )\n\n        self._server = Server(\n            host=host,\n            port=port,\n            proxies=self._proxies,\n            timeout=self._timeout,\n            max_tries=kwargs.pop('max_tries', self._max_tries),\n            loop=self._loop,\n            **kwargs\n        )\n        self._server.start()\n\n        task = asyncio.ensure_future(self.find(limit=limit, **kwargs))\n        self._all_tasks.append(task)", "response": "Start a local proxy server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def _load(self, data, check=True):\n        log.debug('Load proxies from the raw data')\n        if isinstance(data, io.TextIOWrapper):\n            data = data.read()\n        if isinstance(data, str):\n            data = IPPortPatternLine.findall(data)\n        proxies = set(data)\n        for proxy in proxies:\n            await self._handle(proxy, check=check)\n        await self._on_check.join()\n        self._done()", "response": "Load proxies from the passed data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop(self):\n        self._done()\n        if self._server:\n            self._server.stop()\n            self._server = None\n        log.info('Stop!')", "response": "Stop all tasks and the local proxy server if it s running."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show_stats(self, verbose=False, **kwargs):\n        if kwargs:\n            verbose = True\n            warnings.warn(\n                '`full` in `show_stats` is deprecated, '\n                'use `verbose` instead.',\n                DeprecationWarning,\n            )\n\n        found_proxies = self.unique_proxies.values()\n        num_working_proxies = len([p for p in found_proxies if p.is_working])\n\n        if not found_proxies:\n            print('Proxy not found')\n            return\n\n        errors = Counter()\n        for p in found_proxies:\n            errors.update(p.stat['errors'])\n\n        proxies_by_type = {\n            'SOCKS5': [],\n            'SOCKS4': [],\n            'HTTPS': [],\n            'HTTP': [],\n            'CONNECT:80': [],\n            'CONNECT:25': [],\n        }\n\n        stat = {\n            'Wrong country': [],\n            'Wrong protocol/anonymity lvl': [],\n            'Connection success': [],\n            'Connection timeout': [],\n            'Connection failed': [],\n        }\n\n        for p in found_proxies:\n            msgs = ' '.join([l[1] for l in p.get_log()])\n            full_log = [p]\n            for proto in p.types:\n                proxies_by_type[proto].append(p)\n            if 'Location of proxy' in msgs:\n                stat['Wrong country'].append(p)\n            elif 'Connection: success' in msgs:\n                if 'Protocol or the level' in msgs:\n                    stat['Wrong protocol/anonymity lvl'].append(p)\n                stat['Connection success'].append(p)\n                if not verbose:\n                    continue\n                events_by_ngtr = defaultdict(list)\n                for ngtr, event, runtime in p.get_log():\n                    events_by_ngtr[ngtr].append((event, runtime))\n                for ngtr, events in sorted(\n                    events_by_ngtr.items(), key=lambda item: item[0]\n                ):\n                    full_log.append('\\t%s' % ngtr)\n                    for event, runtime in events:\n                        if event.startswith('Initial connection'):\n                            full_log.append('\\t\\t-------------------')\n                        else:\n                            full_log.append(\n                                '\\t\\t{:<66} Runtime: {:.2f}'.format(\n                                    event, runtime\n                                )\n                            )\n                for row in full_log:\n                    print(row)\n            elif 'Connection: failed' in msgs:\n                stat['Connection failed'].append(p)\n            else:\n                stat['Connection timeout'].append(p)\n        if verbose:\n            print('Stats:')\n            pprint(stat)\n\n        print('The number of working proxies: %d' % num_working_proxies)\n        for proto, proxies in proxies_by_type.items():\n            print('%s (%s): %s' % (proto, len(proxies), proxies))\n        print('Errors:', errors)", "response": "Show statistics on the found proxies."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsave proxies to a file.", "response": "async def save(proxies, filename):\n    \"\"\"Save proxies to a file.\"\"\"\n    with open(filename, 'w') as f:\n        while True:\n            proxy = await proxies.get()\n            if proxy is None:\n                break\n            f.write('%s:%d\\n' % (proxy.host, proxy.port))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn geo information about IP address.", "response": "def get_ip_info(ip):\n        \"\"\"Return geo information about IP address.\n\n        `code` - ISO country code\n        `name` - Full name of country\n        `region_code` - ISO region code\n        `region_name` - Full name of region\n        `city_name` - Full name of city\n        \"\"\"\n        # from pprint import pprint\n        try:\n            ipInfo = _mmdb_reader.get(ip) or {}\n        except (maxminddb.errors.InvalidDatabaseError, ValueError):\n            ipInfo = {}\n\n        code, name = '--', 'Unknown'\n        city_name, region_code, region_name = ('Unknown',) * 3\n        if 'country' in ipInfo:\n            code = ipInfo['country']['iso_code']\n            name = ipInfo['country']['names']['en']\n        elif 'continent' in ipInfo:\n            code = ipInfo['continent']['code']\n            name = ipInfo['continent']['names']['en']\n        if 'city' in ipInfo:\n            city_name = ipInfo['city']['names']['en']\n        if 'subdivisions' in ipInfo:\n            region_code = ipInfo['subdivisions'][0]['iso_code']\n            region_name = ipInfo['subdivisions'][0]['names']['en']\n        return GeoData(code, name, region_code, region_name, city_name)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def get_real_ext_ip(self):\n        while self._ip_hosts:\n            try:\n                timeout = aiohttp.ClientTimeout(total=self._timeout)\n                async with aiohttp.ClientSession(\n                    timeout=timeout, loop=self._loop\n                ) as session, session.get(self._pop_random_ip_host()) as resp:\n                    ip = await resp.text()\n            except asyncio.TimeoutError:\n                pass\n            else:\n                ip = ip.strip()\n                if self.host_is_ip(ip):\n                    log.debug('Real external IP: %s', ip)\n                    break\n        else:\n            raise RuntimeError('Could not get the external IP')\n        return ip", "response": "Get real external IP address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def resolve(\n        self, host, port=80, family=None, qtype='A', logging=True\n    ):\n        \"\"\"Return resolving IP address(es) from host name.\"\"\"\n        if self.host_is_ip(host):\n            return host\n\n        _host = self._cached_hosts.get(host)\n        if _host:\n            return _host\n\n        resp = await self._resolve(host, qtype)\n\n        if resp:\n            hosts = [\n                {\n                    'hostname': host,\n                    'host': r.host,\n                    'port': port,\n                    'family': family,\n                    'proto': socket.IPPROTO_IP,\n                    'flags': socket.AI_NUMERICHOST,\n                }\n                for r in resp\n            ]\n            if family:\n                self._cached_hosts[host] = hosts\n            else:\n                self._cached_hosts[host] = hosts[0]['host']\n            if logging:\n                log.debug(\n                    '%s: Host resolved: %s' % (host, self._cached_hosts[host])\n                )\n        else:\n            if logging:\n                log.warning('%s: Could not resolve host' % host)\n        return self._cached_hosts.get(host)", "response": "Return resolving IP address from host name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def create(cls, host, *args, **kwargs):\n        loop = kwargs.pop('loop', None)\n        resolver = kwargs.pop('resolver', Resolver(loop=loop))\n        try:\n            _host = await resolver.resolve(host)\n            self = cls(_host, *args, **kwargs)\n        except (ResolveError, ValueError) as e:\n            log.error('%s:%s: Error at creating: %s' % (host, args[0], e))\n            raise\n        return self", "response": "Asynchronously create a proxy object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef error_rate(self):\n        if not self.stat['requests']:\n            return 0\n        return round(\n            sum(self.stat['errors'].values()) / self.stat['requests'], 2\n        )", "response": "Return the error rate of the current user."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the proxy s properties in JSON format.", "response": "def as_json(self):\n        \"\"\"Return the proxy's properties in JSON format.\n\n        :rtype: dict\n        \"\"\"\n        info = {\n            'host': self.host,\n            'port': self.port,\n            'geo': {\n                'country': {'code': self._geo.code, 'name': self._geo.name},\n                'region': {\n                    'code': self._geo.region_code,\n                    'name': self._geo.region_name,\n                },\n                'city': self._geo.city_name,\n            },\n            'types': [],\n            'avg_resp_time': self.avg_resp_time,\n            'error_rate': self.error_rate,\n        }\n\n        order = lambda tp_lvl: (len(tp_lvl[0]), tp_lvl[0][-1])  # noqa: 731\n        for tp, lvl in sorted(self.types.items(), key=order):\n            info['types'].append({'type': tp, 'level': lvl or ''})\n        return info"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def get_proxies(self):\n        log.debug('Try to get proxies from %s' % self.domain)\n\n        async with aiohttp.ClientSession(\n            headers=get_headers(), cookies=self._cookies, loop=self._loop\n        ) as self._session:\n            await self._pipe()\n\n        log.debug(\n            '%d proxies received from %s: %s'\n            % (len(self.proxies), self.domain, self.proxies)\n        )\n        return self.proxies", "response": "Receive proxies from the provider and return them."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def save(proxies, filename):\n    with open(filename, 'w') as f:\n        while True:\n            proxy = await proxies.get()\n            if proxy is None:\n                break\n            proto = 'https' if 'HTTPS' in proxy.types else 'http'\n            row = '%s://%s:%d\\n' % (proto, proxy.host, proxy.port)\n            f.write(row)", "response": "Save proxies to a file."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, **kwargs):\n        # Update inherited configurations\n        super(CommonConf, self).update(**kwargs)\n        conf_changed = False\n\n        # Validate given configurations and check if value changed\n        for conf_name, conf_value in kwargs.items():\n            rtconf.base.get_validator(conf_name)(conf_value)\n            item1 = self._settings.get(conf_name, None)\n            item2 = kwargs.get(conf_name, None)\n\n            if item1 != item2:\n                conf_changed = True\n\n        # If any configuration changed, we update configuration value and\n        # notify listeners\n        if conf_changed:\n            for conf_name, conf_value in kwargs.items():\n                # Since all new values are already validated, we can use them\n                self._settings[conf_name] = conf_value\n\n            self._notify_listeners(CommonConf.CONF_CHANGED_EVT, self)", "response": "Updates the global configuration settings with given values."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert binary representation label to integer.", "response": "def label_from_bin(buf):\n    \"\"\"\n    Converts binary representation label to integer.\n\n    :param buf: Binary representation of label.\n    :return: MPLS Label and BoS bit.\n    \"\"\"\n\n    mpls_label = type_desc.Int3.to_user(six.binary_type(buf))\n    return mpls_label >> 4, mpls_label & 1"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nremoves VRF table associated with given vrf_conf.", "response": "def on_remove_vrf_conf(self, evt):\n        \"\"\"Removes VRF table associated with given `vrf_conf`.\n\n        Cleans up other links to this table as well.\n        \"\"\"\n        vrf_conf = evt.value\n        # Detach VrfConf change listener.\n        vrf_conf.remove_listener(VrfConf.VRF_CHG_EVT, self.on_chg_vrf_conf)\n\n        self._table_manager.remove_vrf_by_vrf_conf(vrf_conf)\n\n        # Update local RT NLRIs\n        self._rt_manager.update_local_rt_nlris()\n\n        self._signal_bus.vrf_removed(vrf_conf.route_dist)\n\n        # Remove AttributeMaps under the removed vrf\n        rd = vrf_conf.route_dist\n        rf = vrf_conf.route_family\n        peers = self._peer_manager.iterpeers\n        for peer in peers:\n            key = ':'.join([rd, rf])\n            peer.attribute_maps.pop(key, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_zebra_family_prefix(buf):\n    (family,) = struct.unpack_from(_ZEBRA_FAMILY_FMT, buf)\n    rest = buf[_ZEBRA_FAMILY_SIZE:]\n\n    if socket.AF_INET == family:\n        (prefix, p_len) = struct.unpack_from(_ZEBRA_IPV4_PREFIX_FMT, rest)\n        prefix = '%s/%d' % (addrconv.ipv4.bin_to_text(prefix), p_len)\n        rest = rest[_ZEBRA_IPV4_PREFIX_SIZE:]\n    elif socket.AF_INET6 == family:\n        (prefix, p_len) = struct.unpack_from(_ZEBRA_IPV6_PREFIX_FMT, rest)\n        prefix = '%s/%d' % (addrconv.ipv6.bin_to_text(prefix), p_len)\n        rest = rest[_ZEBRA_IPV6_PREFIX_SIZE:]\n    else:\n        raise struct.error('Unsupported family: %d' % family)\n\n    return family, prefix, rest", "response": "Parses family and prefix in Zebra format."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _serialize_zebra_family_prefix(prefix):\n    if ip.valid_ipv4(prefix):\n        family = socket.AF_INET  # fixup\n        prefix_addr, prefix_num = prefix.split('/')\n        return family, struct.pack(\n            _ZEBRA_FAMILY_IPV4_PREFIX_FMT,\n            family,\n            addrconv.ipv4.text_to_bin(prefix_addr),\n            int(prefix_num))\n    elif ip.valid_ipv6(prefix):\n        family = socket.AF_INET6  # fixup\n        prefix_addr, prefix_num = prefix.split('/')\n        return family, struct.pack(\n            _ZEBRA_FAMILY_IPV6_PREFIX_FMT,\n            family,\n            addrconv.ipv6.text_to_bin(prefix_addr),\n            int(prefix_num))\n\n    raise ValueError('Invalid prefix: %s' % prefix)", "response": "Serializes family and prefix in Zebra format."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if the given MAC address is valid.", "response": "def is_valid_mac(mac):\n    \"\"\"Returns True if the given MAC address is valid.\n\n    The given MAC address should be a colon hexadecimal notation string.\n\n    Samples:\n        - valid address: aa:bb:cc:dd:ee:ff, 11:22:33:44:55:66\n        - invalid address: aa:bb:cc:dd, 11-22-33-44-55-66, etc.\n    \"\"\"\n    return bool(re.match(r'^' + r'[\\:\\-]'.join([r'([0-9a-f]{2})'] * 6)\n                         + r'$', mac.lower()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if prefix is a valid IPv4 or IPv6 address prefix.", "response": "def is_valid_ip_prefix(prefix, bits):\n    \"\"\"Returns True if *prefix* is a valid IPv4 or IPv6 address prefix.\n\n    *prefix* should be a number between 0 to *bits* length.\n    \"\"\"\n    try:\n        # Prefix should be a number\n        prefix = int(prefix)\n    except ValueError:\n        return False\n\n    # Prefix should be a number between 0 to *bits*\n    return 0 <= prefix <= bits"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if the given ipv4_prefix is a valid prefix with mask.", "response": "def is_valid_ipv4_prefix(ipv4_prefix):\n    \"\"\"Returns True if *ipv4_prefix* is a valid prefix with mask.\n\n    Samples:\n        - valid prefix: 1.1.1.0/32, 244.244.244.1/10\n        - invalid prefix: 255.2.2.2/2, 2.2.2/22, etc.\n    \"\"\"\n    if not isinstance(ipv4_prefix, str):\n        return False\n\n    tokens = ipv4_prefix.split('/')\n    if len(tokens) != 2:\n        return False\n\n    # Validate address/mask and return\n    return is_valid_ipv4(tokens[0]) and is_valid_ip_prefix(tokens[1], 32)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True if given ipv6_prefix is a valid IPv6 prefix.", "response": "def is_valid_ipv6_prefix(ipv6_prefix):\n    \"\"\"Returns True if given `ipv6_prefix` is a valid IPv6 prefix.\"\"\"\n\n    # Validate input type\n    if not isinstance(ipv6_prefix, str):\n        return False\n\n    tokens = ipv6_prefix.split('/')\n    if len(tokens) != 2:\n        return False\n\n    # Validate address/mask and return\n    return is_valid_ipv6(tokens[0]) and is_valid_ip_prefix(tokens[1], 128)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns True if given prefix is a string represent vpnv4 prefix.", "response": "def is_valid_vpnv4_prefix(prefix):\n    \"\"\"Returns True if given prefix is a string represent vpnv4 prefix.\n\n    Vpnv4 prefix is made up of RD:Ipv4, where RD is represents route\n    distinguisher and Ipv4 represents valid dot-decimal ipv4 notation string.\n    \"\"\"\n    if not isinstance(prefix, str):\n        return False\n\n    # Split the prefix into route distinguisher and IP\n    tokens = prefix.split(':', 2)\n    if len(tokens) != 3:\n        return False\n\n    # Validate route distinguisher\n    if not is_valid_route_dist(':'.join([tokens[0], tokens[1]])):\n        return False\n\n    # Validate IPv4 prefix and return\n    return is_valid_ipv4_prefix(tokens[2])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef is_valid_vpnv6_prefix(prefix):\n    if not isinstance(prefix, str):\n        return False\n\n    # Split the prefix into route distinguisher and IP\n    tokens = prefix.split(':', 2)\n    if len(tokens) != 3:\n        return False\n\n    # Validate route distinguisher\n    if not is_valid_route_dist(':'.join([tokens[0], tokens[1]])):\n        return False\n\n    # Validate IPv6 prefix and return\n    return is_valid_ipv6_prefix(tokens[2])", "response": "Returns True if given prefix is a string represent vpnv6 prefix."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nvalidate the given label according to MPLS label rules RFC says : This 20 - bit field is reserved.", "response": "def is_valid_mpls_label(label):\n    \"\"\"Validates `label` according to MPLS label rules\n\n    RFC says:\n    This 20-bit field.\n    A value of 0 represents the \"IPv4 Explicit NULL Label\".\n    A value of 1 represents the \"Router Alert Label\".\n    A value of 2 represents the \"IPv6 Explicit NULL Label\".\n    A value of 3 represents the \"Implicit NULL Label\".\n    Values 4-15 are reserved.\n    \"\"\"\n    if (not isinstance(label, numbers.Integral) or\n            (4 <= label <= 15) or\n            (label < 0 or label > 2 ** 20)):\n        return False\n\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_valid_mpls_labels(labels):\n    if not isinstance(labels, (list, tuple)):\n        return False\n\n    for label in labels:\n        if not is_valid_mpls_label(label):\n            return False\n\n    return True", "response": "Returns True if the given value is a list of valid MPLS labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_valid_ext_comm_attr(attr):\n    if not isinstance(attr, str):\n        return False\n\n    tokens = attr.rsplit(':', 1)\n    if len(tokens) != 2:\n        return False\n\n    try:\n        if '.' in tokens[0]:\n            if not is_valid_ipv4(tokens[0]):\n                return False\n        else:\n            int(tokens[0])\n        int(tokens[1])\n    except (ValueError, socket.error):\n        return False\n\n    return True", "response": "Validates an attribute as string representation of RT or SOO. Returns True if the attribute is valid otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_valid_esi(esi):\n    if isinstance(esi, numbers.Integral):\n        return 0 <= esi <= 0xffffffffffffffffff\n    return isinstance(esi, dict)", "response": "Returns True if the given EVPN Ethernet SegmentEthernet ID is valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndissecting messages from a raw stream data and dispatch them to the appropriate handler.", "response": "def get_and_dispatch_messages(self, data, disp_table):\n        \"\"\"dissect messages from a raw stream data.\n        disp_table[type] should be a callable for the corresponding\n        MessageType.\n        \"\"\"\n        self._unpacker.feed(data)\n        for m in self._unpacker:\n            self._dispatch_message(m, disp_table)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a request to the master.", "response": "def send_request(self, method, params):\n        \"\"\"Send a request\n        \"\"\"\n        msg, msgid = self._encoder.create_request(method, params)\n        self._send_message(msg)\n        self._pending_requests.add(msgid)\n        return msgid"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_response(self, msgid, error=None, result=None):\n        msg = self._encoder.create_response(msgid, error, result)\n        self._send_message(msg)", "response": "Send a response to the master."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_notification(self, method, params):\n        msg = self._encoder.create_notification(method, params)\n        self._send_message(msg)", "response": "Send a notification to the master"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntries to receive some messages. Returns True if there s something queued for get_xxx methods.", "response": "def receive_messages(self, all=False):\n        \"\"\"Try to receive some messages.\n        Received messages are put on the internal queues.\n        They can be retrieved using get_xxx() methods.\n        Returns True if there's something queued for get_xxx() methods.\n        \"\"\"\n        while all or self._incoming == 0:\n            try:\n                packet = self._sock.recv(4096)  # XXX the size is arbitrary\n            except IOError:\n                packet = None\n            if not packet:\n                if packet is not None:\n                    # socket closed by peer\n                    self._closed_by_peer = True\n                break\n            self._encoder.get_and_dispatch_messages(packet, self._table)\n        return self._incoming > 0"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nwait for a next incoming message and process it.", "response": "def receive_notification(self):\n        \"\"\"wait for the next incoming message.\n        intended to be used when we have nothing to send but want to receive\n        notifications.\n        \"\"\"\n        if not self._endpoint.receive_messages():\n            raise EOFError(\"EOF\")\n        self._process_input_notification()\n        self._process_input_request()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nstart running the pre - set function every interval seconds.", "response": "def start(self, interval, now=True):\n        \"\"\"Start running pre-set function every interval seconds.\n        \"\"\"\n        if interval < 0:\n            raise ValueError('interval must be >= 0')\n\n        if self._running:\n            self.stop()\n\n        self._running = True\n        self._interval = interval\n        if now:\n            self._self_thread = hub.spawn_after(0, self)\n        else:\n            self._self_thread = hub.spawn_after(self._interval, self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping running scheduled function.", "response": "def stop(self):\n        \"\"\"Stop running scheduled function.\n        \"\"\"\n        self._running = False\n        if self._self_thread is not None:\n            self._self_thread.cancel()\n            self._self_thread = None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nskip the next iteration and reset timer.", "response": "def reset(self):\n        \"\"\"Skip the next iteration and reset timer.\n        \"\"\"\n        if self._self_thread is not None:\n            # Cancel currently scheduled call\n            self._self_thread.cancel()\n            self._self_thread = None\n        # Schedule a new call\n        self._self_thread = hub.spawn_after(self._interval, self)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef compute_optional_conf(conf_name, default_value, **all_config):\n    conf_value = all_config.get(conf_name)\n    if conf_value is not None:\n        # Validate configuration value.\n        conf_value = get_validator(conf_name)(conf_value)\n    else:\n        conf_value = default_value\n    return conf_value", "response": "Compute optional configuration value."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nencoding a BFD Control packet without authentication section.", "response": "def pack(self):\n        \"\"\"\n        Encode a BFD Control packet without authentication section.\n        \"\"\"\n        diag = (self.ver << 5) + self.diag\n        flags = (self.state << 6) + self.flags\n        length = len(self)\n\n        return struct.pack(self._PACK_STR, diag, flags, self.detect_mult,\n                           length, self.my_discr, self.your_discr,\n                           self.desired_min_tx_interval,\n                           self.required_min_rx_interval,\n                           self.required_min_echo_rx_interval)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nauthenticating this packet. Returns a boolean indicates whether the packet can be authenticated or not. Returns ``False`` if the Authentication Present (A) is not set in the flag of this packet. Returns ``False`` if the Authentication Section for this packet is not present. For the description of the arguemnts of this method, refer to the authentication method of the Authentication Section classes.", "response": "def authenticate(self, *args, **kwargs):\n        \"\"\"Authenticate this packet.\n\n        Returns a boolean indicates whether the packet can be authenticated\n        or not.\n\n        Returns ``False`` if the Authentication Present (A) is not set in the\n        flag of this packet.\n\n        Returns ``False`` if the Authentication Section for this packet is not\n        present.\n\n        For the description of the arguemnts of this method, refer to the\n        authentication method of the Authentication Section classes.\n        \"\"\"\n        if not self.flags & BFD_FLAG_AUTH_PRESENT or \\\n                not issubclass(self.auth_cls.__class__, BFDAuth):\n            return False\n\n        return self.auth_cls.authenticate(self, *args, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parser_hdr(cls, buf):\n        return struct.unpack_from(cls._PACK_HDR_STR,\n                                  buf[:cls._PACK_HDR_STR_LEN])", "response": "Parser for common part of authentication section."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef serialize_hdr(self):\n        return struct.pack(self._PACK_HDR_STR, self.auth_type, self.auth_len)", "response": "Serialize the header of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _register_parser(cls):\n    '''class decorator to register msg parser'''\n    assert cls.cls_msg_type is not None\n    assert cls.cls_msg_type not in _MSG_PARSERS\n    _MSG_PARSERS[cls.cls_msg_type] = cls.parser\n    return cls", "response": "class decorator to register msg parser"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\niterating object attributes for stringify purposes", "response": "def obj_python_attrs(msg_):\n    \"\"\"iterate object attributes for stringify purposes\n    \"\"\"\n\n    # a special case for namedtuple which seems widely used in\n    # ofp parser implementations.\n    if hasattr(msg_, '_fields'):\n        for k in msg_._fields:\n            yield(k, getattr(msg_, k))\n        return\n    base = getattr(msg_, '_base_attributes', [])\n    opt = getattr(msg_, '_opt_attributes', [])\n    for k, v in inspect.getmembers(msg_):\n        if k in opt:\n            pass\n        elif k.startswith('_'):\n            continue\n        elif callable(v):\n            continue\n        elif k in base:\n            continue\n        elif hasattr(msg_.__class__, k):\n            continue\n        yield (k, v)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef obj_attrs(msg_):\n\n    if isinstance(msg_, StringifyMixin):\n        itr = msg_.stringify_attrs()\n    else:\n        # probably called by msg_str_attr\n        itr = obj_python_attrs(msg_)\n    for k, v in itr:\n        if k.endswith('_') and k[:-1] in _RESERVED_KEYWORD:\n            # XXX currently only StringifyMixin has restoring logic\n            assert isinstance(msg_, StringifyMixin)\n            k = k[:-1]\n        yield (k, v)", "response": "returns a generator of the attributes of a message object"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_jsondict(cls, dict_, decode_string=base64.b64decode,\n                      **additional_args):\n        r\"\"\"Create an instance from a JSON style dict.\n\n        Instantiate this class with parameters specified by the dict.\n\n        This method takes the following arguments.\n\n        .. tabularcolumns:: |l|L|\n\n        =============== =====================================================\n        Argument        Descrpition\n        =============== =====================================================\n        dict\\_          A dictionary which describes the parameters.\n                        For example, {\"Param1\": 100, \"Param2\": 200}\n        decode_string   (Optional) specify how to decode strings.\n                        The default is base64.\n                        This argument is used only for attributes which don't\n                        have explicit type annotations in _TYPE class\n                        attribute.\n        additional_args (Optional) Additional kwargs for constructor.\n        =============== =====================================================\n        \"\"\"\n        decode = lambda k, x: cls._decode_value(k, x, decode_string,\n                                                **additional_args)\n        kwargs = cls._restore_args(_mapdict_kv(decode, dict_))\n        try:\n            return cls(**dict(kwargs, **additional_args))\n        except TypeError:\n            # debug\n            print(\"CLS %s\" % cls)\n            print(\"ARG %s\" % dict_)\n            print(\"KWARG %s\" % kwargs)\n            raise", "response": "r Create an instance of a class from a JSON style dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nemulating jsonrpc. Connection. transact_block without blocking eventlet.", "response": "def transact_block(request, connection):\n    \"\"\"Emulate jsonrpc.Connection.transact_block without blocking eventlet.\n    \"\"\"\n    error = connection.send(request)\n    reply = None\n\n    if error:\n        return error, reply\n\n    ovs_poller = poller.Poller()\n    while not error:\n        ovs_poller.immediate_wake()\n        error, reply = connection.recv()\n\n        if error != errno.EAGAIN:\n            break\n\n        if (reply and\n            reply.id == request.id and\n            reply.type in (jsonrpc.Message.T_REPLY,\n                           jsonrpc.Message.T_ERROR)):\n            break\n\n        connection.run()\n        connection.wait(ovs_poller)\n        connection.recv_wait(ovs_poller)\n        ovs_poller.block()\n\n        hub.sleep(0)\n\n    return error, reply"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps method for _filter_schema to filter multiple schemas.", "response": "def _filter_schemas(schemas, schema_tables, exclude_table_columns):\n    \"\"\"Wrapper method for _filter_schema to filter multiple schemas.\"\"\"\n    return [_filter_schema(s, schema_tables, exclude_table_columns)\n            for s in schemas]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nfilters a schema to only include the specified tables in the schema_tables parameter.", "response": "def _filter_schema(schema, schema_tables, exclude_table_columns):\n    \"\"\"Filters a schema to only include the specified tables in the\n       schema_tables parameter.  This will also filter out any colums for\n       included tables that reference tables that are not included\n       in the schema_tables parameter\n\n    :param schema: Schema dict to be filtered\n    :param schema_tables: List of table names to filter on.\n                          EX: ['Bridge', 'Controller', 'Interface']\n                          NOTE: This list is case sensitive.\n    :return: Schema dict:\n                filtered if the schema_table parameter contains table names,\n                else the original schema dict\n    \"\"\"\n\n    tables = {}\n    for tbl_name, tbl_data in schema['tables'].items():\n        if not schema_tables or tbl_name in schema_tables:\n            columns = {}\n\n            exclude_columns = exclude_table_columns.get(tbl_name, [])\n            for col_name, col_data in tbl_data['columns'].items():\n                if col_name in exclude_columns:\n                    continue\n\n                # NOTE(Alan Quillin) Needs to check and remove\n                # and columns that have references to tables that\n                # are not to be configured\n                type_ = col_data.get('type')\n                if type_:\n                    if type_ and isinstance(type_, dict):\n                        key = type_.get('key')\n                        if key and isinstance(key, dict):\n                            ref_tbl = key.get('refTable')\n                            if ref_tbl and isinstance(ref_tbl,\n                                                      six.string_types):\n                                if ref_tbl not in schema_tables:\n                                    continue\n                        value = type_.get('value')\n                        if value and isinstance(value, dict):\n                            ref_tbl = value.get('refTable')\n                            if ref_tbl and isinstance(ref_tbl,\n                                                      six.string_types):\n                                if ref_tbl not in schema_tables:\n                                    continue\n\n                columns[col_name] = col_data\n\n            tbl_data['columns'] = columns\n            tables[tbl_name] = tbl_data\n\n    schema['tables'] = tables\n\n    return schema"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a Zebra message to the send queue.", "response": "def send_msg(self, msg):\n        \"\"\"\n        Sends Zebra message.\n\n        :param msg: Instance of py:class: `ryu.lib.packet.zebra.ZebraMessage`.\n        :return: Serialized msg if succeeded, otherwise None.\n        \"\"\"\n        if not self.is_active:\n            self.logger.debug(\n                'Cannot send message: Already deactivated: msg=%s', msg)\n            return\n        elif not self.send_q:\n            self.logger.debug(\n                'Cannot send message: Send queue does not exist: msg=%s', msg)\n            return\n        elif self.zserv_ver != msg.version:\n            self.logger.debug(\n                'Zebra protocol version mismatch:'\n                'server_version=%d, msg.version=%d',\n                self.zserv_ver, msg.version)\n            msg.version = self.zserv_ver  # fixup\n\n        self.send_q.put(msg.serialize())"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_peer_down(self, peer):\n        LOG.debug('Cleaning obsolete paths whose source/version: %s/%s',\n                  peer.ip_address, peer.version_num)\n        # Launch clean-up for each global tables.\n        self._table_manager.clean_stale_routes(peer)", "response": "Called when a peer is down. Cleans up the paths in the global tables that were received from this peer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn list of peers in established state.", "response": "def get_peers_in_established(self):\n        \"\"\"Returns list of peers in established state.\"\"\"\n        est_peers = []\n        for peer in self._peers.values():\n            if peer.in_established:\n                est_peers.append(peer)\n        return est_peers"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake refresh request to all peers that have valid RTC capability for given address family.", "response": "def req_rr_to_non_rtc_peers(self, route_family):\n        \"\"\"Makes refresh request to all peers for given address family.\n\n        Skips making request to peer that have valid RTC capability.\n        \"\"\"\n        assert route_family != RF_RTC_UC\n        for peer in self._peers.values():\n            # First check if peer is in established state\n            if (peer.in_established and\n                # Check if peer has valid capability for given address\n                # family\n                    peer.is_mbgp_cap_valid(route_family) and\n                # Check if peer has valid capability for RTC\n                    not peer.is_mbgp_cap_valid(RF_RTC_UC)):\n                peer.request_route_refresh(route_family)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmakes a route - refresh request for a given route family.", "response": "def make_route_refresh_request(self, peer_ip, *route_families):\n        \"\"\"Request route-refresh for peer with `peer_ip` for given\n        `route_families`.\n\n        Will make route-refresh request for a given `route_family` only if such\n        capability is supported and if peer is in ESTABLISHED state. Else, such\n        requests are ignored. Raises appropriate error in other cases. If\n        `peer_ip` is equal to 'all' makes refresh request to all valid peers.\n        \"\"\"\n        LOG.debug('Route refresh requested for peer %s and route families %s',\n                  peer_ip, route_families)\n        if not SUPPORTED_GLOBAL_RF.intersection(route_families):\n            raise ValueError('Given route family(s) % is not supported.' %\n                             route_families)\n\n        peer_list = []\n        # If route-refresh is requested for all peers.\n        if peer_ip == 'all':\n            peer_list.extend(self.get_peers_in_established())\n        else:\n            given_peer = self._peers.get(peer_ip)\n            if not given_peer:\n                raise ValueError('Invalid/unrecognized peer %s' % peer_ip)\n            if not given_peer.in_established:\n                raise ValueError('Peer currently do not have established'\n                                 ' session.')\n            peer_list.append(given_peer)\n\n        # Make route refresh request to valid peers.\n        for peer in peer_list:\n            peer.request_route_refresh(*route_families)\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncommunicating all RT_NLRI paths with this peer.", "response": "def comm_all_rt_nlris(self, peer):\n        \"\"\"Shares/communicates current best rt_nlri paths with this peers.\n\n        Can be used to send initial updates after we have established session\n        with `peer` with which RTC capability is valid. Takes into account\n        peers RTC_AS setting and filters all RT NLRIs whose origin AS do not\n        match this setting.\n        \"\"\"\n        # First check if for this peer mpbgp-rtc is valid.\n        if not peer.is_mbgp_cap_valid(RF_RTC_UC):\n            return\n\n        neigh_conf = self._neighbors_conf.get_neighbor_conf(peer.ip_address)\n        peer_rtc_as = neigh_conf.rtc_as\n        # Iterate over all RT_NLRI destination communicate qualifying RT_NLRIs\n        rtc_table = self._table_manager.get_rtc_table()\n        for dest in rtc_table.values():\n            best_path = dest.best_path\n            # Ignore a destination that currently does not have best path\n            if not best_path:\n                continue\n\n            # If this is a local path\n            if best_path.source is None:\n                # Check RT NLRI's origin AS matches peer RTC_AS setting\n                origin_as = best_path.nlri.origin_as\n                if origin_as == peer_rtc_as:\n                    peer.communicate_path(best_path)\n            else:\n                # Communicate all remote RT NLRIs\n                peer.communicate_path(best_path)\n\n        # Also communicate EOR as per RFC\n        peer.enque_end_of_rib(RF_RTC_UC)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncommunicate all current best paths with this peer.", "response": "def comm_all_best_paths(self, peer):\n        \"\"\"Shares/communicates current best paths with this peers.\n\n        Can be used to send initial updates after we have established session\n        with `peer`.\n        \"\"\"\n        LOG.debug('Communicating current best path for all afi/safi except'\n                  ' 1/132')\n        # We will enqueue best path from all global destination.\n        for route_family, table in self._table_manager.iter:\n            if route_family == RF_RTC_UC:\n                continue\n            if peer.is_mbgp_cap_valid(route_family):\n                for dest in table.values():\n                    if dest.best_path:\n                        peer.communicate_path(dest.best_path)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncommunicate and enqueues given best path to all iBGP peers.", "response": "def comm_new_best_to_bgp_peers(self, new_best_path):\n        \"\"\"Communicates/enqueues given best path to be sent to all qualifying\n        bgp peers.\n\n        If this path came from iBGP peers, it is not sent to other iBGP peers.\n        If this path has community-attribute, and if settings for recognize-\n        well-know attributes is set, we do as per [RFC1997], and queue outgoing\n        route only to qualifying BGP peers.\n        \"\"\"\n        # Filter based on standard community\n        # If new best path has community attribute, it should be taken into\n        # account when sending UPDATE to peers.\n        comm_attr = new_best_path.get_pattr(BGP_ATTR_TYPE_COMMUNITIES)\n        if comm_attr:\n            comm_attr_na = comm_attr.has_comm_attr(\n                BGPPathAttributeCommunities.NO_ADVERTISE\n            )\n            # If we have NO_ADVERTISE attribute is present, we do not send\n            # UPDATE to any peers\n            if comm_attr_na:\n                LOG.debug('New best path has community attr. NO_ADVERTISE = %s'\n                          '. Hence not advertising to any peer', comm_attr_na)\n                return\n\n        qualified_peers = self._collect_peers_of_interest(\n            new_best_path\n        )\n\n        # Distribute new best-path to qualified peers.\n        for peer in qualified_peers:\n            peer.communicate_path(new_best_path)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _collect_peers_of_interest(self, new_best_path):\n        path_rts = new_best_path.get_rts()\n        qualified_peers = set(self._peers.values())\n\n        # Filter out peers based on RTC_AS setting if path is for RT_NLRI\n        qualified_peers = self._rt_manager.filter_by_origin_as(\n            new_best_path, qualified_peers\n        )\n\n        # We continue to filter out qualified peer based on path RTs\n        # If new best path has RTs, we need to share this UPDATE with\n        # qualifying peers\n        if path_rts:\n            # We add Default_RTC_NLRI to path RTs so that we can send it to\n            # peers that have expressed interest in all paths\n            path_rts.append(RouteTargetMembershipNLRI.DEFAULT_RT)\n            # All peers that do not have RTC capability qualify\n            qualified_peers = set(self._get_non_rtc_peers())\n            # Peers that have RTC capability and have common RT with the path\n            # also qualify\n            peer_to_rtfilter_map = self._peer_to_rtfilter_map\n            for peer, rt_filter in peer_to_rtfilter_map.items():\n                # Ignore Network Controller (its not a BGP peer)\n                if peer is None:\n                    continue\n\n                if rt_filter is None:\n                    qualified_peers.add(peer)\n                elif rt_filter.intersection(path_rts):\n                    qualified_peers.add(peer)\n\n        return qualified_peers", "response": "Collect all peers that qualify for sharing a path with given RTs."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _create_prefix_notification(outgoing_msg, rpc_session):\n    assert outgoing_msg\n    path = outgoing_msg.path\n    assert path\n    vpn_nlri = path.nlri\n\n    assert path.source is not None\n    if path.source != VRF_TABLE:\n        # Extract relevant info for update-add/update-delete.\n        params = [{ROUTE_DISTINGUISHER: outgoing_msg.route_dist,\n                   PREFIX: vpn_nlri.prefix,\n                   NEXT_HOP: path.nexthop,\n                   VRF_RF: VrfConf.rf_2_vrf_rf(path.route_family)}]\n        if path.nlri.ROUTE_FAMILY.safi not in (subaddr_family.IP_FLOWSPEC,\n                                               subaddr_family.VPN_FLOWSPEC):\n            params[VPN_LABEL] = path.label_list[0]\n\n        if not path.is_withdraw:\n            # Create notification to NetworkController.\n            rpc_msg = rpc_session.create_notification(\n                NOTIFICATION_ADD_REMOTE_PREFIX, params)\n        else:\n            # Create update-delete request to NetworkController.\n            rpc_msg = rpc_session.create_notification(\n                NOTIFICATION_DELETE_REMOTE_PREFIX, params)\n    else:\n        # Extract relevant info for update-add/update-delete.\n        params = [{ROUTE_DISTINGUISHER: outgoing_msg.route_dist,\n                   PREFIX: vpn_nlri.prefix,\n                   NEXT_HOP: path.nexthop,\n                   VRF_RF: VrfConf.rf_2_vrf_rf(path.route_family),\n                   ORIGIN_RD: path.origin_rd}]\n        if not path.is_withdraw:\n            # Create notification to NetworkController.\n            rpc_msg = rpc_session.create_notification(\n                NOTIFICATION_ADD_LOCAL_PREFIX, params)\n        else:\n            # Create update-delete request to NetworkController.\n            rpc_msg = rpc_session.create_notification(\n                NOTIFICATION_DELETE_LOCAL_PREFIX, params)\n\n    return rpc_msg", "response": "Constructs prefix notification with data from given outgoing message."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates give port for use as rpc server port.", "response": "def _validate_rpc_port(port):\n    \"\"\"Validates give port for use as rpc server port.\n    \"\"\"\n    if not port:\n        raise NetworkControllerError(desc='Invalid rpc port number.')\n    if isinstance(port, str):\n        port = int(port)\n\n    if port <= 0:\n        raise NetworkControllerError(desc='Invalid rpc port number %s' % port)\n    return port"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _process_outgoing_msg(self, sink_iter):\n        LOG.debug('NetworkController processing outgoing request list.')\n        # TODO(PH): We should try not to sent routes from bgp peer that is not\n        # in established state.\n        from ryu.services.protocols.bgp.model import (\n            FlexinetOutgoingRoute)\n        while self.is_connected:\n            # sink iter is Sink instance and next is blocking so this isn't\n            # active wait.\n            for outgoing_msg in sink_iter:\n                if not self.is_connected:\n                    self._socket.close()\n                    return\n                if isinstance(outgoing_msg, FlexinetOutgoingRoute):\n                    rpc_msg = _create_prefix_notification(outgoing_msg, self)\n                else:\n                    raise NotImplementedError(\n                        'Do not handle out going message of type %s' %\n                        outgoing_msg.__class__)\n                if rpc_msg:\n                    self._sendall(rpc_msg)\n            self.pause(0)\n\n        # Stop incoming connection.\n        if self.green_in:\n            self.green_in.kill()", "response": "This function processes outgoing messages from the given sink_iter and sends them to the given socket."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _run(self, *args, **kwargs):\n        apgw_rpc_bind_ip = _validate_rpc_ip(kwargs.pop(NC_RPC_BIND_IP))\n        apgw_rpc_bind_port = _validate_rpc_port(kwargs.pop(NC_RPC_BIND_PORT))\n\n        sock_addr = (apgw_rpc_bind_ip, apgw_rpc_bind_port)\n        LOG.debug('NetworkController started listening for connections...')\n\n        server_thread, _ = self._listen_tcp(sock_addr,\n                                            self._start_rpc_session)\n        self.pause(0)\n        server_thread.wait()", "response": "Runs the RPC server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _start_rpc_session(self, sock):\n        session_name = RpcSession.NAME_FMT % str(sock.getpeername())\n        self._stop_child_activities(session_name)\n\n        rpc_session = RpcSession(sock, self)\n        self._spawn_activity(rpc_session)", "response": "Starts a new RPC session with given connection."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a BGPNotification message corresponding to given codes.", "response": "def notification_factory(code, subcode):\n    \"\"\"Returns a `Notification` message corresponding to given codes.\n\n    Parameters:\n    - `code`: (int) BGP error code\n    - `subcode`: (int) BGP error sub-code\n    \"\"\"\n    notification = BGPNotification(code, subcode)\n    if not notification.reason:\n        raise ValueError('Invalid code/sub-code.')\n\n    return notification"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncompares *True* if local router id is greater when compared to peer bgp id.", "response": "def is_local_router_id_greater(self):\n        \"\"\"Compares *True* if local router id is greater when compared to peer\n        bgp id.\n\n        Should only be called after protocol has reached OpenConfirm state.\n        \"\"\"\n        from ryu.services.protocols.bgp.utils.bgp import from_inet_ptoi\n\n        if not self.state == BGP_FSM_OPEN_CONFIRM:\n            raise BgpProtocolException(desc='Can access remote router id only'\n                                            ' after open message is received')\n        remote_id = self.recv_open_msg.bgp_identifier\n        local_id = self.sent_open_msg.bgp_identifier\n        return from_inet_ptoi(local_id) > from_inet_ptoi(remote_id)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_enhanced_rr_cap_valid(self):\n        if not self.recv_open_msg:\n            raise ValueError('Did not yet receive peers open message.')\n\n        err_cap_enabled = False\n        local_caps = self.sent_open_msg.opt_param\n        peer_caps = self.recv_open_msg.opt_param\n\n        local_cap = [cap for cap in local_caps\n                     if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n        peer_cap = [cap for cap in peer_caps\n                    if cap.cap_code == BGP_CAP_ENHANCED_ROUTE_REFRESH]\n\n        # Both local and peer should advertise ERR capability for it to be\n        # enabled.\n        if local_cap and peer_cap:\n            err_cap_enabled = True\n\n        return err_cap_enabled", "response": "Checks if enhanced route refresh capability is enabled or valid."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends open message to peer and handles received messages.", "response": "def _run(self, peer):\n        \"\"\"Sends open message to peer and handles received messages.\n\n        Parameters:\n            - `peer`: the peer to which this protocol instance is connected to.\n        \"\"\"\n        # We know the peer we are connected to, we send open message.\n        self._peer = peer\n        self.connection_made()\n\n        # We wait for peer to send messages.\n        self._recv_loop()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _data_received(self, next_bytes):\n        # Append buffer with received bytes.\n        self._recv_buff += next_bytes\n\n        while True:\n            # If current buffer size is less then minimum bgp message size, we\n            # return as we do not have a complete bgp message to work with.\n            if len(self._recv_buff) < BGP_MIN_MSG_LEN:\n                return\n\n            # Parse message header into elements.\n            auth, length, ptype = BgpProtocol.parse_msg_header(\n                self._recv_buff[:BGP_MIN_MSG_LEN])\n\n            # Check if we have valid bgp message marker.\n            # We should get default marker since we are not supporting any\n            # authentication.\n            if (auth != BgpProtocol.MESSAGE_MARKER):\n                LOG.error('Invalid message marker received: %s', auth)\n                raise bgp.NotSync()\n\n            # Check if we have valid bgp message length.\n            check = (length < BGP_MIN_MSG_LEN or length > BGP_MAX_MSG_LEN)\n\n            # RFC says: The minimum length of the OPEN message is 29\n            # octets (including the message header).\n            check2 = (ptype == BGP_MSG_OPEN and length < BGPOpen._MIN_LEN)\n\n            # RFC says: A KEEPALIVE message consists of only the\n            # message header and has a length of 19 octets.\n            check3 = (ptype == BGP_MSG_KEEPALIVE and\n                      length != BGPKeepAlive._MIN_LEN)\n\n            # RFC says: The minimum length of the UPDATE message is 23\n            # octets.\n            check4 = (ptype == BGP_MSG_UPDATE and\n                      length < BGPUpdate._MIN_LEN)\n\n            if any((check, check2, check3, check4)):\n                raise bgp.BadLen(ptype, length)\n\n            # If we have partial message we wait for rest of the message.\n            if len(self._recv_buff) < length:\n                return\n            msg, _, rest = BGPMessage.parser(self._recv_buff)\n            self._recv_buff = rest\n\n            # If we have a valid bgp message we call message handler.\n            self._handle_msg(msg)", "response": "Internal method that handles data received from peer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_notification(self, code, subcode):\n        notification = BGPNotification(code, subcode)\n        reason = notification.reason\n        self._send_with_lock(notification)\n        self._signal_bus.bgp_error(self._peer, code, subcode, reason)\n        if len(self._localname):\n            LOG.error('Sent notification to %r >> %s', self._localname,\n                      notification)\n        self._socket.close()", "response": "Utility to send a BGP notification. Closes the socket after sending the message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _validate_open_msg(self, open_msg):\n        assert open_msg.type == BGP_MSG_OPEN\n\n        opt_param_cap_map = open_msg.opt_param_cap_map\n\n        # Validate remote AS number.\n        remote_as = open_msg.my_as\n        # Try to get AS number from Four-Octet AS number capability.\n        cap4as = opt_param_cap_map.get(BGP_CAP_FOUR_OCTET_AS_NUMBER, None)\n        if cap4as is None:\n            if remote_as == AS_TRANS:\n                # Raise Bad Peer AS error message, if my_as is AS_TRANS\n                # and without Four-Octet AS number capability.\n                raise bgp.BadPeerAs()\n            self.cap_four_octet_as_number = False\n        else:\n            # Note: Even if the peer has Four-Octet AS number capability,\n            # keep the local capability setting\n            remote_as = cap4as.as_number\n            self.cap_four_octet_as_number = True\n        #  Validate remote AS number with local setting.\n        if remote_as != self._peer.remote_as:\n            raise bgp.BadPeerAs()\n\n        # Validate bgp version number.\n        if open_msg.version != BGP_VERSION_NUM:\n            raise bgp.UnsupportedVersion(BGP_VERSION_NUM)", "response": "Validates the BGP OPEN message according to application context."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_msg(self, msg):\n        LOG.debug('Received msg from %s << %s', self._remotename, msg)\n\n        # If we receive open message we try to bind to protocol\n        if msg.type == BGP_MSG_OPEN:\n            if self.state == BGP_FSM_OPEN_SENT:\n                # Validate open message.\n                self._validate_open_msg(msg)\n                self.recv_open_msg = msg\n                self.state = BGP_FSM_OPEN_CONFIRM\n                self._peer.state.bgp_state = self.state\n\n                # Try to bind this protocol to peer.\n                self._is_bound = self._peer.bind_protocol(self)\n\n                # If this protocol failed to bind to peer.\n                if not self._is_bound:\n                    # Failure to bind to peer indicates connection collision\n                    # resolution choose different instance of protocol and this\n                    # instance has to close. Before closing it sends\n                    # appropriate notification msg. to peer.\n                    raise bgp.CollisionResolution()\n\n                # If peer sends Hold Time as zero, then according to RFC we do\n                # not set Hold Time and Keep Alive timer.\n                if msg.hold_time == 0:\n                    LOG.info('The Hold Time sent by the peer is zero, hence '\n                             'not setting any Hold Time and Keep Alive'\n                             ' timers.')\n                else:\n                    # Start Keep Alive timer considering Hold Time preference\n                    # of the peer.\n                    self._start_timers(msg.hold_time)\n                    self._send_keepalive()\n\n                # Peer does not see open message.\n                return\n            else:\n                # If we receive a Open message out of order\n                LOG.error('Open message received when current state is not '\n                          'OpenSent')\n                # Received out-of-order open message\n                # We raise Finite state machine error\n                raise bgp.FiniteStateMachineError()\n        elif msg.type == BGP_MSG_NOTIFICATION:\n            if self._peer:\n                self._signal_bus.bgp_notification_received(self._peer, msg)\n            # If we receive notification message\n            LOG.error('Received notification message, hence closing '\n                      'connection %s', msg)\n            self._socket.close()\n            return\n\n        # If we receive keepalive or update message, we reset expire timer.\n        if (msg.type == BGP_MSG_KEEPALIVE or\n                msg.type == BGP_MSG_UPDATE):\n            if self._expiry:\n                self._expiry.reset()\n\n        # Call peer message handler for appropriate messages.\n        if (msg.type in\n                (BGP_MSG_UPDATE, BGP_MSG_KEEPALIVE, BGP_MSG_ROUTE_REFRESH)):\n            self._peer.handle_msg(msg)\n        # We give chance to other threads to run.\n        self.pause(0)", "response": "Handle a BGP message received from the peer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _start_timers(self, peer_holdtime):\n        neg_timer = min(self._holdtime, peer_holdtime)\n        if neg_timer < self._holdtime:\n            LOG.info('Negotiated hold time (%s) is lower then '\n                     'configured/default (%s).', neg_timer, self._holdtime)\n        # We use negotiated timer value.\n        self._holdtime = neg_timer\n        self._keepalive = self._create_timer('Keepalive Timer',\n                                             self._send_keepalive)\n        interval = self._holdtime // 3\n        self._keepalive.start(interval, now=False)\n        # Setup the expire timer.\n        self._expiry = self._create_timer('Holdtime Timer', self._expired)\n        self._expiry.start(self._holdtime, now=False)\n        LOG.debug('Started keep-alive and expire timer for negotiated hold'\n                  'time %s', self._holdtime)", "response": "Starts keepalive and expire timers based on the current hold time."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _expired(self):\n        LOG.info('Negotiated hold time %s expired.', self._holdtime)\n        code = BGP_ERROR_HOLD_TIMER_EXPIRED\n        subcode = BGP_ERROR_SUB_HOLD_TIMER_EXPIRED\n        self.send_notification(code, subcode)\n        self.connection_lost('Negotiated hold time %s expired.' %\n                             self._holdtime)\n        self.stop()", "response": "Hold timer expired event handler."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsits in tight loop collecting data received from peer and processing it.", "response": "def _recv_loop(self):\n        \"\"\"Sits in tight loop collecting data received from peer and\n        processing it.\n        \"\"\"\n        required_len = BGP_MIN_MSG_LEN\n        conn_lost_reason = \"Connection lost as protocol is no longer active\"\n        try:\n            while True:\n                next_bytes = self._socket.recv(required_len)\n                if len(next_bytes) == 0:\n                    conn_lost_reason = 'Peer closed connection'\n                    break\n                self.data_received(next_bytes)\n        except socket.error as err:\n            conn_lost_reason = 'Connection to peer lost: %s.' % err\n        except bgp.BgpExc as ex:\n            conn_lost_reason = 'Connection to peer lost, reason: %s.' % ex\n        except Exception as e:\n            LOG.debug(traceback.format_exc())\n            conn_lost_reason = str(e)\n        finally:\n            self.connection_lost(conn_lost_reason)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef connection_made(self):\n        assert self.state == BGP_FSM_CONNECT\n        # We have a connection with peer we send open message.\n        open_msg = self._peer.create_open_msg()\n        self._holdtime = open_msg.hold_time\n        self.state = BGP_FSM_OPEN_SENT\n        if not self.is_reactive:\n            self._peer.state.bgp_state = self.state\n        self.sent_open_msg = open_msg\n        self.send(open_msg)\n        self._peer.connection_made()", "response": "Connection to peer handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connection_lost(self, reason):\n\n        if self._peer:\n            state = self._peer.state.bgp_state\n            if self._is_bound or state == BGP_FSM_OPEN_SENT:\n                self._peer.connection_lost(reason)\n\n            self._peer = None\n\n        if reason:\n            LOG.info(reason)\n        else:\n            LOG.info('Connection to peer closed for unknown reasons.')", "response": "Stops all timers and notifies peer that the connection is lost."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_configured_capabilities(self):\n\n        capabilities = OrderedDict()\n        mbgp_caps = []\n        if self.cap_mbgp_ipv4:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_IPv4_UC.afi, RF_IPv4_UC.safi))\n\n        if self.cap_mbgp_ipv6:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_IPv6_UC.afi, RF_IPv6_UC.safi))\n\n        if self.cap_mbgp_vpnv4:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_IPv4_VPN.afi, RF_IPv4_VPN.safi))\n\n        if self.cap_mbgp_vpnv6:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_IPv6_VPN.afi, RF_IPv6_VPN.safi))\n\n        if self.cap_rtc:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_RTC_UC.afi, RF_RTC_UC.safi))\n\n        if self.cap_mbgp_evpn:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_L2_EVPN.afi, RF_L2_EVPN.safi))\n\n        if self.cap_mbgp_ipv4fs:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_IPv4_FLOWSPEC.afi, RF_IPv4_FLOWSPEC.safi))\n\n        if self.cap_mbgp_ipv6fs:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_IPv6_FLOWSPEC.afi, RF_IPv6_FLOWSPEC.safi))\n\n        if self.cap_mbgp_vpnv4fs:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_VPNv4_FLOWSPEC.afi, RF_VPNv4_FLOWSPEC.safi))\n\n        if self.cap_mbgp_vpnv6fs:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_VPNv6_FLOWSPEC.afi, RF_VPNv6_FLOWSPEC.safi))\n\n        if self.cap_mbgp_l2vpnfs:\n            mbgp_caps.append(\n                BGPOptParamCapabilityMultiprotocol(\n                    RF_L2VPN_FLOWSPEC.afi, RF_L2VPN_FLOWSPEC.safi))\n\n        if mbgp_caps:\n            capabilities[BGP_CAP_MULTIPROTOCOL] = mbgp_caps\n\n        if self.cap_refresh:\n            capabilities[BGP_CAP_ROUTE_REFRESH] = [\n                BGPOptParamCapabilityRouteRefresh()]\n\n        if self.cap_enhanced_refresh:\n            capabilities[BGP_CAP_ENHANCED_ROUTE_REFRESH] = [\n                BGPOptParamCapabilityEnhancedRouteRefresh()]\n\n        if self.cap_four_octet_as_number:\n            capabilities[BGP_CAP_FOUR_OCTET_AS_NUMBER] = [\n                BGPOptParamCapabilityFourOctetAsNumber(self.local_as)]\n\n        return capabilities", "response": "Returns a dictionary of configured capabilities."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the set of RTC AS configured for current neighbors.", "response": "def rtc_as_set(self):\n        \"\"\"Returns current RTC AS configured for current neighbors.\n        \"\"\"\n        rtc_as_set = set()\n        for neigh in self._neighbors.values():\n            rtc_as_set.add(neigh.rtc_as)\n        return rtc_as_set"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef serialize(self):\n\n        self.data = bytearray()\n        r = self.protocols[::-1]\n        for i, p in enumerate(r):\n            if isinstance(p, packet_base.PacketBase):\n                if i == len(r) - 1:\n                    prev = None\n                else:\n                    prev = r[i + 1]\n                data = p.serialize(self.data, prev)\n            else:\n                data = six.binary_type(p)\n            self.data = bytearray(data + self.data)", "response": "Encode a packet and store the resulted bytearray in self. data."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of protocols that match to the specified protocol.", "response": "def get_protocols(self, protocol):\n        \"\"\"Returns a list of protocols that matches to the specified protocol.\n        \"\"\"\n        if isinstance(protocol, packet_base.PacketBase):\n            protocol = protocol.__class__\n        assert issubclass(protocol, packet_base.PacketBase)\n        return [p for p in self.protocols if isinstance(p, protocol)]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_protocol(self, protocol):\n        result = self.get_protocols(protocol)\n        if len(result) > 0:\n            return result[0]\n        return None", "response": "Returns the first found protocol that matches to the passed protocol."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a bonding i / f. datapath id and list of ports", "response": "def add(self, dpid, ports):\n        \"\"\"add a setting of a bonding i/f.\n        'add' method takes the corresponding args in this order.\n\n        ========= =====================================================\n        Attribute Description\n        ========= =====================================================\n        dpid      datapath id.\n\n        ports     a list of integer values that means the ports face\n                  with the slave i/fs.\n        ========= =====================================================\n\n        if you want to use multi LAG, call 'add' method more than once.\n        \"\"\"\n        assert isinstance(ports, list)\n        assert len(ports) >= 2\n        ifs = {}\n        for port in ports:\n            ifs[port] = {'enabled': False, 'timeout': 0}\n        bond = {dpid: ifs}\n        self._bonds.append(bond)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef packet_in_handler(self, evt):\n        req_pkt = packet.Packet(evt.msg.data)\n        if slow.lacp in req_pkt:\n            (req_lacp, ) = req_pkt.get_protocols(slow.lacp)\n            (req_eth, ) = req_pkt.get_protocols(ethernet.ethernet)\n            self._do_lacp(req_lacp, req_eth.src, evt.msg)\n        else:\n            self.send_event_to_observers(EventPacketIn(evt.msg))", "response": "PacketIn event handler. when the received packet was LACP proceed it. otherwise send a event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef flow_removed_handler(self, evt):\n        msg = evt.msg\n        datapath = msg.datapath\n        ofproto = datapath.ofproto\n        dpid = datapath.id\n        match = msg.match\n        if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:\n            port = match.in_port\n            dl_type = match.dl_type\n        else:\n            port = match['in_port']\n            dl_type = match['eth_type']\n        if ether.ETH_TYPE_SLOW != dl_type:\n            return\n        self.logger.info(\n            \"SW=%s PORT=%d LACP exchange timeout has occurred.\",\n            dpid_to_str(dpid), port)\n        self._set_slave_enabled(dpid, port, False)\n        self._set_slave_timeout(dpid, port, 0)\n        self.send_event_to_observers(\n            EventSlaveStateChanged(datapath, port, False))", "response": "The flow removed event handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _do_lacp(self, req_lacp, src, msg):\n        datapath = msg.datapath\n        dpid = datapath.id\n        ofproto = datapath.ofproto\n        parser = datapath.ofproto_parser\n        if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:\n            port = msg.in_port\n        else:\n            port = msg.match['in_port']\n\n        self.logger.info(\"SW=%s PORT=%d LACP received.\",\n                         dpid_to_str(dpid), port)\n        self.logger.debug(str(req_lacp))\n\n        # when LACP arrived at disabled port, update the status of\n        # the slave i/f to enabled, and send a event.\n        if not self._get_slave_enabled(dpid, port):\n            self.logger.info(\n                \"SW=%s PORT=%d the slave i/f has just been up.\",\n                dpid_to_str(dpid), port)\n            self._set_slave_enabled(dpid, port, True)\n            self.send_event_to_observers(\n                EventSlaveStateChanged(datapath, port, True))\n\n        # set the idle_timeout time using the actor state of the\n        # received packet.\n        if req_lacp.LACP_STATE_SHORT_TIMEOUT == \\\n           req_lacp.actor_state_timeout:\n            idle_timeout = req_lacp.SHORT_TIMEOUT_TIME\n        else:\n            idle_timeout = req_lacp.LONG_TIMEOUT_TIME\n\n        # when the timeout time has changed, update the timeout time of\n        # the slave i/f and re-enter a flow entry for the packet from\n        # the slave i/f with idle_timeout.\n        if idle_timeout != self._get_slave_timeout(dpid, port):\n            self.logger.info(\n                \"SW=%s PORT=%d the timeout time has changed.\",\n                dpid_to_str(dpid), port)\n            self._set_slave_timeout(dpid, port, idle_timeout)\n            func = self._add_flow.get(ofproto.OFP_VERSION)\n            assert func\n            func(src, port, idle_timeout, datapath)\n\n        # create a response packet.\n        res_pkt = self._create_response(datapath, port, req_lacp)\n\n        # packet-out the response packet.\n        out_port = ofproto.OFPP_IN_PORT\n        actions = [parser.OFPActionOutput(out_port)]\n        out = datapath.ofproto_parser.OFPPacketOut(\n            datapath=datapath, buffer_id=ofproto.OFP_NO_BUFFER,\n            data=res_pkt.data, in_port=port, actions=actions)\n        datapath.send_msg(out)", "response": "process a received LACP packet."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a packet including LACP.", "response": "def _create_response(self, datapath, port, req):\n        \"\"\"create a packet including LACP.\"\"\"\n        src = datapath.ports[port].hw_addr\n        res_ether = ethernet.ethernet(\n            slow.SLOW_PROTOCOL_MULTICAST, src, ether.ETH_TYPE_SLOW)\n        res_lacp = self._create_lacp(datapath, port, req)\n        res_pkt = packet.Packet()\n        res_pkt.add_protocol(res_ether)\n        res_pkt.add_protocol(res_lacp)\n        res_pkt.serialize()\n        return res_pkt"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _create_lacp(self, datapath, port, req):\n        actor_system = datapath.ports[datapath.ofproto.OFPP_LOCAL].hw_addr\n        res = slow.lacp(\n            actor_system_priority=0xffff,\n            actor_system=actor_system,\n            actor_key=req.actor_key,\n            actor_port_priority=0xff,\n            actor_port=port,\n            actor_state_activity=req.LACP_STATE_PASSIVE,\n            actor_state_timeout=req.actor_state_timeout,\n            actor_state_aggregation=req.actor_state_aggregation,\n            actor_state_synchronization=req.actor_state_synchronization,\n            actor_state_collecting=req.actor_state_collecting,\n            actor_state_distributing=req.actor_state_distributing,\n            actor_state_defaulted=req.LACP_STATE_OPERATIONAL_PARTNER,\n            actor_state_expired=req.LACP_STATE_NOT_EXPIRED,\n            partner_system_priority=req.actor_system_priority,\n            partner_system=req.actor_system,\n            partner_key=req.actor_key,\n            partner_port_priority=req.actor_port_priority,\n            partner_port=req.actor_port,\n            partner_state_activity=req.actor_state_activity,\n            partner_state_timeout=req.actor_state_timeout,\n            partner_state_aggregation=req.actor_state_aggregation,\n            partner_state_synchronization=req.actor_state_synchronization,\n            partner_state_collecting=req.actor_state_collecting,\n            partner_state_distributing=req.actor_state_distributing,\n            partner_state_defaulted=req.actor_state_defaulted,\n            partner_state_expired=req.actor_state_expired,\n            collector_max_delay=0)\n        self.logger.info(\"SW=%s PORT=%d LACP sent.\",\n                         dpid_to_str(datapath.id), port)\n        self.logger.debug(str(res))\n        return res", "response": "create a LACP packet."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_slave_enabled(self, dpid, port):\n        slave = self._get_slave(dpid, port)\n        if slave:\n            return slave['enabled']\n        else:\n            return False", "response": "get whether a slave i / f at some port is a valid i/f"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_slave_enabled(self, dpid, port, enabled):\n        slave = self._get_slave(dpid, port)\n        if slave:\n            slave['enabled'] = enabled", "response": "set whether a slave i / f at some port is_slave_enabled or not"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_slave_timeout(self, dpid, port):\n        slave = self._get_slave(dpid, port)\n        if slave:\n            return slave['timeout']\n        else:\n            return 0", "response": "get the timeout time at some port of some datapath"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set_slave_timeout(self, dpid, port, timeout):\n        slave = self._get_slave(dpid, port)\n        if slave:\n            slave['timeout'] = timeout", "response": "set the timeout time at some port of some datapath"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting slave i/f at some port of some datapath", "response": "def _get_slave(self, dpid, port):\n        \"\"\"get slave i/f at some port of some datapath.\"\"\"\n        result = None\n        for bond in self._bonds:\n            if dpid in bond:\n                if port in bond[dpid]:\n                    result = bond[dpid][port]\n                    break\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_flow_v1_0(self, src, port, timeout, datapath):\n        ofproto = datapath.ofproto\n        parser = datapath.ofproto_parser\n\n        match = parser.OFPMatch(\n            in_port=port, dl_src=addrconv.mac.text_to_bin(src),\n            dl_type=ether.ETH_TYPE_SLOW)\n        actions = [parser.OFPActionOutput(\n            ofproto.OFPP_CONTROLLER, 65535)]\n        mod = parser.OFPFlowMod(\n            datapath=datapath, match=match, cookie=0,\n            command=ofproto.OFPFC_ADD, idle_timeout=timeout,\n            priority=65535, flags=ofproto.OFPFF_SEND_FLOW_REM,\n            actions=actions)\n        datapath.send_msg(mod)", "response": "enter a flow entry for the packet from the slave i/f\n        with idle_timeout. for OpenFlow ver1. 0."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nentering a flow entry for the packet from the slave i/f with idle_timeout. for OpenFlow ver1. 2 and ver1. 3.", "response": "def _add_flow_v1_2(self, src, port, timeout, datapath):\n        \"\"\"enter a flow entry for the packet from the slave i/f\n        with idle_timeout. for OpenFlow ver1.2 and ver1.3.\"\"\"\n        ofproto = datapath.ofproto\n        parser = datapath.ofproto_parser\n\n        match = parser.OFPMatch(\n            in_port=port, eth_src=src, eth_type=ether.ETH_TYPE_SLOW)\n        actions = [parser.OFPActionOutput(\n            ofproto.OFPP_CONTROLLER, ofproto.OFPCML_MAX)]\n        inst = [parser.OFPInstructionActions(\n            ofproto.OFPIT_APPLY_ACTIONS, actions)]\n        mod = parser.OFPFlowMod(\n            datapath=datapath, command=ofproto.OFPFC_ADD,\n            idle_timeout=timeout, priority=65535,\n            flags=ofproto.OFPFF_SEND_FLOW_REM, match=match,\n            instructions=inst)\n        datapath.send_msg(mod)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef ip_route_show(session, destination, device, **kwargs):\n    intf = interface.ip_link_show(session, ifname=device)\n    if not intf:\n        LOG.debug('Interface \"%s\" does not exist', device)\n        return None\n\n    return session.query(Route).filter_by(\n        destination=destination, ifindex=intf.ifindex, **kwargs).first()", "response": "Get a route record matching the given filtering rules."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a route record into Zebra protocol service database. The arguments are similar to \"ip route add\" command of iproute2. If \"is_selected=True\", disables the existing selected route for the given destination. :param session: Session instance connecting to database. :param destination: Destination prefix. :param device: Source device. :param gateway: Gateway IP address. :param source: Source IP address. :param ifindex: Index of source device. :param route_type: Route type of daemon (or kernel). :param is_selected: If select the given route as \"in use\" or not. :return: Instance of record or \"None\" if failed.", "response": "def ip_route_add(session, destination, device=None, gateway='', source='',\n                 ifindex=0, route_type=zebra.ZEBRA_ROUTE_KERNEL,\n                 is_selected=True):\n    \"\"\"\n    Adds a route record into Zebra protocol service database.\n\n    The arguments are similar to \"ip route add\" command of iproute2.\n\n    If \"is_selected=True\", disables the existing selected route for the\n    given destination.\n\n    :param session: Session instance connecting to database.\n    :param destination: Destination prefix.\n    :param device: Source device.\n    :param gateway: Gateway IP address.\n    :param source: Source IP address.\n    :param ifindex: Index of source device.\n    :param route_type: Route type of daemon (or kernel).\n    :param is_selected: If select the given route as \"in use\" or not.\n    :return: Instance of record or \"None\" if failed.\n    \"\"\"\n    if device:\n        intf = interface.ip_link_show(session, ifname=device)\n        if not intf:\n            LOG.debug('Interface \"%s\" does not exist', device)\n            return None\n        ifindex = ifindex or intf.ifindex\n\n        route = ip_route_show(session, destination=destination, device=device)\n        if route:\n            LOG.debug(\n                'Route to \"%s\" already exists on \"%s\" device',\n                destination, device)\n            return route\n\n    dest_addr, dest_prefix_num = destination.split('/')\n    dest_prefix_num = int(dest_prefix_num)\n    if ip.valid_ipv4(dest_addr) and 0 <= dest_prefix_num <= 32:\n        family = socket.AF_INET\n    elif ip.valid_ipv6(dest_addr) and 0 <= dest_prefix_num <= 128:\n        family = socket.AF_INET6\n    else:\n        LOG.debug('Invalid IP address for \"prefix\": %s', destination)\n        return None\n    safi = packet_safi.UNICAST\n\n    if is_selected:\n        old_routes = ip_route_show_all(\n            session, destination=destination, is_selected=True)\n        for old_route in old_routes:\n            if old_route:\n                LOG.debug('Set existing route to unselected: %s', old_route)\n                old_route.is_selected = False\n\n    new_route = Route(\n        family=family,\n        safi=safi,\n        destination=destination,\n        gateway=gateway,\n        ifindex=ifindex,\n        source=source,\n        route_type=route_type,\n        is_selected=is_selected)\n\n    session.add(new_route)\n\n    return new_route"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ip_route_delete(session, destination, **kwargs):\n    routes = ip_route_show_all(session, destination=destination, **kwargs)\n    for route in routes:\n        session.delete(route)\n\n    return routes", "response": "Delete route records from Zebra protocol service database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef setup_hook(config):\n    metadata = config['metadata']\n    if sys.platform == 'win32':\n        requires = metadata.get('requires_dist', '').split('\\n')\n        metadata['requires_dist'] = \"\\n\".join(requires)\n    config['metadata'] = metadata\n\n    metadata['version'] = str(version)\n\n    # pbr's setup_hook replaces easy_install.get_script_args with\n    # their own version, override_get_script_args, prefering simpler\n    # scripts which are not aware of multi-version.\n    # prevent that by doing the opposite.  it's a horrible hack\n    # but we are in patching wars already...\n    from pbr import packaging\n\n    def my_get_script_args(*args, **kwargs):\n        return _main_module()._orig_get_script_args(*args, **kwargs)\n\n    packaging.override_get_script_args = my_get_script_args\n    easy_install.get_script_args = my_get_script_args\n\n    # another hack to allow setup from tarball.\n    orig_get_version = packaging.get_version\n\n    def my_get_version(package_name, pre_version=None):\n        if package_name == 'ryu':\n            return str(version)\n        return orig_get_version(package_name, pre_version)\n\n    packaging.get_version = my_get_version", "response": "A hook to inject our defaults into the config."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the current learned path.", "response": "def _compare_by_version(path1, path2):\n    \"\"\"Returns the current/latest learned path.\n\n    Checks if given paths are from same source/peer and then compares their\n    version number to determine which path is received later. If paths are from\n    different source/peer return None.\n    \"\"\"\n    if path1.source == path2.source:\n        if path1.source_version_num > path2.source_version_num:\n            return path1\n        else:\n            return path2\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef compute_best_path(local_asn, path1, path2):\n    best_path = None\n    best_path_reason = BPR_UNKNOWN\n\n    # Follow best path calculation algorithm steps.\n    if best_path is None:\n        best_path = _cmp_by_reachable_nh(path1, path2)\n        best_path_reason = BPR_REACHABLE_NEXT_HOP\n    if best_path is None:\n        best_path = _cmp_by_highest_wg(path1, path2)\n        best_path_reason = BPR_HIGHEST_WEIGHT\n    if best_path is None:\n        best_path = _cmp_by_local_pref(path1, path2)\n        best_path_reason = BPR_LOCAL_PREF\n    if best_path is None:\n        best_path = _cmp_by_local_origin(path1, path2)\n        best_path_reason = BPR_LOCAL_ORIGIN\n    if best_path is None:\n        best_path = _cmp_by_aspath(path1, path2)\n        best_path_reason = BPR_ASPATH\n    if best_path is None:\n        best_path = _cmp_by_origin(path1, path2)\n        best_path_reason = BPR_ORIGIN\n    if best_path is None:\n        best_path = _cmp_by_med(path1, path2)\n        best_path_reason = BPR_MED\n    if best_path is None:\n        best_path = _cmp_by_asn(local_asn, path1, path2)\n        best_path_reason = BPR_ASN\n    if best_path is None:\n        best_path = _cmp_by_igp_cost(path1, path2)\n        best_path_reason = BPR_IGP_COST\n    if best_path is None:\n        best_path = _cmp_by_router_id(local_asn, path1, path2)\n        best_path_reason = BPR_ROUTER_ID\n    if best_path is None:\n        best_path = _cmp_by_cluster_list(path1, path2)\n        best_path_reason = BPR_CLUSTER_LIST\n    if best_path is None:\n        best_path_reason = BPR_UNKNOWN\n\n    return best_path, best_path_reason", "response": "Compares given paths and returns the best path among given paths."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cmp_by_local_pref(path1, path2):\n    # TODO(PH): Revisit this when BGPS has concept of policy to be applied to\n    # in-bound NLRIs.\n    # Default local-pref values is 100\n    lp1 = path1.get_pattr(BGP_ATTR_TYPE_LOCAL_PREF)\n    lp2 = path2.get_pattr(BGP_ATTR_TYPE_LOCAL_PREF)\n    if not (lp1 and lp2):\n        return None\n\n    # Highest local-preference value is preferred.\n    lp1 = lp1.value\n    lp2 = lp2.value\n    if lp1 > lp2:\n        return path1\n    elif lp2 > lp1:\n        return path2\n    else:\n        return None", "response": "Selects a path with highest local - preference."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _cmp_by_local_origin(path1, path2):\n    # If both paths are from same sources we cannot compare them here.\n    if path1.source == path2.source:\n        return None\n\n    # Here we consider prefix from NC as locally originating static route.\n    # Hence it is preferred.\n    if path1.source is None:\n        return path1\n\n    if path2.source is None:\n        return path2\n\n    return None", "response": "Select locally originating path as best path."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the best - paths by comparing as - path lengths.", "response": "def _cmp_by_aspath(path1, path2):\n    \"\"\"Calculated the best-paths by comparing as-path lengths.\n\n    Shortest as-path length is preferred. If both path have same lengths,\n    we return None.\n    \"\"\"\n    as_path1 = path1.get_pattr(BGP_ATTR_TYPE_AS_PATH)\n    as_path2 = path2.get_pattr(BGP_ATTR_TYPE_AS_PATH)\n    assert as_path1 and as_path2\n    l1 = as_path1.get_as_path_len()\n    l2 = as_path2.get_as_path_len()\n    assert l1 is not None and l2 is not None\n    if l1 > l2:\n        return path2\n    elif l2 > l1:\n        return path1\n    else:\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting the best path based on origin attribute.", "response": "def _cmp_by_origin(path1, path2):\n    \"\"\"Select the best path based on origin attribute.\n\n    IGP is preferred over EGP; EGP is preferred over Incomplete.\n    If both paths have same origin, we return None.\n    \"\"\"\n    def get_origin_pref(origin):\n        if origin.value == BGP_ATTR_ORIGIN_IGP:\n            return 3\n        elif origin.value == BGP_ATTR_ORIGIN_EGP:\n            return 2\n        elif origin.value == BGP_ATTR_ORIGIN_INCOMPLETE:\n            return 1\n        else:\n            LOG.error('Invalid origin value encountered %s.', origin)\n            return 0\n\n    origin1 = path1.get_pattr(BGP_ATTR_TYPE_ORIGIN)\n    origin2 = path2.get_pattr(BGP_ATTR_TYPE_ORIGIN)\n    assert origin1 is not None and origin2 is not None\n\n    # If both paths have same origins\n    if origin1.value == origin2.value:\n        return None\n\n    # Translate origin values to preference.\n    origin1 = get_origin_pref(origin1)\n    origin2 = get_origin_pref(origin2)\n    # Return preferred path.\n    if origin1 == origin2:\n        return None\n    elif origin1 > origin2:\n        return path1\n    return path2"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cmp_by_med(path1, path2):\n    def get_path_med(path):\n        med = path.get_pattr(BGP_ATTR_TYPE_MULTI_EXIT_DISC)\n        if not med:\n            return 0\n        return med.value\n\n    med1 = get_path_med(path1)\n    med2 = get_path_med(path2)\n\n    if med1 == med2:\n        return None\n    elif med1 < med2:\n        return path1\n    return path2", "response": "Select the path based with lowest MED value."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cmp_by_asn(local_asn, path1, path2):\n    def get_path_source_asn(path):\n        asn = None\n        if path.source is None:\n            asn = local_asn\n        else:\n            asn = path.source.remote_as\n        return asn\n\n    p1_asn = get_path_source_asn(path1)\n    p2_asn = get_path_source_asn(path2)\n    # If path1 is from ibgp peer and path2 is from ebgp peer.\n    if (p1_asn == local_asn) and (p2_asn != local_asn):\n        return path2\n\n    # If path2 is from ibgp peer and path1 is from ebgp peer,\n    if (p2_asn == local_asn) and (p1_asn != local_asn):\n        return path1\n\n    # If both paths are from ebgp or ibpg peers, we cannot decide.\n    return None", "response": "Select the path based on local AS number."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _cmp_by_router_id(local_asn, path1, path2):\n    def get_asn(path_source):\n        if path_source is None:\n            return local_asn\n        else:\n            return path_source.remote_as\n\n    def get_router_id(path, local_bgp_id):\n        path_source = path.source\n        if path_source is None:\n            return local_bgp_id\n        else:\n            originator_id = path.get_pattr(BGP_ATTR_TYPE_ORIGINATOR_ID)\n            if originator_id:\n                return originator_id.value\n            return path_source.protocol.recv_open_msg.bgp_identifier\n\n    path_source1 = path1.source\n    path_source2 = path2.source\n\n    # If both paths are from NC we have same router Id, hence cannot compare.\n    if path_source1 is None and path_source2 is None:\n        return None\n\n    asn1 = get_asn(path_source1)\n    asn2 = get_asn(path_source2)\n\n    is_ebgp1 = asn1 != local_asn\n    is_ebgp2 = asn2 != local_asn\n    # If both paths are from eBGP peers, then according to RFC we need\n    # not tie break using router id.\n    if is_ebgp1 and is_ebgp2:\n        return None\n\n    if ((is_ebgp1 is True and is_ebgp2 is False) or\n            (is_ebgp1 is False and is_ebgp2 is True)):\n        raise ValueError('This method does not support comparing ebgp with'\n                         ' ibgp path')\n\n    # At least one path is not coming from NC, so we get local bgp id.\n    if path_source1 is not None:\n        local_bgp_id = path_source1.protocol.sent_open_msg.bgp_identifier\n    else:\n        local_bgp_id = path_source2.protocol.sent_open_msg.bgp_identifier\n\n    # Get router ids.\n    router_id1 = get_router_id(path1, local_bgp_id)\n    router_id2 = get_router_id(path2, local_bgp_id)\n\n    # If both router ids are same/equal we cannot decide.\n    # This case is possible since router ids are arbitrary.\n    if router_id1 == router_id2:\n        return None\n\n    # Select the path with lowest router Id.\n    from ryu.services.protocols.bgp.utils.bgp import from_inet_ptoi\n    if from_inet_ptoi(router_id1) < from_inet_ptoi(router_id2):\n        return path1\n    else:\n        return path2", "response": "Select the route that is received from the peer with the lowest BGP router ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _cmp_by_cluster_list(path1, path2):\n    def _get_cluster_list_len(path):\n        c_list = path.get_pattr(BGP_ATTR_TYPE_CLUSTER_LIST)\n        if c_list is None:\n            return 0\n        else:\n            return len(c_list.value)\n\n    c_list_len1 = _get_cluster_list_len(path1)\n    c_list_len2 = _get_cluster_list_len(path2)\n    if c_list_len1 < c_list_len2:\n        return path1\n    elif c_list_len1 > c_list_len2:\n        return path2\n    else:\n        return None", "response": "Selects the route received from the peer with the shorter\n    CLUSTER_LIST length. [ RFC4456 ]"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef neighbor_update(self, address, conf_type, conf_value):\n\n        assert conf_type == MULTI_EXIT_DISC or conf_type == CONNECT_MODE\n\n        func_name = 'neighbor.update'\n        attribute_param = {}\n        if conf_type == MULTI_EXIT_DISC:\n            attribute_param = {neighbors.MULTI_EXIT_DISC: conf_value}\n        elif conf_type == CONNECT_MODE:\n            attribute_param = {neighbors.CONNECT_MODE: conf_value}\n\n        param = {neighbors.IP_ADDRESS: address,\n                 neighbors.CHANGES: attribute_param}\n\n        call(func_name, **param)", "response": "This method updates the neighbor configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef prefix_del(self, prefix, route_dist=None):\n        func_name = 'network.del'\n        networks = {\n            PREFIX: prefix,\n        }\n        if route_dist:\n            func_name = 'prefix.delete_local'\n            networks[ROUTE_DISTINGUISHER] = route_dist\n\n            rf, p = self._check_rf_and_normalize(prefix)\n            networks[ROUTE_FAMILY] = rf\n            networks[PREFIX] = p\n\n        call(func_name, **networks)", "response": "This method deletes an advertised prefix."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef flowspec_prefix_del(self, flowspec_family, rules, route_dist=None):\n        func_name = 'flowspec.del'\n\n        # Set required arguments\n        kwargs = {\n            FLOWSPEC_FAMILY: flowspec_family,\n            FLOWSPEC_RULES: rules,\n        }\n\n        if flowspec_family in [FLOWSPEC_FAMILY_VPNV4, FLOWSPEC_FAMILY_VPNV6,\n                               FLOWSPEC_FAMILY_L2VPN]:\n            func_name = 'flowspec.del_local'\n            kwargs.update({ROUTE_DISTINGUISHER: route_dist})\n\n        call(func_name, **kwargs)", "response": "This method deletes an advertised Flow Specification route."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef neighbor_get(self, route_type, address, format='json'):\n        show = {\n            'format': format,\n        }\n        if route_type == 'sent-routes' or route_type == 'received-routes':\n            show['params'] = ['neighbor', route_type, address, 'all']\n        else:\n            show['params'] = ['neighbor', 'received-routes', address, 'all']\n\n        return call('operator.show', **show)", "response": "This method returns the BGP adj - RIB - in and BGP adj - RIB - out information for a given neighbor."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef out_filter_get(self, address):\n\n        func_name = 'neighbor.out_filter.get'\n        param = {\n            neighbors.IP_ADDRESS: address,\n        }\n\n        return call(func_name, **param)", "response": "This method gets the out - filter setting from the specified neighbor."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bmp_server_add(self, address, port):\n\n        func_name = 'bmp.start'\n        param = {\n            'host': address,\n            'port': port,\n        }\n\n        call(func_name, **param)", "response": "This method registers a new BMP server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef bmp_server_del(self, address, port):\n\n        func_name = 'bmp.stop'\n        param = {\n            'host': address,\n            'port': port,\n        }\n\n        call(func_name, **param)", "response": "This method unregisters the registered BMP server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _check_rf_and_normalize(prefix):\n        addr, masklen = prefix.split('/')\n        if ip.valid_ipv6(addr):\n            # normalize IPv6 address\n            ipv6_prefix = str(netaddr.IPAddress(addr)) + '/' + masklen\n            return vrfs.VRF_RF_IPV6, ipv6_prefix\n        else:\n            return vrfs.VRF_RF_IPV4, prefix", "response": "check prefix s route_family and normalize it"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconfiguring remote ethernet and IP addresses.", "response": "def set_remote_addr(self, dst_mac, dst_ip):\n        \"\"\"\n        Configure remote ethernet and IP addresses.\n        \"\"\"\n        self.dst_mac = dst_mac\n        self.dst_ip = dst_ip\n\n        if not (dst_mac == \"FF:FF:FF:FF:FF:FF\" or dst_ip == \"255.255.255.255\"):\n            self._remote_addr_config = True\n\n        LOG.info(\"[BFD][%s][REMOTE] Remote address configured: %s, %s.\",\n                 hex(self._local_discr), self.dst_ip, self.dst_mac)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef recv(self, bfd_pkt):\n        LOG.debug(\"[BFD][%s][RECV] BFD Control received: %s\",\n                  hex(self._local_discr), six.binary_type(bfd_pkt))\n        self._remote_discr = bfd_pkt.my_discr\n        self._remote_state = bfd_pkt.state\n        self._remote_demand_mode = bfd_pkt.flags & bfd.BFD_FLAG_DEMAND\n\n        if self._remote_min_rx_interval != bfd_pkt.required_min_rx_interval:\n            self._remote_min_rx_interval = bfd_pkt.required_min_rx_interval\n            # Update transmit interval (RFC5880 Section 6.8.2.)\n            self._update_xmit_period()\n\n        # TODO: Echo function (RFC5880 Page 35)\n\n        if bfd_pkt.flags & bfd.BFD_FLAG_FINAL and self._is_polling:\n            self._is_polling = False\n\n        # Check and update the session state (RFC5880 Page 35)\n        if self._session_state == bfd.BFD_STATE_ADMIN_DOWN:\n            return\n\n        if bfd_pkt.state == bfd.BFD_STATE_ADMIN_DOWN:\n            if self._session_state != bfd.BFD_STATE_DOWN:\n                self._set_state(bfd.BFD_STATE_DOWN,\n                                bfd.BFD_DIAG_NEIG_SIG_SESS_DOWN)\n        else:\n            if self._session_state == bfd.BFD_STATE_DOWN:\n                if bfd_pkt.state == bfd.BFD_STATE_DOWN:\n                    self._set_state(bfd.BFD_STATE_INIT)\n                elif bfd_pkt.state == bfd.BFD_STATE_INIT:\n                    self._set_state(bfd.BFD_STATE_UP)\n\n            elif self._session_state == bfd.BFD_STATE_INIT:\n                if bfd_pkt.state in [bfd.BFD_STATE_INIT, bfd.BFD_STATE_UP]:\n                    self._set_state(bfd.BFD_STATE_UP)\n\n            else:\n                if bfd_pkt.state == bfd.BFD_STATE_DOWN:\n                    self._set_state(bfd.BFD_STATE_DOWN,\n                                    bfd.BFD_DIAG_NEIG_SIG_SESS_DOWN)\n\n        # TODO: Demand mode support.\n\n        if self._remote_demand_mode and \\\n                self._session_state == bfd.BFD_STATE_UP and \\\n                self._remote_session_state == bfd.BFD_STATE_UP:\n            self._enable_send = False\n\n        if not self._remote_demand_mode or \\\n                self._session_state != bfd.BFD_STATE_UP or \\\n                self._remote_session_state != bfd.BFD_STATE_UP:\n            if not self._enable_send:\n                self._enable_send = True\n                hub.spawn(self._send_loop)\n\n        # Update the detection time (RFC5880 Section 6.8.4.)\n        if self._detect_time == 0:\n            self._detect_time = bfd_pkt.desired_min_tx_interval * \\\n                bfd_pkt.detect_mult / 1000000.0\n            # Start the timeout loop.\n            hub.spawn(self._recv_timeout_loop)\n\n        if bfd_pkt.flags & bfd.BFD_FLAG_POLL:\n            self._pending_final = True\n            self._detect_time = bfd_pkt.desired_min_tx_interval * \\\n                bfd_pkt.detect_mult / 1000000.0\n\n        # Update the remote authentication sequence number.\n        if self._auth_type in [bfd.BFD_AUTH_KEYED_MD5,\n                               bfd.BFD_AUTH_METICULOUS_KEYED_MD5,\n                               bfd.BFD_AUTH_KEYED_SHA1,\n                               bfd.BFD_AUTH_METICULOUS_KEYED_SHA1]:\n            self._rcv_auth_seq = bfd_pkt.auth_cls.seq\n            self._auth_seq_known = 1\n\n        # Set the lock.\n        if self._lock is not None:\n            self._lock.set()", "response": "Receive a BFD packet and update the internal state."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _set_state(self, new_state, diag=None):\n        old_state = self._session_state\n\n        LOG.info(\"[BFD][%s][STATE] State changed from %s to %s.\",\n                 hex(self._local_discr),\n                 bfd.BFD_STATE_NAME[old_state],\n                 bfd.BFD_STATE_NAME[new_state])\n        self._session_state = new_state\n\n        if new_state == bfd.BFD_STATE_DOWN:\n            if diag is not None:\n                self._local_diag = diag\n            self._desired_min_tx_interval = 1000000\n            self._is_polling = True\n            self._update_xmit_period()\n        elif new_state == bfd.BFD_STATE_UP:\n            self._desired_min_tx_interval = self._cfg_desired_min_tx_interval\n            self._is_polling = True\n            self._update_xmit_period()\n\n        self.app.send_event_to_observers(\n            EventBFDSessionStateChanged(self, old_state, new_state))", "response": "Sets the state of the BFD session."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the transmission period of the BFD session.", "response": "def _update_xmit_period(self):\n        \"\"\"\n        Update transmission period of the BFD session.\n        \"\"\"\n        # RFC5880 Section 6.8.7.\n        if self._desired_min_tx_interval > self._remote_min_rx_interval:\n            xmit_period = self._desired_min_tx_interval\n        else:\n            xmit_period = self._remote_min_rx_interval\n\n        # This updates the transmission period of BFD Control packets.\n        # (RFC5880 Section 6.8.2 & 6.8.3.)\n        if self._detect_mult == 1:\n            xmit_period *= random.randint(75, 90) / 100.0\n        else:\n            xmit_period *= random.randint(75, 100) / 100.0\n\n        self._xmit_period = xmit_period / 1000000.0\n        LOG.info(\"[BFD][%s][XMIT] Transmission period changed to %f\",\n                 hex(self._local_discr), self._xmit_period)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a new BFD message.", "response": "def _send(self):\n        \"\"\"\n        BFD packet sender.\n        \"\"\"\n        # If the switch was not connected to controller, exit.\n        if self.datapath is None:\n            return\n\n        # BFD Flags Setup\n        flags = 0\n\n        if self._pending_final:\n            flags |= bfd.BFD_FLAG_FINAL\n            self._pending_final = False\n            self._is_polling = False\n\n        if self._is_polling:\n            flags |= bfd.BFD_FLAG_POLL\n\n        # Authentication Section\n        auth_cls = None\n        if self._auth_type:\n            auth_key_id = list(self._auth_keys.keys())[\n                random.randint(0, len(list(self._auth_keys.keys())) - 1)]\n            auth_key = self._auth_keys[auth_key_id]\n\n            if self._auth_type == bfd.BFD_AUTH_SIMPLE_PASS:\n                auth_cls = bfd.SimplePassword(auth_key_id=auth_key_id,\n                                              password=auth_key)\n\n            if self._auth_type in [bfd.BFD_AUTH_KEYED_MD5,\n                                   bfd.BFD_AUTH_METICULOUS_KEYED_MD5,\n                                   bfd.BFD_AUTH_KEYED_SHA1,\n                                   bfd.BFD_AUTH_METICULOUS_KEYED_SHA1]:\n                if self._auth_type in [bfd.BFD_AUTH_KEYED_MD5,\n                                       bfd.BFD_AUTH_KEYED_SHA1]:\n                    if random.randint(0, 1):\n                        self._xmit_auth_seq = \\\n                            (self._xmit_auth_seq + 1) & UINT32_MAX\n                else:\n                    self._xmit_auth_seq = \\\n                        (self._xmit_auth_seq + 1) & UINT32_MAX\n\n                auth_cls = bfd.bfd._auth_parsers[self._auth_type](\n                    auth_key_id=auth_key_id,\n                    seq=self._xmit_auth_seq,\n                    auth_key=auth_key)\n\n        if auth_cls is not None:\n            flags |= bfd.BFD_FLAG_AUTH_PRESENT\n\n        if self._demand_mode and \\\n                self._session_state == bfd.BFD_STATE_UP and \\\n                self._remote_session_state == bfd.BFD_STATE_UP:\n            flags |= bfd.BFD_FLAG_DEMAND\n\n        diag = self._local_diag\n        state = self._session_state\n        detect_mult = self._detect_mult\n        my_discr = self._local_discr\n        your_discr = self._remote_discr\n        desired_min_tx_interval = self._desired_min_tx_interval\n        required_min_rx_interval = self._required_min_rx_interval\n        required_min_echo_rx_interval = self._cfg_required_min_echo_rx_interval\n\n        # Prepare for Ethernet/IP/UDP header fields\n        src_mac = self.src_mac\n        dst_mac = self.dst_mac\n        src_ip = self.src_ip\n        dst_ip = self.dst_ip\n        self.ipv4_id = (self.ipv4_id + 1) & UINT16_MAX\n        ipv4_id = self.ipv4_id\n        src_port = self.src_port\n        dst_port = self.dst_port\n\n        # Construct BFD Control packet\n        data = BFDPacket.bfd_packet(\n            src_mac=src_mac, dst_mac=dst_mac,\n            src_ip=src_ip, dst_ip=dst_ip, ipv4_id=ipv4_id,\n            src_port=src_port, dst_port=dst_port,\n            diag=diag, state=state, flags=flags, detect_mult=detect_mult,\n            my_discr=my_discr, your_discr=your_discr,\n            desired_min_tx_interval=desired_min_tx_interval,\n            required_min_rx_interval=required_min_rx_interval,\n            required_min_echo_rx_interval=required_min_echo_rx_interval,\n            auth_cls=auth_cls)\n\n        # Prepare for a datapath\n        datapath = self.datapath\n        ofproto = datapath.ofproto\n        parser = datapath.ofproto_parser\n\n        actions = [parser.OFPActionOutput(self.ofport)]\n\n        out = parser.OFPPacketOut(datapath=datapath,\n                                  buffer_id=ofproto.OFP_NO_BUFFER,\n                                  in_port=ofproto.OFPP_CONTROLLER,\n                                  actions=actions,\n                                  data=data)\n\n        datapath.send_msg(out)\n        LOG.debug(\"[BFD][%s][SEND] BFD Control sent.\", hex(self._local_discr))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef bfd_packet(src_mac, dst_mac, src_ip, dst_ip, ipv4_id,\n                   src_port, dst_port,\n                   diag=0, state=0, flags=0, detect_mult=0,\n                   my_discr=0, your_discr=0, desired_min_tx_interval=0,\n                   required_min_rx_interval=0,\n                   required_min_echo_rx_interval=0,\n                   auth_cls=None):\n        \"\"\"\n        Generate BFD packet with Ethernet/IPv4/UDP encapsulated.\n        \"\"\"\n        # Generate ethernet header first.\n        pkt = packet.Packet()\n        eth_pkt = ethernet.ethernet(dst_mac, src_mac, ETH_TYPE_IP)\n        pkt.add_protocol(eth_pkt)\n\n        # IPv4 encapsulation\n        # set ToS to 192 (Network control/CS6)\n        # set TTL to 255 (RFC5881 Section 5.)\n        ipv4_pkt = ipv4.ipv4(proto=inet.IPPROTO_UDP, src=src_ip, dst=dst_ip,\n                             tos=192, identification=ipv4_id, ttl=255)\n        pkt.add_protocol(ipv4_pkt)\n\n        # UDP encapsulation\n        udp_pkt = udp.udp(src_port=src_port, dst_port=dst_port)\n        pkt.add_protocol(udp_pkt)\n\n        # BFD payload\n        bfd_pkt = bfd.bfd(\n            ver=1, diag=diag, state=state, flags=flags,\n            detect_mult=detect_mult,\n            my_discr=my_discr, your_discr=your_discr,\n            desired_min_tx_interval=desired_min_tx_interval,\n            required_min_rx_interval=required_min_rx_interval,\n            required_min_echo_rx_interval=required_min_echo_rx_interval,\n            auth_cls=auth_cls)\n        pkt.add_protocol(bfd_pkt)\n\n        pkt.serialize()\n        return pkt.data", "response": "Generate a BFD packet with Ethernet IPv4 and UDP encapsulated."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bfd_parse(data):\n        pkt = packet.Packet(data)\n        i = iter(pkt)\n        eth_pkt = next(i)\n\n        assert isinstance(eth_pkt, ethernet.ethernet)\n\n        ipv4_pkt = next(i)\n        assert isinstance(ipv4_pkt, ipv4.ipv4)\n\n        udp_pkt = next(i)\n        assert isinstance(udp_pkt, udp.udp)\n\n        udp_payload = next(i)\n\n        return bfd.bfd.parser(udp_payload)[0]", "response": "Parse raw packet and return BFD class from packet library."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ngenerate ARP packet with ethernet encapsulated.", "response": "def arp_packet(opcode, src_mac, src_ip, dst_mac, dst_ip):\n        \"\"\"\n        Generate ARP packet with ethernet encapsulated.\n        \"\"\"\n        # Generate ethernet header first.\n        pkt = packet.Packet()\n        eth_pkt = ethernet.ethernet(dst_mac, src_mac, ETH_TYPE_ARP)\n        pkt.add_protocol(eth_pkt)\n\n        # Use IPv4 ARP wrapper from packet library directly.\n        arp_pkt = arp.arp_ip(opcode, src_mac, src_ip, dst_mac, dst_ip)\n        pkt.add_protocol(arp_pkt)\n\n        pkt.serialize()\n        return pkt.data"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef arp_parse(data):\n        # Iteratize pkt\n        pkt = packet.Packet(data)\n        i = iter(pkt)\n        eth_pkt = next(i)\n        # Ensure it's an ethernet frame.\n        assert isinstance(eth_pkt, ethernet.ethernet)\n\n        arp_pkt = next(i)\n        if not isinstance(arp_pkt, arp.arp):\n            raise ARPPacket.ARPUnknownFormat()\n\n        if arp_pkt.opcode not in (ARP_REQUEST, ARP_REPLY):\n            raise ARPPacket.ARPUnknownFormat(\n                msg='unsupported opcode %d' % arp_pkt.opcode)\n\n        if arp_pkt.proto != ETH_TYPE_IP:\n            raise ARPPacket.ARPUnknownFormat(\n                msg='unsupported arp ethtype 0x%04x' % arp_pkt.proto)\n\n        return arp_pkt", "response": "Parse ARP packet return ARP class from packet library."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_bfd_session(self, dpid, ofport, src_mac, src_ip,\n                        dst_mac=\"FF:FF:FF:FF:FF:FF\", dst_ip=\"255.255.255.255\",\n                        auth_type=0, auth_keys=None):\n        \"\"\"\n        Establish a new BFD session and return My Discriminator of new session.\n\n        Configure the BFD session with the following arguments.\n\n        ================ ======================================================\n        Argument         Description\n        ================ ======================================================\n        dpid             Datapath ID of the BFD interface.\n        ofport           Openflow port number of the BFD interface.\n        src_mac          Source MAC address of the BFD interface.\n        src_ip           Source IPv4 address of the BFD interface.\n        dst_mac          (Optional) Destination MAC address of the BFD\n                         interface.\n        dst_ip           (Optional) Destination IPv4 address of the BFD\n                         interface.\n        auth_type        (Optional) Authentication type.\n        auth_keys        (Optional) A dictionary of authentication key chain\n                         which key is an integer of *Auth Key ID* and value\n                         is a string of *Password* or *Auth Key*.\n        ================ ======================================================\n\n        Example::\n\n            add_bfd_session(dpid=1,\n                            ofport=1,\n                            src_mac=\"01:23:45:67:89:AB\",\n                            src_ip=\"192.168.1.1\",\n                            dst_mac=\"12:34:56:78:9A:BC\",\n                            dst_ip=\"192.168.1.2\",\n                            auth_type=bfd.BFD_AUTH_KEYED_SHA1,\n                            auth_keys={1: \"secret key 1\",\n                                       2: \"secret key 2\"})\n        \"\"\"\n        auth_keys = auth_keys if auth_keys else {}\n        # Generate a unique discriminator\n        while True:\n            # Generate My Discriminator\n            my_discr = random.randint(1, UINT32_MAX)\n\n            # Generate an UDP destination port according to RFC5881 Section 4.\n            src_port = random.randint(49152, 65535)\n\n            # Ensure generated discriminator and UDP port are unique.\n            if my_discr in self.session:\n                continue\n\n            unique_flag = True\n\n            for s in self.session.values():\n                if s.your_discr == my_discr or s.src_port == src_port:\n                    unique_flag = False\n                    break\n\n            if unique_flag:\n                break\n\n        sess = BFDSession(app=self, my_discr=my_discr,\n                          dpid=dpid, ofport=ofport,\n                          src_mac=src_mac, src_ip=src_ip, src_port=src_port,\n                          dst_mac=dst_mac, dst_ip=dst_ip,\n                          auth_type=auth_type, auth_keys=auth_keys)\n\n        self.session[my_discr] = sess\n\n        return my_discr", "response": "This function creates a new BFD session and returns a discriminator of the new session."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_packet(self, primary_ip_address, vlan_id=None):\n        if self.is_ipv6:\n            traffic_class = 0xc0        # set tos to internetwork control\n            flow_label = 0\n            payload_length = ipv6.ipv6._MIN_LEN + len(self)     # XXX _MIN_LEN\n            e = ethernet.ethernet(VRRP_IPV6_DST_MAC_ADDRESS,\n                                  vrrp_ipv6_src_mac_address(self.vrid),\n                                  ether.ETH_TYPE_IPV6)\n            ip = ipv6.ipv6(6, traffic_class, flow_label, payload_length,\n                           inet.IPPROTO_VRRP, VRRP_IPV6_HOP_LIMIT,\n                           primary_ip_address, VRRP_IPV6_DST_ADDRESS)\n        else:\n            header_length = ipv4.ipv4._MIN_LEN // 4      # XXX _MIN_LEN\n            total_length = 0\n            tos = 0xc0  # set tos to internetwork control\n            identification = self.get_identification()\n            e = ethernet.ethernet(VRRP_IPV4_DST_MAC_ADDRESS,\n                                  vrrp_ipv4_src_mac_address(self.vrid),\n                                  ether.ETH_TYPE_IP)\n            ip = ipv4.ipv4(4, header_length, tos, total_length, identification,\n                           0, 0, VRRP_IPV4_TTL, inet.IPPROTO_VRRP, 0,\n                           primary_ip_address, VRRP_IPV4_DST_ADDRESS)\n\n        p = packet.Packet()\n        p.add_protocol(e)\n        if vlan_id is not None:\n            vlan_ = vlan.vlan(0, 0, vlan_id, e.ethertype)\n            e.ethertype = ether.ETH_TYPE_8021Q\n            p.add_protocol(vlan_)\n        p.add_protocol(ip)\n        p.add_protocol(self)\n        return p", "response": "Create a new VRRP packet."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds local prefix from VRF identified by route_dist and sets the source as network controller.", "response": "def add_local(route_dist, prefix, next_hop, route_family=VRF_RF_IPV4):\n    \"\"\"Adds *prefix* from VRF identified by *route_dist* and sets the source as\n    network controller.\n    \"\"\"\n    try:\n        # Create new path and insert into appropriate VRF table.\n        tm = CORE_MANAGER.get_core_service().table_manager\n        label = tm.update_vrf_table(route_dist, prefix, next_hop, route_family)\n        # Currently we only allocate one label per local_prefix,\n        # so we share first label from the list.\n        if label:\n            label = label[0]\n\n        # Send success response with new label.\n        return [{ROUTE_DISTINGUISHER: route_dist, PREFIX: prefix,\n                 VRF_RF: route_family, VPN_LABEL: label}]\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef delete_local(route_dist, prefix, route_family=VRF_RF_IPV4):\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_vrf_table(route_dist, prefix,\n                            route_family=route_family, is_withdraw=True)\n        # Send success response.\n        return [{ROUTE_DISTINGUISHER: route_dist, PREFIX: prefix,\n                 VRF_RF: route_family}]\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)", "response": "Deletes local VRF from local VRF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding EVPN route from VRF identified by route_dist.", "response": "def add_evpn_local(route_type, route_dist, next_hop, **kwargs):\n    \"\"\"Adds EVPN route from VRF identified by *route_dist*.\n    \"\"\"\n\n    if(route_type in [EVPN_ETH_AUTO_DISCOVERY, EVPN_ETH_SEGMENT]\n       and kwargs['esi'] == 0):\n        raise ConfigValueError(conf_name=EVPN_ESI,\n                               conf_value=kwargs['esi'])\n\n    try:\n        # Create new path and insert into appropriate VRF table.\n        tm = CORE_MANAGER.get_core_service().table_manager\n        label = tm.update_vrf_table(route_dist, next_hop=next_hop,\n                                    route_family=VRF_RF_L2_EVPN,\n                                    route_type=route_type, **kwargs)\n        # Currently we only allocate one label per local route,\n        # so we share first label from the list.\n        if label:\n            label = label[0]\n\n        # Send success response with new label.\n        return [{EVPN_ROUTE_TYPE: route_type,\n                 ROUTE_DISTINGUISHER: route_dist,\n                 VRF_RF: VRF_RF_L2_EVPN,\n                 VPN_LABEL: label}.update(kwargs)]\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndeletes EVPN route from VRF identified by route_dist.", "response": "def delete_evpn_local(route_type, route_dist, **kwargs):\n    \"\"\"Deletes/withdraws EVPN route from VRF identified by *route_dist*.\n    \"\"\"\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_vrf_table(route_dist,\n                            route_family=VRF_RF_L2_EVPN,\n                            route_type=route_type, is_withdraw=True, **kwargs)\n        # Send success response.\n        return [{EVPN_ROUTE_TYPE: route_type,\n                 ROUTE_DISTINGUISHER: route_dist,\n                 VRF_RF: VRF_RF_L2_EVPN}.update(kwargs)]\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_flowspec_local(flowspec_family, route_dist, rules, **kwargs):\n    try:\n        # Create new path and insert into appropriate VRF table.\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_flowspec_vrf_table(\n            flowspec_family=flowspec_family, route_dist=route_dist,\n            rules=rules, **kwargs)\n\n        # Send success response.\n        return [{FLOWSPEC_FAMILY: flowspec_family,\n                 ROUTE_DISTINGUISHER: route_dist,\n                 FLOWSPEC_RULES: rules}.update(kwargs)]\n\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)", "response": "Adds Flow Specification route from VRF identified by route_dist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef del_flowspec_local(flowspec_family, route_dist, rules):\n    try:\n        tm = CORE_MANAGER.get_core_service().table_manager\n        tm.update_flowspec_vrf_table(\n            flowspec_family=flowspec_family, route_dist=route_dist,\n            rules=rules, is_withdraw=True)\n\n        # Send success response.\n        return [{FLOWSPEC_FAMILY: flowspec_family,\n                 ROUTE_DISTINGUISHER: route_dist,\n                 FLOWSPEC_RULES: rules}]\n\n    except BgpCoreError as e:\n        raise PrefixError(desc=e)", "response": "Deletes a Flow Specification route from VRF identified by route_dist."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef create_connection(address):\n    host, _port = address\n\n    if ip.valid_ipv4(host) or ip.valid_ipv6(host):\n        return socket.create_connection(address)\n    elif os.path.exists(host):\n        sock = None\n        try:\n            sock = socket.socket(socket.AF_UNIX, socket.SOCK_STREAM)\n            sock.connect(host)\n        except socket.error as e:\n            if sock is not None:\n                sock.close()\n            raise e\n        return sock\n    else:\n        raise ValueError('Invalid IP address or Unix Socket: %s' % host)", "response": "Wrapper for socket. create_connection function."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending a ZEBRA_IPV4_ROUTE_ADD message to the ZEBRA daemon.", "response": "def send_ip_route_add(\n            self, prefix, nexthops=None,\n            safi=packet_safi.UNICAST, flags=zebra.ZEBRA_FLAG_INTERNAL,\n            distance=None, metric=None, mtu=None, tag=None):\n        \"\"\"\n        Sends ZEBRA_IPV4/v6_ROUTE_ADD message to Zebra daemon.\n\n        :param prefix: IPv4/v6 Prefix to advertise.\n        :param nexthops: List of nexthop addresses.\n        :param safi: SAFI to advertise.\n        :param flags: Message flags to advertise. See \"ZEBRA_FLAG_*\".\n        :param distance: (Optional) Distance to advertise.\n        :param metric: (Optional) Metric to advertise.\n        :param mtu: (Optional) MTU size to advertise.\n        :param tag: (Optional) TAG information to advertise.\n        :return: Zebra message instance to be sent. None if failed.\n        \"\"\"\n        try:\n            return self._send_ip_route_impl(\n                prefix=prefix, nexthops=nexthops, safi=safi, flags=flags,\n                distance=distance, metric=metric, mtu=mtu, tag=tag,\n                is_withdraw=False)\n        except ValueError as e:\n            self.logger.exception(\n                'Cannot send IP route add message: %s', e)\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef require_app(app_name, api_style=False):\n    iterable = (inspect.getmodule(frame[0]) for frame in inspect.stack())\n    modules = [module for module in iterable if module is not None]\n    if api_style:\n        m = modules[2]  # skip a frame for \"api\" module\n    else:\n        m = modules[1]\n    m._REQUIRED_APP = getattr(m, '_REQUIRED_APP', [])\n    m._REQUIRED_APP.append(app_name)\n    LOG.debug('require_app: %s is required by %s', app_name, m.__name__)", "response": "Request the application to be automatically loaded."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a list of handlers for the specific event.", "response": "def get_handlers(self, ev, state=None):\n        \"\"\"Returns a list of handlers for the specific event.\n\n        :param ev: The event to handle.\n        :param state: The current state. (\"dispatcher\")\n                      If None is given, returns all handlers for the event.\n                      Otherwise, returns only handlers that are interested\n                      in the specified state.\n                      The default is None.\n        \"\"\"\n        ev_cls = ev.__class__\n        handlers = self.event_handlers.get(ev_cls, [])\n        if state is None:\n            return handlers\n\n        def test(h):\n            if not hasattr(h, 'callers') or ev_cls not in h.callers:\n                # dynamically registered handlers does not have\n                # h.callers element for the event.\n                return True\n            states = h.callers[ev_cls].dispatchers\n            if not states:\n                # empty states means all states\n                return True\n            return state in states\n\n        return filter(test, handlers)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_request(self, req):\n\n        assert isinstance(req, EventRequestBase)\n        req.sync = True\n        req.reply_q = hub.Queue()\n        self.send_event(req.dst, req)\n        # going to sleep for the reply\n        return req.reply_q.get()", "response": "Send a request to a Ryu application specified by req. dst and block until a reply is received."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send_event(self, name, ev, state=None):\n\n        if name in SERVICE_BRICKS:\n            if isinstance(ev, EventRequestBase):\n                ev.src = self.name\n            LOG.debug(\"EVENT %s->%s %s\",\n                      self.name, name, ev.__class__.__name__)\n            SERVICE_BRICKS[name]._send_event(ev, state)\n        else:\n            LOG.debug(\"EVENT LOST %s->%s %s\",\n                      self.name, name, ev.__class__.__name__)", "response": "Send an event to the RyuApp instance specified by name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send_event_to_observers(self, ev, state=None):\n\n        for observer in self.get_observers(ev, state):\n            self.send_event(observer, ev, state)", "response": "Send an event to all the observers of this RyuApp."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reply_to_request(self, req, rep):\n\n        assert isinstance(req, EventRequestBase)\n        assert isinstance(rep, EventReplyBase)\n        rep.dst = req.src\n        if req.sync:\n            req.reply_q.put(rep)\n        else:\n            self.send_event(rep.dst, rep)", "response": "Send a reply to a request."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning a set of Ryu applications A convenient method to load and instantiate apps. This blocks until all relevant apps stop.", "response": "def run_apps(app_lists):\n        \"\"\"Run a set of Ryu applications\n\n        A convenient method to load and instantiate apps.\n        This blocks until all relevant apps stop.\n        \"\"\"\n        app_mgr = AppManager.get_instance()\n        app_mgr.load_apps(app_lists)\n        contexts = app_mgr.create_contexts()\n        services = app_mgr.instantiate_apps(**contexts)\n        webapp = wsgi.start_service(app_mgr)\n        if webapp:\n            services.append(hub.spawn(webapp))\n        try:\n            hub.joinall(services)\n        finally:\n            app_mgr.close()\n            for t in services:\n                t.kill()\n            hub.joinall(services)\n            gc.collect()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_list_cap(self, line):\n\n        def f(p, args):\n            for i in p.netconf.server_capabilities:\n                print(i)\n\n        self._request(line, f)", "response": "list_cap - list all available server capabilities"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef do_raw_get(self, line):\n\n        def f(p, args):\n            result = p.raw_get()\n            tree = ET.fromstring(result)\n            validate(tree)\n            print(et_tostring_pp(tree))\n\n        self._request(line, f)", "response": "raw_get - Get the current node s attributes"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget - get a specific object", "response": "def do_get(self, line):\n        \"\"\"get <peer>\n        eg. get sw1\n        \"\"\"\n\n        def f(p, args):\n            print(p.get())\n\n        self._request(line, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_commit(self, line):\n\n        def f(p, args):\n            print(p.commit())\n\n        self._request(line, f)", "response": "commit - commit the current state of the current user"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndiscard <peer > eg. discard sw1 eg. discard sw1 eg. discard sw1 eg. discard sw1 eg. discard sw1 eg. discard sw1 eg. discard sw1", "response": "def do_discard(self, line):\n        \"\"\"discard <peer>\n        eg. discard sw1\n        \"\"\"\n\n        def f(p, args):\n            print(p.discard_changes())\n\n        self._request(line, f)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef do_get_config(self, line):\n\n        def f(p, args):\n            try:\n                source = args[0]\n            except:\n                print(\"argument error\")\n                return\n            print(p.get_config(source))\n\n        self._request(line, f)", "response": "get_config - get the configuration of a specific node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_list_port(self, line):\n\n        def f(p, args):\n            o = p.get()\n            for p in o.resources.port:\n                print('%s %s %s' % (p.resource_id, p.name, p.number))\n\n        self._request(line, f)", "response": "list_port - list all available ports"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_set_port_config(self, line):\n\n        def f(p, args):\n            try:\n                target, port, key, value = args\n            except:\n                print(\"argument error\")\n                print(args)\n                return\n\n            # get switch id\n            o = p.get()\n            capable_switch_id = o.id\n\n            try:\n                capable_switch = ofc.OFCapableSwitchType(\n                    id=capable_switch_id,\n                    resources=ofc.OFCapableSwitchResourcesType(\n                        port=[\n                            ofc.OFPortType(\n                                resource_id=port,\n                                configuration=ofc.OFPortConfigurationType(\n                                    **{key: value}))\n                        ]\n                    )\n                )\n            except TypeError:\n                print(\"argument error\")\n                return\n            try:\n                p.edit_config(target, capable_switch)\n            except Exception as e:\n                print(e)\n\n        self._request(line, f)", "response": "set_port_config - Set the port configuration of a specific instance of a specific port"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_set_queue_config(self, line):\n\n        def f(p, args):\n            try:\n                target, queue, key, value = args\n            except:\n                print(\"argument error\")\n                print(args)\n                return\n\n            # get switch id\n            o = p.get()\n            capable_switch_id = o.id\n\n            try:\n                capable_switch = ofc.OFCapableSwitchType(\n                    id=capable_switch_id,\n                    resources=ofc.OFCapableSwitchResourcesType(\n                        queue=[\n                            ofc.OFQueueType(\n                                resource_id=queue,\n                                properties=ofc.OFQueuePropertiesType(\n                                    **{key: value})),\n                        ]\n                    )\n                )\n            except TypeError:\n                print(\"argument error\")\n                return\n            try:\n                p.edit_config(target, capable_switch)\n            except Exception as e:\n                print(e)\n\n        self._request(line, f)", "response": "set_queue_config - set queue config"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_set_logical_switch_config(self, line):\n\n        def f(p, args):\n            try:\n                target, lsw, key, value = args\n            except:\n                print(\"argument error\")\n                return\n\n            # get switch id\n            o = p.get_config(target)\n            capable_switch_id = o.id\n\n            try:\n                capable_switch = ofc.OFCapableSwitchType(\n                    id=capable_switch_id,\n                    logical_switches=ofc.OFCapableSwitchLogicalSwitchesType(\n                        switch=[ofc.OFLogicalSwitchType(\n                            id=lsw,\n                            **{key: value}\n                        )]\n                    )\n                )\n            except TypeError:\n                print(\"argument error\")\n                return\n            try:\n                p.edit_config(target, capable_switch)\n            except Exception as e:\n                print(e)\n\n        self._request(line, f)", "response": "set_logical_switch_config - set the logical switch config"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_ev_cls(ev_cls, dispatchers=None):\n    def _set_ev_cls_dec(handler):\n        if 'callers' not in dir(handler):\n            handler.callers = {}\n        for e in _listify(ev_cls):\n            handler.callers[e] = _Caller(_listify(dispatchers), e.__module__)\n        return handler\n    return _set_ev_cls_dec", "response": "Decorator for Ryu application to declare an event handler class."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_service(service):\n    frame = inspect.currentframe()\n    m_name = frame.f_back.f_globals['__name__']\n    m = sys.modules[m_name]\n    m._SERVICE_NAME = service", "response": "Register the service application in the current module."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_neighbor_conf(neigh_ip_address):\n    neigh_conf = \\\n        CORE_MANAGER.neighbors_conf.get_neighbor_conf(neigh_ip_address)\n    if not neigh_conf:\n        raise RuntimeConfigError(desc='No Neighbor configuration with IP'\n                                 ' address %s' % neigh_ip_address)\n    assert isinstance(neigh_conf, NeighborConf)\n    return neigh_conf", "response": "Returns neighbor configuration for given neighbor ip address. Raises exception if no neighbor with given ip address exists."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a neighbor in_filter for given ip address if exists.", "response": "def get_neighbor_in_filter(neigh_ip_address):\n    \"\"\"Returns a neighbor in_filter for given ip address if exists.\"\"\"\n    core = CORE_MANAGER.get_core_service()\n    peer = core.peer_manager.get_by_addr(neigh_ip_address)\n    return peer.in_filters"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_neighbor_in_filter(neigh_ip_address, filters):\n    core = CORE_MANAGER.get_core_service()\n    peer = core.peer_manager.get_by_addr(neigh_ip_address)\n    peer.in_filters = filters\n    return True", "response": "Returns a neighbor in_filter for given ip address if exists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_neighbor_out_filter(neigh_ip_address):\n    core = CORE_MANAGER.get_core_service()\n    ret = core.peer_manager.get_by_addr(neigh_ip_address).out_filters\n    return ret", "response": "Returns a neighbor out_filter for given ip address if exists."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_neighbor_out_filter(neigh_ip_address, filters):\n    core = CORE_MANAGER.get_core_service()\n    peer = core.peer_manager.get_by_addr(neigh_ip_address)\n    peer.out_filters = filters\n    return True", "response": "Sets the out_filter of a neighbor."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef set_neighbor_attribute_map(neigh_ip_address, at_maps,\n                               route_dist=None, route_family=VRF_RF_IPV4):\n    \"\"\"set attribute_maps to the neighbor.\"\"\"\n    core = CORE_MANAGER.get_core_service()\n    peer = core.peer_manager.get_by_addr(neigh_ip_address)\n\n    at_maps_key = const.ATTR_MAPS_LABEL_DEFAULT\n    at_maps_dict = {}\n\n    if route_dist is not None:\n        vrf_conf =\\\n            CORE_MANAGER.vrfs_conf.get_vrf_conf(route_dist, route_family)\n        if vrf_conf:\n            at_maps_key = ':'.join([route_dist, route_family])\n        else:\n            raise RuntimeConfigError(desc='No VrfConf with rd %s' %\n                                          route_dist)\n\n    at_maps_dict[const.ATTR_MAPS_LABEL_KEY] = at_maps_key\n    at_maps_dict[const.ATTR_MAPS_VALUE] = at_maps\n    peer.attribute_maps = at_maps_dict\n\n    return True", "response": "set attribute_maps to the neighbor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_neighbor_attribute_map(neigh_ip_address, route_dist=None,\n                               route_family=VRF_RF_IPV4):\n    \"\"\"Returns a neighbor attribute_map for given ip address if exists.\"\"\"\n    core = CORE_MANAGER.get_core_service()\n    peer = core.peer_manager.get_by_addr(neigh_ip_address)\n    at_maps_key = const.ATTR_MAPS_LABEL_DEFAULT\n\n    if route_dist is not None:\n        at_maps_key = ':'.join([route_dist, route_family])\n    at_maps = peer.attribute_maps.get(at_maps_key)\n    if at_maps:\n        return at_maps.get(const.ATTR_MAPS_ORG_KEY)\n    else:\n        return []", "response": "Returns a neighbor attribute_map for given ip address if exists."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(**kwargs):\n    def decorator(func):\n        _CALL_REGISTRY[kwargs.get(API_SYM, func.__name__)] = func\n        return func\n\n    return decorator", "response": "Decorator for registering API function."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls BGPS public API identified by given symbol and passes given kwargs as param.", "response": "def call(symbol, **kwargs):\n    \"\"\"Calls/executes BGPS public API identified by given symbol and passes\n    given kwargs as param.\n    \"\"\"\n    LOG.info(\"API method %s called with args: %s\", symbol, str(kwargs))\n\n    # TODO(PH, JK) improve the way api function modules are loaded\n    from . import all  # noqa\n    if not is_call_registered(symbol):\n        message = 'Did not find any method registered by symbol %s' % symbol\n        raise MethodNotFound(message)\n\n    if not symbol.startswith('core') and not CORE_MANAGER.started:\n        raise CoreNotStarted(desc='CoreManager is not active.')\n\n    call = get_call(symbol)\n    try:\n        return call(**kwargs)\n    except BGPSException as r:\n        LOG.error(traceback.format_exc())\n        raise r\n    except Exception as e:\n        LOG.error(traceback.format_exc())\n        raise ApiException(desc=str(e))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _error_to_jsondict(mod, type_, code):\n    (t_name, c_name) = _get_error_names(mod, type_, code)\n    return {'type': '%s(%d)' % (t_name, type_),\n            'code': '%s(%d)' % (c_name, code)}", "response": "This method returns a json dict for given error type and code."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a copy of current settings.", "response": "def settings(self):\n        \"\"\"Returns a copy of current settings.\n\n        As some of the attributes are themselves containers, we clone the\n        settings to provide clones for those containers as well.\n        \"\"\"\n        # Shallow copy first\n        cloned_setting = self._settings.copy()\n        # Don't want clone to link to same RT containers\n        cloned_setting[IMPORT_RTS] = self.import_rts\n        cloned_setting[EXPORT_RTS] = self.export_rts\n        cloned_setting[SITE_OF_ORIGINS] = self.soo_list\n        return cloned_setting"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef update(self, **kwargs):\n        # Update inherited configurations\n        super(VrfConf, self).update(**kwargs)\n        vrf_id = kwargs.get(ConfWithId.ID)\n        vrf_rd = kwargs.get(ROUTE_DISTINGUISHER)\n        vrf_rf = kwargs.get(VRF_RF)\n        if (vrf_id != self.id or\n                vrf_rd != self.route_dist or\n                vrf_rf != self.route_family):\n            raise ConfigValueError(desc='id/route-distinguisher/route-family'\n                                   ' do not match configured value.')\n\n        # Validate and update individual settings\n        new_imp_rts, old_imp_rts = \\\n            self._update_import_rts(**kwargs)\n        export_rts_changed = self._update_export_rts(**kwargs)\n        soos_list_changed = self._update_soo_list(**kwargs)\n        med_changed = self._update_med(**kwargs)\n        re_export_needed = (export_rts_changed or\n                            soos_list_changed or\n                            med_changed)\n        import_maps = kwargs.get(IMPORT_MAPS, [])\n        re_import_needed = self._update_importmaps(import_maps)\n\n        # If we did have any change in value of any settings, we notify\n        # listeners\n        if (new_imp_rts is not None or\n                old_imp_rts is not None or\n                re_export_needed or re_import_needed):\n            evt_value = (\n                new_imp_rts,\n                old_imp_rts,\n                import_maps,\n                re_export_needed,\n                re_import_needed\n            )\n            self._notify_listeners(VrfConf.VRF_CHG_EVT, evt_value)\n        return True", "response": "Updates this VrfConf settings. Returns True if update was successful."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nremoves VRFConf for given route_dist or vrf_id or vrf_rf.", "response": "def remove_vrf_conf(self, route_dist=None, vrf_id=None,\n                        vrf_rf=None):\n        \"\"\"Removes any matching `VrfConf` for given `route_dist` or `vrf_id`\n\n        Parameters:\n            - `route_dist`: (str) route distinguisher of a configured VRF\n            - `vrf_id`: (str) vrf ID\n            - `vrf_rf`: (str) route family of the VRF configuration\n        If only `route_dist` is given, removes `VrfConf`s for all supported\n        address families for this `route_dist`. If `vrf_rf` is given, than only\n        removes `VrfConf` for that specific route family. If only `vrf_id` is\n        given, matching `VrfConf` will be removed.\n        \"\"\"\n        if route_dist is None and vrf_id is None:\n            raise RuntimeConfigError(desc='To delete supply route_dist or id.')\n\n        # By default we remove all VRFs for given Id or RD\n        vrf_rfs = SUPPORTED_VRF_RF\n        # If asked to delete specific route family vrf conf.\n        if vrf_rf:\n            vrf_rfs = vrf_rf\n\n        # For all vrf route family asked to be deleted, we collect all deleted\n        # VrfConfs\n        removed_vrf_confs = []\n        for route_family in vrf_rfs:\n            if route_dist is not None:\n                rd_rf_id = VrfConf.create_rd_rf_id(route_dist, route_family)\n                vrf_conf = self._vrfs_by_rd_rf.pop(rd_rf_id, None)\n                if vrf_conf:\n                    self._vrfs_by_id.pop(vrf_conf.id, None)\n                    removed_vrf_confs.append(vrf_conf)\n            else:\n                vrf_conf = self._vrfs_by_id.pop(vrf_id, None)\n                if vrf_conf:\n                    self._vrfs_by_rd_rf.pop(vrf_conf.rd_rd_id, None)\n                    removed_vrf_confs.append(vrf_conf)\n\n        # We do not raise any exception if we cannot find asked VRF.\n        for vrf_conf in removed_vrf_confs:\n            self._notify_listeners(VrfsConf.REMOVE_VRF_CONF_EVT, vrf_conf)\n        return removed_vrf_confs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef packet_in_handler(self, evt):\n        msg = evt.msg\n        dpid = msg.datapath.id\n\n        req_pkt = packet.Packet(msg.data)\n        req_igmp = req_pkt.get_protocol(igmp.igmp)\n        if req_igmp:\n            if self._querier.dpid == dpid:\n                self._querier.packet_in_handler(req_igmp, msg)\n            else:\n                self._snooper.packet_in_handler(req_pkt, req_igmp, msg)\n        else:\n            self.send_event_to_observers(EventPacketIn(msg))", "response": "PacketIn event handler. when the received packet was IGMP proceed it. otherwise send a event."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef state_change_handler(self, evt):\n        datapath = evt.datapath\n        assert datapath is not None\n        if datapath.id == self._querier.dpid:\n            if evt.state == MAIN_DISPATCHER:\n                self._querier.start_loop(datapath)\n            elif evt.state == DEAD_DISPATCHER:\n                self._querier.stop_loop()", "response": "State change event handler."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _set_flow_entry(self, datapath, actions, in_port, dst, src=None):\n        set_flow = self._set_flow_func.get(datapath.ofproto.OFP_VERSION)\n        assert set_flow\n        set_flow(datapath, actions, in_port, dst, src)", "response": "set a flow entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves a flow entry.", "response": "def _del_flow_entry(self, datapath, in_port, dst, src=None):\n        \"\"\"remove a flow entry.\"\"\"\n        del_flow = self._del_flow_func.get(datapath.ofproto.OFP_VERSION)\n        assert del_flow\n        del_flow(datapath, in_port, dst, src)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _do_packet_out(self, datapath, data, in_port, actions):\n        ofproto = datapath.ofproto\n        parser = datapath.ofproto_parser\n\n        out = parser.OFPPacketOut(\n            datapath=datapath, buffer_id=ofproto.OFP_NO_BUFFER,\n            data=data, in_port=in_port, actions=actions)\n        datapath.send_msg(out)", "response": "send a packet out."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _ipv4_text_to_int(self, ip_text):\n        if ip_text is None:\n            return None\n        assert isinstance(ip_text, str)\n        return struct.unpack('!I', addrconv.ipv4.text_to_bin(ip_text))[0]", "response": "convert ip v4 string to integer"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the datapath to work as a querier", "response": "def set_querier_mode(self, dpid, server_port):\n        \"\"\"set the datapath to work as a querier. note that you can set\n        up only the one querier. when you called this method several\n        times, only the last one becomes effective.\"\"\"\n        self.dpid = dpid\n        self.server_port = server_port\n        if self._querier_thread:\n            hub.kill(self._querier_thread)\n            self._querier_thread = None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef packet_in_handler(self, req_igmp, msg):\n        ofproto = msg.datapath.ofproto\n        if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:\n            in_port = msg.in_port\n        else:\n            in_port = msg.match['in_port']\n        if (igmp.IGMP_TYPE_REPORT_V1 == req_igmp.msgtype or\n                igmp.IGMP_TYPE_REPORT_V2 == req_igmp.msgtype):\n            self._do_report(req_igmp, in_port, msg)\n        elif igmp.IGMP_TYPE_LEAVE == req_igmp.msgtype:\n            self._do_leave(req_igmp, in_port, msg)", "response": "the process when the querier received an IGMP."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts a QUERY thread.", "response": "def start_loop(self, datapath):\n        \"\"\"start QUERY thread.\"\"\"\n        self._datapath = datapath\n        self._querier_thread = hub.spawn(self._send_query)\n        self.logger.info(\"started a querier.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a QUERY message periodically.", "response": "def _send_query(self):\n        \"\"\" send a QUERY message periodically.\"\"\"\n        timeout = 60\n        ofproto = self._datapath.ofproto\n        parser = self._datapath.ofproto_parser\n        if ofproto_v1_0.OFP_VERSION == ofproto.OFP_VERSION:\n            send_port = ofproto.OFPP_NONE\n        else:\n            send_port = ofproto.OFPP_ANY\n\n        # create a general query.\n        res_igmp = igmp.igmp(\n            msgtype=igmp.IGMP_TYPE_QUERY,\n            maxresp=igmp.QUERY_RESPONSE_INTERVAL * 10,\n            csum=0,\n            address='0.0.0.0')\n        res_ipv4 = ipv4.ipv4(\n            total_length=len(ipv4.ipv4()) + len(res_igmp),\n            proto=inet.IPPROTO_IGMP, ttl=1,\n            src='0.0.0.0',\n            dst=igmp.MULTICAST_IP_ALL_HOST)\n        res_ether = ethernet.ethernet(\n            dst=igmp.MULTICAST_MAC_ALL_HOST,\n            src=self._datapath.ports[ofproto.OFPP_LOCAL].hw_addr,\n            ethertype=ether.ETH_TYPE_IP)\n        res_pkt = packet.Packet()\n        res_pkt.add_protocol(res_ether)\n        res_pkt.add_protocol(res_ipv4)\n        res_pkt.add_protocol(res_igmp)\n        res_pkt.serialize()\n\n        flood = [parser.OFPActionOutput(ofproto.OFPP_FLOOD)]\n\n        while True:\n            # reset reply status.\n            for status in self._mcast.values():\n                for port in status.keys():\n                    status[port] = False\n\n            # send a general query to the host that sent this message.\n            self._do_packet_out(\n                self._datapath, res_pkt.data, send_port, flood)\n            hub.sleep(igmp.QUERY_RESPONSE_INTERVAL)\n\n            # QUERY timeout expired.\n            del_groups = []\n            for group, status in self._mcast.items():\n                del_ports = []\n                actions = []\n                for port in status.keys():\n                    if not status[port]:\n                        del_ports.append(port)\n                    else:\n                        actions.append(parser.OFPActionOutput(port))\n                if len(actions) and len(del_ports):\n                    self._set_flow_entry(\n                        self._datapath, actions, self.server_port, group)\n                if not len(actions):\n                    self._del_flow_entry(\n                        self._datapath, self.server_port, group)\n                    del_groups.append(group)\n                if len(del_ports):\n                    for port in del_ports:\n                        self._del_flow_entry(self._datapath, port, group)\n                for port in del_ports:\n                    del status[port]\n            for group in del_groups:\n                del self._mcast[group]\n\n            rest_time = timeout - igmp.QUERY_RESPONSE_INTERVAL\n            hub.sleep(rest_time)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _do_leave(self, leave, in_port, msg):\n        datapath = msg.datapath\n        parser = datapath.ofproto_parser\n\n        self._mcast.setdefault(leave.address, {})\n        if in_port in self._mcast[leave.address]:\n            self._del_flow_entry(\n                datapath, in_port, leave.address)\n            del self._mcast[leave.address][in_port]\n            actions = []\n            for port in self._mcast[leave.address]:\n                actions.append(parser.OFPActionOutput(port))\n            if len(actions):\n                self._set_flow_entry(\n                    datapath, actions, self.server_port, leave.address)\n            else:\n                self._del_flow_entry(\n                    datapath, self.server_port, leave.address)", "response": "the process when the querier received a LEAVE message."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef packet_in_handler(self, req_pkt, req_igmp, msg):\n        dpid = msg.datapath.id\n        ofproto = msg.datapath.ofproto\n        if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:\n            in_port = msg.in_port\n        else:\n            in_port = msg.match['in_port']\n\n        log = \"SW=%s PORT=%d IGMP received. \" % (\n            dpid_to_str(dpid), in_port)\n        self.logger.debug(str(req_igmp))\n        if igmp.IGMP_TYPE_QUERY == req_igmp.msgtype:\n            self.logger.info(log + \"[QUERY]\")\n            (req_ipv4, ) = req_pkt.get_protocols(ipv4.ipv4)\n            (req_eth, ) = req_pkt.get_protocols(ethernet.ethernet)\n            self._do_query(req_igmp, req_ipv4, req_eth, in_port, msg)\n        elif (igmp.IGMP_TYPE_REPORT_V1 == req_igmp.msgtype or\n              igmp.IGMP_TYPE_REPORT_V2 == req_igmp.msgtype):\n            self.logger.info(log + \"[REPORT]\")\n            self._do_report(req_igmp, in_port, msg)\n        elif igmp.IGMP_TYPE_LEAVE == req_igmp.msgtype:\n            self.logger.info(log + \"[LEAVE]\")\n            self._do_leave(req_igmp, in_port, msg)\n        elif igmp.IGMP_TYPE_REPORT_V3 == req_igmp.msgtype:\n            self.logger.info(log + \"V3 is not supported yet.\")\n            self._do_flood(in_port, msg)\n        else:\n            self.logger.info(log + \"[unknown type:%d]\",\n                             req_igmp.msgtype)\n            self._do_flood(in_port, msg)", "response": "the process when the snooper received a IGMP."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _do_report(self, report, in_port, msg):\n        datapath = msg.datapath\n        dpid = datapath.id\n        ofproto = datapath.ofproto\n        parser = datapath.ofproto_parser\n\n        if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:\n            size = 65535\n        else:\n            size = ofproto.OFPCML_MAX\n\n        # check whether the querier port has been specified.\n        outport = None\n        value = self._to_querier.get(dpid)\n        if value:\n            outport = value['port']\n\n        # send a event when the multicast group address is new.\n        self._to_hosts.setdefault(dpid, {})\n        if not self._to_hosts[dpid].get(report.address):\n            self._send_event(\n                EventMulticastGroupStateChanged(\n                    MG_GROUP_ADDED, report.address, outport, []))\n            self._to_hosts[dpid].setdefault(\n                report.address,\n                {'replied': False, 'leave': None, 'ports': {}})\n\n        # set a flow entry from a host to the controller when\n        # a host sent a REPORT message.\n        if not self._to_hosts[dpid][report.address]['ports'].get(\n                in_port):\n            self._to_hosts[dpid][report.address]['ports'][\n                in_port] = {'out': False, 'in': False}\n            self._set_flow_entry(\n                datapath,\n                [parser.OFPActionOutput(ofproto.OFPP_CONTROLLER, size)],\n                in_port, report.address)\n\n        if not self._to_hosts[dpid][report.address]['ports'][\n                in_port]['out']:\n            self._to_hosts[dpid][report.address]['ports'][\n                in_port]['out'] = True\n\n        if not outport:\n            self.logger.info(\"no querier exists.\")\n            return\n\n        # set a flow entry from a multicast server to hosts.\n        if not self._to_hosts[dpid][report.address]['ports'][\n                in_port]['in']:\n            actions = []\n            ports = []\n            for port in self._to_hosts[dpid][report.address]['ports']:\n                actions.append(parser.OFPActionOutput(port))\n                ports.append(port)\n            self._send_event(\n                EventMulticastGroupStateChanged(\n                    MG_MEMBER_CHANGED, report.address, outport, ports))\n            self._set_flow_entry(\n                datapath, actions, outport, report.address)\n            self._to_hosts[dpid][report.address]['ports'][\n                in_port]['in'] = True\n\n        # send a REPORT message to the querier if this message arrived\n        # first after a QUERY message was sent.\n        if not self._to_hosts[dpid][report.address]['replied']:\n            actions = [parser.OFPActionOutput(outport, size)]\n            self._do_packet_out(datapath, msg.data, in_port, actions)\n            self._to_hosts[dpid][report.address]['replied'] = True", "response": "the process when the snooper received a REPORT message."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _do_timeout_for_query(self, timeout, datapath):\n        dpid = datapath.id\n\n        hub.sleep(timeout)\n        outport = self._to_querier[dpid]['port']\n\n        remove_dsts = []\n        for dst in self._to_hosts[dpid]:\n            if not self._to_hosts[dpid][dst]['replied']:\n                # if no REPORT message sent from any members of\n                # the group, remove flow entries about the group and\n                # send a LEAVE message if exists.\n                self._remove_multicast_group(datapath, outport, dst)\n                remove_dsts.append(dst)\n\n        for dst in remove_dsts:\n            del self._to_hosts[dpid][dst]", "response": "the process when the QUERY from the querier timeout expired."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _remove_multicast_group(self, datapath, outport, dst):\n        ofproto = datapath.ofproto\n        parser = datapath.ofproto_parser\n        dpid = datapath.id\n\n        self._send_event(\n            EventMulticastGroupStateChanged(\n                MG_GROUP_REMOVED, dst, outport, []))\n        self._del_flow_entry(datapath, outport, dst)\n        for port in self._to_hosts[dpid][dst]['ports']:\n            self._del_flow_entry(datapath, port, dst)\n        leave = self._to_hosts[dpid][dst]['leave']\n        if leave:\n            if ofproto.OFP_VERSION == ofproto_v1_0.OFP_VERSION:\n                in_port = leave.in_port\n            else:\n                in_port = leave.match['in_port']\n            actions = [parser.OFPActionOutput(outport)]\n            self._do_packet_out(\n                datapath, leave.data, in_port, actions)", "response": "remove flow entries about the group and send a LEAVE message if exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the set of RTC AS entries for the given set of resources.", "response": "def update_rtc_as_set(self):\n        \"\"\"Syncs RT NLRIs for new and removed RTC_ASes.\n\n        This method should be called when a neighbor is added or removed.\n        \"\"\"\n        # Compute the diffs in RTC_ASes\n        curr_rtc_as_set = self._neighbors_conf.rtc_as_set\n        # Always add local AS to RTC AS set\n        curr_rtc_as_set.add(self._core_service.asn)\n        removed_rtc_as_set = self._all_rtc_as_set - curr_rtc_as_set\n        new_rtc_as_set = curr_rtc_as_set - self._all_rtc_as_set\n\n        # Update to new RTC_AS set\n        self._all_rtc_as_set = curr_rtc_as_set\n\n        # Sync RT NLRI by adding/withdrawing as appropriate\n        for new_rtc_as in new_rtc_as_set:\n            for import_rt in self._all_vrfs_import_rts_set:\n                self._add_rt_nlri_for_as(new_rtc_as, import_rt)\n        for removed_rtc_as in removed_rtc_as_set:\n            for import_rt in self._all_vrfs_import_rts_set:\n                self._add_rt_nlri_for_as(removed_rtc_as, import_rt,\n                                         is_withdraw=True)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate all local RT NLRIs based on all configured VRFs.", "response": "def update_local_rt_nlris(self):\n        \"\"\"Does book-keeping of local RT NLRIs based on all configured VRFs.\n\n        Syncs all import RTs and RT NLRIs.\n        The method should be called when any VRFs are added/removed/changed.\n        \"\"\"\n        current_conf_import_rts = set()\n        for vrf in self._vrfs_conf.vrf_confs:\n            current_conf_import_rts.update(vrf.import_rts)\n\n        removed_rts = self._all_vrfs_import_rts_set - current_conf_import_rts\n        new_rts = current_conf_import_rts - self._all_vrfs_import_rts_set\n        self._all_vrfs_import_rts_set = current_conf_import_rts\n\n        # Add new and withdraw removed local RtNlris\n        for new_rt in new_rts:\n            self.add_rt_nlri(new_rt)\n        for removed_rt in removed_rts:\n            self.add_rt_nlri(removed_rt, is_withdraw=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _compute_global_interested_rts(self):\n        interested_rts = set()\n        for rtfilter in self._peer_to_rtfilter_map.values():\n            interested_rts.update(rtfilter)\n\n        interested_rts.update(self._vrfs_conf.vrf_interested_rts)\n        # Remove default RT as it is not a valid RT for paths\n        # TODO(PH): Check if we have better alternative than add and remove\n        interested_rts.add(RouteTargetMembershipNLRI.DEFAULT_RT)\n        interested_rts.remove(RouteTargetMembershipNLRI.DEFAULT_RT)\n        return interested_rts", "response": "Computes current global interested RTs based on peers and RT filters."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nupdate the list of interesting RTs for this entry.", "response": "def update_interested_rts(self):\n        \"\"\"Updates interested RT list.\n\n        Check if interested RTs have changes from previous check.\n        Takes appropriate action for new interesting RTs and removal of un-\n        interesting RTs.\n        \"\"\"\n        prev_global_rts = self._global_interested_rts\n        curr_global_rts = self._compute_global_interested_rts()\n\n        new_global_rts = curr_global_rts - prev_global_rts\n        removed_global_rts = prev_global_rts - curr_global_rts\n\n        # Update current interested RTs for next iteration\n        self._global_interested_rts = curr_global_rts\n\n        LOG.debug('Global Interested RT changed, new RTs %s, removed RTs %s',\n                  new_global_rts, removed_global_rts)\n        tm = self._core_service.table_manager\n        tm.on_interesting_rts_change(new_global_rts, removed_global_rts)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nconvert binary or int type representation of IPv4 address to human readable IPv4 string.", "response": "def ipv4_to_str(ip):\n    \"\"\"\n    Converts binary or int type representation to human readable IPv4 string.\n    :param ip: binary or int type representation of IPv4 address\n    :return: IPv4 address string\n    \"\"\"\n    if isinstance(ip, numbers.Integral):\n        return addrconv.ipv4.bin_to_text(struct.pack(\"!I\", ip))\n    else:\n        return addrconv.ipv4.bin_to_text(ip)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef ipv6_to_str(ip):\n    if isinstance(ip, numbers.Integral):\n        return addrconv.ipv6.bin_to_text(type_desc.Int16.from_user(ip))\n    else:\n        return addrconv.ipv6.bin_to_text(ip)", "response": "Converts binary or int type representation of IPv6 address to human readable IPv6 string."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef bin_to_text(ip):\n    if len(ip) == 4:\n        return ipv4_to_str(ip)\n    elif len(ip) == 16:\n        return ipv6_to_str(ip)\n    else:\n        raise struct.error('Invalid ip address length: %s' % len(ip))", "response": "Converts binary representation of IPv4 or IPv6 address to human readable IPv4 or IPv6 string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef do_request(self, line):\n\n        def f(p, method, params):\n            result = p.call(method, params)\n            print(\"RESULT %s\" % result)\n\n        self._request(line, f)", "response": "request msgpack - rpc method"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_notify(self, line):\n\n        def f(p, method, params):\n            p.send_notification(method, params)\n\n        self._request(line, f)", "response": "send a msgpack - rpc notification."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a dict expressing the flow match.", "response": "def to_jsondict(self):\n        \"\"\"\n        Returns a dict expressing the flow match.\n        \"\"\"\n        # XXX old api compat\n        if self._composed_with_old_api():\n            # copy object first because serialize_old is destructive\n            o2 = OFPMatch()\n            o2.fields = self.fields[:]\n            # serialize and parse to fill OFPMatch._fields2\n            buf = bytearray()\n            o2.serialize(buf, 0)\n            o = OFPMatch.parser(six.binary_type(buf), 0)\n        else:\n            o = self\n\n        body = {\"oxm_fields\": [ofproto.oxm_to_jsondict(k, uv) for k, uv\n                               in o._fields2],\n                \"length\": o.length,\n                \"type\": o.type}\n        return {self.__class__.__name__: body}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef from_jsondict(cls, dict_):\n        fields = [ofproto.oxm_from_jsondict(f) for f\n                  in dict_['oxm_fields']]\n        o = OFPMatch(_ordered_fields=fields)\n        # XXX old api compat\n        # serialize and parse to fill OFPMatch.fields\n        buf = bytearray()\n        o.serialize(buf, 0)\n        return OFPMatch.parser(six.binary_type(buf), 0)", "response": "Returns an object which is generated from a dict containing a dict containing the keys oxm_fields of the match."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef append_field(self, header, value, mask=None):\n        self.fields.append(OFPMatchField.make(header, value, mask))", "response": "Append a match field to the current match field."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the total number of UPDATE NOTIFICATION ROUTE_REFRESH message sent to this peer.", "response": "def total_msg_sent(self):\n        \"\"\"Returns total number of UPDATE, NOTIFICATION and ROUTE_REFRESH\n         message sent to this peer.\n         \"\"\"\n        return (self.get_count(PeerCounterNames.SENT_REFRESH) +\n                self.get_count(PeerCounterNames.SENT_UPDATES))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef total_msg_recv(self):\n        return (self.get_count(PeerCounterNames.RECV_UPDATES) +\n                self.get_count(PeerCounterNames.RECV_REFRESH) +\n                self.get_count(PeerCounterNames.RECV_NOTIFICATION))", "response": "Returns the total number of UPDATE NOTIFICATION and ROUTE_REFRESH messages received from this peer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_stats_summary_dict(self):\n        uptime = time.time() - self._established_time \\\n            if self._established_time != 0 else -1\n        return {\n            stats.UPDATE_MSG_IN: self.get_count(PeerCounterNames.RECV_UPDATES),\n            stats.UPDATE_MSG_OUT: self.get_count(\n                PeerCounterNames.SENT_UPDATES\n            ),\n            stats.TOTAL_MSG_IN: self.total_msg_recv,\n            stats.TOTAL_MSG_OUT: self.total_msg_sent,\n            stats.FMS_EST_TRANS: self.get_count(\n                PeerCounterNames.FSM_ESTB_TRANSITIONS\n            ),\n            stats.UPTIME: uptime\n        }", "response": "Returns a dict with basic stats."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nimplementing neighbor configuration change listener.", "response": "def on_update_enabled(self, conf_evt):\n        \"\"\"Implements neighbor configuration change listener.\n        \"\"\"\n        enabled = conf_evt.value\n        # If we do not have any protocol bound and configuration asks us to\n        # enable this peer, we try to establish connection again.\n        if enabled:\n            LOG.info('%s enabled', self)\n            if self._protocol and self._protocol.started:\n                LOG.error('Tried to enable neighbor that is already enabled')\n            else:\n                self.state.bgp_state = const.BGP_FSM_CONNECT\n                # Restart connect loop if not already running.\n                if not self._connect_retry_event.is_set():\n                    self._connect_retry_event.set()\n                    LOG.debug('Starting connect loop as neighbor is enabled.')\n        else:\n            LOG.info('%s disabled', self)\n            if self._protocol:\n                # Stopping protocol will eventually trigger connection_lost\n                # handler which will do some clean-up.\n                # But the greenlet that is in charge of the socket may be kill\n                # when we stop the protocol, hence we call connection_lost\n                # here as we triggered socket to close.\n                self._protocol.send_notification(\n                    BGP_ERROR_CEASE,\n                    BGP_ERROR_SUB_ADMINISTRATIVE_SHUTDOWN\n                )\n                self._protocol.stop()\n                self._protocol = None\n                self.state.bgp_state = const.BGP_FSM_IDLE\n            # If this peer is not enabled any-more we stop trying to make any\n            # connection.\n            LOG.debug('Disabling connect-retry as neighbor was disabled')\n            self._connect_retry_event.clear()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _send_outgoing_route_refresh_msg(self, rr_msg):\n        assert rr_msg.type == BGP_MSG_ROUTE_REFRESH\n        self._protocol.send(rr_msg)\n        LOG.debug('RouteRefresh %s>> %s',\n                  self._neigh_conf.ip_address, rr_msg)\n        # Collect update statistics for sent refresh request.\n        if rr_msg.demarcation == 0:\n            self.state.incr(PeerCounterNames.SENT_REFRESH)\n        # If SOR is sent, we set Max. EOR timer if needed.\n        elif (rr_msg.demarcation == 1 and\n              self._common_conf.refresh_max_eor_time != 0):\n            eor_timer = self._common_conf.refresh_max_eor_time\n            # Set timer to send EOR demarcation.\n            self._spawn_after('end-of-rib-timer', eor_timer,\n                              self._enqueue_eor_msg, rr_msg)\n            LOG.debug('Enhanced RR max. EOR timer set.')", "response": "Sends given route refresh message to peer."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsend an outgoing route to the peer.", "response": "def _send_outgoing_route(self, outgoing_route):\n        \"\"\"Constructs `Update` message from given `outgoing_route` and sends\n        it to peer.\n\n        Also, checks if any policies prevent sending this message.\n        Populates Adj-RIB-out with corresponding `SentRoute`.\n        \"\"\"\n\n        path = outgoing_route.path\n        block, blocked_cause = self._apply_out_filter(path)\n\n        nlri_str = outgoing_route.path.nlri.formatted_nlri_str\n        sent_route = SentRoute(outgoing_route.path, self, block)\n        self._adj_rib_out[nlri_str] = sent_route\n        self._signal_bus.adj_rib_out_changed(self, sent_route)\n\n        # TODO(PH): optimized by sending several prefixes per update.\n        # Construct and send update message.\n        if not block:\n            update_msg = self._construct_update(outgoing_route)\n            self._protocol.send(update_msg)\n            # Collect update statistics.\n            self.state.incr(PeerCounterNames.SENT_UPDATES)\n        else:\n            LOG.debug('prefix : %s is not sent by filter : %s',\n                      path.nlri, blocked_cause)\n\n        # We have to create sent_route for every OutgoingRoute which is\n        # not a withdraw or was for route-refresh msg.\n        if (not outgoing_route.path.is_withdraw and\n                not outgoing_route.for_route_refresh):\n            # Update the destination with new sent route.\n            tm = self._core_service.table_manager\n            tm.remember_sent_route(sent_route)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrequesting route refresh to peer for given route families.", "response": "def request_route_refresh(self, *route_families):\n        \"\"\"Request route refresh to peer for given `route_families`.\n\n         If no `route_families` are given, we make request for all supported\n         route families with this peer.\n        Parameters:\n            - `route_families`: list of route families to request route\n            refresh for.\n\n        If this peer is currently not in Established state, we raise exception.\n        If any of the `route_families` are invalid we raise exception.\n        \"\"\"\n        # If this peer has not established session yet\n        if not self.in_established:\n            raise ValueError('Peer not in established state to satisfy'\n                             ' this request.')\n\n        skip_validation = False\n        # If request is made for all supported route_families for current\n        # session, we collect all route_families for valid for current session.\n        if len(route_families) == 0:\n            route_families = []\n            # We skip validation of route families that we collect ourselves\n            # below.\n            skip_validation = True\n            for route_family in SUPPORTED_GLOBAL_RF:\n                if self.is_mbgp_cap_valid(route_family):\n                    route_families.append(route_family)\n\n        for route_family in route_families:\n            if (skip_validation or\n                    ((route_family in SUPPORTED_GLOBAL_RF) and\n                     # We ignore request for route_family not valid\n                     # for current session.\n                     self._protocol.is_mbgp_cap_valid(route_family))):\n                rr_req = BGPRouteRefresh(route_family.afi, route_family.safi)\n                self.enque_outgoing_msg(rr_req)\n                LOG.debug('Enqueued Route Refresh message to '\n                          'peer %s for rf: %s', self, route_family)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _session_next_hop(self, path):\n        route_family = path.route_family\n\n        # By default we use BGPS's interface IP with this peer as next_hop.\n        if self._neigh_conf.next_hop:\n            next_hop = self._neigh_conf.next_hop\n        else:\n            next_hop = self.host_bind_ip\n        if route_family == RF_IPv6_VPN:\n            next_hop = self._ipv4_mapped_ipv6(next_hop)\n\n        return next_hop", "response": "Returns the nexthop address relevant to the current session."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _construct_as_path_attr(self, as_path_attr, as4_path_attr):\n\n        def _listify(li):\n            \"\"\"Reconstruct AS_PATH list.\n\n            Example::\n\n                >>> _listify([[1, 2, 3], {4, 5}, [6, 7]])\n                [1, 2, 3, {4, 5}, 6, 7]\n            \"\"\"\n            lo = []\n            for l in li:\n                if isinstance(l, list):\n                    lo.extend(l)\n                elif isinstance(l, set):\n                    lo.append(l)\n                else:\n                    pass\n            return lo\n\n        # If AS4_PATH attribute is None, returns the given AS_PATH attribute\n        if as4_path_attr is None:\n            return as_path_attr\n\n        # If AS_PATH is shorter than AS4_PATH, AS4_PATH should be ignored.\n        if as_path_attr.get_as_path_len() < as4_path_attr.get_as_path_len():\n            return as_path_attr\n\n        org_as_path_list = _listify(as_path_attr.path_seg_list)\n        as4_path_list = _listify(as4_path_attr.path_seg_list)\n\n        # Reverse to compare backward.\n        org_as_path_list.reverse()\n        as4_path_list.reverse()\n\n        new_as_path_list = []\n        tmp_list = []\n        for as_path, as4_path in zip_longest(org_as_path_list, as4_path_list):\n            if as4_path is None:\n                if isinstance(as_path, int):\n                    tmp_list.insert(0, as_path)\n                elif isinstance(as_path, set):\n                    if tmp_list:\n                        new_as_path_list.insert(0, tmp_list)\n                        tmp_list = []\n                    new_as_path_list.insert(0, as_path)\n                else:\n                    pass\n            elif isinstance(as4_path, int):\n                tmp_list.insert(0, as4_path)\n            elif isinstance(as4_path, set):\n                if tmp_list:\n                    new_as_path_list.insert(0, tmp_list)\n                    tmp_list = []\n                new_as_path_list.insert(0, as4_path)\n            else:\n                pass\n        if tmp_list:\n            new_as_path_list.insert(0, tmp_list)\n\n        return bgp.BGPPathAttributeAsPath(new_as_path_list)", "response": "This method constructs the AS_PATH attribute instances into a single AS_PATH instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _trans_as_path(self, as_path_list):\n\n        def _swap(n):\n            if is_valid_old_asn(n):\n                # mappable\n                return n\n            else:\n                # non-mappable\n                return bgp.AS_TRANS\n\n        # If the neighbor supports Four-Octet AS number, returns\n        # the given AS_PATH list and None.\n        if self.is_four_octet_as_number_cap_valid():\n            return as_path_list, None\n\n        # If the neighbor does not support Four-Octet AS number,\n        # constructs AS4_PATH list from AS_PATH list and swaps\n        # non-mappable AS number in AS_PATH with AS_TRANS.\n        else:\n            new_as_path_list = []\n            for as_path in as_path_list:\n                if isinstance(as_path, set):\n                    path_set = set()\n                    for as_num in as_path:\n                        path_set.add(_swap(as_num))\n                    new_as_path_list.append(path_set)\n                elif isinstance(as_path, list):\n                    path_list = list()\n                    for as_num in as_path:\n                        path_list.append(_swap(as_num))\n                    new_as_path_list.append(path_list)\n                else:\n                    # Ignore invalid as_path type\n                    pass\n\n            # If all of the AS_PATH list is composed of mappable four-octet\n            # AS numbers only, returns the given AS_PATH list\n            # Assumption: If the constructed AS_PATH list is the same as\n            # the given AS_PATH list, all AS number is mappable.\n            if as_path_list == new_as_path_list:\n                return as_path_list, None\n\n            return new_as_path_list, as_path_list", "response": "Translates AS number to AS_TRANS and separates AS_PATH list into AS_PATH and AS4_PATH lists if needed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconstructing update message with path attributes appropriately cloned and updated.", "response": "def _construct_update(self, outgoing_route):\n        \"\"\"Construct update message with Outgoing-routes path attribute\n        appropriately cloned/copied/updated.\n        \"\"\"\n        update = None\n        path = outgoing_route.path\n        # Get copy of path's path attributes.\n        pathattr_map = path.pathattr_map\n        new_pathattr = []\n\n        if path.is_withdraw:\n            if isinstance(path, Ipv4Path):\n                update = BGPUpdate(withdrawn_routes=[path.nlri])\n                return update\n            else:\n                mpunreach_attr = BGPPathAttributeMpUnreachNLRI(\n                    path.route_family.afi, path.route_family.safi, [path.nlri]\n                )\n                new_pathattr.append(mpunreach_attr)\n        elif self.is_route_server_client:\n            nlri_list = [path.nlri]\n            new_pathattr.extend(pathattr_map.values())\n        else:\n            if self.is_route_reflector_client:\n                # Append ORIGINATOR_ID attribute if not already exist.\n                if BGP_ATTR_TYPE_ORIGINATOR_ID not in pathattr_map:\n                    originator_id = path.source\n                    if originator_id is None:\n                        originator_id = self._common_conf.router_id\n                    elif isinstance(path.source, Peer):\n                        originator_id = path.source.ip_address\n                    new_pathattr.append(\n                        BGPPathAttributeOriginatorId(value=originator_id))\n\n                # Preppend own CLUSTER_ID into CLUSTER_LIST attribute if exist.\n                # Otherwise append CLUSTER_LIST attribute.\n                cluster_lst_attr = pathattr_map.get(BGP_ATTR_TYPE_CLUSTER_LIST)\n                if cluster_lst_attr:\n                    cluster_list = list(cluster_lst_attr.value)\n                    if self._common_conf.cluster_id not in cluster_list:\n                        cluster_list.insert(0, self._common_conf.cluster_id)\n                    new_pathattr.append(\n                        BGPPathAttributeClusterList(cluster_list))\n                else:\n                    new_pathattr.append(\n                        BGPPathAttributeClusterList(\n                            [self._common_conf.cluster_id]))\n\n            # Supported and un-supported/unknown attributes.\n            origin_attr = None\n            nexthop_attr = None\n            as_path_attr = None\n            as4_path_attr = None\n            aggregator_attr = None\n            as4_aggregator_attr = None\n            extcomm_attr = None\n            community_attr = None\n            localpref_attr = None\n            pmsi_tunnel_attr = None\n            unknown_opttrans_attrs = None\n            nlri_list = [path.nlri]\n\n            if path.route_family.safi in (subaddr_family.IP_FLOWSPEC,\n                                          subaddr_family.VPN_FLOWSPEC):\n                # Flow Specification does not have next_hop.\n                next_hop = []\n            elif self.is_ebgp_peer():\n                next_hop = self._session_next_hop(path)\n                if path.is_local() and path.has_nexthop():\n                    next_hop = path.nexthop\n            else:\n                next_hop = path.nexthop\n                # RFC 4271 allows us to change next_hop\n                # if configured to announce its own ip address.\n                # Also if the BGP route is configured without next_hop,\n                # we use path._session_next_hop() as next_hop.\n                if (self._neigh_conf.is_next_hop_self\n                        or (path.is_local() and not path.has_nexthop())):\n                    next_hop = self._session_next_hop(path)\n                    LOG.debug('using %s as a next_hop address instead'\n                              ' of path.nexthop %s', next_hop, path.nexthop)\n\n            nexthop_attr = BGPPathAttributeNextHop(next_hop)\n            assert nexthop_attr, 'Missing NEXTHOP mandatory attribute.'\n\n            if not isinstance(path, Ipv4Path):\n                # We construct mpreach-nlri attribute.\n                mpnlri_attr = BGPPathAttributeMpReachNLRI(\n                    path.route_family.afi,\n                    path.route_family.safi,\n                    next_hop,\n                    nlri_list\n                )\n\n            # ORIGIN Attribute.\n            # According to RFC this attribute value SHOULD NOT be changed by\n            # any other speaker.\n            origin_attr = pathattr_map.get(BGP_ATTR_TYPE_ORIGIN)\n            assert origin_attr, 'Missing ORIGIN mandatory attribute.'\n\n            # AS_PATH Attribute.\n            # Construct AS-path-attr using paths AS_PATH attr. with local AS as\n            # first item.\n            path_aspath = pathattr_map.get(BGP_ATTR_TYPE_AS_PATH)\n            assert path_aspath, 'Missing AS_PATH mandatory attribute.'\n            # Deep copy AS_PATH attr value\n            as_path_list = path_aspath.path_seg_list\n            # If this is a iBGP peer.\n            if not self.is_ebgp_peer():\n                # When a given BGP speaker advertises the route to an internal\n                # peer, the advertising speaker SHALL NOT modify the AS_PATH\n                # attribute associated with the route.\n                pass\n            else:\n                # When a given BGP speaker advertises the route to an external\n                # peer, the advertising speaker updates the AS_PATH attribute\n                # as follows:\n                # 1) if the first path segment of the AS_PATH is of type\n                #    AS_SEQUENCE, the local system prepends its own AS num as\n                #    the last element of the sequence (put it in the left-most\n                #    position with respect to the position of  octets in the\n                #    protocol message).  If the act of prepending will cause an\n                #    overflow in the AS_PATH segment (i.e.,  more than 255\n                #    ASes), it SHOULD prepend a new segment of type AS_SEQUENCE\n                #    and prepend its own AS number to this new segment.\n                #\n                # 2) if the first path segment of the AS_PATH is of type AS_SET\n                #    , the local system prepends a new path segment of type\n                #    AS_SEQUENCE to the AS_PATH, including its own AS number in\n                #    that segment.\n                #\n                # 3) if the AS_PATH is empty, the local system creates a path\n                #    segment of type AS_SEQUENCE, places its own AS into that\n                #    segment, and places that segment into the AS_PATH.\n                if (len(as_path_list) > 0 and\n                        isinstance(as_path_list[0], list) and\n                        len(as_path_list[0]) < 255):\n                    as_path_list[0].insert(0, self.local_as)\n                else:\n                    as_path_list.insert(0, [self.local_as])\n            # Construct AS4_PATH list from AS_PATH list and swap\n            # non-mappable AS number with AS_TRANS in AS_PATH.\n            as_path_list, as4_path_list = self._trans_as_path(\n                as_path_list)\n            # If the neighbor supports Four-Octet AS number, send AS_PATH\n            # in Four-Octet.\n            if self.is_four_octet_as_number_cap_valid():\n                as_path_attr = BGPPathAttributeAsPath(\n                    as_path_list, as_pack_str='!I')  # specify Four-Octet.\n            # Otherwise, send AS_PATH in Two-Octet.\n            else:\n                as_path_attr = BGPPathAttributeAsPath(as_path_list)\n            # If needed, send AS4_PATH attribute.\n            if as4_path_list:\n                as4_path_attr = BGPPathAttributeAs4Path(as4_path_list)\n\n            # AGGREGATOR Attribute.\n            aggregator_attr = pathattr_map.get(BGP_ATTR_TYPE_AGGREGATOR)\n            # If the neighbor does not support Four-Octet AS number,\n            # swap non-mappable AS number with AS_TRANS.\n            if (aggregator_attr and\n                    not self.is_four_octet_as_number_cap_valid()):\n                # If AS number of AGGREGATOR is Four-Octet AS number,\n                # swap with AS_TRANS, else do not.\n                aggregator_as_number = aggregator_attr.as_number\n                if not is_valid_old_asn(aggregator_as_number):\n                    aggregator_attr = bgp.BGPPathAttributeAggregator(\n                        bgp.AS_TRANS, aggregator_attr.addr)\n                    as4_aggregator_attr = bgp.BGPPathAttributeAs4Aggregator(\n                        aggregator_as_number, aggregator_attr.addr)\n\n            # MULTI_EXIT_DISC Attribute.\n            # For eBGP session we can send multi-exit-disc if configured.\n            multi_exit_disc = None\n            if self.is_ebgp_peer():\n                if self._neigh_conf.multi_exit_disc:\n                    multi_exit_disc = BGPPathAttributeMultiExitDisc(\n                        self._neigh_conf.multi_exit_disc\n                    )\n                else:\n                    pass\n            if not self.is_ebgp_peer():\n                multi_exit_disc = pathattr_map.get(\n                    BGP_ATTR_TYPE_MULTI_EXIT_DISC)\n\n            # LOCAL_PREF Attribute.\n            if not self.is_ebgp_peer():\n                # For iBGP peers we are required to send local-pref attribute\n                # for connected or local prefixes. We check if the path matches\n                # attribute_maps and set local-pref value.\n                # If the path doesn't match, we set default local-pref given\n                # from the user. The default value is 100.\n                localpref_attr = BGPPathAttributeLocalPref(\n                    self._common_conf.local_pref)\n                key = const.ATTR_MAPS_LABEL_DEFAULT\n\n                if isinstance(path, (Vpnv4Path, Vpnv6Path)):\n                    nlri = nlri_list[0]\n                    rf = VRF_RF_IPV4 if isinstance(path, Vpnv4Path)\\\n                        else VRF_RF_IPV6\n                    key = ':'.join([nlri.route_dist, rf])\n\n                attr_type = AttributeMap.ATTR_LOCAL_PREF\n                at_maps = self._attribute_maps.get(key, {})\n                result = self._lookup_attribute_map(at_maps, attr_type, path)\n                if result:\n                    localpref_attr = result\n\n            # COMMUNITY Attribute.\n            community_attr = pathattr_map.get(BGP_ATTR_TYPE_COMMUNITIES)\n\n            # EXTENDED COMMUNITY Attribute.\n            # Construct ExtCommunity path-attr based on given.\n            path_extcomm_attr = pathattr_map.get(\n                BGP_ATTR_TYPE_EXTENDED_COMMUNITIES\n            )\n            if path_extcomm_attr:\n                # SOO list can be configured per VRF and/or per Neighbor.\n                # NeighborConf has this setting we add this to existing list.\n                communities = path_extcomm_attr.communities\n                if self._neigh_conf.soo_list:\n                    # construct extended community\n                    soo_list = self._neigh_conf.soo_list\n                    subtype = 0x03\n                    for soo in soo_list:\n                        first, second = soo.split(':')\n                        if '.' in first:\n                            c = BGPIPv4AddressSpecificExtendedCommunity(\n                                subtype=subtype,\n                                ipv4_address=first,\n                                local_administrator=int(second))\n                        else:\n                            c = BGPTwoOctetAsSpecificExtendedCommunity(\n                                subtype=subtype,\n                                as_number=int(first),\n                                local_administrator=int(second))\n                        communities.append(c)\n\n                extcomm_attr = BGPPathAttributeExtendedCommunities(\n                    communities=communities\n                )\n\n                pmsi_tunnel_attr = pathattr_map.get(\n                    BGP_ATTR_TYEP_PMSI_TUNNEL_ATTRIBUTE\n                )\n\n            # UNKNOWN Attributes.\n            # Get optional transitive path attributes\n            unknown_opttrans_attrs = bgp_utils.get_unknown_opttrans_attr(path)\n\n            # Ordering path attributes according to type as RFC says. We set\n            # MPReachNLRI first as advised by experts as a new trend in BGP\n            # implementation.\n            if isinstance(path, Ipv4Path):\n                new_pathattr.append(nexthop_attr)\n            else:\n                new_pathattr.append(mpnlri_attr)\n\n            new_pathattr.append(origin_attr)\n            new_pathattr.append(as_path_attr)\n            if as4_path_attr:\n                new_pathattr.append(as4_path_attr)\n            if aggregator_attr:\n                new_pathattr.append(aggregator_attr)\n            if as4_aggregator_attr:\n                new_pathattr.append(as4_aggregator_attr)\n            if multi_exit_disc:\n                new_pathattr.append(multi_exit_disc)\n            if localpref_attr:\n                new_pathattr.append(localpref_attr)\n            if community_attr:\n                new_pathattr.append(community_attr)\n            if extcomm_attr:\n                new_pathattr.append(extcomm_attr)\n            if pmsi_tunnel_attr:\n                new_pathattr.append(pmsi_tunnel_attr)\n            if unknown_opttrans_attrs:\n                new_pathattr.extend(unknown_opttrans_attrs.values())\n\n        if isinstance(path, Ipv4Path):\n            update = BGPUpdate(path_attributes=new_pathattr,\n                               nlri=nlri_list)\n        else:\n            update = BGPUpdate(path_attributes=new_pathattr)\n        return update"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _connect_loop(self, client_factory):\n        # If current configuration allow, enable active session establishment.\n        if self._neigh_conf.enabled:\n            self._connect_retry_event.set()\n\n        while True:\n            self._connect_retry_event.wait()\n\n            # Reconnecting immediately after closing connection may be not very\n            # well seen by some peers (ALU?)\n            self.pause(1.0)\n            if self.state.bgp_state in \\\n                    (const.BGP_FSM_IDLE, const.BGP_FSM_ACTIVE):\n\n                # Check if we have to stop or retry\n                self.state.bgp_state = const.BGP_FSM_CONNECT\n                # If we have specific host interface to bind to, we will do so\n                # else we will bind to system default.\n                if self._neigh_conf.host_bind_ip and \\\n                        self._neigh_conf.host_bind_port:\n                    bind_addr = (self._neigh_conf.host_bind_ip,\n                                 self._neigh_conf.host_bind_port)\n                else:\n                    bind_addr = None\n                peer_address = (self._neigh_conf.ip_address,\n                                self._neigh_conf.port)\n\n                if bind_addr:\n                    LOG.debug('%s trying to connect from'\n                              '%s to %s', self, bind_addr, peer_address)\n                else:\n                    LOG.debug('%s trying to connect to %s', self, peer_address)\n                tcp_conn_timeout = self._common_conf.tcp_conn_timeout\n                try:\n                    password = self._neigh_conf.password\n                    self._connect_tcp(peer_address,\n                                      client_factory,\n                                      time_out=tcp_conn_timeout,\n                                      bind_address=bind_addr,\n                                      password=password)\n                except socket.error:\n                    self.state.bgp_state = const.BGP_FSM_ACTIVE\n                    if LOG.isEnabledFor(logging.DEBUG):\n                        LOG.debug('Socket could not be created in time'\n                                  ' (%s secs), reason %s', tcp_conn_timeout,\n                                  traceback.format_exc())\n                    LOG.info('Will try to reconnect to %s after %s secs: %s',\n                             self._neigh_conf.ip_address,\n                             self._common_conf.bgp_conn_retry_time,\n                             self._connect_retry_event.is_set())\n\n            self.pause(self._common_conf.bgp_conn_retry_time)", "response": "Connect loop for incoming data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bind_protocol(self, proto):\n        LOG.debug('Trying to bind protocol %s to peer %s', proto, self)\n        # Validate input.\n        if not isinstance(proto, BgpProtocol):\n            raise ValueError('Currently only supports valid instances of'\n                             ' `BgpProtocol`')\n\n        if proto.state != const.BGP_FSM_OPEN_CONFIRM:\n            raise ValueError('Only protocols in OpenConfirm state can be'\n                             ' bound')\n\n        # If we are not bound to any protocol\n        is_bound = False\n        if not self._protocol:\n            self._set_protocol(proto)\n            is_bound = True\n        else:\n            # If existing protocol is already established, we raise exception.\n            if self.state.bgp_state != const.BGP_FSM_IDLE:\n                LOG.debug('Currently in %s state, hence will send collision'\n                          ' Notification to close this protocol.',\n                          self.state.bgp_state)\n                self._send_collision_err_and_stop(proto)\n                return\n\n            # If we have a collision that need to be resolved\n            assert proto.is_colliding(self._protocol), \\\n                ('Tried to bind second protocol that is not colliding with '\n                 'first/bound protocol')\n            LOG.debug('Currently have one protocol in %s state and '\n                      'another protocol in %s state',\n                      self._protocol.state, proto.state)\n            # Protocol that is already bound\n            first_protocol = self._protocol\n            assert ((first_protocol.is_reactive and not proto.is_reactive) or\n                    (proto.is_reactive and not first_protocol.is_reactive))\n            # Connection initiated by peer.\n            reactive_proto = None\n            # Connection initiated locally.\n            proactive_proto = None\n            # Identify which protocol was initiated by which peer.\n            if proto.is_reactive:\n                reactive_proto = proto\n                proactive_proto = self._protocol\n            else:\n                reactive_proto = self._protocol\n                proactive_proto = proto\n\n            LOG.debug('Pro-active/Active protocol %s', proactive_proto)\n            # We compare bgp local and remote router id and keep the protocol\n            # that was initiated by peer with highest id.\n            if proto.is_local_router_id_greater():\n                self._set_protocol(proactive_proto)\n            else:\n                self._set_protocol(reactive_proto)\n\n            if self._protocol is not proto:\n                # If new proto did not win collision we return False to\n                # indicate this.\n                is_bound = False\n            else:\n                # If first protocol did not win collision resolution we\n                # we send notification to peer and stop it\n                self._send_collision_err_and_stop(first_protocol)\n                is_bound = True\n\n        return is_bound", "response": "Bind a given protocol instance to this peer."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_open_msg(self):\n        asnum = self.local_as\n        # If local AS number is not Two-Octet AS number, swaps with AS_TRANS.\n        if not is_valid_old_asn(asnum):\n            asnum = bgp.AS_TRANS\n        bgpid = self._common_conf.router_id\n        holdtime = self._neigh_conf.hold_time\n\n        def flatten(L):\n            if isinstance(L, list):\n                for i in range(len(L)):\n                    for e in flatten(L[i]):\n                        yield e\n            else:\n                yield L\n        opts = list(flatten(\n            list(self._neigh_conf.get_configured_capabilities().values())))\n        open_msg = BGPOpen(\n            my_as=asnum,\n            bgp_identifier=bgpid,\n            version=const.BGP_VERSION_NUM,\n            hold_time=holdtime,\n            opt_param=opts\n        )\n        return open_msg", "response": "Create Open message using current settings."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the update message.", "response": "def _validate_update_msg(self, update_msg):\n        \"\"\"Validate update message as per RFC.\n\n        Here we validate the message after it has been parsed. Message\n        has already been validated against some errors inside parsing\n        library.\n        \"\"\"\n        # TODO(PH): finish providing implementation, currently low priority\n        assert update_msg.type == BGP_MSG_UPDATE\n        # An UPDATE message may be received only in the Established state.\n        # Receiving an UPDATE message in any other state is an error.\n        if self.state.bgp_state != const.BGP_FSM_ESTABLISHED:\n            LOG.error('Received UPDATE message when not in ESTABLISHED'\n                      ' state.')\n            raise bgp.FiniteStateMachineError()\n\n        mp_reach_attr = update_msg.get_path_attr(\n            BGP_ATTR_TYPE_MP_REACH_NLRI\n        )\n        mp_unreach_attr = update_msg.get_path_attr(\n            BGP_ATTR_TYPE_MP_UNREACH_NLRI\n        )\n\n        # non-MPBGP Update msg.\n        if not (mp_reach_attr or mp_unreach_attr):\n            if not self.is_mpbgp_cap_valid(RF_IPv4_UC):\n                LOG.error('Got UPDATE message with un-available'\n                          ' afi/safi %s', RF_IPv4_UC)\n            nlri_list = update_msg.nlri\n            if len(nlri_list) > 0:\n                # Check for missing well-known mandatory attributes.\n                aspath = update_msg.get_path_attr(BGP_ATTR_TYPE_AS_PATH)\n                if not aspath:\n                    raise bgp.MissingWellKnown(\n                        BGP_ATTR_TYPE_AS_PATH)\n\n                if (self.check_first_as and self.is_ebgp_peer() and\n                        not aspath.has_matching_leftmost(self.remote_as)):\n                    LOG.error('First AS check fails. Raise appropriate'\n                              ' exception.')\n                    raise bgp.MalformedAsPath()\n\n                origin = update_msg.get_path_attr(BGP_ATTR_TYPE_ORIGIN)\n                if not origin:\n                    raise bgp.MissingWellKnown(BGP_ATTR_TYPE_ORIGIN)\n\n                nexthop = update_msg.get_path_attr(BGP_ATTR_TYPE_NEXT_HOP)\n                if not nexthop:\n                    raise bgp.MissingWellKnown(BGP_ATTR_TYPE_NEXT_HOP)\n\n            return True\n\n        # Check if received MP_UNREACH path attribute is of available afi/safi\n        if mp_unreach_attr:\n            if not self.is_mpbgp_cap_valid(mp_unreach_attr.route_family):\n                LOG.error('Got UPDATE message with un-available afi/safi for'\n                          ' MP_UNREACH path attribute (non-negotiated'\n                          ' afi/safi) %s', mp_unreach_attr.route_family)\n                # raise bgp.OptAttrError()\n\n        if mp_reach_attr:\n            # Check if received MP_REACH path attribute is of available\n            # afi/safi\n            if not self.is_mpbgp_cap_valid(mp_reach_attr.route_family):\n                LOG.error('Got UPDATE message with un-available afi/safi for'\n                          ' MP_UNREACH path attribute (non-negotiated'\n                          ' afi/safi) %s', mp_reach_attr.route_family)\n                # raise bgp.OptAttrError()\n\n            # Check for missing well-known mandatory attributes.\n            aspath = update_msg.get_path_attr(BGP_ATTR_TYPE_AS_PATH)\n            if not aspath:\n                raise bgp.MissingWellKnown(BGP_ATTR_TYPE_AS_PATH)\n\n            if (self.check_first_as and self.is_ebgp_peer() and\n                    not aspath.has_matching_leftmost(self.remote_as)):\n                LOG.error('First AS check fails. Raise appropriate exception.')\n                raise bgp.MalformedAsPath()\n\n            origin = update_msg.get_path_attr(BGP_ATTR_TYPE_ORIGIN)\n            if not origin:\n                raise bgp.MissingWellKnown(BGP_ATTR_TYPE_ORIGIN)\n\n            # Validate Next hop.\n            if mp_reach_attr.route_family.safi in (\n                    subaddr_family.IP_FLOWSPEC,\n                    subaddr_family.VPN_FLOWSPEC):\n                # Because the Flow Specification does not have nexthop,\n                # skips check.\n                pass\n            elif (not mp_reach_attr.next_hop or\n                  mp_reach_attr.next_hop == self.host_bind_ip):\n                LOG.error('Nexthop of received UPDATE msg. (%s) same as local'\n                          ' interface address %s.',\n                          mp_reach_attr.next_hop,\n                          self.host_bind_ip)\n                return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _handle_update_msg(self, update_msg):\n        assert self.state.bgp_state == const.BGP_FSM_ESTABLISHED\n\n        # Increment count of update received.\n        self.state.incr(PeerCounterNames.RECV_UPDATES)\n\n        if not self._validate_update_msg(update_msg):\n            # If update message was not valid for some reason, we ignore its\n            # routes.\n            LOG.error('UPDATE message was invalid, hence ignoring its routes.')\n            return\n\n        # Extract advertised path attributes and reconstruct AS_PATH attribute\n        self._extract_and_reconstruct_as_path(update_msg)\n\n        # Check if path attributes have loops.\n        if self._is_looped_path_attrs(update_msg):\n            return\n\n        umsg_pattrs = update_msg.pathattr_map\n        mp_reach_attr = umsg_pattrs.get(BGP_ATTR_TYPE_MP_REACH_NLRI, None)\n        if mp_reach_attr:\n            # Extract advertised MP-BGP paths from given message.\n            self._extract_and_handle_mpbgp_new_paths(update_msg)\n\n        mp_unreach_attr = umsg_pattrs.get(BGP_ATTR_TYPE_MP_UNREACH_NLRI, None)\n        if mp_unreach_attr:\n            # Extract MP-BGP withdraws from given message.\n            self._extract_and_handle_mpbgp_withdraws(mp_unreach_attr)\n\n        nlri_list = update_msg.nlri\n        if nlri_list:\n            # Extract advertised BGP paths from given message.\n            self._extract_and_handle_bgp4_new_paths(update_msg)\n\n        withdraw_list = update_msg.withdrawn_routes\n        if withdraw_list:\n            # Extract BGP withdraws from given message.\n            self._extract_and_handle_bgp4_withdraws(withdraw_list)", "response": "Extracts and processes new paths or withdrawals in given BGP message."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nextract advertised AS path attributes in the given update message and reconstructs AS_PATH from AS_PATH and AS4_PATH.", "response": "def _extract_and_reconstruct_as_path(self, update_msg):\n        \"\"\"Extracts advertised AS path attributes in the given update message\n        and reconstructs AS_PATH from AS_PATH and AS4_PATH if needed.\"\"\"\n        umsg_pattrs = update_msg.pathattr_map\n\n        as_aggregator = umsg_pattrs.get(BGP_ATTR_TYPE_AGGREGATOR, None)\n        as4_aggregator = umsg_pattrs.get(BGP_ATTR_TYPE_AS4_AGGREGATOR, None)\n        if as_aggregator and as4_aggregator:\n            # When both AGGREGATOR and AS4_AGGREGATOR are received,\n            # if the AS number in the AGGREGATOR attribute is not AS_TRANS,\n            # then:\n            #  -  the AS4_AGGREGATOR attribute and the AS4_PATH attribute SHALL\n            #     be ignored,\n            #  -  the AGGREGATOR attribute SHALL be taken as the information\n            #     about the aggregating node, and\n            #  -  the AS_PATH attribute SHALL be taken as the AS path\n            #     information.\n            if as_aggregator.as_number != bgp.AS_TRANS:\n                update_msg.path_attributes.remove(as4_aggregator)\n                as4_path = umsg_pattrs.pop(BGP_ATTR_TYPE_AS4_PATH, None)\n                if as4_path:\n                    update_msg.path_attributes.remove(as4_path)\n            # Otherwise,\n            #  -  the AGGREGATOR attribute SHALL be ignored,\n            #  -  the AS4_AGGREGATOR attribute SHALL be taken as the\n            #     information about the aggregating node, and\n            #  -  the AS path information would need to be constructed,\n            #     as in all other cases.\n            else:\n                update_msg.path_attributes.remove(as_aggregator)\n                update_msg.path_attributes.remove(as4_aggregator)\n                update_msg.path_attributes.append(\n                    bgp.BGPPathAttributeAggregator(\n                        as_number=as4_aggregator.as_number,\n                        addr=as4_aggregator.addr,\n                    )\n                )\n\n        as_path = umsg_pattrs.get(BGP_ATTR_TYPE_AS_PATH, None)\n        as4_path = umsg_pattrs.get(BGP_ATTR_TYPE_AS4_PATH, None)\n        if as_path and as4_path:\n            # If the number of AS numbers in the AS_PATH attribute is\n            # less than the number of AS numbers in the AS4_PATH attribute,\n            # then the AS4_PATH attribute SHALL be ignored, and the AS_PATH\n            # attribute SHALL be taken as the AS path information.\n            if as_path.get_as_path_len() < as4_path.get_as_path_len():\n                update_msg.path_attributes.remove(as4_path)\n\n            # If the number of AS numbers in the AS_PATH attribute is larger\n            # than or equal to the number of AS numbers in the AS4_PATH\n            # attribute, then the AS path information SHALL be constructed\n            # by taking as many AS numbers and path segments as necessary\n            # from the leading part of the AS_PATH attribute, and then\n            # prepending them to the AS4_PATH attribute so that the AS path\n            # information has a number of AS numbers identical to that of\n            # the AS_PATH attribute.\n            else:\n                update_msg.path_attributes.remove(as_path)\n                update_msg.path_attributes.remove(as4_path)\n                as_path = self._construct_as_path_attr(as_path, as4_path)\n                update_msg.path_attributes.append(as_path)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if the given path attributes have loops.", "response": "def _is_looped_path_attrs(self, update_msg):\n        \"\"\"\n        Extracts path attributes from the given UPDATE message and checks\n        if the given attributes have loops or not.\n\n        :param update_msg: UPDATE message instance.\n        :return: True if attributes have loops. Otherwise False.\n        \"\"\"\n        umsg_pattrs = update_msg.pathattr_map\n        recv_open_msg = self.protocol.recv_open_msg\n\n        # Check if AS_PATH has loops.\n        aspath = umsg_pattrs.get(BGP_ATTR_TYPE_AS_PATH)\n        if (aspath is not None\n                and aspath.has_local_as(\n                    self.local_as,\n                    max_count=self._common_conf.allow_local_as_in_count)):\n            LOG.error(\n                'AS_PATH on UPDATE message has loops. '\n                'Ignoring this message: %s',\n                update_msg)\n            return\n\n        # Check if ORIGINATOR_ID has loops. [RFC4456]\n        originator_id = umsg_pattrs.get(BGP_ATTR_TYPE_ORIGINATOR_ID, None)\n        if (originator_id\n                and recv_open_msg.bgp_identifier == originator_id):\n            LOG.error(\n                'ORIGINATOR_ID on UPDATE message has loops. '\n                'Ignoring this message: %s',\n                update_msg)\n            return\n\n        # Check if CLUSTER_LIST has loops. [RFC4456]\n        cluster_list = umsg_pattrs.get(BGP_ATTR_TYPE_CLUSTER_LIST, None)\n        if (cluster_list\n                and self._common_conf.cluster_id in cluster_list.value):\n            LOG.error(\n                'CLUSTER_LIST on UPDATE message has loops. '\n                'Ignoring this message: %s', update_msg)\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _extract_and_handle_bgp4_new_paths(self, update_msg):\n        umsg_pattrs = update_msg.pathattr_map\n        next_hop = update_msg.get_path_attr(BGP_ATTR_TYPE_NEXT_HOP).value\n\n        # Nothing to do if we do not have any new NLRIs in this message.\n        msg_nlri_list = update_msg.nlri\n        if not msg_nlri_list:\n            LOG.debug('Update message did not have any new MP_REACH_NLRIs.')\n            return\n\n        # Create path instances for each NLRI from the update message.\n        for msg_nlri in msg_nlri_list:\n            LOG.debug('NLRI: %s', msg_nlri)\n            new_path = bgp_utils.create_path(\n                self,\n                msg_nlri,\n                pattrs=umsg_pattrs,\n                nexthop=next_hop\n            )\n            LOG.debug('Extracted paths from Update msg.: %s', new_path)\n\n            block, blocked_cause = self._apply_in_filter(new_path)\n\n            nlri_str = new_path.nlri.formatted_nlri_str\n            received_route = ReceivedRoute(new_path, self, block)\n            self._adj_rib_in[nlri_str] = received_route\n            self._signal_bus.adj_rib_in_changed(self, received_route)\n\n            if not block:\n                # Update appropriate table with new paths.\n                tm = self._core_service.table_manager\n                tm.learn_path(new_path)\n            else:\n                LOG.debug('prefix : %s is blocked by in-bound filter: %s',\n                          msg_nlri, blocked_cause)\n\n        # If update message had any qualifying new paths, do some book-keeping.\n        if msg_nlri_list:\n            # Update prefix statistics.\n            self.state.incr(PeerCounterNames.RECV_PREFIXES,\n                            incr_by=len(msg_nlri_list))\n            # Check if we exceed max. prefixes allowed for this neighbor.\n            if self._neigh_conf.exceeds_max_prefix_allowed(\n                    self.state.get_count(PeerCounterNames.RECV_PREFIXES)):\n                LOG.error('Max. prefix allowed for this neighbor '\n                          'exceeded.')", "response": "Extracts new paths advertised in the given update message s NLRIs and adds them to appropriate destination table."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_and_handle_bgp4_withdraws(self, withdraw_list):\n        msg_rf = RF_IPv4_UC\n        w_nlris = withdraw_list\n        if not w_nlris:\n            # If this is EOR of some kind, handle it\n            self._handle_eor(msg_rf)\n\n        for w_nlri in w_nlris:\n            w_path = bgp_utils.create_path(\n                self,\n                w_nlri,\n                is_withdraw=True\n            )\n\n            block, blocked_cause = self._apply_in_filter(w_path)\n\n            received_route = ReceivedRoute(w_path, self, block)\n            nlri_str = w_nlri.formatted_nlri_str\n\n            if nlri_str in self._adj_rib_in:\n                del self._adj_rib_in[nlri_str]\n                self._signal_bus.adj_rib_in_changed(self, received_route)\n\n            if not block:\n                # Update appropriate table with withdraws.\n                tm = self._core_service.table_manager\n                tm.learn_path(w_path)\n            else:\n                LOG.debug('prefix : %s is blocked by in-bound filter: %s',\n                          nlri_str, blocked_cause)", "response": "Extracts and handles bgp4 withdraws from the given update message s BGPUnReachNlri attribute."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_and_handle_mpbgp_new_paths(self, update_msg):\n        umsg_pattrs = update_msg.pathattr_map\n        mpreach_nlri_attr = umsg_pattrs.get(BGP_ATTR_TYPE_MP_REACH_NLRI)\n        assert mpreach_nlri_attr\n\n        msg_rf = mpreach_nlri_attr.route_family\n        # Check if this route family is among supported route families.\n        if msg_rf not in SUPPORTED_GLOBAL_RF:\n            LOG.info(('Received route for route family %s which is'\n                      ' not supported. Ignoring paths from this UPDATE: %s') %\n                     (msg_rf, update_msg))\n            return\n\n        if msg_rf in (RF_IPv4_VPN, RF_IPv6_VPN):\n            # Check if we have Extended Communities Attribute.\n            # TODO(PH): Check if RT_NLRI afi/safi will ever have this attribute\n            ext_comm_attr = umsg_pattrs.get(BGP_ATTR_TYPE_EXTENDED_COMMUNITIES)\n            # Check if we have at-least one RT is of interest to us.\n            if not ext_comm_attr:\n                LOG.info('Missing Extended Communities Attribute. '\n                         'Ignoring paths from this UPDATE: %s', update_msg)\n                return\n\n            msg_rts = ext_comm_attr.rt_list\n            # If we do not have any RTs associated with this msg., we do not\n            # extract any paths.\n            if not msg_rts:\n                LOG.info('Received route with no RTs. Ignoring paths in this'\n                         ' UPDATE: %s', update_msg)\n                return\n\n            # If none of the RTs in the message are of interest, we do not\n            # extract any paths.\n            interested_rts = self._core_service.global_interested_rts\n            if not interested_rts.intersection(msg_rts):\n                LOG.info('Received route with RT %s that is of no interest to'\n                         ' any VRFs or Peers %s.'\n                         ' Ignoring paths from this UPDATE: %s',\n                         msg_rts, interested_rts, update_msg)\n                return\n\n        next_hop = mpreach_nlri_attr.next_hop\n\n        # Nothing to do if we do not have any new NLRIs in this message.\n        msg_nlri_list = mpreach_nlri_attr.nlri\n        if not msg_nlri_list:\n            LOG.debug('Update message did not have any new MP_REACH_NLRIs.')\n            return\n\n        # Create path instances for each NLRI from the update message.\n        for msg_nlri in msg_nlri_list:\n            new_path = bgp_utils.create_path(\n                self,\n                msg_nlri,\n                pattrs=umsg_pattrs,\n                nexthop=next_hop\n            )\n            LOG.debug('Extracted paths from Update msg.: %s', new_path)\n\n            block, blocked_cause = self._apply_in_filter(new_path)\n\n            received_route = ReceivedRoute(new_path, self, block)\n            nlri_str = msg_nlri.formatted_nlri_str\n            self._adj_rib_in[nlri_str] = received_route\n            self._signal_bus.adj_rib_in_changed(self, received_route)\n\n            if not block:\n                if msg_rf == RF_RTC_UC \\\n                        and self._init_rtc_nlri_path is not None:\n                    self._init_rtc_nlri_path.append(new_path)\n                else:\n                    # Update appropriate table with new paths.\n                    tm = self._core_service.table_manager\n                    tm.learn_path(new_path)\n            else:\n                LOG.debug('prefix : %s is blocked by in-bound filter: %s',\n                          msg_nlri, blocked_cause)\n\n        # If update message had any qualifying new paths, do some book-keeping.\n        if msg_nlri_list:\n            # Update prefix statistics.\n            self.state.incr(PeerCounterNames.RECV_PREFIXES,\n                            incr_by=len(msg_nlri_list))\n            # Check if we exceed max. prefixes allowed for this neighbor.\n            if self._neigh_conf.exceeds_max_prefix_allowed(\n                    self.state.get_count(PeerCounterNames.RECV_PREFIXES)):\n                LOG.error('Max. prefix allowed for this neighbor '\n                          'exceeded.')", "response": "Extracts new paths advertised in the given update message s BGPReachNlri attribute and handles them if they are present."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _extract_and_handle_mpbgp_withdraws(self, mp_unreach_attr):\n        msg_rf = mp_unreach_attr.route_family\n        # Check if this route family is among supported route families.\n        if msg_rf not in SUPPORTED_GLOBAL_RF:\n            LOG.info(\n                'Received route family %s is not supported. '\n                'Ignoring withdraw routes on this UPDATE message.',\n                msg_rf)\n            return\n\n        w_nlris = mp_unreach_attr.withdrawn_routes\n        if not w_nlris:\n            # If this is EOR of some kind, handle it\n            self._handle_eor(msg_rf)\n\n        for w_nlri in w_nlris:\n            w_path = bgp_utils.create_path(\n                self,\n                w_nlri,\n                is_withdraw=True\n            )\n            block, blocked_cause = self._apply_in_filter(w_path)\n\n            received_route = ReceivedRoute(w_path, self, block)\n            nlri_str = w_nlri.formatted_nlri_str\n\n            if nlri_str in self._adj_rib_in:\n                del self._adj_rib_in[nlri_str]\n                self._signal_bus.adj_rib_in_changed(self, received_route)\n\n            if not block:\n                # Update appropriate table with withdraws.\n                tm = self._core_service.table_manager\n                tm.learn_path(w_path)\n            else:\n                LOG.debug('prefix : %s is blocked by in-bound filter: %s',\n                          w_nlri, blocked_cause)", "response": "Extracts and handles withdraws advertised in the given update message s bgpUnReachNlri attribute."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _handle_eor(self, route_family):\n        LOG.debug('Handling EOR for %s', route_family)\n#         assert (route_family in SUPPORTED_GLOBAL_RF)\n#         assert self.is_mbgp_cap_valid(route_family)\n\n        if route_family == RF_RTC_UC:\n            self._unschedule_sending_init_updates()\n\n            # Learn all rt_nlri at the same time As RT are learned and RT\n            # filter get updated, qualifying NLRIs are automatically sent to\n            # peer including initial update\n            tm = self._core_service.table_manager\n            for rt_nlri in self._init_rtc_nlri_path:\n                tm.learn_path(rt_nlri)\n                # Give chance to process new RT_NLRI so that we have updated RT\n                # filter for all peer including this peer before we communicate\n                # NLRIs for other address-families\n                self.pause(0)\n            # Clear collection of initial RTs as we no longer need to wait for\n            # EOR for RT NLRIs and to indicate that new RT NLRIs should be\n            # handled in a regular fashion\n            self._init_rtc_nlri_path = None", "response": "Handle an EOR for a specific RT - DC."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nhandle a single BGP message.", "response": "def handle_msg(self, msg):\n        \"\"\"BGP message handler.\n\n        BGP message handling is shared between protocol instance and peer. Peer\n        only handles limited messages under suitable state. Here we handle\n        KEEPALIVE, UPDATE and ROUTE_REFRESH messages. UPDATE and ROUTE_REFRESH\n        messages are handled only after session is established.\n        \"\"\"\n        if msg.type == BGP_MSG_KEEPALIVE:\n            # If we receive a Keep Alive message in open_confirm state, we\n            # transition to established state.\n            if self.state.bgp_state == const.BGP_FSM_OPEN_CONFIRM:\n                self.state.bgp_state = const.BGP_FSM_ESTABLISHED\n                self._enqueue_init_updates()\n\n        elif msg.type == BGP_MSG_UPDATE:\n            assert self.state.bgp_state == const.BGP_FSM_ESTABLISHED\n            # Will try to process this UDPATE message further\n            self._handle_update_msg(msg)\n\n        elif msg.type == BGP_MSG_ROUTE_REFRESH:\n            # If its route-refresh message\n            assert self.state.bgp_state == const.BGP_FSM_ESTABLISHED\n            self._handle_route_refresh_msg(msg)\n\n        else:\n            # Open/Notification messages are currently handled by protocol and\n            # nothing is done inside peer, so should not see them here.\n            raise ValueError('Peer does not support handling of %s'\n                             ' message during %s state' %\n                             (msg, self.state.bgp_state))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _enqueue_eor_msg(self, sor):\n        if self._protocol.is_enhanced_rr_cap_valid() and not sor.eor_sent:\n            afi = sor.afi\n            safi = sor.safi\n            eor = BGPRouteRefresh(afi, safi, demarcation=2)\n            self.enque_outgoing_msg(eor)\n            sor.eor_sent = True", "response": "Enqueues Enhanced RR EOR for given SOR."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _schedule_sending_init_updates(self):\n\n        def _enqueue_non_rtc_init_updates():\n            LOG.debug('Scheduled queuing of initial Non-RTC UPDATEs')\n            tm = self._core_service.table_manager\n            self.comm_all_best_paths(tm.global_tables)\n            self._sent_init_non_rtc_update = True\n            # Stop the timer as we have handled RTC EOR\n            self._rtc_eor_timer.stop()\n            self._rtc_eor_timer = None\n\n        self._sent_init_non_rtc_update = False\n        self._rtc_eor_timer = self._create_timer(\n            Peer.RTC_EOR_TIMER_NAME,\n            _enqueue_non_rtc_init_updates\n        )\n        # Start timer for sending initial updates\n        self._rtc_eor_timer.start(const.RTC_EOR_DEFAULT_TIME, now=False)\n        LOG.debug('Scheduled sending of initial Non-RTC UPDATEs after:'\n                  ' %s sec', const.RTC_EOR_DEFAULT_TIME)", "response": "Schedule for initial updates of the current state of the local table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _unschedule_sending_init_updates(self):\n        LOG.debug('Un-scheduling sending of initial Non-RTC UPDATEs'\n                  ' (init. UPDATEs already sent: %s)',\n                  self._sent_init_non_rtc_update)\n        if self._rtc_eor_timer:\n            self._rtc_eor_timer.stop()\n            self._rtc_eor_timer = None\n            return True\n        return False", "response": "Un - schedules sending of initial updates."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncommunicates all current best paths with this peers.", "response": "def comm_all_best_paths(self, global_tables):\n        \"\"\"Shares/communicates current best paths with this peers.\n\n        Can be used to send initial updates after we have established session\n        with `peer`.\n        \"\"\"\n        LOG.debug('Communicating current best path for all afi/safi except'\n                  ' 1/132')\n        # We will enqueue best path from all global destination.\n        for route_family, table in global_tables.items():\n            if route_family == RF_RTC_UC:\n                continue\n            if self.is_mbgp_cap_valid(route_family):\n                for dest in table.values():\n                    if dest.best_path:\n                        self.communicate_path(dest.best_path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef communicate_path(self, path):\n        LOG.debug('Peer %s asked to communicate path', self)\n        if not path:\n            raise ValueError('Invalid path %s given.' % path)\n\n        # We do not send anything to peer who is not in established state.\n        if not self.in_established():\n            LOG.debug('Skipping sending path as peer is not in '\n                      'ESTABLISHED state %s', path)\n            return\n\n        # Check if this session is available for given paths afi/safi\n        path_rf = path.route_family\n        if not (self.is_mpbgp_cap_valid(path_rf) or\n                path_rf in [RF_IPv4_UC, RF_IPv6_UC]):\n            LOG.debug('Skipping sending path as %s route family is not'\n                      ' available for this session', path_rf)\n            return\n\n        # If RTC capability is available and path afi/saif is other than  RT\n        # nlri\n        if path_rf != RF_RTC_UC and \\\n                self.is_mpbgp_cap_valid(RF_RTC_UC):\n            rtfilter = self._peer_manager.curr_peer_rtfilter(self)\n            # If peer does not have any rtfilter or if rtfilter does not have\n            # any RTs common with path RTs we do not share this path with the\n            # peer\n            if rtfilter and not path.has_rts_in(rtfilter):\n                LOG.debug('Skipping sending path as rffilter %s and path '\n                          'rts %s have no RT in common',\n                          rtfilter, path.get_rts())\n                return\n\n        # Transmit side loop detection: We check if leftmost AS matches\n        # peers AS, if so we do not send UPDATE message to this peer.\n        as_path = path.get_pattr(BGP_ATTR_TYPE_AS_PATH)\n        if as_path and as_path.has_matching_leftmost(self.remote_as):\n            LOG.debug('Skipping sending path as AS_PATH has peer AS %s',\n                      self.remote_as)\n            return\n\n        # If this peer is a route server client, we forward the path\n        # regardless of AS PATH loop, whether the connection is iBGP or eBGP,\n        # or path's communities.\n        if self.is_route_server_client:\n            outgoing_route = OutgoingRoute(path)\n            self.enque_outgoing_msg(outgoing_route)\n\n        if self._neigh_conf.multi_exit_disc:\n            med_attr = path.get_pattr(BGP_ATTR_TYPE_MULTI_EXIT_DISC)\n            if not med_attr:\n                path = bgp_utils.clone_path_and_update_med_for_target_neighbor(\n                    path,\n                    self._neigh_conf.multi_exit_disc\n                )\n\n        # For connected/local-prefixes, we send update to all peers.\n        if path.source is None:\n            # Construct OutgoingRoute specific for this peer and put it in\n            # its sink.\n            outgoing_route = OutgoingRoute(path)\n            self.enque_outgoing_msg(outgoing_route)\n\n        # If path from a bgp-peer is new best path, we share it with\n        # all bgp-peers except the source peer and other peers in his AS.\n        # This is default Junos setting that in Junos can be disabled with\n        # 'advertise-peer-as' setting.\n        elif (self != path.source or\n              self.remote_as != path.source.remote_as):\n            # When BGP speaker receives an UPDATE message from an internal\n            # peer, the receiving BGP speaker SHALL NOT re-distribute the\n            # routing information contained in that UPDATE message to other\n            # internal peers (unless the speaker acts as a BGP Route\n            # Reflector) [RFC4271].\n            if (self.remote_as == self._core_service.asn\n                    and self.remote_as == path.source.remote_as\n                    and isinstance(path.source, Peer)\n                    and not path.source.is_route_reflector_client\n                    and not self.is_route_reflector_client):\n                LOG.debug(\n                    'Skipping sending iBGP route to iBGP peer %s AS %s',\n                    self.ip_address, self.remote_as)\n                return\n\n            # If new best path has community attribute, it should be taken into\n            # account when sending UPDATE to peers.\n            comm_attr = path.get_pattr(BGP_ATTR_TYPE_COMMUNITIES)\n            if comm_attr:\n                comm_attr_na = comm_attr.has_comm_attr(\n                    BGPPathAttributeCommunities.NO_ADVERTISE\n                )\n                # If we have NO_ADVERTISE attribute present, we do not send\n                # UPDATE to any peers\n                if comm_attr_na:\n                    LOG.debug('Path has community attr. NO_ADVERTISE = %s'\n                              '. Hence not advertising to peer',\n                              comm_attr_na)\n                    return\n\n                comm_attr_ne = comm_attr.has_comm_attr(\n                    BGPPathAttributeCommunities.NO_EXPORT\n                )\n                comm_attr_nes = comm_attr.has_comm_attr(\n                    BGPPathAttributeCommunities.NO_EXPORT_SUBCONFED\n                )\n                # If NO_EXPORT_SUBCONFED/NO_EXPORT is one of the attribute, we\n                # do not advertise to eBGP peers as we do not have any\n                # confederation feature at this time.\n                if ((comm_attr_nes or comm_attr_ne) and\n                        (self.remote_as != self._core_service.asn)):\n                    LOG.debug('Skipping sending UPDATE to peer: %s as per '\n                              'community attribute configuration', self)\n                    return\n\n            # Construct OutgoingRoute specific for this peer and put it in\n            # its sink.\n            outgoing_route = OutgoingRoute(path)\n            self.enque_outgoing_msg(outgoing_route)\n            LOG.debug('Enqueued outgoing route %s for peer %s',\n                      outgoing_route.path.nlri, self)", "response": "Communicates path to this peer if it qualifies."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef connection_made(self):\n        LOG.info(\n            'Connection to peer: %s established',\n            self._neigh_conf.ip_address,\n            extra={\n                'resource_name': self._neigh_conf.name,\n                'resource_id': self._neigh_conf.id\n            }\n        )", "response": "Handler for Protocols connection established handler."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when a connection to the peer is lost.", "response": "def connection_lost(self, reason):\n        \"\"\"Protocols connection lost handler.\n        \"\"\"\n        LOG.info(\n            'Connection to peer %s lost, reason: %s Resetting '\n            'retry connect loop: %s' %\n            (self._neigh_conf.ip_address, reason,\n             self._connect_retry_event.is_set()),\n            extra={\n                'resource_name': self._neigh_conf.name,\n                'resource_id': self._neigh_conf.id\n            }\n        )\n        self.state.bgp_state = const.BGP_FSM_IDLE\n        if self._protocol:\n            self._protocol.stop()\n            self._protocol = None\n            # Create new collection for initial RT NLRIs\n            self._init_rtc_nlri_path = []\n            self._sent_init_non_rtc_update = False\n            # Clear sink.\n            self.clear_outgoing_msg_list()\n            # Un-schedule timers\n            self._unschedule_sending_init_updates()\n\n            # Increment the version number of this source.\n            self.version_num += 1\n            self._peer_manager.on_peer_down(self)\n\n            # Check configuration if neighbor is still enabled, we try\n            # reconnecting.\n            if self._neigh_conf.enabled:\n                if not self._connect_retry_event.is_set():\n                    self._connect_retry_event.set()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef has_local_as(self, local_as, max_count=0):\n        _count = 0\n        for as_path_seg in self.value:\n            _count += list(as_path_seg).count(local_as)\n        return _count > max_count", "response": "Check if a local_as is already present on path list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef has_matching_leftmost(self, remote_as):\n        if not self.value or not remote_as:\n            return False\n\n        leftmost_seg = self.path_seg_list[0]\n        if leftmost_seg and leftmost_seg[0] == remote_as:\n            return True\n\n        return False", "response": "Check if the current AS matches the given AS."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef run_command(cmd, redirect_output=True, check_exit_code=True):\n    if redirect_output:\n        stdout = subprocess.PIPE\n    else:\n        stdout = None\n    proc = subprocess.Popen(cmd, cwd=ROOT, stdout=stdout)\n    output = proc.communicate()[0]\n    if check_exit_code and proc.returncode != 0:\n        raise Exception('Command \"%s\" failed.\\n%s' % (' '.join(cmd), output))\n    return output", "response": "Runs a command in an out - of - process shell returning the output of that command."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef create_virtualenv(venv=VENV, install_pip=False):\n    print 'Creating venv...',\n\n    install = ['virtualenv', '-q', venv]\n    run_command(install)\n\n    print 'done.'\n    print 'Installing pip in virtualenv...',\n    if install_pip and \\\n            not run_command(['tools/with_venv.sh', 'easy_install',\n                             'pip>1.0']):\n        die(\"Failed to install pip.\")\n    print 'done.'", "response": "Creates the virtual environment and installs pip only into the virtual environment."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_column_value(manager, table, record, column):\n    row = row_by_name(manager, record, table)\n    value = getattr(row, column, \"\")\n\n    if isinstance(value, list) and len(value) == 1:\n        value = value[0]\n\n    return str(value)", "response": "Get the value of a column in the record."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_speaker(self, **kwargs):\n        try:\n            body = self.vtep_app.add_speaker(**kwargs)\n        except DatapathNotFound as e:\n            return e.to_response(status=404)\n\n        return Response(content_type='application/json',\n                        body=json.dumps(body))", "response": "Adds a new speaker to the current instance of the current instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the info of the BGPSpeaker instance.", "response": "def get_speakers(self, **kwargs):\n        \"\"\"\n        Gets the info of BGPSpeaker instance.\n\n        Usage:\n\n            ======= ================\n            Method  URI\n            ======= ================\n            GET     /vtep/speakers\n            ======= ================\n\n        Example::\n\n            $ curl -X GET http://localhost:8080/vtep/speakers |\n             python -m json.tool\n\n        ::\n\n            {\n                \"172.17.0.1\": {\n                    \"EvpnSpeaker\": {\n                        \"as_number\": 65000,\n                        \"dpid\": 1,\n                        \"neighbors\": {\n                            \"172.17.0.2\": {\n                                \"EvpnNeighbor\": {\n                                    \"address\": \"172.17.0.2\",\n                                    \"remote_as\": 65000,\n                                    \"state\": \"up\"\n                                }\n                            }\n                        },\n                        \"router_id\": \"172.17.0.1\"\n                    }\n                }\n            }\n        \"\"\"\n        try:\n            body = self.vtep_app.get_speaker()\n        except BGPSpeakerNotFound as e:\n            return e.to_response(status=404)\n\n        return Response(content_type='application/json',\n                        body=json.dumps(body))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef del_network(self, **kwargs):\n        try:\n            body = self.vtep_app.del_network(**kwargs)\n        except (BGPSpeakerNotFound, DatapathNotFound, VniNotFound) as e:\n            return e.to_response(status=404)\n\n        return Response(content_type='application/json',\n                        body=json.dumps(body))", "response": "Delete the network for the specified VNI."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef del_client(self, **kwargs):\n        try:\n            body = self.vtep_app.del_client(**kwargs)\n        except (BGPSpeakerNotFound, DatapathNotFound,\n                VniNotFound, ClientNotFound, ClientNotLocal) as e:\n            return Response(body=str(e), status=500)\n\n        return Response(content_type='application/json',\n                        body=json.dumps(body))", "response": "This method is used to register a new client to the specified virtual network."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nimport VPNv4 and 6 or EVPN paths from Global VPN table into VRFv4 or 6 or VRFVPN table into given VRFVPN table.", "response": "def import_all_vpn_paths_to_vrf(self, vrf_table, import_rts=None):\n        \"\"\"Imports VPNv4/6 or EVPN paths from Global/VPN table into given\n        VRFv4/6  or VRFEVPN table.\n        :param vrf_table: Vrf table to which we import\n        :type vrf_table: VrfTable\n        :param import_rts: import RTs to override default import_rts of\n         vrf table for this import\n        :type import_rts: set of strings\n\n        Checks if we have any path RT common with VRF table's import RT.\n        \"\"\"\n        if vrf_table.route_family == Vrf4Table.ROUTE_FAMILY:\n            vpn_table = self.get_vpn4_table()\n        elif vrf_table.route_family == Vrf6Table.ROUTE_FAMILY:\n            vpn_table = self.get_vpn6_table()\n        elif vrf_table.route_family == VrfEvpnTable.ROUTE_FAMILY:\n            vpn_table = self.get_evpn_table()\n        elif vrf_table.route_family == Vrf4FlowSpecTable.ROUTE_FAMILY:\n            vpn_table = self.get_vpnv4fs_table()\n        elif vrf_table.route_family == Vrf6FlowSpecTable.ROUTE_FAMILY:\n            vpn_table = self.get_vpnv6fs_table()\n        elif vrf_table.route_family == L2vpnFlowSpecTable.ROUTE_FAMILY:\n            vpn_table = self.get_l2vpnfs_table()\n        else:\n            raise ValueError('Invalid VRF table route family: %s' %\n                             vrf_table.route_family)\n\n        vrf_table.import_vpn_paths_from_table(vpn_table, import_rts)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef learn_path(self, path):\n        # Get VPN/Global table\n        table = self.get_global_table_by_route_family(path.route_family)\n        gpath_dest = table.insert(path)\n        # Since destination was updated, we enqueue it for processing.\n        self._signal_bus.dest_changed(gpath_dest)", "response": "Inserts path into correct global table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remember_sent_route(self, sent_route):\n        route_family = sent_route.path.route_family\n        table = self.get_global_table_by_route_family(route_family)\n        table.insert_sent_route(sent_route)", "response": "Stores the sent_route in the appropriate table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating global tables as interested RTs changed.", "response": "def on_interesting_rts_change(self, new_global_rts, removed_global_rts):\n        \"\"\"Update global tables as interested RTs changed.\n\n        Adds `new_rts` and removes `removed_rts` rt nlris. Does not check if\n        `new_rts` or `removed_rts` are already present. Schedules refresh\n        request to peers that do not participate in RTC address-family.\n        \"\"\"\n        # We add new RT NLRI and request RR for other peers.\n        if new_global_rts:\n            LOG.debug(\n                'Sending route_refresh to all neighbors that'\n                ' did not negotiate RTC capability.'\n            )\n\n            pm = self._core_service.peer_manager\n            pm.schedule_rr_to_non_rtc_peers()\n        if removed_global_rts:\n            LOG.debug(\n                'Cleaning up global tables as some interested RTs were removed'\n            )\n            self._clean_global_uninteresting_paths()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ipv4_table(self):\n\n        vpn_table = self._global_tables.get(RF_IPv4_UC)\n        # Lazy initialize the table.\n        if not vpn_table:\n            vpn_table = Ipv4Table(self._core_service, self._signal_bus)\n            self._global_tables[RF_IPv4_UC] = vpn_table\n            self._tables[(None, RF_IPv4_UC)] = vpn_table\n\n        return vpn_table", "response": "Returns global IPv4 table. Creates the table if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns global VPNv6 table. Creates the table if it does not exist.", "response": "def get_vpn6_table(self):\n        \"\"\"Returns global VPNv6 table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n        vpn_table = self._global_tables.get(RF_IPv6_VPN)\n        # Lazy initialize the table.\n        if not vpn_table:\n            vpn_table = Vpnv6Table(self._core_service, self._signal_bus)\n            self._global_tables[RF_IPv6_VPN] = vpn_table\n            self._tables[(None, RF_IPv6_VPN)] = vpn_table\n\n        return vpn_table"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_vpn4_table(self):\n        vpn_table = self._global_tables.get(RF_IPv4_VPN)\n        # Lazy initialize the table.\n        if not vpn_table:\n            vpn_table = Vpnv4Table(self._core_service, self._signal_bus)\n            self._global_tables[RF_IPv4_VPN] = vpn_table\n            self._tables[(None, RF_IPv4_VPN)] = vpn_table\n\n        return vpn_table", "response": "Returns global VPNv6 table. Creates the table if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn global EVPN table. Creates the table if it does not exist.", "response": "def get_evpn_table(self):\n        \"\"\"Returns global EVPN table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n        evpn_table = self._global_tables.get(RF_L2_EVPN)\n        # Lazy initialization of the table.\n        if not evpn_table:\n            evpn_table = EvpnTable(self._core_service, self._signal_bus)\n            self._global_tables[RF_L2_EVPN] = evpn_table\n            self._tables[(None, RF_L2_EVPN)] = evpn_table\n\n        return evpn_table"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns global RTC table.", "response": "def get_rtc_table(self):\n        \"\"\"Returns global RTC table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n        rtc_table = self._global_tables.get(RF_RTC_UC)\n        # Lazy initialization of the table.\n        if not rtc_table:\n            rtc_table = RtcTable(self._core_service, self._signal_bus)\n            self._global_tables[RF_RTC_UC] = rtc_table\n            self._tables[(None, RF_RTC_UC)] = rtc_table\n        return rtc_table"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns global IPv4 Flow Specification table. Creates the table if it does not exist.", "response": "def get_ipv4fs_table(self):\n        \"\"\"Returns global IPv4 Flow Specification table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n        ipv4fs_table = self._global_tables.get(RF_IPv4_FLOWSPEC)\n        # Lazy initialization of the table.\n        if not ipv4fs_table:\n            ipv4fs_table = IPv4FlowSpecTable(self._core_service,\n                                             self._signal_bus)\n            self._global_tables[RF_IPv4_FLOWSPEC] = ipv4fs_table\n            self._tables[(None, RF_IPv4_FLOWSPEC)] = ipv4fs_table\n\n        return ipv4fs_table"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn global IPv6 Flow Specification table. Creates the table if it does not exist.", "response": "def get_ipv6fs_table(self):\n        \"\"\"Returns global IPv6 Flow Specification table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n        ipv6fs_table = self._global_tables.get(RF_IPv6_FLOWSPEC)\n        # Lazy initialization of the table.\n        if not ipv6fs_table:\n            ipv6fs_table = IPv6FlowSpecTable(self._core_service,\n                                             self._signal_bus)\n            self._global_tables[RF_IPv6_FLOWSPEC] = ipv6fs_table\n            self._tables[(None, RF_IPv6_FLOWSPEC)] = ipv6fs_table\n\n        return ipv6fs_table"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn global VPNv4 Flow Specification table. Creates the table if it does not exist.", "response": "def get_vpnv4fs_table(self):\n        \"\"\"Returns global VPNv4 Flow Specification table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n        vpnv4fs_table = self._global_tables.get(RF_VPNv4_FLOWSPEC)\n        # Lazy initialization of the table.\n        if not vpnv4fs_table:\n            vpnv4fs_table = VPNv4FlowSpecTable(self._core_service,\n                                               self._signal_bus)\n            self._global_tables[RF_VPNv4_FLOWSPEC] = vpnv4fs_table\n            self._tables[(None, RF_VPNv4_FLOWSPEC)] = vpnv4fs_table\n\n        return vpnv4fs_table"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn global VPNv6 Flow Specification table. Creates the table if it does not exist.", "response": "def get_vpnv6fs_table(self):\n        \"\"\"Returns global VPNv6 Flow Specification table.\n\n        Creates the table if it does not exist.\n        \"\"\"\n        vpnv6fs_table = self._global_tables.get(RF_VPNv6_FLOWSPEC)\n        # Lazy initialization of the table.\n        if not vpnv6fs_table:\n            vpnv6fs_table = VPNv6FlowSpecTable(self._core_service,\n                                               self._signal_bus)\n            self._global_tables[RF_VPNv6_FLOWSPEC] = vpnv6fs_table\n            self._tables[(None, RF_VPNv6_FLOWSPEC)] = vpnv6fs_table\n\n        return vpnv6fs_table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_l2vpnfs_table(self):\n        l2vpnfs_table = self._global_tables.get(RF_L2VPN_FLOWSPEC)\n        # Lazy initialization of the table.\n        if not l2vpnfs_table:\n            l2vpnfs_table = L2VPNFlowSpecTable(self._core_service,\n                                               self._signal_bus)\n            self._global_tables[RF_L2VPN_FLOWSPEC] = l2vpnfs_table\n            self._tables[(None, RF_L2VPN_FLOWSPEC)] = l2vpnfs_table\n\n        return l2vpnfs_table", "response": "Returns the global L2VPN Flow Specification table. Creates the table if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nupdating links to VRF tables.", "response": "def update_vrf_table_links(self, vrf_table, new_imp_rts,\n                               removed_imp_rts):\n        \"\"\"Update mapping from RT to VRF table.\"\"\"\n        assert vrf_table\n        if new_imp_rts:\n            self._link_vrf_table(vrf_table, new_imp_rts)\n        if removed_imp_rts:\n            self._remove_links_to_vrf_table_for_rts(vrf_table,\n                                                    removed_imp_rts)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nre-installs paths from NC with current BGP policy. Iterates over known paths from NC installed in `vrf4_table` and adds new path with path attributes as per current VRF configuration.", "response": "def re_install_net_ctrl_paths(self, vrf_table):\n        \"\"\"Re-installs paths from NC with current BGP policy.\n\n        Iterates over known paths from NC installed in `vrf4_table` and\n        adds new path with path attributes as per current VRF configuration.\n        \"\"\"\n        assert vrf_table\n        for dest in vrf_table.values():\n            for path in dest.known_path_list:\n                if path.source is None:\n                    vrf_table.insert_vrf_path(\n                        nlri=path.nlri,\n                        next_hop=path.nexthop,\n                        gen_lbl=True\n                    )\n        LOG.debug('Re-installed NC paths with current policy for table %s.',\n                  vrf_table)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nremoves any links to given vrf_table.", "response": "def _remove_links_to_vrf_table(self, vrf_table):\n        \"\"\"Removes any links to given `vrf_table`.\"\"\"\n        assert vrf_table\n        vrf_conf = vrf_table.vrf_conf\n        self._remove_links_to_vrf_table_for_rts(vrf_table,\n                                                vrf_conf.import_rts)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating and links VRF tables for given vrf_conf.", "response": "def create_and_link_vrf_table(self, vrf_conf):\n        \"\"\"Factory method to create VRF table for given `vrf_conf`.\n\n        Adds mapping to this table with appropriate scope. Also, adds mapping\n        for import RT of this VRF to created table to facilitate\n        importing/installing of paths from global tables.\n        Returns created table.\n        \"\"\"\n        route_family = vrf_conf.route_family\n\n        if route_family == VRF_RF_IPV4:\n            vrf_table = Vrf4Table\n        elif route_family == VRF_RF_IPV6:\n            vrf_table = Vrf6Table\n        elif route_family == VRF_RF_L2_EVPN:\n            vrf_table = VrfEvpnTable\n        elif route_family == VRF_RF_IPV4_FLOWSPEC:\n            vrf_table = Vrf4FlowSpecTable\n        elif route_family == VRF_RF_IPV6_FLOWSPEC:\n            vrf_table = Vrf6FlowSpecTable\n        elif route_family == VRF_RF_L2VPN_FLOWSPEC:\n            vrf_table = L2vpnFlowSpecTable\n        else:\n            raise ValueError('Unsupported route family for VRF: %s' %\n                             route_family)\n\n        vrf_table = vrf_table(vrf_conf, self._core_service, self._signal_bus)\n        table_id = (vrf_conf.route_dist, route_family)\n        self._tables[table_id] = vrf_table\n\n        assert vrf_table is not None\n        LOG.debug('Added new VrfTable with route_dist:%s and route_family:%s',\n                  vrf_conf.route_dist, route_family)\n\n        import_rts = vrf_conf.import_rts\n        # If VRF is configured with import RT, we put this table\n        # in a list corresponding to this RT for easy access.\n        if import_rts:\n            self._link_vrf_table(vrf_table, import_rts)\n\n        return vrf_table"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _clean_global_uninteresting_paths(self):\n        uninteresting_dest_count = 0\n        interested_rts = self._rt_mgr.global_interested_rts\n        LOG.debug('Cleaning uninteresting paths. Global interested RTs %s',\n                  interested_rts)\n        for route_family in [RF_IPv4_VPN, RF_IPv6_VPN, RF_RTC_UC]:\n            # TODO(PH): We currently do not install RT_NLRI paths based on\n            # extended path attributes (RT)\n            if route_family == RF_RTC_UC:\n                continue\n            table = self.get_global_table_by_route_family(route_family)\n            uninteresting_dest_count += \\\n                table.clean_uninteresting_paths(interested_rts)\n\n        LOG.debug('Found %s number of destinations had uninteresting paths.',\n                  uninteresting_dest_count)", "response": "Clean up global uninteresting paths."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef import_single_vpn_path_to_all_vrfs(self, vpn_path, path_rts=None):\n        LOG.debug('Importing path %s to qualifying VRFs', vpn_path)\n\n        # If this path has no RTs we are done.\n        if not path_rts:\n            LOG.info('Encountered a path with no RTs: %s', vpn_path)\n            return\n\n        # We match path RTs with all VRFs that are interested in them.\n        interested_tables = set()\n\n        # Get route family of VRF to when this VPN Path can be imported to\n        if vpn_path.route_family == RF_IPv4_VPN:\n            route_family = RF_IPv4_UC\n        elif vpn_path.route_family == RF_IPv6_VPN:\n            route_family = RF_IPv6_UC\n        elif vpn_path.route_family == RF_L2_EVPN:\n            route_family = RF_L2_EVPN\n        elif vpn_path.route_family == RF_VPNv4_FLOWSPEC:\n            route_family = RF_IPv4_FLOWSPEC\n        elif vpn_path.route_family == RF_VPNv6_FLOWSPEC:\n            route_family = RF_IPv6_FLOWSPEC\n        elif vpn_path.route_family == RF_L2VPN_FLOWSPEC:\n            route_family = RF_L2VPN_FLOWSPEC\n        else:\n            raise ValueError('Unsupported route family for VRF: %s' %\n                             vpn_path.route_family)\n\n        for rt in path_rts:\n            rt_rf_id = rt + ':' + str(route_family)\n            vrf_rt_tables = self._tables_for_rt.get(rt_rf_id)\n            if vrf_rt_tables:\n                interested_tables.update(vrf_rt_tables)\n\n        if interested_tables:\n            # We iterate over all VRF tables that are interested in the RT\n            # of the given path and import this path into them.\n            route_dist = vpn_path.nlri.route_dist\n            for vrf_table in interested_tables:\n                if (vpn_path.source is not None or\n                        route_dist != vrf_table.vrf_conf.route_dist):\n                    update_vrf_dest = vrf_table.import_vpn_path(vpn_path)\n                    # Queue the destination for further processing.\n                    if update_vrf_dest is not None:\n                        self._signal_bus.\\\n                            dest_changed(update_vrf_dest)\n        else:\n            # If we do not have any VRF with import RT that match with path RT\n            LOG.debug('No VRF table found that imports RTs: %s', path_rts)", "response": "Imports the path to qualifying VRFs."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update_vrf_table(self, route_dist, prefix=None, next_hop=None,\n                         route_family=None, route_type=None, tunnel_type=None,\n                         is_withdraw=False, redundancy_mode=None,\n                         pmsi_tunnel_type=None, **kwargs):\n        \"\"\"Update a BGP route in the VRF table identified by `route_dist`\n        with the given `next_hop`.\n\n        If `is_withdraw` is False, which is the default, add a BGP route\n        to the VRF table identified by `route_dist` with the given\n        `next_hop`.\n        If `is_withdraw` is True, remove a BGP route from the VRF table\n        and the given `next_hop` is ignored.\n\n        If `route_family` is VRF_RF_L2_EVPN, `route_type` and `kwargs`\n        are required to construct EVPN NLRI and `prefix` is ignored.\n\n        ``redundancy_mode`` specifies a redundancy mode type.\n\n`       `pmsi_tunnel_type` specifies the type of the PMSI tunnel attribute\n         used to encode the multicast tunnel identifier.\n        This field is advertised only if route_type is\n        EVPN_MULTICAST_ETAG_ROUTE.\n\n        Returns assigned VPN label.\n        \"\"\"\n        from ryu.services.protocols.bgp.core import BgpCoreError\n\n        assert route_dist\n\n        if is_withdraw:\n            gen_lbl = False\n            next_hop = None\n        else:\n            gen_lbl = True\n            if not (is_valid_ipv4(next_hop) or is_valid_ipv6(next_hop)):\n                raise BgpCoreError(\n                    desc='Invalid IPv4/IPv6 nexthop: %s' % next_hop)\n\n        vrf_table = self._tables.get((route_dist, route_family))\n        if vrf_table is None:\n            raise BgpCoreError(\n                desc='VRF table  does not exist: route_dist=%s, '\n                     'route_family=%s' % (route_dist, route_family))\n\n        vni = kwargs.get('vni', None)\n\n        if route_family == VRF_RF_IPV4:\n            if not is_valid_ipv4_prefix(prefix):\n                raise BgpCoreError(desc='Invalid IPv4 prefix: %s' % prefix)\n            ip, masklen = prefix.split('/')\n            prefix = IPAddrPrefix(int(masklen), ip)\n        elif route_family == VRF_RF_IPV6:\n            if not is_valid_ipv6_prefix(prefix):\n                raise BgpCoreError(desc='Invalid IPv6 prefix: %s' % prefix)\n            ip6, masklen = prefix.split('/')\n            prefix = IP6AddrPrefix(int(masklen), ip6)\n        elif route_family == VRF_RF_L2_EVPN:\n            assert route_type\n            if route_type == EvpnMacIPAdvertisementNLRI.ROUTE_TYPE_NAME:\n                # MPLS labels will be assigned automatically\n                kwargs['mpls_labels'] = []\n            if route_type == EvpnInclusiveMulticastEthernetTagNLRI.ROUTE_TYPE_NAME:\n                # Inclusive Multicast Ethernet Tag Route does not have \"vni\",\n                # omit \"vni\" from \"kwargs\" here.\n                vni = kwargs.pop('vni', None)\n            subclass = EvpnNLRI._lookup_type_name(route_type)\n            kwargs['route_dist'] = route_dist\n            esi = kwargs.get('esi', None)\n            if esi is not None:\n                if isinstance(esi, dict):\n                    esi_type = esi.get('type', 0)\n                    esi_class = EvpnEsi._lookup_type(esi_type)\n                    kwargs['esi'] = esi_class.from_jsondict(esi)\n                else:  # isinstance(esi, numbers.Integral)\n                    kwargs['esi'] = EvpnArbitraryEsi(\n                        type_desc.Int9.from_user(esi))\n            if vni is not None:\n                # Disable to generate MPLS labels,\n                # because encapsulation type is not MPLS.\n                from ryu.services.protocols.bgp.api.prefix import (\n                    TUNNEL_TYPE_VXLAN, TUNNEL_TYPE_NVGRE)\n                assert tunnel_type in [\n                    None, TUNNEL_TYPE_VXLAN, TUNNEL_TYPE_NVGRE]\n                gen_lbl = False\n            prefix = subclass(**kwargs)\n        else:\n            raise BgpCoreError(\n                desc='Unsupported route family %s' % route_family)\n\n        # We do not check if we have a path to given prefix, we issue\n        # withdrawal. Hence multiple withdrawals have not side effect.\n        return vrf_table.insert_vrf_path(\n            nlri=prefix, next_hop=next_hop, gen_lbl=gen_lbl,\n            is_withdraw=is_withdraw, redundancy_mode=redundancy_mode,\n            vni=vni, tunnel_type=tunnel_type,\n            pmsi_tunnel_type=pmsi_tunnel_type)", "response": "Update a BGP route in the VRF table identified by route_dist with the given next_hop."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating a BGP route in the VRF table for Flow Specification.", "response": "def update_flowspec_vrf_table(self, flowspec_family, route_dist, rules,\n                                  actions=None, is_withdraw=False):\n        \"\"\"Update a BGP route in the VRF table for Flow Specification.\n\n        ``flowspec_family`` specifies one of the flowspec family name.\n\n        ``route_dist`` specifies a route distinguisher value.\n\n        ``rules`` specifies NLRIs of Flow Specification as\n        a dictionary type value.\n\n        `` actions`` specifies Traffic Filtering Actions of\n        Flow Specification as a dictionary type value.\n\n        If `is_withdraw` is False, which is the default, add a BGP route\n        to the VRF table identified by `route_dist`.\n        If `is_withdraw` is True, remove a BGP route from the VRF table.\n        \"\"\"\n        from ryu.services.protocols.bgp.core import BgpCoreError\n        from ryu.services.protocols.bgp.api.prefix import (\n            FLOWSPEC_FAMILY_VPNV4,\n            FLOWSPEC_FAMILY_VPNV6,\n            FLOWSPEC_FAMILY_L2VPN,\n        )\n\n        if flowspec_family == FLOWSPEC_FAMILY_VPNV4:\n            vrf_table = self._tables.get((route_dist, VRF_RF_IPV4_FLOWSPEC))\n            prefix = FlowSpecIPv4NLRI.from_user(**rules)\n            try:\n                communities = create_v4flowspec_actions(actions)\n            except ValueError as e:\n                raise BgpCoreError(desc=str(e))\n        elif flowspec_family == FLOWSPEC_FAMILY_VPNV6:\n            vrf_table = self._tables.get((route_dist, VRF_RF_IPV6_FLOWSPEC))\n            prefix = FlowSpecIPv6NLRI.from_user(**rules)\n            try:\n                communities = create_v6flowspec_actions(actions)\n            except ValueError as e:\n                raise BgpCoreError(desc=str(e))\n        elif flowspec_family == FLOWSPEC_FAMILY_L2VPN:\n            vrf_table = self._tables.get((route_dist, VRF_RF_L2VPN_FLOWSPEC))\n            prefix = FlowSpecL2VPNNLRI.from_user(route_dist, **rules)\n            try:\n                communities = create_l2vpnflowspec_actions(actions)\n            except ValueError as e:\n                raise BgpCoreError(desc=str(e))\n        else:\n            raise BgpCoreError(\n                desc='Unsupported flowspec_family %s' % flowspec_family)\n\n        if vrf_table is None:\n            raise BgpCoreError(\n                desc='VRF table does not exist: route_dist=%s, '\n                     'flowspec_family=%s' % (route_dist, flowspec_family))\n\n        # We do not check if we have a path to given prefix, we issue\n        # withdrawal. Hence multiple withdrawals have not side effect.\n        vrf_table.insert_vrffs_path(\n            nlri=prefix, communities=communities,\n            is_withdraw=is_withdraw)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef update_global_table(self, prefix, next_hop=None, is_withdraw=False):\n        src_ver_num = 1\n        peer = None\n        # set mandatory path attributes\n        origin = BGPPathAttributeOrigin(BGP_ATTR_ORIGIN_IGP)\n        aspath = BGPPathAttributeAsPath([[]])\n\n        pathattrs = OrderedDict()\n        pathattrs[BGP_ATTR_TYPE_ORIGIN] = origin\n        pathattrs[BGP_ATTR_TYPE_AS_PATH] = aspath\n\n        net = netaddr.IPNetwork(prefix)\n        addr = str(net.ip)\n        masklen = net.prefixlen\n        if ip.valid_ipv4(addr):\n            _nlri = IPAddrPrefix(masklen, addr)\n            if next_hop is None:\n                next_hop = '0.0.0.0'\n            p = Ipv4Path\n        else:\n            _nlri = IP6AddrPrefix(masklen, addr)\n            if next_hop is None:\n                next_hop = '::'\n            p = Ipv6Path\n\n        new_path = p(peer, _nlri, src_ver_num,\n                     pattrs=pathattrs, nexthop=next_hop,\n                     is_withdraw=is_withdraw)\n\n        # add to global table and propagates to neighbors\n        self.learn_path(new_path)", "response": "Update a BGP route in the Global table for the given prefix with the given next_hop."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the BGP route in the Global table for Flow Specification.", "response": "def update_flowspec_global_table(self, flowspec_family, rules,\n                                     actions=None, is_withdraw=False):\n        \"\"\"Update a BGP route in the Global table for Flow Specification.\n\n        ``flowspec_family`` specifies one of the Flow Specification\n         family name.\n\n        ``rules`` specifies NLRIs of Flow Specification as\n        a dictionary type value.\n\n        `` actions`` specifies Traffic Filtering Actions of\n        Flow Specification as a dictionary type value.\n\n        If `is_withdraw` is False, which is the default, add a BGP route\n        to the Global table.\n        If `is_withdraw` is True, remove a BGP route from the Global table.\n        \"\"\"\n\n        from ryu.services.protocols.bgp.core import BgpCoreError\n        from ryu.services.protocols.bgp.api.prefix import (\n            FLOWSPEC_FAMILY_IPV4,\n            FLOWSPEC_FAMILY_IPV6,\n            FLOWSPEC_FAMILY_L2VPN,\n        )\n\n        src_ver_num = 1\n        peer = None\n\n        # set mandatory path attributes\n        origin = BGPPathAttributeOrigin(BGP_ATTR_ORIGIN_IGP)\n        aspath = BGPPathAttributeAsPath([[]])\n\n        pathattrs = OrderedDict()\n        pathattrs[BGP_ATTR_TYPE_ORIGIN] = origin\n        pathattrs[BGP_ATTR_TYPE_AS_PATH] = aspath\n\n        if flowspec_family == FLOWSPEC_FAMILY_IPV4:\n            _nlri = FlowSpecIPv4NLRI.from_user(**rules)\n            p = IPv4FlowSpecPath\n\n            try:\n                communities = create_v4flowspec_actions(actions)\n            except ValueError as e:\n                raise BgpCoreError(desc=str(e))\n\n            if communities:\n                pathattrs[BGP_ATTR_TYPE_EXTENDED_COMMUNITIES] = (\n                    BGPPathAttributeExtendedCommunities(\n                        communities=communities))\n        elif flowspec_family == FLOWSPEC_FAMILY_IPV6:\n            _nlri = FlowSpecIPv6NLRI.from_user(**rules)\n            p = IPv6FlowSpecPath\n\n            try:\n                communities = create_v6flowspec_actions(actions)\n            except ValueError as e:\n                raise BgpCoreError(desc=str(e))\n\n            if communities:\n                pathattrs[BGP_ATTR_TYPE_EXTENDED_COMMUNITIES] = (\n                    BGPPathAttributeExtendedCommunities(\n                        communities=communities))\n        elif flowspec_family == FLOWSPEC_FAMILY_L2VPN:\n            _nlri = FlowSpecL2VPNNLRI.from_user(**rules)\n            p = L2vpnFlowSpecPath\n\n            try:\n                communities = create_l2vpnflowspec_actions(actions)\n            except ValueError as e:\n                raise BgpCoreError(desc=str(e))\n\n            if communities:\n                pathattrs[BGP_ATTR_TYPE_EXTENDED_COMMUNITIES] = (\n                    BGPPathAttributeExtendedCommunities(\n                        communities=communities))\n        else:\n            raise BgpCoreError(\n                desc='Unsupported flowspec family %s' % flowspec_family)\n\n        new_path = p(peer, _nlri, src_ver_num,\n                     pattrs=pathattrs, is_withdraw=is_withdraw)\n\n        # add to global table and propagates to neighbors\n        self.learn_path(new_path)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clean_stale_routes(self, peer, route_family=None):\n\n        if route_family is not None:\n            if route_family not in SUPPORTED_GLOBAL_RF:\n                raise ValueError('Given route family %s is not supported.' %\n                                 route_family)\n\n            tables = [self._global_tables.get(route_family)]\n        else:\n            tables = self._global_tables.values()\n        for table in tables:\n            table.cleanup_paths_for_peer(peer)", "response": "Removes old routes from peer from route_family table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a new port entry to the cache.", "response": "def port_add(self, dpid, port, mac):\n        \"\"\"\n        :returns: old port if learned. (this may be = port)\n                  None otherwise\n        \"\"\"\n        old_port = self.mac_to_port[dpid].get(mac, None)\n        self.mac_to_port[dpid][mac] = port\n\n        if old_port is not None and old_port != port:\n            LOG.debug('port_add: 0x%016x 0x%04x %s',\n                      dpid, port, haddr_to_str(mac))\n\n        return old_port"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _register_make(cls):\n    '''class decorator to Register mf make'''\n    assert cls.nxm_headers is not None\n    assert cls.nxm_headers is not []\n    for nxm_header in cls.nxm_headers:\n        assert nxm_header not in _MF_FIELDS\n        _MF_FIELDS[nxm_header] = cls.make\n    return cls", "response": "class decorator to Register mf make"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef match_tuple(self):\n        assert self.flow_format() == ofproto_v1_0.NXFF_OPENFLOW10\n        wildcards = ofproto_v1_0.OFPFW_ALL\n\n        if not self.wc.wildcards & FWW_IN_PORT:\n            wildcards &= ~ofproto_v1_0.OFPFW_IN_PORT\n\n        if self.flow.dl_src != mac.DONTCARE:\n            wildcards &= ~ofproto_v1_0.OFPFW_DL_SRC\n\n        if self.flow.dl_dst != mac.DONTCARE:\n            wildcards &= ~ofproto_v1_0.OFPFW_DL_DST\n\n        if not self.wc.wildcards & FWW_DL_TYPE:\n            wildcards &= ~ofproto_v1_0.OFPFW_DL_TYPE\n\n        if self.flow.dl_vlan != 0:\n            wildcards &= ~ofproto_v1_0.OFPFW_DL_VLAN\n\n        if self.flow.dl_vlan_pcp != 0:\n            wildcards &= ~ofproto_v1_0.OFPFW_DL_VLAN_PCP\n\n        if self.flow.nw_tos != 0:\n            wildcards &= ~ofproto_v1_0.OFPFW_NW_TOS\n\n        if self.flow.nw_proto != 0:\n            wildcards &= ~ofproto_v1_0.OFPFW_NW_PROTO\n\n        if self.wc.nw_src_mask != 0 and \"01\" not in bin(self.wc.nw_src_mask):\n            wildcards &= ~ofproto_v1_0.OFPFW_NW_SRC_MASK\n            maskbits = (bin(self.wc.nw_src_mask).count(\"0\") - 1)\n            wildcards |= (maskbits << ofproto_v1_0.OFPFW_NW_SRC_SHIFT)\n\n        if self.wc.nw_dst_mask != 0 and \"01\" not in bin(self.wc.nw_dst_mask):\n            wildcards &= ~ofproto_v1_0.OFPFW_NW_DST_MASK\n            maskbits = (bin(self.wc.nw_dst_mask).count(\"0\") - 1)\n            wildcards |= (maskbits << ofproto_v1_0.OFPFW_NW_DST_SHIFT)\n\n        if self.flow.tp_src != 0:\n            wildcards &= ~ofproto_v1_0.OFPFW_TP_SRC\n\n        if self.flow.tp_dst != 0:\n            wildcards &= ~ofproto_v1_0.OFPFW_TP_DST\n\n        return (wildcards, self.flow.in_port, self.flow.dl_src,\n                self.flow.dl_dst, self.flow.dl_vlan, self.flow.dl_vlan_pcp,\n                self.flow.dl_type, self.flow.nw_tos & IP_DSCP_MASK,\n                self.flow.nw_proto, self.flow.nw_src, self.flow.nw_dst,\n                self.flow.tp_src, self.flow.tp_dst)", "response": "return a tuple which can be used as args for the match of the flow."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef load_source(name, pathname):\n    if six.PY2:\n        import imp\n        return imp.load_source(name, pathname)\n    else:\n        loader = importlib.machinery.SourceFileLoader(name, pathname)\n        return loader.load_module(name)", "response": "This function provides the backward compatibility for imp. load_source in Python 2."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsplits a str of IP address and port pair into host and port.", "response": "def _split_addr(addr):\n    \"\"\"\n    Splits a str of IP address and port pair into (host, port).\n\n    Example::\n\n        >>> _split_addr('127.0.0.1:6653')\n        ('127.0.0.1', 6653)\n        >>> _split_addr('[::1]:6653')\n        ('::1', 6653)\n\n    Raises ValueError if invalid format.\n\n    :param addr: A pair of IP address and port.\n    :return: IP address and port\n    \"\"\"\n    e = ValueError('Invalid IP address and port pair: \"%s\"' % addr)\n    pair = addr.rsplit(':', 1)\n    if len(pair) != 2:\n        raise e\n\n    addr, port = pair\n    if addr.startswith('[') and addr.endswith(']'):\n        addr = addr.lstrip('[').rstrip(']')\n        if not ip.valid_ipv6(addr):\n            raise e\n    elif not ip.valid_ipv4(addr):\n        raise e\n\n    return addr, int(port, 0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef may_add_vlan(packet, vlan_id):\n    if vlan_id is None:\n        return\n\n    e = packet.protocols[0]\n    assert isinstance(e, ethernet.ethernet)\n    v = vlan.vlan(0, 0, vlan_id, e.ethertype)\n    e.ethertype = ether.ETH_TYPE_8021Q\n    packet.add_protocol(v)", "response": "Add a VLAN to the packet."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn datapath corresponding to dpid", "response": "def get_dp(app, dpid):\n    \"\"\"\n    :type dpid: datapath id\n    :param dpid:\n    :rtype: ryu.controller.controller.Datapath\n    :returns: datapath corresponding to dpid\n    \"\"\"\n    switches = topo_api.get_switch(app, dpid)\n    if not switches:\n        return None\n    assert len(switches) == 1\n    return switches[0].dp"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncloning given RT NLRI path and updates it with new RT_NLRI AS.", "response": "def clone_rtcpath_update_rt_as(path, new_rt_as):\n    \"\"\"Clones given RT NLRI `path`, and updates it with new RT_NLRI AS.\n\n        Parameters:\n            - `path`: (Path) RT_NLRI path\n            - `new_rt_as`: AS value of cloned paths' RT_NLRI\n    \"\"\"\n    assert path and new_rt_as\n    if not path or path.route_family != RF_RTC_UC:\n        raise ValueError('Expected RT_NLRI path')\n    old_nlri = path.nlri\n    new_rt_nlri = RouteTargetMembershipNLRI(new_rt_as, old_nlri.route_target)\n    return RtcPath(path.source, new_rt_nlri, path.source_version_num,\n                   pattrs=path.pathattr_map, nexthop=path.nexthop,\n                   is_withdraw=path.is_withdraw)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert an IPv4 address string format to a four byte long.", "response": "def from_inet_ptoi(bgp_id):\n    \"\"\"Convert an IPv4 address string format to a four byte long.\n    \"\"\"\n    four_byte_id = None\n    try:\n        four_byte_id = ip.ipv4_to_int(bgp_id)\n    except ValueError:\n        LOG.debug('Invalid bgp id given for conversion to integer value %s',\n                  bgp_id)\n\n    return four_byte_id"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_unknown_opttrans_attr(path):\n    path_attrs = path.pathattr_map\n    unknown_opt_tran_attrs = {}\n    for _, attr in path_attrs.items():\n        if (isinstance(attr, BGPPathAttributeUnknown) and\n                attr.flags & (BGP_ATTR_FLAG_OPTIONAL |\n                              BGP_ATTR_FLAG_TRANSITIVE)) or \\\n                isinstance(attr, BGPPathAttributeAs4Path) or \\\n                isinstance(attr, BGPPathAttributeAs4Aggregator):\n            unknown_opt_tran_attrs[attr.type] = attr\n\n    return unknown_opt_tran_attrs", "response": "Utility method that gives a dict of unknown and unsupported optional - transitive path attributes of path."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_end_of_rib_update():\n    mpunreach_attr = BGPPathAttributeMpUnreachNLRI(RF_IPv4_VPN.afi,\n                                                   RF_IPv4_VPN.safi,\n                                                   [])\n    eor = BGPUpdate(path_attributes=[mpunreach_attr])\n    return eor", "response": "Construct end - of - ribble update instance."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating an instance of BGP Route Target or Route Origin Community.", "response": "def create_rt_extended_community(value, subtype=2):\n    \"\"\"\n    Creates an instance of the BGP Route Target Community (if \"subtype=2\")\n    or Route Origin Community (\"subtype=3\").\n\n    :param value: String of Route Target or Route Origin value.\n    :param subtype: Subtype of Extended Community.\n    :return: An instance of Route Target or Route Origin Community.\n    \"\"\"\n    global_admin, local_admin = value.split(':')\n    local_admin = int(local_admin)\n    if global_admin.isdigit() and 0 <= int(global_admin) <= 0xffff:\n        ext_com = BGPTwoOctetAsSpecificExtendedCommunity(\n            subtype=subtype,\n            as_number=int(global_admin),\n            local_administrator=local_admin)\n    elif global_admin.isdigit() and 0xffff < int(global_admin) <= 0xffffffff:\n        ext_com = BGPFourOctetAsSpecificExtendedCommunity(\n            subtype=subtype,\n            as_number=int(global_admin),\n            local_administrator=local_admin)\n    elif ip.valid_ipv4(global_admin):\n        ext_com = BGPIPv4AddressSpecificExtendedCommunity(\n            subtype=subtype,\n            ipv4_address=global_admin,\n            local_administrator=local_admin)\n    else:\n        raise ValueError(\n            'Invalid Route Target or Route Origin value: %s' % value)\n\n    return ext_com"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create_v4flowspec_actions(actions=None):\n    from ryu.services.protocols.bgp.api.prefix import (\n        FLOWSPEC_ACTION_TRAFFIC_RATE,\n        FLOWSPEC_ACTION_TRAFFIC_ACTION,\n        FLOWSPEC_ACTION_REDIRECT,\n        FLOWSPEC_ACTION_TRAFFIC_MARKING,\n    )\n\n    # Supported action type for IPv4 and VPNv4.\n    action_types = {\n        FLOWSPEC_ACTION_TRAFFIC_RATE: BGPFlowSpecTrafficRateCommunity,\n        FLOWSPEC_ACTION_TRAFFIC_ACTION: BGPFlowSpecTrafficActionCommunity,\n        FLOWSPEC_ACTION_REDIRECT: BGPFlowSpecRedirectCommunity,\n        FLOWSPEC_ACTION_TRAFFIC_MARKING: BGPFlowSpecTrafficMarkingCommunity,\n    }\n\n    return _create_actions(actions, action_types)", "response": "Create list of traffic filtering actions for Ipv4 Flow Specification and VPNv4 Flow Specification."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef create_l2vpnflowspec_actions(actions=None):\n    from ryu.services.protocols.bgp.api.prefix import (\n        FLOWSPEC_ACTION_TRAFFIC_RATE,\n        FLOWSPEC_ACTION_TRAFFIC_ACTION,\n        FLOWSPEC_ACTION_REDIRECT,\n        FLOWSPEC_ACTION_TRAFFIC_MARKING,\n        FLOWSPEC_ACTION_VLAN,\n        FLOWSPEC_ACTION_TPID,\n    )\n\n    # Supported action type for L2VPN.\n    action_types = {\n        FLOWSPEC_ACTION_TRAFFIC_RATE: BGPFlowSpecTrafficRateCommunity,\n        FLOWSPEC_ACTION_TRAFFIC_ACTION: BGPFlowSpecTrafficActionCommunity,\n        FLOWSPEC_ACTION_REDIRECT: BGPFlowSpecRedirectCommunity,\n        FLOWSPEC_ACTION_TRAFFIC_MARKING: BGPFlowSpecTrafficMarkingCommunity,\n        FLOWSPEC_ACTION_VLAN: BGPFlowSpecVlanActionCommunity,\n        FLOWSPEC_ACTION_TPID: BGPFlowSpecTPIDActionCommunity,\n    }\n\n    return _create_actions(actions, action_types)", "response": "Create list of traffic filtering actions for L2VPN Flow Specification."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_packet_type(cls, type_):\n        if type_ <= ether.ETH_TYPE_IEEE802_3:\n            type_ = ether.ETH_TYPE_IEEE802_3\n        return cls._TYPES.get(type_)", "response": "Override method for the Length and Type field."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sql_function(func):\n    @functools.wraps(func)\n    def _wrapper(session, *args, **kwargs):\n        ret = None\n        try:\n            ret = func(session, *args, **kwargs)\n            if session.dirty:\n                # If the given function has any update to records,\n                # commits them.\n                session.commit()\n        except Exception as e:\n            # If any exception raised, rollbacks the transaction.\n            LOG.error('Error in %s: %s', func.__name__, e)\n            if session.dirty:\n                LOG.error('Do rolling back %s table',\n                          session.dirty[0].__tablename__)\n                session.rollback()\n\n        return ret\n\n    return _wrapper", "response": "Decorator for SQL functions that modify the records safely."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_msg(app, msg, reply_cls=None, reply_multi=False):\n    return app.send_request(event.SendMsgRequest(msg=msg,\n                                                 reply_cls=reply_cls,\n                                                 reply_multi=reply_multi))()", "response": "Send an OpenFlow message and wait for reply messages."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nstarting a new context using provided configuration.", "response": "def start(**kwargs):\n    \"\"\"Starts new context using provided configuration.\n\n    Raises RuntimeConfigError if a context is already active.\n    \"\"\"\n    if CORE_MANAGER.started:\n        raise RuntimeConfigError('Current context has to be stopped to start '\n                                 'a new context.')\n\n    try:\n        waiter = kwargs.pop('waiter')\n    except KeyError:\n        waiter = hub.Event()\n    common_config = CommonConf(**kwargs)\n    hub.spawn(CORE_MANAGER.start, *[], **{'common_conf': common_config,\n                                          'waiter': waiter})\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the given ip for use as RPC server address.", "response": "def validate_rpc_host(ip):\n    \"\"\"\n    Validates the given ip for use as RPC server address.\n    \"\"\"\n    if not is_valid_ipv4(ip) and not is_valid_ipv6(ip):\n        raise ApplicationException(\n            desc='Invalid RPC ip address: %s' % ip)\n    return ip"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nloads the configuration from a given file.", "response": "def load_config(config_file):\n    \"\"\"\n    Validates the given file for use as the settings file for BGPSpeaker\n    and loads the configuration from the given file as a module instance.\n    \"\"\"\n    if not config_file or not os.path.isfile(config_file):\n        raise ApplicationException(\n            desc='Invalid configuration file: %s' % config_file)\n\n    # Loads the configuration from the given file, if available.\n    try:\n        return load_source('bgpspeaker.application.settings', config_file)\n    except Exception as e:\n        raise ApplicationException(desc=str(e))"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nstart the BGPSpeaker instance based on the given settings.", "response": "def _start_speaker(self, settings):\n        \"\"\"\n        Starts BGPSpeaker using the given settings.\n        \"\"\"\n        # Check required settings.\n        _required_settings = (\n            LOCAL_AS,\n            ROUTER_ID,\n        )\n        for required in _required_settings:\n            if required not in settings:\n                raise ApplicationException(\n                    desc='Required BGP configuration missing: %s' % required)\n\n        # Set event notify handlers if no corresponding handler specified.\n        settings.setdefault(\n            'best_path_change_handler', self._notify_best_path_changed_event)\n        settings.setdefault(\n            'adj_rib_in_change_handler', self._notify_adj_rib_in_changed_event)\n        settings.setdefault(\n            'peer_down_handler', self._notify_peer_down_event)\n        settings.setdefault(\n            'peer_up_handler', self._notify_peer_up_event)\n\n        # Pop settings other than creating BGPSpeaker instance.\n        neighbors_settings = settings.pop('neighbors', [])\n        vrfs_settings = settings.pop('vrfs', [])\n        routes_settings = settings.pop('routes', [])\n\n        # Create BGPSpeaker instance.\n        LOG.debug('Starting BGPSpeaker...')\n        settings.setdefault('as_number', settings.pop(LOCAL_AS))\n        self.speaker = BGPSpeaker(**settings)\n\n        # Add neighbors.\n        LOG.debug('Adding neighbors...')\n        self._add_neighbors(neighbors_settings)\n\n        # Add VRFs.\n        LOG.debug('Adding VRFs...')\n        self._add_vrfs(vrfs_settings)\n\n        # Add routes\n        LOG.debug('Adding routes...')\n        self._add_routes(routes_settings)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nadds BGP neighbors from the given settings.", "response": "def _add_neighbors(self, settings):\n        \"\"\"\n        Add BGP neighbors from the given settings.\n\n        All valid neighbors are loaded.\n        Miss-configured neighbors are ignored and errors are logged.\n        \"\"\"\n        for neighbor_settings in settings:\n            LOG.debug('Adding neighbor settings: %s', neighbor_settings)\n            try:\n                self.speaker.neighbor_add(**neighbor_settings)\n            except RuntimeConfigError as e:\n                LOG.exception(e)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _add_vrfs(self, settings):\n        for vrf_settings in settings:\n            LOG.debug('Adding VRF settings: %s', vrf_settings)\n            try:\n                self.speaker.vrf_add(**vrf_settings)\n            except RuntimeConfigError as e:\n                LOG.exception(e)", "response": "Add BGP VRFs from the given settings."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _add_routes(self, settings):\n        for route_settings in settings:\n            if 'prefix' in route_settings:\n                prefix_add = self.speaker.prefix_add\n            elif 'route_type' in route_settings:\n                prefix_add = self.speaker.evpn_prefix_add\n            elif 'flowspec_family' in route_settings:\n                prefix_add = self.speaker.flowspec_prefix_add\n            else:\n                LOG.debug('Skip invalid route settings: %s', route_settings)\n                continue\n\n            LOG.debug('Adding route settings: %s', route_settings)\n            try:\n                prefix_add(**route_settings)\n            except RuntimeConfigError as e:\n                LOG.exception(e)", "response": "Add BGP routes from given settings."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef valid_ovsdb_addr(addr):\n    # Assumes Unix socket format: \"unix:file\"\n    m = re.match(r'unix:(\\S+)', addr)\n    if m:\n        file = m.group(1)\n        return os.path.isfile(file)\n    # Assumes TCP/SSL socket format: \"tcp:ip:port\" or \"ssl:ip:port\"\n    m = re.match(r'(tcp|ssl):(\\S+):(\\d+)', addr)\n    if m:\n        address = m.group(2)\n        port = m.group(3)\n        if '[' in address:\n            address = address.strip('[').strip(']')\n            return ip.valid_ipv6(address) and port.isdigit()\n        else:\n            return ip.valid_ipv4(address) and port.isdigit()\n    # Assumes invalid format or unsupported type\n    return False", "response": "Returns True if the given addr is valid OVSDB server address otherwise False."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_port(self, br_name, port_name, may_exist, fake_iface,\n                 iface_names, settings=None):\n        \"\"\"\n        :type settings: list of (column, value_json)\n                                where column is str,\n                                      value_json is json that is represented\n                                      by Datum.to_json()\n        \"\"\"\n        settings = settings or []\n\n        self.populate_cache()\n        if may_exist:\n            vsctl_port = self.find_port(port_name, False)\n            if vsctl_port:\n                want_names = set(iface_names)\n                have_names = set(ovsrec_iface.name for ovsrec_iface in\n                                 vsctl_port.port_cfg.interfaces)\n                if vsctl_port.bridge().name != br_name:\n                    vsctl_fatal('\"%s\" but %s is actually attached to '\n                                'vsctl_bridge %s' %\n                                (br_name, port_name, vsctl_port.bridge().name))\n                if want_names != have_names:\n                    want_names_string = ','.join(want_names)\n                    have_names_string = ','.join(have_names)\n                    vsctl_fatal('\"%s\" but %s actually has interface(s) %s' %\n                                (want_names_string,\n                                 port_name, have_names_string))\n                return\n        self.check_conflicts(port_name,\n                             'cannot create a port named %s' % port_name)\n        for iface_name in iface_names:\n            self.check_conflicts(\n                iface_name, 'cannot create an interface named %s' % iface_name)\n\n        vsctl_bridge = self.find_bridge(br_name, True)\n        ifaces = []\n        for iface_name in iface_names:\n            ovsrec_iface = self.txn.insert(\n                self.idl.tables[vswitch_idl.OVSREC_TABLE_INTERFACE])\n            ovsrec_iface.name = iface_name\n            ifaces.append(ovsrec_iface)\n\n        ovsrec_port = self.txn.insert(\n            self.idl.tables[vswitch_idl.OVSREC_TABLE_PORT])\n        ovsrec_port.name = port_name\n        ovsrec_port.interfaces = ifaces\n        ovsrec_port.bond_fake_iface = fake_iface\n\n        if vsctl_bridge.parent:\n            tag = vsctl_bridge.vlan\n            ovsrec_port.tag = tag\n        for column, value in settings:\n            # TODO:XXX self.symtab:\n            self.set_column(ovsrec_port, column, value)\n\n        if vsctl_bridge.parent:\n            ovsrec_bridge = vsctl_bridge.parent.br_cfg\n        else:\n            ovsrec_bridge = vsctl_bridge.br_cfg\n        self.bridge_insert_port(ovsrec_bridge, ovsrec_port)\n        vsctl_port = self.add_port_to_cache(vsctl_bridge, ovsrec_port)\n        for ovsrec_iface in ifaces:\n            self.add_iface_to_cache(vsctl_port, ovsrec_iface)", "response": "Adds a new port to the VSwitch."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef parse_column_key(setting_string):\n        if ':' in setting_string:\n            # splits <column>:<key> into <column> and <key>\n            column, key = setting_string.split(':', 1)\n        else:\n            # stores <column> and <value>=None\n            column = setting_string\n            key = None\n\n        return column, key", "response": "Parses a string formatted in column[:key ] and returns column and key"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_column_key_value(table_schema, setting_string):\n        if ':' in setting_string:\n            # splits <column>:<key>=<value> into <column> and <key>=<value>\n            column, value = setting_string.split(':', 1)\n        elif '=' in setting_string:\n            # splits <column>=<value> into <column> and <value>\n            column, value = setting_string.split('=', 1)\n        else:\n            # stores <column> and <value>=None\n            column = setting_string\n            value = None\n\n        if value is not None:\n            type_ = table_schema.columns[column].type\n            value = datum_from_string(type_, value)\n\n        return column, value", "response": "Parses setting_string into column and value and returns column and value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _run_command(self, commands):\n        all_commands = {\n            # Open vSwitch commands.\n            'init': (None, self._cmd_init),\n            'show': (self._pre_cmd_show, self._cmd_show),\n            # 'emer-reset':\n\n            # Bridge commands.\n            'add-br': (self._pre_add_br, self._cmd_add_br),\n            'del-br': (self._pre_get_info, self._cmd_del_br),\n            'list-br': (self._pre_get_info, self._cmd_list_br),\n            'br-exists': (self._pre_get_info, self._cmd_br_exists),\n            'br-to-vlan': (self._pre_get_info, self._cmd_br_to_vlan),\n            'br-to-parent': (self._pre_get_info, self._cmd_br_to_parent),\n            'br-set-external-id': (self._pre_cmd_br_set_external_id,\n                                   self._cmd_br_set_external_id),\n            'br-get-external-id': (self._pre_cmd_br_get_external_id,\n                                   self._cmd_br_get_external_id),\n\n            # Port. commands\n            'list-ports': (self._pre_get_info, self._cmd_list_ports),\n            'add-port': (self._pre_cmd_add_port, self._cmd_add_port),\n            'add-bond': (self._pre_cmd_add_bond, self._cmd_add_bond),\n            'del-port': (self._pre_get_info, self._cmd_del_port),\n            'port-to-br': (self._pre_get_info, self._cmd_port_to_br),\n\n            # Interface commands.\n            'list-ifaces': (self._pre_get_info, self._cmd_list_ifaces),\n            'iface-to-br': (self._pre_get_info, self._cmd_iface_to_br),\n\n            # Controller commands.\n            'get-controller': (self._pre_controller, self._cmd_get_controller),\n            'del-controller': (self._pre_controller, self._cmd_del_controller),\n            'set-controller': (self._pre_controller, self._cmd_set_controller),\n            'get-fail-mode': (self._pre_fail_mode, self._cmd_get_fail_mode),\n            'del-fail-mode': (self._pre_fail_mode, self._cmd_del_fail_mode),\n            'set-fail-mode': (self._pre_fail_mode, self._cmd_set_fail_mode),\n\n            # Manager commands.\n            # 'get-manager':\n            # 'del-manager':\n            # 'set-manager':\n\n            # SSL commands.\n            # 'get-ssl':\n            # 'del-ssl':\n            # 'set-ssl':\n\n            # Auto Attach commands.\n            # 'add-aa-mapping':\n            # 'del-aa-mapping':\n            # 'get-aa-mapping':\n\n            # Switch commands.\n            # 'emer-reset':\n\n            # Database commands.\n            'list': (self._pre_cmd_list, self._cmd_list),\n            'find': (self._pre_cmd_find, self._cmd_find),\n            'get': (self._pre_cmd_get, self._cmd_get),\n            'set': (self._pre_cmd_set, self._cmd_set),\n            'add': (self._pre_cmd_add, self._cmd_add),\n            'remove': (self._pre_cmd_remove, self._cmd_remove),\n            'clear': (self._pre_cmd_clear, self._cmd_clear),\n            # 'create':\n            # 'destroy':\n            # 'wait-until':\n\n            # Utility commands. (No corresponding command in ovs-vsctl)\n            'set-qos': (self._pre_cmd_set_qos, self._cmd_set_qos),\n            'set-queue': (self._pre_cmd_set_queue, self._cmd_set_queue),\n            'del-qos': (self._pre_get_info, self._cmd_del_qos),\n            # for quantum_adapter\n            'list-ifaces-verbose': (self._pre_cmd_list_ifaces_verbose,\n                                    self._cmd_list_ifaces_verbose),\n        }\n\n        for command in commands:\n            funcs = all_commands[command.command]\n            command._prerequisite, command._run = funcs\n        self._do_main(commands)", "response": "Runs the commands in the command list."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run_command(self, commands, timeout_sec=None, exception=None):\n        if timeout_sec is None:\n            self._run_command(commands)\n        else:\n            with hub.Timeout(timeout_sec, exception):\n                self._run_command(commands)", "response": "Executes the given commands and sends the results to the OVSDB server."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncheck if the value of the column_value is valid.", "response": "def _check_value(self, ovsrec_row, column_value):\n        \"\"\"\n        :type column_value: tuple of column and value_json\n        \"\"\"\n        column, value_json = column_value\n        column_schema = ovsrec_row._table.columns[column]\n        value = ovs.db.data.Datum.from_json(\n            column_schema.type, value_json).to_python(ovs.db.idl._uuid_to_row)\n        datum = getattr(ovsrec_row, column)\n        if column_schema.type.is_map():\n            for k, v in value.items():\n                if k in datum and datum[k] == v:\n                    return True\n        elif datum == value:\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfinds all the tables in the specified table.", "response": "def _find(self, ctx, table_name, column_values):\n        \"\"\"\n        :type column_values: list of (column, value_json)\n        \"\"\"\n        result = []\n        for ovsrec_row in ctx.idl.tables[table_name].rows.values():\n            LOG.debug('ovsrec_row %s', ovsrec_row_to_string(ovsrec_row))\n            if all(self._check_value(ovsrec_row, column_value)\n                   for column_value in column_values):\n                result.append(ovsrec_row)\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _set(self, ctx, table_name, record_id, column_values):\n        vsctl_table = self._get_table(table_name)\n        ovsrec_row = ctx.must_get_row(vsctl_table, record_id)\n        for column, value in column_values:\n            ctx.set_column(ovsrec_row, column, value)\n        ctx.invalidate_cache()", "response": "Set the values of the specified column in the specified table."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _add(self, ctx, table_name, record_id, column_values):\n        vsctl_table = self._get_table(table_name)\n        ovsrec_row = ctx.must_get_row(vsctl_table, record_id)\n        for column, value in column_values:\n            ctx.add_column(ovsrec_row, column, value)\n        ctx.invalidate_cache()", "response": "Add a set of values to the record."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nremoving the values from the specified column_values.", "response": "def _remove(self, ctx, table_name, record_id, column_values):\n        \"\"\"\n        :type column_values: list of (column, value_json)\n        \"\"\"\n        vsctl_table = self._get_table(table_name)\n        ovsrec_row = ctx.must_get_row(vsctl_table, record_id)\n        for column, value in column_values:\n            ctx.remove_column(ovsrec_row, column, value)\n        ctx.invalidate_cache()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef validate(**kwargs):\n    def decorator(func):\n        _VALIDATORS[kwargs.pop('name', func.__name__)] = func\n        return func\n\n    return decorator", "response": "Defines a decorator to register a validator with a name for look - up."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef next(self):\n        # We pick the first outgoing available and send it.\n        outgoing_msg = self.outgoing_msg_list.pop_first()\n        # If we do not have any outgoing msg., we wait.\n        if outgoing_msg is None:\n            self.outgoing_msg_event.clear()\n            self.outgoing_msg_event.wait()\n            outgoing_msg = self.outgoing_msg_list.pop_first()\n\n        return outgoing_msg", "response": "Pops and returns the first outgoing message from the list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting the given commands and sends the results to the VSCtl server.", "response": "def run_command(self, commands):\n        \"\"\"\n        Executes the given commands and sends OVSDB messages.\n\n        ``commands`` must be a list of\n        :py:mod:`ryu.lib.ovs.vsctl.VSCtlCommand`.\n\n        The given ``timeout`` and ``exception`` when instantiation will be used\n        to call :py:mod:`ryu.lib.ovs.vsctl.VSCtl.run_command`.\n        \"\"\"\n        self.vsctl.run_command(commands, self.timeout, self.exception)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ninitializes the internal state of the object.", "response": "def init(self):\n        \"\"\"\n        Validates the given ``ovsdb_addr`` and connects to OVS instance.\n\n        If failed to connect to OVS instance or the given ``datapath_id`` does\n        not match with the Datapath ID of the connected OVS instance, raises\n        :py:mod:`ryu.lib.ovs.bridge.OVSBridgeNotFound` exception.\n        \"\"\"\n        if not valid_ovsdb_addr(self.ovsdb_addr):\n            raise ValueError('Invalid OVSDB address: %s' % self.ovsdb_addr)\n        if self.br_name is None:\n            self.br_name = self._get_bridge_name()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_bridge_name(self):\n        command = ovs_vsctl.VSCtlCommand(\n            'find',\n            ('Bridge',\n             'datapath_id=%s' % dpid_lib.dpid_to_str(self.datapath_id)))\n        self.run_command([command])\n        if not isinstance(command.result, list) or len(command.result) != 1:\n            raise OVSBridgeNotFound(\n                datapath_id=dpid_lib.dpid_to_str(self.datapath_id))\n        return command.result[0].name", "response": "get Bridge name of a given datapath_id"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the configured OpenFlow controller address.", "response": "def get_controller(self):\n        \"\"\"\n        Gets the configured OpenFlow controller address.\n\n        This method is corresponding to the following ovs-vsctl command::\n\n            $ ovs-vsctl get-controller <bridge>\n        \"\"\"\n        command = ovs_vsctl.VSCtlCommand('get-controller', [self.br_name])\n        self.run_command([command])\n        result = command.result\n        return result[0] if len(result) == 1 else result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_controller(self, controllers):\n        command = ovs_vsctl.VSCtlCommand('set-controller', [self.br_name])\n        command.args.extend(controllers)\n        self.run_command([command])", "response": "Sets the OpenFlow controller address."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef del_controller(self):\n        command = ovs_vsctl.VSCtlCommand('del-controller', [self.br_name])\n        self.run_command([command])", "response": "Deletes the configured OpenFlow controller address."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef list_db_attributes(self, table, record=None):\n        command = ovs_vsctl.VSCtlCommand('list', (table, record))\n        self.run_command([command])\n        if command.result:\n            return command.result\n        return []", "response": "Lists the attributes of the record in table."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef find_db_attributes(self, table, *conditions):\n        args = [table]\n        args.extend(conditions)\n        command = ovs_vsctl.VSCtlCommand('find', args)\n        self.run_command([command])\n        if command.result:\n            return command.result\n        return []", "response": "Returns a list of attributes that match the conditions in table."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the value of column in record in table.", "response": "def get_db_attribute(self, table, record, column, key=None):\n        \"\"\"\n        Gets values of 'column' in 'record' in 'table'.\n\n        This method is corresponding to the following ovs-vsctl command::\n\n            $ ovs-vsctl get TBL REC COL[:KEY]\n        \"\"\"\n        if key is not None:\n            column = '%s:%s' % (column, key)\n        command = ovs_vsctl.VSCtlCommand(\n            'get', (table, record, column))\n        self.run_command([command])\n        if command.result:\n            return command.result[0]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nset value into column in record in table.", "response": "def set_db_attribute(self, table, record, column, value, key=None):\n        \"\"\"\n        Sets 'value' into 'column' in 'record' in 'table'.\n\n        This method is corresponding to the following ovs-vsctl command::\n\n            $ ovs-vsctl set TBL REC COL[:KEY]=VALUE\n        \"\"\"\n        if key is not None:\n            column = '%s:%s' % (column, key)\n        command = ovs_vsctl.VSCtlCommand(\n            'set', (table, record, '%s=%s' % (column, value)))\n        self.run_command([command])"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef clear_db_attribute(self, table, record, column):\n        command = ovs_vsctl.VSCtlCommand('clear', (table, record, column))\n        self.run_command([command])", "response": "Clears the value of column in record in table."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef db_get_val(self, table, record, column):\n        command = ovs_vsctl.VSCtlCommand('get', (table, record, column))\n        self.run_command([command])\n        assert len(command.result) == 1\n        return command.result[0]", "response": "Gets the value of column in record in table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget dict type value of column in record in table.", "response": "def db_get_map(self, table, record, column):\n        \"\"\"\n        Gets dict type value of 'column' in 'record' in 'table'.\n\n        This method is corresponding to the following ovs-vsctl command::\n\n            $ ovs-vsctl get TBL REC COL\n        \"\"\"\n        val = self.db_get_val(table, record, column)\n        assert isinstance(val, dict)\n        return val"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete_port(self, port_name):\n        command = ovs_vsctl.VSCtlCommand(\n            'del-port', (self.br_name, port_name), '--if-exists')\n        self.run_command([command])", "response": "Deletes a port on the OVS instance."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the OpenFlow port number.", "response": "def get_ofport(self, port_name):\n        \"\"\"\n        Gets the OpenFlow port number.\n\n        This method is corresponding to the following ovs-vsctl command::\n\n            $ ovs-vsctl get Interface <port> ofport\n        \"\"\"\n        ofport_list = self.db_get_val('Interface', port_name, 'ofport')\n        assert len(ofport_list) == 1\n        return int(ofport_list[0])"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_port_name_list(self):\n        command = ovs_vsctl.VSCtlCommand('list-ports', (self.br_name, ))\n        self.run_command([command])\n        return command.result", "response": "Gets a list of all ports on OVS instance."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a bonded port.", "response": "def add_bond(self, name, ifaces, bond_mode=None, lacp=None):\n        \"\"\"\n        Creates a bonded port.\n\n        :param name: Port name to be created\n        :param ifaces: List of interfaces containing at least 2 interfaces\n        :param bond_mode: Bonding mode (active-backup, balance-tcp\n                          or balance-slb)\n        :param lacp: LACP mode (active, passive or off)\n        \"\"\"\n        assert len(ifaces) >= 2\n\n        options = ''\n        if bond_mode:\n            options += 'bond_mode=%(bond_mode)s' % locals()\n        if lacp:\n            options += 'lacp=%(lacp)s' % locals()\n\n        command_add = ovs_vsctl.VSCtlCommand(\n            'add-bond', (self.br_name, name, ifaces), options)\n        self.run_command([command_add])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nadd a tunnel port to the set.", "response": "def add_tunnel_port(self, name, tunnel_type, remote_ip,\n                        local_ip=None, key=None, ofport=None):\n        \"\"\"\n        Creates a tunnel port.\n\n        :param name: Port name to be created\n        :param tunnel_type: Type of tunnel (gre or vxlan)\n        :param remote_ip: Remote IP address of tunnel\n        :param local_ip: Local IP address of tunnel\n        :param key: Key of GRE or VNI of VxLAN\n        :param ofport: Requested OpenFlow port number\n        \"\"\"\n        options = 'remote_ip=%(remote_ip)s' % locals()\n        if key:\n            options += ',key=%(key)s' % locals()\n        if local_ip:\n            options += ',local_ip=%(local_ip)s' % locals()\n\n        args = ['Interface', name, 'type=%s' % tunnel_type,\n                'options:%s' % options]\n        if ofport:\n            args.append('ofport_request=%(ofport)s' % locals())\n\n        command_add = ovs_vsctl.VSCtlCommand('add-port', (self.br_name, name))\n        command_set = ovs_vsctl.VSCtlCommand('set', args)\n        self.run_command([command_add, command_set])"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_gre_port(self, name, remote_ip,\n                     local_ip=None, key=None, ofport=None):\n        \"\"\"\n        Creates a GRE tunnel port.\n\n        See the description of ``add_tunnel_port()``.\n        \"\"\"\n        self.add_tunnel_port(name, 'gre', remote_ip,\n                             local_ip=local_ip, key=key, ofport=ofport)", "response": "Adds a GRE tunnel port."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd a VxLAN tunnel port.", "response": "def add_vxlan_port(self, name, remote_ip,\n                       local_ip=None, key=None, ofport=None):\n        \"\"\"\n        Creates a VxLAN tunnel port.\n\n        See the description of ``add_tunnel_port()``.\n        \"\"\"\n        self.add_tunnel_port(name, 'vxlan', remote_ip,\n                             local_ip=local_ip, key=key, ofport=ofport)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef del_port(self, port_name):\n        command = ovs_vsctl.VSCtlCommand('del-port', (self.br_name, port_name))\n        self.run_command([command])", "response": "Deletes a port on OVS instance."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_qos(self, port_name, type='linux-htb', max_rate=None, queues=None):\n        queues = queues if queues else []\n        command_qos = ovs_vsctl.VSCtlCommand(\n            'set-qos',\n            [port_name, type, max_rate])\n        command_queue = ovs_vsctl.VSCtlCommand(\n            'set-queue',\n            [port_name, queues])\n        self.run_command([command_qos, command_queue])\n        if command_qos.result and command_queue.result:\n            return command_qos.result + command_queue.result\n        return None", "response": "Sets a Qos rule and creates Queues on the given port."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting the Qos rule on the given port.", "response": "def del_qos(self, port_name):\n        \"\"\"\n        Deletes the Qos rule on the given port.\n        \"\"\"\n        command = ovs_vsctl.VSCtlCommand(\n            'del-qos',\n            [port_name])\n        self.run_command([command])"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates checksum of IP pseudo header", "response": "def checksum_ip(ipvx, length, payload):\n    \"\"\"\n    calculate checksum of IP pseudo header\n\n    IPv4 pseudo header\n    UDP RFC768\n    TCP RFC793 3.1\n\n     0      7 8     15 16    23 24    31\n    +--------+--------+--------+--------+\n    |          source address           |\n    +--------+--------+--------+--------+\n    |        destination address        |\n    +--------+--------+--------+--------+\n    |  zero  |protocol|    length       |\n    +--------+--------+--------+--------+\n\n\n    IPv6 pseudo header\n    RFC2460 8.1\n    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    |                                                               |\n    +                                                               +\n    |                                                               |\n    +                         Source Address                        +\n    |                                                               |\n    +                                                               +\n    |                                                               |\n    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    |                                                               |\n    +                                                               +\n    |                                                               |\n    +                      Destination Address                      +\n    |                                                               |\n    +                                                               +\n    |                                                               |\n    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    |                   Upper-Layer Packet Length                   |\n    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    |                      zero                     |  Next Header  |\n    +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    \"\"\"\n    if ipvx.version == 4:\n        header = struct.pack(_IPV4_PSEUDO_HEADER_PACK_STR,\n                             addrconv.ipv4.text_to_bin(ipvx.src),\n                             addrconv.ipv4.text_to_bin(ipvx.dst),\n                             ipvx.proto, length)\n    elif ipvx.version == 6:\n        header = struct.pack(_IPV6_PSEUDO_HEADER_PACK_STR,\n                             addrconv.ipv6.text_to_bin(ipvx.src),\n                             addrconv.ipv6.text_to_bin(ipvx.dst),\n                             length, ipvx.nxt)\n    else:\n        raise ValueError('Unknown IP version %d' % ipvx.version)\n\n    buf = header + payload\n    return checksum(buf)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the Fletcher checksum for the given data.", "response": "def fletcher_checksum(data, offset):\n    \"\"\"\n    Fletcher Checksum -- Refer to RFC1008\n\n    calling with offset == _FLETCHER_CHECKSUM_VALIDATE will validate the\n    checksum without modifying the buffer; a valid checksum returns 0.\n    \"\"\"\n    c0 = 0\n    c1 = 0\n    pos = 0\n    length = len(data)\n    data = bytearray(data)\n    data[offset:offset + 2] = [0] * 2\n\n    while pos < length:\n        tlen = min(length - pos, _MODX)\n        for d in data[pos:pos + tlen]:\n            c0 += d\n            c1 += c0\n        c0 %= 255\n        c1 %= 255\n        pos += tlen\n\n    x = ((length - offset - 1) * c0 - c1) % 255\n    if x <= 0:\n        x += 255\n    y = 510 - c0 - x\n    if y > 255:\n        y -= 255\n\n    data[offset] = x\n    data[offset + 1] = y\n    return (x << 8) | (y & 0xff)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vrrp_config(app, interface, config):\n    config_request = vrrp_event.EventVRRPConfigRequest(interface, config)\n    config_request.sync = True\n    return app.send_request(config_request)", "response": "create an instance and send it to the VRRP"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef vrrp_transmit(app, monitor_name, data):\n    transmit_request = vrrp_event.EventVRRPTransmitRequest(data)\n    app.send_event(monitor_name, transmit_request)", "response": "transmit a packet from the switch."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vrrp_list(app, instance_name=None):\n    list_request = vrrp_event.EventVRRPListRequest(instance_name)\n    list_request.dst = vrrp_event.VRRP_MANAGER_NAME\n    return app.send_request(list_request)", "response": "list instances.\n    returns EventVRRPListReply([VRRPInstance])."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef vrrp_config_change(app, instance_name,\n                       priority=None, advertisement_interval=None,\n                       preempt_mode=None, accept_mode=None):\n    \"\"\"change configuration of an instance.\n    None means no change.\n    \"\"\"\n    config_change = vrrp_event.EventVRRPConfigChangeRequest(\n        instance_name, priority, advertisement_interval,\n        preempt_mode, accept_mode)\n    return app.send_event(vrrp_event.VRRP_MANAGER_NAME, config_change)", "response": "send a config change request to the VRRP manager"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nparsing a buffer including the flow match.", "response": "def parser(cls, buf, offset):\n        \"\"\"\n        Returns an object which is generated from a buffer including the\n        expression of the wire protocol of the flow match.\n        \"\"\"\n        match = OFPMatch()\n        type_, length = struct.unpack_from('!HH', buf, offset)\n\n        match.type = type_\n        match.length = length\n\n        # ofp_match adjustment\n        offset += 4\n        length -= 4\n\n        fields = []\n        while length > 0:\n            n, value, mask, field_len = ofproto.oxm_parse(buf, offset)\n            k, uv = ofproto.oxm_to_user(n, value, mask)\n            fields.append((k, uv))\n            offset += field_len\n            length -= field_len\n        match._fields2 = fields\n        return match"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef serialize(self, buf, offset):\n        fields = [ofproto.oxm_from_user(k, uv) for (k, uv)\n                  in self._fields2]\n\n        hdr_pack_str = '!HH'\n        field_offset = offset + struct.calcsize(hdr_pack_str)\n        for (n, value, mask) in fields:\n            field_offset += ofproto.oxm_serialize(n, value, mask, buf,\n                                                  field_offset)\n\n        length = field_offset - offset\n        msg_pack_into(hdr_pack_str, buf, offset, ofproto.OFPMT_OXM, length)\n        self.length = length\n\n        pad_len = utils.round_up(length, 8) - length\n        msg_pack_into(\"%dx\" % pad_len, buf, field_offset)\n\n        return length + pad_len", "response": "Serializes the expression of the wire protocol of the flow match into the buf. Returns the output length."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dict expressing the flow match.", "response": "def to_jsondict(self):\n        \"\"\"\n        Returns a dict expressing the flow match.\n        \"\"\"\n        body = {\"oxm_fields\": [ofproto.oxm_to_jsondict(k, uv) for k, uv\n                               in self._fields2],\n                \"length\": self.length,\n                \"type\": self.type}\n        return {self.__class__.__name__: body}"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an object which is generated from a dict containing the dict of oxidation fields.", "response": "def from_jsondict(cls, dict_):\n        \"\"\"\n        Returns an object which is generated from a dict.\n\n        Exception raises:\n        KeyError -- Unknown match field is defined in dict\n        \"\"\"\n        fields = [ofproto.oxm_from_jsondict(f) for f\n                  in dict_['oxm_fields']]\n        return OFPMatch(_ordered_fields=fields)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse a buffer including the flow stats.", "response": "def parser(cls, buf, offset):\n        \"\"\"\n        Returns an object which is generated from a buffer including the\n        expression of the wire protocol of the flow stats.\n        \"\"\"\n        stats = OFPStats()\n        reserved, length = struct.unpack_from('!HH', buf, offset)\n\n        stats.length = length\n\n        # ofp_stats adjustment\n        offset += 4\n        length -= 4\n\n        fields = []\n        while length > 0:\n            n, value, _, field_len = ofproto.oxs_parse(buf, offset)\n            k, uv = ofproto.oxs_to_user(n, value, None)  # No mask\n            fields.append((k, uv))\n            offset += field_len\n            length -= field_len\n        stats.fields = fields\n        return stats"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nserializing the flow stats into the buf.", "response": "def serialize(self, buf, offset):\n        \"\"\"\n        Outputs the expression of the wire protocol of the flow stats into\n        the buf.\n        Returns the output length.\n        \"\"\"\n        fields = [ofproto.oxs_from_user(k, uv) for (k, uv)\n                  in self.fields]\n\n        hdr_pack_str = '!HH'\n        field_offset = offset + struct.calcsize(hdr_pack_str)\n        for (n, value, _) in fields:\n            # No mask\n            field_offset += ofproto.oxs_serialize(n, value, None, buf,\n                                                  field_offset)\n\n        reserved = 0\n        length = field_offset - offset\n        msg_pack_into(hdr_pack_str, buf, offset, reserved, length)\n        self.length = length\n\n        pad_len = utils.round_up(length, 8) - length\n        msg_pack_into(\"%dx\" % pad_len, buf, field_offset)\n\n        return length + pad_len"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_jsondict(self):\n        body = {\"oxs_fields\": [ofproto.oxs_to_jsondict(k, uv) for k, uv\n                               in self.fields],\n                \"length\": self.length}\n        return {self.__class__.__name__: body}", "response": "Returns a dict expressing the flow stats."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn an object which is generated from a dict.", "response": "def from_jsondict(cls, dict_):\n        \"\"\"\n        Returns an object which is generated from a dict.\n\n        Exception raises:\n        KeyError -- Unknown stats field is defined in dict\n        \"\"\"\n        fields = [ofproto.oxs_from_jsondict(f) for f\n                  in dict_['oxs_fields']]\n        return OFPStats(_ordered_fields=fields)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ofp_instruction_from_str(ofproto, action_str):\n    action_re = re.compile(r\"([a-z_]+)(\\([^)]*\\)|[^a-z_,()][^,()]*)*\")\n    result = []\n    while len(action_str):\n        m = action_re.match(action_str)\n        if not m:\n            raise ryu.exception.OFPInvalidActionString(action_str=action_str)\n        action_name = m.group(1)\n        this_action = m.group(0)\n        paren_level = this_action.count('(') - this_action.count(')')\n        assert paren_level >= 0\n        try:\n            # Parens can be nested. Look for as many ')'s as '('s.\n            if paren_level > 0:\n                this_action, rest = _tokenize_paren_block(action_str, m.end(0))\n            else:\n                rest = action_str[m.end(0):]\n            if len(rest):\n                assert rest[0] == ','\n                rest = rest[1:]\n        except Exception:\n            raise ryu.exception.OFPInvalidActionString(action_str=action_str)\n        if action_name == 'drop':\n            assert this_action == 'drop'\n            assert len(result) == 0 and rest == ''\n            return []\n        converter = getattr(OfctlActionConverter, action_name, None)\n        if converter is None or not callable(converter):\n            raise ryu.exception.OFPInvalidActionString(action_str=action_name)\n        result.append(converter(ofproto, this_action))\n        action_str = rest\n\n    return result", "response": "Parse an ovs - ofctl style action string and return a list of jsondict representations of the actions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef ofp_ofctl_field_name_to_ryu(field):\n    mapped = _OXM_FIELD_OFCTL_ALIASES.get(field)\n    if mapped:\n        return mapped\n    if field.endswith(\"_dst\"):\n        mapped = _OXM_FIELD_OFCTL_ALIASES.get(field[:-3] + \"src\")\n        if mapped:\n            return mapped[:-3] + \"dst\"\n    return field", "response": "Convert an ovs - ofctl field name to ryu equivalent."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert an NXM_NX_ or NXM_OF_ field name to NXM_OF_ field name a ryu match field name.", "response": "def nxm_field_name_to_ryu(field):\n    \"\"\"\n    Convert an ovs-ofctl style NXM_/OXM_ field name to\n    a ryu match field name.\n    \"\"\"\n    if field.endswith(\"_W\"):\n        field = field[:-2]\n    prefix = field[:7]\n    field = field[7:].lower()\n    mapped_result = None\n\n    if prefix == 'NXM_NX_':\n        mapped_result = _NXM_FIELD_MAP.get(field)\n    elif prefix == \"NXM_OF_\":\n        mapped_result = _NXM_OF_FIELD_MAP.get(field)\n    elif prefix == \"OXM_OF_\":\n        # no mapping needed\n        pass\n    else:\n        raise ValueError\n    if mapped_result is not None:\n        return mapped_result\n    return field"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef message_to_event(zclient, msg):\n    if not isinstance(msg, zebra.ZebraMessage):\n        return None\n\n    body_cls = msg.get_body_class(msg.version, msg.command)\n    ev_cls = getattr(MOD, _event_name(body_cls), None)\n    if ev_cls is None:\n        return None\n\n    return ev_cls(zclient, msg)", "response": "Converts Zebra protocol message instance to Zebra protocol service event instance."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef node_del_attrs(self, node):\n        try:\n            delattr(node, self.next_name)\n            delattr(node, self.prev_name)\n        except AttributeError:\n            pass", "response": "Remove all attributes that are used for putting this node\n            on this type of list."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning True if this node is on some list.", "response": "def node_is_on_list(self, node):\n        \"\"\"Returns True if this node is on *some* list.\n\n        A node is not on any list if it is linked to itself, or if it\n        does not have the next and/prev attributes at all.\n        \"\"\"\n        next = self.node_next(node)\n        if next == node or next is None:\n            assert(self.node_prev(node) is next)\n            return False\n\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef node_insert_after(self, node, new_node):\n\n        assert(not self.node_is_on_list(new_node))\n        assert(node is not new_node)\n\n        next = self.node_next(node)\n        assert(next is not None)\n        self.node_set_next(node, new_node)\n        self.node_set_prev(new_node, node)\n\n        self.node_set_next(new_node, next)\n        self.node_set_prev(next, new_node)", "response": "Insert the new node after node."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ninsert the new node before node.", "response": "def node_insert_before(self, node, new_node):\n        \"\"\"Insert the new node before node.\"\"\"\n\n        assert(not self.node_is_on_list(new_node))\n        assert(node is not new_node)\n\n        prev = self.node_prev(node)\n        assert(prev is not None)\n        self.node_set_prev(node, new_node)\n        self.node_set_next(new_node, node)\n\n        self.node_set_prev(new_node, prev)\n        self.node_set_next(prev, new_node)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_rtfilters(self):\n        # Update RT filter for all peers\n        # TODO(PH): Check if getting this map can be optimized (if expensive)\n        new_peer_to_rtfilter_map = self._compute_rtfilter_map()\n\n        # If we have new best path for RT NLRI, we have to update peer RT\n        # filters and take appropriate action of sending them NLRIs for other\n        # address-families as per new RT filter if necessary.\n        for peer in self._peer_manager.iterpeers:\n            pre_rt_filter = self._rt_mgr.peer_to_rtfilter_map.get(peer, set())\n            curr_rt_filter = new_peer_to_rtfilter_map.get(peer, set())\n\n            old_rts = pre_rt_filter - curr_rt_filter\n            new_rts = curr_rt_filter - pre_rt_filter\n            # If interested RTs for a peer changes\n            if new_rts or old_rts:\n                LOG.debug('RT Filter for peer %s updated: '\n                          'Added RTs %s, Removed Rts %s',\n                          peer.ip_address, new_rts, old_rts)\n                self._on_update_rt_filter(peer, new_rts, old_rts)\n                # Update to new RT filters\n        self._peer_manager.set_peer_to_rtfilter_map(new_peer_to_rtfilter_map)\n        self._rt_mgr.peer_to_rtfilter_map = new_peer_to_rtfilter_map\n        LOG.debug('Updated RT filters: %s', self._rt_mgr.peer_to_rtfilter_map)\n        # Update interested RTs i.e. RTs on the path that will be installed\n        # into global tables\n        self._rt_mgr.update_interested_rts()", "response": "Updates the RT filters for all peers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling update of peer RT filter.", "response": "def _on_update_rt_filter(self, peer, new_rts, old_rts):\n        \"\"\"Handles update of peer RT filter.\n\n        Parameters:\n            - `peer`: (Peer) whose RT filter has changed.\n            - `new_rts`: (set) of new RTs that peer is interested in.\n            - `old_rts`: (set) of RTs that peers is no longer interested in.\n        \"\"\"\n        for table in self._table_manager._global_tables.values():\n            if table.route_family == RF_RTC_UC:\n                continue\n            self._spawn('rt_filter_chg_%s' % peer,\n                        self._rt_mgr.on_rt_filter_chg_sync_peer,\n                        peer, new_rts, old_rts, table)\n            LOG.debug('RT Filter change handler launched for route_family %s',\n                      table.route_family)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncomputing the RT filter map for each peer that has advertised RT NLRIs.", "response": "def _compute_rtfilter_map(self):\n        \"\"\"Returns neighbor's RT filter (permit/allow filter based on RT).\n\n        Walks RT filter tree and computes current RT filters for each peer that\n        have advertised RT NLRIs.\n        Returns:\n            dict of peer, and `set` of rts that a particular neighbor is\n            interested in.\n        \"\"\"\n        rtfilter_map = {}\n\n        def get_neigh_filter(neigh):\n            neigh_filter = rtfilter_map.get(neigh)\n            # Lazy creation of neighbor RT filter\n            if neigh_filter is None:\n                neigh_filter = set()\n                rtfilter_map[neigh] = neigh_filter\n            return neigh_filter\n\n        # Check if we have to use all paths or just best path\n        if self._common_config.max_path_ext_rtfilter_all:\n            # We have to look at all paths for a RtDest\n            for rtcdest in self._table_manager.get_rtc_table().values():\n                known_path_list = rtcdest.known_path_list\n                for path in known_path_list:\n                    neigh = path.source\n\n                    # We ignore NC\n                    if neigh is None:\n                        continue\n\n                    neigh_filter = get_neigh_filter(neigh)\n                    neigh_filter.add(path.nlri.route_target)\n        else:\n            # We iterate over all destination of the RTC table and for iBGP\n            # peers we use all known paths' RTs for RT filter and for eBGP\n            # peers we only consider best-paths' RTs for RT filter\n            for rtcdest in self._table_manager.get_rtc_table().values():\n                path = rtcdest.best_path\n                # If this destination does not have any path, we continue\n                if not path:\n                    continue\n\n                neigh = path.source\n                # Consider only eBGP peers and ignore NC\n                if neigh and neigh.is_ebgp_peer():\n                    # For eBGP peers we use only best-path to learn RT filter\n                    neigh_filter = get_neigh_filter(neigh)\n                    neigh_filter.add(path.nlri.route_target)\n                else:\n                    # For iBGP peers we use all known paths to learn RT filter\n                    known_path_list = rtcdest.known_path_list\n                    for path in known_path_list:\n                        neigh = path.source\n                        # We ignore NC, and eBGP peers\n                        if neigh and not neigh.is_ebgp_peer():\n                            neigh_filter = get_neigh_filter(neigh)\n                            neigh_filter.add(path.nlri.route_target)\n\n        return rtfilter_map"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts new connection protocol.", "response": "def start_protocol(self, socket):\n        \"\"\"Handler of new connection requests on bgp server port.\n\n        Checks if new connection request is valid and starts new instance of\n        protocol.\n        \"\"\"\n        assert socket\n        peer_addr, peer_port = self.get_remotename(socket)\n        peer = self._peer_manager.get_by_addr(peer_addr)\n        bgp_proto = self.build_protocol(socket)\n\n        # We reject this connection request from peer:\n        # 1) If we have connection initiated by a peer that is not in our\n        #     configuration.\n        # 2) If this neighbor is not enabled according to configuration.\n        if not peer or not peer.enabled:\n            LOG.debug('Closed connection %s %s:%s as it is not a recognized'\n                      ' peer.', 'from' if bgp_proto.is_reactive else 'to',\n                      peer_addr, peer_port)\n            # Send connection rejected notification as per RFC\n            code = BGP_ERROR_CEASE\n            subcode = BGP_ERROR_SUB_CONNECTION_RESET\n            bgp_proto.send_notification(code, subcode)\n        elif bgp_proto.is_reactive and \\\n                peer.connect_mode is CONNECT_MODE_ACTIVE:\n            LOG.debug('Closed connection from %s:%s as connect_mode is'\n                      ' configured ACTIVE.', peer_addr, peer_port)\n            # Send connection rejected notification as per RFC\n            code = BGP_ERROR_CEASE\n            subcode = BGP_ERROR_SUB_CONNECTION_RESET\n            bgp_proto.send_notification(code, subcode)\n        elif not (peer.in_idle() or peer.in_active() or peer.in_connect()):\n            LOG.debug('Closing connection to %s:%s as we have connection'\n                      ' in state other than IDLE or ACTIVE,'\n                      ' i.e. connection resolution',\n                      peer_addr, peer_port)\n            # Send Connection Collision Resolution notification as per RFC.\n            code = BGP_ERROR_CEASE\n            subcode = BGP_ERROR_SUB_CONNECTION_COLLISION_RESOLUTION\n            bgp_proto.send_notification(code, subcode)\n        else:\n            bind_ip, bind_port = self.get_localname(socket)\n            peer._host_bind_ip = bind_ip\n            peer._host_bind_port = bind_port\n            self._spawn_activity(bgp_proto, peer)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef compare_root_path(path_cost1, path_cost2, bridge_id1, bridge_id2,\n                          port_id1, port_id2):\n        \"\"\" Decide the port of the side near a root bridge.\n            It is compared by the following priorities.\n             1. root path cost\n             2. designated bridge ID value\n             3. designated port ID value \"\"\"\n        result = Stp._cmp_value(path_cost1, path_cost2)\n        if not result:\n            result = Stp._cmp_value(bridge_id1, bridge_id2)\n            if not result:\n                result = Stp._cmp_value(port_id1, port_id2)\n        return result", "response": "Compare two root path costs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef compare_bpdu_info(my_priority, my_times, rcv_priority, rcv_times):\n        if my_priority is None:\n            result = SUPERIOR\n        else:\n            result = Stp._cmp_value(rcv_priority.root_id.value,\n                                    my_priority.root_id.value)\n            if not result:\n                result = Stp.compare_root_path(\n                    rcv_priority.root_path_cost,\n                    my_priority.root_path_cost,\n                    rcv_priority.designated_bridge_id.value,\n                    my_priority.designated_bridge_id.value,\n                    rcv_priority.designated_port_id.value,\n                    my_priority.designated_port_id.value)\n                if not result:\n                    result1 = Stp._cmp_value(\n                        rcv_priority.designated_bridge_id.value,\n                        mac.haddr_to_int(\n                            my_priority.designated_bridge_id.mac_addr))\n                    result2 = Stp._cmp_value(\n                        rcv_priority.designated_port_id.value,\n                        my_priority.designated_port_id.port_no)\n                    if not result1 and not result2:\n                        result = SUPERIOR\n                    else:\n                        result = Stp._cmp_obj(rcv_times, my_times)\n        return result", "response": "Compare two BPDU objects."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchange status to DISABLE and recalculate STP.", "response": "def link_down(self, ofp_port):\n        \"\"\" DESIGNATED_PORT/NON_DESIGNATED_PORT: change status to DISABLE.\n            ROOT_PORT: change status to DISABLE and recalculate STP. \"\"\"\n        port = self.ports[ofp_port.port_no]\n        init_stp_flg = bool(port.role is ROOT_PORT)\n\n        port.down(PORT_STATE_DISABLE, msg_init=True)\n        self.ports_state[ofp_port.port_no] = ofp_port.state\n        if init_stp_flg:\n            self.recalculate_spanning_tree()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _spanning_tree_algorithm(self):\n        port_roles = {}\n\n        root_port = self._select_root_port()\n\n        if root_port is None:\n            # My bridge is a root bridge.\n            self.logger.info('Root bridge.', extra=self.dpid_str)\n            root_priority = self.root_priority\n            root_times = self.root_times\n\n            for port_no in self.ports:\n                if self.ports[port_no].state is not PORT_STATE_DISABLE:\n                    port_roles[port_no] = DESIGNATED_PORT\n        else:\n            # Other bridge is a root bridge.\n            self.logger.info('Non root bridge.', extra=self.dpid_str)\n            root_priority = root_port.designated_priority\n            root_times = root_port.designated_times\n\n            port_roles[root_port.ofport.port_no] = ROOT_PORT\n\n            d_ports = self._select_designated_port(root_port)\n            for port_no in d_ports:\n                port_roles[port_no] = DESIGNATED_PORT\n\n            for port in self.ports.values():\n                if port.state is not PORT_STATE_DISABLE:\n                    port_roles.setdefault(port.ofport.port_no,\n                                          NON_DESIGNATED_PORT)\n\n        return port_roles, root_priority, root_times", "response": "Update tree roles.\n             - Root bridge:\n                all port is DESIGNATED_PORT.\n             - Non root bridge:\n                select one ROOT_PORT and some DESIGNATED_PORT,\n                and the other port is set to NON_DESIGNATED_PORT."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nselecting the ROOT_PORT from the list of ports.", "response": "def _select_root_port(self):\n        \"\"\" ROOT_PORT is the nearest port to a root bridge.\n            It is determined by the cost of path, etc. \"\"\"\n        root_port = None\n\n        for port in self.ports.values():\n            root_msg = (self.root_priority if root_port is None\n                        else root_port.designated_priority)\n            port_msg = port.designated_priority\n            if port.state is PORT_STATE_DISABLE or port_msg is None:\n                continue\n            if root_msg.root_id.value > port_msg.root_id.value:\n                result = SUPERIOR\n            elif root_msg.root_id.value == port_msg.root_id.value:\n                if root_msg.designated_bridge_id is None:\n                    result = INFERIOR\n                else:\n                    result = Stp.compare_root_path(\n                        port_msg.root_path_cost,\n                        root_msg.root_path_cost,\n                        port_msg.designated_bridge_id.value,\n                        root_msg.designated_bridge_id.value,\n                        port_msg.designated_port_id.value,\n                        root_msg.designated_port_id.value)\n            else:\n                result = INFERIOR\n\n            if result is SUPERIOR:\n                root_port = port\n\n        return root_port"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nselecting the DESIGNATED_PORT from the list of ports.", "response": "def _select_designated_port(self, root_port):\n        \"\"\" DESIGNATED_PORT is a port of the side near the root bridge\n            of each link. It is determined by the cost of each path, etc\n            same as ROOT_PORT. \"\"\"\n        d_ports = []\n        root_msg = root_port.designated_priority\n\n        for port in self.ports.values():\n            port_msg = port.designated_priority\n            if (port.state is PORT_STATE_DISABLE\n                    or port.ofport.port_no == root_port.ofport.port_no):\n                continue\n            if (port_msg is None or\n                    (port_msg.root_id.value != root_msg.root_id.value)):\n                d_ports.append(port.ofport.port_no)\n            else:\n                result = Stp.compare_root_path(\n                    root_msg.root_path_cost,\n                    port_msg.root_path_cost - port.path_cost,\n                    self.bridge_id.value,\n                    port_msg.designated_bridge_id.value,\n                    port.port_id.value,\n                    port_msg.designated_port_id.value)\n                if result is SUPERIOR:\n                    d_ports.append(port.ofport.port_no)\n\n        return d_ports"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef up(self, role, root_priority, root_times):\n        self.port_priority = root_priority\n        self.port_times = root_times\n\n        state = (PORT_STATE_LISTEN if self.config_enable\n                 else PORT_STATE_DISABLE)\n        self._change_role(role)\n        self._change_status(state)", "response": "Start a new port."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nchange the status of a port to DISABLE or BLOCK.", "response": "def down(self, state, msg_init=False):\n        \"\"\" A port will be in the state of DISABLE or BLOCK,\n             and be stopped.  \"\"\"\n        assert (state is PORT_STATE_DISABLE\n                or state is PORT_STATE_BLOCK)\n        if not self.config_enable:\n            return\n\n        if msg_init:\n            self.designated_priority = None\n            self.designated_times = None\n\n        self._change_role(DESIGNATED_PORT)\n        self._change_status(state)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _state_machine(self):\n        role_str = {ROOT_PORT: 'ROOT_PORT          ',\n                    DESIGNATED_PORT: 'DESIGNATED_PORT    ',\n                    NON_DESIGNATED_PORT: 'NON_DESIGNATED_PORT'}\n        state_str = {PORT_STATE_DISABLE: 'DISABLE',\n                     PORT_STATE_BLOCK: 'BLOCK',\n                     PORT_STATE_LISTEN: 'LISTEN',\n                     PORT_STATE_LEARN: 'LEARN',\n                     PORT_STATE_FORWARD: 'FORWARD'}\n\n        if self.state is PORT_STATE_DISABLE:\n            self.ofctl.set_port_status(self.ofport, self.state)\n\n        while True:\n            self.logger.info('[port=%d] %s / %s', self.ofport.port_no,\n                             role_str[self.role], state_str[self.state],\n                             extra=self.dpid_str)\n\n            self.state_event = hub.Event()\n            timer = self._get_timer()\n            if timer:\n                timeout = hub.Timeout(timer)\n                try:\n                    self.state_event.wait()\n                except hub.Timeout as t:\n                    if t is not timeout:\n                        err_msg = 'Internal error. Not my timeout.'\n                        raise RyuException(msg=err_msg)\n                    new_state = self._get_next_state()\n                    self._change_status(new_state, thread_switch=False)\n                finally:\n                    timeout.cancel()\n            else:\n                self.state_event.wait()\n\n            self.state_event = None", "response": "Internal method to change the state of the port."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef transmit_tc_bpdu(self):\n        if not self.send_tc_flg:\n            timer = datetime.timedelta(seconds=self.port_times.max_age\n                                       + self.port_times.forward_delay)\n            self.send_tc_timer = datetime.datetime.today() + timer\n            self.send_tc_flg = True", "response": "Transmit Topology Change BPDU."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef transmit_ack_bpdu(self):\n        ack_flags = 0b10000001\n        bpdu_data = self._generate_config_bpdu(ack_flags)\n        self.ofctl.send_packet_out(self.ofport.port_no, bpdu_data)", "response": "Send Topology Change Ack BPDU."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if all passed flags are set on this packet.", "response": "def has_flags(self, *flags):\n        \"\"\"Check if flags are set on this packet.\n\n        returns boolean if all passed flags is set\n\n        Example::\n\n            >>> pkt = tcp.tcp(bits=(tcp.TCP_SYN | tcp.TCP_ACK))\n            >>> pkt.has_flags(tcp.TCP_SYN, tcp.TCP_ACK)\n            True\n        \"\"\"\n\n        mask = sum(flags)\n        return (self.bits & mask) == mask"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_user_flags():\n    try:\n        idx = list(sys.argv).index('--user-flags')\n        user_flags_file = sys.argv[idx + 1]\n    except (ValueError, IndexError):\n        user_flags_file = ''\n\n    if user_flags_file and os.path.isfile(user_flags_file):\n        from ryu.utils import _import_module_file\n        _import_module_file(user_flags_file)", "response": "Parses user - flags file and loads it to register user defined options."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a GRE object with information for NVGRE.", "response": "def nvgre(version=0, vsid=0, flow_id=0):\n    \"\"\"\n    Generate instance of GRE class with information for NVGRE (RFC7637).\n\n    :param version: Version.\n    :param vsid: Virtual Subnet ID.\n    :param flow_id: FlowID.\n    :return: Instance of GRE class with information for NVGRE.\n    \"\"\"\n\n    # NVGRE header\n    #  0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1 2 3 4 5 6 7 8 9 0 1\n    # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    # |0| |1|0|   Reserved0     | Ver |   Protocol Type 0x6558        |\n    # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    # |               Virtual Subnet ID (VSID)        |    FlowID     |\n    # +-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+-+\n    return gre(version=version, protocol=ether_types.ETH_TYPE_TEB,\n               vsid=vsid, flow_id=flow_id)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef ofp_instruction_from_jsondict(dp, jsonlist, encap=True):\n    proto = dp.ofproto\n    parser = dp.ofproto_parser\n    actions = []\n    result = []\n    for jsondict in jsonlist:\n        assert len(jsondict) == 1\n        k, v = list(jsondict.items())[0]\n        cls = getattr(parser, k)\n        if issubclass(cls, parser.OFPAction):\n            if encap:\n                actions.append(cls.from_jsondict(v))\n                continue\n        else:\n            ofpinst = getattr(parser, 'OFPInstruction', None)\n            if not ofpinst or not issubclass(cls, ofpinst):\n                raise ValueError(\"Supplied jsondict is of wrong type: %s\",\n                                 jsondict)\n        result.append(cls.from_jsondict(v))\n\n    if not encap:\n        return result\n\n    if actions:\n        # Although the OpenFlow spec says Apply Actions is executed first,\n        # let's place it in the head as a precaution.\n        result = [parser.OFPInstructionActions(\n            proto.OFPIT_APPLY_ACTIONS, actions)] + result\n    return result", "response": "This function takes a Datapath dp and a list of JSON dictionaries and returns a list of OFPInstructions and OFPActions."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_tcp_md5sig(s, addr, key):\n    impls = {\n        'FreeBSD': _set_tcp_md5sig_bsd,\n        'Linux': _set_tcp_md5sig_linux,\n        'NetBSD': _set_tcp_md5sig_bsd,\n    }\n    system = platform.system()\n    try:\n        impl = impls[system]\n    except KeyError:\n        raise NotImplementedError(\"TCP-MD5 unsupported on this platform\")\n    impl(s, addr, key)", "response": "Enable TCP - MD5 on the given socket."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a new address to initiate connection to switch.", "response": "def register_switch_address(addr, interval=None):\n    \"\"\"\n    Registers a new address to initiate connection to switch.\n\n    Registers a new IP address and port pair of switch to let\n    ryu.controller.controller.OpenFlowController to try to initiate\n    connection to switch.\n\n    :param addr: A tuple of (host, port) pair of switch.\n    :param interval: Interval in seconds to try to connect to switch\n    \"\"\"\n    assert len(addr) == 2\n    assert ip.valid_ipv4(addr[0]) or ip.valid_ipv6(addr[0])\n    ofp_handler = app_manager.lookup_service_brick(ofp_event.NAME)\n    _TMP_ADDRESSES[addr] = interval\n\n    def _retry_loop():\n        # Delays registration if ofp_handler is not started yet\n        while True:\n            if ofp_handler.controller is not None:\n                for a, i in _TMP_ADDRESSES.items():\n                    ofp_handler.controller.spawn_client_loop(a, i)\n                    hub.sleep(1)\n                break\n            hub.sleep(1)\n\n    hub.spawn(_retry_loop)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef default_help_formatter(quick_helps):\n    ret = ''\n    for line in quick_helps:\n        cmd_path, param_hlp, cmd_hlp = line\n        ret += ' '.join(cmd_path) + ' '\n        if param_hlp:\n            ret += param_hlp + ' '\n        ret += '- ' + cmd_hlp + '\\n'\n    return ret", "response": "Apply default formatting for help messages\n    "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cli_resp_formatter(cls, resp):\n        if not resp.value:\n            return ''\n\n        if resp.status == STATUS_OK:\n\n            if type(resp.value) in (str, bool, int, float, six.text_type):\n                return str(resp.value)\n\n            ret = ''\n            val = resp.value\n            if not isinstance(val, list):\n                val = [val]\n            for line in val:\n                for k, v in line.items():\n                    if isinstance(v, dict):\n                        ret += cls.cli_resp_line_template.format(\n                            k, '\\n' + pprint.pformat(v)\n                        )\n                    else:\n                        ret += cls.cli_resp_line_template.format(k, v)\n            return ret\n        else:\n            return \"Error: {0}\".format(resp.value)", "response": "Override this method to provide custom formatting of cli response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfilters response of action. Used to make printed results more specific .", "response": "def filter_resp(self, action_resp, filter_params):\n        \"\"\"Filter response of action. Used to make printed results more\n        specific\n\n        :param action_resp: named tuple (CommandsResponse)\n            containing response from action.\n        :param filter_params: params used after '|' specific for given filter\n        :return: filtered response.\n        \"\"\"\n        if action_resp.status == STATUS_OK:\n            try:\n                return CommandsResponse(\n                    STATUS_OK,\n                    TextFilter.filter(action_resp.value, filter_params)\n                )\n            except FilterError as e:\n                return CommandsResponse(STATUS_ERROR, str(e))\n        else:\n            return action_resp"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow help for this command and it s sub - commands.", "response": "def question_mark(self):\n        \"\"\"Shows help for this command and it's sub-commands.\n        \"\"\"\n        ret = []\n        if self.param_help_msg or len(self.subcommands) == 0:\n            ret.append(self._quick_help())\n\n        if len(self.subcommands) > 0:\n            for k, _ in sorted(self.subcommands.items()):\n                command_path, param_help, cmd_help = \\\n                    self._instantiate_subcommand(k)._quick_help(nested=True)\n                if command_path or param_help or cmd_help:\n                    ret.append((command_path, param_help, cmd_help))\n\n        return (\n            CommandsResponse(STATUS_OK, self.help_formatter(ret)),\n            self.__class__\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _quick_help(self, nested=False):\n        if nested:\n            return self.command_path(), None, self.help_msg\n        else:\n            return self.command_path(), self.param_help_msg, self.help_msg", "response": "returns the command path and help message for this command"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ip_link_add(session, name, type_='loopback', lladdr='00:00:00:00:00:00'):\n    intf = ip_link_show(session, ifname=name)\n    if intf:\n        LOG.debug('Interface \"%s\" already exists: %s', intf.ifname, intf)\n        return intf\n\n    if type_ == 'ethernet':\n        intf = Interface(\n            ifname=name,\n            flags=DEFAULT_ETH_FLAGS,\n            ifmtu=DEFAULT_ETH_MTU,\n            ifmtu6=DEFAULT_ETH_MTU,\n            hw_addr=lladdr)\n    else:  # type_ == 'loopback':\n        intf = Interface(\n            ifname=name,\n            inet='127.0.0.1/8',\n            inet6='::1/128')\n\n    session.add(intf)\n\n    return intf", "response": "Adds an interface record into Zebra protocol service database."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef ip_link_delete(session, name):\n    intf = ip_link_show(session, ifname=name)\n    if not intf:\n        LOG.debug('Interface \"%s\" does not exist', name)\n        return None\n\n    session.delete(intf)\n\n    return name", "response": "Delete an interface record from Zebra protocol service database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd an IP address to an interface record identified with the given ifname.", "response": "def ip_address_add(session, ifname, ifaddr):\n    \"\"\"\n    Adds an IP address to interface record identified with the given \"ifname\".\n\n    The arguments are similar to \"ip address add\" command of iproute2.\n\n    :param session: Session instance connecting to database.\n    :param ifname: Name of interface.\n    :param ifaddr: IPv4 or IPv6 address.\n    :return: Instance of record or \"None\" if failed.\n    \"\"\"\n    def _append_inet_addr(intf_inet, addr):\n        addr_list = intf_inet.split(',')\n        if addr in addr_list:\n            LOG.debug(\n                'Interface \"%s\" has already \"ifaddr\": %s',\n                intf.ifname, addr)\n            return intf_inet\n        else:\n            addr_list.append(addr)\n            return ','.join(addr_list)\n\n    intf = ip_link_show(session, ifname=ifname)\n    if not intf:\n        LOG.debug('Interface \"%s\" does not exist', ifname)\n        return None\n\n    if ip.valid_ipv4(ifaddr):\n        intf.inet = _append_inet_addr(intf.inet, ifaddr)\n    elif ip.valid_ipv6(ifaddr):\n        intf.inet6 = _append_inet_addr(intf.inet6, ifaddr)\n    else:\n        LOG.debug('Invalid IP address for \"ifaddr\": %s', ifaddr)\n        return None\n\n    return intf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ip_address_delete(session, ifname, ifaddr):\n    def _remove_inet_addr(intf_inet, addr):\n        addr_list = intf_inet.split(',')\n        if addr not in addr_list:\n            LOG.debug(\n                'Interface \"%s\" does not have \"ifaddr\": %s',\n                intf.ifname, addr)\n            return intf_inet\n        else:\n            addr_list.remove(addr)\n            return ','.join(addr_list)\n\n    intf = ip_link_show(session, ifname=ifname)\n    if not intf:\n        LOG.debug('Interface \"%s\" does not exist', ifname)\n        return None\n\n    if ip.valid_ipv4(ifaddr):\n        intf.inet = _remove_inet_addr(intf.inet, ifaddr)\n    elif ip.valid_ipv6(ifaddr):\n        intf.inet6 = _remove_inet_addr(intf.inet6, ifaddr)\n    else:\n        LOG.debug('Invalid IP address for \"ifaddr\": %s', ifaddr)\n        return None\n\n    return intf", "response": "Delete an IP address from an interface record identified with the given ifname."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _split_str(s, n):\n    length = len(s)\n    return [s[i:i + n] for i in range(0, length, n)]", "response": "split string into list of strings by specified number."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngenerating a dictionary of system parameters used throughout a simulation.", "response": "def systemdef(meta, surface_tilt, surface_azimuth, albedo, modules_per_string,\n              strings_per_inverter):\n    '''\n    Generates a dict of system parameters used throughout a simulation.\n\n    Parameters\n    ----------\n\n    meta : dict\n        meta dict either generated from a TMY file using readtmy2 or\n        readtmy3, or a dict containing at least the following fields:\n\n            ===============   ======  ====================\n            meta field        format  description\n            ===============   ======  ====================\n            meta.altitude     Float   site elevation\n            meta.latitude     Float   site latitude\n            meta.longitude    Float   site longitude\n            meta.Name         String  site name\n            meta.State        String  state\n            meta.TZ           Float   timezone\n            ===============   ======  ====================\n\n    surface_tilt : float or Series\n        Surface tilt angles in decimal degrees.\n        The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth : float or Series\n        Surface azimuth angles in decimal degrees.\n        The azimuth convention is defined\n        as degrees east of north\n        (North=0, South=180, East=90, West=270).\n\n    albedo : float or Series\n        Ground reflectance, typically 0.1-0.4 for surfaces on Earth\n        (land), may increase over snow, ice, etc. May also be known as\n        the reflection coefficient. Must be >=0 and <=1.\n\n    modules_per_string : int\n        Number of modules connected in series in a string.\n\n    strings_per_inverter : int\n        Number of strings connected in parallel.\n\n    Returns\n    -------\n    Result : dict\n\n        A dict with the following fields.\n\n            * 'surface_tilt'\n            * 'surface_azimuth'\n            * 'albedo'\n            * 'modules_per_string'\n            * 'strings_per_inverter'\n            * 'latitude'\n            * 'longitude'\n            * 'tz'\n            * 'name'\n            * 'altitude'\n\n    See also\n    --------\n    pvlib.tmy.readtmy3\n    pvlib.tmy.readtmy2\n    '''\n\n    try:\n        name = meta['Name']\n    except KeyError:\n        name = meta['City']\n\n    system = {'surface_tilt': surface_tilt,\n              'surface_azimuth': surface_azimuth,\n              'albedo': albedo,\n              'modules_per_string': modules_per_string,\n              'strings_per_inverter': strings_per_inverter,\n              'latitude': meta['latitude'],\n              'longitude': meta['longitude'],\n              'tz': meta['TZ'],\n              'name': name,\n              'altitude': meta['altitude']}\n\n    return system"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ashraeiam(aoi, b=0.05):\n    '''\n    Determine the incidence angle modifier using the ASHRAE transmission\n    model.\n\n    ashraeiam calculates the incidence angle modifier as developed in\n    [1], and adopted by ASHRAE (American Society of Heating,\n    Refrigeration, and Air Conditioning Engineers) [2]. The model has\n    been used by model programs such as PVSyst [3].\n\n    Note: For incident angles near 90 degrees, this model has a\n    discontinuity which has been addressed in this function.\n\n    Parameters\n    ----------\n    aoi : numeric\n        The angle of incidence between the module normal vector and the\n        sun-beam vector in degrees. Angles of nan will result in nan.\n\n    b : float, default 0.05\n        A parameter to adjust the modifier as a function of angle of\n        incidence. Typical values are on the order of 0.05 [3].\n\n    Returns\n    -------\n    IAM : numeric\n        The incident angle modifier calculated as 1-b*(sec(aoi)-1) as\n        described in [2,3].\n\n        Returns zeros for all abs(aoi) >= 90 and for all IAM values that\n        would be less than 0.\n\n    References\n    ----------\n    [1] Souka A.F., Safwat H.H., \"Determination of the optimum\n    orientations for the double exposure flat-plate collector and its\n    reflections\". Solar Energy vol .10, pp 170-174. 1966.\n\n    [2] ASHRAE standard 93-77\n\n    [3] PVsyst Contextual Help.\n    http://files.pvsyst.com/help/index.html?iam_loss.htm retrieved on\n    September 10, 2012\n\n    See Also\n    --------\n    irradiance.aoi\n    physicaliam\n    '''\n\n    iam = 1 - b * ((1 / np.cos(np.radians(aoi)) - 1))\n    aoi_gte_90 = np.full_like(aoi, False, dtype='bool')\n    np.greater_equal(np.abs(aoi), 90, where=~np.isnan(aoi), out=aoi_gte_90)\n    iam = np.where(aoi_gte_90, 0, iam)\n    iam = np.maximum(0, iam)\n\n    if isinstance(iam, pd.Series):\n        iam = pd.Series(iam, index=aoi.index)\n\n    return iam", "response": "This function calculates the incidence angle modifier using the ASHRAE transmission\n    model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetermining the incidence angle modifier using refractive index, extinction coefficient, and glazing thickness. physicaliam calculates the incidence angle modifier as described in De Soto et al. \"Improvement and validation of a model for photovoltaic array performance\", section 3. The calculation is based on a physical model of absorbtion and transmission through a cover. Note: The authors of this function believe that eqn. 14 in [1] is incorrect. This function uses the following equation in its place: theta_r = arcsin(1/n * sin(aoi)) Parameters ---------- aoi : numeric The angle of incidence between the module normal vector and the sun-beam vector in degrees. Angles of 0 are replaced with 1e-06 to ensure non-nan results. Angles of nan will result in nan. n : numeric, default 1.526 The effective index of refraction (unitless). Reference [1] indicates that a value of 1.526 is acceptable for glass. n must be a numeric scalar or vector with all values >=0. If n is a vector, it must be the same size as all other input vectors. K : numeric, default 4.0 The glazing extinction coefficient in units of 1/meters. Reference [1] indicates that a value of 4 is reasonable for \"water white\" glass. K must be a numeric scalar or vector with all values >=0. If K is a vector, it must be the same size as all other input vectors. L : numeric, default 0.002 The glazing thickness in units of meters. Reference [1] indicates that 0.002 meters (2 mm) is reasonable for most glass-covered PV panels. L must be a numeric scalar or vector with all values >=0. If L is a vector, it must be the same size as all other input vectors. Returns ------- iam : numeric The incident angle modifier References ---------- [1] W. De Soto et al., \"Improvement and validation of a model for photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88, 2006. [2] Duffie, John A. & Beckman, William A.. (2006). Solar Engineering of Thermal Processes, third edition. [Books24x7 version] Available from http://common.books24x7.com/toc.aspx?bookid=17160. See Also -------- getaoi ephemeris spa ashraeiam", "response": "def physicaliam(aoi, n=1.526, K=4., L=0.002):\n    '''\n    Determine the incidence angle modifier using refractive index,\n    extinction coefficient, and glazing thickness.\n\n    physicaliam calculates the incidence angle modifier as described in\n    De Soto et al. \"Improvement and validation of a model for\n    photovoltaic array performance\", section 3. The calculation is based\n    on a physical model of absorbtion and transmission through a\n    cover.\n\n    Note: The authors of this function believe that eqn. 14 in [1] is\n    incorrect. This function uses the following equation in its place:\n    theta_r = arcsin(1/n * sin(aoi))\n\n    Parameters\n    ----------\n    aoi : numeric\n        The angle of incidence between the module normal vector and the\n        sun-beam vector in degrees. Angles of 0 are replaced with 1e-06\n        to ensure non-nan results. Angles of nan will result in nan.\n\n    n : numeric, default 1.526\n        The effective index of refraction (unitless). Reference [1]\n        indicates that a value of 1.526 is acceptable for glass. n must\n        be a numeric scalar or vector with all values >=0. If n is a\n        vector, it must be the same size as all other input vectors.\n\n    K : numeric, default 4.0\n        The glazing extinction coefficient in units of 1/meters.\n        Reference [1] indicates that a value of  4 is reasonable for\n        \"water white\" glass. K must be a numeric scalar or vector with\n        all values >=0. If K is a vector, it must be the same size as\n        all other input vectors.\n\n    L : numeric, default 0.002\n        The glazing thickness in units of meters. Reference [1]\n        indicates that 0.002 meters (2 mm) is reasonable for most\n        glass-covered PV panels. L must be a numeric scalar or vector\n        with all values >=0. If L is a vector, it must be the same size\n        as all other input vectors.\n\n    Returns\n    -------\n    iam : numeric\n        The incident angle modifier\n\n    References\n    ----------\n    [1] W. De Soto et al., \"Improvement and validation of a model for\n    photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88,\n    2006.\n\n    [2] Duffie, John A. & Beckman, William A.. (2006). Solar Engineering\n    of Thermal Processes, third edition. [Books24x7 version] Available\n    from http://common.books24x7.com/toc.aspx?bookid=17160.\n\n    See Also\n    --------\n    getaoi\n    ephemeris\n    spa\n    ashraeiam\n    '''\n    zeroang = 1e-06\n\n    # hold a new reference to the input aoi object since we're going to\n    # overwrite the aoi reference below, but we'll need it for the\n    # series check at the end of the function\n    aoi_input = aoi\n\n    aoi = np.where(aoi == 0, zeroang, aoi)\n\n    # angle of reflection\n    thetar_deg = tools.asind(1.0 / n*(tools.sind(aoi)))\n\n    # reflectance and transmittance for normal incidence light\n    rho_zero = ((1-n) / (1+n)) ** 2\n    tau_zero = np.exp(-K*L)\n\n    # reflectance for parallel and perpendicular polarized light\n    rho_para = (tools.tand(thetar_deg - aoi) /\n                tools.tand(thetar_deg + aoi)) ** 2\n    rho_perp = (tools.sind(thetar_deg - aoi) /\n                tools.sind(thetar_deg + aoi)) ** 2\n\n    # transmittance for non-normal light\n    tau = np.exp(-K*L / tools.cosd(thetar_deg))\n\n    # iam is ratio of non-normal to normal incidence transmitted light\n    # after deducting the reflected portion of each\n    iam = ((1 - (rho_para + rho_perp) / 2) / (1 - rho_zero) * tau / tau_zero)\n\n    with np.errstate(invalid='ignore'):\n        # angles near zero produce nan, but iam is defined as one\n        small_angle = 1e-06\n        iam = np.where(np.abs(aoi) < small_angle, 1.0, iam)\n\n        # angles at 90 degrees can produce tiny negative values,\n        # which should be zero. this is a result of calculation precision\n        # rather than the physical model\n        iam = np.where(iam < 0, 0, iam)\n\n        # for light coming from behind the plane, none can enter the module\n        iam = np.where(aoi > 90, 0, iam)\n\n    if isinstance(aoi_input, pd.Series):\n        iam = pd.Series(iam, index=aoi_input.index)\n\n    return iam"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the five parameter values for a single diode equation at the given effective irradiance and cell temperature using the De Soto et al model.", "response": "def calcparams_desoto(effective_irradiance, temp_cell,\n                      alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,\n                      EgRef=1.121, dEgdT=-0.0002677,\n                      irrad_ref=1000, temp_ref=25):\n    '''\n    Calculates five parameter values for the single diode equation at\n    effective irradiance and cell temperature using the De Soto et al.\n    model described in [1]. The five values returned by calcparams_desoto\n    can be used by singlediode to calculate an IV curve.\n\n    Parameters\n    ----------\n    effective_irradiance : numeric\n        The irradiance (W/m2) that is converted to photocurrent.\n\n    temp_cell : numeric\n        The average cell temperature of cells within a module in C.\n\n    alpha_sc : float\n        The short-circuit current temperature coefficient of the\n        module in units of A/C.\n\n    a_ref : float\n        The product of the usual diode ideality factor (n, unitless),\n        number of cells in series (Ns), and cell thermal voltage at reference\n        conditions, in units of V.\n\n    I_L_ref : float\n        The light-generated current (or photocurrent) at reference conditions,\n        in amperes.\n\n    I_o_ref : float\n        The dark or diode reverse saturation current at reference conditions,\n        in amperes.\n\n    R_sh_ref : float\n        The shunt resistance at reference conditions, in ohms.\n\n    R_s : float\n        The series resistance at reference conditions, in ohms.\n\n    EgRef : float\n        The energy bandgap at reference temperature in units of eV.\n        1.121 eV for crystalline silicon. EgRef must be >0.  For parameters\n        from the SAM CEC module database, EgRef=1.121 is implicit for all\n        cell types in the parameter estimation algorithm used by NREL.\n\n    dEgdT : float\n        The temperature dependence of the energy bandgap at reference\n        conditions in units of 1/K. May be either a scalar value\n        (e.g. -0.0002677 as in [1]) or a DataFrame (this may be useful if\n        dEgdT is a modeled as a function of temperature). For parameters from\n        the SAM CEC module database, dEgdT=-0.0002677 is implicit for all cell\n        types in the parameter estimation algorithm used by NREL.\n\n    irrad_ref : float (optional, default=1000)\n        Reference irradiance in W/m^2.\n\n    temp_ref : float (optional, default=25)\n        Reference cell temperature in C.\n\n    Returns\n    -------\n    Tuple of the following results:\n\n    photocurrent : numeric\n        Light-generated current in amperes\n\n    saturation_current : numeric\n        Diode saturation curent in amperes\n\n    resistance_series : float\n        Series resistance in ohms\n\n    resistance_shunt : numeric\n        Shunt resistance in ohms\n\n    nNsVth : numeric\n        The product of the usual diode ideality factor (n, unitless),\n        number of cells in series (Ns), and cell thermal voltage at\n        specified effective irradiance and cell temperature.\n\n    References\n    ----------\n    [1] W. De Soto et al., \"Improvement and validation of a model for\n    photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88,\n    2006.\n\n    [2] System Advisor Model web page. https://sam.nrel.gov.\n\n    [3] A. Dobos, \"An Improved Coefficient Calculator for the California\n    Energy Commission 6 Parameter Photovoltaic Module Model\", Journal of\n    Solar Energy Engineering, vol 134, 2012.\n\n    [4] O. Madelung, \"Semiconductors: Data Handbook, 3rd ed.\" ISBN\n    3-540-40488-0\n\n    See Also\n    --------\n    singlediode\n    retrieve_sam\n\n    Notes\n    -----\n    If the reference parameters in the ModuleParameters struct are read\n    from a database or library of parameters (e.g. System Advisor\n    Model), it is important to use the same EgRef and dEgdT values that\n    were used to generate the reference parameters, regardless of the\n    actual bandgap characteristics of the semiconductor. For example, in\n    the case of the System Advisor Model library, created as described\n    in [3], EgRef and dEgdT for all modules were 1.121 and -0.0002677,\n    respectively.\n\n    This table of reference bandgap energies (EgRef), bandgap energy\n    temperature dependence (dEgdT), and \"typical\" airmass response (M)\n    is provided purely as reference to those who may generate their own\n    reference module parameters (a_ref, IL_ref, I0_ref, etc.) based upon\n    the various PV semiconductors. Again, we stress the importance of\n    using identical EgRef and dEgdT when generation reference parameters\n    and modifying the reference parameters (for irradiance, temperature,\n    and airmass) per DeSoto's equations.\n\n     Crystalline Silicon (Si):\n         * EgRef = 1.121\n         * dEgdT = -0.0002677\n\n         >>> M = np.polyval([-1.26E-4, 2.816E-3, -0.024459, 0.086257, 0.9181],\n         ...                AMa) # doctest: +SKIP\n\n         Source: [1]\n\n     Cadmium Telluride (CdTe):\n         * EgRef = 1.475\n         * dEgdT = -0.0003\n\n         >>> M = np.polyval([-2.46E-5, 9.607E-4, -0.0134, 0.0716, 0.9196],\n         ...                AMa) # doctest: +SKIP\n\n         Source: [4]\n\n     Copper Indium diSelenide (CIS):\n         * EgRef = 1.010\n         * dEgdT = -0.00011\n\n         >>> M = np.polyval([-3.74E-5, 0.00125, -0.01462, 0.0718, 0.9210],\n         ...                AMa) # doctest: +SKIP\n\n         Source: [4]\n\n     Copper Indium Gallium diSelenide (CIGS):\n         * EgRef = 1.15\n         * dEgdT = ????\n\n         >>> M = np.polyval([-9.07E-5, 0.0022, -0.0202, 0.0652, 0.9417],\n         ...                AMa) # doctest: +SKIP\n\n         Source: Wikipedia\n\n     Gallium Arsenide (GaAs):\n         * EgRef = 1.424\n         * dEgdT = -0.000433\n         * M = unknown\n\n         Source: [4]\n    '''\n\n    # test for use of function pre-v0.6.0 API change\n    if isinstance(a_ref, dict) or \\\n       (isinstance(a_ref, pd.Series) and ('a_ref' in a_ref.keys())):\n        import warnings\n        warnings.warn('module_parameters detected as fourth positional'\n                      + ' argument of calcparams_desoto. calcparams_desoto'\n                      + ' will require one argument for each module model'\n                      + ' parameter in v0.7.0 and later', DeprecationWarning)\n        try:\n            module_parameters = a_ref\n            a_ref = module_parameters['a_ref']\n            I_L_ref = module_parameters['I_L_ref']\n            I_o_ref = module_parameters['I_o_ref']\n            R_sh_ref = module_parameters['R_sh_ref']\n            R_s = module_parameters['R_s']\n        except Exception as e:\n            raise e('Module parameters could not be extracted from fourth'\n                    + ' positional argument of calcparams_desoto. Check that'\n                    + ' parameters are from the CEC database and/or update'\n                    + ' your code for the new API for calcparams_desoto')\n\n    # Boltzmann constant in eV/K\n    k = 8.617332478e-05\n\n    # reference temperature\n    Tref_K = temp_ref + 273.15\n    Tcell_K = temp_cell + 273.15\n\n    E_g = EgRef * (1 + dEgdT*(Tcell_K - Tref_K))\n\n    nNsVth = a_ref * (Tcell_K / Tref_K)\n\n    # In the equation for IL, the single factor effective_irradiance is\n    # used, in place of the product S*M in [1]. effective_irradiance is\n    # equivalent to the product of S (irradiance reaching a module's cells) *\n    # M (spectral adjustment factor) as described in [1].\n    IL = effective_irradiance / irrad_ref * \\\n        (I_L_ref + alpha_sc * (Tcell_K - Tref_K))\n    I0 = (I_o_ref * ((Tcell_K / Tref_K) ** 3) *\n          (np.exp(EgRef / (k*(Tref_K)) - (E_g / (k*(Tcell_K))))))\n    # Note that the equation for Rsh differs from [1]. In [1] Rsh is given as\n    # Rsh = Rsh_ref * (S_ref / S) where S is broadband irradiance reaching\n    # the module's cells. If desired this model behavior can be duplicated\n    # by applying reflection and soiling losses to broadband plane of array\n    # irradiance and not applying a spectral loss modifier, i.e.,\n    # spectral_modifier = 1.0.\n    Rsh = R_sh_ref * (irrad_ref / effective_irradiance)\n    Rs = R_s\n\n    return IL, I0, Rs, Rsh, nNsVth"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the five parameter values for a single diode equation at the given effective irradiance and cell temperature.", "response": "def calcparams_cec(effective_irradiance, temp_cell,\n                   alpha_sc, a_ref, I_L_ref, I_o_ref, R_sh_ref, R_s,\n                   Adjust, EgRef=1.121, dEgdT=-0.0002677,\n                   irrad_ref=1000, temp_ref=25):\n    '''\n    Calculates five parameter values for the single diode equation at\n    effective irradiance and cell temperature using the CEC\n    model described in [1]. The CEC model differs from the De soto et al.\n    model [3] by the parameter Adjust. The five values returned by\n    calcparams_cec can be used by singlediode to calculate an IV curve.\n\n    Parameters\n    ----------\n    effective_irradiance : numeric\n        The irradiance (W/m2) that is converted to photocurrent.\n\n    temp_cell : numeric\n        The average cell temperature of cells within a module in C.\n\n    alpha_sc : float\n        The short-circuit current temperature coefficient of the\n        module in units of A/C.\n\n    a_ref : float\n        The product of the usual diode ideality factor (n, unitless),\n        number of cells in series (Ns), and cell thermal voltage at reference\n        conditions, in units of V.\n\n    I_L_ref : float\n        The light-generated current (or photocurrent) at reference conditions,\n        in amperes.\n\n    I_o_ref : float\n        The dark or diode reverse saturation current at reference conditions,\n        in amperes.\n\n    R_sh_ref : float\n        The shunt resistance at reference conditions, in ohms.\n\n    R_s : float\n        The series resistance at reference conditions, in ohms.\n\n    Adjust : float\n        The adjustment to the temperature coefficient for short circuit\n        current, in percent\n\n    EgRef : float\n        The energy bandgap at reference temperature in units of eV.\n        1.121 eV for crystalline silicon. EgRef must be >0.  For parameters\n        from the SAM CEC module database, EgRef=1.121 is implicit for all\n        cell types in the parameter estimation algorithm used by NREL.\n\n    dEgdT : float\n        The temperature dependence of the energy bandgap at reference\n        conditions in units of 1/K. May be either a scalar value\n        (e.g. -0.0002677 as in [3]) or a DataFrame (this may be useful if\n        dEgdT is a modeled as a function of temperature). For parameters from\n        the SAM CEC module database, dEgdT=-0.0002677 is implicit for all cell\n        types in the parameter estimation algorithm used by NREL.\n\n    irrad_ref : float (optional, default=1000)\n        Reference irradiance in W/m^2.\n\n    temp_ref : float (optional, default=25)\n        Reference cell temperature in C.\n\n    Returns\n    -------\n    Tuple of the following results:\n\n    photocurrent : numeric\n        Light-generated current in amperes\n\n    saturation_current : numeric\n        Diode saturation curent in amperes\n\n    resistance_series : float\n        Series resistance in ohms\n\n    resistance_shunt : numeric\n        Shunt resistance in ohms\n\n    nNsVth : numeric\n        The product of the usual diode ideality factor (n, unitless),\n        number of cells in series (Ns), and cell thermal voltage at\n        specified effective irradiance and cell temperature.\n\n    References\n    ----------\n    [1] A. Dobos, \"An Improved Coefficient Calculator for the California\n    Energy Commission 6 Parameter Photovoltaic Module Model\", Journal of\n    Solar Energy Engineering, vol 134, 2012.\n\n    [2] System Advisor Model web page. https://sam.nrel.gov.\n\n    [3] W. De Soto et al., \"Improvement and validation of a model for\n    photovoltaic array performance\", Solar Energy, vol 80, pp. 78-88,\n    2006.\n\n    See Also\n    --------\n    calcparams_desoto\n    singlediode\n    retrieve_sam\n\n    '''\n\n    # pass adjusted temperature coefficient to desoto\n    return calcparams_desoto(effective_irradiance, temp_cell,\n                             alpha_sc*(1.0 - Adjust/100),\n                             a_ref, I_L_ref, I_o_ref,\n                             R_sh_ref, R_s,\n                             EgRef=1.121, dEgdT=-0.0002677,\n                             irrad_ref=1000, temp_ref=25)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate the five parameter values for a single diode equation at the given effective irradiance and cell temperature using the PVsyst v6 formula.", "response": "def calcparams_pvsyst(effective_irradiance, temp_cell,\n                      alpha_sc, gamma_ref, mu_gamma,\n                      I_L_ref, I_o_ref,\n                      R_sh_ref, R_sh_0, R_s,\n                      cells_in_series,\n                      R_sh_exp=5.5,\n                      EgRef=1.121,\n                      irrad_ref=1000, temp_ref=25):\n    '''\n    Calculates five parameter values for the single diode equation at\n    effective irradiance and cell temperature using the PVsyst v6\n    model described in [1,2,3]. The five values returned by calcparams_pvsyst\n    can be used by singlediode to calculate an IV curve.\n\n    Parameters\n    ----------\n    effective_irradiance : numeric\n        The irradiance (W/m2) that is converted to photocurrent.\n\n    temp_cell : numeric\n        The average cell temperature of cells within a module in C.\n\n    alpha_sc : float\n        The short-circuit current temperature coefficient of the\n        module in units of A/C.\n\n    gamma_ref : float\n        The diode ideality factor\n\n    mu_gamma : float\n        The temperature coefficient for the diode ideality factor, 1/K\n\n    I_L_ref : float\n        The light-generated current (or photocurrent) at reference conditions,\n        in amperes.\n\n    I_o_ref : float\n        The dark or diode reverse saturation current at reference conditions,\n        in amperes.\n\n    R_sh_ref : float\n        The shunt resistance at reference conditions, in ohms.\n\n    R_sh_0 : float\n        The shunt resistance at zero irradiance conditions, in ohms.\n\n    R_s : float\n        The series resistance at reference conditions, in ohms.\n\n    cells_in_series : integer\n        The number of cells connected in series.\n\n    R_sh_exp : float\n        The exponent in the equation for shunt resistance, unitless. Defaults\n        to 5.5.\n\n    EgRef : float\n        The energy bandgap at reference temperature in units of eV.\n        1.121 eV for crystalline silicon. EgRef must be >0.\n\n    irrad_ref : float (optional, default=1000)\n        Reference irradiance in W/m^2.\n\n    temp_ref : float (optional, default=25)\n        Reference cell temperature in C.\n\n    Returns\n    -------\n    Tuple of the following results:\n\n    photocurrent : numeric\n        Light-generated current in amperes\n\n    saturation_current : numeric\n        Diode saturation current in amperes\n\n    resistance_series : float\n        Series resistance in ohms\n\n    resistance_shunt : numeric\n        Shunt resistance in ohms\n\n    nNsVth : numeric\n        The product of the usual diode ideality factor (n, unitless),\n        number of cells in series (Ns), and cell thermal voltage at\n        specified effective irradiance and cell temperature.\n\n    References\n    ----------\n    [1] K. Sauer, T. Roessler, C. W. Hansen, Modeling the Irradiance and\n     Temperature Dependence of Photovoltaic Modules in PVsyst,\n     IEEE Journal of Photovoltaics v5(1), January 2015.\n\n    [2] A. Mermoud, PV modules modelling, Presentation at the 2nd PV\n     Performance Modeling Workshop, Santa Clara, CA, May 2013\n\n    [3] A. Mermoud, T. Lejeune, Performance Assessment of a Simulation Model\n     for PV modules of any available technology, 25th European Photovoltaic\n     Solar Energy Conference, Valencia, Spain, Sept. 2010\n\n    See Also\n    --------\n    calcparams_desoto\n    singlediode\n\n    '''\n\n    # Boltzmann constant in J/K\n    k = 1.38064852e-23\n\n    # elementary charge in coulomb\n    q = 1.6021766e-19\n\n    # reference temperature\n    Tref_K = temp_ref + 273.15\n    Tcell_K = temp_cell + 273.15\n\n    gamma = gamma_ref + mu_gamma * (Tcell_K - Tref_K)\n    nNsVth = gamma * k / q * cells_in_series * Tcell_K\n\n    IL = effective_irradiance / irrad_ref * \\\n        (I_L_ref + alpha_sc * (Tcell_K - Tref_K))\n\n    I0 = I_o_ref * ((Tcell_K / Tref_K) ** 3) * \\\n        (np.exp((q * EgRef) / (k * gamma) * (1 / Tref_K - 1 / Tcell_K)))\n\n    Rsh_tmp = \\\n        (R_sh_ref - R_sh_0 * np.exp(-R_sh_exp)) / (1.0 - np.exp(-R_sh_exp))\n    Rsh_base = np.maximum(0.0, Rsh_tmp)\n\n    Rsh = Rsh_base + (R_sh_0 - Rsh_base) * \\\n        np.exp(-R_sh_exp * effective_irradiance / irrad_ref)\n\n    Rs = R_s\n\n    return IL, I0, Rs, Rsh, nNsVth"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef retrieve_sam(name=None, path=None):\n    '''\n    Retrieve latest module and inverter info from a local file or the\n    SAM website.\n\n    This function will retrieve either:\n\n        * CEC module database\n        * Sandia Module database\n        * CEC Inverter database\n        * Anton Driesse Inverter database\n\n    and return it as a pandas DataFrame.\n\n    Parameters\n    ----------\n    name : None or string, default None\n        Name can be one of:\n\n        * 'CECMod' - returns the CEC module database\n        * 'CECInverter' - returns the CEC Inverter database\n        * 'SandiaInverter' - returns the CEC Inverter database\n          (CEC is only current inverter db available; tag kept for\n          backwards compatibility)\n        * 'SandiaMod' - returns the Sandia Module database\n        * 'ADRInverter' - returns the ADR Inverter database\n\n    path : None or string, default None\n        Path to the SAM file. May also be a URL.\n\n    If both name and path are None, a dialogue will open allowing the\n    user to select a file.\n\n    Returns\n    -------\n    samfile : DataFrame\n        A DataFrame containing all the elements of the desired database.\n        Each column represents a module or inverter, and a specific\n        dataset can be retrieved by the command\n\n    Notes\n    -----\n    Files available at https://sam.nrel.gov/sites/default/files/\n\n    Examples\n    --------\n\n    >>> from pvlib import pvsystem\n    >>> invdb = pvsystem.retrieve_sam('CECInverter')\n    >>> inverter = invdb.AE_Solar_Energy__AE6_0__277V__277V__CEC_2012_\n    >>> inverter\n    Vac           277.000000\n    Paco         6000.000000\n    Pdco         6165.670000\n    Vdco          361.123000\n    Pso            36.792300\n    C0             -0.000002\n    C1             -0.000047\n    C2             -0.001861\n    C3              0.000721\n    Pnt             0.070000\n    Vdcmax        600.000000\n    Idcmax         32.000000\n    Mppt_low      200.000000\n    Mppt_high     500.000000\n    Name: AE_Solar_Energy__AE6_0__277V__277V__CEC_2012_, dtype: float64\n    '''\n\n    if name is not None:\n        name = name.lower()\n        data_path = os.path.join(\n            os.path.dirname(os.path.abspath(__file__)), 'data')\n        if name == 'cecmod':\n            csvdata = os.path.join(\n                data_path, 'sam-library-cec-modules-2017-6-5.csv')\n        elif name == 'sandiamod':\n            csvdata = os.path.join(\n                data_path, 'sam-library-sandia-modules-2015-6-30.csv')\n        elif name == 'adrinverter':\n            csvdata = os.path.join(data_path, 'adr-library-2013-10-01.csv')\n        elif name in ['cecinverter', 'sandiainverter']:\n            # Allowing either, to provide for old code,\n            # while aligning with current expectations\n            csvdata = os.path.join(\n                data_path, 'sam-library-cec-inverters-2018-3-18.csv')\n        else:\n            raise ValueError('invalid name {}'.format(name))\n    elif path is not None:\n        if path.startswith('http'):\n            response = urlopen(path)\n            csvdata = io.StringIO(response.read().decode(errors='ignore'))\n        else:\n            csvdata = path\n    elif name is None and path is None:\n        try:\n            # python 2\n            import Tkinter as tkinter\n            from tkFileDialog import askopenfilename\n        except ImportError:\n            # python 3\n            import tkinter\n            from tkinter.filedialog import askopenfilename\n\n        tkinter.Tk().withdraw()\n        csvdata = askopenfilename()\n\n    return _parse_raw_sam_df(csvdata)", "response": "Retrieve the latest module and inverter info from a local file or the SAM website."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate cell temperature using an emperical heat loss factor model and model parameters.", "response": "def pvsyst_celltemp(poa_global, temp_air, wind_speed=1.0, eta_m=0.1,\n                    alpha_absorption=0.9, model_params='freestanding'):\n    \"\"\"\n    Calculate cell temperature using an emperical heat loss factor model\n    as implemented in PVsyst.\n\n    The heat loss factors provided through the 'model_params' argument\n    represent the combined effect of convection, radiation and conduction,\n    and their values are experimentally determined.\n\n    Parameters\n    ----------\n    poa_global : numeric\n        Total incident irradiance in W/m^2.\n\n    temp_air : numeric\n        Ambient dry bulb temperature in degrees C.\n\n    wind_speed : numeric, default 1.0\n        Wind speed in m/s measured at the same height for which the wind loss\n        factor was determined.  The default value is 1.0, which is the wind\n        speed at module height used to determine NOCT.\n\n    eta_m : numeric, default 0.1\n        Module external efficiency as a fraction, i.e., DC power / poa_global.\n\n    alpha_absorption : numeric, default 0.9\n        Absorption coefficient\n\n    model_params : string, tuple, or list (no dict), default 'freestanding'\n        Heat loss factors to be used.\n\n        If string, can be:\n\n            * 'freestanding' (default)\n                Modules with rear surfaces exposed to open air (e.g. rack\n                mounted).\n            * 'insulated'\n                Modules with rear surfaces in close proximity to another\n                surface (e.g. roof mounted).\n\n        If tuple/list, supply parameters in the following order:\n\n            * constant_loss_factor : float\n                Combined heat loss factor coefficient. Freestanding\n                default is 29, fully insulated arrays is 15.\n\n            * wind_loss_factor : float\n                Combined heat loss factor influenced by wind. Default is 0.\n\n    Returns\n    -------\n    temp_cell : numeric or Series\n        Cell temperature in degrees Celsius\n\n    References\n    ----------\n    [1]\"PVsyst 6 Help\", Files.pvsyst.com, 2018. [Online]. Available:\n    http://files.pvsyst.com/help/index.html. [Accessed: 10- Dec- 2018].\n\n    [2] Faiman, D. (2008). \"Assessing the outdoor operating temperature of\n    photovoltaic modules.\" Progress in Photovoltaics 16(4): 307-315.\n    \"\"\"\n\n    pvsyst_presets = TEMP_MODEL_PARAMS['pvsyst']\n\n    if isinstance(model_params, str):\n        model_params = model_params.lower()\n        constant_loss_factor, wind_loss_factor = pvsyst_presets[model_params]\n    elif isinstance(model_params, (tuple, list)):\n        constant_loss_factor, wind_loss_factor = model_params\n    else:\n        raise TypeError(\n            \"Please provide model_params as a str, or tuple/list.\"\n        )\n\n    total_loss_factor = wind_loss_factor * wind_speed + constant_loss_factor\n    heat_input = poa_global * alpha_absorption * (1 - eta_m)\n    temp_difference = heat_input / total_loss_factor\n    temp_cell = temp_air + temp_difference\n\n    return temp_cell"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sapm_spectral_loss(airmass_absolute, module):\n\n    am_coeff = [module['A4'], module['A3'], module['A2'], module['A1'],\n                module['A0']]\n\n    spectral_loss = np.polyval(am_coeff, airmass_absolute)\n\n    spectral_loss = np.where(np.isnan(spectral_loss), 0, spectral_loss)\n\n    spectral_loss = np.maximum(0, spectral_loss)\n\n    if isinstance(airmass_absolute, pd.Series):\n        spectral_loss = pd.Series(spectral_loss, airmass_absolute.index)\n\n    return spectral_loss", "response": "Calculates the SAPM spectral loss coefficient for a single airmass."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sapm_aoi_loss(aoi, module, upper=None):\n\n    aoi_coeff = [module['B5'], module['B4'], module['B3'], module['B2'],\n                 module['B1'], module['B0']]\n\n    aoi_loss = np.polyval(aoi_coeff, aoi)\n    aoi_loss = np.clip(aoi_loss, 0, upper)\n    # nan tolerant masking\n    aoi_lt_0 = np.full_like(aoi, False, dtype='bool')\n    np.less(aoi, 0, where=~np.isnan(aoi), out=aoi_lt_0)\n    aoi_loss = np.where(aoi_lt_0, 0, aoi_loss)\n\n    if isinstance(aoi, pd.Series):\n        aoi_loss = pd.Series(aoi_loss, aoi.index)\n\n    return aoi_loss", "response": "Calculates the SAPM angle of incidence loss coefficient F2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the SAPM effective irradiance for a given set of poasisons.", "response": "def sapm_effective_irradiance(poa_direct, poa_diffuse, airmass_absolute, aoi,\n                              module, reference_irradiance=1000):\n    \"\"\"\n    Calculates the SAPM effective irradiance using the SAPM spectral\n    loss and SAPM angle of incidence loss functions.\n\n    Parameters\n    ----------\n    poa_direct : numeric\n        The direct irradiance incident upon the module.\n\n    poa_diffuse : numeric\n        The diffuse irradiance incident on module.\n\n    airmass_absolute : numeric\n        Absolute airmass.\n\n    aoi : numeric\n        Angle of incidence in degrees.\n\n    module : dict-like\n        A dict, Series, or DataFrame defining the SAPM performance\n        parameters. See the :py:func:`sapm` notes section for more\n        details.\n\n    reference_irradiance : numeric, default 1000\n        Reference irradiance by which to divide the input irradiance.\n\n    Returns\n    -------\n    effective_irradiance : numeric\n        The SAPM effective irradiance.\n    \"\"\"\n\n    F1 = sapm_spectral_loss(airmass_absolute, module)\n    F2 = sapm_aoi_loss(aoi, module)\n\n    E0 = reference_irradiance\n\n    Ee = F1 * (poa_direct*F2 + module['FD']*poa_diffuse) / E0\n\n    return Ee"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef singlediode(photocurrent, saturation_current, resistance_series,\n                resistance_shunt, nNsVth, ivcurve_pnts=None,\n                method='lambertw'):\n    \"\"\"\n    Solve the single-diode model to obtain a photovoltaic IV curve.\n\n    Singlediode solves the single diode equation [1]\n\n    .. math::\n\n        I = IL - I0*[exp((V+I*Rs)/(nNsVth))-1] - (V + I*Rs)/Rsh\n\n    for ``I`` and ``V`` when given ``IL, I0, Rs, Rsh,`` and ``nNsVth\n    (nNsVth = n*Ns*Vth)`` which are described later. Returns a DataFrame\n    which contains the 5 points on the I-V curve specified in\n    SAND2004-3535 [3]. If all IL, I0, Rs, Rsh, and nNsVth are scalar, a\n    single curve will be returned, if any are Series (of the same\n    length), multiple IV curves will be calculated.\n\n    The input parameters can be calculated using calcparams_desoto from\n    meteorological data.\n\n    Parameters\n    ----------\n    photocurrent : numeric\n        Light-generated current (photocurrent) in amperes under desired\n        IV curve conditions. Often abbreviated ``I_L``.\n        0 <= photocurrent\n\n    saturation_current : numeric\n        Diode saturation current in amperes under desired IV curve\n        conditions. Often abbreviated ``I_0``.\n        0 < saturation_current\n\n    resistance_series : numeric\n        Series resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rs``.\n        0 <= resistance_series < numpy.inf\n\n    resistance_shunt : numeric\n        Shunt resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rsh``.\n        0 < resistance_shunt <= numpy.inf\n\n    nNsVth : numeric\n        The product of three components. 1) The usual diode ideal factor\n        (n), 2) the number of cells in series (Ns), and 3) the cell\n        thermal voltage under the desired IV curve conditions (Vth). The\n        thermal voltage of the cell (in volts) may be calculated as\n        ``k*temp_cell/q``, where k is Boltzmann's constant (J/K),\n        temp_cell is the temperature of the p-n junction in Kelvin, and\n        q is the charge of an electron (coulombs).\n        0 < nNsVth\n\n    ivcurve_pnts : None or int, default None\n        Number of points in the desired IV curve. If None or 0, no\n        IV curves will be produced.\n\n    method : str, default 'lambertw'\n        Determines the method used to calculate points on the IV curve. The\n        options are ``'lambertw'``, ``'newton'``, or ``'brentq'``.\n\n    Returns\n    -------\n    OrderedDict or DataFrame\n\n    The returned dict-like object always contains the keys/columns:\n\n        * i_sc - short circuit current in amperes.\n        * v_oc - open circuit voltage in volts.\n        * i_mp - current at maximum power point in amperes.\n        * v_mp - voltage at maximum power point in volts.\n        * p_mp - power at maximum power point in watts.\n        * i_x - current, in amperes, at ``v = 0.5*v_oc``.\n        * i_xx - current, in amperes, at ``V = 0.5*(v_oc+v_mp)``.\n\n    If ivcurve_pnts is greater than 0, the output dictionary will also\n    include the keys:\n\n        * i - IV curve current in amperes.\n        * v - IV curve voltage in volts.\n\n    The output will be an OrderedDict if photocurrent is a scalar,\n    array, or ivcurve_pnts is not None.\n\n    The output will be a DataFrame if photocurrent is a Series and\n    ivcurve_pnts is None.\n\n    Notes\n    -----\n    If the method is ``'lambertw'`` then the solution employed to solve the\n    implicit diode equation utilizes the Lambert W function to obtain an\n    explicit function of :math:`V=f(I)` and :math:`I=f(V)` as shown in [2].\n\n    If the method is ``'newton'`` then the root-finding Newton-Raphson method\n    is used. It should be safe for well behaved IV-curves, but the ``'brentq'``\n    method is recommended for reliability.\n\n    If the method is ``'brentq'`` then Brent's bisection search method is used\n    that guarantees convergence by bounding the voltage between zero and\n    open-circuit.\n\n    If the method is either ``'newton'`` or ``'brentq'`` and ``ivcurve_pnts``\n    are indicated, then :func:`pvlib.singlediode.bishop88` [4] is used to\n    calculate the points on the IV curve points at diode voltages from zero to\n    open-circuit voltage with a log spacing that gets closer as voltage\n    increases. If the method is ``'lambertw'`` then the calculated points on\n    the IV curve are linearly spaced.\n\n    References\n    -----------\n    [1] S.R. Wenham, M.A. Green, M.E. Watt, \"Applied Photovoltaics\" ISBN\n    0 86758 909 4\n\n    [2] A. Jain, A. Kapoor, \"Exact analytical solutions of the\n    parameters of real solar cells using Lambert W-function\", Solar\n    Energy Materials and Solar Cells, 81 (2004) 269-277.\n\n    [3] D. King et al, \"Sandia Photovoltaic Array Performance Model\",\n    SAND2004-3535, Sandia National Laboratories, Albuquerque, NM\n\n    [4] \"Computer simulation of the effects of electrical mismatches in\n    photovoltaic cell interconnection circuits\" JW Bishop, Solar Cell (1988)\n    https://doi.org/10.1016/0379-6787(88)90059-2\n\n    See also\n    --------\n    sapm\n    calcparams_desoto\n    pvlib.singlediode.bishop88\n    \"\"\"\n    # Calculate points on the IV curve using the LambertW solution to the\n    # single diode equation\n    if method.lower() == 'lambertw':\n        out = _singlediode._lambertw(\n            photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth, ivcurve_pnts\n        )\n        i_sc, v_oc, i_mp, v_mp, p_mp, i_x, i_xx = out[:7]\n        if ivcurve_pnts:\n            ivcurve_i, ivcurve_v = out[7:]\n    else:\n        # Calculate points on the IV curve using either 'newton' or 'brentq'\n        # methods. Voltages are determined by first solving the single diode\n        # equation for the diode voltage V_d then backing out voltage\n        args = (photocurrent, saturation_current, resistance_series,\n                resistance_shunt, nNsVth)  # collect args\n        v_oc = _singlediode.bishop88_v_from_i(\n            0.0, *args, method=method.lower()\n        )\n        i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(\n            *args, method=method.lower()\n        )\n        i_sc = _singlediode.bishop88_i_from_v(\n            0.0, *args, method=method.lower()\n        )\n        i_x = _singlediode.bishop88_i_from_v(\n            v_oc / 2.0, *args, method=method.lower()\n        )\n        i_xx = _singlediode.bishop88_i_from_v(\n            (v_oc + v_mp) / 2.0, *args, method=method.lower()\n        )\n\n        # calculate the IV curve if requested using bishop88\n        if ivcurve_pnts:\n            vd = v_oc * (\n                    (11.0 - np.logspace(np.log10(11.0), 0.0,\n                                        ivcurve_pnts)) / 10.0\n            )\n            ivcurve_i, ivcurve_v, _ = _singlediode.bishop88(vd, *args)\n\n    out = OrderedDict()\n    out['i_sc'] = i_sc\n    out['v_oc'] = v_oc\n    out['i_mp'] = i_mp\n    out['v_mp'] = v_mp\n    out['p_mp'] = p_mp\n    out['i_x'] = i_x\n    out['i_xx'] = i_xx\n\n    if ivcurve_pnts:\n\n        out['v'] = ivcurve_v\n        out['i'] = ivcurve_i\n\n    if isinstance(photocurrent, pd.Series) and not ivcurve_pnts:\n        out = pd.DataFrame(out, index=photocurrent.index)\n\n    return out", "response": "Solve the single - diode model to obtain a single - diode structure."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalculates the maximum power point for a single diode.", "response": "def max_power_point(photocurrent, saturation_current, resistance_series,\n                    resistance_shunt, nNsVth, method='brentq'):\n    \"\"\"\n    Given the single diode equation coefficients, calculates the maximum power\n    point (MPP).\n\n    Parameters\n    ----------\n    photocurrent : numeric\n        photo-generated current [A]\n    saturation_current : numeric\n        diode reverse saturation current [A]\n    resistance_series : numeric\n        series resitance [ohms]\n    resistance_shunt : numeric\n        shunt resitance [ohms]\n    nNsVth : numeric\n        product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n        and number of serices cells ``Ns``\n    method : str\n        either ``'newton'`` or ``'brentq'``\n\n    Returns\n    -------\n    OrderedDict or pandas.Datafrane\n        ``(i_mp, v_mp, p_mp)``\n\n    Notes\n    -----\n    Use this function when you only want to find the maximum power point. Use\n    :func:`singlediode` when you need to find additional points on the IV\n    curve. This function uses Brent's method by default because it is\n    guaranteed to converge.\n    \"\"\"\n    i_mp, v_mp, p_mp = _singlediode.bishop88_mpp(\n        photocurrent, saturation_current, resistance_series,\n        resistance_shunt, nNsVth, method=method.lower()\n    )\n    if isinstance(photocurrent, pd.Series):\n        ivp = {'i_mp': i_mp, 'v_mp': v_mp, 'p_mp': p_mp}\n        out = pd.DataFrame(ivp, index=photocurrent.index)\n    else:\n        out = OrderedDict()\n        out['i_mp'] = i_mp\n        out['v_mp'] = v_mp\n        out['p_mp'] = p_mp\n    return out"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n             saturation_current, photocurrent, method='lambertw'):\n    '''\n    Device current at the given device voltage for the single diode model.\n\n    Uses the single diode model (SDM) as described in, e.g.,\n     Jain and Kapoor 2004 [1].\n    The solution is per Eq 2 of [1] except when resistance_series=0,\n     in which case the explict solution for current is used.\n    Ideal device parameters are specified by resistance_shunt=np.inf and\n     resistance_series=0.\n    Inputs to this function can include scalars and pandas.Series, but it is\n     the caller's responsibility to ensure that the arguments are all float64\n     and within the proper ranges.\n\n    Parameters\n    ----------\n    resistance_shunt : numeric\n        Shunt resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rsh``.\n        0 < resistance_shunt <= numpy.inf\n\n    resistance_series : numeric\n        Series resistance in ohms under desired IV curve conditions.\n        Often abbreviated ``Rs``.\n        0 <= resistance_series < numpy.inf\n\n    nNsVth : numeric\n        The product of three components. 1) The usual diode ideal factor\n        (n), 2) the number of cells in series (Ns), and 3) the cell\n        thermal voltage under the desired IV curve conditions (Vth). The\n        thermal voltage of the cell (in volts) may be calculated as\n        ``k*temp_cell/q``, where k is Boltzmann's constant (J/K),\n        temp_cell is the temperature of the p-n junction in Kelvin, and\n        q is the charge of an electron (coulombs).\n        0 < nNsVth\n\n    voltage : numeric\n        The voltage in Volts under desired IV curve conditions.\n\n    saturation_current : numeric\n        Diode saturation current in amperes under desired IV curve\n        conditions. Often abbreviated ``I_0``.\n        0 < saturation_current\n\n    photocurrent : numeric\n        Light-generated current (photocurrent) in amperes under desired\n        IV curve conditions. Often abbreviated ``I_L``.\n        0 <= photocurrent\n\n    method : str\n        Method to use: ``'lambertw'``, ``'newton'``, or ``'brentq'``. *Note*:\n        ``'brentq'`` is limited to 1st quadrant only.\n\n    Returns\n    -------\n    current : np.ndarray or scalar\n\n    References\n    ----------\n    [1] A. Jain, A. Kapoor, \"Exact analytical solutions of the\n    parameters of real solar cells using Lambert W-function\", Solar\n    Energy Materials and Solar Cells, 81 (2004) 269-277.\n    '''\n    if method.lower() == 'lambertw':\n        return _singlediode._lambertw_i_from_v(\n            resistance_shunt, resistance_series, nNsVth, voltage,\n            saturation_current, photocurrent\n        )\n    else:\n        # Calculate points on the IV curve using either 'newton' or 'brentq'\n        # methods. Voltages are determined by first solving the single diode\n        # equation for the diode voltage V_d then backing out voltage\n        args = (voltage, photocurrent, saturation_current, resistance_series,\n                resistance_shunt, nNsVth)\n        I = _singlediode.bishop88_i_from_v(*args, method=method.lower())\n        # find the right size and shape for returns\n        size, shape = _singlediode._get_size_and_shape(args)\n        if size <= 1:\n            if shape is not None:\n                I = np.tile(I, shape)\n        if np.isnan(I).any() and size <= 1:\n            I = np.repeat(I, size)\n            if shape is not None:\n                I = I.reshape(shape)\n        return I", "response": "This function returns the i_from_v method for a single diode model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nscales the voltage current and power of the DataFrames of a single - node node.", "response": "def scale_voltage_current_power(data, voltage=1, current=1):\n    \"\"\"\n    Scales the voltage, current, and power of the DataFrames\n    returned by :py:func:`singlediode` and :py:func:`sapm`.\n\n    Parameters\n    ----------\n    data: DataFrame\n        Must contain columns `'v_mp', 'v_oc', 'i_mp' ,'i_x', 'i_xx',\n        'i_sc', 'p_mp'`.\n    voltage: numeric, default 1\n        The amount by which to multiply the voltages.\n    current: numeric, default 1\n        The amount by which to multiply the currents.\n\n    Returns\n    -------\n    scaled_data: DataFrame\n        A scaled copy of the input data.\n        `'p_mp'` is scaled by `voltage * current`.\n    \"\"\"\n\n    # as written, only works with a DataFrame\n    # could make it work with a dict, but it would be more verbose\n    data = data.copy()\n    voltages = ['v_mp', 'v_oc']\n    currents = ['i_mp', 'i_x', 'i_xx', 'i_sc']\n    data[voltages] *= voltage\n    data[currents] *= current\n    data['p_mp'] *= voltage * current\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef pvwatts_ac(pdc, pdc0, eta_inv_nom=0.96, eta_inv_ref=0.9637):\n\n    pac0 = eta_inv_nom * pdc0\n    zeta = pdc / pdc0\n\n    # arrays to help avoid divide by 0 for scalar and array\n    eta = np.zeros_like(pdc, dtype=float)\n    pdc_neq_0 = ~np.equal(pdc, 0)\n\n    # eta < 0 if zeta < 0.006. pac is forced to be >= 0 below. GH 541\n    eta = eta_inv_nom / eta_inv_ref * (\n        - 0.0162*zeta\n        - np.divide(0.0059, zeta, out=eta, where=pdc_neq_0)\n        + 0.9858)\n\n    pac = eta * pdc\n    pac = np.minimum(pac0, pac)\n    pac = np.maximum(0, pac)     # GH 541\n\n    return pac", "response": "r Returns a new object containing the given PDC and AC power."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the angle of incidence on the system.", "response": "def get_aoi(self, solar_zenith, solar_azimuth):\n        \"\"\"Get the angle of incidence on the system.\n\n        Parameters\n        ----------\n        solar_zenith : float or Series.\n            Solar zenith angle.\n        solar_azimuth : float or Series.\n            Solar azimuth angle.\n\n        Returns\n        -------\n        aoi : Series\n            The angle of incidence\n        \"\"\"\n\n        aoi = irradiance.aoi(self.surface_tilt, self.surface_azimuth,\n                             solar_zenith, solar_azimuth)\n        return aoi"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the incidence angle modifier using the ashraeiam function.", "response": "def ashraeiam(self, aoi):\n        \"\"\"\n        Determine the incidence angle modifier using\n        ``self.module_parameters['b']``, ``aoi``,\n        and the :py:func:`ashraeiam` function.\n\n        Uses default arguments if keys not in module_parameters.\n\n        Parameters\n        ----------\n        aoi : numeric\n            The angle of incidence in degrees.\n\n        Returns\n        -------\n        modifier : numeric\n            The AOI modifier.\n        \"\"\"\n        kwargs = _build_kwargs(['b'], self.module_parameters)\n\n        return ashraeiam(aoi, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndetermine the incidence angle modifier using the AOI modifier using the physicaliam function.", "response": "def physicaliam(self, aoi):\n        \"\"\"\n        Determine the incidence angle modifier using ``aoi``,\n        ``self.module_parameters['K']``,\n        ``self.module_parameters['L']``,\n        ``self.module_parameters['n']``,\n        and the\n        :py:func:`physicaliam` function.\n\n        Uses default arguments if keys not in module_parameters.\n\n        Parameters\n        ----------\n        aoi : numeric\n            The angle of incidence in degrees.\n\n        Returns\n        -------\n        modifier : numeric\n            The AOI modifier.\n        \"\"\"\n        kwargs = _build_kwargs(['K', 'L', 'n'], self.module_parameters)\n\n        return physicaliam(aoi, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef calcparams_desoto(self, effective_irradiance, temp_cell, **kwargs):\n\n        kwargs = _build_kwargs(['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',\n                                'R_s', 'alpha_sc', 'EgRef', 'dEgdT',\n                                'irrad_ref', 'temp_ref'],\n                               self.module_parameters)\n\n        return calcparams_desoto(effective_irradiance, temp_cell, **kwargs)", "response": "Calculate the parameters of the module currents and resistances of the entry in the entry set."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the parameters of the CIC and the currents and resistances of the module.", "response": "def calcparams_cec(self, effective_irradiance, temp_cell, **kwargs):\n        \"\"\"\n        Use the :py:func:`calcparams_cec` function, the input\n        parameters and ``self.module_parameters`` to calculate the\n        module currents and resistances.\n\n        Parameters\n        ----------\n        effective_irradiance : numeric\n            The irradiance (W/m2) that is converted to photocurrent.\n\n        temp_cell : float or Series\n            The average cell temperature of cells within a module in C.\n\n        **kwargs\n            See pvsystem.calcparams_cec for details\n\n        Returns\n        -------\n        See pvsystem.calcparams_cec for details\n        \"\"\"\n\n        kwargs = _build_kwargs(['a_ref', 'I_L_ref', 'I_o_ref', 'R_sh_ref',\n                                'R_s', 'alpha_sc', 'Adjust', 'EgRef', 'dEgdT',\n                                'irrad_ref', 'temp_ref'],\n                               self.module_parameters)\n\n        return calcparams_cec(effective_irradiance, temp_cell, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calcparams_pvsyst(self, effective_irradiance, temp_cell):\n\n        kwargs = _build_kwargs(['gamma_ref', 'mu_gamma', 'I_L_ref', 'I_o_ref',\n                                'R_sh_ref', 'R_sh_0', 'R_sh_exp',\n                                'R_s', 'alpha_sc', 'EgRef',\n                                'irrad_ref', 'temp_ref',\n                                'cells_in_series'],\n                               self.module_parameters)\n\n        return calcparams_pvsyst(effective_irradiance, temp_cell, **kwargs)", "response": "Calculate the parameters of the PVsyst of the current entry in a module."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sapm(self, effective_irradiance, temp_cell, **kwargs):\n        return sapm(effective_irradiance, temp_cell, self.module_parameters)", "response": "Calculates the SAPM of a specific entry in a set of irradiance and temperature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating the SAPM cell temperature for a given irradiance and temperature.", "response": "def sapm_celltemp(self, irrad, wind, temp):\n        \"\"\"Uses :py:func:`sapm_celltemp` to calculate module and cell\n        temperatures based on ``self.racking_model`` and\n        the input parameters.\n\n        Parameters\n        ----------\n        See pvsystem.sapm_celltemp for details\n\n        Returns\n        -------\n        See pvsystem.sapm_celltemp for details\n        \"\"\"\n        return sapm_celltemp(irrad, wind, temp, self.racking_model)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef sapm_effective_irradiance(self, poa_direct, poa_diffuse,\n                                  airmass_absolute, aoi,\n                                  reference_irradiance=1000):\n        \"\"\"\n        Use the :py:func:`sapm_effective_irradiance` function, the input\n        parameters, and ``self.module_parameters`` to calculate\n        effective irradiance.\n\n        Parameters\n        ----------\n        poa_direct : numeric\n            The direct irradiance incident upon the module.\n\n        poa_diffuse : numeric\n            The diffuse irradiance incident on module.\n\n        airmass_absolute : numeric\n            Absolute airmass.\n\n        aoi : numeric\n            Angle of incidence in degrees.\n\n        reference_irradiance : numeric, default 1000\n            Reference irradiance by which to divide the input irradiance.\n\n        Returns\n        -------\n        effective_irradiance : numeric\n            The SAPM effective irradiance.\n        \"\"\"\n        return sapm_effective_irradiance(\n            poa_direct, poa_diffuse, airmass_absolute, aoi,\n            self.module_parameters, reference_irradiance=reference_irradiance)", "response": "Calculates the SAPM effective irradiance for the given airmass."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncalculate the PVsyst cell temperature for a given global pressure and temperature.", "response": "def pvsyst_celltemp(self, poa_global, temp_air, wind_speed=1.0):\n        \"\"\"Uses :py:func:`pvsyst_celltemp` to calculate module temperatures\n        based on ``self.racking_model`` and the input parameters.\n\n        Parameters\n        ----------\n        See pvsystem.pvsyst_celltemp for details\n\n        Returns\n        -------\n        See pvsystem.pvsyst_celltemp for details\n        \"\"\"\n        kwargs = _build_kwargs(['eta_m', 'alpha_absorption'],\n                               self.module_parameters)\n        return pvsyst_celltemp(poa_global, temp_air, wind_speed,\n                               model_params=self.racking_model, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef first_solar_spectral_loss(self, pw, airmass_absolute):\n\n        \"\"\"\n        Use the :py:func:`first_solar_spectral_correction` function to\n        calculate the spectral loss modifier. The model coefficients are\n        specific to the module's cell type, and are determined by searching\n        for one of the following keys in self.module_parameters (in order):\n            'first_solar_spectral_coefficients' (user-supplied coefficients)\n            'Technology' - a string describing the cell type, can be read from\n            the CEC module parameter database\n            'Material' - a string describing the cell type, can be read from\n            the Sandia module database.\n\n        Parameters\n        ----------\n        pw : array-like\n            atmospheric precipitable water (cm).\n\n        airmass_absolute : array-like\n            absolute (pressure corrected) airmass.\n\n        Returns\n        -------\n        modifier: array-like\n            spectral mismatch factor (unitless) which can be multiplied\n            with broadband irradiance reaching a module's cells to estimate\n            effective irradiance, i.e., the irradiance that is converted to\n            electrical current.\n        \"\"\"\n\n        if 'first_solar_spectral_coefficients' in \\\n                self.module_parameters.keys():\n            coefficients = \\\n                   self.module_parameters['first_solar_spectral_coefficients']\n            module_type = None\n        else:\n            module_type = self._infer_cell_type()\n            coefficients = None\n\n        return atmosphere.first_solar_spectral_correction(pw,\n                                                          airmass_absolute,\n                                                          module_type,\n                                                          coefficients)", "response": "Calculates the spectral loss modifier of the CEC model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _infer_cell_type(self):\n\n        \"\"\"\n        Examines module_parameters and maps the Technology key for the CEC\n        database and the Material key for the Sandia database to a common\n        list of strings for cell type.\n\n        Returns\n        -------\n        cell_type: str\n\n        \"\"\"\n\n        _cell_type_dict = {'Multi-c-Si': 'multisi',\n                           'Mono-c-Si': 'monosi',\n                           'Thin Film': 'cigs',\n                           'a-Si/nc': 'asi',\n                           'CIS': 'cigs',\n                           'CIGS': 'cigs',\n                           '1-a-Si': 'asi',\n                           'CdTe': 'cdte',\n                           'a-Si': 'asi',\n                           '2-a-Si': None,\n                           '3-a-Si': None,\n                           'HIT-Si': 'monosi',\n                           'mc-Si': 'multisi',\n                           'c-Si': 'multisi',\n                           'Si-Film': 'asi',\n                           'EFG mc-Si': 'multisi',\n                           'GaAs': None,\n                           'a-Si / mono-Si': 'monosi'}\n\n        if 'Technology' in self.module_parameters.keys():\n            # CEC module parameter set\n            cell_type = _cell_type_dict[self.module_parameters['Technology']]\n        elif 'Material' in self.module_parameters.keys():\n            # Sandia module parameter set\n            cell_type = _cell_type_dict[self.module_parameters['Material']]\n        else:\n            cell_type = None\n\n        return cell_type", "response": "Infer the cell type from the module parameters."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap around the Singlediode class method that returns a new object with the same attributes as the passed in object.", "response": "def singlediode(self, photocurrent, saturation_current,\n                    resistance_series, resistance_shunt, nNsVth,\n                    ivcurve_pnts=None):\n        \"\"\"Wrapper around the :py:func:`singlediode` function.\n\n        Parameters\n        ----------\n        See pvsystem.singlediode for details\n\n        Returns\n        -------\n        See pvsystem.singlediode for details\n        \"\"\"\n        return singlediode(photocurrent, saturation_current,\n                           resistance_series, resistance_shunt, nNsVth,\n                           ivcurve_pnts=ivcurve_pnts)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nwraps around the i_from_v method that returns the i_from_v object for the given resistance_shunt resistance_series nNsVth voltage and saturation_current.", "response": "def i_from_v(self, resistance_shunt, resistance_series, nNsVth, voltage,\n                 saturation_current, photocurrent):\n        \"\"\"Wrapper around the :py:func:`i_from_v` function.\n\n        Parameters\n        ----------\n        See pvsystem.i_from_v for details\n\n        Returns\n        -------\n        See pvsystem.i_from_v for details\n        \"\"\"\n        return i_from_v(resistance_shunt, resistance_series, nNsVth, voltage,\n                        saturation_current, photocurrent)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef scale_voltage_current_power(self, data):\n\n        return scale_voltage_current_power(data,\n                                           voltage=self.modules_per_string,\n                                           current=self.strings_per_inverter)", "response": "Scales the voltage current and power of the DataFrames\n            by the number of modules per string and the power of the DataFrames\n            by the number of inverters per module."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pvwatts_dc(self, g_poa_effective, temp_cell):\n        kwargs = _build_kwargs(['temp_ref'], self.module_parameters)\n\n        return pvwatts_dc(g_poa_effective, temp_cell,\n                          self.module_parameters['pdc0'],\n                          self.module_parameters['gamma_pdc'],\n                          **kwargs)", "response": "Calculates DC power according to the PVWatts model using\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalculates the AC power according to the PVWatts model using the given PDCs.", "response": "def pvwatts_ac(self, pdc):\n        \"\"\"\n        Calculates AC power according to the PVWatts model using\n        :py:func:`pvwatts_ac`, `self.module_parameters['pdc0']`, and\n        `eta_inv_nom=self.inverter_parameters['eta_inv_nom']`.\n\n        See :py:func:`pvwatts_ac` for details.\n        \"\"\"\n        kwargs = _build_kwargs(['eta_inv_nom', 'eta_inv_ref'],\n                               self.inverter_parameters)\n\n        return pvwatts_ac(pdc, self.module_parameters['pdc0'], **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a LocalizedPVSystem object using this object s object latitude and longitude and optional location kwargs.", "response": "def localize(self, location=None, latitude=None, longitude=None,\n                 **kwargs):\n        \"\"\"Creates a LocalizedPVSystem object using this object\n        and location data. Must supply either location object or\n        latitude, longitude, and any location kwargs\n\n        Parameters\n        ----------\n        location : None or Location, default None\n        latitude : None or float, default None\n        longitude : None or float, default None\n        **kwargs : see Location\n\n        Returns\n        -------\n        localized_system : LocalizedPVSystem\n        \"\"\"\n\n        if location is None:\n            location = Location(latitude, longitude, **kwargs)\n\n        return LocalizedPVSystem(pvsystem=self, location=location)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the project root directory.", "response": "def get_root():\n    \"\"\"Get the project root directory.\n\n    We require that all commands are run from the project root, i.e. the\n    directory that contains setup.py, setup.cfg, and versioneer.py .\n    \"\"\"\n    root = os.path.realpath(os.path.abspath(os.getcwd()))\n    setup_py = os.path.join(root, \"setup.py\")\n    versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        # allow 'python path/to/setup.py COMMAND'\n        root = os.path.dirname(os.path.realpath(os.path.abspath(sys.argv[0])))\n        setup_py = os.path.join(root, \"setup.py\")\n        versioneer_py = os.path.join(root, \"versioneer.py\")\n    if not (os.path.exists(setup_py) or os.path.exists(versioneer_py)):\n        err = (\"Versioneer was unable to run the project root directory. \"\n               \"Versioneer requires setup.py to be executed from \"\n               \"its immediate directory (like 'python setup.py COMMAND'), \"\n               \"or in a way that lets it use sys.argv[0] to find the root \"\n               \"(like 'python path/to/setup.py COMMAND').\")\n        raise VersioneerBadRootError(err)\n    try:\n        # Certain runtime workflows (setup.py install/develop in a setuptools\n        # tree) execute all dependencies in a single python process, so\n        # \"versioneer\" may be imported multiple times, and python's shared\n        # module-import table will cache the first one. So we can't use\n        # os.path.dirname(__file__), as that will find whichever\n        # versioneer.py was first imported, even in later projects.\n        me = os.path.realpath(os.path.abspath(__file__))\n        if os.path.splitext(me)[0] != os.path.splitext(versioneer_py)[0]:\n            print(\"Warning: build in %s is using versioneer.py from %s\"\n                  % (os.path.dirname(me), versioneer_py))\n    except NameError:\n        pass\n    return root"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef git_pieces_from_vcs(tag_prefix, root, verbose, run_command=run_command):\n    if not os.path.exists(os.path.join(root, \".git\")):\n        if verbose:\n            print(\"no .git in %s\" % root)\n        raise NotThisMethod(\"no .git directory\")\n\n    GITS = [\"git\"]\n    if sys.platform == \"win32\":\n        GITS = [\"git.cmd\", \"git.exe\"]\n    # if there is a tag matching tag_prefix, this yields TAG-NUM-gHEX[-dirty]\n    # if there isn't one, this yields HEX[-dirty] (no NUM)\n    describe_out = run_command(GITS, [\"describe\", \"--tags\", \"--dirty\",\n                                      \"--always\", \"--long\",\n                                      \"--match\", \"%s*\" % tag_prefix],\n                               cwd=root)\n    # --long was added in git-1.5.5\n    if describe_out is None:\n        raise NotThisMethod(\"'git describe' failed\")\n    describe_out = describe_out.strip()\n    full_out = run_command(GITS, [\"rev-parse\", \"HEAD\"], cwd=root)\n    if full_out is None:\n        raise NotThisMethod(\"'git rev-parse' failed\")\n    full_out = full_out.strip()\n\n    pieces = {}\n    pieces[\"long\"] = full_out\n    pieces[\"short\"] = full_out[:7]  # maybe improved later\n    pieces[\"error\"] = None\n\n    # parse describe_out. It will be like TAG-NUM-gHEX[-dirty] or HEX[-dirty]\n    # TAG might have hyphens.\n    git_describe = describe_out\n\n    # look for -dirty suffix\n    dirty = git_describe.endswith(\"-dirty\")\n    pieces[\"dirty\"] = dirty\n    if dirty:\n        git_describe = git_describe[:git_describe.rindex(\"-dirty\")]\n\n    # now we have TAG-NUM-gHEX or HEX\n\n    if \"-\" in git_describe:\n        # TAG-NUM-gHEX\n        mo = re.search(r'^(.+)-(\\d+)-g([0-9a-f]+)$', git_describe)\n        if not mo:\n            # unparseable. Maybe git-describe is misbehaving?\n            pieces[\"error\"] = (\"unable to parse git-describe output: '%s'\"\n                               % describe_out)\n            return pieces\n\n        # tag\n        full_tag = mo.group(1)\n        if not full_tag.startswith(tag_prefix):\n            if verbose:\n                fmt = \"tag '%s' doesn't start with prefix '%s'\"\n                print(fmt % (full_tag, tag_prefix))\n            pieces[\"error\"] = (\"tag '%s' doesn't start with prefix '%s'\"\n                               % (full_tag, tag_prefix))\n            return pieces\n        pieces[\"closest-tag\"] = full_tag[len(tag_prefix):]\n\n        # distance: number of commits since tag\n        pieces[\"distance\"] = int(mo.group(2))\n\n        # commit: short hex revision ID\n        pieces[\"short\"] = mo.group(3)\n\n    else:\n        # HEX: no tags\n        pieces[\"closest-tag\"] = None\n        count_out = run_command(GITS, [\"rev-list\", \"HEAD\", \"--count\"],\n                                cwd=root)\n        pieces[\"distance\"] = int(count_out)  # total number of commits\n\n    return pieces", "response": "Get version from git describe and _version. py."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef versions_from_parentdir(parentdir_prefix, root, verbose):\n    dirname = os.path.basename(root)\n    if not dirname.startswith(parentdir_prefix):\n        if verbose:\n            print(\"guessing rootdir is '%s', but '%s' doesn't start with \"\n                  \"prefix '%s'\" % (root, dirname, parentdir_prefix))\n        raise NotThisMethod(\"rootdir doesn't start with parentdir_prefix\")\n    return {\"version\": dirname[len(parentdir_prefix):],\n            \"full-revisionid\": None,\n            \"dirty\": False, \"error\": None}", "response": "Try to determine the version from the parent directory name."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_versions(verbose=False):\n    if \"versioneer\" in sys.modules:\n        # see the discussion in cmdclass.py:get_cmdclass()\n        del sys.modules[\"versioneer\"]\n\n    root = get_root()\n    cfg = get_config_from_root(root)\n\n    assert cfg.VCS is not None, \"please set [versioneer]VCS= in setup.cfg\"\n    handlers = HANDLERS.get(cfg.VCS)\n    assert handlers, \"unrecognized VCS '%s'\" % cfg.VCS\n    verbose = verbose or cfg.verbose\n    assert cfg.versionfile_source is not None, \\\n        \"please set versioneer.versionfile_source\"\n    assert cfg.tag_prefix is not None, \"please set versioneer.tag_prefix\"\n\n    versionfile_abs = os.path.join(root, cfg.versionfile_source)\n\n    # extract version from first of: _version.py, VCS command (e.g. 'git\n    # describe'), parentdir. This is meant to work for developers using a\n    # source checkout, for users of a tarball created by 'setup.py sdist',\n    # and for users of a tarball/zipball created by 'git archive' or github's\n    # download-from-tag feature or the equivalent in other VCSes.\n\n    get_keywords_f = handlers.get(\"get_keywords\")\n    from_keywords_f = handlers.get(\"keywords\")\n    if get_keywords_f and from_keywords_f:\n        try:\n            keywords = get_keywords_f(versionfile_abs)\n            ver = from_keywords_f(keywords, cfg.tag_prefix, verbose)\n            if verbose:\n                print(\"got version from expanded keyword %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        ver = versions_from_file(versionfile_abs)\n        if verbose:\n            print(\"got version from file %s %s\" % (versionfile_abs, ver))\n        return ver\n    except NotThisMethod:\n        pass\n\n    from_vcs_f = handlers.get(\"pieces_from_vcs\")\n    if from_vcs_f:\n        try:\n            pieces = from_vcs_f(cfg.tag_prefix, root, verbose)\n            ver = render(pieces, cfg.style)\n            if verbose:\n                print(\"got version from VCS %s\" % ver)\n            return ver\n        except NotThisMethod:\n            pass\n\n    try:\n        if cfg.parentdir_prefix:\n            ver = versions_from_parentdir(cfg.parentdir_prefix, root, verbose)\n            if verbose:\n                print(\"got version from parentdir %s\" % ver)\n            return ver\n    except NotThisMethod:\n        pass\n\n    if verbose:\n        print(\"unable to compute version\")\n\n    return {\"version\": \"0+unknown\", \"full-revisionid\": None,\n            \"dirty\": None, \"error\": \"unable to compute version\"}", "response": "Get the project version from what source is available."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_cmdclass():\n    if \"versioneer\" in sys.modules:\n        del sys.modules[\"versioneer\"]\n        # this fixes the \"python setup.py develop\" case (also 'install' and\n        # 'easy_install .'), in which subdependencies of the main project are\n        # built (using setup.py bdist_egg) in the same python process. Assume\n        # a main project A and a dependency B, which use different versions\n        # of Versioneer. A's setup.py imports A's Versioneer, leaving it in\n        # sys.modules by the time B's setup.py is executed, causing B to run\n        # with the wrong versioneer. Setuptools wraps the sub-dep builds in a\n        # sandbox that restores sys.modules to it's pre-build state, so the\n        # parent is protected against the child's \"import versioneer\". By\n        # removing ourselves from sys.modules here, before the child build\n        # happens, we protect the child from the parent's versioneer too.\n        # Also see https://github.com/warner/python-versioneer/issues/52\n\n    cmds = {}\n\n    # we add \"version\" to both distutils and setuptools\n    from distutils.core import Command\n\n    class cmd_version(Command):\n        description = \"report generated version string\"\n        user_options = []\n        boolean_options = []\n\n        def initialize_options(self):\n            pass\n\n        def finalize_options(self):\n            pass\n\n        def run(self):\n            vers = get_versions(verbose=True)\n            print(\"Version: %s\" % vers[\"version\"])\n            print(\" full-revisionid: %s\" % vers.get(\"full-revisionid\"))\n            print(\" dirty: %s\" % vers.get(\"dirty\"))\n            if vers[\"error\"]:\n                print(\" error: %s\" % vers[\"error\"])\n    cmds[\"version\"] = cmd_version\n\n    # we override \"build_py\" in both distutils and setuptools\n    #\n    # most invocation pathways end up running build_py:\n    #  distutils/build -> build_py\n    #  distutils/install -> distutils/build ->..\n    #  setuptools/bdist_wheel -> distutils/install ->..\n    #  setuptools/bdist_egg -> distutils/install_lib -> build_py\n    #  setuptools/install -> bdist_egg ->..\n    #  setuptools/develop -> ?\n\n    # we override different \"build_py\" commands for both environments\n    if \"setuptools\" in sys.modules:\n        from setuptools.command.build_py import build_py as _build_py\n    else:\n        from distutils.command.build_py import build_py as _build_py\n\n    class cmd_build_py(_build_py):\n        def run(self):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            versions = get_versions()\n            _build_py.run(self)\n            # now locate _version.py in the new build/ directory and replace\n            # it with an updated value\n            if cfg.versionfile_build:\n                target_versionfile = os.path.join(self.build_lib,\n                                                  cfg.versionfile_build)\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n    cmds[\"build_py\"] = cmd_build_py\n\n    if \"cx_Freeze\" in sys.modules:  # cx_freeze enabled?\n        from cx_Freeze.dist import build_exe as _build_exe\n\n        class cmd_build_exe(_build_exe):\n            def run(self):\n                root = get_root()\n                cfg = get_config_from_root(root)\n                versions = get_versions()\n                target_versionfile = cfg.versionfile_source\n                print(\"UPDATING %s\" % target_versionfile)\n                write_to_version_file(target_versionfile, versions)\n\n                _build_exe.run(self)\n                os.unlink(target_versionfile)\n                with open(cfg.versionfile_source, \"w\") as f:\n                    LONG = LONG_VERSION_PY[cfg.VCS]\n                    f.write(LONG %\n                            {\"DOLLAR\": \"$\",\n                             \"STYLE\": cfg.style,\n                             \"TAG_PREFIX\": cfg.tag_prefix,\n                             \"PARENTDIR_PREFIX\": cfg.parentdir_prefix,\n                             \"VERSIONFILE_SOURCE\": cfg.versionfile_source,\n                             })\n        cmds[\"build_exe\"] = cmd_build_exe\n        del cmds[\"build_py\"]\n\n    # we override different \"sdist\" commands for both environments\n    if \"setuptools\" in sys.modules:\n        from setuptools.command.sdist import sdist as _sdist\n    else:\n        from distutils.command.sdist import sdist as _sdist\n\n    class cmd_sdist(_sdist):\n        def run(self):\n            versions = get_versions()\n            self._versioneer_generated_versions = versions\n            # unless we update this, the command will keep using the old\n            # version\n            self.distribution.metadata.version = versions[\"version\"]\n            return _sdist.run(self)\n\n        def make_release_tree(self, base_dir, files):\n            root = get_root()\n            cfg = get_config_from_root(root)\n            _sdist.make_release_tree(self, base_dir, files)\n            # now locate _version.py in the new base_dir directory\n            # (remembering that it may be a hardlink) and replace it with an\n            # updated value\n            target_versionfile = os.path.join(base_dir, cfg.versionfile_source)\n            print(\"UPDATING %s\" % target_versionfile)\n            write_to_version_file(target_versionfile,\n                                  self._versioneer_generated_versions)\n    cmds[\"sdist\"] = cmd_sdist\n\n    return cmds", "response": "Get the custom setuptools and distutils subclasses used by Versioneer."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_tmy3(filename=None, coerce_year=None, recolumn=True):\n    '''\n    Read a TMY3 file in to a pandas dataframe.\n\n    Note that values contained in the metadata dictionary are unchanged\n    from the TMY3 file (i.e. units are retained). In the case of any\n    discrepencies between this documentation and the TMY3 User's Manual\n    [1], the TMY3 User's Manual takes precedence.\n\n    The TMY3 files were updated in Jan. 2015. This function requires the\n    use of the updated files.\n\n    Parameters\n    ----------\n    filename : None or string, default None\n        If None, attempts to use a Tkinter file browser. A string can be\n        a relative file path, absolute file path, or url.\n\n    coerce_year : None or int, default None\n        If supplied, the year of the data will be set to this value.\n\n    recolumn : bool, default True\n        If True, apply standard names to TMY3 columns. Typically this\n        results in stripping the units from the column name.\n\n    Returns\n    -------\n    Tuple of the form (data, metadata).\n\n    data : DataFrame\n        A pandas dataframe with the columns described in the table\n        below. For more detailed descriptions of each component, please\n        consult the TMY3 User's Manual ([1]), especially tables 1-1\n        through 1-6.\n\n    metadata : dict\n        The site metadata available in the file.\n\n    Notes\n    -----\n\n    The returned structures have the following fields.\n\n    ===============   ======  ===================\n    key               format  description\n    ===============   ======  ===================\n    altitude          Float   site elevation\n    latitude          Float   site latitudeitude\n    longitude         Float   site longitudeitude\n    Name              String  site name\n    State             String  state\n    TZ                Float   UTC offset\n    USAF              Int     USAF identifier\n    ===============   ======  ===================\n\n    =============================       ======================================================================================================================================================\n    TMYData field                       description\n    =============================       ======================================================================================================================================================\n    TMYData.Index                       A pandas datetime index. NOTE, the index is currently timezone unaware, and times are set to local standard time (daylight savings is not included)\n    TMYData.ETR                         Extraterrestrial horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    TMYData.ETRN                        Extraterrestrial normal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    TMYData.GHI                         Direct and diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    TMYData.GHISource                   See [1], Table 1-4\n    TMYData.GHIUncertainty              Uncertainty based on random and bias error estimates                        see [2]\n    TMYData.DNI                         Amount of direct normal radiation (modeled) recv'd during 60 mintues prior to timestamp, Wh/m^2\n    TMYData.DNISource                   See [1], Table 1-4\n    TMYData.DNIUncertainty              Uncertainty based on random and bias error estimates                        see [2]\n    TMYData.DHI                         Amount of diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    TMYData.DHISource                   See [1], Table 1-4\n    TMYData.DHIUncertainty              Uncertainty based on random and bias error estimates                        see [2]\n    TMYData.GHillum                     Avg. total horizontal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    TMYData.GHillumSource               See [1], Table 1-4\n    TMYData.GHillumUncertainty          Uncertainty based on random and bias error estimates                        see [2]\n    TMYData.DNillum                     Avg. direct normal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    TMYData.DNillumSource               See [1], Table 1-4\n    TMYData.DNillumUncertainty          Uncertainty based on random and bias error estimates                        see [2]\n    TMYData.DHillum                     Avg. horizontal diffuse illuminance recv'd during the 60 minutes prior to timestamp, lx\n    TMYData.DHillumSource               See [1], Table 1-4\n    TMYData.DHillumUncertainty          Uncertainty based on random and bias error estimates                        see [2]\n    TMYData.Zenithlum                   Avg. luminance at the sky's zenith during the 60 minutes prior to timestamp, cd/m^2\n    TMYData.ZenithlumSource             See [1], Table 1-4\n    TMYData.ZenithlumUncertainty        Uncertainty based on random and bias error estimates                        see [1] section 2.10\n    TMYData.TotCld                      Amount of sky dome covered by clouds or obscuring phenonema at time stamp, tenths of sky\n    TMYData.TotCldSource                See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.TotCldUnertainty            See [1], Table 1-6\n    TMYData.OpqCld                      Amount of sky dome covered by clouds or obscuring phenonema that prevent observing the sky at time stamp, tenths of sky\n    TMYData.OpqCldSource                See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.OpqCldUncertainty           See [1], Table 1-6\n    TMYData.DryBulb                     Dry bulb temperature at the time indicated, deg C\n    TMYData.DryBulbSource               See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.DryBulbUncertainty          See [1], Table 1-6\n    TMYData.DewPoint                    Dew-point temperature at the time indicated, deg C\n    TMYData.DewPointSource              See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.DewPointUncertainty         See [1], Table 1-6\n    TMYData.RHum                        Relatitudeive humidity at the time indicated, percent\n    TMYData.RHumSource                  See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.RHumUncertainty             See [1], Table 1-6\n    TMYData.Pressure                    Station pressure at the time indicated, 1 mbar\n    TMYData.PressureSource              See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.PressureUncertainty         See [1], Table 1-6\n    TMYData.Wdir                        Wind direction at time indicated, degrees from north (360 = north; 0 = undefined,calm)\n    TMYData.WdirSource                  See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.WdirUncertainty             See [1], Table 1-6\n    TMYData.Wspd                        Wind speed at the time indicated, meter/second\n    TMYData.WspdSource                  See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.WspdUncertainty             See [1], Table 1-6\n    TMYData.Hvis                        Distance to discernable remote objects at time indicated (7777=unlimited), meter\n    TMYData.HvisSource                  See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.HvisUncertainty             See [1], Table 1-6\n    TMYData.CeilHgt                     Height of cloud base above local terrain (7777=unlimited), meter\n    TMYData.CeilHgtSource               See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.CeilHgtUncertainty          See [1], Table 1-6\n    TMYData.Pwat                        Total precipitable water contained in a column of unit cross section from earth to top of atmosphere, cm\n    TMYData.PwatSource                  See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.PwatUncertainty             See [1], Table 1-6\n    TMYData.AOD                         The broadband aerosol optical depth per unit of air mass due to extinction by aerosol component of atmosphere, unitless\n    TMYData.AODSource                   See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.AODUncertainty              See [1], Table 1-6\n    TMYData.Alb                         The ratio of reflected solar irradiance to global horizontal irradiance, unitless\n    TMYData.AlbSource                   See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.AlbUncertainty              See [1], Table 1-6\n    TMYData.Lprecipdepth                The amount of liquid precipitation observed at indicated time for the period indicated in the liquid precipitation quantity field, millimeter\n    TMYData.Lprecipquantity             The period of accumulatitudeion for the liquid precipitation depth field, hour\n    TMYData.LprecipSource               See [1], Table 1-5, 8760x1 cell array of strings\n    TMYData.LprecipUncertainty          See [1], Table 1-6\n    TMYData.PresWth                     Present weather code, see [2].\n    TMYData.PresWthSource               Present weather code source, see [2].\n    TMYData.PresWthUncertainty          Present weather code uncertainty, see [2].\n    =============================       ======================================================================================================================================================\n\n    References\n    ----------\n\n    [1] Wilcox, S and Marion, W. \"Users Manual for TMY3 Data Sets\".\n    NREL/TP-581-43156, Revised May 2008.\n\n    [2] Wilcox, S. (2007). National Solar Radiation Database 1991 2005\n    Update: Users Manual. 472 pp.; NREL Report No. TP-581-41364.\n    '''\n\n    if filename is None:\n        try:\n            filename = _interactive_load()\n        except ImportError:\n            raise ImportError('Interactive load failed. Tkinter not supported '\n                              'on this system. Try installing X-Quartz and '\n                              'reloading')\n\n    head = ['USAF', 'Name', 'State', 'TZ', 'latitude', 'longitude', 'altitude']\n\n    if filename.startswith('http'):\n        request = Request(filename, headers={'User-Agent': (\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) '\n            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 '\n            'Safari/537.36')})\n        response = urlopen(request)\n        csvdata = io.StringIO(response.read().decode(errors='ignore'))\n    else:\n        # assume it's accessible via the file system\n        csvdata = open(filename, 'r')\n\n    # read in file metadata, advance buffer to second line\n    firstline = csvdata.readline()\n    if 'Request Rejected' in firstline:\n        raise IOError('Remote server rejected TMY file request')\n\n    meta = dict(zip(head, firstline.rstrip('\\n').split(\",\")))\n\n    # convert metadata strings to numeric types\n    meta['altitude'] = float(meta['altitude'])\n    meta['latitude'] = float(meta['latitude'])\n    meta['longitude'] = float(meta['longitude'])\n    meta['TZ'] = float(meta['TZ'])\n    meta['USAF'] = int(meta['USAF'])\n\n    # use pandas to read the csv file/stringio buffer\n    # header is actually the second line in file, but tell pandas to look for\n    # header information on the 1st line (0 indexing) because we've already\n    # advanced past the true first line with the readline call above.\n    data = pd.read_csv(\n        csvdata, header=0,\n        parse_dates={'datetime': ['Date (MM/DD/YYYY)', 'Time (HH:MM)']},\n        date_parser=lambda *x: _parsedate(*x, year=coerce_year),\n        index_col='datetime')\n\n    if recolumn:\n        data = _recolumn(data)  # rename to standard column names\n\n    data = data.tz_localize(int(meta['TZ'] * 3600))\n\n    return data, meta", "response": "Read a TMY3 file into a pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _recolumn(tmy3_dataframe):\n    # paste in the header as one long line\n    raw_columns = 'ETR (W/m^2),ETRN (W/m^2),GHI (W/m^2),GHI source,GHI uncert (%),DNI (W/m^2),DNI source,DNI uncert (%),DHI (W/m^2),DHI source,DHI uncert (%),GH illum (lx),GH illum source,Global illum uncert (%),DN illum (lx),DN illum source,DN illum uncert (%),DH illum (lx),DH illum source,DH illum uncert (%),Zenith lum (cd/m^2),Zenith lum source,Zenith lum uncert (%),TotCld (tenths),TotCld source,TotCld uncert (code),OpqCld (tenths),OpqCld source,OpqCld uncert (code),Dry-bulb (C),Dry-bulb source,Dry-bulb uncert (code),Dew-point (C),Dew-point source,Dew-point uncert (code),RHum (%),RHum source,RHum uncert (code),Pressure (mbar),Pressure source,Pressure uncert (code),Wdir (degrees),Wdir source,Wdir uncert (code),Wspd (m/s),Wspd source,Wspd uncert (code),Hvis (m),Hvis source,Hvis uncert (code),CeilHgt (m),CeilHgt source,CeilHgt uncert (code),Pwat (cm),Pwat source,Pwat uncert (code),AOD (unitless),AOD source,AOD uncert (code),Alb (unitless),Alb source,Alb uncert (code),Lprecip depth (mm),Lprecip quantity (hr),Lprecip source,Lprecip uncert (code),PresWth (METAR code),PresWth source,PresWth uncert (code)'  # noqa: E501\n\n    new_columns = [\n        'ETR', 'ETRN', 'GHI', 'GHISource', 'GHIUncertainty',\n        'DNI', 'DNISource', 'DNIUncertainty', 'DHI', 'DHISource',\n        'DHIUncertainty', 'GHillum', 'GHillumSource', 'GHillumUncertainty',\n        'DNillum', 'DNillumSource', 'DNillumUncertainty', 'DHillum',\n        'DHillumSource', 'DHillumUncertainty', 'Zenithlum',\n        'ZenithlumSource', 'ZenithlumUncertainty', 'TotCld', 'TotCldSource',\n        'TotCldUnertainty', 'OpqCld', 'OpqCldSource', 'OpqCldUncertainty',\n        'DryBulb', 'DryBulbSource', 'DryBulbUncertainty', 'DewPoint',\n        'DewPointSource', 'DewPointUncertainty', 'RHum', 'RHumSource',\n        'RHumUncertainty', 'Pressure', 'PressureSource',\n        'PressureUncertainty', 'Wdir', 'WdirSource', 'WdirUncertainty',\n        'Wspd', 'WspdSource', 'WspdUncertainty', 'Hvis', 'HvisSource',\n        'HvisUncertainty', 'CeilHgt', 'CeilHgtSource', 'CeilHgtUncertainty',\n        'Pwat', 'PwatSource', 'PwatUncertainty', 'AOD', 'AODSource',\n        'AODUncertainty', 'Alb', 'AlbSource', 'AlbUncertainty',\n        'Lprecipdepth', 'Lprecipquantity', 'LprecipSource',\n        'LprecipUncertainty', 'PresWth', 'PresWthSource',\n        'PresWthUncertainty']\n\n    mapping = dict(zip(raw_columns.split(','), new_columns))\n\n    return tmy3_dataframe.rename(columns=mapping)", "response": "Recolumns the columns of the TMY3 DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading a TMY2 file into a DataFrame.", "response": "def read_tmy2(filename):\n    '''\n    Read a TMY2 file in to a DataFrame.\n\n    Note that values contained in the DataFrame are unchanged from the\n    TMY2 file (i.e. units  are retained). Time/Date and location data\n    imported from the TMY2 file have been modified to a \"friendlier\"\n    form conforming to modern conventions (e.g. N latitude is postive, E\n    longitude is positive, the \"24th\" hour of any day is technically the\n    \"0th\" hour of the next day). In the case of any discrepencies\n    between this documentation and the TMY2 User's Manual [1], the TMY2\n    User's Manual takes precedence.\n\n    Parameters\n    ----------\n    filename : None or string\n        If None, attempts to use a Tkinter file browser. A string can be\n        a relative file path, absolute file path, or url.\n\n    Returns\n    -------\n    Tuple of the form (data, metadata).\n\n    data : DataFrame\n        A dataframe with the columns described in the table below. For a\n        more detailed descriptions of each component, please consult the\n        TMY2 User's Manual ([1]), especially tables 3-1 through 3-6, and\n        Appendix B.\n\n    metadata : dict\n        The site metadata available in the file.\n\n    Notes\n    -----\n\n    The returned structures have the following fields.\n\n    =============    ==================================\n    key              description\n    =============    ==================================\n    WBAN             Site identifier code (WBAN number)\n    City             Station name\n    State            Station state 2 letter designator\n    TZ               Hours from Greenwich\n    latitude         Latitude in decimal degrees\n    longitude        Longitude in decimal degrees\n    altitude         Site elevation in meters\n    =============    ==================================\n\n    ============================   ==========================================================================================================================================================================\n    TMYData field                   description\n    ============================   ==========================================================================================================================================================================\n    index                           Pandas timeseries object containing timestamps\n    year\n    month\n    day\n    hour\n    ETR                             Extraterrestrial horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    ETRN                            Extraterrestrial normal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    GHI                             Direct and diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    GHISource                       See [1], Table 3-3\n    GHIUncertainty                  See [1], Table 3-4\n    DNI                             Amount of direct normal radiation (modeled) recv'd during 60 mintues prior to timestamp, Wh/m^2\n    DNISource                       See [1], Table 3-3\n    DNIUncertainty                  See [1], Table 3-4\n    DHI                             Amount of diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    DHISource                       See [1], Table 3-3\n    DHIUncertainty                  See [1], Table 3-4\n    GHillum                         Avg. total horizontal illuminance recv'd during the 60 minutes prior to timestamp, units of 100 lux (e.g. value of 50 = 5000 lux)\n    GHillumSource                   See [1], Table 3-3\n    GHillumUncertainty              See [1], Table 3-4\n    DNillum                         Avg. direct normal illuminance recv'd during the 60 minutes prior to timestamp, units of 100 lux\n    DNillumSource                   See [1], Table 3-3\n    DNillumUncertainty              See [1], Table 3-4\n    DHillum                         Avg. horizontal diffuse illuminance recv'd during the 60 minutes prior to timestamp, units of 100 lux\n    DHillumSource                   See [1], Table 3-3\n    DHillumUncertainty              See [1], Table 3-4\n    Zenithlum                       Avg. luminance at the sky's zenith during the 60 minutes prior to timestamp, units of 10 Cd/m^2 (e.g. value of 700 = 7,000 Cd/m^2)\n    ZenithlumSource                 See [1], Table 3-3\n    ZenithlumUncertainty            See [1], Table 3-4\n    TotCld                          Amount of sky dome covered by clouds or obscuring phenonema at time stamp, tenths of sky\n    TotCldSource                    See [1], Table 3-5, 8760x1 cell array of strings\n    TotCldUnertainty                See [1], Table 3-6\n    OpqCld                          Amount of sky dome covered by clouds or obscuring phenonema that prevent observing the sky at time stamp, tenths of sky\n    OpqCldSource                    See [1], Table 3-5, 8760x1 cell array of strings\n    OpqCldUncertainty               See [1], Table 3-6\n    DryBulb                         Dry bulb temperature at the time indicated, in tenths of degree C (e.g. 352 = 35.2 C).\n    DryBulbSource                   See [1], Table 3-5, 8760x1 cell array of strings\n    DryBulbUncertainty              See [1], Table 3-6\n    DewPoint                        Dew-point temperature at the time indicated, in tenths of degree C (e.g. 76 = 7.6 C).\n    DewPointSource                  See [1], Table 3-5, 8760x1 cell array of strings\n    DewPointUncertainty             See [1], Table 3-6\n    RHum                            Relative humidity at the time indicated, percent\n    RHumSource                      See [1], Table 3-5, 8760x1 cell array of strings\n    RHumUncertainty                 See [1], Table 3-6\n    Pressure                        Station pressure at the time indicated, 1 mbar\n    PressureSource                  See [1], Table 3-5, 8760x1 cell array of strings\n    PressureUncertainty             See [1], Table 3-6\n    Wdir                            Wind direction at time indicated, degrees from east of north (360 = 0 = north; 90 = East; 0 = undefined,calm)\n    WdirSource                      See [1], Table 3-5, 8760x1 cell array of strings\n    WdirUncertainty                 See [1], Table 3-6\n    Wspd                            Wind speed at the time indicated, in tenths of meters/second (e.g. 212 = 21.2 m/s)\n    WspdSource                      See [1], Table 3-5, 8760x1 cell array of strings\n    WspdUncertainty                 See [1], Table 3-6\n    Hvis                            Distance to discernable remote objects at time indicated (7777=unlimited, 9999=missing data), in tenths of kilometers (e.g. 341 = 34.1 km).\n    HvisSource                      See [1], Table 3-5, 8760x1 cell array of strings\n    HvisUncertainty                 See [1], Table 3-6\n    CeilHgt                         Height of cloud base above local terrain (7777=unlimited, 88888=cirroform, 99999=missing data), in meters\n    CeilHgtSource                   See [1], Table 3-5, 8760x1 cell array of strings\n    CeilHgtUncertainty              See [1], Table 3-6\n    Pwat                            Total precipitable water contained in a column of unit cross section from Earth to top of atmosphere, in millimeters\n    PwatSource                      See [1], Table 3-5, 8760x1 cell array of strings\n    PwatUncertainty                 See [1], Table 3-6\n    AOD                             The broadband aerosol optical depth (broadband turbidity) in thousandths on the day indicated (e.g. 114 = 0.114)\n    AODSource                       See [1], Table 3-5, 8760x1 cell array of strings\n    AODUncertainty                  See [1], Table 3-6\n    SnowDepth                       Snow depth in centimeters on the day indicated, (999 = missing data).\n    SnowDepthSource                 See [1], Table 3-5, 8760x1 cell array of strings\n    SnowDepthUncertainty            See [1], Table 3-6\n    LastSnowfall                    Number of days since last snowfall (maximum value of 88, where 88 = 88 or greater days; 99 = missing data)\n    LastSnowfallSource              See [1], Table 3-5, 8760x1 cell array of strings\n    LastSnowfallUncertainty         See [1], Table 3-6\n    PresentWeather                  See [1], Appendix B, an 8760x1 cell array of strings. Each string contains 10 numeric values. The string can be parsed to determine each of 10 observed weather metrics.\n    ============================   ==========================================================================================================================================================================\n\n    References\n    ----------\n\n    [1] Marion, W and Urban, K. \"Wilcox, S and Marion, W. \"User's Manual\n    for TMY2s\". NREL 1995.\n    '''\n\n    if filename is None:\n        try:\n            filename = _interactive_load()\n        except ImportError:\n            raise ImportError('Interactive load failed. Tkinter not supported '\n                              'on this system. Try installing X-Quartz and '\n                              'reloading')\n\n    # paste in the column info as one long line\n    string = '%2d%2d%2d%2d%4d%4d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%4d%1s%1d%2d%1s%1d%2d%1s%1d%4d%1s%1d%4d%1s%1d%3d%1s%1d%4d%1s%1d%3d%1s%1d%3d%1s%1d%4d%1s%1d%5d%1s%1d%10d%3d%1s%1d%3d%1s%1d%3d%1s%1d%2d%1s%1d'  # noqa: E501\n    columns = 'year,month,day,hour,ETR,ETRN,GHI,GHISource,GHIUncertainty,DNI,DNISource,DNIUncertainty,DHI,DHISource,DHIUncertainty,GHillum,GHillumSource,GHillumUncertainty,DNillum,DNillumSource,DNillumUncertainty,DHillum,DHillumSource,DHillumUncertainty,Zenithlum,ZenithlumSource,ZenithlumUncertainty,TotCld,TotCldSource,TotCldUnertainty,OpqCld,OpqCldSource,OpqCldUncertainty,DryBulb,DryBulbSource,DryBulbUncertainty,DewPoint,DewPointSource,DewPointUncertainty,RHum,RHumSource,RHumUncertainty,Pressure,PressureSource,PressureUncertainty,Wdir,WdirSource,WdirUncertainty,Wspd,WspdSource,WspdUncertainty,Hvis,HvisSource,HvisUncertainty,CeilHgt,CeilHgtSource,CeilHgtUncertainty,PresentWeather,Pwat,PwatSource,PwatUncertainty,AOD,AODSource,AODUncertainty,SnowDepth,SnowDepthSource,SnowDepthUncertainty,LastSnowfall,LastSnowfallSource,LastSnowfallUncertaint'  # noqa: E501\n    hdr_columns = 'WBAN,City,State,TZ,latitude,longitude,altitude'\n\n    tmy2, tmy2_meta = _read_tmy2(string, columns, hdr_columns, filename)\n\n    return tmy2, tmy2_meta"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the top line of the tmy2 file and returns a dictionary of metadata contained in the header string.", "response": "def _parsemeta_tmy2(columns, line):\n    \"\"\"Retrieves metadata from the top line of the tmy2 file.\n\n    Parameters\n    ----------\n    columns : string\n        String of column headings in the header\n\n    line : string\n        Header string containing DataFrame\n\n    Returns\n    -------\n    meta : Dict of metadata contained in the header string\n    \"\"\"\n    # Remove duplicated spaces, and read in each element\n    rawmeta = \" \".join(line.split()).split(\" \")\n    meta = rawmeta[:3]  # take the first string entries\n    meta.append(int(rawmeta[3]))\n    # Convert to decimal notation with S negative\n    longitude = (\n        float(rawmeta[5]) + float(rawmeta[6])/60) * (2*(rawmeta[4] == 'N') - 1)\n    # Convert to decimal notation with W negative\n    latitude = (\n        float(rawmeta[8]) + float(rawmeta[9])/60) * (2*(rawmeta[7] == 'E') - 1)\n    meta.append(longitude)\n    meta.append(latitude)\n    meta.append(float(rawmeta[10]))\n\n    # Creates a dictionary of metadata\n    meta_dict = dict(zip(columns.split(','), meta))\n    return meta_dict"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the solar position using the NREL SPA C implementation.", "response": "def spa_c(time, latitude, longitude, pressure=101325, altitude=0,\n          temperature=12, delta_t=67.0,\n          raw_spa_output=False):\n    \"\"\"\n    Calculate the solar position using the C implementation of the NREL\n    SPA code.\n\n    The source files for this code are located in './spa_c_files/', along with\n    a README file which describes how the C code is wrapped in Python.\n    Due to license restrictions, the C code must be downloaded seperately\n    and used in accordance with it's license.\n\n    This function is slower and no more accurate than :py:func:`spa_python`.\n\n    Parameters\n    ----------\n    time : pandas.DatetimeIndex\n        Localized or UTC.\n    latitude : float\n    longitude : float\n    pressure : float, default 101325\n        Pressure in Pascals\n    altitude : float, default 0\n        Elevation above sea level.\n    temperature : float, default 12\n        Temperature in C\n    delta_t : float, default 67.0\n        Difference between terrestrial time and UT1.\n        USNO has previous values and predictions.\n    raw_spa_output : bool, default False\n        If true, returns the raw SPA output.\n\n    Returns\n    -------\n    DataFrame\n        The DataFrame will have the following columns:\n        elevation,\n        azimuth,\n        zenith,\n        apparent_elevation,\n        apparent_zenith.\n\n    References\n    ----------\n    NREL SPA reference: http://rredc.nrel.gov/solar/codesandalgorithms/spa/\n    NREL SPA C files: https://midcdmz.nrel.gov/spa/\n\n    Note: The ``timezone`` field in the SPA C files is replaced with\n    ``time_zone`` to avoid a nameclash with the function ``__timezone`` that is\n    redefined by Python>=3.5. This issue is\n    `Python bug 24643 <https://bugs.python.org/issue24643>`_.\n\n    USNO delta T:\n    http://www.usno.navy.mil/USNO/earth-orientation/eo-products/long-term\n\n    See also\n    --------\n    pyephem, spa_python, ephemeris\n    \"\"\"\n\n    # Added by Rob Andrews (@Calama-Consulting), Calama Consulting, 2014\n    # Edited by Will Holmgren (@wholmgren), University of Arizona, 2014\n    # Edited by Tony Lorenzo (@alorenzo175), University of Arizona, 2015\n\n    try:\n        from pvlib.spa_c_files.spa_py import spa_calc\n    except ImportError:\n        raise ImportError('Could not import built-in SPA calculator. ' +\n                          'You may need to recompile the SPA code.')\n\n    # if localized, convert to UTC. otherwise, assume UTC.\n    try:\n        time_utc = time.tz_convert('UTC')\n    except TypeError:\n        time_utc = time\n\n    spa_out = []\n\n    for date in time_utc:\n        spa_out.append(spa_calc(year=date.year,\n                                month=date.month,\n                                day=date.day,\n                                hour=date.hour,\n                                minute=date.minute,\n                                second=date.second,\n                                time_zone=0,  # date uses utc time\n                                latitude=latitude,\n                                longitude=longitude,\n                                elevation=altitude,\n                                pressure=pressure / 100,\n                                temperature=temperature,\n                                delta_t=delta_t\n                                ))\n\n    spa_df = pd.DataFrame(spa_out, index=time)\n\n    if raw_spa_output:\n        # rename \"time_zone\" from raw output from spa_c_files.spa_py.spa_calc()\n        # to \"timezone\" to match the API of pvlib.solarposition.spa_c()\n        return spa_df.rename(columns={'time_zone': 'timezone'})\n    else:\n        dfout = pd.DataFrame({'azimuth': spa_df['azimuth'],\n                              'apparent_zenith': spa_df['zenith'],\n                              'apparent_elevation': spa_df['e'],\n                              'elevation': spa_df['e0'],\n                              'zenith': 90 - spa_df['e0']})\n\n        return dfout"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _spa_python_import(how):\n\n    from pvlib import spa\n\n    # check to see if the spa module was compiled with numba\n    using_numba = spa.USE_NUMBA\n\n    if how == 'numpy' and using_numba:\n        # the spa module was compiled to numba code, so we need to\n        # reload the module without compiling\n        # the PVLIB_USE_NUMBA env variable is used to tell the module\n        # to not compile with numba\n        warnings.warn('Reloading spa to use numpy')\n        os.environ['PVLIB_USE_NUMBA'] = '0'\n        spa = reload(spa)\n        del os.environ['PVLIB_USE_NUMBA']\n    elif how == 'numba' and not using_numba:\n        # The spa module was not compiled to numba code, so set\n        # PVLIB_USE_NUMBA so it does compile to numba on reload.\n        warnings.warn('Reloading spa to use numba')\n        os.environ['PVLIB_USE_NUMBA'] = '1'\n        spa = reload(spa)\n        del os.environ['PVLIB_USE_NUMBA']\n    elif how != 'numba' and how != 'numpy':\n        raise ValueError(\"how must be either 'numba' or 'numpy'\")\n\n    return spa", "response": "Compile spa. py appropriately"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef spa_python(time, latitude, longitude,\n               altitude=0, pressure=101325, temperature=12, delta_t=67.0,\n               atmos_refract=None, how='numpy', numthreads=4, **kwargs):\n    \"\"\"\n    Calculate the solar position using a python implementation of the\n    NREL SPA algorithm described in [1].\n\n    If numba is installed, the functions can be compiled to\n    machine code and the function can be multithreaded.\n    Without numba, the function evaluates via numpy with\n    a slight performance hit.\n\n    Parameters\n    ----------\n    time : pandas.DatetimeIndex\n        Localized or UTC.\n    latitude : float\n    longitude : float\n    altitude : float, default 0\n    pressure : int or float, optional, default 101325\n        avg. yearly air pressure in Pascals.\n    temperature : int or float, optional, default 12\n        avg. yearly air temperature in degrees C.\n    delta_t : float, optional, default 67.0\n        If delta_t is None, uses spa.calculate_deltat\n        using time.year and time.month from pandas.DatetimeIndex.\n        For most simulations specifing delta_t is sufficient.\n        Difference between terrestrial time and UT1.\n        *Note: delta_t = None will break code using nrel_numba,\n        this will be fixed in a future version.*\n        The USNO has historical and forecasted delta_t [3].\n    atmos_refrac : None or float, optional, default None\n        The approximate atmospheric refraction (in degrees)\n        at sunrise and sunset.\n    how : str, optional, default 'numpy'\n        Options are 'numpy' or 'numba'. If numba >= 0.17.0\n        is installed, how='numba' will compile the spa functions\n        to machine code and run them multithreaded.\n    numthreads : int, optional, default 4\n        Number of threads to use if how == 'numba'.\n\n    Returns\n    -------\n    DataFrame\n        The DataFrame will have the following columns:\n        apparent_zenith (degrees),\n        zenith (degrees),\n        apparent_elevation (degrees),\n        elevation (degrees),\n        azimuth (degrees),\n        equation_of_time (minutes).\n\n\n    References\n    ----------\n    [1] I. Reda and A. Andreas, Solar position algorithm for solar\n    radiation applications. Solar Energy, vol. 76, no. 5, pp. 577-589, 2004.\n\n    [2] I. Reda and A. Andreas, Corrigendum to Solar position algorithm for\n    solar radiation applications. Solar Energy, vol. 81, no. 6, p. 838, 2007.\n\n    [3] USNO delta T:\n    http://www.usno.navy.mil/USNO/earth-orientation/eo-products/long-term\n\n    See also\n    --------\n    pyephem, spa_c, ephemeris\n    \"\"\"\n\n    # Added by Tony Lorenzo (@alorenzo175), University of Arizona, 2015\n\n    lat = latitude\n    lon = longitude\n    elev = altitude\n    pressure = pressure / 100  # pressure must be in millibars for calculation\n\n    atmos_refract = atmos_refract or 0.5667\n\n    if not isinstance(time, pd.DatetimeIndex):\n        try:\n            time = pd.DatetimeIndex(time)\n        except (TypeError, ValueError):\n            time = pd.DatetimeIndex([time, ])\n\n    unixtime = np.array(time.astype(np.int64)/10**9)\n\n    spa = _spa_python_import(how)\n\n    delta_t = delta_t or spa.calculate_deltat(time.year, time.month)\n\n    app_zenith, zenith, app_elevation, elevation, azimuth, eot = \\\n        spa.solar_position(unixtime, lat, lon, elev, pressure, temperature,\n                           delta_t, atmos_refract, numthreads)\n\n    result = pd.DataFrame({'apparent_zenith': app_zenith, 'zenith': zenith,\n                           'apparent_elevation': app_elevation,\n                           'elevation': elevation, 'azimuth': azimuth,\n                           'equation_of_time': eot},\n                          index=time)\n\n    return result", "response": "Returns a DataFrame with the solar position of the NREL SPA algorithm described in [ 1 ]."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sun_rise_set_transit_spa(times, latitude, longitude, how='numpy',\n                             delta_t=67.0, numthreads=4):\n    \"\"\"\n    Calculate the sunrise, sunset, and sun transit times using the\n    NREL SPA algorithm described in [1].\n\n    If numba is installed, the functions can be compiled to\n    machine code and the function can be multithreaded.\n    Without numba, the function evaluates via numpy with\n    a slight performance hit.\n\n    Parameters\n    ----------\n    times : pandas.DatetimeIndex\n        Must be localized to the timezone for ``latitude`` and ``longitude``.\n    latitude : float\n        Latitude in degrees, positive north of equator, negative to south\n    longitude : float\n        Longitude in degrees, positive east of prime meridian, negative to west\n    delta_t : float, optional\n        If delta_t is None, uses spa.calculate_deltat\n        using times.year and times.month from pandas.DatetimeIndex.\n        For most simulations specifing delta_t is sufficient.\n        Difference between terrestrial time and UT1.\n        delta_t = None will break code using nrel_numba,\n        this will be fixed in a future version.\n        By default, use USNO historical data and predictions\n    how : str, optional, default 'numpy'\n        Options are 'numpy' or 'numba'. If numba >= 0.17.0\n        is installed, how='numba' will compile the spa functions\n        to machine code and run them multithreaded.\n    numthreads : int, optional, default 4\n        Number of threads to use if how == 'numba'.\n\n    Returns\n    -------\n    pandas.DataFrame\n        index is the same as input `times` argument\n        columns are 'sunrise', 'sunset', and 'transit'\n\n    References\n    ----------\n    [1] Reda, I., Andreas, A., 2003. Solar position algorithm for solar\n    radiation applications. Technical report: NREL/TP-560- 34302. Golden,\n    USA, http://www.nrel.gov.\n    \"\"\"\n    # Added by Tony Lorenzo (@alorenzo175), University of Arizona, 2015\n\n    lat = latitude\n    lon = longitude\n\n    # times must be localized\n    if times.tz:\n        tzinfo = times.tz\n    else:\n        raise ValueError('times must be localized')\n\n    # must convert to midnight UTC on day of interest\n    utcday = pd.DatetimeIndex(times.date).tz_localize('UTC')\n    unixtime = np.array(utcday.astype(np.int64)/10**9)\n\n    spa = _spa_python_import(how)\n\n    delta_t = delta_t or spa.calculate_deltat(times.year, times.month)\n\n    transit, sunrise, sunset = spa.transit_sunrise_sunset(\n        unixtime, lat, lon, delta_t, numthreads)\n\n    # arrays are in seconds since epoch format, need to conver to timestamps\n    transit = pd.to_datetime(transit*1e9, unit='ns', utc=True).tz_convert(\n        tzinfo).tolist()\n    sunrise = pd.to_datetime(sunrise*1e9, unit='ns', utc=True).tz_convert(\n        tzinfo).tolist()\n    sunset = pd.to_datetime(sunset*1e9, unit='ns', utc=True).tz_convert(\n        tzinfo).tolist()\n\n    return pd.DataFrame(index=times, data={'sunrise': sunrise,\n                                           'sunset': sunset,\n                                           'transit': transit})", "response": "Calculate sunrise sunset and sun transit times using the NREL SPA algorithm described in [ 1 ]."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconvert a PyEphem date into seconds and microseconds", "response": "def _ephem_convert_to_seconds_and_microseconds(date):\n    # utility from unreleased PyEphem 3.6.7.1\n    \"\"\"Converts a PyEphem date into seconds\"\"\"\n    microseconds = int(round(24 * 60 * 60 * 1000000 * date))\n    seconds, microseconds = divmod(microseconds, 1000000)\n    seconds -= 2209032000  # difference between epoch 1900 and epoch 1970\n    return seconds, microseconds"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts a PyEphem Date into a timezone aware python datetime", "response": "def _ephem_to_timezone(date, tzinfo):\n    # utility from unreleased PyEphem 3.6.7.1\n    \"\"\"\"Convert a PyEphem Date into a timezone aware python datetime\"\"\"\n    seconds, microseconds = _ephem_convert_to_seconds_and_microseconds(date)\n    date = dt.datetime.fromtimestamp(seconds, tzinfo)\n    date = date.replace(microsecond=microseconds)\n    return date"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sun_rise_set_transit_ephem(times, latitude, longitude,\n                               next_or_previous='next',\n                               altitude=0,\n                               pressure=101325,\n                               temperature=12, horizon='0:00'):\n    \"\"\"\n    Calculate the next sunrise and sunset times using the PyEphem package.\n\n    Parameters\n    ----------\n    time : pandas.DatetimeIndex\n        Must be localized\n    latitude : float\n        Latitude in degrees, positive north of equator, negative to south\n    longitude : float\n        Longitude in degrees, positive east of prime meridian, negative to west\n    next_or_previous : str\n        'next' or 'previous' sunrise and sunset relative to time\n    altitude : float, default 0\n        distance above sea level in meters.\n    pressure : int or float, optional, default 101325\n        air pressure in Pascals.\n    temperature : int or float, optional, default 12\n        air temperature in degrees C.\n    horizon : string, format +/-X:YY\n        arc degrees:arc minutes from geometrical horizon for sunrise and\n        sunset, e.g., horizon='+0:00' to use sun center crossing the\n        geometrical horizon to define sunrise and sunset,\n        horizon='-0:34' for when the sun's upper edge crosses the\n        geometrical horizon\n\n    Returns\n    -------\n    pandas.DataFrame\n        index is the same as input `time` argument\n        columns are 'sunrise', 'sunset', and 'transit'\n\n    See also\n    --------\n    pyephem\n    \"\"\"\n\n    try:\n        import ephem\n    except ImportError:\n        raise ImportError('PyEphem must be installed')\n\n    # times must be localized\n    if times.tz:\n        tzinfo = times.tz\n    else:\n        raise ValueError('times must be localized')\n\n    obs, sun = _ephem_setup(latitude, longitude, altitude,\n                            pressure, temperature, horizon)\n    # create lists of sunrise and sunset time localized to time.tz\n    if next_or_previous.lower() == 'next':\n        rising = obs.next_rising\n        setting = obs.next_setting\n        transit = obs.next_transit\n    elif next_or_previous.lower() == 'previous':\n        rising = obs.previous_rising\n        setting = obs.previous_setting\n        transit = obs.previous_transit\n    else:\n        raise ValueError(\"next_or_previous must be either 'next' or\" +\n                         \" 'previous'\")\n\n    sunrise = []\n    sunset = []\n    trans = []\n    for thetime in times:\n        thetime = thetime.to_pydatetime()\n        # pyephem drops timezone when converting to its internal datetime\n        # format, so handle timezone explicitly here\n        obs.date = ephem.Date(thetime - thetime.utcoffset())\n        sunrise.append(_ephem_to_timezone(rising(sun), tzinfo))\n        sunset.append(_ephem_to_timezone(setting(sun), tzinfo))\n        trans.append(_ephem_to_timezone(transit(sun), tzinfo))\n\n    return pd.DataFrame(index=times, data={'sunrise': sunrise,\n                                           'sunset': sunset,\n                                           'transit': trans})", "response": "Returns a DataFrame containing the next sunrise and sunset times for the given time."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pyephem(time, latitude, longitude, altitude=0, pressure=101325,\n            temperature=12, horizon='+0:00'):\n    \"\"\"\n    Calculate the solar position using the PyEphem package.\n\n    Parameters\n    ----------\n    time : pandas.DatetimeIndex\n        Localized or UTC.\n    latitude : float\n        positive is north of 0\n    longitude : float\n        positive is east of 0\n    altitude : float, default 0\n        distance above sea level in meters.\n    pressure : int or float, optional, default 101325\n        air pressure in Pascals.\n    temperature : int or float, optional, default 12\n        air temperature in degrees C.\n    horizon : string, optional, default '+0:00'\n        arc degrees:arc minutes from geometrical horizon for sunrise and\n        sunset, e.g., horizon='+0:00' to use sun center crossing the\n        geometrical horizon to define sunrise and sunset,\n        horizon='-0:34' for when the sun's upper edge crosses the\n        geometrical horizon\n\n    Returns\n    -------\n    pandas.DataFrame\n        index is the same as input `time` argument\n        The DataFrame will have the following columns:\n        apparent_elevation, elevation,\n        apparent_azimuth, azimuth,\n        apparent_zenith, zenith.\n\n    See also\n    --------\n    spa_python, spa_c, ephemeris\n    \"\"\"\n\n    # Written by Will Holmgren (@wholmgren), University of Arizona, 2014\n    try:\n        import ephem\n    except ImportError:\n        raise ImportError('PyEphem must be installed')\n\n    # if localized, convert to UTC. otherwise, assume UTC.\n    try:\n        time_utc = time.tz_convert('UTC')\n    except TypeError:\n        time_utc = time\n\n    sun_coords = pd.DataFrame(index=time)\n\n    obs, sun = _ephem_setup(latitude, longitude, altitude,\n                            pressure, temperature, horizon)\n\n    # make and fill lists of the sun's altitude and azimuth\n    # this is the pressure and temperature corrected apparent alt/az.\n    alts = []\n    azis = []\n    for thetime in time_utc:\n        obs.date = ephem.Date(thetime)\n        sun.compute(obs)\n        alts.append(sun.alt)\n        azis.append(sun.az)\n\n    sun_coords['apparent_elevation'] = alts\n    sun_coords['apparent_azimuth'] = azis\n\n    # redo it for p=0 to get no atmosphere alt/az\n    obs.pressure = 0\n    alts = []\n    azis = []\n    for thetime in time_utc:\n        obs.date = ephem.Date(thetime)\n        sun.compute(obs)\n        alts.append(sun.alt)\n        azis.append(sun.az)\n\n    sun_coords['elevation'] = alts\n    sun_coords['azimuth'] = azis\n\n    # convert to degrees. add zenith\n    sun_coords = np.rad2deg(sun_coords)\n    sun_coords['apparent_zenith'] = 90 - sun_coords['apparent_elevation']\n    sun_coords['zenith'] = 90 - sun_coords['elevation']\n\n    return sun_coords", "response": "Returns a DataFrame that contains solar positions for the given time."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef ephemeris(time, latitude, longitude, pressure=101325, temperature=12):\n\n    # Added by Rob Andrews (@Calama-Consulting), Calama Consulting, 2014\n    # Edited by Will Holmgren (@wholmgren), University of Arizona, 2014\n\n    # Most comments in this function are from PVLIB_MATLAB or from\n    # pvlib-python's attempt to understand and fix problems with the\n    # algorithm. The comments are *not* based on the reference material.\n    # This helps a little bit:\n    # http://www.cv.nrao.edu/~rfisher/Ephemerides/times.html\n\n    # the inversion of longitude is due to the fact that this code was\n    # originally written for the convention that positive longitude were for\n    # locations west of the prime meridian. However, the correct convention (as\n    # of 2009) is to use negative longitudes for locations west of the prime\n    # meridian. Therefore, the user should input longitude values under the\n    # correct convention (e.g. Albuquerque is at -106 longitude), but it needs\n    # to be inverted for use in the code.\n\n    Latitude = latitude\n    Longitude = -1 * longitude\n\n    Abber = 20 / 3600.\n    LatR = np.radians(Latitude)\n\n    # the SPA algorithm needs time to be expressed in terms of\n    # decimal UTC hours of the day of the year.\n\n    # if localized, convert to UTC. otherwise, assume UTC.\n    try:\n        time_utc = time.tz_convert('UTC')\n    except TypeError:\n        time_utc = time\n\n    # strip out the day of the year and calculate the decimal hour\n    DayOfYear = time_utc.dayofyear\n    DecHours = (time_utc.hour + time_utc.minute/60. + time_utc.second/3600. +\n                time_utc.microsecond/3600.e6)\n\n    # np.array needed for pandas > 0.20\n    UnivDate = np.array(DayOfYear)\n    UnivHr = np.array(DecHours)\n\n    Yr = np.array(time_utc.year) - 1900\n    YrBegin = 365 * Yr + np.floor((Yr - 1) / 4.) - 0.5\n\n    Ezero = YrBegin + UnivDate\n    T = Ezero / 36525.\n\n    # Calculate Greenwich Mean Sidereal Time (GMST)\n    GMST0 = 6 / 24. + 38 / 1440. + (\n        45.836 + 8640184.542 * T + 0.0929 * T ** 2) / 86400.\n    GMST0 = 360 * (GMST0 - np.floor(GMST0))\n    GMSTi = np.mod(GMST0 + 360 * (1.0027379093 * UnivHr / 24.), 360)\n\n    # Local apparent sidereal time\n    LocAST = np.mod((360 + GMSTi - Longitude), 360)\n\n    EpochDate = Ezero + UnivHr / 24.\n    T1 = EpochDate / 36525.\n\n    ObliquityR = np.radians(\n        23.452294 - 0.0130125 * T1 - 1.64e-06 * T1 ** 2 + 5.03e-07 * T1 ** 3)\n    MlPerigee = 281.22083 + 4.70684e-05 * EpochDate + 0.000453 * T1 ** 2 + (\n        3e-06 * T1 ** 3)\n    MeanAnom = np.mod((358.47583 + 0.985600267 * EpochDate - 0.00015 *\n                       T1 ** 2 - 3e-06 * T1 ** 3), 360)\n    Eccen = 0.01675104 - 4.18e-05 * T1 - 1.26e-07 * T1 ** 2\n    EccenAnom = MeanAnom\n    E = 0\n\n    while np.max(abs(EccenAnom - E)) > 0.0001:\n        E = EccenAnom\n        EccenAnom = MeanAnom + np.degrees(Eccen)*np.sin(np.radians(E))\n\n    TrueAnom = (\n        2 * np.mod(np.degrees(np.arctan2(((1 + Eccen) / (1 - Eccen)) ** 0.5 *\n                   np.tan(np.radians(EccenAnom) / 2.), 1)), 360))\n    EcLon = np.mod(MlPerigee + TrueAnom, 360) - Abber\n    EcLonR = np.radians(EcLon)\n    DecR = np.arcsin(np.sin(ObliquityR)*np.sin(EcLonR))\n\n    RtAscen = np.degrees(np.arctan2(np.cos(ObliquityR)*np.sin(EcLonR),\n                                    np.cos(EcLonR)))\n\n    HrAngle = LocAST - RtAscen\n    HrAngleR = np.radians(HrAngle)\n    HrAngle = HrAngle - (360 * ((abs(HrAngle) > 180)))\n\n    SunAz = np.degrees(np.arctan2(-np.sin(HrAngleR),\n                                  np.cos(LatR)*np.tan(DecR) -\n                                  np.sin(LatR)*np.cos(HrAngleR)))\n    SunAz[SunAz < 0] += 360\n\n    SunEl = np.degrees(np.arcsin(\n        np.cos(LatR) * np.cos(DecR) * np.cos(HrAngleR) +\n        np.sin(LatR) * np.sin(DecR)))\n\n    SolarTime = (180 + HrAngle) / 15.\n\n    # Calculate refraction correction\n    Elevation = SunEl\n    TanEl = pd.Series(np.tan(np.radians(Elevation)), index=time_utc)\n    Refract = pd.Series(0, index=time_utc)\n\n    Refract[(Elevation > 5) & (Elevation <= 85)] = (\n        58.1/TanEl - 0.07/(TanEl**3) + 8.6e-05/(TanEl**5))\n\n    Refract[(Elevation > -0.575) & (Elevation <= 5)] = (\n        Elevation *\n        (-518.2 + Elevation*(103.4 + Elevation*(-12.79 + Elevation*0.711))) +\n        1735)\n\n    Refract[(Elevation > -1) & (Elevation <= -0.575)] = -20.774 / TanEl\n\n    Refract *= (283/(273. + temperature)) * (pressure/101325.) / 3600.\n\n    ApparentSunEl = SunEl + Refract\n\n    # make output DataFrame\n    DFOut = pd.DataFrame(index=time_utc)\n    DFOut['apparent_elevation'] = ApparentSunEl\n    DFOut['elevation'] = SunEl\n    DFOut['azimuth'] = SunAz\n    DFOut['apparent_zenith'] = 90 - ApparentSunEl\n    DFOut['zenith'] = 90 - SunEl\n    DFOut['solar_time'] = SolarTime\n    DFOut.index = time\n\n    return DFOut", "response": "Returns a DataFrame with the information about the solar position."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates the time between lower_bound and upper_bound where the attribute is equal to value. Uses PyEphem for solar position calculations.", "response": "def calc_time(lower_bound, upper_bound, latitude, longitude, attribute, value,\n              altitude=0, pressure=101325, temperature=12, horizon='+0:00',\n              xtol=1.0e-12):\n    \"\"\"\n    Calculate the time between lower_bound and upper_bound\n    where the attribute is equal to value. Uses PyEphem for\n    solar position calculations.\n\n    Parameters\n    ----------\n    lower_bound : datetime.datetime\n    upper_bound : datetime.datetime\n    latitude : float\n    longitude : float\n    attribute : str\n        The attribute of a pyephem.Sun object that\n        you want to solve for. Likely options are 'alt'\n        and 'az' (which must be given in radians).\n    value : int or float\n        The value of the attribute to solve for\n    altitude : float, default 0\n        Distance above sea level.\n    pressure : int or float, optional, default 101325\n        Air pressure in Pascals. Set to 0 for no\n        atmospheric correction.\n    temperature : int or float, optional, default 12\n        Air temperature in degrees C.\n    horizon : string, optional, default '+0:00'\n        arc degrees:arc minutes from geometrical horizon for sunrise and\n        sunset, e.g., horizon='+0:00' to use sun center crossing the\n        geometrical horizon to define sunrise and sunset,\n        horizon='-0:34' for when the sun's upper edge crosses the\n        geometrical horizon\n    xtol : float, optional, default 1.0e-12\n        The allowed error in the result from value\n\n    Returns\n    -------\n    datetime.datetime\n\n    Raises\n    ------\n    ValueError\n        If the value is not contained between the bounds.\n    AttributeError\n        If the given attribute is not an attribute of a\n        PyEphem.Sun object.\n    \"\"\"\n\n    try:\n        import scipy.optimize as so\n    except ImportError:\n        raise ImportError('The calc_time function requires scipy')\n\n    obs, sun = _ephem_setup(latitude, longitude, altitude,\n                            pressure, temperature, horizon)\n\n    def compute_attr(thetime, target, attr):\n        obs.date = thetime\n        sun.compute(obs)\n        return getattr(sun, attr) - target\n\n    lb = datetime_to_djd(lower_bound)\n    ub = datetime_to_djd(upper_bound)\n\n    djd_root = so.brentq(compute_attr, lb, ub,\n                         (value, attribute), xtol=xtol)\n\n    return djd_to_datetime(djd_root)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef pyephem_earthsun_distance(time):\n\n    import ephem\n\n    sun = ephem.Sun()\n    earthsun = []\n    for thetime in time:\n        sun.compute(ephem.Date(thetime))\n        earthsun.append(sun.earth_distance)\n\n    return pd.Series(earthsun, index=time)", "response": "Calculates the distance from the earth to the sun using pyephem."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nrel_earthsun_distance(time, how='numpy', delta_t=67.0, numthreads=4):\n\n    if not isinstance(time, pd.DatetimeIndex):\n        try:\n            time = pd.DatetimeIndex(time)\n        except (TypeError, ValueError):\n            time = pd.DatetimeIndex([time, ])\n\n    unixtime = np.array(time.astype(np.int64)/10**9)\n\n    spa = _spa_python_import(how)\n\n    delta_t = delta_t or spa.calculate_deltat(time.year, time.month)\n\n    dist = spa.earthsun_distance(unixtime, delta_t, numthreads)\n\n    dist = pd.Series(dist, index=time)\n\n    return dist", "response": "Calculates the distance from the earth to the sun using the NREL SPA algorithm described in [ 1 ]."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef declination_spencer71(dayofyear):\n    day_angle = _calculate_simple_day_angle(dayofyear)\n    return (\n        0.006918 -\n        0.399912 * np.cos(day_angle) + 0.070257 * np.sin(day_angle) -\n        0.006758 * np.cos(2. * day_angle) + 0.000907 * np.sin(2. * day_angle) -\n        0.002697 * np.cos(3. * day_angle) + 0.00148 * np.sin(3. * day_angle)\n    )", "response": "Return the declination of the current sun at the given day of year."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the declination of a cooper - based sun at a given day of the year.", "response": "def declination_cooper69(dayofyear):\n    \"\"\"\n    Solar declination from Duffie & Beckman [1] and attributed to Cooper (1969)\n\n    .. warning::\n        Return units are radians, not degrees.\n\n    Declination can be expressed using either sine or cosine:\n\n    .. math::\n\n       \\\\delta = 23.45 \\\\sin \\\\left( \\\\frac{2 \\\\pi}{365} \\\\left(n_{day} + 284\n       \\\\right) \\\\right) = -23.45 \\\\cos \\\\left( \\\\frac{2 \\\\pi}{365}\n       \\\\left(n_{day} + 10 \\\\right) \\\\right)\n\n    Parameters\n    ----------\n    dayofyear : numeric\n\n    Returns\n    -------\n    declination (radians) : numeric\n        Angular position of the sun at solar noon relative to the plane of the\n        equator, approximately between +/-23.45 (degrees).\n\n    References\n    ----------\n    [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n    Processes, 3rd Edition\" pp. 13-14, J. Wiley and Sons, New York (2006)\n\n    [2] J. H. Seinfeld and S. N. Pandis, \"Atmospheric Chemistry and Physics\"\n    p. 129, J. Wiley (1998)\n\n    [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for Renewable\n    Energy Applications\", p. 4 CRC Press (2013)\n\n    See Also\n    --------\n    declination_spencer71\n    \"\"\"\n    day_angle = _calculate_simple_day_angle(dayofyear)\n    dec = np.deg2rad(23.45 * np.sin(day_angle + (2.0 * np.pi / 365.0) * 285.0))\n    return dec"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning an analytical expression of solar azimuth angle based on spherical azimuth.", "response": "def solar_azimuth_analytical(latitude, hourangle, declination, zenith):\n    \"\"\"\n    Analytical expression of solar azimuth angle based on spherical\n    trigonometry.\n\n    Parameters\n    ----------\n    latitude : numeric\n        Latitude of location in radians.\n    hourangle : numeric\n        Hour angle in the local solar time in radians.\n    declination : numeric\n        Declination of the sun in radians.\n    zenith : numeric\n        Solar zenith angle in radians.\n\n    Returns\n    -------\n    azimuth : numeric\n        Solar azimuth angle in radians.\n\n    References\n    ----------\n    [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n    Processes, 3rd Edition\" pp. 14, J. Wiley and Sons, New York (2006)\n\n    [2] J. H. Seinfeld and S. N. Pandis, \"Atmospheric Chemistry and Physics\"\n    p. 132, J. Wiley (1998)\n\n    [3] `Wikipedia: Solar Azimuth Angle\n    <https://en.wikipedia.org/wiki/Solar_azimuth_angle>`_\n\n    [4] `PVCDROM: Azimuth Angle <http://www.pveducation.org/pvcdrom/2-\n    properties-sunlight/azimuth-angle>`_\n\n    See Also\n    --------\n    declination_spencer71\n    declination_cooper69\n    hour_angle\n    solar_zenith_analytical\n    \"\"\"\n\n    numer = (np.cos(zenith) * np.sin(latitude) - np.sin(declination))\n    denom = (np.sin(zenith) * np.cos(latitude))\n\n    # cases that would generate new NaN values are safely ignored here\n    # since they are dealt with further below\n    with np.errstate(invalid='ignore', divide='ignore'):\n        cos_azi = numer / denom\n\n    # when zero division occurs, use the limit value of the analytical\n    # expression\n    cos_azi = \\\n        np.where(np.isclose(denom,    0.0, rtol=0.0, atol=1e-8),  1.0, cos_azi)\n\n    # when too many round-ups in floating point math take cos_azi beyond\n    # 1.0, use 1.0\n    cos_azi = \\\n        np.where(np.isclose(cos_azi,  1.0, rtol=0.0, atol=1e-8),  1.0, cos_azi)\n    cos_azi = \\\n        np.where(np.isclose(cos_azi, -1.0, rtol=0.0, atol=1e-8), -1.0, cos_azi)\n\n    # when NaN values occur in input, ignore and pass to output\n    with np.errstate(invalid='ignore'):\n        sign_ha = np.sign(hourangle)\n\n    return sign_ha * np.arccos(cos_azi) + np.pi"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an analytical expression of solar zenith angle based on spherical azimuth and declination.", "response": "def solar_zenith_analytical(latitude, hourangle, declination):\n    \"\"\"\n    Analytical expression of solar zenith angle based on spherical\n    trigonometry.\n\n    .. warning:: The analytic form neglects the effect of atmospheric\n        refraction.\n\n    Parameters\n    ----------\n    latitude : numeric\n        Latitude of location in radians.\n    hourangle : numeric\n        Hour angle in the local solar time in radians.\n    declination : numeric\n        Declination of the sun in radians.\n\n    Returns\n    -------\n    zenith : numeric\n        Solar zenith angle in radians.\n\n    References\n    ----------\n    [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n    Processes, 3rd Edition\" pp. 14, J. Wiley and Sons, New York (2006)\n\n    [2] J. H. Seinfeld and S. N. Pandis, \"Atmospheric Chemistry and\n    Physics\" p. 132, J. Wiley (1998)\n\n    [3] Daryl R. Myers, \"Solar Radiation: Practical Modeling for\n    Renewable Energy Applications\", p. 5 CRC Press (2013)\n\n    `Wikipedia: Solar Zenith Angle\n    <https://en.wikipedia.org/wiki/Solar_zenith_angle>`_\n\n    `PVCDROM: Sun's Position\n    <http://www.pveducation.org/pvcdrom/2-properties-sunlight/suns-position>`_\n\n    See Also\n    --------\n    declination_spencer71\n    declination_cooper69\n    hour_angle\n    \"\"\"\n    return np.arccos(\n        np.cos(declination) * np.cos(latitude) * np.cos(hourangle) +\n        np.sin(declination) * np.sin(latitude)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef hour_angle(times, longitude, equation_of_time):\n    naive_times = times.tz_localize(None)  # naive but still localized\n    # hours - timezone = (times - normalized_times) - (naive_times - times)\n    hrs_minus_tzs = 1 / NS_PER_HR * (\n        2 * times.astype(np.int64) - times.normalize().astype(np.int64) -\n        naive_times.astype(np.int64))\n    # ensure array return instead of a version-dependent pandas <T>Index\n    return np.asarray(\n        15. * (hrs_minus_tzs - 12.) + longitude + equation_of_time / 4.)", "response": "Returns the hour angle in the local solar time."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert hour angles in degrees to hours as a numpy array", "response": "def _hour_angle_to_hours(times, hourangle, longitude, equation_of_time):\n    \"\"\"converts hour angles in degrees to hours as a numpy array\"\"\"\n    naive_times = times.tz_localize(None)  # naive but still localized\n    tzs = 1 / NS_PER_HR * (\n        naive_times.astype(np.int64) - times.astype(np.int64))\n    hours = (hourangle - longitude - equation_of_time / 4.) / 15. + 12. + tzs\n    return np.asarray(hours)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _local_times_from_hours_since_midnight(times, hours):\n    tz_info = times.tz  # pytz timezone info\n    naive_times = times.tz_localize(None)  # naive but still localized\n    # normalize local, naive times to previous midnight and add the hours until\n    # sunrise, sunset, and transit\n    return pd.DatetimeIndex(\n        (naive_times.normalize().astype(np.int64) +\n         (hours * NS_PER_HR).astype(np.int64)).astype('datetime64[ns]'),\n        tz=tz_info)", "response": "converts a datetime array of floats to localized times\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _times_to_hours_after_local_midnight(times):\n    times = times.tz_localize(None)\n    hrs = 1 / NS_PER_HR * (\n        times.astype(np.int64) - times.normalize().astype(np.int64))\n    return np.array(hrs)", "response": "convert local pandas datetime indices to array of hours as floats"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef sun_rise_set_transit_geometric(times, latitude, longitude, declination,\n                                   equation_of_time):\n    \"\"\"\n    Geometric calculation of solar sunrise, sunset, and transit.\n\n    .. warning:: The geometric calculation assumes a circular earth orbit with\n        the sun as a point source at its center, and neglects the effect of\n        atmospheric refraction on zenith. The error depends on location and\n        time of year but is of order 10 minutes.\n\n    Parameters\n    ----------\n    times : pandas.DatetimeIndex\n        Corresponding timestamps, must be localized to the timezone for the\n        ``latitude`` and ``longitude``.\n    latitude : float\n        Latitude in degrees, positive north of equator, negative to south\n    longitude : float\n        Longitude in degrees, positive east of prime meridian, negative to west\n    declination : numeric\n        declination angle in radians at ``times``\n    equation_of_time : numeric\n        difference in time between solar time and mean solar time in minutes\n\n    Returns\n    -------\n    sunrise : datetime\n        localized sunrise time\n    sunset : datetime\n        localized sunset time\n    transit : datetime\n        localized sun transit time\n\n    References\n    ----------\n    [1] J. A. Duffie and W. A. Beckman,  \"Solar Engineering of Thermal\n    Processes, 3rd Edition,\" J. Wiley and Sons, New York (2006)\n\n    [2] Frank Vignola et al., \"Solar And Infrared Radiation Measurements,\"\n    CRC Press (2012)\n\n    \"\"\"\n    latitude_rad = np.radians(latitude)  # radians\n    sunset_angle_rad = np.arccos(-np.tan(declination) * np.tan(latitude_rad))\n    sunset_angle = np.degrees(sunset_angle_rad)  # degrees\n    # solar noon is at hour angle zero\n    # so sunrise is just negative of sunset\n    sunrise_angle = -sunset_angle\n    sunrise_hour = _hour_angle_to_hours(\n        times, sunrise_angle, longitude, equation_of_time)\n    sunset_hour = _hour_angle_to_hours(\n        times, sunset_angle, longitude, equation_of_time)\n    transit_hour = _hour_angle_to_hours(times, 0, longitude, equation_of_time)\n    sunrise = _local_times_from_hours_since_midnight(times, sunrise_hour)\n    sunset = _local_times_from_hours_since_midnight(times, sunset_hour)\n    transit = _local_times_from_hours_since_midnight(times, transit_hour)\n    return sunrise, sunset, transit", "response": "Returns a geometric calculation of solar sunrise sunset and transit."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef basic_chain(times, latitude, longitude,\n                module_parameters, inverter_parameters,\n                irradiance=None, weather=None,\n                surface_tilt=None, surface_azimuth=None,\n                orientation_strategy=None,\n                transposition_model='haydavies',\n                solar_position_method='nrel_numpy',\n                airmass_model='kastenyoung1989',\n                altitude=None, pressure=None,\n                **kwargs):\n    \"\"\"\n    An experimental function that computes all of the modeling steps\n    necessary for calculating power or energy for a PV system at a given\n    location.\n\n    Parameters\n    ----------\n    times : DatetimeIndex\n        Times at which to evaluate the model.\n\n    latitude : float.\n        Positive is north of the equator.\n        Use decimal degrees notation.\n\n    longitude : float.\n        Positive is east of the prime meridian.\n        Use decimal degrees notation.\n\n    module_parameters : None, dict or Series\n        Module parameters as defined by the SAPM.\n\n    inverter_parameters : None, dict or Series\n        Inverter parameters as defined by the CEC.\n\n    irradiance : None or DataFrame, default None\n        If None, calculates clear sky data.\n        Columns must be 'dni', 'ghi', 'dhi'.\n\n    weather : None or DataFrame, default None\n        If None, assumes air temperature is 20 C and\n        wind speed is 0 m/s.\n        Columns must be 'wind_speed', 'temp_air'.\n\n    surface_tilt : None, float or Series, default None\n        Surface tilt angles in decimal degrees.\n        The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth : None, float or Series, default None\n        Surface azimuth angles in decimal degrees.\n        The azimuth convention is defined\n        as degrees east of north\n        (North=0, South=180, East=90, West=270).\n\n    orientation_strategy : None or str, default None\n        The strategy for aligning the modules.\n        If not None, sets the ``surface_azimuth`` and ``surface_tilt``\n        properties of the ``system``. Allowed strategies include 'flat',\n        'south_at_latitude_tilt'. Ignored for SingleAxisTracker systems.\n\n    transposition_model : str, default 'haydavies'\n        Passed to system.get_irradiance.\n\n    solar_position_method : str, default 'nrel_numpy'\n        Passed to solarposition.get_solarposition.\n\n    airmass_model : str, default 'kastenyoung1989'\n        Passed to atmosphere.relativeairmass.\n\n    altitude : None or float, default None\n        If None, computed from pressure. Assumed to be 0 m\n        if pressure is also None.\n\n    pressure : None or float, default None\n        If None, computed from altitude. Assumed to be 101325 Pa\n        if altitude is also None.\n\n    **kwargs\n        Arbitrary keyword arguments.\n        See code for details.\n\n    Returns\n    -------\n    output : (dc, ac)\n        Tuple of DC power (with SAPM parameters) (DataFrame) and AC\n        power (Series).\n    \"\"\"\n\n    # use surface_tilt and surface_azimuth if provided,\n    # otherwise set them using the orientation_strategy\n    if surface_tilt is not None and surface_azimuth is not None:\n        pass\n    elif orientation_strategy is not None:\n        surface_tilt, surface_azimuth = \\\n            get_orientation(orientation_strategy, latitude=latitude)\n    else:\n        raise ValueError('orientation_strategy or surface_tilt and '\n                         'surface_azimuth must be provided')\n\n    if altitude is None and pressure is None:\n        altitude = 0.\n        pressure = 101325.\n    elif altitude is None:\n        altitude = atmosphere.pres2alt(pressure)\n    elif pressure is None:\n        pressure = atmosphere.alt2pres(altitude)\n\n    solar_position = solarposition.get_solarposition(\n        times, latitude, longitude, altitude=altitude, pressure=pressure,\n        method=solar_position_method, **kwargs)\n\n    # possible error with using apparent zenith with some models\n    airmass = atmosphere.get_relative_airmass(\n        solar_position['apparent_zenith'], model=airmass_model)\n    airmass = atmosphere.get_absolute_airmass(airmass, pressure)\n    dni_extra = pvlib.irradiance.get_extra_radiation(solar_position.index)\n\n    aoi = pvlib.irradiance.aoi(surface_tilt, surface_azimuth,\n                               solar_position['apparent_zenith'],\n                               solar_position['azimuth'])\n\n    if irradiance is None:\n        linke_turbidity = clearsky.lookup_linke_turbidity(\n            solar_position.index, latitude, longitude)\n        irradiance = clearsky.ineichen(\n            solar_position['apparent_zenith'],\n            airmass,\n            linke_turbidity,\n            altitude=altitude,\n            dni_extra=dni_extra\n            )\n\n    total_irrad = pvlib.irradiance.get_total_irradiance(\n        surface_tilt,\n        surface_azimuth,\n        solar_position['apparent_zenith'],\n        solar_position['azimuth'],\n        irradiance['dni'],\n        irradiance['ghi'],\n        irradiance['dhi'],\n        model=transposition_model,\n        dni_extra=dni_extra)\n\n    if weather is None:\n        weather = {'wind_speed': 0, 'temp_air': 20}\n\n    temps = pvsystem.sapm_celltemp(total_irrad['poa_global'],\n                                   weather['wind_speed'],\n                                   weather['temp_air'])\n\n    effective_irradiance = pvsystem.sapm_effective_irradiance(\n        total_irrad['poa_direct'], total_irrad['poa_diffuse'], airmass, aoi,\n        module_parameters)\n\n    dc = pvsystem.sapm(effective_irradiance, temps['temp_cell'],\n                       module_parameters)\n\n    ac = pvsystem.snlinverter(dc['v_mp'], dc['p_mp'], inverter_parameters)\n\n    return dc, ac", "response": "This function computes the basic chain of the modules at a given location."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the surface tilt and surface azimuth of a PV system.", "response": "def get_orientation(strategy, **kwargs):\n    \"\"\"\n    Determine a PV system's surface tilt and surface azimuth\n    using a named strategy.\n\n    Parameters\n    ----------\n    strategy: str\n        The orientation strategy.\n        Allowed strategies include 'flat', 'south_at_latitude_tilt'.\n    **kwargs:\n        Strategy-dependent keyword arguments. See code for details.\n\n    Returns\n    -------\n    surface_tilt, surface_azimuth\n    \"\"\"\n\n    if strategy == 'south_at_latitude_tilt':\n        surface_azimuth = 180\n        surface_tilt = kwargs['latitude']\n    elif strategy == 'flat':\n        surface_azimuth = 180\n        surface_tilt = 0\n    else:\n        raise ValueError('invalid orientation strategy. strategy must '\n                         'be one of south_at_latitude, flat,')\n\n    return surface_tilt, surface_azimuth"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine the missing irradiation columns. Only two of the following data columns (dni, ghi, dhi) are needed to calculate the missing data. This function is not safe at the moment. Results can be too high or negative. Please contribute and help to improve this function on https://github.com/pvlib/pvlib-python Parameters ---------- times : None or DatetimeIndex, default None Times at which to evaluate the model. Can be None if attribute `times` is already set. weather : None or pandas.DataFrame, default None Table with at least two columns containing one of the following data sets: dni, dhi, ghi. Can be None if attribute `weather` is already set. Returns ------- self Assigns attributes: times, weather Examples -------- This example does not work until the parameters `my_system`, `my_location`, `my_datetime` and `my_weather` are not defined properly but shows the basic idea how this method can be used. >>> from pvlib.modelchain import ModelChain >>> # my_weather containing 'dhi' and 'ghi'. >>> mc = ModelChain(my_system, my_location) # doctest: +SKIP >>> mc.complete_irradiance(my_datetime, my_weather) # doctest: +SKIP >>> mc.run_model() # doctest: +SKIP >>> # my_weather containing 'dhi', 'ghi' and 'dni'. >>> mc = ModelChain(my_system, my_location) # doctest: +SKIP >>> mc.run_model(my_datetime, my_weather) # doctest: +SKIP", "response": "def complete_irradiance(self, times=None, weather=None):\n        \"\"\"\n        Determine the missing irradiation columns. Only two of the\n        following data columns (dni, ghi, dhi) are needed to calculate\n        the missing data.\n\n        This function is not safe at the moment. Results can be too high\n        or negative. Please contribute and help to improve this function\n        on https://github.com/pvlib/pvlib-python\n\n        Parameters\n        ----------\n        times : None or DatetimeIndex, default None\n            Times at which to evaluate the model. Can be None if\n            attribute `times` is already set.\n        weather : None or pandas.DataFrame, default None\n            Table with at least two columns containing one of the\n            following data sets: dni, dhi, ghi. Can be None if attribute\n            `weather` is already set.\n\n        Returns\n        -------\n        self\n\n        Assigns attributes: times, weather\n\n        Examples\n        --------\n        This example does not work until the parameters `my_system`,\n        `my_location`, `my_datetime` and `my_weather` are not defined\n        properly but shows the basic idea how this method can be used.\n\n        >>> from pvlib.modelchain import ModelChain\n\n        >>> # my_weather containing 'dhi' and 'ghi'.\n        >>> mc = ModelChain(my_system, my_location)  # doctest: +SKIP\n        >>> mc.complete_irradiance(my_datetime, my_weather)  # doctest: +SKIP\n        >>> mc.run_model()  # doctest: +SKIP\n\n        >>> # my_weather containing 'dhi', 'ghi' and 'dni'.\n        >>> mc = ModelChain(my_system, my_location)  # doctest: +SKIP\n        >>> mc.run_model(my_datetime, my_weather)  # doctest: +SKIP\n        \"\"\"\n        if weather is not None:\n            self.weather = weather\n        if times is not None:\n            self.times = times\n        self.solar_position = self.location.get_solarposition(\n            self.times, method=self.solar_position_method)\n        icolumns = set(self.weather.columns)\n        wrn_txt = (\"This function is not safe at the moment.\\n\" +\n                   \"Results can be too high or negative.\\n\" +\n                   \"Help to improve this function on github:\\n\" +\n                   \"https://github.com/pvlib/pvlib-python \\n\")\n\n        if {'ghi', 'dhi'} <= icolumns and 'dni' not in icolumns:\n            clearsky = self.location.get_clearsky(\n                times, solar_position=self.solar_position)\n            self.weather.loc[:, 'dni'] = pvlib.irradiance.dni(\n                self.weather.loc[:, 'ghi'], self.weather.loc[:, 'dhi'],\n                self.solar_position.zenith,\n                clearsky_dni=clearsky['dni'],\n                clearsky_tolerance=1.1)\n        elif {'dni', 'dhi'} <= icolumns and 'ghi' not in icolumns:\n            warnings.warn(wrn_txt, UserWarning)\n            self.weather.loc[:, 'ghi'] = (\n                self.weather.dni * tools.cosd(self.solar_position.zenith) +\n                self.weather.dhi)\n        elif {'dni', 'ghi'} <= icolumns and 'dhi' not in icolumns:\n            warnings.warn(wrn_txt, UserWarning)\n            self.weather.loc[:, 'dhi'] = (\n                self.weather.ghi - self.weather.dni *\n                tools.cosd(self.solar_position.zenith))\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nprepares the solar position irradiance and weather inputs to the internal object.", "response": "def prepare_inputs(self, times=None, weather=None):\n        \"\"\"\n        Prepare the solar position, irradiance, and weather inputs to\n        the model.\n\n        Parameters\n        ----------\n        times : None or DatetimeIndex, default None\n            Times at which to evaluate the model. Can be None if\n            attribute `times` is already set.\n        weather : None or DataFrame, default None\n            If ``None``, the weather attribute is used. If the weather\n            attribute is also ``None`` assumes air temperature is 20 C, wind\n            speed is 0 m/s and irradiation calculated from clear sky\n            data. Column names must be ``'wind_speed'``, ``'temp_air'``,\n            ``'dni'``, ``'ghi'``, ``'dhi'``. Do not pass incomplete irradiation\n            data. Use method\n            :py:meth:`~pvlib.modelchain.ModelChain.complete_irradiance`\n            instead.\n\n        Notes\n        -----\n        Assigns attributes: ``times``, ``solar_position``, ``airmass``,\n        ``total_irrad``, `aoi`\n        \"\"\"\n        if weather is not None:\n            self.weather = weather\n        if self.weather is None:\n            self.weather = pd.DataFrame(index=times)\n\n        if times is not None:\n            self.times = times\n\n        self.solar_position = self.location.get_solarposition(\n            self.times, method=self.solar_position_method)\n\n        self.airmass = self.location.get_airmass(\n            solar_position=self.solar_position, model=self.airmass_model)\n\n        if not any([x in ['ghi', 'dni', 'dhi'] for x in self.weather.columns]):\n            self.weather[['ghi', 'dni', 'dhi']] = self.location.get_clearsky(\n                self.solar_position.index, self.clearsky_model,\n                solar_position=self.solar_position,\n                airmass_absolute=self.airmass['airmass_absolute'])\n\n        if not {'ghi', 'dni', 'dhi'} <= set(self.weather.columns):\n            raise ValueError(\n                \"Uncompleted irradiance data set. Please check you input \" +\n                \"data.\\nData set needs to have 'dni', 'dhi' and 'ghi'.\\n\" +\n                \"Detected data: {0}\".format(list(self.weather.columns)))\n\n        # PVSystem.get_irradiance and SingleAxisTracker.get_irradiance\n        # and PVSystem.get_aoi and SingleAxisTracker.get_aoi\n        # have different method signatures. Use partial to handle\n        # the differences.\n        if isinstance(self.system, SingleAxisTracker):\n            self.tracking = self.system.singleaxis(\n                self.solar_position['apparent_zenith'],\n                self.solar_position['azimuth'])\n            self.tracking['surface_tilt'] = (\n                self.tracking['surface_tilt']\n                    .fillna(self.system.axis_tilt))\n            self.tracking['surface_azimuth'] = (\n                self.tracking['surface_azimuth']\n                    .fillna(self.system.axis_azimuth))\n            self.aoi = self.tracking['aoi']\n            get_irradiance = partial(\n                self.system.get_irradiance,\n                self.tracking['surface_tilt'],\n                self.tracking['surface_azimuth'],\n                self.solar_position['apparent_zenith'],\n                self.solar_position['azimuth'])\n        else:\n            self.aoi = self.system.get_aoi(\n                self.solar_position['apparent_zenith'],\n                self.solar_position['azimuth'])\n            get_irradiance = partial(\n                self.system.get_irradiance,\n                self.solar_position['apparent_zenith'],\n                self.solar_position['azimuth'])\n\n        self.total_irrad = get_irradiance(\n            self.weather['dni'],\n            self.weather['ghi'],\n            self.weather['dhi'],\n            airmass=self.airmass['airmass_relative'],\n            model=self.transposition_model)\n\n        if self.weather.get('wind_speed') is None:\n            self.weather['wind_speed'] = 0\n        if self.weather.get('temp_air') is None:\n            self.weather['temp_air'] = 20\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nruns the model for the current object.", "response": "def run_model(self, times=None, weather=None):\n        \"\"\"\n        Run the model.\n\n        Parameters\n        ----------\n        times : None or DatetimeIndex, default None\n            Times at which to evaluate the model. Can be None if\n            attribute `times` is already set.\n        weather : None or DataFrame, default None\n            If None, assumes air temperature is 20 C, wind speed is 0\n            m/s and irradiation calculated from clear sky data. Column\n            names must be 'wind_speed', 'temp_air', 'dni', 'ghi', 'dhi'.\n            Do not pass incomplete irradiation data. Use method\n            :py:meth:`~pvlib.modelchain.ModelChain.complete_irradiance`\n            instead.\n\n        Returns\n        -------\n        self\n\n        Assigns attributes: times, solar_position, airmass, irradiance,\n        total_irrad, effective_irradiance, weather, temps, aoi,\n        aoi_modifier, spectral_modifier, dc, ac, losses.\n        \"\"\"\n\n        self.prepare_inputs(times, weather)\n        self.aoi_model()\n        self.spectral_model()\n        self.effective_irradiance_model()\n        self.temp_model()\n        self.dc_model()\n        self.ac_model()\n        self.losses_model()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nestimates the open circuit voltage of a given set of photometry and saturation current.", "response": "def estimate_voc(photocurrent, saturation_current, nNsVth):\n    \"\"\"\n    Rough estimate of open circuit voltage useful for bounding searches for\n    ``i`` of ``v`` when using :func:`~pvlib.pvsystem.singlediode`.\n\n    Parameters\n    ----------\n    photocurrent : numeric\n        photo-generated current [A]\n    saturation_current : numeric\n        diode reverse saturation current [A]\n    nNsVth : numeric\n        product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n        and number of series cells ``Ns``\n\n    Returns\n    -------\n    numeric\n        rough estimate of open circuit voltage [V]\n\n    Notes\n    -----\n    Calculating the open circuit voltage, :math:`V_{oc}`, of an ideal device\n    with infinite shunt resistance, :math:`R_{sh} \\\\to \\\\infty`, and zero\n    series resistance, :math:`R_s = 0`, yields the following equation [1]. As\n    an estimate of :math:`V_{oc}` it is useful as an upper bound for the\n    bisection method.\n\n    .. math::\n\n        V_{oc, est}=n Ns V_{th} \\\\log \\\\left( \\\\frac{I_L}{I_0} + 1 \\\\right)\n\n    [1] http://www.pveducation.org/pvcdrom/open-circuit-voltage\n    \"\"\"\n\n    return nNsVth * np.log(np.asarray(photocurrent) / saturation_current + 1.0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef bishop88(diode_voltage, photocurrent, saturation_current,\n             resistance_series, resistance_shunt, nNsVth, d2mutau=0,\n             NsVbi=np.Inf, gradients=False):\n    \"\"\"\n    Explicit calculation of points on the IV curve described by the single\n    diode equation [1]_.\n\n    .. warning::\n       * Do not use ``d2mutau`` with CEC coefficients.\n       * Usage of ``d2mutau`` with PVSyst coefficients is required for cadmium-\n         telluride (CdTe) and amorphous-silicon (a:Si) PV modules only.\n\n    Parameters\n    ----------\n    diode_voltage : numeric\n        diode voltages [V]\n    photocurrent : numeric\n        photo-generated current [A]\n    saturation_current : numeric\n        diode reverse saturation current [A]\n    resistance_series : numeric\n        series resistance [ohms]\n    resistance_shunt: numeric\n        shunt resistance [ohms]\n    nNsVth : numeric\n        product of thermal voltage ``Vth`` [V], diode ideality factor ``n``,\n        and number of series cells ``Ns``\n    d2mutau : numeric\n        PVSyst thin-film recombination parameter that is the ratio of thickness\n        of the intrinsic layer squared :math:`d^2` and the diffusion length of\n        charge carriers :math:`\\\\mu \\\\tau`, in volts [V], defaults to 0[V]\n    NsVbi : numeric\n        PVSyst thin-film recombination parameter that is the product of the PV\n        module number of series cells ``Ns`` and the builtin voltage ``Vbi`` of\n        the intrinsic layer, in volts [V], defaults to ``np.inf``\n    gradients : bool\n        False returns only I, V, and P. True also returns gradients\n\n    Returns\n    -------\n    tuple\n        currents [A], voltages [V], power [W], and optionally\n        :math:`\\\\frac{dI}{dV_d}`, :math:`\\\\frac{dV}{dV_d}`,\n        :math:`\\\\frac{dI}{dV}`, :math:`\\\\frac{dP}{dV}`, and\n        :math:`\\\\frac{d^2 P}{dV dV_d}`\n\n    Notes\n    -----\n    The PVSyst thin-film recombination losses parameters ``d2mutau`` and\n    ``NsVbi`` are only applied to cadmium-telluride (CdTe) and amorphous-\n    silicon (a:Si) PV modules, [2]_, [3]_. The builtin voltage :math:`V_{bi}`\n    should account for all junctions. For example: tandem and triple junction\n    cells would have builtin voltages of 1.8[V] and 2.7[V] respectively, based\n    on the default of 0.9[V] for a single junction. The parameter ``NsVbi``\n    should only account for the number of series cells in a single parallel\n    sub-string if the module has cells in parallel greater than 1.\n\n    References\n    ----------\n    .. [1] \"Computer simulation of the effects of electrical mismatches in\n       photovoltaic cell interconnection circuits\" JW Bishop, Solar Cell (1988)\n       :doi:`10.1016/0379-6787(88)90059-2`\n\n    .. [2] \"Improved equivalent circuit and Analytical Model for Amorphous\n       Silicon Solar Cells and Modules.\" J. Mertens, et al., IEEE Transactions\n       on Electron Devices, Vol 45, No 2, Feb 1998.\n       :doi:`10.1109/16.658676`\n\n    .. [3] \"Performance assessment of a simulation model for PV modules of any\n       available technology\", Andr\u00e9 Mermoud and Thibault Lejeune, 25th EUPVSEC,\n       2010\n       :doi:`10.4229/25thEUPVSEC2010-4BV.1.114`\n    \"\"\"\n    # calculate recombination loss current where d2mutau > 0\n    is_recomb = d2mutau > 0  # True where there is thin-film recombination loss\n    v_recomb = np.where(is_recomb, NsVbi - diode_voltage, np.inf)\n    i_recomb = np.where(is_recomb, photocurrent * d2mutau / v_recomb, 0)\n    # calculate temporary values to simplify calculations\n    v_star = diode_voltage / nNsVth  # non-dimensional diode voltage\n    g_sh = 1.0 / resistance_shunt  # conductance\n    i = (photocurrent - saturation_current * np.expm1(v_star)\n         - diode_voltage * g_sh - i_recomb)\n    v = diode_voltage - i * resistance_series\n    retval = (i, v, i*v)\n    if gradients:\n        # calculate recombination loss current gradients where d2mutau > 0\n        grad_i_recomb = np.where(is_recomb, i_recomb / v_recomb, 0)\n        grad_2i_recomb = np.where(is_recomb, 2 * grad_i_recomb / v_recomb, 0)\n        g_diode = saturation_current * np.exp(v_star) / nNsVth  # conductance\n        grad_i = -g_diode - g_sh - grad_i_recomb  # di/dvd\n        grad_v = 1.0 - grad_i * resistance_series  # dv/dvd\n        # dp/dv = d(iv)/dv = v * di/dv + i\n        grad = grad_i / grad_v  # di/dv\n        grad_p = v * grad + i  # dp/dv\n        grad2i = -g_diode / nNsVth - grad_2i_recomb  # d2i/dvd\n        grad2v = -grad2i * resistance_series  # d2v/dvd\n        grad2p = (\n            grad_v * grad + v * (grad2i/grad_v - grad_i*grad2v/grad_v**2)\n            + grad_i\n        )  # d2p/dv/dvd\n        retval += (grad_i, grad_v, grad, grad_p, grad2p)\n    return retval", "response": "This function is used to calculate the bishop of a single - level CEC entry in the PV module."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfind current given any voltage.", "response": "def bishop88_i_from_v(voltage, photocurrent, saturation_current,\n                      resistance_series, resistance_shunt, nNsVth,\n                      method='newton'):\n    \"\"\"\n    Find current given any voltage.\n\n    Parameters\n    ----------\n    voltage : numeric\n        voltage (V) in volts [V]\n    photocurrent : numeric\n        photogenerated current (Iph or IL) in amperes [A]\n    saturation_current : numeric\n        diode dark or saturation current (Io or Isat) in amperes [A]\n    resistance_series : numeric\n        series resistance (Rs) in ohms\n    resistance_shunt : numeric\n        shunt resistance (Rsh) in ohms\n    nNsVth : numeric\n        product of diode ideality factor (n), number of series cells (Ns), and\n        thermal voltage (Vth = k_b * T / q_e) in volts [V]\n    method : str\n        one of two optional search methods: either ``'brentq'``, a reliable and\n        bounded method or ``'newton'`` which is the default.\n\n    Returns\n    -------\n    current : numeric\n        current (I) at the specified voltage (V) in amperes [A]\n    \"\"\"\n    # collect args\n    args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth)\n\n    def fv(x, v, *a):\n        # calculate voltage residual given diode voltage \"x\"\n        return bishop88(x, *a)[1] - v\n\n    if method.lower() == 'brentq':\n        # first bound the search using voc\n        voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n\n        # brentq only works with scalar inputs, so we need a set up function\n        # and np.vectorize to repeatedly call the optimizer with the right\n        # arguments for possible array input\n        def vd_from_brent(voc, v, iph, isat, rs, rsh, gamma):\n            return brentq(fv, 0.0, voc, args=(v, iph, isat, rs, rsh, gamma))\n\n        vd_from_brent_vectorized = np.vectorize(vd_from_brent)\n        vd = vd_from_brent_vectorized(voc_est, voltage, *args)\n    elif method.lower() == 'newton':\n        # make sure all args are numpy arrays if max size > 1\n        # if voltage is an array, then make a copy to use for initial guess, v0\n        args, v0 = _prepare_newton_inputs((voltage,), args, voltage)\n        vd = newton(func=lambda x, *a: fv(x, voltage, *a), x0=v0,\n                    fprime=lambda x, *a: bishop88(x, *a, gradients=True)[4],\n                    args=args)\n    else:\n        raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n    return bishop88(vd, *args)[0]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bishop88_mpp(photocurrent, saturation_current, resistance_series,\n                 resistance_shunt, nNsVth, method='newton'):\n    \"\"\"\n    Find max power point.\n\n    Parameters\n    ----------\n    photocurrent : numeric\n        photogenerated current (Iph or IL) in amperes [A]\n    saturation_current : numeric\n        diode dark or saturation current (Io or Isat) in amperes [A]\n    resistance_series : numeric\n        series resistance (Rs) in ohms\n    resistance_shunt : numeric\n        shunt resistance (Rsh) in ohms\n    nNsVth : numeric\n        product of diode ideality factor (n), number of series cells (Ns), and\n        thermal voltage (Vth = k_b * T / q_e) in volts [V]\n    method : str\n        one of two optional search methods: either ``'brentq'``, a reliable and\n        bounded method or ``'newton'`` which is the default.\n\n    Returns\n    -------\n    OrderedDict or pandas.DataFrame\n        max power current ``i_mp`` [A], max power voltage ``v_mp`` [V], and\n        max power ``p_mp`` [W]\n    \"\"\"\n    # collect args\n    args = (photocurrent, saturation_current, resistance_series,\n            resistance_shunt, nNsVth)\n    # first bound the search using voc\n    voc_est = estimate_voc(photocurrent, saturation_current, nNsVth)\n\n    def fmpp(x, *a):\n        return bishop88(x, *a, gradients=True)[6]\n\n    if method.lower() == 'brentq':\n        # break out arguments for numpy.vectorize to handle broadcasting\n        vec_fun = np.vectorize(\n            lambda voc, iph, isat, rs, rsh, gamma:\n                brentq(fmpp, 0.0, voc, args=(iph, isat, rs, rsh, gamma))\n        )\n        vd = vec_fun(voc_est, *args)\n    elif method.lower() == 'newton':\n        # make sure all args are numpy arrays if max size > 1\n        # if voc_est is an array, then make a copy to use for initial guess, v0\n        args, v0 = _prepare_newton_inputs((), args, voc_est)\n        vd = newton(\n            func=fmpp, x0=v0,\n            fprime=lambda x, *a: bishop88(x, *a, gradients=True)[7], args=args\n        )\n    else:\n        raise NotImplementedError(\"Method '%s' isn't implemented\" % method)\n    return bishop88(vd, *args)", "response": "Find max power point in a set of modules."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfunctions to find power from i_from_v.", "response": "def _pwr_optfcn(df, loc):\n    '''\n    Function to find power from ``i_from_v``.\n    '''\n\n    I = _lambertw_i_from_v(df['r_sh'], df['r_s'],           # noqa: E741, N806\n                           df['nNsVth'], df[loc], df['i_0'], df['i_l'])\n\n    return I * df[loc]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nretrieving the designated dataset creates NCSS object and creates a NCSS query object.", "response": "def set_dataset(self):\n        '''\n        Retrieves the designated dataset, creates NCSS object, and\n        creates a NCSS query object.\n        '''\n\n        keys = list(self.model.datasets.keys())\n        labels = [item.split()[0].lower() for item in keys]\n        if self.set_type == 'best':\n            self.dataset = self.model.datasets[keys[labels.index('best')]]\n        elif self.set_type == 'latest':\n            self.dataset = self.model.datasets[keys[labels.index('latest')]]\n        elif self.set_type == 'full':\n            self.dataset = self.model.datasets[keys[labels.index('full')]]\n\n        self.access_url = self.dataset.access_urls[self.access_url_key]\n        self.ncss = NCSS(self.access_url)\n        self.query = self.ncss.query()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_query_latlon(self):\n        '''\n        Sets the NCSS query location latitude and longitude.\n        '''\n\n        if (isinstance(self.longitude, list) and\n                isinstance(self.latitude, list)):\n            self.lbox = True\n            # west, east, south, north\n            self.query.lonlat_box(self.longitude[0], self.longitude[1],\n                                  self.latitude[0], self.latitude[1])\n        else:\n            self.lbox = False\n            self.query.lonlat_point(self.longitude, self.latitude)", "response": "Sets the NCSS query location latitude and longitude."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the location for the object.", "response": "def set_location(self, time, latitude, longitude):\n        '''\n        Sets the location for the query.\n\n        Parameters\n        ----------\n        time: datetime or DatetimeIndex\n            Time range of the query.\n        '''\n        if isinstance(time, datetime.datetime):\n            tzinfo = time.tzinfo\n        else:\n            tzinfo = time.tz\n\n        if tzinfo is None:\n            self.location = Location(latitude, longitude)\n        else:\n            self.location = Location(latitude, longitude, tz=tzinfo)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubmitting a query to the UNIDATA servers using Siphon NCSS and converts the netcdf data to a pandas DataFrame. Parameters ---------- latitude: float The latitude value. longitude: float The longitude value. start: datetime or timestamp The start time. end: datetime or timestamp The end time. vert_level: None, float or integer, default None Vertical altitude of interest. query_variables: None or list, default None If None, uses self.variables. close_netcdf_data: bool, default True Controls if the temporary netcdf data file should be closed. Set to False to access the raw data. Returns ------- forecast_data : DataFrame column names are the weather model's variable names.", "response": "def get_data(self, latitude, longitude, start, end,\n                 vert_level=None, query_variables=None,\n                 close_netcdf_data=True):\n        \"\"\"\n        Submits a query to the UNIDATA servers using Siphon NCSS and\n        converts the netcdf data to a pandas DataFrame.\n\n        Parameters\n        ----------\n        latitude: float\n            The latitude value.\n        longitude: float\n            The longitude value.\n        start: datetime or timestamp\n            The start time.\n        end: datetime or timestamp\n            The end time.\n        vert_level: None, float or integer, default None\n            Vertical altitude of interest.\n        query_variables: None or list, default None\n            If None, uses self.variables.\n        close_netcdf_data: bool, default True\n            Controls if the temporary netcdf data file should be closed.\n            Set to False to access the raw data.\n\n        Returns\n        -------\n        forecast_data : DataFrame\n            column names are the weather model's variable names.\n        \"\"\"\n\n        if not self.connected:\n            self.connect_to_catalog()\n\n        if vert_level is not None:\n            self.vert_level = vert_level\n\n        if query_variables is None:\n            self.query_variables = list(self.variables.values())\n        else:\n            self.query_variables = query_variables\n\n        self.latitude = latitude\n        self.longitude = longitude\n        self.set_query_latlon()  # modifies self.query\n        self.set_location(start, latitude, longitude)\n\n        self.start = start\n        self.end = end\n        self.query.time_range(self.start, self.end)\n\n        if self.vert_level is not None:\n            self.query.vertical_level(self.vert_level)\n\n        self.query.variables(*self.query_variables)\n        self.query.accept(self.data_format)\n\n        self.netcdf_data = self.ncss.get_data(self.query)\n\n        # might be better to go to xarray here so that we can handle\n        # higher dimensional data for more advanced applications\n        self.data = self._netcdf2pandas(self.netcdf_data, self.query_variables,\n                                        self.start, self.end)\n\n        if close_netcdf_data:\n            self.netcdf_data.close()\n\n        return self.data"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_processed_data(self, *args, **kwargs):\n        return self.process_data(self.get_data(*args, **kwargs), **kwargs)", "response": "Get and process forecast data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef rename(self, data, variables=None):\n        if variables is None:\n            variables = self.variables\n        return data.rename(columns={y: x for x, y in variables.items()})", "response": "Renames the columns according the variable mapping."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _netcdf2pandas(self, netcdf_data, query_variables, start, end):\n        # set self.time\n        try:\n            time_var = 'time'\n            self.set_time(netcdf_data.variables[time_var])\n        except KeyError:\n            # which model does this dumb thing?\n            time_var = 'time1'\n            self.set_time(netcdf_data.variables[time_var])\n\n        data_dict = {}\n        for key, data in netcdf_data.variables.items():\n            # if accounts for possibility of extra variable returned\n            if key not in query_variables:\n                continue\n            squeezed = data[:].squeeze()\n            if squeezed.ndim == 1:\n                data_dict[key] = squeezed\n            elif squeezed.ndim == 2:\n                for num, data_level in enumerate(squeezed.T):\n                    data_dict[key + '_' + str(num)] = data_level\n            else:\n                raise ValueError('cannot parse ndim > 2')\n\n        data = pd.DataFrame(data_dict, index=self.time)\n        # sometimes data is returned as hours since T0\n        # where T0 is before start. Then the hours between\n        # T0 and start are added *after* end. So sort and slice\n        # to remove the garbage\n        data = data.sort_index().loc[start:end]\n        return data", "response": "Transforms data from netcdf to pandas DataFrame."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_time(self, time):\n        '''\n        Converts time data into a pandas date object.\n\n        Parameters\n        ----------\n        time: netcdf\n            Contains time information.\n\n        Returns\n        -------\n        pandas.DatetimeIndex\n        '''\n        times = num2date(time[:].squeeze(), time.units)\n        self.time = pd.DatetimeIndex(pd.Series(times), tz=self.location.tz)", "response": "Sets the time of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nconverts cloud cover to GHI using a linear relationship.", "response": "def cloud_cover_to_ghi_linear(self, cloud_cover, ghi_clear, offset=35,\n                                  **kwargs):\n        \"\"\"\n        Convert cloud cover to GHI using a linear relationship.\n\n        0% cloud cover returns ghi_clear.\n\n        100% cloud cover returns offset*ghi_clear.\n\n        Parameters\n        ----------\n        cloud_cover: numeric\n            Cloud cover in %.\n        ghi_clear: numeric\n            GHI under clear sky conditions.\n        offset: numeric, default 35\n            Determines the minimum GHI.\n        kwargs\n            Not used.\n\n        Returns\n        -------\n        ghi: numeric\n            Estimated GHI.\n\n        References\n        ----------\n        Larson et. al. \"Day-ahead forecasting of solar power output from\n        photovoltaic plants in the American Southwest\" Renewable Energy\n        91, 11-20 (2016).\n        \"\"\"\n\n        offset = offset / 100.\n        cloud_cover = cloud_cover / 100.\n        ghi = (offset + (1 - offset) * (1 - cloud_cover)) * ghi_clear\n        return ghi"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestimate irradiance from cloud cover in the following steps: 1. Determine clear sky GHI using Ineichen model and climatological turbidity. 2. Estimate cloudy sky GHI using a function of cloud_cover e.g. :py:meth:`~ForecastModel.cloud_cover_to_ghi_linear` 3. Estimate cloudy sky DNI using the DISC model. 4. Calculate DHI from DNI and DHI. Parameters ---------- cloud_cover : Series Cloud cover in %. method : str, default 'linear' Method for converting cloud cover to GHI. 'linear' is currently the only option. **kwargs Passed to the method that does the conversion Returns ------- irrads : DataFrame Estimated GHI, DNI, and DHI.", "response": "def cloud_cover_to_irradiance_clearsky_scaling(self, cloud_cover,\n                                                   method='linear',\n                                                   **kwargs):\n        \"\"\"\n        Estimates irradiance from cloud cover in the following steps:\n\n        1. Determine clear sky GHI using Ineichen model and\n           climatological turbidity.\n        2. Estimate cloudy sky GHI using a function of\n           cloud_cover e.g.\n           :py:meth:`~ForecastModel.cloud_cover_to_ghi_linear`\n        3. Estimate cloudy sky DNI using the DISC model.\n        4. Calculate DHI from DNI and DHI.\n\n        Parameters\n        ----------\n        cloud_cover : Series\n            Cloud cover in %.\n        method : str, default 'linear'\n            Method for converting cloud cover to GHI.\n            'linear' is currently the only option.\n        **kwargs\n            Passed to the method that does the conversion\n\n        Returns\n        -------\n        irrads : DataFrame\n            Estimated GHI, DNI, and DHI.\n        \"\"\"\n        solpos = self.location.get_solarposition(cloud_cover.index)\n        cs = self.location.get_clearsky(cloud_cover.index, model='ineichen',\n                                        solar_position=solpos)\n\n        method = method.lower()\n        if method == 'linear':\n            ghi = self.cloud_cover_to_ghi_linear(cloud_cover, cs['ghi'],\n                                                 **kwargs)\n        else:\n            raise ValueError('invalid method argument')\n\n        dni = disc(ghi, solpos['zenith'], cloud_cover.index)['dni']\n        dhi = ghi - dni * np.cos(np.radians(solpos['zenith']))\n\n        irrads = pd.DataFrame({'ghi': ghi, 'dni': dni, 'dhi': dhi}).fillna(0)\n        return irrads"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating irradiance from a cloud cover.", "response": "def cloud_cover_to_irradiance_liujordan(self, cloud_cover, **kwargs):\n        \"\"\"\n        Estimates irradiance from cloud cover in the following steps:\n\n        1. Determine transmittance using a function of cloud cover e.g.\n           :py:meth:`~ForecastModel.cloud_cover_to_transmittance_linear`\n        2. Calculate GHI, DNI, DHI using the\n           :py:func:`pvlib.irradiance.liujordan` model\n\n        Parameters\n        ----------\n        cloud_cover : Series\n\n        Returns\n        -------\n        irradiance : DataFrame\n            Columns include ghi, dni, dhi\n        \"\"\"\n        # in principle, get_solarposition could use the forecast\n        # pressure, temp, etc., but the cloud cover forecast is not\n        # accurate enough to justify using these minor corrections\n        solar_position = self.location.get_solarposition(cloud_cover.index)\n        dni_extra = get_extra_radiation(cloud_cover.index)\n        airmass = self.location.get_airmass(cloud_cover.index)\n\n        transmittance = self.cloud_cover_to_transmittance_linear(cloud_cover,\n                                                                 **kwargs)\n\n        irrads = liujordan(solar_position['apparent_zenith'],\n                           transmittance, airmass['airmass_absolute'],\n                           dni_extra=dni_extra)\n        irrads = irrads.fillna(0)\n\n        return irrads"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef cloud_cover_to_irradiance(self, cloud_cover, how='clearsky_scaling',\n                                  **kwargs):\n        \"\"\"\n        Convert cloud cover to irradiance. A wrapper method.\n\n        Parameters\n        ----------\n        cloud_cover : Series\n        how : str, default 'clearsky_scaling'\n            Selects the method for conversion. Can be one of\n            clearsky_scaling or liujordan.\n        **kwargs\n            Passed to the selected method.\n\n        Returns\n        -------\n        irradiance : DataFrame\n            Columns include ghi, dni, dhi\n        \"\"\"\n\n        how = how.lower()\n        if how == 'clearsky_scaling':\n            irrads = self.cloud_cover_to_irradiance_clearsky_scaling(\n                cloud_cover, **kwargs)\n        elif how == 'liujordan':\n            irrads = self.cloud_cover_to_irradiance_liujordan(\n                cloud_cover, **kwargs)\n        else:\n            raise ValueError('invalid how argument')\n\n        return irrads", "response": "A wrapper method for cloud_cover_to_irradiance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef isobaric_to_ambient_temperature(self, data):\n\n        P = data['pressure'] / 100.0                            # noqa: N806\n        Tiso = data['temperature_iso']                          # noqa: N806\n        Td = data['temperature_dew_iso'] - 273.15               # noqa: N806\n\n        # saturation water vapor pressure\n        e = 6.11 * 10**((7.5 * Td) / (Td + 273.3))\n\n        # saturation water vapor mixing ratio\n        w = 0.622 * (e / (P - e))\n\n        temperature = Tiso - ((2.501 * 10.**6) / 1005.7) * w\n\n        return temperature", "response": "Calculates the temperature from an isobaric temperature."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconverting raw forecast data into processed forecast data.", "response": "def process_data(self, data, cloud_cover='total_clouds', **kwargs):\n        \"\"\"\n        Defines the steps needed to convert raw forecast data\n        into processed forecast data.\n\n        Parameters\n        ----------\n        data: DataFrame\n            Raw forecast data\n        cloud_cover: str, default 'total_clouds'\n            The type of cloud cover used to infer the irradiance.\n\n        Returns\n        -------\n        data: DataFrame\n            Processed forecast data.\n        \"\"\"\n        data = super(GFS, self).process_data(data, **kwargs)\n        data['temp_air'] = self.kelvin_to_celsius(data['temp_air'])\n        data['wind_speed'] = self.uv_to_speed(data)\n        irrads = self.cloud_cover_to_irradiance(data[cloud_cover], **kwargs)\n        data = data.join(irrads, how='outer')\n        return data[self.output_variables]"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process_data(self, data, cloud_cover='total_clouds', **kwargs):\n        data = super(HRRR, self).process_data(data, **kwargs)\n        wind_mapping = {\n            'wind_speed_u': 'u-component_of_wind_height_above_ground_0',\n            'wind_speed_v': 'v-component_of_wind_height_above_ground_0',\n        }\n        data = self.rename(data, variables=wind_mapping)\n        data['temp_air'] = self.kelvin_to_celsius(data['temp_air'])\n        data['wind_speed'] = self.uv_to_speed(data)\n        irrads = self.cloud_cover_to_irradiance(data[cloud_cover], **kwargs)\n        data = data.join(irrads, how='outer')\n        data = data.iloc[:-1, :]  # issue with last point\n        return data[self.output_variables]", "response": "Convert raw forecast data into processed forecast data."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_extra_radiation(datetime_or_doy, solar_constant=1366.1,\n                        method='spencer', epoch_year=2014, **kwargs):\n    \"\"\"\n    Determine extraterrestrial radiation from day of year.\n\n    Parameters\n    ----------\n    datetime_or_doy : numeric, array, date, datetime, Timestamp, DatetimeIndex\n        Day of year, array of days of year, or datetime-like object\n\n    solar_constant : float, default 1366.1\n        The solar constant.\n\n    method : string, default 'spencer'\n        The method by which the ET radiation should be calculated.\n        Options include ``'pyephem', 'spencer', 'asce', 'nrel'``.\n\n    epoch_year : int, default 2014\n        The year in which a day of year input will be calculated. Only\n        applies to day of year input used with the pyephem or nrel\n        methods.\n\n    kwargs :\n        Passed to solarposition.nrel_earthsun_distance\n\n    Returns\n    -------\n    dni_extra : float, array, or Series\n        The extraterrestrial radiation present in watts per square meter\n        on a surface which is normal to the sun. Pandas Timestamp and\n        DatetimeIndex inputs will yield a Pandas TimeSeries. All other\n        inputs will yield a float or an array of floats.\n\n    References\n    ----------\n    [1] M. Reno, C. Hansen, and J. Stein, \"Global Horizontal Irradiance\n    Clear Sky Models: Implementation and Analysis\", Sandia National\n    Laboratories, SAND2012-2389, 2012.\n\n    [2] <http://solardat.uoregon.edu/SolarRadiationBasics.html>, Eqs.\n    SR1 and SR2\n\n    [3] Partridge, G. W. and Platt, C. M. R. 1976. Radiative Processes\n    in Meteorology and Climatology.\n\n    [4] Duffie, J. A. and Beckman, W. A. 1991. Solar Engineering of\n    Thermal Processes, 2nd edn. J. Wiley and Sons, New York.\n    \"\"\"\n\n    to_doy, to_datetimeindex, to_output = \\\n        _handle_extra_radiation_types(datetime_or_doy, epoch_year)\n\n    # consider putting asce and spencer methods in their own functions\n    method = method.lower()\n    if method == 'asce':\n        B = solarposition._calculate_simple_day_angle(to_doy(datetime_or_doy))\n        RoverR0sqrd = 1 + 0.033 * np.cos(B)\n    elif method == 'spencer':\n        B = solarposition._calculate_simple_day_angle(to_doy(datetime_or_doy))\n        RoverR0sqrd = (1.00011 + 0.034221 * np.cos(B) + 0.00128 * np.sin(B) +\n                       0.000719 * np.cos(2 * B) + 7.7e-05 * np.sin(2 * B))\n    elif method == 'pyephem':\n        times = to_datetimeindex(datetime_or_doy)\n        RoverR0sqrd = solarposition.pyephem_earthsun_distance(times) ** (-2)\n    elif method == 'nrel':\n        times = to_datetimeindex(datetime_or_doy)\n        RoverR0sqrd = \\\n            solarposition.nrel_earthsun_distance(times, **kwargs) ** (-2)\n    else:\n        raise ValueError('Invalid method: %s', method)\n\n    Ea = solar_constant * RoverR0sqrd\n\n    Ea = to_output(Ea)\n\n    return Ea", "response": "Returns the extra radiation of the current sun in the base of the base of the base of the solar position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef aoi_projection(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth):\n\n    projection = (\n        tools.cosd(surface_tilt) * tools.cosd(solar_zenith) +\n        tools.sind(surface_tilt) * tools.sind(solar_zenith) *\n        tools.cosd(solar_azimuth - surface_azimuth))\n\n    try:\n        projection.name = 'aoi_projection'\n    except AttributeError:\n        pass\n\n    return projection", "response": "Calculates the dot product of the sun position unit vector and the sun normal unit vector."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the angle of incidence of the solar vector on a surface.", "response": "def aoi(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth):\n    \"\"\"\n    Calculates the angle of incidence of the solar vector on a surface.\n    This is the angle between the solar vector and the surface normal.\n\n    Input all angles in degrees.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Panel tilt from horizontal.\n    surface_azimuth : numeric\n        Panel azimuth from north.\n    solar_zenith : numeric\n        Solar zenith angle.\n    solar_azimuth : numeric\n        Solar azimuth angle.\n\n    Returns\n    -------\n    aoi : numeric\n        Angle of incidence in degrees.\n    \"\"\"\n\n    projection = aoi_projection(surface_tilt, surface_azimuth,\n                                solar_zenith, solar_azimuth)\n    aoi_value = np.rad2deg(np.arccos(projection))\n\n    try:\n        aoi_value.name = 'aoi'\n    except AttributeError:\n        pass\n\n    return aoi_value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalculating the ratio of the PAO of the horizontal beam of array irradiance and the horizontal beam of array irradiance.", "response": "def poa_horizontal_ratio(surface_tilt, surface_azimuth,\n                         solar_zenith, solar_azimuth):\n    \"\"\"\n    Calculates the ratio of the beam components of the plane of array\n    irradiance and the horizontal irradiance.\n\n    Input all angles in degrees.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Panel tilt from horizontal.\n    surface_azimuth : numeric\n        Panel azimuth from north.\n    solar_zenith : numeric\n        Solar zenith angle.\n    solar_azimuth : numeric\n        Solar azimuth angle.\n\n    Returns\n    -------\n    ratio : numeric\n        Ratio of the plane of array irradiance to the horizontal plane\n        irradiance\n    \"\"\"\n\n    cos_poa_zen = aoi_projection(surface_tilt, surface_azimuth,\n                                 solar_zenith, solar_azimuth)\n\n    cos_solar_zenith = tools.cosd(solar_zenith)\n\n    # ratio of tilted and horizontal beam irradiance\n    ratio = cos_poa_zen / cos_solar_zenith\n\n    try:\n        ratio.name = 'poa_ratio'\n    except AttributeError:\n        pass\n\n    return ratio"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the beam component of the array irradiance.", "response": "def beam_component(surface_tilt, surface_azimuth, solar_zenith, solar_azimuth,\n                   dni):\n    \"\"\"\n    Calculates the beam component of the plane of array irradiance.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Panel tilt from horizontal.\n    surface_azimuth : numeric\n        Panel azimuth from north.\n    solar_zenith : numeric\n        Solar zenith angle.\n    solar_azimuth : numeric\n        Solar azimuth angle.\n    dni : numeric\n        Direct Normal Irradiance\n\n    Returns\n    -------\n    beam : numeric\n        Beam component\n    \"\"\"\n    beam = dni * aoi_projection(surface_tilt, surface_azimuth,\n                                solar_zenith, solar_azimuth)\n    beam = np.maximum(beam, 0)\n\n    return beam"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_sky_diffuse(surface_tilt, surface_azimuth,\n                    solar_zenith, solar_azimuth,\n                    dni, ghi, dhi, dni_extra=None, airmass=None,\n                    model='isotropic',\n                    model_perez='allsitescomposite1990'):\n    r\"\"\"\n    Determine in-plane sky diffuse irradiance component\n    using the specified sky diffuse irradiance model.\n\n    Sky diffuse models include:\n        * isotropic (default)\n        * klucher\n        * haydavies\n        * reindl\n        * king\n        * perez\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Panel tilt from horizontal.\n    surface_azimuth : numeric\n        Panel azimuth from north.\n    solar_zenith : numeric\n        Solar zenith angle.\n    solar_azimuth : numeric\n        Solar azimuth angle.\n    dni : numeric\n        Direct Normal Irradiance\n    ghi : numeric\n        Global horizontal irradiance\n    dhi : numeric\n        Diffuse horizontal irradiance\n    dni_extra : None or numeric, default None\n        Extraterrestrial direct normal irradiance\n    airmass : None or numeric, default None\n        Airmass\n    model : String, default 'isotropic'\n        Irradiance model.\n    model_perez : String, default 'allsitescomposite1990'\n        See perez.\n\n    Returns\n    -------\n    poa_sky_diffuse : numeric\n    \"\"\"\n\n    model = model.lower()\n    if model == 'isotropic':\n        sky = isotropic(surface_tilt, dhi)\n    elif model == 'klucher':\n        sky = klucher(surface_tilt, surface_azimuth, dhi, ghi,\n                      solar_zenith, solar_azimuth)\n    elif model == 'haydavies':\n        sky = haydavies(surface_tilt, surface_azimuth, dhi, dni, dni_extra,\n                        solar_zenith, solar_azimuth)\n    elif model == 'reindl':\n        sky = reindl(surface_tilt, surface_azimuth, dhi, dni, ghi, dni_extra,\n                     solar_zenith, solar_azimuth)\n    elif model == 'king':\n        sky = king(surface_tilt, dhi, ghi, solar_zenith)\n    elif model == 'perez':\n        sky = perez(surface_tilt, surface_azimuth, dhi, dni, dni_extra,\n                    solar_zenith, solar_azimuth, airmass,\n                    model=model_perez)\n    else:\n        raise ValueError('invalid model selection {}'.format(model))\n\n    return sky", "response": "r Returns in - plane sky diffuse irradiance component of the specified model."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef poa_components(aoi, dni, poa_sky_diffuse, poa_ground_diffuse):\n    r'''\n    Determine in-plane irradiance components.\n\n    Combines DNI with sky diffuse and ground-reflected irradiance to calculate\n    total, direct and diffuse irradiance components in the plane of array.\n\n    Parameters\n    ----------\n    aoi : numeric\n        Angle of incidence of solar rays with respect to the module\n        surface, from :func:`aoi`.\n\n    dni : numeric\n        Direct normal irradiance (W/m^2), as measured from a TMY file or\n        calculated with a clearsky model.\n\n    poa_sky_diffuse : numeric\n        Diffuse irradiance (W/m^2) in the plane of the modules, as\n        calculated by a diffuse irradiance translation function\n\n    poa_ground_diffuse : numeric\n        Ground reflected irradiance (W/m^2) in the plane of the modules,\n        as calculated by an albedo model (eg. :func:`grounddiffuse`)\n\n    Returns\n    -------\n    irrads : OrderedDict or DataFrame\n        Contains the following keys:\n\n        * ``poa_global`` : Total in-plane irradiance (W/m^2)\n        * ``poa_direct`` : Total in-plane beam irradiance (W/m^2)\n        * ``poa_diffuse`` : Total in-plane diffuse irradiance (W/m^2)\n        * ``poa_sky_diffuse`` : In-plane diffuse irradiance from sky (W/m^2)\n        * ``poa_ground_diffuse`` : In-plane diffuse irradiance from ground\n          (W/m^2)\n\n    Notes\n    ------\n    Negative beam irradiation due to aoi :math:`> 90^{\\circ}` or AOI\n    :math:`< 0^{\\circ}` is set to zero.\n    '''\n\n    poa_direct = np.maximum(dni * np.cos(np.radians(aoi)), 0)\n    poa_diffuse = poa_sky_diffuse + poa_ground_diffuse\n    poa_global = poa_direct + poa_diffuse\n\n    irrads = OrderedDict()\n    irrads['poa_global'] = poa_global\n    irrads['poa_direct'] = poa_direct\n    irrads['poa_diffuse'] = poa_diffuse\n    irrads['poa_sky_diffuse'] = poa_sky_diffuse\n    irrads['poa_ground_diffuse'] = poa_ground_diffuse\n\n    if isinstance(poa_direct, pd.Series):\n        irrads = pd.DataFrame(irrads)\n\n    return irrads", "response": "r Returns an OrderedDict of irradiance components for the given set of in - plane irradiance components."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nestimates diffuse irradiance from ground reflections given irradiance, albedo, and surface tilt Function to determine the portion of irradiance on a tilted surface due to ground reflections. Any of the inputs may be DataFrames or scalars. Parameters ---------- surface_tilt : numeric Surface tilt angles in decimal degrees. Tilt must be >=0 and <=180. The tilt angle is defined as degrees from horizontal (e.g. surface facing up = 0, surface facing horizon = 90). ghi : numeric Global horizontal irradiance in W/m^2. albedo : numeric, default 0.25 Ground reflectance, typically 0.1-0.4 for surfaces on Earth (land), may increase over snow, ice, etc. May also be known as the reflection coefficient. Must be >=0 and <=1. Will be overridden if surface_type is supplied. surface_type: None or string, default None If not None, overrides albedo. String can be one of 'urban', 'grass', 'fresh grass', 'snow', 'fresh snow', 'asphalt', 'concrete', 'aluminum', 'copper', 'fresh steel', 'dirty steel', 'sea'. Returns ------- grounddiffuse : numeric Ground reflected irradiances in W/m^2. References ---------- [1] Loutzenhiser P.G. et. al. \"Empirical validation of models to compute solar irradiance on inclined surfaces for building energy simulation\" 2007, Solar Energy vol. 81. pp. 254-267. The calculation is the last term of equations 3, 4, 7, 8, 10, 11, and 12. [2] albedos from: http://files.pvsyst.com/help/albedo.htm and http://en.wikipedia.org/wiki/Albedo and https://doi.org/10.1175/1520-0469(1972)029<0959:AOTSS>2.0.CO;2", "response": "def get_ground_diffuse(surface_tilt, ghi, albedo=.25, surface_type=None):\n    '''\n    Estimate diffuse irradiance from ground reflections given\n    irradiance, albedo, and surface tilt\n\n    Function to determine the portion of irradiance on a tilted surface\n    due to ground reflections. Any of the inputs may be DataFrames or\n    scalars.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angles in decimal degrees. Tilt must be >=0 and\n        <=180. The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90).\n\n    ghi : numeric\n        Global horizontal irradiance in W/m^2.\n\n    albedo : numeric, default 0.25\n        Ground reflectance, typically 0.1-0.4 for surfaces on Earth\n        (land), may increase over snow, ice, etc. May also be known as\n        the reflection coefficient. Must be >=0 and <=1. Will be\n        overridden if surface_type is supplied.\n\n    surface_type: None or string, default None\n        If not None, overrides albedo. String can be one of 'urban',\n        'grass', 'fresh grass', 'snow', 'fresh snow', 'asphalt', 'concrete',\n        'aluminum', 'copper', 'fresh steel', 'dirty steel', 'sea'.\n\n    Returns\n    -------\n    grounddiffuse : numeric\n        Ground reflected irradiances in W/m^2.\n\n\n    References\n    ----------\n    [1] Loutzenhiser P.G. et. al. \"Empirical validation of models to compute\n    solar irradiance on inclined surfaces for building energy simulation\"\n    2007, Solar Energy vol. 81. pp. 254-267.\n\n    The calculation is the last term of equations 3, 4, 7, 8, 10, 11, and 12.\n\n    [2] albedos from:\n    http://files.pvsyst.com/help/albedo.htm\n    and\n    http://en.wikipedia.org/wiki/Albedo\n    and\n    https://doi.org/10.1175/1520-0469(1972)029<0959:AOTSS>2.0.CO;2\n    '''\n\n    if surface_type is not None:\n        albedo = SURFACE_ALBEDOS[surface_type]\n\n    diffuse_irrad = ghi * albedo * (1 - np.cos(np.radians(surface_tilt))) * 0.5\n\n    try:\n        diffuse_irrad.name = 'diffuse_ground'\n    except AttributeError:\n        pass\n\n    return diffuse_irrad"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef klucher(surface_tilt, surface_azimuth, dhi, ghi, solar_zenith,\n            solar_azimuth):\n    r'''\n    Determine diffuse irradiance from the sky on a tilted surface\n    using Klucher's 1979 model\n\n    .. math::\n\n       I_{d} = DHI \\frac{1 + \\cos\\beta}{2} (1 + F' \\sin^3(\\beta/2))\n       (1 + F' \\cos^2\\theta\\sin^3\\theta_z)\n\n    where\n\n    .. math::\n\n       F' = 1 - (I_{d0} / GHI)\n\n    Klucher's 1979 model determines the diffuse irradiance from the sky\n    (ground reflected irradiance is not included in this algorithm) on a\n    tilted surface using the surface tilt angle, surface azimuth angle,\n    diffuse horizontal irradiance, direct normal irradiance, global\n    horizontal irradiance, extraterrestrial irradiance, sun zenith\n    angle, and sun azimuth angle.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angles in decimal degrees. surface_tilt must be >=0\n        and <=180. The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth : numeric\n        Surface azimuth angles in decimal degrees. surface_azimuth must\n        be >=0 and <=360. The Azimuth convention is defined as degrees\n        east of north (e.g. North = 0, South=180 East = 90, West = 270).\n\n    dhi : numeric\n        Diffuse horizontal irradiance in W/m^2. DHI must be >=0.\n\n    ghi : numeric\n        Global irradiance in W/m^2. DNI must be >=0.\n\n    solar_zenith : numeric\n        Apparent (refraction-corrected) zenith angles in decimal\n        degrees. solar_zenith must be >=0 and <=180.\n\n    solar_azimuth : numeric\n        Sun azimuth angles in decimal degrees. solar_azimuth must be >=0\n        and <=360. The Azimuth convention is defined as degrees east of\n        north (e.g. North = 0, East = 90, West = 270).\n\n    Returns\n    -------\n    diffuse : numeric\n        The sky diffuse component of the solar radiation.\n\n    References\n    ----------\n    [1] Loutzenhiser P.G. et. al. \"Empirical validation of models to compute\n    solar irradiance on inclined surfaces for building energy simulation\"\n    2007, Solar Energy vol. 81. pp. 254-267\n\n    [2] Klucher, T.M., 1979. Evaluation of models to predict insolation on\n    tilted surfaces. Solar Energy 23 (2), 111-114.\n    '''\n\n    # zenith angle with respect to panel normal.\n    cos_tt = aoi_projection(surface_tilt, surface_azimuth,\n                            solar_zenith, solar_azimuth)\n    cos_tt = np.maximum(cos_tt, 0)  # GH 526\n\n    F = 1 - ((dhi / ghi) ** 2)\n    try:\n        # fails with single point input\n        F.fillna(0, inplace=True)\n    except AttributeError:\n        F = np.where(np.isnan(F), 0, F)\n\n    term1 = 0.5 * (1 + tools.cosd(surface_tilt))\n    term2 = 1 + F * (tools.sind(0.5 * surface_tilt) ** 3)\n    term3 = 1 + F * (cos_tt ** 2) * (tools.sind(solar_zenith) ** 3)\n\n    sky_diffuse = dhi * term1 * term2 * term3\n\n    return sky_diffuse", "response": "r Returns a base for a Klucher s 1979 model."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines diffuse irradiance from the sky on a tilted surface using the King model. King's model determines the diffuse irradiance from the sky (ground reflected irradiance is not included in this algorithm) on a tilted surface using the surface tilt angle, diffuse horizontal irradiance, global horizontal irradiance, and sun zenith angle. Note that this model is not well documented and has not been published in any fashion (as of January 2012). Parameters ---------- surface_tilt : numeric Surface tilt angles in decimal degrees. The tilt angle is defined as degrees from horizontal (e.g. surface facing up = 0, surface facing horizon = 90) dhi : numeric Diffuse horizontal irradiance in W/m^2. ghi : numeric Global horizontal irradiance in W/m^2. solar_zenith : numeric Apparent (refraction-corrected) zenith angles in decimal degrees. Returns -------- poa_sky_diffuse : numeric The diffuse component of the solar radiation.", "response": "def king(surface_tilt, dhi, ghi, solar_zenith):\n    '''\n    Determine diffuse irradiance from the sky on a tilted surface using\n    the King model.\n\n    King's model determines the diffuse irradiance from the sky (ground\n    reflected irradiance is not included in this algorithm) on a tilted\n    surface using the surface tilt angle, diffuse horizontal irradiance,\n    global horizontal irradiance, and sun zenith angle. Note that this\n    model is not well documented and has not been published in any\n    fashion (as of January 2012).\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angles in decimal degrees. The tilt angle is\n        defined as degrees from horizontal (e.g. surface facing up = 0,\n        surface facing horizon = 90)\n\n    dhi : numeric\n        Diffuse horizontal irradiance in W/m^2.\n\n    ghi : numeric\n        Global horizontal irradiance in W/m^2.\n\n    solar_zenith : numeric\n        Apparent (refraction-corrected) zenith angles in decimal degrees.\n\n    Returns\n    --------\n    poa_sky_diffuse : numeric\n        The diffuse component of the solar radiation.\n    '''\n\n    sky_diffuse = (dhi * ((1 + tools.cosd(surface_tilt))) / 2 + ghi *\n                   ((0.012 * solar_zenith - 0.04)) *\n                   ((1 - tools.cosd(surface_tilt))) / 2)\n    sky_diffuse = np.maximum(sky_diffuse, 0)\n\n    return sky_diffuse"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndetermines diffuse irradiance from the sky on a tilted surface using one of the Perez models. Perez models determine the diffuse irradiance from the sky (ground reflected irradiance is not included in this algorithm) on a tilted surface using the surface tilt angle, surface azimuth angle, diffuse horizontal irradiance, direct normal irradiance, extraterrestrial irradiance, sun zenith angle, sun azimuth angle, and relative (not pressure-corrected) airmass. Optionally a selector may be used to use any of Perez's model coefficient sets. Parameters ---------- surface_tilt : numeric Surface tilt angles in decimal degrees. surface_tilt must be >=0 and <=180. The tilt angle is defined as degrees from horizontal (e.g. surface facing up = 0, surface facing horizon = 90) surface_azimuth : numeric Surface azimuth angles in decimal degrees. surface_azimuth must be >=0 and <=360. The azimuth convention is defined as degrees east of north (e.g. North = 0, South=180 East = 90, West = 270). dhi : numeric Diffuse horizontal irradiance in W/m^2. DHI must be >=0. dni : numeric Direct normal irradiance in W/m^2. DNI must be >=0. dni_extra : numeric Extraterrestrial normal irradiance in W/m^2. solar_zenith : numeric apparent (refraction-corrected) zenith angles in decimal degrees. solar_zenith must be >=0 and <=180. solar_azimuth : numeric Sun azimuth angles in decimal degrees. solar_azimuth must be >=0 and <=360. The azimuth convention is defined as degrees east of north (e.g. North = 0, East = 90, West = 270). airmass : numeric Relative (not pressure-corrected) airmass values. If AM is a DataFrame it must be of the same size as all other DataFrame inputs. AM must be >=0 (careful using the 1/sec(z) model of AM generation) model : string (optional, default='allsitescomposite1990') A string which selects the desired set of Perez coefficients. If model is not provided as an input, the default, '1990' will be used. All possible model selections are: * '1990' * 'allsitescomposite1990' (same as '1990') * 'allsitescomposite1988' * 'sandiacomposite1988' * 'usacomposite1988' * 'france1988' * 'phoenix1988' * 'elmonte1988' * 'osage1988' * 'albuquerque1988' * 'capecanaveral1988' * 'albany1988' return_components: bool (optional, default=False) Flag used to decide whether to return the calculated diffuse components or not. Returns -------- numeric, OrderedDict, or DataFrame Return type controlled by `return_components` argument. If ``return_components=False``, `sky_diffuse` is returned. If ``return_components=True``, `diffuse_components` is returned. sky_diffuse : numeric The sky diffuse component of the solar radiation on a tilted surface. diffuse_components : OrderedDict (array input) or DataFrame (Series input) Keys/columns are: * sky_diffuse: Total sky diffuse * isotropic * circumsolar * horizon References ---------- [1] Loutzenhiser P.G. et. al. \"Empirical validation of models to compute solar irradiance on inclined surfaces for building energy simulation\" 2007, Solar Energy vol. 81. pp. 254-267 [2] Perez, R., Seals, R., Ineichen, P., Stewart, R., Menicucci, D., 1987. A new simplified version of the Perez diffuse irradiance model for tilted surfaces. Solar Energy 39(3), 221-232. [3] Perez, R., Ineichen, P., Seals, R., Michalsky, J., Stewart, R., 1990. Modeling daylight availability and irradiance components from direct and global irradiance. Solar Energy 44 (5), 271-289. [4] Perez, R. et. al 1988. \"The Development and Verification of the Perez Diffuse Radiation Model\". SAND88-7030", "response": "def perez(surface_tilt, surface_azimuth, dhi, dni, dni_extra,\n          solar_zenith, solar_azimuth, airmass,\n          model='allsitescomposite1990', return_components=False):\n    '''\n    Determine diffuse irradiance from the sky on a tilted surface using\n    one of the Perez models.\n\n    Perez models determine the diffuse irradiance from the sky (ground\n    reflected irradiance is not included in this algorithm) on a tilted\n    surface using the surface tilt angle, surface azimuth angle, diffuse\n    horizontal irradiance, direct normal irradiance, extraterrestrial\n    irradiance, sun zenith angle, sun azimuth angle, and relative (not\n    pressure-corrected) airmass. Optionally a selector may be used to\n    use any of Perez's model coefficient sets.\n\n    Parameters\n    ----------\n    surface_tilt : numeric\n        Surface tilt angles in decimal degrees. surface_tilt must be >=0\n        and <=180. The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90)\n\n    surface_azimuth : numeric\n        Surface azimuth angles in decimal degrees. surface_azimuth must\n        be >=0 and <=360. The azimuth convention is defined as degrees\n        east of north (e.g. North = 0, South=180 East = 90, West = 270).\n\n    dhi : numeric\n        Diffuse horizontal irradiance in W/m^2. DHI must be >=0.\n\n    dni : numeric\n        Direct normal irradiance in W/m^2. DNI must be >=0.\n\n    dni_extra : numeric\n        Extraterrestrial normal irradiance in W/m^2.\n\n    solar_zenith : numeric\n        apparent (refraction-corrected) zenith angles in decimal\n        degrees. solar_zenith must be >=0 and <=180.\n\n    solar_azimuth : numeric\n        Sun azimuth angles in decimal degrees. solar_azimuth must be >=0\n        and <=360. The azimuth convention is defined as degrees east of\n        north (e.g. North = 0, East = 90, West = 270).\n\n    airmass : numeric\n        Relative (not pressure-corrected) airmass values. If AM is a\n        DataFrame it must be of the same size as all other DataFrame\n        inputs. AM must be >=0 (careful using the 1/sec(z) model of AM\n        generation)\n\n    model : string (optional, default='allsitescomposite1990')\n        A string which selects the desired set of Perez coefficients. If\n        model is not provided as an input, the default, '1990' will be\n        used. All possible model selections are:\n\n        * '1990'\n        * 'allsitescomposite1990' (same as '1990')\n        * 'allsitescomposite1988'\n        * 'sandiacomposite1988'\n        * 'usacomposite1988'\n        * 'france1988'\n        * 'phoenix1988'\n        * 'elmonte1988'\n        * 'osage1988'\n        * 'albuquerque1988'\n        * 'capecanaveral1988'\n        * 'albany1988'\n\n    return_components: bool (optional, default=False)\n        Flag used to decide whether to return the calculated diffuse components\n        or not.\n\n    Returns\n    --------\n    numeric, OrderedDict, or DataFrame\n        Return type controlled by `return_components` argument.\n        If ``return_components=False``, `sky_diffuse` is returned.\n        If ``return_components=True``, `diffuse_components` is returned.\n\n    sky_diffuse : numeric\n        The sky diffuse component of the solar radiation on a tilted\n        surface.\n\n    diffuse_components : OrderedDict (array input) or DataFrame (Series input)\n        Keys/columns are:\n            * sky_diffuse: Total sky diffuse\n            * isotropic\n            * circumsolar\n            * horizon\n\n\n    References\n    ----------\n    [1] Loutzenhiser P.G. et. al. \"Empirical validation of models to\n    compute solar irradiance on inclined surfaces for building energy\n    simulation\" 2007, Solar Energy vol. 81. pp. 254-267\n\n    [2] Perez, R., Seals, R., Ineichen, P., Stewart, R., Menicucci, D.,\n    1987. A new simplified version of the Perez diffuse irradiance model\n    for tilted surfaces. Solar Energy 39(3), 221-232.\n\n    [3] Perez, R., Ineichen, P., Seals, R., Michalsky, J., Stewart, R.,\n    1990. Modeling daylight availability and irradiance components from\n    direct and global irradiance. Solar Energy 44 (5), 271-289.\n\n    [4] Perez, R. et. al 1988. \"The Development and Verification of the\n    Perez Diffuse Radiation Model\". SAND88-7030\n    '''\n\n    kappa = 1.041  # for solar_zenith in radians\n    z = np.radians(solar_zenith)  # convert to radians\n\n    # delta is the sky's \"brightness\"\n    delta = dhi * airmass / dni_extra\n\n    # epsilon is the sky's \"clearness\"\n    with np.errstate(invalid='ignore'):\n        eps = ((dhi + dni) / dhi + kappa * (z ** 3)) / (1 + kappa * (z ** 3))\n\n    # numpy indexing below will not work with a Series\n    if isinstance(eps, pd.Series):\n        eps = eps.values\n\n    # Perez et al define clearness bins according to the following\n    # rules. 1 = overcast ... 8 = clear (these names really only make\n    # sense for small zenith angles, but...) these values will\n    # eventually be used as indicies for coeffecient look ups\n    ebin = np.digitize(eps, (0., 1.065, 1.23, 1.5, 1.95, 2.8, 4.5, 6.2))\n    ebin = np.array(ebin)  # GH 642\n    ebin[np.isnan(eps)] = 0\n\n    # correct for 0 indexing in coeffecient lookup\n    # later, ebin = -1 will yield nan coefficients\n    ebin -= 1\n\n    # The various possible sets of Perez coefficients are contained\n    # in a subfunction to clean up the code.\n    F1c, F2c = _get_perez_coefficients(model)\n\n    # results in invalid eps (ebin = -1) being mapped to nans\n    nans = np.array([np.nan, np.nan, np.nan])\n    F1c = np.vstack((F1c, nans))\n    F2c = np.vstack((F2c, nans))\n\n    F1 = (F1c[ebin, 0] + F1c[ebin, 1] * delta + F1c[ebin, 2] * z)\n    F1 = np.maximum(F1, 0)\n\n    F2 = (F2c[ebin, 0] + F2c[ebin, 1] * delta + F2c[ebin, 2] * z)\n    F2 = np.maximum(F2, 0)\n\n    A = aoi_projection(surface_tilt, surface_azimuth,\n                       solar_zenith, solar_azimuth)\n    A = np.maximum(A, 0)\n\n    B = tools.cosd(solar_zenith)\n    B = np.maximum(B, tools.cosd(85))\n\n    # Calculate Diffuse POA from sky dome\n    term1 = 0.5 * (1 - F1) * (1 + tools.cosd(surface_tilt))\n    term2 = F1 * A / B\n    term3 = F2 * tools.sind(surface_tilt)\n\n    sky_diffuse = np.maximum(dhi * (term1 + term2 + term3), 0)\n\n    # we've preserved the input type until now, so don't ruin it!\n    if isinstance(sky_diffuse, pd.Series):\n        sky_diffuse[np.isnan(airmass)] = 0\n    else:\n        sky_diffuse = np.where(np.isnan(airmass), 0, sky_diffuse)\n\n    if return_components:\n        diffuse_components = OrderedDict()\n        diffuse_components['sky_diffuse'] = sky_diffuse\n\n        # Calculate the different components\n        diffuse_components['isotropic'] = dhi * term1\n        diffuse_components['circumsolar'] = dhi * term2\n        diffuse_components['horizon'] = dhi * term3\n\n        # Set values of components to 0 when sky_diffuse is 0\n        mask = sky_diffuse == 0\n        if isinstance(sky_diffuse, pd.Series):\n            diffuse_components = pd.DataFrame(diffuse_components)\n            diffuse_components.loc[mask] = 0\n        else:\n            diffuse_components = {k: np.where(mask, 0, v) for k, v in\n                                  diffuse_components.items()}\n        return diffuse_components\n    else:\n        return sky_diffuse"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef clearsky_index(ghi, clearsky_ghi, max_clearsky_index=2.0):\n    clearsky_index = ghi / clearsky_ghi\n    # set +inf, -inf, and nans to zero\n    clearsky_index = np.where(~np.isfinite(clearsky_index), 0,\n                              clearsky_index)\n    # but preserve nans in the input arrays\n    input_is_nan = ~np.isfinite(ghi) | ~np.isfinite(clearsky_ghi)\n    clearsky_index = np.where(input_is_nan, np.nan, clearsky_index)\n\n    clearsky_index = np.maximum(clearsky_index, 0)\n    clearsky_index = np.minimum(clearsky_index, max_clearsky_index)\n\n    # preserve input type\n    if isinstance(ghi, pd.Series):\n        clearsky_index = pd.Series(clearsky_index, index=ghi.index)\n\n    return clearsky_index", "response": "Calculate the clearsky index."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the clearness index for a given global solar_zenith and extra_radiation.", "response": "def clearness_index(ghi, solar_zenith, extra_radiation, min_cos_zenith=0.065,\n                    max_clearness_index=2.0):\n    \"\"\"\n    Calculate the clearness index.\n\n    The clearness index is the ratio of global to extraterrestrial\n    irradiance on a horizontal plane.\n\n    Parameters\n    ----------\n    ghi : numeric\n        Global horizontal irradiance in W/m^2.\n\n    solar_zenith : numeric\n        True (not refraction-corrected) solar zenith angle in decimal\n        degrees.\n\n    extra_radiation : numeric\n        Irradiance incident at the top of the atmosphere\n\n    min_cos_zenith : numeric, default 0.065\n        Minimum value of cos(zenith) to allow when calculating global\n        clearness index `kt`. Equivalent to zenith = 86.273 degrees.\n\n    max_clearness_index : numeric, default 2.0\n        Maximum value of the clearness index. The default, 2.0, allows\n        for over-irradiance events typically seen in sub-hourly data.\n        NREL's SRRL Fortran code used 0.82 for hourly data.\n\n    Returns\n    -------\n    kt : numeric\n        Clearness index\n\n    References\n    ----------\n    .. [1] Maxwell, E. L., \"A Quasi-Physical Model for Converting Hourly\n           Global Horizontal to Direct Normal Insolation\", Technical\n           Report No. SERI/TR-215-3087, Golden, CO: Solar Energy Research\n           Institute, 1987.\n    \"\"\"\n    cos_zenith = tools.cosd(solar_zenith)\n    I0h = extra_radiation * np.maximum(cos_zenith, min_cos_zenith)\n    # consider adding\n    # with np.errstate(invalid='ignore', divide='ignore'):\n    # to kt calculation, but perhaps it's good to allow these\n    # warnings to the users that override min_cos_zenith\n    kt = ghi / I0h\n    kt = np.maximum(kt, 0)\n    kt = np.minimum(kt, max_clearness_index)\n    return kt"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef clearness_index_zenith_independent(clearness_index, airmass,\n                                       max_clearness_index=2.0):\n    \"\"\"\n    Calculate the zenith angle independent clearness index.\n\n    Parameters\n    ----------\n    clearness_index : numeric\n        Ratio of global to extraterrestrial irradiance on a horizontal\n        plane\n\n    airmass : numeric\n        Airmass\n\n    max_clearness_index : numeric, default 2.0\n        Maximum value of the clearness index. The default, 2.0, allows\n        for over-irradiance events typically seen in sub-hourly data.\n        NREL's SRRL Fortran code used 0.82 for hourly data.\n\n    Returns\n    -------\n    kt_prime : numeric\n        Zenith independent clearness index\n\n    References\n    ----------\n    .. [1] Perez, R., P. Ineichen, E. Maxwell, R. Seals and A. Zelenka,\n           (1992). \"Dynamic Global-to-Direct Irradiance Conversion Models\".\n           ASHRAE Transactions-Research Series, pp. 354-369\n    \"\"\"\n    # Perez eqn 1\n    kt_prime = clearness_index / _kt_kt_prime_factor(airmass)\n    kt_prime = np.maximum(kt_prime, 0)\n    kt_prime = np.minimum(kt_prime, max_clearness_index)\n    return kt_prime", "response": "Calculates the zenith angle independent clearness index."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nestimating Direct Normal Irradiance from Global Horizontal Irradiance using the DISC model. The DISC algorithm converts global horizontal irradiance to direct normal irradiance through empirical relationships between the global and direct clearness indices. The pvlib implementation limits the clearness index to 1. The original report describing the DISC model [1]_ uses the relative airmass rather than the absolute (pressure-corrected) airmass. However, the NREL implementation of the DISC model [2]_ uses absolute airmass. PVLib Matlab also uses the absolute airmass. pvlib python defaults to absolute airmass, but the relative airmass can be used by supplying `pressure=None`. Parameters ---------- ghi : numeric Global horizontal irradiance in W/m^2. solar_zenith : numeric True (not refraction-corrected) solar zenith angles in decimal degrees. datetime_or_doy : int, float, array, pd.DatetimeIndex Day of year or array of days of year e.g. pd.DatetimeIndex.dayofyear, or pd.DatetimeIndex. pressure : None or numeric, default 101325 Site pressure in Pascal. If None, relative airmass is used instead of absolute (pressure-corrected) airmass. min_cos_zenith : numeric, default 0.065 Minimum value of cos(zenith) to allow when calculating global clearness index `kt`. Equivalent to zenith = 86.273 degrees. max_zenith : numeric, default 87 Maximum value of zenith to allow in DNI calculation. DNI will be set to 0 for times with zenith values greater than `max_zenith`. max_airmass : numeric, default 12 Maximum value of the airmass to allow in Kn calculation. Default value (12) comes from range over which Kn was fit to airmass in the original paper. Returns ------- output : OrderedDict or DataFrame Contains the following keys: * ``dni``: The modeled direct normal irradiance in W/m^2 provided by the Direct Insolation Simulation Code (DISC) model. * ``kt``: Ratio of global to extraterrestrial irradiance on a horizontal plane. * ``airmass``: Airmass References ---------- .. [1] Maxwell, E. L., \"A Quasi-Physical Model for Converting Hourly Global Horizontal to Direct Normal Insolation\", Technical Report No. SERI/TR-215-3087, Golden, CO: Solar Energy Research Institute, 1987. .. [2] Maxwell, E. \"DISC Model\", Excel Worksheet. https://www.nrel.gov/grid/solar-resource/disc.html See Also -------- dirint", "response": "def disc(ghi, solar_zenith, datetime_or_doy, pressure=101325,\n         min_cos_zenith=0.065, max_zenith=87, max_airmass=12):\n    \"\"\"\n    Estimate Direct Normal Irradiance from Global Horizontal Irradiance\n    using the DISC model.\n\n    The DISC algorithm converts global horizontal irradiance to direct\n    normal irradiance through empirical relationships between the global\n    and direct clearness indices.\n\n    The pvlib implementation limits the clearness index to 1.\n\n    The original report describing the DISC model [1]_ uses the\n    relative airmass rather than the absolute (pressure-corrected)\n    airmass. However, the NREL implementation of the DISC model [2]_\n    uses absolute airmass. PVLib Matlab also uses the absolute airmass.\n    pvlib python defaults to absolute airmass, but the relative airmass\n    can be used by supplying `pressure=None`.\n\n    Parameters\n    ----------\n    ghi : numeric\n        Global horizontal irradiance in W/m^2.\n\n    solar_zenith : numeric\n        True (not refraction-corrected) solar zenith angles in decimal\n        degrees.\n\n    datetime_or_doy : int, float, array, pd.DatetimeIndex\n        Day of year or array of days of year e.g.\n        pd.DatetimeIndex.dayofyear, or pd.DatetimeIndex.\n\n    pressure : None or numeric, default 101325\n        Site pressure in Pascal. If None, relative airmass is used\n        instead of absolute (pressure-corrected) airmass.\n\n    min_cos_zenith : numeric, default 0.065\n        Minimum value of cos(zenith) to allow when calculating global\n        clearness index `kt`. Equivalent to zenith = 86.273 degrees.\n\n    max_zenith : numeric, default 87\n        Maximum value of zenith to allow in DNI calculation. DNI will be\n        set to 0 for times with zenith values greater than `max_zenith`.\n\n    max_airmass : numeric, default 12\n        Maximum value of the airmass to allow in Kn calculation.\n        Default value (12) comes from range over which Kn was fit\n        to airmass in the original paper.\n\n    Returns\n    -------\n    output : OrderedDict or DataFrame\n        Contains the following keys:\n\n        * ``dni``: The modeled direct normal irradiance\n          in W/m^2 provided by the\n          Direct Insolation Simulation Code (DISC) model.\n        * ``kt``: Ratio of global to extraterrestrial\n          irradiance on a horizontal plane.\n        * ``airmass``: Airmass\n\n    References\n    ----------\n    .. [1] Maxwell, E. L., \"A Quasi-Physical Model for Converting Hourly\n       Global Horizontal to Direct Normal Insolation\", Technical\n       Report No. SERI/TR-215-3087, Golden, CO: Solar Energy Research\n       Institute, 1987.\n\n    .. [2] Maxwell, E. \"DISC Model\", Excel Worksheet.\n       https://www.nrel.gov/grid/solar-resource/disc.html\n\n    See Also\n    --------\n    dirint\n    \"\"\"\n\n    # this is the I0 calculation from the reference\n    # SSC uses solar constant = 1367.0 (checked 2018 08 15)\n    I0 = get_extra_radiation(datetime_or_doy, 1370., 'spencer')\n\n    kt = clearness_index(ghi, solar_zenith, I0, min_cos_zenith=min_cos_zenith,\n                         max_clearness_index=1)\n\n    am = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')\n    if pressure is not None:\n        am = atmosphere.get_absolute_airmass(am, pressure)\n\n    Kn, am = _disc_kn(kt, am, max_airmass=max_airmass)\n    dni = Kn * I0\n\n    bad_values = (solar_zenith > max_zenith) | (ghi < 0) | (dni < 0)\n    dni = np.where(bad_values, 0, dni)\n\n    output = OrderedDict()\n    output['dni'] = dni\n    output['kt'] = kt\n    output['airmass'] = am\n\n    if isinstance(datetime_or_doy, pd.DatetimeIndex):\n        output = pd.DataFrame(output, index=datetime_or_doy)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncalculating Kn for disc based on clearness index and airmass.", "response": "def _disc_kn(clearness_index, airmass, max_airmass=12):\n    \"\"\"\n    Calculate Kn for `disc`\n\n    Parameters\n    ----------\n    clearness_index : numeric\n    airmass : numeric\n    max_airmass : float\n        airmass > max_airmass is set to max_airmass before being used\n        in calculating Kn.\n\n    Returns\n    -------\n    Kn : numeric\n    am : numeric\n        airmass used in the calculation of Kn. am <= max_airmass.\n    \"\"\"\n    # short names for equations\n    kt = clearness_index\n    am = airmass\n\n    am = np.minimum(am, max_airmass)  # GH 450\n\n    # powers of kt will be used repeatedly, so compute only once\n    kt2 = kt * kt  # about the same as kt ** 2\n    kt3 = kt2 * kt  # 5-10x faster than kt ** 3\n\n    bools = (kt <= 0.6)\n    a = np.where(bools,\n                 0.512 - 1.56*kt + 2.286*kt2 - 2.222*kt3,\n                 -5.743 + 21.77*kt - 27.49*kt2 + 11.56*kt3)\n    b = np.where(bools,\n                 0.37 + 0.962*kt,\n                 41.4 - 118.5*kt + 66.05*kt2 + 31.9*kt3)\n    c = np.where(bools,\n                 -0.28 + 0.932*kt - 2.048*kt2,\n                 -47.01 + 184.2*kt - 222.0*kt2 + 73.81*kt3)\n\n    delta_kn = a + b * np.exp(c*am)\n\n    Knc = 0.866 - 0.122*am + 0.0121*am**2 - 0.000653*am**3 + 1.4e-05*am**4\n    Kn = Knc - delta_kn\n    return Kn, am"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine the DNI from a GHI using the DISC model.", "response": "def dirint(ghi, solar_zenith, times, pressure=101325., use_delta_kt_prime=True,\n           temp_dew=None, min_cos_zenith=0.065, max_zenith=87):\n    \"\"\"\n    Determine DNI from GHI using the DIRINT modification of the DISC\n    model.\n\n    Implements the modified DISC model known as \"DIRINT\" introduced in\n    [1]. DIRINT predicts direct normal irradiance (DNI) from measured\n    global horizontal irradiance (GHI). DIRINT improves upon the DISC\n    model by using time-series GHI data and dew point temperature\n    information. The effectiveness of the DIRINT model improves with\n    each piece of information provided.\n\n    The pvlib implementation limits the clearness index to 1.\n\n    Parameters\n    ----------\n    ghi : array-like\n        Global horizontal irradiance in W/m^2.\n\n    solar_zenith : array-like\n        True (not refraction-corrected) solar_zenith angles in decimal\n        degrees.\n\n    times : DatetimeIndex\n\n    pressure : float or array-like, default 101325.0\n        The site pressure in Pascal. Pressure may be measured or an\n        average pressure may be calculated from site altitude.\n\n    use_delta_kt_prime : bool, default True\n        If True, indicates that the stability index delta_kt_prime is\n        included in the model. The stability index adjusts the estimated\n        DNI in response to dynamics in the time series of GHI. It is\n        recommended that delta_kt_prime is not used if the time between\n        GHI points is 1.5 hours or greater. If use_delta_kt_prime=True,\n        input data must be Series.\n\n    temp_dew : None, float, or array-like, default None\n        Surface dew point temperatures, in degrees C. Values of temp_dew\n        may be numeric or NaN. Any single time period point with a\n        temp_dew=NaN does not have dew point improvements applied. If\n        temp_dew is not provided, then dew point improvements are not\n        applied.\n\n    min_cos_zenith : numeric, default 0.065\n        Minimum value of cos(zenith) to allow when calculating global\n        clearness index `kt`. Equivalent to zenith = 86.273 degrees.\n\n    max_zenith : numeric, default 87\n        Maximum value of zenith to allow in DNI calculation. DNI will be\n        set to 0 for times with zenith values greater than `max_zenith`.\n\n    Returns\n    -------\n    dni : array-like\n        The modeled direct normal irradiance in W/m^2 provided by the\n        DIRINT model.\n\n    Notes\n    -----\n    DIRINT model requires time series data (ie. one of the inputs must\n    be a vector of length > 2).\n\n    References\n    ----------\n    [1] Perez, R., P. Ineichen, E. Maxwell, R. Seals and A. Zelenka,\n    (1992). \"Dynamic Global-to-Direct Irradiance Conversion Models\".\n    ASHRAE Transactions-Research Series, pp. 354-369\n\n    [2] Maxwell, E. L., \"A Quasi-Physical Model for Converting Hourly\n    Global Horizontal to Direct Normal Insolation\", Technical Report No.\n    SERI/TR-215-3087, Golden, CO: Solar Energy Research Institute, 1987.\n    \"\"\"\n\n    disc_out = disc(ghi, solar_zenith, times, pressure=pressure,\n                    min_cos_zenith=min_cos_zenith, max_zenith=max_zenith)\n    airmass = disc_out['airmass']\n    kt = disc_out['kt']\n\n    kt_prime = clearness_index_zenith_independent(\n        kt, airmass, max_clearness_index=1)\n    delta_kt_prime = _delta_kt_prime_dirint(kt_prime, use_delta_kt_prime,\n                                            times)\n    w = _temp_dew_dirint(temp_dew, times)\n\n    dirint_coeffs = _dirint_coeffs(times, kt_prime, solar_zenith, w,\n                                   delta_kt_prime)\n\n    # Perez eqn 5\n    dni = disc_out['dni'] * dirint_coeffs\n\n    return dni"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _dirint_from_dni_ktprime(dni, kt_prime, solar_zenith, use_delta_kt_prime,\n                             temp_dew):\n    \"\"\"\n    Calculate DIRINT DNI from supplied DISC DNI and Kt'.\n\n    Supports :py:func:`gti_dirint`\n    \"\"\"\n    times = dni.index\n    delta_kt_prime = _delta_kt_prime_dirint(kt_prime, use_delta_kt_prime,\n                                            times)\n    w = _temp_dew_dirint(temp_dew, times)\n    dirint_coeffs = _dirint_coeffs(times, kt_prime, solar_zenith, w,\n                                   delta_kt_prime)\n    dni_dirint = dni * dirint_coeffs\n    return dni_dirint", "response": "Calculate DIRINT DNI from supplied DISC DNI and Kt."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _delta_kt_prime_dirint(kt_prime, use_delta_kt_prime, times):\n    if use_delta_kt_prime:\n        # Perez eqn 2\n        kt_next = kt_prime.shift(-1)\n        kt_previous = kt_prime.shift(1)\n        # replace nan with values that implement Perez Eq 3 for first and last\n        # positions. Use kt_previous and kt_next to handle series of length 1\n        kt_next.iloc[-1] = kt_previous.iloc[-1]\n        kt_previous.iloc[0] = kt_next.iloc[0]\n        delta_kt_prime = 0.5 * ((kt_prime - kt_next).abs().add(\n                                (kt_prime - kt_previous).abs(),\n                                fill_value=0))\n    else:\n        # do not change unless also modifying _dirint_bins\n        delta_kt_prime = pd.Series(-1, index=times)\n    return delta_kt_prime", "response": "Calculate delta_kt_prime for use with _dirint_bins."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _temp_dew_dirint(temp_dew, times):\n    if temp_dew is not None:\n        # Perez eqn 4\n        w = pd.Series(np.exp(0.07 * temp_dew - 0.075), index=times)\n    else:\n        # do not change unless also modifying _dirint_bins\n        w = pd.Series(-1, index=times)\n    return w", "response": "Calculate precipitable water from surface dew point temp"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndetermine the DISC to DIRINT multiplier dirint_coeffs.", "response": "def _dirint_coeffs(times, kt_prime, solar_zenith, w, delta_kt_prime):\n    \"\"\"\n    Determine the DISC to DIRINT multiplier `dirint_coeffs`.\n\n    dni = disc_out['dni'] * dirint_coeffs\n\n    Parameters\n    ----------\n    times : pd.DatetimeIndex\n    kt_prime : Zenith-independent clearness index\n    solar_zenith : Solar zenith angle\n    w : precipitable water estimated from surface dew-point temperature\n    delta_kt_prime : stability index\n\n    Returns\n    -------\n    dirint_coeffs : array-like\n    \"\"\"\n    kt_prime_bin, zenith_bin, w_bin, delta_kt_prime_bin = \\\n        _dirint_bins(times, kt_prime, solar_zenith, w, delta_kt_prime)\n\n    # get the coefficients\n    coeffs = _get_dirint_coeffs()\n\n    # subtract 1 to account for difference between MATLAB-style bin\n    # assignment and Python-style array lookup.\n    dirint_coeffs = coeffs[kt_prime_bin-1, zenith_bin-1,\n                           delta_kt_prime_bin-1, w_bin-1]\n\n    # convert unassigned bins to nan\n    dirint_coeffs = np.where((kt_prime_bin == 0) | (zenith_bin == 0) |\n                             (w_bin == 0) | (delta_kt_prime_bin == 0),\n                             np.nan, dirint_coeffs)\n    return dirint_coeffs"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _dirint_bins(times, kt_prime, zenith, w, delta_kt_prime):\n    # @wholmgren: the following bin assignments use MATLAB's 1-indexing.\n    # Later, we'll subtract 1 to conform to Python's 0-indexing.\n\n    # Create kt_prime bins\n    kt_prime_bin = pd.Series(0, index=times, dtype=np.int64)\n    kt_prime_bin[(kt_prime >= 0) & (kt_prime < 0.24)] = 1\n    kt_prime_bin[(kt_prime >= 0.24) & (kt_prime < 0.4)] = 2\n    kt_prime_bin[(kt_prime >= 0.4) & (kt_prime < 0.56)] = 3\n    kt_prime_bin[(kt_prime >= 0.56) & (kt_prime < 0.7)] = 4\n    kt_prime_bin[(kt_prime >= 0.7) & (kt_prime < 0.8)] = 5\n    kt_prime_bin[(kt_prime >= 0.8) & (kt_prime <= 1)] = 6\n\n    # Create zenith angle bins\n    zenith_bin = pd.Series(0, index=times, dtype=np.int64)\n    zenith_bin[(zenith >= 0) & (zenith < 25)] = 1\n    zenith_bin[(zenith >= 25) & (zenith < 40)] = 2\n    zenith_bin[(zenith >= 40) & (zenith < 55)] = 3\n    zenith_bin[(zenith >= 55) & (zenith < 70)] = 4\n    zenith_bin[(zenith >= 70) & (zenith < 80)] = 5\n    zenith_bin[(zenith >= 80)] = 6\n\n    # Create the bins for w based on dew point temperature\n    w_bin = pd.Series(0, index=times, dtype=np.int64)\n    w_bin[(w >= 0) & (w < 1)] = 1\n    w_bin[(w >= 1) & (w < 2)] = 2\n    w_bin[(w >= 2) & (w < 3)] = 3\n    w_bin[(w >= 3)] = 4\n    w_bin[(w == -1)] = 5\n\n    # Create delta_kt_prime binning.\n    delta_kt_prime_bin = pd.Series(0, index=times, dtype=np.int64)\n    delta_kt_prime_bin[(delta_kt_prime >= 0) & (delta_kt_prime < 0.015)] = 1\n    delta_kt_prime_bin[(delta_kt_prime >= 0.015) &\n                       (delta_kt_prime < 0.035)] = 2\n    delta_kt_prime_bin[(delta_kt_prime >= 0.035) & (delta_kt_prime < 0.07)] = 3\n    delta_kt_prime_bin[(delta_kt_prime >= 0.07) & (delta_kt_prime < 0.15)] = 4\n    delta_kt_prime_bin[(delta_kt_prime >= 0.15) & (delta_kt_prime < 0.3)] = 5\n    delta_kt_prime_bin[(delta_kt_prime >= 0.3) & (delta_kt_prime <= 1)] = 6\n    delta_kt_prime_bin[delta_kt_prime == -1] = 7\n\n    return kt_prime_bin, zenith_bin, w_bin, delta_kt_prime_bin", "response": "Determines the bins for the DIRINT coefficients."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dirindex(ghi, ghi_clearsky, dni_clearsky, zenith, times, pressure=101325.,\n             use_delta_kt_prime=True, temp_dew=None, min_cos_zenith=0.065,\n             max_zenith=87):\n    \"\"\"\n    Determine DNI from GHI using the DIRINDEX model, which is a modification of\n    the DIRINT model with information from a clear sky model.\n\n    DIRINDEX [1] improves upon the DIRINT model by taking into account\n    turbidity when used with the Ineichen clear sky model results.\n\n    The pvlib implementation limits the clearness index to 1.\n\n    Parameters\n    ----------\n    ghi : array-like\n        Global horizontal irradiance in W/m^2.\n\n    ghi_clearsky : array-like\n        Global horizontal irradiance from clear sky model, in W/m^2.\n\n    dni_clearsky : array-like\n        Direct normal irradiance from clear sky model, in W/m^2.\n\n    zenith : array-like\n        True (not refraction-corrected) zenith angles in decimal\n        degrees. If Z is a vector it must be of the same size as all\n        other vector inputs. Z must be >=0 and <=180.\n\n    times : DatetimeIndex\n\n    pressure : float or array-like, default 101325.0\n        The site pressure in Pascal. Pressure may be measured or an\n        average pressure may be calculated from site altitude.\n\n    use_delta_kt_prime : bool, default True\n        If True, indicates that the stability index delta_kt_prime is\n        included in the model. The stability index adjusts the estimated\n        DNI in response to dynamics in the time series of GHI. It is\n        recommended that delta_kt_prime is not used if the time between\n        GHI points is 1.5 hours or greater. If use_delta_kt_prime=True,\n        input data must be Series.\n\n    temp_dew : None, float, or array-like, default None\n        Surface dew point temperatures, in degrees C. Values of temp_dew\n        may be numeric or NaN. Any single time period point with a\n        temp_dew=NaN does not have dew point improvements applied. If\n        temp_dew is not provided, then dew point improvements are not\n        applied.\n\n    min_cos_zenith : numeric, default 0.065\n        Minimum value of cos(zenith) to allow when calculating global\n        clearness index `kt`. Equivalent to zenith = 86.273 degrees.\n\n    max_zenith : numeric, default 87\n        Maximum value of zenith to allow in DNI calculation. DNI will be\n        set to 0 for times with zenith values greater than `max_zenith`.\n\n    Returns\n    -------\n    dni : array-like\n        The modeled direct normal irradiance in W/m^2.\n\n    Notes\n    -----\n    DIRINDEX model requires time series data (ie. one of the inputs must\n    be a vector of length > 2).\n\n    References\n    ----------\n    [1] Perez, R., Ineichen, P., Moore, K., Kmiecik, M., Chain, C., George, R.,\n    & Vignola, F. (2002). A new operational model for satellite-derived\n    irradiances: description and validation. Solar Energy, 73(5), 307-317.\n    \"\"\"\n\n    dni_dirint = dirint(ghi, zenith, times, pressure=pressure,\n                        use_delta_kt_prime=use_delta_kt_prime,\n                        temp_dew=temp_dew, min_cos_zenith=min_cos_zenith,\n                        max_zenith=max_zenith)\n\n    dni_dirint_clearsky = dirint(ghi_clearsky, zenith, times,\n                                 pressure=pressure,\n                                 use_delta_kt_prime=use_delta_kt_prime,\n                                 temp_dew=temp_dew,\n                                 min_cos_zenith=min_cos_zenith,\n                                 max_zenith=max_zenith)\n\n    dni_dirindex = dni_clearsky * dni_dirint / dni_dirint_clearsky\n\n    dni_dirindex[dni_dirindex < 0] = 0.\n\n    return dni_dirindex", "response": "This function calculates the stability index of a set of GHIs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining GHI, DNI, DHI from POA global using the GTI DIRINT model. .. warning:: Model performance is poor for AOI greater than approximately 80 degrees `and` plane of array irradiance greater than approximately 200 W/m^2. Parameters ---------- poa_global : array-like Plane of array global irradiance in W/m^2. aoi : array-like Angle of incidence of solar rays with respect to the module surface normal. solar_zenith : array-like True (not refraction-corrected) solar zenith angles in decimal degrees. solar_azimuth : array-like Solar azimuth angles in decimal degrees. times : DatetimeIndex Time indices for the input array-like data. surface_tilt : numeric Surface tilt angles in decimal degrees. Tilt must be >=0 and <=180. The tilt angle is defined as degrees from horizontal (e.g. surface facing up = 0, surface facing horizon = 90). surface_azimuth : numeric Surface azimuth angles in decimal degrees. surface_azimuth must be >=0 and <=360. The Azimuth convention is defined as degrees east of north (e.g. North = 0, South=180 East = 90, West = 270). pressure : numeric, default 101325.0 The site pressure in Pascal. Pressure may be measured or an average pressure may be calculated from site altitude. use_delta_kt_prime : bool, default True If True, indicates that the stability index delta_kt_prime is included in the model. The stability index adjusts the estimated DNI in response to dynamics in the time series of GHI. It is recommended that delta_kt_prime is not used if the time between GHI points is 1.5 hours or greater. If use_delta_kt_prime=True, input data must be Series. temp_dew : None, float, or array-like, default None Surface dew point temperatures, in degrees C. Values of temp_dew may be numeric or NaN. Any single time period point with a temp_dew=NaN does not have dew point improvements applied. If temp_dew is not provided, then dew point improvements are not applied. albedo : numeric, default 0.25 Surface albedo model : String, default 'isotropic' Irradiance model. model_perez : String, default 'allsitescomposite1990' Used only if model='perez'. See :py:func:`perez`. calculate_gt_90 : bool, default True Controls if the algorithm evaluates inputs with AOI >= 90 degrees. If False, returns nan for AOI >= 90 degrees. Significant speed ups can be achieved by setting this parameter to False. max_iterations : int, default 30 Maximum number of iterations for the aoi < 90 deg algorithm. Returns ------- data : OrderedDict or DataFrame Contains the following keys/columns: * ``ghi``: the modeled global horizontal irradiance in W/m^2. * ``dni``: the modeled direct normal irradiance in W/m^2. * ``dhi``: the modeled diffuse horizontal irradiance in W/m^2. References ---------- .. [1] B. Marion, A model for deriving the direct normal and diffuse horizontal irradiance from the global tilted irradiance, Solar Energy 122, 1037-1046. :doi:`10.1016/j.solener.2015.10.024`", "response": "def gti_dirint(poa_global, aoi, solar_zenith, solar_azimuth, times,\n               surface_tilt, surface_azimuth, pressure=101325.,\n               use_delta_kt_prime=True, temp_dew=None, albedo=.25,\n               model='perez', model_perez='allsitescomposite1990',\n               calculate_gt_90=True, max_iterations=30):\n    \"\"\"\n    Determine GHI, DNI, DHI from POA global using the GTI DIRINT model.\n\n    .. warning::\n\n        Model performance is poor for AOI greater than approximately\n        80 degrees `and` plane of array irradiance greater than\n        approximately 200 W/m^2.\n\n    Parameters\n    ----------\n    poa_global : array-like\n        Plane of array global irradiance in W/m^2.\n\n    aoi : array-like\n        Angle of incidence of solar rays with respect to the module\n        surface normal.\n\n    solar_zenith : array-like\n        True (not refraction-corrected) solar zenith angles in decimal\n        degrees.\n\n    solar_azimuth : array-like\n        Solar azimuth angles in decimal degrees.\n\n    times : DatetimeIndex\n        Time indices for the input array-like data.\n\n    surface_tilt : numeric\n        Surface tilt angles in decimal degrees. Tilt must be >=0 and\n        <=180. The tilt angle is defined as degrees from horizontal\n        (e.g. surface facing up = 0, surface facing horizon = 90).\n\n    surface_azimuth : numeric\n        Surface azimuth angles in decimal degrees. surface_azimuth must\n        be >=0 and <=360. The Azimuth convention is defined as degrees\n        east of north (e.g. North = 0, South=180 East = 90, West = 270).\n\n    pressure : numeric, default 101325.0\n        The site pressure in Pascal. Pressure may be measured or an\n        average pressure may be calculated from site altitude.\n\n    use_delta_kt_prime : bool, default True\n        If True, indicates that the stability index delta_kt_prime is\n        included in the model. The stability index adjusts the estimated\n        DNI in response to dynamics in the time series of GHI. It is\n        recommended that delta_kt_prime is not used if the time between\n        GHI points is 1.5 hours or greater. If use_delta_kt_prime=True,\n        input data must be Series.\n\n    temp_dew : None, float, or array-like, default None\n        Surface dew point temperatures, in degrees C. Values of temp_dew\n        may be numeric or NaN. Any single time period point with a\n        temp_dew=NaN does not have dew point improvements applied. If\n        temp_dew is not provided, then dew point improvements are not\n        applied.\n\n    albedo : numeric, default 0.25\n        Surface albedo\n\n    model : String, default 'isotropic'\n        Irradiance model.\n\n    model_perez : String, default 'allsitescomposite1990'\n        Used only if model='perez'. See :py:func:`perez`.\n\n    calculate_gt_90 : bool, default True\n        Controls if the algorithm evaluates inputs with AOI >= 90 degrees.\n        If False, returns nan for AOI >= 90 degrees. Significant speed ups\n        can be achieved by setting this parameter to False.\n\n    max_iterations : int, default 30\n        Maximum number of iterations for the aoi < 90 deg algorithm.\n\n    Returns\n    -------\n    data : OrderedDict or DataFrame\n        Contains the following keys/columns:\n\n            * ``ghi``: the modeled global horizontal irradiance in W/m^2.\n            * ``dni``: the modeled direct normal irradiance in W/m^2.\n            * ``dhi``: the modeled diffuse horizontal irradiance in\n              W/m^2.\n\n    References\n    ----------\n    .. [1] B. Marion, A model for deriving the direct normal and\n           diffuse horizontal irradiance from the global tilted\n           irradiance, Solar Energy 122, 1037-1046.\n           :doi:`10.1016/j.solener.2015.10.024`\n    \"\"\"\n\n    aoi_lt_90 = aoi < 90\n\n    # for AOI less than 90 degrees\n    ghi, dni, dhi, kt_prime = _gti_dirint_lt_90(\n        poa_global, aoi, aoi_lt_90, solar_zenith, solar_azimuth, times,\n        surface_tilt, surface_azimuth, pressure=pressure,\n        use_delta_kt_prime=use_delta_kt_prime, temp_dew=temp_dew,\n        albedo=albedo, model=model, model_perez=model_perez,\n        max_iterations=max_iterations)\n\n    # for AOI greater than or equal to 90 degrees\n    if calculate_gt_90:\n        ghi_gte_90, dni_gte_90, dhi_gte_90 = _gti_dirint_gte_90(\n            poa_global, aoi, solar_zenith, solar_azimuth,\n            surface_tilt, times, kt_prime,\n            pressure=pressure, temp_dew=temp_dew, albedo=albedo)\n    else:\n        ghi_gte_90, dni_gte_90, dhi_gte_90 = np.nan, np.nan, np.nan\n\n    # put the AOI < 90 and AOI >= 90 conditions together\n    output = OrderedDict()\n    output['ghi'] = ghi.where(aoi_lt_90, ghi_gte_90)\n    output['dni'] = dni.where(aoi_lt_90, dni_gte_90)\n    output['dhi'] = dhi.where(aoi_lt_90, dhi_gte_90)\n\n    output = pd.DataFrame(output, index=times)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _gti_dirint_gte_90(poa_global, aoi, solar_zenith, solar_azimuth,\n                       surface_tilt, times, kt_prime,\n                       pressure=101325., temp_dew=None, albedo=.25):\n    \"\"\"\n    GTI-DIRINT model for AOI >= 90 degrees. See Marion 2015 Section 2.2.\n\n    See gti_dirint signature for parameter details.\n    \"\"\"\n    kt_prime_gte_90 = _gti_dirint_gte_90_kt_prime(aoi, solar_zenith,\n                                                  solar_azimuth, times,\n                                                  kt_prime)\n\n    I0 = get_extra_radiation(times, 1370, 'spencer')\n    airmass = atmosphere.get_relative_airmass(solar_zenith, model='kasten1966')\n    airmass = atmosphere.get_absolute_airmass(airmass, pressure)\n    kt = kt_prime_gte_90 * _kt_kt_prime_factor(airmass)\n    disc_dni = np.maximum(_disc_kn(kt, airmass)[0] * I0, 0)\n\n    dni_gte_90 = _dirint_from_dni_ktprime(disc_dni, kt_prime, solar_zenith,\n                                          False, temp_dew)\n\n    dni_gte_90_proj = dni_gte_90 * tools.cosd(solar_zenith)\n    cos_surface_tilt = tools.cosd(surface_tilt)\n\n    # isotropic sky plus ground diffuse\n    dhi_gte_90 = (\n        (2 * poa_global - dni_gte_90_proj * albedo * (1 - cos_surface_tilt)) /\n        (1 + cos_surface_tilt + albedo * (1 - cos_surface_tilt)))\n\n    ghi_gte_90 = dni_gte_90_proj + dhi_gte_90\n\n    return ghi_gte_90, dni_gte_90, dhi_gte_90", "response": "GTI - DIRINT model for AOI > 90 degrees."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _gti_dirint_gte_90_kt_prime(aoi, solar_zenith, solar_azimuth, times,\n                                kt_prime):\n    \"\"\"\n    Determine kt' values to be used in GTI-DIRINT AOI >= 90 deg case.\n    See Marion 2015 Section 2.2.\n\n    For AOI >= 90 deg: average of the kt_prime values for 65 < AOI < 80\n    in each day's morning and afternoon. Morning and afternoon are treated\n    separately.\n\n    For AOI < 90 deg: NaN.\n\n    See gti_dirint signature for parameter details.\n\n    Returns\n    -------\n    kt_prime_gte_90 : Series\n        Index is `times`.\n    \"\"\"\n    # kt_prime values from DIRINT calculation for AOI < 90 case\n    # set the kt_prime from sunrise to AOI=90 to be equal to\n    # the kt_prime for 65 < AOI < 80 during the morning.\n    # similar for the afternoon. repeat for every day.\n    aoi_gte_90 = aoi >= 90\n    aoi_65_80 = (aoi > 65) & (aoi < 80)\n    zenith_lt_90 = solar_zenith < 90\n    morning = solar_azimuth < 180\n    afternoon = solar_azimuth > 180\n    aoi_65_80_morning = aoi_65_80 & morning\n    aoi_65_80_afternoon = aoi_65_80 & afternoon\n    zenith_lt_90_aoi_gte_90_morning = zenith_lt_90 & aoi_gte_90 & morning\n    zenith_lt_90_aoi_gte_90_afternoon = zenith_lt_90 & aoi_gte_90 & afternoon\n\n    kt_prime_gte_90 = []\n    for date, data in kt_prime.groupby(times.date):\n        kt_prime_am_avg = data[aoi_65_80_morning].mean()\n        kt_prime_pm_avg = data[aoi_65_80_afternoon].mean()\n\n        kt_prime_by_date = pd.Series(np.nan, index=data.index)\n        kt_prime_by_date[zenith_lt_90_aoi_gte_90_morning] = kt_prime_am_avg\n        kt_prime_by_date[zenith_lt_90_aoi_gte_90_afternoon] = kt_prime_pm_avg\n        kt_prime_gte_90.append(kt_prime_by_date)\n    kt_prime_gte_90 = pd.concat(kt_prime_gte_90)\n\n    return kt_prime_gte_90", "response": "Determine kt values to be used in GTI - DIRINT AOI > 90 deg case."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef liujordan(zenith, transmittance, airmass, dni_extra=1367.0):\n    '''\n    Determine DNI, DHI, GHI from extraterrestrial flux, transmittance,\n    and optical air mass number.\n\n    Liu and Jordan, 1960, developed a simplified direct radiation model.\n    DHI is from an empirical equation for diffuse radiation from Liu and\n    Jordan, 1960.\n\n    Parameters\n    ----------\n    zenith: pd.Series\n        True (not refraction-corrected) zenith angles in decimal\n        degrees. If Z is a vector it must be of the same size as all\n        other vector inputs. Z must be >=0 and <=180.\n\n    transmittance: float\n        Atmospheric transmittance between 0 and 1.\n\n    pressure: float, default 101325.0\n        Air pressure\n\n    dni_extra: float, default 1367.0\n        Direct irradiance incident at the top of the atmosphere.\n\n    Returns\n    -------\n    irradiance: DataFrame\n        Modeled direct normal irradiance, direct horizontal irradiance,\n        and global horizontal irradiance in W/m^2\n\n    References\n    ----------\n    [1] Campbell, G. S., J. M. Norman (1998) An Introduction to\n    Environmental Biophysics. 2nd Ed. New York: Springer.\n\n    [2] Liu, B. Y., R. C. Jordan, (1960). \"The interrelationship and\n    characteristic distribution of direct, diffuse, and total solar\n    radiation\".  Solar Energy 4:1-19\n    '''\n\n    tau = transmittance\n\n    dni = dni_extra*tau**airmass\n    dhi = 0.3 * (1.0 - tau**airmass) * dni_extra * np.cos(np.radians(zenith))\n    ghi = dhi + dni * np.cos(np.radians(zenith))\n\n    irrads = OrderedDict()\n    irrads['ghi'] = ghi\n    irrads['dni'] = dni\n    irrads['dhi'] = dhi\n\n    if isinstance(ghi, pd.Series):\n        irrads = pd.DataFrame(irrads)\n\n    return irrads", "response": "Return a base class for a single liujordan system."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the coefficients for the Perez model in the base - tree.", "response": "def _get_perez_coefficients(perezmodel):\n    '''\n    Find coefficients for the Perez model\n\n    Parameters\n    ----------\n\n    perezmodel : string (optional, default='allsitescomposite1990')\n\n          a character string which selects the desired set of Perez\n          coefficients. If model is not provided as an input, the default,\n          '1990' will be used.\n\n    All possible model selections are:\n\n          * '1990'\n          * 'allsitescomposite1990' (same as '1990')\n          * 'allsitescomposite1988'\n          * 'sandiacomposite1988'\n          * 'usacomposite1988'\n          * 'france1988'\n          * 'phoenix1988'\n          * 'elmonte1988'\n          * 'osage1988'\n          * 'albuquerque1988'\n          * 'capecanaveral1988'\n          * 'albany1988'\n\n    Returns\n    --------\n    F1coeffs, F2coeffs : (array, array)\n          F1 and F2 coefficients for the Perez model\n\n    References\n    ----------\n    [1] Loutzenhiser P.G. et. al. \"Empirical validation of models to\n    compute solar irradiance on inclined surfaces for building energy\n    simulation\" 2007, Solar Energy vol. 81. pp. 254-267\n\n    [2] Perez, R., Seals, R., Ineichen, P., Stewart, R., Menicucci, D.,\n    1987. A new simplified version of the Perez diffuse irradiance model\n    for tilted surfaces. Solar Energy 39(3), 221-232.\n\n    [3] Perez, R., Ineichen, P., Seals, R., Michalsky, J., Stewart, R.,\n    1990. Modeling daylight availability and irradiance components from\n    direct and global irradiance. Solar Energy 44 (5), 271-289.\n\n    [4] Perez, R. et. al 1988. \"The Development and Verification of the\n    Perez Diffuse Radiation Model\". SAND88-7030\n\n    '''\n    coeffdict = {\n        'allsitescomposite1990': [\n            [-0.0080,    0.5880,   -0.0620,   -0.0600,    0.0720,   -0.0220],\n            [0.1300,    0.6830,   -0.1510,   -0.0190,    0.0660,   -0.0290],\n            [0.3300,    0.4870,   -0.2210,    0.0550,   -0.0640,   -0.0260],\n            [0.5680,    0.1870,   -0.2950,    0.1090,   -0.1520,   -0.0140],\n            [0.8730,   -0.3920,   -0.3620,    0.2260,   -0.4620,    0.0010],\n            [1.1320,   -1.2370,   -0.4120,    0.2880,   -0.8230,    0.0560],\n            [1.0600,   -1.6000,   -0.3590,    0.2640,   -1.1270,    0.1310],\n            [0.6780,   -0.3270,   -0.2500,    0.1560,   -1.3770,    0.2510]],\n        'allsitescomposite1988': [\n            [-0.0180,    0.7050,   -0.071,   -0.0580,    0.1020,   -0.0260],\n            [0.1910,    0.6450,   -0.1710,    0.0120,    0.0090,   -0.0270],\n            [0.4400,    0.3780,   -0.2560,    0.0870,   -0.1040,   -0.0250],\n            [0.7560,   -0.1210,   -0.3460,    0.1790,   -0.3210,   -0.0080],\n            [0.9960,   -0.6450,   -0.4050,    0.2600,   -0.5900,    0.0170],\n            [1.0980,   -1.2900,   -0.3930,    0.2690,   -0.8320,    0.0750],\n            [0.9730,   -1.1350,   -0.3780,    0.1240,   -0.2580,    0.1490],\n            [0.6890,   -0.4120,   -0.2730,    0.1990,   -1.6750,    0.2370]],\n        'sandiacomposite1988': [\n            [-0.1960,    1.0840,   -0.0060,   -0.1140,    0.1800,   -0.0190],\n            [0.2360,    0.5190,   -0.1800,   -0.0110,    0.0200,   -0.0380],\n            [0.4540,    0.3210,   -0.2550,    0.0720,   -0.0980,   -0.0460],\n            [0.8660,   -0.3810,   -0.3750,    0.2030,   -0.4030,   -0.0490],\n            [1.0260,   -0.7110,   -0.4260,    0.2730,   -0.6020,   -0.0610],\n            [0.9780,   -0.9860,   -0.3500,    0.2800,   -0.9150,   -0.0240],\n            [0.7480,   -0.9130,   -0.2360,    0.1730,   -1.0450,    0.0650],\n            [0.3180,   -0.7570,    0.1030,    0.0620,   -1.6980,    0.2360]],\n        'usacomposite1988': [\n            [-0.0340,    0.6710,   -0.0590,   -0.0590,    0.0860,   -0.0280],\n            [0.2550,    0.4740,   -0.1910,    0.0180,   -0.0140,   -0.0330],\n            [0.4270,    0.3490,   -0.2450,    0.0930,   -0.1210,   -0.0390],\n            [0.7560,   -0.2130,   -0.3280,    0.1750,   -0.3040,   -0.0270],\n            [1.0200,   -0.8570,   -0.3850,    0.2800,   -0.6380,   -0.0190],\n            [1.0500,   -1.3440,   -0.3480,    0.2800,   -0.8930,    0.0370],\n            [0.9740,   -1.5070,   -0.3700,    0.1540,   -0.5680,    0.1090],\n            [0.7440,   -1.8170,   -0.2560,    0.2460,   -2.6180,    0.2300]],\n        'france1988': [\n            [0.0130,    0.7640,   -0.1000,   -0.0580,    0.1270,   -0.0230],\n            [0.0950,    0.9200,   -0.1520,         0,    0.0510,   -0.0200],\n            [0.4640,    0.4210,   -0.2800,    0.0640,   -0.0510,   -0.0020],\n            [0.7590,   -0.0090,   -0.3730,    0.2010,   -0.3820,    0.0100],\n            [0.9760,   -0.4000,   -0.4360,    0.2710,   -0.6380,    0.0510],\n            [1.1760,   -1.2540,   -0.4620,    0.2950,   -0.9750,    0.1290],\n            [1.1060,   -1.5630,   -0.3980,    0.3010,   -1.4420,    0.2120],\n            [0.9340,   -1.5010,   -0.2710,    0.4200,   -2.9170,    0.2490]],\n        'phoenix1988': [\n            [-0.0030,    0.7280,   -0.0970,   -0.0750,    0.1420,   -0.0430],\n            [0.2790,    0.3540,   -0.1760,    0.0300,   -0.0550,   -0.0540],\n            [0.4690,    0.1680,   -0.2460,    0.0480,   -0.0420,   -0.0570],\n            [0.8560,   -0.5190,   -0.3400,    0.1760,   -0.3800,   -0.0310],\n            [0.9410,   -0.6250,   -0.3910,    0.1880,   -0.3600,   -0.0490],\n            [1.0560,   -1.1340,   -0.4100,    0.2810,   -0.7940,   -0.0650],\n            [0.9010,   -2.1390,   -0.2690,    0.1180,   -0.6650,    0.0460],\n            [0.1070,    0.4810,    0.1430,   -0.1110,   -0.1370,    0.2340]],\n        'elmonte1988': [\n            [0.0270,    0.7010,   -0.1190,   -0.0580,    0.1070,  -0.0600],\n            [0.1810,    0.6710,   -0.1780,   -0.0790,    0.1940,  -0.0350],\n            [0.4760,    0.4070,   -0.2880,    0.0540,   -0.0320,  -0.0550],\n            [0.8750,   -0.2180,   -0.4030,    0.1870,   -0.3090,  -0.0610],\n            [1.1660,   -1.0140,   -0.4540,    0.2110,   -0.4100,  -0.0440],\n            [1.1430,   -2.0640,   -0.2910,    0.0970,   -0.3190,   0.0530],\n            [1.0940,   -2.6320,   -0.2590,    0.0290,   -0.4220,   0.1470],\n            [0.1550,    1.7230,    0.1630,   -0.1310,   -0.0190,   0.2770]],\n        'osage1988': [\n            [-0.3530,    1.4740,   0.0570,   -0.1750,    0.3120,   0.0090],\n            [0.3630,    0.2180,  -0.2120,    0.0190,   -0.0340,  -0.0590],\n            [-0.0310,    1.2620,  -0.0840,   -0.0820,    0.2310,  -0.0170],\n            [0.6910,    0.0390,  -0.2950,    0.0910,   -0.1310,  -0.0350],\n            [1.1820,   -1.3500,  -0.3210,    0.4080,   -0.9850,  -0.0880],\n            [0.7640,    0.0190,  -0.2030,    0.2170,   -0.2940,  -0.1030],\n            [0.2190,    1.4120,   0.2440,    0.4710,   -2.9880,   0.0340],\n            [3.5780,   22.2310, -10.7450,    2.4260,    4.8920,  -5.6870]],\n        'albuquerque1988': [\n            [0.0340,    0.5010,  -0.0940,   -0.0630,    0.1060,  -0.0440],\n            [0.2290,    0.4670,  -0.1560,   -0.0050,   -0.0190,  -0.0230],\n            [0.4860,    0.2410,  -0.2530,    0.0530,   -0.0640,  -0.0220],\n            [0.8740,   -0.3930,  -0.3970,    0.1810,   -0.3270,  -0.0370],\n            [1.1930,   -1.2960,  -0.5010,    0.2810,   -0.6560,  -0.0450],\n            [1.0560,   -1.7580,  -0.3740,    0.2260,   -0.7590,   0.0340],\n            [0.9010,   -4.7830,  -0.1090,    0.0630,   -0.9700,   0.1960],\n            [0.8510,   -7.0550,  -0.0530,    0.0600,   -2.8330,   0.3300]],\n        'capecanaveral1988': [\n            [0.0750,    0.5330,   -0.1240,  -0.0670,   0.0420,  -0.0200],\n            [0.2950,    0.4970,   -0.2180,  -0.0080,   0.0030,  -0.0290],\n            [0.5140,    0.0810,   -0.2610,   0.0750,  -0.1600,  -0.0290],\n            [0.7470,   -0.3290,   -0.3250,   0.1810,  -0.4160,  -0.0300],\n            [0.9010,   -0.8830,   -0.2970,   0.1780,  -0.4890,   0.0080],\n            [0.5910,   -0.0440,   -0.1160,   0.2350,  -0.9990,   0.0980],\n            [0.5370,   -2.4020,    0.3200,   0.1690,  -1.9710,   0.3100],\n            [-0.8050,    4.5460,    1.0720,  -0.2580,  -0.9500,    0.7530]],\n        'albany1988': [\n            [0.0120,    0.5540,   -0.0760, -0.0520,   0.0840,  -0.0290],\n            [0.2670,    0.4370,   -0.1940,  0.0160,   0.0220,  -0.0360],\n            [0.4200,    0.3360,   -0.2370,  0.0740,  -0.0520,  -0.0320],\n            [0.6380,   -0.0010,   -0.2810,  0.1380,  -0.1890,  -0.0120],\n            [1.0190,   -1.0270,   -0.3420,  0.2710,  -0.6280,   0.0140],\n            [1.1490,   -1.9400,   -0.3310,  0.3220,  -1.0970,   0.0800],\n            [1.4340,   -3.9940,   -0.4920,  0.4530,  -2.3760,   0.1170],\n            [1.0070,   -2.2920,   -0.4820,  0.3900,  -3.3680,   0.2290]], }\n\n    array = np.array(coeffdict[perezmodel])\n\n    F1coeffs = array[:, 0:3]\n    F2coeffs = array[:, 3:7]\n\n    return F1coeffs, F2coeffs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_dirint_coeffs():\n\n    # To allow for maximum copy/paste from the MATLAB 1-indexed code,\n    # we create and assign values to an oversized array.\n    # Then, we return the [1:, 1:, :, :] slice.\n\n    coeffs = np.zeros((7, 7, 7, 5))\n\n    coeffs[1, 1, :, :] = [\n        [0.385230, 0.385230, 0.385230, 0.462880, 0.317440],\n        [0.338390, 0.338390, 0.221270, 0.316730, 0.503650],\n        [0.235680, 0.235680, 0.241280, 0.157830, 0.269440],\n        [0.830130, 0.830130, 0.171970, 0.841070, 0.457370],\n        [0.548010, 0.548010, 0.478000, 0.966880, 1.036370],\n        [0.548010, 0.548010, 1.000000, 3.012370, 1.976540],\n        [0.582690, 0.582690, 0.229720, 0.892710, 0.569950]]\n\n    coeffs[1, 2, :, :] = [\n        [0.131280, 0.131280, 0.385460, 0.511070, 0.127940],\n        [0.223710, 0.223710, 0.193560, 0.304560, 0.193940],\n        [0.229970, 0.229970, 0.275020, 0.312730, 0.244610],\n        [0.090100, 0.184580, 0.260500, 0.687480, 0.579440],\n        [0.131530, 0.131530, 0.370190, 1.380350, 1.052270],\n        [1.116250, 1.116250, 0.928030, 3.525490, 2.316920],\n        [0.090100, 0.237000, 0.300040, 0.812470, 0.664970]]\n\n    coeffs[1, 3, :, :] = [\n        [0.587510, 0.130000, 0.400000, 0.537210, 0.832490],\n        [0.306210, 0.129830, 0.204460, 0.500000, 0.681640],\n        [0.224020, 0.260620, 0.334080, 0.501040, 0.350470],\n        [0.421540, 0.753970, 0.750660, 3.706840, 0.983790],\n        [0.706680, 0.373530, 1.245670, 0.864860, 1.992630],\n        [4.864400, 0.117390, 0.265180, 0.359180, 3.310820],\n        [0.392080, 0.493290, 0.651560, 1.932780, 0.898730]]\n\n    coeffs[1, 4, :, :] = [\n        [0.126970, 0.126970, 0.126970, 0.126970, 0.126970],\n        [0.810820, 0.810820, 0.810820, 0.810820, 0.810820],\n        [3.241680, 2.500000, 2.291440, 2.291440, 2.291440],\n        [4.000000, 3.000000, 2.000000, 0.975430, 1.965570],\n        [12.494170, 12.494170, 8.000000, 5.083520, 8.792390],\n        [21.744240, 21.744240, 21.744240, 21.744240, 21.744240],\n        [3.241680, 12.494170, 1.620760, 1.375250, 2.331620]]\n\n    coeffs[1, 5, :, :] = [\n        [0.126970, 0.126970, 0.126970, 0.126970, 0.126970],\n        [0.810820, 0.810820, 0.810820, 0.810820, 0.810820],\n        [3.241680, 2.500000, 2.291440, 2.291440, 2.291440],\n        [4.000000, 3.000000, 2.000000, 0.975430, 1.965570],\n        [12.494170, 12.494170, 8.000000, 5.083520, 8.792390],\n        [21.744240, 21.744240, 21.744240, 21.744240, 21.744240],\n        [3.241680, 12.494170, 1.620760, 1.375250, 2.331620]]\n\n    coeffs[1, 6, :, :] = [\n        [0.126970, 0.126970, 0.126970, 0.126970, 0.126970],\n        [0.810820, 0.810820, 0.810820, 0.810820, 0.810820],\n        [3.241680, 2.500000, 2.291440, 2.291440, 2.291440],\n        [4.000000, 3.000000, 2.000000, 0.975430, 1.965570],\n        [12.494170, 12.494170, 8.000000, 5.083520, 8.792390],\n        [21.744240, 21.744240, 21.744240, 21.744240, 21.744240],\n        [3.241680, 12.494170, 1.620760, 1.375250, 2.331620]]\n\n    coeffs[2, 1, :, :] = [\n        [0.337440, 0.337440, 0.969110, 1.097190, 1.116080],\n        [0.337440, 0.337440, 0.969110, 1.116030, 0.623900],\n        [0.337440, 0.337440, 1.530590, 1.024420, 0.908480],\n        [0.584040, 0.584040, 0.847250, 0.914940, 1.289300],\n        [0.337440, 0.337440, 0.310240, 1.435020, 1.852830],\n        [0.337440, 0.337440, 1.015010, 1.097190, 2.117230],\n        [0.337440, 0.337440, 0.969110, 1.145730, 1.476400]]\n\n    coeffs[2, 2, :, :] = [\n        [0.300000, 0.300000, 0.700000, 1.100000, 0.796940],\n        [0.219870, 0.219870, 0.526530, 0.809610, 0.649300],\n        [0.386650, 0.386650, 0.119320, 0.576120, 0.685460],\n        [0.746730, 0.399830, 0.470970, 0.986530, 0.785370],\n        [0.575420, 0.936700, 1.649200, 1.495840, 1.335590],\n        [1.319670, 4.002570, 1.276390, 2.644550, 2.518670],\n        [0.665190, 0.678910, 1.012360, 1.199940, 0.986580]]\n\n    coeffs[2, 3, :, :] = [\n        [0.378870, 0.974060, 0.500000, 0.491880, 0.665290],\n        [0.105210, 0.263470, 0.407040, 0.553460, 0.582590],\n        [0.312900, 0.345240, 1.144180, 0.854790, 0.612280],\n        [0.119070, 0.365120, 0.560520, 0.793720, 0.802600],\n        [0.781610, 0.837390, 1.270420, 1.537980, 1.292950],\n        [1.152290, 1.152290, 1.492080, 1.245370, 2.177100],\n        [0.424660, 0.529550, 0.966910, 1.033460, 0.958730]]\n\n    coeffs[2, 4, :, :] = [\n        [0.310590, 0.714410, 0.252450, 0.500000, 0.607600],\n        [0.975190, 0.363420, 0.500000, 0.400000, 0.502800],\n        [0.175580, 0.196250, 0.476360, 1.072470, 0.490510],\n        [0.719280, 0.698620, 0.657770, 1.190840, 0.681110],\n        [0.426240, 1.464840, 0.678550, 1.157730, 0.978430],\n        [2.501120, 1.789130, 1.387090, 2.394180, 2.394180],\n        [0.491640, 0.677610, 0.685610, 1.082400, 0.735410]]\n\n    coeffs[2, 5, :, :] = [\n        [0.597000, 0.500000, 0.300000, 0.310050, 0.413510],\n        [0.314790, 0.336310, 0.400000, 0.400000, 0.442460],\n        [0.166510, 0.460440, 0.552570, 1.000000, 0.461610],\n        [0.401020, 0.559110, 0.403630, 1.016710, 0.671490],\n        [0.400360, 0.750830, 0.842640, 1.802600, 1.023830],\n        [3.315300, 1.510380, 2.443650, 1.638820, 2.133990],\n        [0.530790, 0.745850, 0.693050, 1.458040, 0.804500]]\n\n    coeffs[2, 6, :, :] = [\n        [0.597000, 0.500000, 0.300000, 0.310050, 0.800920],\n        [0.314790, 0.336310, 0.400000, 0.400000, 0.237040],\n        [0.166510, 0.460440, 0.552570, 1.000000, 0.581990],\n        [0.401020, 0.559110, 0.403630, 1.016710, 0.898570],\n        [0.400360, 0.750830, 0.842640, 1.802600, 3.400390],\n        [3.315300, 1.510380, 2.443650, 1.638820, 2.508780],\n        [0.204340, 1.157740, 2.003080, 2.622080, 1.409380]]\n\n    coeffs[3, 1, :, :] = [\n        [1.242210, 1.242210, 1.242210, 1.242210, 1.242210],\n        [0.056980, 0.056980, 0.656990, 0.656990, 0.925160],\n        [0.089090, 0.089090, 1.040430, 1.232480, 1.205300],\n        [1.053850, 1.053850, 1.399690, 1.084640, 1.233340],\n        [1.151540, 1.151540, 1.118290, 1.531640, 1.411840],\n        [1.494980, 1.494980, 1.700000, 1.800810, 1.671600],\n        [1.018450, 1.018450, 1.153600, 1.321890, 1.294670]]\n\n    coeffs[3, 2, :, :] = [\n        [0.700000, 0.700000, 1.023460, 0.700000, 0.945830],\n        [0.886300, 0.886300, 1.333620, 0.800000, 1.066620],\n        [0.902180, 0.902180, 0.954330, 1.126690, 1.097310],\n        [1.095300, 1.075060, 1.176490, 1.139470, 1.096110],\n        [1.201660, 1.201660, 1.438200, 1.256280, 1.198060],\n        [1.525850, 1.525850, 1.869160, 1.985410, 1.911590],\n        [1.288220, 1.082810, 1.286370, 1.166170, 1.119330]]\n\n    coeffs[3, 3, :, :] = [\n        [0.600000, 1.029910, 0.859890, 0.550000, 0.813600],\n        [0.604450, 1.029910, 0.859890, 0.656700, 0.928840],\n        [0.455850, 0.750580, 0.804930, 0.823000, 0.911000],\n        [0.526580, 0.932310, 0.908620, 0.983520, 0.988090],\n        [1.036110, 1.100690, 0.848380, 1.035270, 1.042380],\n        [1.048440, 1.652720, 0.900000, 2.350410, 1.082950],\n        [0.817410, 0.976160, 0.861300, 0.974780, 1.004580]]\n\n    coeffs[3, 4, :, :] = [\n        [0.782110, 0.564280, 0.600000, 0.600000, 0.665740],\n        [0.894480, 0.680730, 0.541990, 0.800000, 0.669140],\n        [0.487460, 0.818950, 0.841830, 0.872540, 0.709040],\n        [0.709310, 0.872780, 0.908480, 0.953290, 0.844350],\n        [0.863920, 0.947770, 0.876220, 1.078750, 0.936910],\n        [1.280350, 0.866720, 0.769790, 1.078750, 0.975130],\n        [0.725420, 0.869970, 0.868810, 0.951190, 0.829220]]\n\n    coeffs[3, 5, :, :] = [\n        [0.791750, 0.654040, 0.483170, 0.409000, 0.597180],\n        [0.566140, 0.948990, 0.971820, 0.653570, 0.718550],\n        [0.648710, 0.637730, 0.870510, 0.860600, 0.694300],\n        [0.637630, 0.767610, 0.925670, 0.990310, 0.847670],\n        [0.736380, 0.946060, 1.117590, 1.029340, 0.947020],\n        [1.180970, 0.850000, 1.050000, 0.950000, 0.888580],\n        [0.700560, 0.801440, 0.961970, 0.906140, 0.823880]]\n\n    coeffs[3, 6, :, :] = [\n        [0.500000, 0.500000, 0.586770, 0.470550, 0.629790],\n        [0.500000, 0.500000, 1.056220, 1.260140, 0.658140],\n        [0.500000, 0.500000, 0.631830, 0.842620, 0.582780],\n        [0.554710, 0.734730, 0.985820, 0.915640, 0.898260],\n        [0.712510, 1.205990, 0.909510, 1.078260, 0.885610],\n        [1.899260, 1.559710, 1.000000, 1.150000, 1.120390],\n        [0.653880, 0.793120, 0.903320, 0.944070, 0.796130]]\n\n    coeffs[4, 1, :, :] = [\n        [1.000000, 1.000000, 1.050000, 1.170380, 1.178090],\n        [0.960580, 0.960580, 1.059530, 1.179030, 1.131690],\n        [0.871470, 0.871470, 0.995860, 1.141910, 1.114600],\n        [1.201590, 1.201590, 0.993610, 1.109380, 1.126320],\n        [1.065010, 1.065010, 0.828660, 0.939970, 1.017930],\n        [1.065010, 1.065010, 0.623690, 1.119620, 1.132260],\n        [1.071570, 1.071570, 0.958070, 1.114130, 1.127110]]\n\n    coeffs[4, 2, :, :] = [\n        [0.950000, 0.973390, 0.852520, 1.092200, 1.096590],\n        [0.804120, 0.913870, 0.980990, 1.094580, 1.042420],\n        [0.737540, 0.935970, 0.999940, 1.056490, 1.050060],\n        [1.032980, 1.034540, 0.968460, 1.032080, 1.015780],\n        [0.900000, 0.977210, 0.945960, 1.008840, 0.969960],\n        [0.600000, 0.750000, 0.750000, 0.844710, 0.899100],\n        [0.926800, 0.965030, 0.968520, 1.044910, 1.032310]]\n\n    coeffs[4, 3, :, :] = [\n        [0.850000, 1.029710, 0.961100, 1.055670, 1.009700],\n        [0.818530, 0.960010, 0.996450, 1.081970, 1.036470],\n        [0.765380, 0.953500, 0.948260, 1.052110, 1.000140],\n        [0.775610, 0.909610, 0.927800, 0.987800, 0.952100],\n        [1.000990, 0.881880, 0.875950, 0.949100, 0.893690],\n        [0.902370, 0.875960, 0.807990, 0.942410, 0.917920],\n        [0.856580, 0.928270, 0.946820, 1.032260, 0.972990]]\n\n    coeffs[4, 4, :, :] = [\n        [0.750000, 0.857930, 0.983800, 1.056540, 0.980240],\n        [0.750000, 0.987010, 1.013730, 1.133780, 1.038250],\n        [0.800000, 0.947380, 1.012380, 1.091270, 0.999840],\n        [0.800000, 0.914550, 0.908570, 0.999190, 0.915230],\n        [0.778540, 0.800590, 0.799070, 0.902180, 0.851560],\n        [0.680190, 0.317410, 0.507680, 0.388910, 0.646710],\n        [0.794920, 0.912780, 0.960830, 1.057110, 0.947950]]\n\n    coeffs[4, 5, :, :] = [\n        [0.750000, 0.833890, 0.867530, 1.059890, 0.932840],\n        [0.979700, 0.971470, 0.995510, 1.068490, 1.030150],\n        [0.858850, 0.987920, 1.043220, 1.108700, 1.044900],\n        [0.802400, 0.955110, 0.911660, 1.045070, 0.944470],\n        [0.884890, 0.766210, 0.885390, 0.859070, 0.818190],\n        [0.615680, 0.700000, 0.850000, 0.624620, 0.669300],\n        [0.835570, 0.946150, 0.977090, 1.049350, 0.979970]]\n\n    coeffs[4, 6, :, :] = [\n        [0.689220, 0.809600, 0.900000, 0.789500, 0.853990],\n        [0.854660, 0.852840, 0.938200, 0.923110, 0.955010],\n        [0.938600, 0.932980, 1.010390, 1.043950, 1.041640],\n        [0.843620, 0.981300, 0.951590, 0.946100, 0.966330],\n        [0.694740, 0.814690, 0.572650, 0.400000, 0.726830],\n        [0.211370, 0.671780, 0.416340, 0.297290, 0.498050],\n        [0.843540, 0.882330, 0.911760, 0.898420, 0.960210]]\n\n    coeffs[5, 1, :, :] = [\n        [1.054880, 1.075210, 1.068460, 1.153370, 1.069220],\n        [1.000000, 1.062220, 1.013470, 1.088170, 1.046200],\n        [0.885090, 0.993530, 0.942590, 1.054990, 1.012740],\n        [0.920000, 0.950000, 0.978720, 1.020280, 0.984440],\n        [0.850000, 0.908500, 0.839940, 0.985570, 0.962180],\n        [0.800000, 0.800000, 0.810080, 0.950000, 0.961550],\n        [1.038590, 1.063200, 1.034440, 1.112780, 1.037800]]\n\n    coeffs[5, 2, :, :] = [\n        [1.017610, 1.028360, 1.058960, 1.133180, 1.045620],\n        [0.920000, 0.998970, 1.033590, 1.089030, 1.022060],\n        [0.912370, 0.949930, 0.979770, 1.020420, 0.981770],\n        [0.847160, 0.935300, 0.930540, 0.955050, 0.946560],\n        [0.880260, 0.867110, 0.874130, 0.972650, 0.883420],\n        [0.627150, 0.627150, 0.700000, 0.774070, 0.845130],\n        [0.973700, 1.006240, 1.026190, 1.071960, 1.017240]]\n\n    coeffs[5, 3, :, :] = [\n        [1.028710, 1.017570, 1.025900, 1.081790, 1.024240],\n        [0.924980, 0.985500, 1.014100, 1.092210, 0.999610],\n        [0.828570, 0.934920, 0.994950, 1.024590, 0.949710],\n        [0.900810, 0.901330, 0.928830, 0.979570, 0.913100],\n        [0.761030, 0.845150, 0.805360, 0.936790, 0.853460],\n        [0.626400, 0.546750, 0.730500, 0.850000, 0.689050],\n        [0.957630, 0.985480, 0.991790, 1.050220, 0.987900]]\n\n    coeffs[5, 4, :, :] = [\n        [0.992730, 0.993880, 1.017150, 1.059120, 1.017450],\n        [0.975610, 0.987160, 1.026820, 1.075440, 1.007250],\n        [0.871090, 0.933190, 0.974690, 0.979840, 0.952730],\n        [0.828750, 0.868090, 0.834920, 0.905510, 0.871530],\n        [0.781540, 0.782470, 0.767910, 0.764140, 0.795890],\n        [0.743460, 0.693390, 0.514870, 0.630150, 0.715660],\n        [0.934760, 0.957870, 0.959640, 0.972510, 0.981640]]\n\n    coeffs[5, 5, :, :] = [\n        [0.965840, 0.941240, 0.987100, 1.022540, 1.011160],\n        [0.988630, 0.994770, 0.976590, 0.950000, 1.034840],\n        [0.958200, 1.018080, 0.974480, 0.920000, 0.989870],\n        [0.811720, 0.869090, 0.812020, 0.850000, 0.821050],\n        [0.682030, 0.679480, 0.632450, 0.746580, 0.738550],\n        [0.668290, 0.445860, 0.500000, 0.678920, 0.696510],\n        [0.926940, 0.953350, 0.959050, 0.876210, 0.991490]]\n\n    coeffs[5, 6, :, :] = [\n        [0.948940, 0.997760, 0.850000, 0.826520, 0.998470],\n        [1.017860, 0.970000, 0.850000, 0.700000, 0.988560],\n        [1.000000, 0.950000, 0.850000, 0.606240, 0.947260],\n        [1.000000, 0.746140, 0.751740, 0.598390, 0.725230],\n        [0.922210, 0.500000, 0.376800, 0.517110, 0.548630],\n        [0.500000, 0.450000, 0.429970, 0.404490, 0.539940],\n        [0.960430, 0.881630, 0.775640, 0.596350, 0.937680]]\n\n    coeffs[6, 1, :, :] = [\n        [1.030000, 1.040000, 1.000000, 1.000000, 1.049510],\n        [1.050000, 0.990000, 0.990000, 0.950000, 0.996530],\n        [1.050000, 0.990000, 0.990000, 0.820000, 0.971940],\n        [1.050000, 0.790000, 0.880000, 0.820000, 0.951840],\n        [1.000000, 0.530000, 0.440000, 0.710000, 0.928730],\n        [0.540000, 0.470000, 0.500000, 0.550000, 0.773950],\n        [1.038270, 0.920180, 0.910930, 0.821140, 1.034560]]\n\n    coeffs[6, 2, :, :] = [\n        [1.041020, 0.997520, 0.961600, 1.000000, 1.035780],\n        [0.948030, 0.980000, 0.900000, 0.950360, 0.977460],\n        [0.950000, 0.977250, 0.869270, 0.800000, 0.951680],\n        [0.951870, 0.850000, 0.748770, 0.700000, 0.883850],\n        [0.900000, 0.823190, 0.727450, 0.600000, 0.839870],\n        [0.850000, 0.805020, 0.692310, 0.500000, 0.788410],\n        [1.010090, 0.895270, 0.773030, 0.816280, 1.011680]]\n\n    coeffs[6, 3, :, :] = [\n        [1.022450, 1.004600, 0.983650, 1.000000, 1.032940],\n        [0.943960, 0.999240, 0.983920, 0.905990, 0.978150],\n        [0.936240, 0.946480, 0.850000, 0.850000, 0.930320],\n        [0.816420, 0.885000, 0.644950, 0.817650, 0.865310],\n        [0.742960, 0.765690, 0.561520, 0.700000, 0.827140],\n        [0.643870, 0.596710, 0.474460, 0.600000, 0.651200],\n        [0.971740, 0.940560, 0.714880, 0.864380, 1.001650]]\n\n    coeffs[6, 4, :, :] = [\n        [0.995260, 0.977010, 1.000000, 1.000000, 1.035250],\n        [0.939810, 0.975250, 0.939980, 0.950000, 0.982550],\n        [0.876870, 0.879440, 0.850000, 0.900000, 0.917810],\n        [0.873480, 0.873450, 0.751470, 0.850000, 0.863040],\n        [0.761470, 0.702360, 0.638770, 0.750000, 0.783120],\n        [0.734080, 0.650000, 0.600000, 0.650000, 0.715660],\n        [0.942160, 0.919100, 0.770340, 0.731170, 0.995180]]\n\n    coeffs[6, 5, :, :] = [\n        [0.952560, 0.916780, 0.920000, 0.900000, 1.005880],\n        [0.928620, 0.994420, 0.900000, 0.900000, 0.983720],\n        [0.913070, 0.850000, 0.850000, 0.800000, 0.924280],\n        [0.868090, 0.807170, 0.823550, 0.600000, 0.844520],\n        [0.769570, 0.719870, 0.650000, 0.550000, 0.733500],\n        [0.580250, 0.650000, 0.600000, 0.500000, 0.628850],\n        [0.904770, 0.852650, 0.708370, 0.493730, 0.949030]]\n\n    coeffs[6, 6, :, :] = [\n        [0.911970, 0.800000, 0.800000, 0.800000, 0.956320],\n        [0.912620, 0.682610, 0.750000, 0.700000, 0.950110],\n        [0.653450, 0.659330, 0.700000, 0.600000, 0.856110],\n        [0.648440, 0.600000, 0.641120, 0.500000, 0.695780],\n        [0.570000, 0.550000, 0.598800, 0.400000, 0.560150],\n        [0.475230, 0.500000, 0.518640, 0.339970, 0.520230],\n        [0.743440, 0.592190, 0.603060, 0.316930, 0.794390]]\n\n    return coeffs[1:, 1:, :, :]", "response": "Returns the dirint coefficients for the current language."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndetermining DNI from GHI and DHI. When calculating the DNI from GHI and DHI the calculated DNI may be unreasonably high or negative for zenith angles close to 90 degrees (sunrise/sunset transitions). This function identifies unreasonable DNI values and sets them to NaN. If the clearsky DNI is given unreasonably high values are cut off. Parameters ---------- ghi : Series Global horizontal irradiance. dhi : Series Diffuse horizontal irradiance. zenith : Series True (not refraction-corrected) zenith angles in decimal degrees. Angles must be >=0 and <=180. clearsky_dni : None or Series, default None Clearsky direct normal irradiance. clearsky_tolerance : float, default 1.1 If 'clearsky_dni' is given this parameter can be used to allow a tolerance by how much the calculated DNI value can be greater than the clearsky value before it is identified as an unreasonable value. zenith_threshold_for_zero_dni : float, default 88.0 Non-zero DNI values for zenith angles greater than or equal to 'zenith_threshold_for_zero_dni' will be set to NaN. zenith_threshold_for_clearsky_limit : float, default 80.0 DNI values for zenith angles greater than or equal to 'zenith_threshold_for_clearsky_limit' and smaller the 'zenith_threshold_for_zero_dni' that are greater than the clearsky DNI (times allowed tolerance) will be corrected. Only applies if 'clearsky_dni' is not None. Returns ------- dni : Series The modeled direct normal irradiance.", "response": "def dni(ghi, dhi, zenith, clearsky_dni=None, clearsky_tolerance=1.1,\n        zenith_threshold_for_zero_dni=88.0,\n        zenith_threshold_for_clearsky_limit=80.0):\n    \"\"\"\n    Determine DNI from GHI and DHI.\n\n    When calculating the DNI from GHI and DHI the calculated DNI may be\n    unreasonably high or negative for zenith angles close to 90 degrees\n    (sunrise/sunset transitions). This function identifies unreasonable DNI\n    values and sets them to NaN. If the clearsky DNI is given unreasonably high\n    values are cut off.\n\n    Parameters\n    ----------\n    ghi : Series\n        Global horizontal irradiance.\n\n    dhi : Series\n        Diffuse horizontal irradiance.\n\n    zenith : Series\n        True (not refraction-corrected) zenith angles in decimal\n        degrees. Angles must be >=0 and <=180.\n\n    clearsky_dni : None or Series, default None\n        Clearsky direct normal irradiance.\n\n    clearsky_tolerance : float, default 1.1\n        If 'clearsky_dni' is given this parameter can be used to allow a\n        tolerance by how much the calculated DNI value can be greater than\n        the clearsky value before it is identified as an unreasonable value.\n\n    zenith_threshold_for_zero_dni : float, default 88.0\n        Non-zero DNI values for zenith angles greater than or equal to\n        'zenith_threshold_for_zero_dni' will be set to NaN.\n\n    zenith_threshold_for_clearsky_limit : float, default 80.0\n        DNI values for zenith angles greater than or equal to\n        'zenith_threshold_for_clearsky_limit' and smaller the\n        'zenith_threshold_for_zero_dni' that are greater than the clearsky DNI\n        (times allowed tolerance) will be corrected. Only applies if\n        'clearsky_dni' is not None.\n\n    Returns\n    -------\n    dni : Series\n        The modeled direct normal irradiance.\n    \"\"\"\n\n    # calculate DNI\n    dni = (ghi - dhi) / tools.cosd(zenith)\n\n    # cutoff negative values\n    dni[dni < 0] = float('nan')\n\n    # set non-zero DNI values for zenith angles >=\n    # zenith_threshold_for_zero_dni to NaN\n    dni[(zenith >= zenith_threshold_for_zero_dni) & (dni != 0)] = float('nan')\n\n    # correct DNI values for zenith angles greater or equal to the\n    # zenith_threshold_for_clearsky_limit and smaller than the\n    # upper_cutoff_zenith that are greater than the clearsky DNI (times\n    # clearsky_tolerance)\n    if clearsky_dni is not None:\n        max_dni = clearsky_dni * clearsky_tolerance\n        dni[(zenith >= zenith_threshold_for_clearsky_limit) &\n            (zenith < zenith_threshold_for_zero_dni) &\n            (dni > max_dni)] = max_dni\n    return dni"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ineichen(apparent_zenith, airmass_absolute, linke_turbidity,\n             altitude=0, dni_extra=1364., perez_enhancement=False):\n    '''\n    Determine clear sky GHI, DNI, and DHI from Ineichen/Perez model.\n\n    Implements the Ineichen and Perez clear sky model for global\n    horizontal irradiance (GHI), direct normal irradiance (DNI), and\n    calculates the clear-sky diffuse horizontal (DHI) component as the\n    difference between GHI and DNI*cos(zenith) as presented in [1, 2]. A\n    report on clear sky models found the Ineichen/Perez model to have\n    excellent performance with a minimal input data set [3].\n\n    Default values for monthly Linke turbidity provided by SoDa [4, 5].\n\n    Parameters\n    -----------\n    apparent_zenith : numeric\n        Refraction corrected solar zenith angle in degrees.\n\n    airmass_absolute : numeric\n        Pressure corrected airmass.\n\n    linke_turbidity : numeric\n        Linke Turbidity.\n\n    altitude : numeric, default 0\n        Altitude above sea level in meters.\n\n    dni_extra : numeric, default 1364\n        Extraterrestrial irradiance. The units of ``dni_extra``\n        determine the units of the output.\n\n    perez_enhancement : bool, default False\n        Controls if the Perez enhancement factor should be applied.\n        Setting to True may produce spurious results for times when\n        the Sun is near the horizon and the airmass is high.\n        See https://github.com/pvlib/pvlib-python/issues/435\n\n    Returns\n    -------\n    clearsky : DataFrame (if Series input) or OrderedDict of arrays\n        DataFrame/OrderedDict contains the columns/keys\n        ``'dhi', 'dni', 'ghi'``.\n\n    See also\n    --------\n    lookup_linke_turbidity\n    pvlib.location.Location.get_clearsky\n\n    References\n    ----------\n    [1] P. Ineichen and R. Perez, \"A New airmass independent formulation for\n        the Linke turbidity coefficient\", Solar Energy, vol 73, pp. 151-157,\n        2002.\n\n    [2] R. Perez et. al., \"A New Operational Model for Satellite-Derived\n        Irradiances: Description and Validation\", Solar Energy, vol 73, pp.\n        307-317, 2002.\n\n    [3] M. Reno, C. Hansen, and J. Stein, \"Global Horizontal Irradiance Clear\n        Sky Models: Implementation and Analysis\", Sandia National\n        Laboratories, SAND2012-2389, 2012.\n\n    [4] http://www.soda-is.com/eng/services/climat_free_eng.php#c5 (obtained\n        July 17, 2012).\n\n    [5] J. Remund, et. al., \"Worldwide Linke Turbidity Information\", Proc.\n        ISES Solar World Congress, June 2003. Goteborg, Sweden.\n    '''\n\n    # ghi is calculated using either the equations in [1] by setting\n    # perez_enhancement=False (default behavior) or using the model\n    # in [2] by setting perez_enhancement=True.\n\n    # The NaN handling is a little subtle. The AM input is likely to\n    # have NaNs that we'll want to map to 0s in the output. However, we\n    # want NaNs in other inputs to propagate through to the output. This\n    # is accomplished by judicious use and placement of np.maximum,\n    # np.minimum, and np.fmax\n\n    # use max so that nighttime values will result in 0s instead of\n    # negatives. propagates nans.\n    cos_zenith = np.maximum(tools.cosd(apparent_zenith), 0)\n\n    tl = linke_turbidity\n\n    fh1 = np.exp(-altitude/8000.)\n    fh2 = np.exp(-altitude/1250.)\n    cg1 = 5.09e-05 * altitude + 0.868\n    cg2 = 3.92e-05 * altitude + 0.0387\n\n    ghi = np.exp(-cg2*airmass_absolute*(fh1 + fh2*(tl - 1)))\n\n    # https://github.com/pvlib/pvlib-python/issues/435\n    if perez_enhancement:\n        ghi *= np.exp(0.01*airmass_absolute**1.8)\n\n    # use fmax to map airmass nans to 0s. multiply and divide by tl to\n    # reinsert tl nans\n    ghi = cg1 * dni_extra * cos_zenith * tl / tl * np.fmax(ghi, 0)\n\n    # BncI = \"normal beam clear sky radiation\"\n    b = 0.664 + 0.163/fh1\n    bnci = b * np.exp(-0.09 * airmass_absolute * (tl - 1))\n    bnci = dni_extra * np.fmax(bnci, 0)\n\n    # \"empirical correction\" SE 73, 157 & SE 73, 312.\n    bnci_2 = ((1 - (0.1 - 0.2*np.exp(-tl))/(0.1 + 0.882/fh1)) /\n              cos_zenith)\n    bnci_2 = ghi * np.fmin(np.fmax(bnci_2, 0), 1e20)\n\n    dni = np.minimum(bnci, bnci_2)\n\n    dhi = ghi - dni*cos_zenith\n\n    irrads = OrderedDict()\n    irrads['ghi'] = ghi\n    irrads['dni'] = dni\n    irrads['dhi'] = dhi\n\n    if isinstance(dni, pd.Series):\n        irrads = pd.DataFrame.from_dict(irrads)\n\n    return irrads", "response": "This function calculates the clear sky GHI DNI and DHI difference between GHI and DNI and DHI."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlooks up the Linke Turibidity from the ``LinkeTurbidities.h5`` data file supplied with pvlib. Parameters ---------- time : pandas.DatetimeIndex latitude : float longitude : float filepath : None or string, default None The path to the ``.h5`` file. interp_turbidity : bool, default True If ``True``, interpolates the monthly Linke turbidity values found in ``LinkeTurbidities.h5`` to daily values. Returns ------- turbidity : Series", "response": "def lookup_linke_turbidity(time, latitude, longitude, filepath=None,\n                           interp_turbidity=True):\n    \"\"\"\n    Look up the Linke Turibidity from the ``LinkeTurbidities.h5``\n    data file supplied with pvlib.\n\n    Parameters\n    ----------\n    time : pandas.DatetimeIndex\n\n    latitude : float\n\n    longitude : float\n\n    filepath : None or string, default None\n        The path to the ``.h5`` file.\n\n    interp_turbidity : bool, default True\n        If ``True``, interpolates the monthly Linke turbidity values\n        found in ``LinkeTurbidities.h5`` to daily values.\n\n    Returns\n    -------\n    turbidity : Series\n    \"\"\"\n\n    # The .h5 file 'LinkeTurbidities.h5' contains a single 2160 x 4320 x 12\n    # matrix of type uint8 called 'LinkeTurbidity'. The rows represent global\n    # latitudes from 90 to -90 degrees; the columns represent global longitudes\n    # from -180 to 180; and the depth (third dimension) represents months of\n    # the year from January (1) to December (12). To determine the Linke\n    # turbidity for a position on the Earth's surface for a given month do the\n    # following: LT = LinkeTurbidity(LatitudeIndex, LongitudeIndex, month).\n    # Note that the numbers within the matrix are 20 * Linke Turbidity,\n    # so divide the number from the file by 20 to get the\n    # turbidity.\n\n    # The nodes of the grid are 5' (1/12=0.0833[arcdeg]) apart.\n    # From Section 8 of Aerosol optical depth and Linke turbidity climatology\n    # http://www.meteonorm.com/images/uploads/downloads/ieashc36_report_TL_AOD_climatologies.pdf\n    # 1st row: 89.9583 S, 2nd row: 89.875 S\n    # 1st column: 179.9583 W, 2nd column: 179.875 W\n\n    try:\n        import tables\n    except ImportError:\n        raise ImportError('The Linke turbidity lookup table requires tables. '\n                          'You can still use clearsky.ineichen if you '\n                          'supply your own turbidities.')\n\n    if filepath is None:\n        pvlib_path = os.path.dirname(os.path.abspath(__file__))\n        filepath = os.path.join(pvlib_path, 'data', 'LinkeTurbidities.h5')\n\n    latitude_index = (\n        np.around(_linearly_scale(latitude, 90, -90, 0, 2160))\n        .astype(np.int64))\n    longitude_index = (\n        np.around(_linearly_scale(longitude, -180, 180, 0, 4320))\n        .astype(np.int64))\n\n    lt_h5_file = tables.open_file(filepath)\n    try:\n        lts = lt_h5_file.root.LinkeTurbidity[latitude_index,\n                                             longitude_index, :]\n    except IndexError:\n        raise IndexError('Latitude should be between 90 and -90, '\n                         'longitude between -180 and 180.')\n    finally:\n        lt_h5_file.close()\n\n    if interp_turbidity:\n        linke_turbidity = _interpolate_turbidity(lts, time)\n    else:\n        months = time.month - 1\n        linke_turbidity = pd.Series(lts[months], index=time)\n\n    linke_turbidity /= 20.\n\n    return linke_turbidity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _is_leap_year(year):\n    isleap = ((np.mod(year, 4) == 0) &\n              ((np.mod(year, 100) != 0) | (np.mod(year, 400) == 0)))\n    return isleap", "response": "Determines if a year is leap year."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ninterpolating monthly Linke turbidity onto daily values.", "response": "def _interpolate_turbidity(lts, time):\n    \"\"\"\n    Interpolated monthly Linke turbidity onto daily values.\n\n    Parameters\n    ----------\n    lts : np.array\n        Monthly Linke turbidity values.\n    time : pd.DatetimeIndex\n        Times to be interpolated onto.\n\n    Returns\n    -------\n    linke_turbidity : pd.Series\n        The interpolated turbidity.\n    \"\"\"\n    # Data covers 1 year. Assume that data corresponds to the value at the\n    # middle of each month. This means that we need to add previous Dec and\n    # next Jan to the array so that the interpolation will work for\n    # Jan 1 - Jan 15 and Dec 16 - Dec 31.\n    lts_concat = np.concatenate([[lts[-1]], lts, [lts[0]]])\n\n    # handle leap years\n    try:\n        isleap = time.is_leap_year\n    except AttributeError:\n        year = time.year\n        isleap = _is_leap_year(year)\n\n    dayofyear = time.dayofyear\n    days_leap = _calendar_month_middles(2016)\n    days_no_leap = _calendar_month_middles(2015)\n\n    # Then we map the month value to the day of year value.\n    # Do it for both leap and non-leap years.\n    lt_leap = np.interp(dayofyear, days_leap, lts_concat)\n    lt_no_leap = np.interp(dayofyear, days_no_leap, lts_concat)\n    linke_turbidity = np.where(isleap, lt_leap, lt_no_leap)\n\n    linke_turbidity = pd.Series(linke_turbidity, index=time)\n\n    return linke_turbidity"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _calendar_month_middles(year):\n    # remove mdays[0] since January starts at mdays[1]\n    # make local copy of mdays since we need to change\n    # February for leap years\n    mdays = np.array(calendar.mdays[1:])\n    ydays = 365\n    # handle leap years\n    if calendar.isleap(year):\n        mdays[1] = mdays[1] + 1\n        ydays = 366\n    middles = np.concatenate(\n        [[-calendar.mdays[-1] / 2.0],  # Dec last year\n         np.cumsum(mdays) - np.array(mdays) / 2.,  # this year\n         [ydays + calendar.mdays[1] / 2.0]])  # Jan next year\n    return middles", "response": "List of middle day of each month used by Linke turbidity lookup"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _linearly_scale(inputmatrix, inputmin, inputmax, outputmin, outputmax):\n    inputrange = inputmax - inputmin\n    outputrange = outputmax - outputmin\n    delta = outputrange/inputrange  # number of indices per input unit\n    inputmin = inputmin + 1.0 / delta / 2.0  # shift to center of index\n    outputmax = outputmax - 1  # shift index to zero indexing\n    outputmatrix = (inputmatrix - inputmin) * delta + outputmin\n    err = IndexError('Input, %g, is out of range (%g, %g).' %\n                     (inputmatrix, inputmax - inputrange, inputmax))\n    # round down if input is within half an index or else raise index error\n    if outputmatrix > outputmax:\n        if np.around(outputmatrix - outputmax, 1) <= 0.5:\n            outputmatrix = outputmax\n        else:\n            raise err\n    elif outputmatrix < outputmin:\n        if np.around(outputmin - outputmatrix, 1) <= 0.5:\n            outputmatrix = outputmin\n        else:\n            raise err\n    return outputmatrix", "response": "linearly scale input to output used by Linke turbidity lookup"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining clear sky GHI using the Haurwitz model. Implements the Haurwitz clear sky model for global horizontal irradiance (GHI) as presented in [1, 2]. A report on clear sky models found the Haurwitz model to have the best performance in terms of average monthly error among models which require only zenith angle [3]. Parameters ---------- apparent_zenith : Series The apparent (refraction corrected) sun zenith angle in degrees. Returns ------- ghi : DataFrame The modeled global horizonal irradiance in W/m^2 provided by the Haurwitz clear-sky model. References ---------- [1] B. Haurwitz, \"Insolation in Relation to Cloudiness and Cloud Density,\" Journal of Meteorology, vol. 2, pp. 154-166, 1945. [2] B. Haurwitz, \"Insolation in Relation to Cloud Type,\" Journal of Meteorology, vol. 3, pp. 123-124, 1946. [3] M. Reno, C. Hansen, and J. Stein, \"Global Horizontal Irradiance Clear Sky Models: Implementation and Analysis\", Sandia National Laboratories, SAND2012-2389, 2012.", "response": "def haurwitz(apparent_zenith):\n    '''\n    Determine clear sky GHI using the Haurwitz model.\n\n    Implements the Haurwitz clear sky model for global horizontal\n    irradiance (GHI) as presented in [1, 2]. A report on clear\n    sky models found the Haurwitz model to have the best performance\n    in terms of average monthly error among models which require only\n    zenith angle [3].\n\n    Parameters\n    ----------\n    apparent_zenith : Series\n        The apparent (refraction corrected) sun zenith angle\n        in degrees.\n\n    Returns\n    -------\n    ghi : DataFrame\n        The modeled global horizonal irradiance in W/m^2 provided\n        by the Haurwitz clear-sky model.\n\n    References\n    ----------\n\n    [1] B. Haurwitz, \"Insolation in Relation to Cloudiness and Cloud\n     Density,\" Journal of Meteorology, vol. 2, pp. 154-166, 1945.\n\n    [2] B. Haurwitz, \"Insolation in Relation to Cloud Type,\" Journal of\n     Meteorology, vol. 3, pp. 123-124, 1946.\n\n    [3] M. Reno, C. Hansen, and J. Stein, \"Global Horizontal Irradiance Clear\n     Sky Models: Implementation and Analysis\", Sandia National\n     Laboratories, SAND2012-2389, 2012.\n    '''\n\n    cos_zenith = tools.cosd(apparent_zenith.values)\n    clearsky_ghi = np.zeros_like(apparent_zenith.values)\n    cos_zen_gte_0 = cos_zenith > 0\n    clearsky_ghi[cos_zen_gte_0] = (1098.0 * cos_zenith[cos_zen_gte_0] *\n                                   np.exp(-0.059/cos_zenith[cos_zen_gte_0]))\n\n    df_out = pd.DataFrame(index=apparent_zenith.index,\n                          data=clearsky_ghi,\n                          columns=['ghi'])\n\n    return df_out"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef simplified_solis(apparent_elevation, aod700=0.1, precipitable_water=1.,\n                     pressure=101325., dni_extra=1364.):\n    \"\"\"\n    Calculate the clear sky GHI, DNI, and DHI according to the\n    simplified Solis model [1]_.\n\n    Reference [1]_ describes the accuracy of the model as being 15, 20,\n    and 18 W/m^2 for the beam, global, and diffuse components. Reference\n    [2]_ provides comparisons with other clear sky models.\n\n    Parameters\n    ----------\n    apparent_elevation : numeric\n        The apparent elevation of the sun above the horizon (deg).\n\n    aod700 : numeric, default 0.1\n        The aerosol optical depth at 700 nm (unitless).\n        Algorithm derived for values between 0 and 0.45.\n\n    precipitable_water : numeric, default 1.0\n        The precipitable water of the atmosphere (cm).\n        Algorithm derived for values between 0.2 and 10 cm.\n        Values less than 0.2 will be assumed to be equal to 0.2.\n\n    pressure : numeric, default 101325.0\n        The atmospheric pressure (Pascals).\n        Algorithm derived for altitudes between sea level and 7000 m,\n        or 101325 and 41000 Pascals.\n\n    dni_extra : numeric, default 1364.0\n        Extraterrestrial irradiance. The units of ``dni_extra``\n        determine the units of the output.\n\n    Returns\n    -------\n    clearsky : DataFrame (if Series input) or OrderedDict of arrays\n        DataFrame/OrderedDict contains the columns/keys\n        ``'dhi', 'dni', 'ghi'``.\n\n    References\n    ----------\n    .. [1] P. Ineichen, \"A broadband simplified version of the\n       Solis clear sky model,\" Solar Energy, 82, 758-762 (2008).\n\n    .. [2] P. Ineichen, \"Validation of models that estimate the clear\n       sky global and beam solar irradiance,\" Solar Energy, 132,\n       332-344 (2016).\n    \"\"\"\n\n    p = pressure\n\n    w = precipitable_water\n\n    # algorithm fails for pw < 0.2\n    w = np.maximum(w, 0.2)\n\n    # this algorithm is reasonably fast already, but it could be made\n    # faster by precalculating the powers of aod700, the log(p/p0), and\n    # the log(w) instead of repeating the calculations as needed in each\n    # function\n\n    i0p = _calc_i0p(dni_extra, w, aod700, p)\n\n    taub = _calc_taub(w, aod700, p)\n    b = _calc_b(w, aod700)\n\n    taug = _calc_taug(w, aod700, p)\n    g = _calc_g(w, aod700)\n\n    taud = _calc_taud(w, aod700, p)\n    d = _calc_d(aod700, p)\n\n    # this prevents the creation of nans at night instead of 0s\n    # it's also friendly to scalar and series inputs\n    sin_elev = np.maximum(1.e-30, np.sin(np.radians(apparent_elevation)))\n\n    dni = i0p * np.exp(-taub/sin_elev**b)\n    ghi = i0p * np.exp(-taug/sin_elev**g) * sin_elev\n    dhi = i0p * np.exp(-taud/sin_elev**d)\n\n    irrads = OrderedDict()\n    irrads['ghi'] = ghi\n    irrads['dni'] = dni\n    irrads['dhi'] = dhi\n\n    if isinstance(dni, pd.Series):\n        irrads = pd.DataFrame.from_dict(irrads)\n\n    return irrads", "response": "A broadband simplified version of the clear sky GHI DNI and DHI model."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the enhanced extraterrestrial irradiance.", "response": "def _calc_i0p(i0, w, aod700, p):\n    \"\"\"Calculate the \"enhanced extraterrestrial irradiance\".\"\"\"\n    p0 = 101325.\n    io0 = 1.08 * w**0.0051\n    i01 = 0.97 * w**0.032\n    i02 = 0.12 * w**0.56\n    i0p = i0 * (i02*aod700**2 + i01*aod700 + io0 + 0.071*np.log(p/p0))\n\n    return i0p"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _calc_taub(w, aod700, p):\n    p0 = 101325.\n    tb1 = 1.82 + 0.056*np.log(w) + 0.0071*np.log(w)**2\n    tb0 = 0.33 + 0.045*np.log(w) + 0.0096*np.log(w)**2\n    tbp = 0.0089*w + 0.13\n\n    taub = tb1*aod700 + tb0 + tbp*np.log(p/p0)\n\n    return taub", "response": "Calculate the taub coefficient"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calc_b(w, aod700):\n\n    b1 = 0.00925*aod700**2 + 0.0148*aod700 - 0.0172\n    b0 = -0.7565*aod700**2 + 0.5057*aod700 + 0.4557\n\n    b = b1 * np.log(w) + b0\n\n    return b", "response": "Calculate the b coefficient."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate the taug coefficient", "response": "def _calc_taug(w, aod700, p):\n    \"\"\"Calculate the taug coefficient\"\"\"\n    p0 = 101325.\n    tg1 = 1.24 + 0.047*np.log(w) + 0.0061*np.log(w)**2\n    tg0 = 0.27 + 0.043*np.log(w) + 0.0090*np.log(w)**2\n    tgp = 0.0079*w + 0.1\n    taug = tg1*aod700 + tg0 + tgp*np.log(p/p0)\n\n    return taug"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate the g coefficient.", "response": "def _calc_g(w, aod700):\n    \"\"\"Calculate the g coefficient.\"\"\"\n\n    g = -0.0147*np.log(w) - 0.3079*aod700**2 + 0.2846*aod700 + 0.3798\n\n    return g"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _calc_taud(w, aod700, p):\n\n    # isscalar tests needed to ensure that the arrays will have the\n    # right shape in the tds calculation.\n    # there's probably a better way to do this.\n\n    if np.isscalar(w) and np.isscalar(aod700):\n        w = np.array([w])\n        aod700 = np.array([aod700])\n    elif np.isscalar(w):\n        w = np.full_like(aod700, w)\n    elif np.isscalar(aod700):\n        aod700 = np.full_like(w, aod700)\n\n    # set up nan-tolerant masks\n    aod700_lt_0p05 = np.full_like(aod700, False, dtype='bool')\n    np.less(aod700, 0.05, where=~np.isnan(aod700), out=aod700_lt_0p05)\n    aod700_mask = np.array([aod700_lt_0p05, ~aod700_lt_0p05], dtype=np.int)\n\n    # create tuples of coefficients for\n    # aod700 < 0.05, aod700 >= 0.05\n    td4 = 86*w - 13800, -0.21*w + 11.6\n    td3 = -3.11*w + 79.4, 0.27*w - 20.7\n    td2 = -0.23*w + 74.8, -0.134*w + 15.5\n    td1 = 0.092*w - 8.86, 0.0554*w - 5.71\n    td0 = 0.0042*w + 3.12, 0.0057*w + 2.94\n    tdp = -0.83*(1+aod700)**(-17.2), -0.71*(1+aod700)**(-15.0)\n\n    tds = (np.array([td0, td1, td2, td3, td4, tdp]) * aod700_mask).sum(axis=1)\n\n    p0 = 101325.\n    taud = (tds[4]*aod700**4 + tds[3]*aod700**3 + tds[2]*aod700**2 +\n            tds[1]*aod700 + tds[0] + tds[5]*np.log(p/p0))\n\n    # be polite about matching the output type to the input type(s)\n    if len(taud) == 1:\n        taud = taud[0]\n\n    return taud", "response": "Calculate the taud coefficient."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _calc_d(aod700, p):\n\n    p0 = 101325.\n    dp = 1/(18 + 152*aod700)\n    d = -0.337*aod700**2 + 0.63*aod700 + 0.116 + dp*np.log(p/p0)\n\n    return d", "response": "Calculate the d coefficient."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef detect_clearsky(measured, clearsky, times, window_length,\n                    mean_diff=75, max_diff=75,\n                    lower_line_length=-5, upper_line_length=10,\n                    var_diff=0.005, slope_dev=8, max_iterations=20,\n                    return_components=False):\n    \"\"\"\n    Detects clear sky times according to the algorithm developed by Reno\n    and Hansen for GHI measurements [1]. The algorithm was designed and\n    validated for analyzing GHI time series only. Users may attempt to\n    apply it to other types of time series data using different filter\n    settings, but should be skeptical of the results.\n\n    The algorithm detects clear sky times by comparing statistics for a\n    measured time series and an expected clearsky time series.\n    Statistics are calculated using a sliding time window (e.g., 10\n    minutes). An iterative algorithm identifies clear periods, uses the\n    identified periods to estimate bias in the clearsky data, scales the\n    clearsky data and repeats.\n\n    Clear times are identified by meeting 5 criteria. Default values for\n    these thresholds are appropriate for 10 minute windows of 1 minute\n    GHI data.\n\n    Parameters\n    ----------\n    measured : array or Series\n        Time series of measured values.\n    clearsky : array or Series\n        Time series of the expected clearsky values.\n    times : DatetimeIndex\n        Times of measured and clearsky values.\n    window_length : int\n        Length of sliding time window in minutes. Must be greater than 2\n        periods.\n    mean_diff : float, default 75\n        Threshold value for agreement between mean values of measured\n        and clearsky in each interval, see Eq. 6 in [1].\n    max_diff : float, default 75\n        Threshold value for agreement between maxima of measured and\n        clearsky values in each interval, see Eq. 7 in [1].\n    lower_line_length : float, default -5\n        Lower limit of line length criterion from Eq. 8 in [1].\n        Criterion satisfied when\n        lower_line_length < line length difference < upper_line_length\n    upper_line_length : float, default 10\n        Upper limit of line length criterion from Eq. 8 in [1].\n    var_diff : float, default 0.005\n        Threshold value in Hz for the agreement between normalized\n        standard deviations of rate of change in irradiance, see Eqs. 9\n        through 11 in [1].\n    slope_dev : float, default 8\n        Threshold value for agreement between the largest magnitude of\n        change in successive values, see Eqs. 12 through 14 in [1].\n    max_iterations : int, default 20\n        Maximum number of times to apply a different scaling factor to\n        the clearsky and redetermine clear_samples. Must be 1 or larger.\n    return_components : bool, default False\n        Controls if additional output should be returned. See below.\n\n    Returns\n    -------\n    clear_samples : array or Series\n        Boolean array or Series of whether or not the given time is\n        clear. Return type is the same as the input type.\n\n    components : OrderedDict, optional\n        Dict of arrays of whether or not the given time window is clear\n        for each condition. Only provided if return_components is True.\n\n    alpha : scalar, optional\n        Scaling factor applied to the clearsky_ghi to obtain the\n        detected clear_samples. Only provided if return_components is\n        True.\n\n    References\n    ----------\n    [1] Reno, M.J. and C.W. Hansen, \"Identification of periods of clear\n    sky irradiance in time series of GHI measurements\" Renewable Energy,\n    v90, p. 520-531, 2016.\n\n    Notes\n    -----\n    Initial implementation in MATLAB by Matthew Reno. Modifications for\n    computational efficiency by Joshua Patrick and Curtis Martin. Ported\n    to Python by Will Holmgren, Tony Lorenzo, and Cliff Hansen.\n\n    Differences from MATLAB version:\n\n        * no support for unequal times\n        * automatically determines sample_interval\n        * requires a reference clear sky series instead calculating one\n          from a user supplied location and UTCoffset\n        * parameters are controllable via keyword arguments\n        * option to return individual test components and clearsky scaling\n          parameter\n    \"\"\"\n\n    # calculate deltas in units of minutes (matches input window_length units)\n    deltas = np.diff(times.values) / np.timedelta64(1, '60s')\n\n    # determine the unique deltas and if we can proceed\n    unique_deltas = np.unique(deltas)\n    if len(unique_deltas) == 1:\n        sample_interval = unique_deltas[0]\n    else:\n        raise NotImplementedError('algorithm does not yet support unequal '\n                                  'times. consider resampling your data.')\n\n    intervals_per_window = int(window_length / sample_interval)\n\n    # generate matrix of integers for creating windows with indexing\n    from scipy.linalg import hankel\n    H = hankel(np.arange(intervals_per_window),                   # noqa: N806\n               np.arange(intervals_per_window - 1, len(times)))\n\n    # calculate measurement statistics\n    meas_mean = np.mean(measured[H], axis=0)\n    meas_max = np.max(measured[H], axis=0)\n    meas_diff = np.diff(measured[H], n=1, axis=0)\n    meas_slope = np.diff(measured[H], n=1, axis=0) / sample_interval\n    # matlab std function normalizes by N-1, so set ddof=1 here\n    meas_slope_nstd = np.std(meas_slope, axis=0, ddof=1) / meas_mean\n    meas_line_length = np.sum(np.sqrt(\n        meas_diff * meas_diff +\n        sample_interval * sample_interval), axis=0)\n\n    # calculate clear sky statistics\n    clear_mean = np.mean(clearsky[H], axis=0)\n    clear_max = np.max(clearsky[H], axis=0)\n    clear_diff = np.diff(clearsky[H], n=1, axis=0)\n    clear_slope = np.diff(clearsky[H], n=1, axis=0) / sample_interval\n\n    from scipy.optimize import minimize_scalar\n\n    alpha = 1\n    for iteration in range(max_iterations):\n        clear_line_length = np.sum(np.sqrt(\n            alpha * alpha * clear_diff * clear_diff +\n            sample_interval * sample_interval), axis=0)\n\n        line_diff = meas_line_length - clear_line_length\n\n        # evaluate comparison criteria\n        c1 = np.abs(meas_mean - alpha*clear_mean) < mean_diff\n        c2 = np.abs(meas_max - alpha*clear_max) < max_diff\n        c3 = (line_diff > lower_line_length) & (line_diff < upper_line_length)\n        c4 = meas_slope_nstd < var_diff\n        c5 = np.max(np.abs(meas_slope -\n                           alpha * clear_slope), axis=0) < slope_dev\n        c6 = (clear_mean != 0) & ~np.isnan(clear_mean)\n        clear_windows = c1 & c2 & c3 & c4 & c5 & c6\n\n        # create array to return\n        clear_samples = np.full_like(measured, False, dtype='bool')\n        # find the samples contained in any window classified as clear\n        clear_samples[np.unique(H[:, clear_windows])] = True\n\n        # find a new alpha\n        previous_alpha = alpha\n        clear_meas = measured[clear_samples]\n        clear_clear = clearsky[clear_samples]\n\n        def rmse(alpha):\n            return np.sqrt(np.mean((clear_meas - alpha*clear_clear)**2))\n\n        alpha = minimize_scalar(rmse).x\n        if round(alpha*10000) == round(previous_alpha*10000):\n            break\n    else:\n        import warnings\n        warnings.warn('failed to converge after %s iterations'\n                      % max_iterations, RuntimeWarning)\n\n    # be polite about returning the same type as was input\n    if isinstance(measured, pd.Series):\n        clear_samples = pd.Series(clear_samples, index=times)\n\n    if return_components:\n        components = OrderedDict()\n        components['mean_diff_flag'] = c1\n        components['max_diff_flag'] = c2\n        components['line_length_flag'] = c3\n        components['slope_nstd_flag'] = c4\n        components['slope_max_flag'] = c5\n        components['mean_nan_flag'] = c6\n        components['windows'] = clear_windows\n\n        components['mean_diff'] = np.abs(meas_mean - alpha * clear_mean)\n        components['max_diff'] = np.abs(meas_max - alpha * clear_max)\n        components['line_length'] = meas_line_length - clear_line_length\n        components['slope_nstd'] = meas_slope_nstd\n        components['slope_max'] = (np.max(\n            meas_slope - alpha * clear_slope, axis=0))\n\n        return clear_samples, components, alpha\n    else:\n        return clear_samples", "response": "This function detects clear sky times according to the algorithm developed by Reno\n    and Hansen for GHI measurements."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreads in a daily NOAA SURFRAD file and return a tuple of the data and metadata.", "response": "def read_surfrad(filename, map_variables=True):\n    \"\"\"Read in a daily NOAA SURFRAD[1] file.\n\n    Parameters\n    ----------\n    filename: str\n        Filepath or url.\n    map_variables: bool\n        When true, renames columns of the Dataframe to pvlib variable names\n        where applicable. See variable SURFRAD_COLUMNS.\n\n    Returns\n    -------\n    Tuple of the form (data, metadata).\n\n    data: Dataframe\n        Dataframe with the fields found below.\n    metadata: dict\n        Site metadata included in the file.\n\n    Notes\n    -----\n    Metadata dictionary includes the following fields:\n\n    ===============  ======  ===============\n    Key              Format  Description\n    ===============  ======  ===============\n    station          String  site name\n    latitude         Float   site latitude\n    longitude        Float   site longitude\n    elevation        Int     site elevation\n    surfrad_version  Int     surfrad version\n    tz               String  Timezone (UTC)\n    ===============  ======  ===============\n\n    Dataframe includes the following fields:\n\n    =======================  ======  ==========================================\n    raw, mapped              Format  Description\n    =======================  ======  ==========================================\n    **Mapped field names are returned when the map_variables argument is True**\n    ---------------------------------------------------------------------------\n    year                     int     year as 4 digit int\n    jday                     int     day of year 1-365(or 366)\n    month                    int     month (1-12)\n    day                      int     day of month(1-31)\n    hour                     int     hour (0-23)\n    minute                   int     minute (0-59)\n    dt                       float   decimal time i.e. 23.5 = 2330\n    zen, solar_zenith        float   solar zenith angle (deg)\n    **Fields below have associated qc flags labeled <field>_flag.**\n    ---------------------------------------------------------------------------\n    dw_solar, ghi            float   downwelling global solar(W/m^2)\n    uw_solar                 float   updownwelling global solar(W/m^2)\n    direct_n, dni            float   direct normal solar (W/m^2)\n    diffuse, dhi             float   downwelling diffuse solar (W/m^2)\n    dw_ir                    float   downwelling thermal infrared (W/m^2)\n    dw_casetemp              float   downwelling IR case temp (K)\n    dw_dometemp              float   downwelling IR dome temp (K)\n    uw_ir                    float   upwelling thermal infrared (W/m^2)\n    uw_casetemp              float   upwelling IR case temp (K)\n    uw_dometemp              float   upwelling IR case temp (K)\n    uvb                      float   global uvb (miliWatts/m^2)\n    par                      float   photosynthetically active radiation(W/m^2)\n    netsolar                 float   net solar (dw_solar - uw_solar) (W/m^2)\n    netir                    float   net infrared (dw_ir - uw_ir) (W/m^2)\n    totalnet                 float   net radiation (netsolar+netir) (W/m^2)\n    temp, temp_air           float   10-meter air temperature (?C)\n    rh, relative_humidity    float   relative humidity (%)\n    windspd, wind_speed      float   wind speed (m/s)\n    winddir, wind_direction  float   wind direction (deg, clockwise from north)\n    pressure                 float   station pressure (mb)\n    =======================  ======  ==========================================\n\n    See README files located in the station directories in the SURFRAD\n    data archives[2] for details on SURFRAD daily data files.\n\n    References\n    ----------\n    [1] NOAA Earth System Research Laboratory Surface Radiation Budget Network\n        `SURFRAD Homepage <https://www.esrl.noaa.gov/gmd/grad/surfrad/>`_\n    [2] NOAA SURFRAD Data Archive\n        `SURFRAD Archive <ftp://aftp.cmdl.noaa.gov/data/radiation/surfrad/>`_\n    \"\"\"\n    if filename.startswith('ftp'):\n        req = Request(filename)\n        response = urlopen(req)\n        file_buffer = io.StringIO(response.read().decode(errors='ignore'))\n    else:\n        file_buffer = open(filename, 'r')\n\n    # Read and parse the first two lines to build the metadata dict.\n    station = file_buffer.readline()\n    file_metadata = file_buffer.readline()\n\n    metadata_list = file_metadata.split()\n    metadata = {}\n    metadata['name'] = station.strip()\n    metadata['latitude'] = float(metadata_list[0])\n    metadata['longitude'] = float(metadata_list[1])\n    metadata['elevation'] = float(metadata_list[2])\n    metadata['surfrad_version'] = int(metadata_list[-1])\n    metadata['tz'] = 'UTC'\n\n    data = pd.read_csv(file_buffer, delim_whitespace=True,\n                       header=None, names=SURFRAD_COLUMNS)\n    file_buffer.close()\n\n    data = format_index(data)\n    missing = data == -9999.9\n    data = data.where(~missing, np.NaN)\n\n    if map_variables:\n        data.rename(columns=VARIABLE_MAP, inplace=True)\n    return data, metadata"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a UTC localized DatetimeIndex for the dataframe.", "response": "def format_index(data):\n    \"\"\"Create UTC localized DatetimeIndex for the dataframe.\n\n    Parameters\n    ----------\n    data: Dataframe\n        Must contain columns 'year', 'jday', 'hour' and\n        'minute'.\n\n    Return\n    ------\n    data: Dataframe\n        Dataframe with a DatetimeIndex localized to UTC.\n    \"\"\"\n    year = data.year.apply(str)\n    jday = data.jday.apply(lambda x: '{:03d}'.format(x))\n    hours = data.hour.apply(lambda x: '{:02d}'.format(x))\n    minutes = data.minute.apply(lambda x: '{:02d}'.format(x))\n    index = pd.to_datetime(year + jday + hours + minutes, format=\"%Y%j%H%M\")\n    data.index = index\n    data = data.tz_localize('UTC')\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses to display deprecation in a standard way. Parameters ---------- since : str The release at which this API became deprecated. message : str, optional Override the default deprecation message. The format specifier `%(name)s` may be used for the name of the function, and `%(alternative)s` may be used in the deprecation message to insert the name of an alternative to the deprecated function. `%(obj_type)s` may be used to insert a friendly name for the type of object being deprecated. name : str, optional The name of the deprecated object. alternative : str, optional An alternative API that the user may use in place of the deprecated API. The deprecation warning will tell the user about this alternative if provided. pending : bool, optional If True, uses a PendingDeprecationWarning instead of a DeprecationWarning. Cannot be used together with *removal*. removal : str, optional The expected removal version. With the default (an empty string), a removal version is automatically computed from *since*. Set to other Falsy values to not schedule a removal date. Cannot be used together with *pending*. obj_type : str, optional The object type being deprecated. addendum : str, optional Additional text appended directly to the final message. Examples -------- Basic example:: # To warn of the deprecation of \"matplotlib.name_of_module\" warn_deprecated('1.4.0', name='matplotlib.name_of_module', obj_type='module')", "response": "def warn_deprecated(\n        since, message='', name='', alternative='', pending=False,\n        obj_type='attribute', addendum='', removal=''):\n    \"\"\"\n    Used to display deprecation in a standard way.\n    Parameters\n    ----------\n    since : str\n        The release at which this API became deprecated.\n    message : str, optional\n        Override the default deprecation message.  The format\n        specifier `%(name)s` may be used for the name of the function,\n        and `%(alternative)s` may be used in the deprecation message\n        to insert the name of an alternative to the deprecated\n        function.  `%(obj_type)s` may be used to insert a friendly name\n        for the type of object being deprecated.\n    name : str, optional\n        The name of the deprecated object.\n    alternative : str, optional\n        An alternative API that the user may use in place of the deprecated\n        API.  The deprecation warning will tell the user about this alternative\n        if provided.\n    pending : bool, optional\n        If True, uses a PendingDeprecationWarning instead of a\n        DeprecationWarning.  Cannot be used together with *removal*.\n    removal : str, optional\n        The expected removal version.  With the default (an empty string), a\n        removal version is automatically computed from *since*.  Set to other\n        Falsy values to not schedule a removal date.  Cannot be used together\n        with *pending*.\n    obj_type : str, optional\n        The object type being deprecated.\n    addendum : str, optional\n        Additional text appended directly to the final message.\n    Examples\n    --------\n        Basic example::\n            # To warn of the deprecation of \"matplotlib.name_of_module\"\n            warn_deprecated('1.4.0', name='matplotlib.name_of_module',\n                            obj_type='module')\n    \"\"\"\n    message = '\\n' + _generate_deprecation_message(\n        since, message, name, alternative, pending, obj_type, addendum,\n        removal=removal)\n    category = (PendingDeprecationWarning if pending\n                else _projectWarning)\n    warnings.warn(message, category, stacklevel=2)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a new version of a deprecated function or a class.", "response": "def deprecated(since, message='', name='', alternative='', pending=False,\n               addendum='', removal=''):\n    \"\"\"\n    Decorator to mark a function or a class as deprecated.\n    Parameters\n    ----------\n    since : str\n        The release at which this API became deprecated.  This is\n        required.\n    message : str, optional\n        Override the default deprecation message.  The format\n        specifier `%(name)s` may be used for the name of the object,\n        and `%(alternative)s` may be used in the deprecation message\n        to insert the name of an alternative to the deprecated\n        object.\n    name : str, optional\n        The name of the deprecated object; if not provided the name\n        is automatically determined from the passed in object,\n        though this is useful in the case of renamed functions, where\n        the new function is just assigned to the name of the\n        deprecated function.  For example::\n            def new_function():\n                ...\n            oldFunction = new_function\n    alternative : str, optional\n        An alternative API that the user may use in place of the deprecated\n        API.  The deprecation warning will tell the user about this alternative\n        if provided.\n    pending : bool, optional\n        If True, uses a PendingDeprecationWarning instead of a\n        DeprecationWarning.  Cannot be used together with *removal*.\n    removal : str, optional\n        The expected removal version.  With the default (an empty string), a\n        removal version is automatically computed from *since*.  Set to other\n        Falsy values to not schedule a removal date.  Cannot be used together\n        with *pending*.\n    addendum : str, optional\n        Additional text appended directly to the final message.\n    Examples\n    --------\n        Basic example::\n            @deprecated('1.4.0')\n            def the_function_to_deprecate():\n                pass\n    \"\"\"\n\n    def deprecate(obj, message=message, name=name, alternative=alternative,\n                  pending=pending, addendum=addendum):\n\n        if not name:\n            name = obj.__name__\n\n        if isinstance(obj, type):\n            obj_type = \"class\"\n            old_doc = obj.__doc__\n            func = obj.__init__\n\n            def finalize(wrapper, new_doc):\n                obj.__doc__ = new_doc\n                obj.__init__ = wrapper\n                return obj\n        else:\n            obj_type = \"function\"\n            if isinstance(obj, classmethod):\n                func = obj.__func__\n                old_doc = func.__doc__\n\n                def finalize(wrapper, new_doc):\n                    wrapper = functools.wraps(func)(wrapper)\n                    wrapper.__doc__ = new_doc\n                    return classmethod(wrapper)\n            else:\n                func = obj\n                old_doc = func.__doc__\n\n                def finalize(wrapper, new_doc):\n                    wrapper = functools.wraps(func)(wrapper)\n                    wrapper.__doc__ = new_doc\n                    return wrapper\n\n        message = _generate_deprecation_message(\n            since, message, name, alternative, pending, obj_type, addendum,\n            removal=removal)\n        category = (PendingDeprecationWarning if pending\n                    else _projectWarning)\n\n        def wrapper(*args, **kwargs):\n            warnings.warn(message, category, stacklevel=2)\n            return func(*args, **kwargs)\n\n        old_doc = textwrap.dedent(old_doc or '').strip('\\n')\n        message = message.strip()\n        new_doc = (('\\n.. deprecated:: %(since)s'\n                    '\\n    %(message)s\\n\\n' %\n                    {'since': since, 'message': message}) + old_doc)\n        if not old_doc:\n            # This is to prevent a spurious 'unexected unindent' warning from\n            # docutils when the original docstring was blank.\n            new_doc += r'\\ '\n\n        return finalize(wrapper, new_doc)\n\n    return deprecate"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef format_index(data):\n    tz_raw = data.columns[1]\n    timezone = TZ_MAP.get(tz_raw, tz_raw)\n    datetime = data['DATE (MM/DD/YYYY)'] + data[tz_raw]\n    datetime = pd.to_datetime(datetime, format='%m/%d/%Y%H:%M')\n    data = data.set_index(datetime)\n    data = data.tz_localize(timezone)\n    return data", "response": "Create DatetimeIndex for the Dataframe localized to the provided timezone."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate DatetimeIndex for the Dataframe localized to the provided timezone.", "response": "def format_index_raw(data):\n    \"\"\"Create DatetimeIndex for the Dataframe localized to the timezone provided\n    as the label of the third column.\n\n    Parameters\n    ----------\n    data: Dataframe\n        Must contain columns 'Year' and 'DOY'. Timezone must be found as the\n        label of the third (time) column.\n\n    Returns\n    -------\n    data: Dataframe\n        The data with a Datetime index localized to the provided timezone.\n    \"\"\"\n    tz_raw = data.columns[3]\n    timezone = TZ_MAP.get(tz_raw, tz_raw)\n    year = data.Year.apply(str)\n    jday = data.DOY.apply(lambda x: '{:03d}'.format(x))\n    time = data[tz_raw].apply(lambda x: '{:04d}'.format(x))\n    index = pd.to_datetime(year + jday + time, format=\"%Y%j%H%M\")\n    data = data.set_index(index)\n    data = data.tz_localize(timezone)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_midc(filename, variable_map=VARIABLE_MAP, raw_data=False):\n    data = pd.read_csv(filename)\n    if raw_data:\n        data = format_index_raw(data)\n    else:\n        data = format_index(data)\n    mapper = partial(map_midc_to_pvlib, variable_map)\n    data = data.rename(columns=mapper)\n    return data", "response": "Read in National Renewable Energy Laboratory Measurement and Instrumentation Data Center files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrequest and read MIDC data directly from the raw data api.", "response": "def read_midc_raw_data_from_nrel(site, start, end):\n    \"\"\"Request and read MIDC data directly from the raw data api.\n\n    Parameters\n    ----------\n    site: string\n        The MIDC station id.\n    start: datetime\n        Start date for requested data.\n    end: datetime\n        End date for requested data.\n\n    Returns\n    -------\n    data:\n        Dataframe with DatetimeIndex localized to the station location.\n\n    Notes\n    -----\n    Requests spanning an instrumentation change will yield an error. See the\n    MIDC raw data api page\n    `here <https://midcdmz.nrel.gov/apps/data_api_doc.pl?_idtextlist>`_\n    for more details and considerations.\n    \"\"\"\n    args = {'site': site,\n            'begin': start.strftime('%Y%m%d'),\n            'end': end.strftime('%Y%m%d')}\n    endpoint = 'https://midcdmz.nrel.gov/apps/data_api.pl?'\n    url = endpoint + '&'.join(['{}={}'.format(k, v) for k, v in args.items()])\n    return read_midc(url, raw_data=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_relative_airmass(zenith, model='kastenyoung1989'):\n    '''\n    Gives the relative (not pressure-corrected) airmass.\n\n    Gives the airmass at sea-level when given a sun zenith angle (in\n    degrees). The ``model`` variable allows selection of different\n    airmass models (described below). If ``model`` is not included or is\n    not valid, the default model is 'kastenyoung1989'.\n\n    Parameters\n    ----------\n    zenith : numeric\n        Zenith angle of the sun in degrees. Note that some models use\n        the apparent (refraction corrected) zenith angle, and some\n        models use the true (not refraction-corrected) zenith angle. See\n        model descriptions to determine which type of zenith angle is\n        required. Apparent zenith angles must be calculated at sea level.\n\n    model : string, default 'kastenyoung1989'\n        Available models include the following:\n\n        * 'simple' - secant(apparent zenith angle) -\n          Note that this gives -inf at zenith=90\n        * 'kasten1966' - See reference [1] -\n          requires apparent sun zenith\n        * 'youngirvine1967' - See reference [2] -\n          requires true sun zenith\n        * 'kastenyoung1989' - See reference [3] -\n          requires apparent sun zenith\n        * 'gueymard1993' - See reference [4] -\n          requires apparent sun zenith\n        * 'young1994' - See reference [5] -\n          requries true sun zenith\n        * 'pickering2002' - See reference [6] -\n          requires apparent sun zenith\n\n    Returns\n    -------\n    airmass_relative : numeric\n        Relative airmass at sea level. Will return NaN values for any\n        zenith angle greater than 90 degrees.\n\n    References\n    ----------\n    [1] Fritz Kasten. \"A New Table and Approximation Formula for the\n    Relative Optical Air Mass\". Technical Report 136, Hanover, N.H.:\n    U.S. Army Material Command, CRREL.\n\n    [2] A. T. Young and W. M. Irvine, \"Multicolor Photoelectric\n    Photometry of the Brighter Planets,\" The Astronomical Journal, vol.\n    72, pp. 945-950, 1967.\n\n    [3] Fritz Kasten and Andrew Young. \"Revised optical air mass tables\n    and approximation formula\". Applied Optics 28:4735-4738\n\n    [4] C. Gueymard, \"Critical analysis and performance assessment of\n    clear sky solar irradiance models using theoretical and measured\n    data,\" Solar Energy, vol. 51, pp. 121-138, 1993.\n\n    [5] A. T. Young, \"AIR-MASS AND REFRACTION,\" Applied Optics, vol. 33,\n    pp. 1108-1110, Feb 1994.\n\n    [6] Keith A. Pickering. \"The Ancient Star Catalog\". DIO 12:1, 20,\n\n    [7] Matthew J. Reno, Clifford W. Hansen and Joshua S. Stein, \"Global\n    Horizontal Irradiance Clear Sky Models: Implementation and Analysis\"\n    Sandia Report, (2012).\n    '''\n\n    # need to filter first because python 2.7 does not support raising a\n    # negative number to a negative power.\n    z = np.where(zenith > 90, np.nan, zenith)\n    zenith_rad = np.radians(z)\n\n    model = model.lower()\n\n    if 'kastenyoung1989' == model:\n        am = (1.0 / (np.cos(zenith_rad) +\n              0.50572*(((6.07995 + (90 - z)) ** - 1.6364))))\n    elif 'kasten1966' == model:\n        am = 1.0 / (np.cos(zenith_rad) + 0.15*((93.885 - z) ** - 1.253))\n    elif 'simple' == model:\n        am = 1.0 / np.cos(zenith_rad)\n    elif 'pickering2002' == model:\n        am = (1.0 / (np.sin(np.radians(90 - z +\n              244.0 / (165 + 47.0 * (90 - z) ** 1.1)))))\n    elif 'youngirvine1967' == model:\n        sec_zen = 1.0 / np.cos(zenith_rad)\n        am = sec_zen * (1 - 0.0012 * (sec_zen * sec_zen - 1))\n    elif 'young1994' == model:\n        am = ((1.002432*((np.cos(zenith_rad)) ** 2) +\n              0.148386*(np.cos(zenith_rad)) + 0.0096467) /\n              (np.cos(zenith_rad) ** 3 +\n              0.149864*(np.cos(zenith_rad) ** 2) +\n              0.0102963*(np.cos(zenith_rad)) + 0.000303978))\n    elif 'gueymard1993' == model:\n        am = (1.0 / (np.cos(zenith_rad) +\n              0.00176759*(z)*((94.37515 - z) ** - 1.21563)))\n    else:\n        raise ValueError('%s is not a valid model for relativeairmass', model)\n\n    if isinstance(zenith, pd.Series):\n        am = pd.Series(am, index=zenith.index)\n\n    return am", "response": "Returns the relative airmass at sea - level."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef first_solar_spectral_correction(pw, airmass_absolute, module_type=None,\n                                    coefficients=None):\n    r\"\"\"\n    Spectral mismatch modifier based on precipitable water and absolute\n    (pressure corrected) airmass.\n\n    Estimates a spectral mismatch modifier M representing the effect on\n    module short circuit current of variation in the spectral\n    irradiance. M is estimated from absolute (pressure currected) air\n    mass, AMa, and precipitable water, Pwat, using the following\n    function:\n\n    .. math::\n\n        M = c_1 + c_2*AMa  + c_3*Pwat  + c_4*AMa^.5\n            + c_5*Pwat^.5 + c_6*AMa/Pwat^.5\n\n    Default coefficients are determined for several cell types with\n    known quantum efficiency curves, by using the Simple Model of the\n    Atmospheric Radiative Transfer of Sunshine (SMARTS) [1]_. Using\n    SMARTS, spectrums are simulated with all combinations of AMa and\n    Pwat where:\n\n       * 0.5 cm <= Pwat <= 5 cm\n       * 1.0 <= AMa <= 5.0\n       * Spectral range is limited to that of CMP11 (280 nm to 2800 nm)\n       * spectrum simulated on a plane normal to the sun\n       * All other parameters fixed at G173 standard\n\n    From these simulated spectra, M is calculated using the known\n    quantum efficiency curves. Multiple linear regression is then\n    applied to fit Eq. 1 to determine the coefficients for each module.\n\n    Based on the PVLIB Matlab function ``pvl_FSspeccorr`` by Mitchell\n    Lee and Alex Panchula, at First Solar, 2016 [2]_.\n\n    Parameters\n    ----------\n    pw : array-like\n        atmospheric precipitable water (cm).\n\n    airmass_absolute : array-like\n        absolute (pressure corrected) airmass.\n\n    module_type : None or string, default None\n        a string specifying a cell type. Can be lower or upper case\n        letters. Admits values of 'cdte', 'monosi', 'xsi', 'multisi',\n        'polysi'. If provided, this input selects coefficients for the\n        following default modules:\n\n            * 'cdte' - First Solar Series 4-2 CdTe modules.\n            * 'monosi', 'xsi' - First Solar TetraSun modules.\n            * 'multisi', 'polysi' - multi-crystalline silicon modules.\n            * 'cigs' - anonymous copper indium gallium selenide PV module\n            * 'asi' - anonymous amorphous silicon PV module\n\n        The module used to calculate the spectral correction\n        coefficients corresponds to the Mult-crystalline silicon\n        Manufacturer 2 Model C from [3]_. Spectral Response (SR) of CIGS\n        and a-Si modules used to derive coefficients can be found in [4]_\n\n    coefficients : None or array-like, default None\n        allows for entry of user defined spectral correction\n        coefficients. Coefficients must be of length 6. Derivation of\n        coefficients requires use of SMARTS and PV module quantum\n        efficiency curve. Useful for modeling PV module types which are\n        not included as defaults, or to fine tune the spectral\n        correction to a particular mono-Si, multi-Si, or CdTe PV module.\n        Note that the parameters for modules with very similar QE should\n        be similar, in most cases limiting the need for module specific\n        coefficients.\n\n    Returns\n    -------\n    modifier: array-like\n        spectral mismatch factor (unitless) which is can be multiplied\n        with broadband irradiance reaching a module's cells to estimate\n        effective irradiance, i.e., the irradiance that is converted to\n        electrical current.\n\n    References\n    ----------\n    .. [1] Gueymard, Christian. SMARTS2: a simple model of the atmospheric\n       radiative transfer of sunshine: algorithms and performance\n       assessment. Cocoa, FL: Florida Solar Energy Center, 1995.\n    .. [2] Lee, Mitchell, and Panchula, Alex. \"Spectral Correction for\n       Photovoltaic Module Performance Based on Air Mass and Precipitable\n       Water.\" IEEE Photovoltaic Specialists Conference, Portland, 2016\n    .. [3] Marion, William F., et al. User's Manual for Data for Validating\n       Models for PV Module Performance. National Renewable Energy\n       Laboratory, 2014. http://www.nrel.gov/docs/fy14osti/61610.pdf\n    .. [4] Schweiger, M. and Hermann, W, Influence of Spectral Effects\n        on Energy Yield of Different PV Modules: Comparison of Pwat and\n        MMF Approach, TUV Rheinland Energy GmbH report 21237296.003,\n        January 2017\n    \"\"\"\n\n    # --- Screen Input Data ---\n\n    # *** Pwat ***\n    # Replace Pwat Values below 0.1 cm with 0.1 cm to prevent model from\n    # diverging\"\n\n    if np.min(pw) < 0.1:\n        pw = np.maximum(pw, 0.1)\n        warn('Exceptionally low Pwat values replaced with 0.1 cm to prevent' +\n             ' model divergence')\n\n    # Warn user about Pwat data that is exceptionally high\n    if np.max(pw) > 8:\n        warn('Exceptionally high Pwat values. Check input data:' +\n             ' model may diverge in this range')\n\n    # *** AMa ***\n    # Replace Extremely High AM with AM 10 to prevent model divergence\n    # AM > 10 will only occur very close to sunset\n    if np.max(airmass_absolute) > 10:\n        airmass_absolute = np.minimum(airmass_absolute, 10)\n\n    # Warn user about AMa data that is exceptionally low\n    if np.min(airmass_absolute) < 0.58:\n        warn('Exceptionally low air mass: ' +\n             'model not intended for extra-terrestrial use')\n        # pvl_absoluteairmass(1,pvl_alt2pres(4340)) = 0.58 Elevation of\n        # Mina Pirquita, Argentian = 4340 m. Highest elevation city with\n        # population over 50,000.\n\n    _coefficients = {}\n    _coefficients['cdte'] = (\n        0.86273, -0.038948, -0.012506, 0.098871, 0.084658, -0.0042948)\n    _coefficients['monosi'] = (\n        0.85914, -0.020880, -0.0058853, 0.12029, 0.026814, -0.0017810)\n    _coefficients['xsi'] = _coefficients['monosi']\n    _coefficients['polysi'] = (\n        0.84090, -0.027539, -0.0079224, 0.13570, 0.038024, -0.0021218)\n    _coefficients['multisi'] = _coefficients['polysi']\n    _coefficients['cigs'] = (\n        0.85252, -0.022314, -0.0047216, 0.13666, 0.013342, -0.0008945)\n    _coefficients['asi'] = (\n        1.12094, -0.047620, -0.0083627, -0.10443, 0.098382, -0.0033818)\n\n    if module_type is not None and coefficients is None:\n        coefficients = _coefficients[module_type.lower()]\n    elif module_type is None and coefficients is not None:\n        pass\n    elif module_type is None and coefficients is None:\n        raise TypeError('No valid input provided, both module_type and ' +\n                        'coefficients are None')\n    else:\n        raise TypeError('Cannot resolve input, must supply only one of ' +\n                        'module_type and coefficients')\n\n    # Evaluate Spectral Shift\n    coeff = coefficients\n    ama = airmass_absolute\n    modifier = (\n        coeff[0] + coeff[1]*ama + coeff[2]*pw + coeff[3]*np.sqrt(ama) +\n        coeff[4]*np.sqrt(pw) + coeff[5]*ama/np.sqrt(pw))\n\n    return modifier", "response": "r Returns the first Solar Spectral Correction of the given module type and absolute pressure of the given module."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef kasten96_lt(airmass_absolute, precipitable_water, aod_bb):\n    # \"From numerically integrated spectral simulations done with Modtran\n    # (Berk, 1989), Molineaux (1998) obtained for the broadband optical depth\n    # of a clean and dry atmospshere (fictitious atmosphere that comprises only\n    # the effects of Rayleigh scattering and absorption by the atmosphere gases\n    # other than the water vapor) the following expression\"\n    # - P. Ineichen (2008)\n    delta_cda = -0.101 + 0.235 * airmass_absolute ** (-0.16)\n    # \"and the broadband water vapor optical depth where pwat is the integrated\n    # precipitable water vapor content of the atmosphere expressed in cm and am\n    # the optical air mass. The precision of these fits is better than 1% when\n    # compared with Modtran simulations in the range 1 < am < 5 and\n    # 0 < pwat < 5 cm at sea level\" - P. Ineichen (2008)\n    delta_w = 0.112 * airmass_absolute ** (-0.55) * precipitable_water ** 0.34\n    # broadband AOD\n    delta_a = aod_bb\n    # \"Then using the Kasten pyrheliometric formula (1980, 1996), the Linke\n    # turbidity at am = 2 can be written. The extension of the Linke turbidity\n    # coefficient to other values of air mass was published by Ineichen and\n    # Perez (2002)\" - P. Ineichen (2008)\n    lt = -(9.4 + 0.9 * airmass_absolute) * np.log(\n        np.exp(-airmass_absolute * (delta_cda + delta_w + delta_a))\n    ) / airmass_absolute\n    # filter out of extrapolated values\n    return lt", "response": "Calculates the Linke turbidity factor using Kasten pyrheliometric formula."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef localize_to_utc(time, location):\n    if isinstance(time, dt.datetime):\n        if time.tzinfo is None:\n            time = pytz.timezone(location.tz).localize(time)\n        time_utc = time.astimezone(pytz.utc)\n    else:\n        try:\n            time_utc = time.tz_convert('UTC')\n        except TypeError:\n            time_utc = time.tz_localize(location.tz).tz_convert('UTC')\n\n    return time_utc", "response": "Converts or localizes a time series to UTC."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef datetime_to_djd(time):\n\n    if time.tzinfo is None:\n        time_utc = pytz.utc.localize(time)\n    else:\n        time_utc = time.astimezone(pytz.utc)\n\n    djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))\n    djd = (time_utc - djd_start).total_seconds() * 1.0/(60 * 60 * 24)\n\n    return djd", "response": "Converts a datetime to the Dublin Julian Day"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconvert a Dublin Julian Day float to a datetime. datetime object", "response": "def djd_to_datetime(djd, tz='UTC'):\n    \"\"\"\n    Converts a Dublin Julian Day float to a datetime.datetime object\n\n    Parameters\n    ----------\n    djd : float\n        fractional days since 12/31/1899+0000\n    tz : str, default 'UTC'\n        timezone to localize the result to\n\n    Returns\n    -------\n    datetime.datetime\n       The resultant datetime localized to tz\n    \"\"\"\n\n    djd_start = pytz.utc.localize(dt.datetime(1899, 12, 31, 12))\n\n    utc_time = djd_start + dt.timedelta(days=djd)\n    return utc_time.astimezone(pytz.timezone(tz))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _doy_to_datetimeindex(doy, epoch_year=2014):\n    doy = np.atleast_1d(doy).astype('float')\n    epoch = pd.Timestamp('{}-12-31'.format(epoch_year - 1))\n    timestamps = [epoch + dt.timedelta(days=adoy) for adoy in doy]\n    return pd.DatetimeIndex(timestamps)", "response": "Convert a day of year scalar or array to a pd. DatetimeIndex."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nbuild the keyword arguments for the class.", "response": "def _build_kwargs(keys, input_dict):\n    \"\"\"\n    Parameters\n    ----------\n    keys : iterable\n        Typically a list of strings.\n    adict : dict-like\n        A dictionary from which to attempt to pull each key.\n\n    Returns\n    -------\n    kwargs : dict\n        A dictionary with only the keys that were in input_dict\n    \"\"\"\n\n    kwargs = {}\n    for key in keys:\n        try:\n            kwargs[key] = input_dict[key]\n        except KeyError:\n            pass\n\n    return kwargs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _array_newton(func, x0, fprime, args, tol, maxiter, fprime2,\n                  converged=False):\n    \"\"\"\n    A vectorized version of Newton, Halley, and secant methods for arrays. Do\n    not use this method directly. This method is called from :func:`newton`\n    when ``np.isscalar(x0)`` is true. For docstring, see :func:`newton`.\n    \"\"\"\n    try:\n        p = np.asarray(x0, dtype=float)\n    except TypeError:  # can't convert complex to float\n        p = np.asarray(x0)\n    failures = np.ones_like(p, dtype=bool)  # at start, nothing converged\n    nz_der = np.copy(failures)\n    if fprime is not None:\n        # Newton-Raphson method\n        for iteration in range(maxiter):\n            # first evaluate fval\n            fval = np.asarray(func(p, *args))\n            # If all fval are 0, all roots have been found, then terminate\n            if not fval.any():\n                failures = fval.astype(bool)\n                break\n            fder = np.asarray(fprime(p, *args))\n            nz_der = (fder != 0)\n            # stop iterating if all derivatives are zero\n            if not nz_der.any():\n                break\n            # Newton step\n            dp = fval[nz_der] / fder[nz_der]\n            if fprime2 is not None:\n                fder2 = np.asarray(fprime2(p, *args))\n                dp = dp / (1.0 - 0.5 * dp * fder2[nz_der] / fder[nz_der])\n            # only update nonzero derivatives\n            p[nz_der] -= dp\n            failures[nz_der] = np.abs(dp) >= tol  # items not yet converged\n            # stop iterating if there aren't any failures, not incl zero der\n            if not failures[nz_der].any():\n                break\n    else:\n        # Secant method\n        dx = np.finfo(float).eps**0.33\n        p1 = p * (1 + dx) + np.where(p >= 0, dx, -dx)\n        q0 = np.asarray(func(p, *args))\n        q1 = np.asarray(func(p1, *args))\n        active = np.ones_like(p, dtype=bool)\n        for iteration in range(maxiter):\n            nz_der = (q1 != q0)\n            # stop iterating if all derivatives are zero\n            if not nz_der.any():\n                p = (p1 + p) / 2.0\n                break\n            # Secant Step\n            dp = (q1 * (p1 - p))[nz_der] / (q1 - q0)[nz_der]\n            # only update nonzero derivatives\n            p[nz_der] = p1[nz_der] - dp\n            active_zero_der = ~nz_der & active\n            p[active_zero_der] = (p1 + p)[active_zero_der] / 2.0\n            active &= nz_der  # don't assign zero derivatives again\n            failures[nz_der] = np.abs(dp) >= tol  # not yet converged\n            # stop iterating if there aren't any failures, not incl zero der\n            if not failures[nz_der].any():\n                break\n            p1, p = p, p1\n            q0 = q1\n            q1 = np.asarray(func(p1, *args))\n    zero_der = ~nz_der & failures  # don't include converged with zero-ders\n    if zero_der.any():\n        # secant warnings\n        if fprime is None:\n            nonzero_dp = (p1 != p)\n            # non-zero dp, but infinite newton step\n            zero_der_nz_dp = (zero_der & nonzero_dp)\n            if zero_der_nz_dp.any():\n                rms = np.sqrt(\n                    sum((p1[zero_der_nz_dp] - p[zero_der_nz_dp]) ** 2)\n                )\n                warnings.warn('RMS of {:g} reached'.format(rms),\n                              RuntimeWarning)\n        # newton or halley warnings\n        else:\n            all_or_some = 'all' if zero_der.all() else 'some'\n            msg = '{:s} derivatives were zero'.format(all_or_some)\n            warnings.warn(msg, RuntimeWarning)\n    elif failures.any():\n        all_or_some = 'all' if failures.all() else 'some'\n        msg = '{0:s} failed to converge after {1:d} iterations'.format(\n            all_or_some, maxiter\n        )\n        if failures.all():\n            raise RuntimeError(msg)\n        warnings.warn(msg, RuntimeWarning)\n    if converged:\n        result = namedtuple('result', ('root', 'converged', 'zero_der'))\n        p = result(p, ~failures, zero_der)\n    return p", "response": "This function is used to iterate over the array of items in a newton - halley system."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef singleaxis(apparent_zenith, apparent_azimuth,\n               axis_tilt=0, axis_azimuth=0, max_angle=90,\n               backtrack=True, gcr=2.0/7.0):\n    \"\"\"\n    Determine the rotation angle of a single axis tracker using the\n    equations in [1] when given a particular sun zenith and azimuth\n    angle. backtracking may be specified, and if so, a ground coverage\n    ratio is required.\n\n    Rotation angle is determined in a panel-oriented coordinate system.\n    The tracker azimuth axis_azimuth defines the positive y-axis; the\n    positive x-axis is 90 degress clockwise from the y-axis and parallel\n    to the earth surface, and the positive z-axis is normal and oriented\n    towards the sun. Rotation angle tracker_theta indicates tracker\n    position relative to horizontal: tracker_theta = 0 is horizontal,\n    and positive tracker_theta is a clockwise rotation around the y axis\n    in the x, y, z coordinate system. For example, if tracker azimuth\n    axis_azimuth is 180 (oriented south), tracker_theta = 30 is a\n    rotation of 30 degrees towards the west, and tracker_theta = -90 is\n    a rotation to the vertical plane facing east.\n\n    Parameters\n    ----------\n    apparent_zenith : float, 1d array, or Series\n        Solar apparent zenith angles in decimal degrees.\n\n    apparent_azimuth : float, 1d array, or Series\n        Solar apparent azimuth angles in decimal degrees.\n\n    axis_tilt : float, default 0\n        The tilt of the axis of rotation (i.e, the y-axis defined by\n        axis_azimuth) with respect to horizontal, in decimal degrees.\n\n    axis_azimuth : float, default 0\n        A value denoting the compass direction along which the axis of\n        rotation lies. Measured in decimal degrees East of North.\n\n    max_angle : float, default 90\n        A value denoting the maximum rotation angle, in decimal degrees,\n        of the one-axis tracker from its horizontal position (horizontal\n        if axis_tilt = 0). A max_angle of 90 degrees allows the tracker\n        to rotate to a vertical position to point the panel towards a\n        horizon. max_angle of 180 degrees allows for full rotation.\n\n    backtrack : bool, default True\n        Controls whether the tracker has the capability to \"backtrack\"\n        to avoid row-to-row shading. False denotes no backtrack\n        capability. True denotes backtrack capability.\n\n    gcr : float, default 2.0/7.0\n        A value denoting the ground coverage ratio of a tracker system\n        which utilizes backtracking; i.e. the ratio between the PV array\n        surface area to total ground area. A tracker system with modules\n        2 meters wide, centered on the tracking axis, with 6 meters\n        between the tracking axes has a gcr of 2/6=0.333. If gcr is not\n        provided, a gcr of 2/7 is default. gcr must be <=1.\n\n    Returns\n    -------\n    dict or DataFrame with the following columns:\n\n    * tracker_theta: The rotation angle of the tracker.\n        tracker_theta = 0 is horizontal, and positive rotation angles are\n        clockwise.\n    * aoi: The angle-of-incidence of direct irradiance onto the\n        rotated panel surface.\n    * surface_tilt: The angle between the panel surface and the earth\n        surface, accounting for panel rotation.\n    * surface_azimuth: The azimuth of the rotated panel, determined by\n        projecting the vector normal to the panel's surface to the earth's\n        surface.\n\n    References\n    ----------\n    [1] Lorenzo, E et al., 2011, \"Tracking and back-tracking\", Prog. in\n    Photovoltaics: Research and Applications, v. 19, pp. 747-753.\n    \"\"\"\n\n    # MATLAB to Python conversion by\n    # Will Holmgren (@wholmgren), U. Arizona. March, 2015.\n\n    if isinstance(apparent_zenith, pd.Series):\n        index = apparent_zenith.index\n    else:\n        index = None\n\n    # convert scalars to arrays\n    apparent_azimuth = np.atleast_1d(apparent_azimuth)\n    apparent_zenith = np.atleast_1d(apparent_zenith)\n\n    if apparent_azimuth.ndim > 1 or apparent_zenith.ndim > 1:\n        raise ValueError('Input dimensions must not exceed 1')\n\n    # Calculate sun position x, y, z using coordinate system as in [1], Eq 2.\n\n    # Positive y axis is oriented parallel to earth surface along tracking axis\n    # (for the purpose of illustration, assume y is oriented to the south);\n    # positive x axis is orthogonal, 90 deg clockwise from y-axis, and parallel\n    # to the earth's surface (if y axis is south, x axis is west);\n    # positive z axis is normal to x, y axes, pointed upward.\n\n    # Equations in [1] assume solar azimuth is relative to reference vector\n    # pointed south, with clockwise positive.\n    # Here, the input solar azimuth is degrees East of North,\n    # i.e., relative to a reference vector pointed\n    # north with clockwise positive.\n    # Rotate sun azimuth to coordinate system as in [1]\n    # to calculate sun position.\n\n    az = apparent_azimuth - 180\n    apparent_elevation = 90 - apparent_zenith\n    x = cosd(apparent_elevation) * sind(az)\n    y = cosd(apparent_elevation) * cosd(az)\n    z = sind(apparent_elevation)\n\n    # translate array azimuth from compass bearing to [1] coord system\n    # wholmgren: strange to see axis_azimuth calculated differently from az,\n    # (not that it matters, or at least it shouldn't...).\n    axis_azimuth_south = axis_azimuth - 180\n\n    # translate input array tilt angle axis_tilt to [1] coordinate system.\n\n    # In [1] coordinates, axis_tilt is a rotation about the x-axis.\n    # For a system with array azimuth (y-axis) oriented south,\n    # the x-axis is oriented west, and a positive axis_tilt is a\n    # counterclockwise rotation, i.e, lifting the north edge of the panel.\n    # Thus, in [1] coordinate system, in the northern hemisphere a positive\n    # axis_tilt indicates a rotation toward the equator,\n    # whereas in the southern hemisphere rotation toward the equator is\n    # indicated by axis_tilt<0.  Here, the input axis_tilt is\n    # always positive and is a rotation toward the equator.\n\n    # Calculate sun position (xp, yp, zp) in panel-oriented coordinate system:\n    # positive y-axis is oriented along tracking axis at panel tilt;\n    # positive x-axis is orthogonal, clockwise, parallel to earth surface;\n    # positive z-axis is normal to x-y axes, pointed upward.\n    # Calculate sun position (xp,yp,zp) in panel coordinates using [1] Eq 11\n    # note that equation for yp (y' in Eq. 11 of Lorenzo et al 2011) is\n    # corrected, after conversation with paper's authors.\n\n    xp = x*cosd(axis_azimuth_south) - y*sind(axis_azimuth_south)\n    yp = (x*cosd(axis_tilt)*sind(axis_azimuth_south) +\n          y*cosd(axis_tilt)*cosd(axis_azimuth_south) -\n          z*sind(axis_tilt))\n    zp = (x*sind(axis_tilt)*sind(axis_azimuth_south) +\n          y*sind(axis_tilt)*cosd(axis_azimuth_south) +\n          z*cosd(axis_tilt))\n\n    # The ideal tracking angle wid is the rotation to place the sun position\n    # vector (xp, yp, zp) in the (y, z) plane; i.e., normal to the panel and\n    # containing the axis of rotation.  wid = 0 indicates that the panel is\n    # horizontal.  Here, our convention is that a clockwise rotation is\n    # positive, to view rotation angles in the same frame of reference as\n    # azimuth.  For example, for a system with tracking axis oriented south,\n    # a rotation toward the east is negative, and a rotation to the west is\n    # positive.\n\n    # Use arctan2 and avoid the tmp corrections.\n\n    # angle from x-y plane to projection of sun vector onto x-z plane\n#     tmp = np.degrees(np.arctan(zp/xp))\n\n    # Obtain wid by translating tmp to convention for rotation angles.\n    # Have to account for which quadrant of the x-z plane in which the sun\n    # vector lies.  Complete solution here but probably not necessary to\n    # consider QIII and QIV.\n#     wid = pd.Series(index=times)\n#     wid[(xp>=0) & (zp>=0)] =  90 - tmp[(xp>=0) & (zp>=0)]  # QI\n#     wid[(xp<0)  & (zp>=0)] = -90 - tmp[(xp<0)  & (zp>=0)]  # QII\n#     wid[(xp<0)  & (zp<0)]  = -90 - tmp[(xp<0)  & (zp<0)]   # QIII\n#     wid[(xp>=0) & (zp<0)]  =  90 - tmp[(xp>=0) & (zp<0)]   # QIV\n\n    # Calculate angle from x-y plane to projection of sun vector onto x-z plane\n    # and then obtain wid by translating tmp to convention for rotation angles.\n    wid = 90 - np.degrees(np.arctan2(zp, xp))\n\n    # filter for sun above panel horizon\n    zen_gt_90 = apparent_zenith > 90\n    wid[zen_gt_90] = np.nan\n\n    # Account for backtracking; modified from [1] to account for rotation\n    # angle convention being used here.\n    if backtrack:\n        axes_distance = 1/gcr\n        temp = np.minimum(axes_distance*cosd(wid), 1)\n\n        # backtrack angle\n        # (always positive b/c acosd returns values between 0 and 180)\n        wc = np.degrees(np.arccos(temp))\n\n        # Eq 4 applied when wid in QIV (wid < 0 evalulates True), QI\n        tracker_theta = np.where(wid < 0, wid + wc, wid - wc)\n    else:\n        tracker_theta = wid\n\n    tracker_theta[tracker_theta > max_angle] = max_angle\n    tracker_theta[tracker_theta < -max_angle] = -max_angle\n\n    # calculate panel normal vector in panel-oriented x, y, z coordinates.\n    # y-axis is axis of tracker rotation.  tracker_theta is a compass angle\n    # (clockwise is positive) rather than a trigonometric angle.\n    # the *0 is a trick to preserve NaN values.\n    panel_norm = np.array([sind(tracker_theta),\n                           tracker_theta*0,\n                           cosd(tracker_theta)])\n\n    # sun position in vector format in panel-oriented x, y, z coordinates\n    sun_vec = np.array([xp, yp, zp])\n\n    # calculate angle-of-incidence on panel\n    aoi = np.degrees(np.arccos(np.abs(np.sum(sun_vec*panel_norm, axis=0))))\n\n    # calculate panel tilt and azimuth\n    # in a coordinate system where the panel tilt is the\n    # angle from horizontal, and the panel azimuth is\n    # the compass angle (clockwise from north) to the projection\n    # of the panel's normal to the earth's surface.\n    # These outputs are provided for convenience and comparison\n    # with other PV software which use these angle conventions.\n\n    # project normal vector to earth surface.\n    # First rotate about x-axis by angle -axis_tilt so that y-axis is\n    # also parallel to earth surface, then project.\n\n    # Calculate standard rotation matrix\n    rot_x = np.array([[1, 0, 0],\n                      [0, cosd(-axis_tilt), -sind(-axis_tilt)],\n                      [0, sind(-axis_tilt), cosd(-axis_tilt)]])\n\n    # panel_norm_earth contains the normal vector\n    # expressed in earth-surface coordinates\n    # (z normal to surface, y aligned with tracker axis parallel to earth)\n    panel_norm_earth = np.dot(rot_x, panel_norm).T\n\n    # projection to plane tangent to earth surface,\n    # in earth surface coordinates\n    projected_normal = np.array([panel_norm_earth[:, 0],\n                                 panel_norm_earth[:, 1],\n                                 panel_norm_earth[:, 2]*0]).T\n\n    # calculate vector magnitudes\n    projected_normal_mag = np.sqrt(np.nansum(projected_normal**2, axis=1))\n\n    # renormalize the projected vector\n    # avoid creating nan values.\n    non_zeros = projected_normal_mag != 0\n    projected_normal[non_zeros] = (projected_normal[non_zeros].T /\n                                   projected_normal_mag[non_zeros]).T\n\n    # calculation of surface_azimuth\n    # 1. Find the angle.\n#     surface_azimuth = pd.Series(\n#         np.degrees(np.arctan(projected_normal[:,1]/projected_normal[:,0])),\n#                                 index=times)\n    surface_azimuth = \\\n        np.degrees(np.arctan2(projected_normal[:, 1], projected_normal[:, 0]))\n\n    # 2. Clean up atan when x-coord or y-coord is zero\n#     surface_azimuth[(projected_normal[:,0]==0) & (projected_normal[:,1]>0)] =  90\n#     surface_azimuth[(projected_normal[:,0]==0) & (projected_normal[:,1]<0)] =  -90\n#     surface_azimuth[(projected_normal[:,1]==0) & (projected_normal[:,0]>0)] =  0\n#     surface_azimuth[(projected_normal[:,1]==0) & (projected_normal[:,0]<0)] = 180\n\n    # 3. Correct atan for QII and QIII\n#     surface_azimuth[(projected_normal[:,0]<0) & (projected_normal[:,1]>0)] += 180 # QII\n#     surface_azimuth[(projected_normal[:,0]<0) & (projected_normal[:,1]<0)] += 180 # QIII\n\n    # 4. Skip to below\n\n    # at this point surface_azimuth contains angles between -90 and +270,\n    # where 0 is along the positive x-axis,\n    # the y-axis is in the direction of the tracker azimuth,\n    # and positive angles are rotations from the positive x axis towards\n    # the positive y-axis.\n    # Adjust to compass angles\n    # (clockwise rotation from 0 along the positive y-axis)\n#    surface_azimuth[surface_azimuth<=90] = 90 - surface_azimuth[surface_azimuth<=90]\n#    surface_azimuth[surface_azimuth>90] = 450 - surface_azimuth[surface_azimuth>90]\n\n    # finally rotate to align y-axis with true north\n    # PVLIB_MATLAB has this latitude correction,\n    # but I don't think it's latitude dependent if you always\n    # specify axis_azimuth with respect to North.\n#     if latitude > 0 or True:\n#         surface_azimuth = surface_azimuth - axis_azimuth\n#     else:\n#         surface_azimuth = surface_azimuth - axis_azimuth - 180\n#     surface_azimuth[surface_azimuth<0] = 360 + surface_azimuth[surface_azimuth<0]\n\n    # the commented code above is mostly part of PVLIB_MATLAB.\n    # My (wholmgren) take is that it can be done more simply.\n    # Say that we're pointing along the postive x axis (likely west).\n    # We just need to rotate 90 degrees to get from the x axis\n    # to the y axis (likely south),\n    # and then add the axis_azimuth to get back to North.\n    # Anything left over is the azimuth that we want,\n    # and we can map it into the [0,360) domain.\n\n    # 4. Rotate 0 reference from panel's x axis to it's y axis and\n    #    then back to North.\n    surface_azimuth = 90 - surface_azimuth + axis_azimuth\n\n    # 5. Map azimuth into [0,360) domain.\n    surface_azimuth[surface_azimuth < 0] += 360\n    surface_azimuth[surface_azimuth >= 360] -= 360\n\n    # Calculate surface_tilt\n    dotproduct = (panel_norm_earth * projected_normal).sum(axis=1)\n    surface_tilt = 90 - np.degrees(np.arccos(dotproduct))\n\n    # Bundle DataFrame for return values and filter for sun below horizon.\n    out = {'tracker_theta': tracker_theta, 'aoi': aoi,\n           'surface_azimuth': surface_azimuth, 'surface_tilt': surface_tilt}\n    if index is not None:\n        out = pd.DataFrame(out, index=index)\n        out = out[['tracker_theta', 'aoi', 'surface_azimuth', 'surface_tilt']]\n        out[zen_gt_90] = np.nan\n    else:\n        out = {k: np.where(zen_gt_90, np.nan, v) for k, v in out.items()}\n\n    return out", "response": "Returns a single - axis base on a single - axis tracker."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a dictionary of tracking data for a single axis.", "response": "def singleaxis(self, apparent_zenith, apparent_azimuth):\n        \"\"\"\n        Get tracking data. See :py:func:`pvlib.tracking.singleaxis` more\n        detail.\n\n        Parameters\n        ----------\n        apparent_zenith : float, 1d array, or Series\n            Solar apparent zenith angles in decimal degrees.\n\n        apparent_azimuth : float, 1d array, or Series\n            Solar apparent azimuth angles in decimal degrees.\n\n        Returns\n        -------\n        tracking data\n        \"\"\"\n        tracking_data = singleaxis(apparent_zenith, apparent_azimuth,\n                                   self.axis_tilt, self.axis_azimuth,\n                                   self.max_angle,\n                                   self.backtrack, self.gcr)\n\n        return tracking_data"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the irradiance of a tilted solar z - axis and the irradiance components of a tilted solar z - axis and the irradiance components of a tilted solar z - axis.", "response": "def get_irradiance(self, surface_tilt, surface_azimuth,\n                       solar_zenith, solar_azimuth, dni, ghi, dhi,\n                       dni_extra=None, airmass=None, model='haydavies',\n                       **kwargs):\n        \"\"\"\n        Uses the :func:`irradiance.get_total_irradiance` function to\n        calculate the plane of array irradiance components on a tilted\n        surface defined by the input data and ``self.albedo``.\n\n        For a given set of solar zenith and azimuth angles, the\n        surface tilt and azimuth parameters are typically determined\n        by :py:meth:`~SingleAxisTracker.singleaxis`.\n\n        Parameters\n        ----------\n        surface_tilt : numeric\n            Panel tilt from horizontal.\n        surface_azimuth : numeric\n            Panel azimuth from north\n        solar_zenith : numeric\n            Solar zenith angle.\n        solar_azimuth : numeric\n            Solar azimuth angle.\n        dni : float or Series\n            Direct Normal Irradiance\n        ghi : float or Series\n            Global horizontal irradiance\n        dhi : float or Series\n            Diffuse horizontal irradiance\n        dni_extra : float or Series, default None\n            Extraterrestrial direct normal irradiance\n        airmass : float or Series, default None\n            Airmass\n        model : String, default 'haydavies'\n            Irradiance model.\n\n        **kwargs\n            Passed to :func:`irradiance.total_irrad`.\n\n        Returns\n        -------\n        poa_irradiance : DataFrame\n            Column names are: ``total, beam, sky, ground``.\n        \"\"\"\n\n        # not needed for all models, but this is easier\n        if dni_extra is None:\n            dni_extra = irradiance.get_extra_radiation(solar_zenith.index)\n\n        if airmass is None:\n            airmass = atmosphere.get_relative_airmass(solar_zenith)\n\n        return irradiance.get_total_irradiance(surface_tilt,\n                                               surface_azimuth,\n                                               solar_zenith,\n                                               solar_azimuth,\n                                               dni, ghi, dhi,\n                                               dni_extra=dni_extra,\n                                               airmass=airmass,\n                                               model=model,\n                                               albedo=self.albedo,\n                                               **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef read_srml(filename):\n    tsv_data = pd.read_csv(filename, delimiter='\\t')\n    data = format_index(tsv_data)\n    # Drop day of year and time columns\n    data = data[data.columns[2:]]\n\n    data = data.rename(columns=map_columns)\n\n    # Quality flag columns are all labeled 0 in the original data. They\n    # appear immediately after their associated variable and are suffixed\n    # with an integer value when read from the file. So we map flags to\n    # the preceding variable with a '_flag' suffix.\n    #\n    # Example:\n    #   Columns ['ghi_0', '0.1', 'temp_air_2', '0.2']\n    #\n    #   Yields a flag_label_map of:\n    #       { '0.1': 'ghi_0_flag',\n    #         '0.2': 'temp_air_2'}\n    #\n    columns = data.columns\n    flag_label_map = {flag: columns[columns.get_loc(flag) - 1] + '_flag'\n                      for flag in columns[1::2]}\n    data = data.rename(columns=flag_label_map)\n\n    # Mask data marked with quality flag 99 (bad or missing data)\n    for col in columns[::2]:\n        missing = data[col + '_flag'] == 99\n        data[col] = data[col].where(~(missing), np.NaN)\n    return data", "response": "Read University of Oregon SRML file into pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nmaps data element numbers to pvlib names.", "response": "def map_columns(col):\n    \"\"\"Map data element numbers to pvlib names.\n\n    Parameters\n    ----------\n    col: str\n        Column label to be mapped.\n\n    Returns\n    -------\n    str\n        The pvlib label if it was found in the mapping,\n        else the original label.\n    \"\"\"\n    if col.startswith('7'):\n        # spectral data\n        try:\n            return VARIABLE_MAP[col]\n        except KeyError:\n            return col\n    try:\n        variable_name = VARIABLE_MAP[col[:3]]\n        variable_number = col[3:]\n        return variable_name + '_' + variable_number\n    except KeyError:\n        return col"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a datetime index from day of year and time columns.", "response": "def format_index(df):\n    \"\"\"Create a datetime index from day of year, and time columns.\n\n    Parameters\n    ----------\n    df: pd.Dataframe\n        The srml data to reindex.\n\n    Returns\n    -------\n    df: pd.Dataframe\n        The Dataframe with a DatetimeIndex localized to 'Etc/GMT+8'.\n    \"\"\"\n    # Name of the second column indicates the year of the file, but\n    # the column contains times.\n    year = int(df.columns[1])\n    df_doy = df[df.columns[0]]\n    # Times are expressed as integers from 1-2400, we convert to 0-2359 by\n    # subracting one and then correcting the minutes at each former hour.\n    df_time = df[df.columns[1]] - 1\n    fifty_nines = df_time % 100 == 99\n    times = df_time.where(~fifty_nines, df_time - 40)\n\n    times = times.apply(lambda x: '{:04.0f}'.format(x))\n    doy = df_doy.apply(lambda x: '{:03.0f}'.format(x))\n    dts = pd.to_datetime(str(year) + '-' + doy + '-' + times,\n                         format='%Y-%j-%H%M')\n    df.index = dts\n    df = df.tz_localize('Etc/GMT+8')\n    return df"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrequest a month of SRML data from solardat and read it into a Dataframe.", "response": "def read_srml_month_from_solardat(station, year, month, filetype='PO'):\n    \"\"\"Request a month of SRML[1] data from solardat and read it into\n    a Dataframe.\n\n    Parameters\n    ----------\n    station: str\n        The name of the SRML station to request.\n    year: int\n        Year to request data for\n    month: int\n        Month to request data for.\n    filetype: string\n        SRML file type to gather. 'RO' and 'PO' are the\n        only minute resolution files.\n\n    Returns\n    -------\n    data: pd.DataFrame\n        One month of data from SRML.\n\n    References\n    ----------\n    [1] University of Oregon Solar Radiation Measurement Laboratory\n        `http://solardat.uoregon.edu/ <http://solardat.uoregon.edu/>`_\n    \"\"\"\n    file_name = \"{station}{filetype}{year:02d}{month:02d}.txt\".format(\n        station=station,\n        filetype=filetype,\n        year=year % 100,\n        month=month)\n    url = \"http://solardat.uoregon.edu/download/Archive/\"\n    data = read_srml(url + file_name)\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload data from ECMWF MACC Reanalysis API.", "response": "def get_ecmwf_macc(filename, params, startdate, stopdate, lookup_params=True,\n                   server=None, target=_ecmwf):\n    \"\"\"\n    Download data from ECMWF MACC Reanalysis API.\n\n    Parameters\n    ----------\n    filename : str\n        full path of file where to save data, ``.nc`` appended if not given\n    params : str or sequence of str\n        keynames of parameter[s] to download\n    startdate : datetime.datetime or datetime.date\n        UTC date\n    stopdate : datetime.datetime or datetime.date\n        UTC date\n    lookup_params : bool, default True\n        optional flag, if ``False``, then codes are already formatted\n    server : ecmwfapi.api.ECMWFDataServer\n        optionally provide a server object, default is ``None``\n    target : callable\n        optional function that calls ``server.retrieve`` to pass to thread\n\n    Returns\n    -------\n    t : thread\n        a thread object, use it to check status by calling `t.is_alive()`\n\n    Notes\n    -----\n    To download data from ECMWF requires the API client and a registration\n    key. Please read the documentation in `Access ECMWF Public Datasets\n    <https://confluence.ecmwf.int/display/WEBAPI/Access+ECMWF+Public+Datasets>`_.\n    Follow the instructions in step 4 and save the ECMWF registration key\n    as `$HOME\\.ecmwfapirc` or set `ECMWF_API_KEY` as the path to the key.\n\n    This function returns a daemon thread that runs in the background. Exiting\n    Python will kill this thread, however this thread will not block the main\n    thread or other threads. This thread will terminate when the file is\n    downloaded or if the thread raises an unhandled exception. You may submit\n    multiple requests simultaneously to break up large downloads. You can also\n    check the status and retrieve downloads online at\n    http://apps.ecmwf.int/webmars/joblist/. This is useful if you kill the\n    thread. Downloads expire after 24 hours.\n\n    .. warning:: Your request may be queued online for an hour or more before\n        it begins to download\n\n    Precipitable water :math:`P_{wat}` is equivalent to the total column of\n    water vapor (TCWV), but the units given by ECMWF MACC Reanalysis are kg/m^2\n    at STP (1-atm, 25-C). Divide by ten to convert to centimeters of\n    precipitable water:\n\n    .. math::\n        P_{wat} \\\\left( \\\\text{cm} \\\\right) \\\n        = TCWV \\\\left( \\\\frac{\\\\text{kg}}{\\\\text{m}^2} \\\\right) \\\n        \\\\frac{100 \\\\frac{\\\\text{cm}}{\\\\text{m}}} \\\n        {1000 \\\\frac{\\\\text{kg}}{\\\\text{m}^3}}\n\n    The keynames available for the ``params`` argument are given by\n    :const:`pvlib.iotools.ecmwf_macc.PARAMS` which maps the keys to codes used\n    in the API. The following keynames are available:\n\n    =======  =========================================\n    keyname  description\n    =======  =========================================\n    tcwv     total column water vapor in kg/m^2 at STP\n    aod550   aerosol optical depth measured at 550-nm\n    aod469   aerosol optical depth measured at 469-nm\n    aod670   aerosol optical depth measured at 670-nm\n    aod865   aerosol optical depth measured at 865-nm\n    aod1240  aerosol optical depth measured at 1240-nm\n    =======  =========================================\n\n    If ``lookup_params`` is ``False`` then ``params`` must contain the codes\n    preformatted according to the ECMWF MACC Reanalysis API. This is useful if\n    you want to retrieve codes that are not mapped in\n    :const:`pvlib.iotools.ecmwf_macc.PARAMS`.\n\n    Specify a custom ``target`` function to modify how the ECMWF API function\n    ``server.retrieve`` is called. The ``target`` function must have the\n    following signature in which the parameter definitions are similar to\n    :func:`pvlib.iotools.get_ecmwf_macc`. ::\n\n\n        target(server, startdate, stopdate, params, filename) -> None\n\n    Examples\n    --------\n    Retrieve the AOD measured at 550-nm and the total column of water vapor for\n    November 1, 2012.\n\n    >>> from datetime import date\n    >>> from pvlib.iotools import get_ecmwf_macc\n    >>> filename = 'aod_tcwv_20121101.nc'  # .nc extension added if missing\n    >>> params = ('aod550', 'tcwv')\n    >>> start = end = date(2012, 11, 1)\n    >>> t = get_ecmwf_macc(filename, params, start, end)\n    >>> t.is_alive()\n    True\n\n    \"\"\"\n    if not filename.endswith('nc'):\n        filename += '.nc'\n    if lookup_params:\n        try:\n            params = '/'.join(PARAMS.get(p) for p in params)\n        except TypeError:\n            params = PARAMS.get(params)\n    startdate = startdate.strftime('%Y-%m-%d')\n    stopdate = stopdate.strftime('%Y-%m-%d')\n    if not server:\n        server = ECMWFDataServer()\n    t = threading.Thread(target=target, daemon=True,\n                         args=(server, startdate, stopdate, params, filename))\n    t.start()\n    return t"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_ecmwf_macc(filename, latitude, longitude, utc_time_range=None):\n    ecmwf_macc = ECMWF_MACC(filename)\n    try:\n        ilat, ilon = ecmwf_macc.get_nearest_indices(latitude, longitude)\n        nctime = ecmwf_macc.data['time']\n        if utc_time_range:\n            start_idx = netCDF4.date2index(\n                utc_time_range[0], nctime, select='before')\n            stop_idx = netCDF4.date2index(\n                utc_time_range[-1], nctime, select='after')\n            time_slice = slice(start_idx, stop_idx + 1)\n        else:\n            time_slice = slice(0, ecmwf_macc.time_size)\n        times = netCDF4.num2date(nctime[time_slice], nctime.units)\n        df = {k: ecmwf_macc.data[k][time_slice, ilat, ilon]\n              for k in ecmwf_macc.keys}\n        if ECMWF_MACC.TCWV in df:\n            # convert total column water vapor in kg/m^2 at (1-atm, 25-degC) to\n            # precipitable water in cm\n            df['precipitable_water'] = df[ECMWF_MACC.TCWV] / 10.0\n    finally:\n        ecmwf_macc.data.close()\n    return pd.DataFrame(df, index=times.astype('datetime64[s]'))", "response": "Read data from ECMWF MACC reanalysis netCDF4 file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_nearest_indices(self, latitude, longitude):\n        # index of nearest latitude\n        idx_lat = int(round((latitude - 90.0) / self.delta_lat))\n        # avoid out of bounds latitudes\n        if idx_lat < 0:\n            idx_lat = 0  # if latitude == 90, north pole\n        elif idx_lat > self.lat_size:\n            idx_lat = self.lat_size  # if latitude == -90, south pole\n        # adjust longitude from -180/180 to 0/360\n        longitude = longitude % 360.0\n        # index of nearest longitude\n        idx_lon = int(round(longitude / self.delta_lon)) % self.lon_size\n        return idx_lat, idx_lon", "response": "Get nearest indices to latitude and longitude."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef interp_data(self, latitude, longitude, utc_time, param):\n        nctime = self.data['time']  # time\n        ilat, ilon = self.get_nearest_indices(latitude, longitude)\n        # time index before\n        before = netCDF4.date2index(utc_time, nctime, select='before')\n        fbefore = self.data[param][before, ilat, ilon]\n        fafter = self.data[param][before + 1, ilat, ilon]\n        dt_num = netCDF4.date2num(utc_time, nctime.units)\n        time_ratio = (dt_num - nctime[before]) / self.delta_time\n        return fbefore + (fafter - fbefore) * time_ratio", "response": "Interpolate the data at a specific time and set of parameters."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreading the SOLRAD file into pandas dataframe.", "response": "def read_solrad(filename):\n    \"\"\"\n    Read NOAA SOLRAD [1]_ [2]_ fixed-width file into pandas dataframe.\n\n    Parameters\n    ----------\n    filename: str\n        filepath or url to read for the fixed-width file.\n\n    Returns\n    -------\n    data: Dataframe\n        A dataframe with DatetimeIndex and all of the variables in the\n        file.\n\n    Notes\n    -----\n    SOLRAD data resolution is described by the README_SOLRAD.txt:\n    \"Before 1-jan. 2015 the data were reported as 3-min averages;\n    on and after 1-Jan. 2015, SOLRAD data are reported as 1-min.\n    averages of 1-sec. samples.\"\n    Here, missing data is flagged as NaN, rather than -9999.9.\n\n    References\n    ----------\n    .. [1] NOAA SOLRAD Network\n       `https://www.esrl.noaa.gov/gmd/grad/solrad/index.html\n       <https://www.esrl.noaa.gov/gmd/grad/solrad/index.html>`_\n\n    .. [2] B. B. Hicks et. al., (1996), The NOAA Integrated Surface\n       Irradiance Study (ISIS). A New Surface Radiation Monitoring\n       Program. Bull. Amer. Meteor. Soc., 77, 2857-2864.\n       :doi:`10.1175/1520-0477(1996)077<2857:TNISIS>2.0.CO;2`\n    \"\"\"\n    if 'msn' in filename:\n        names = MADISON_HEADERS\n        widths = MADISON_WIDTHS\n        dtypes = MADISON_DTYPES\n    else:\n        names = HEADERS\n        widths = WIDTHS\n        dtypes = DTYPES\n\n    # read in data\n    data = pd.read_fwf(filename, header=None, skiprows=2, names=names,\n                       widths=widths, na_values=-9999.9)\n\n    # loop here because dtype kwarg not supported in read_fwf until 0.20\n    for (col, _dtype) in zip(data.columns, dtypes):\n        ser = data[col].astype(_dtype)\n        if _dtype == 'float64':\n            # older verions of pandas/numpy read '-9999.9' as\n            # -9999.8999999999996 and fail to set nan in read_fwf,\n            # so manually set nan\n            ser = ser.where(ser > -9999, other=np.nan)\n        data[col] = ser\n\n    # set index\n    # columns do not have leading 0s, so must zfill(2) to comply\n    # with %m%d%H%M format\n    dts = data[['month', 'day', 'hour', 'minute']].astype(str).apply(\n        lambda x: x.str.zfill(2))\n    dtindex = pd.to_datetime(\n        data['year'].astype(str) + dts['month'] + dts['day'] + dts['hour'] +\n        dts['minute'], format='%Y%m%d%H%M', utc=True)\n    data = data.set_index(dtindex)\n    try:\n        # to_datetime(utc=True) does not work in older versions of pandas\n        data = data.tz_localize('UTC')\n    except TypeError:\n        pass\n\n    return data"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read_crn(filename):\n\n    # read in data\n    data = pd.read_fwf(filename, header=None, names=HEADERS.split(' '),\n                       widths=WIDTHS)\n    # loop here because dtype kwarg not supported in read_fwf until 0.20\n    for (col, _dtype) in zip(data.columns, DTYPES):\n        data[col] = data[col].astype(_dtype)\n\n    # set index\n    # UTC_TIME does not have leading 0s, so must zfill(4) to comply\n    # with %H%M format\n    dts = data[['UTC_DATE', 'UTC_TIME']].astype(str)\n    dtindex = pd.to_datetime(dts['UTC_DATE'] + dts['UTC_TIME'].str.zfill(4),\n                             format='%Y%m%d%H%M', utc=True)\n    data = data.set_index(dtindex)\n    try:\n        # to_datetime(utc=True) does not work in older versions of pandas\n        data = data.tz_localize('UTC')\n    except TypeError:\n        pass\n\n    # set nans\n    for val in [-99, -999, -9999]:\n        data = data.where(data != val, np.nan)\n\n    data = data.rename(columns=VARIABLE_MAP)\n\n    return data", "response": "Read NOAA USCRN fixed - width file into pandas dataframe."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nread an EPW file into a pandas dataframe.", "response": "def read_epw(filename, coerce_year=None):\n    '''\n    Read an EPW file in to a pandas dataframe.\n\n    Note that values contained in the metadata dictionary are unchanged\n    from the EPW file.\n\n    EPW files are commonly used by building simulation professionals\n    and are widely available on the web. For example via:\n    https://energyplus.net/weather , http://climate.onebuilding.org or\n    http://www.ladybug.tools/epwmap/\n\n\n    Parameters\n    ----------\n    filename : String\n        Can be a relative file path, absolute file path, or url.\n\n    coerce_year : None or int, default None\n        If supplied, the year of the data will be set to this value. This can\n        be a useful feature because EPW data is composed of data from\n        different years.\n        Warning: EPW files always have 365*24 = 8760 data rows;\n        be careful with the use of leap years.\n\n\n    Returns\n    -------\n    Tuple of the form (data, metadata).\n\n    data : DataFrame\n        A pandas dataframe with the columns described in the table\n        below. For more detailed descriptions of each component, please\n        consult the EnergyPlus Auxiliary Programs documentation\n        available at: https://energyplus.net/documentation.\n\n    metadata : dict\n        The site metadata available in the file.\n\n    Notes\n    -----\n\n    The returned structures have the following fields.\n\n    ===============   ======  =========================================\n    key               format  description\n    ===============   ======  =========================================\n    loc               String  default identifier, not used\n    city              String  site loccation\n    state-prov        String  state, province or region (if available)\n    country           String  site country code\n    data_type         String  type of original data source\n    WMO_code          String  WMO identifier\n    latitude          Float   site latitude\n    longitude         Float   site longitude\n    TZ                Float   UTC offset\n    altitude          Float   site elevation\n    ===============   ======  =========================================\n\n\n    =============================       ==============================================================================================================================================================\n    EPWData field                       description\n    =============================       ==============================================================================================================================================================\n    index                               A pandas datetime index. NOTE, times are set to local standard time (daylight savings is not included). Days run from 0-23h to comply with PVLIB's convention\n    year                                Year, from original EPW file. Can be overwritten using coerce function.\n    month                               Month, from original EPW file\n    day                                 Day of the month, from original EPW file.\n    hour                                Hour of the day from original EPW file. Note that EPW's convention of 1-24h is not taken over in the index dataframe used in PVLIB.\n    minute                              Minute, from original EPW file. Not used.\n    data_source_unct                    Data source and uncertainty flags. See [1], chapter 2.13\n    temp_air                            Dry bulb temperature at the time indicated, deg C\n    temp_dew                            Dew-point temperature at the time indicated, deg C\n    relative_humidity                   Relatitudeive humidity at the time indicated, percent\n    atmospheric_pressure                Station pressure at the time indicated, Pa\n    etr                                 Extraterrestrial horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    etrn                                Extraterrestrial normal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    ghi_infrared                        Horizontal infrared radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    ghi                                 Direct and diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    dni                                 Amount of direct normal radiation (modeled) recv'd during 60 mintues prior to timestamp, Wh/m^2\n    dhi                                 Amount of diffuse horizontal radiation recv'd during 60 minutes prior to timestamp, Wh/m^2\n    global_hor_illum                    Avg. total horizontal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    direct_normal_illum                 Avg. direct normal illuminance recv'd during the 60 minutes prior to timestamp, lx\n    diffuse_horizontal_illum            Avg. horizontal diffuse illuminance recv'd during the 60 minutes prior to timestamp, lx\n    zenith_luminance                    Avg. luminance at the sky's zenith during the 60 minutes prior to timestamp, cd/m^2\n    wind_direction                      Wind direction at time indicated, degrees from north (360 = north; 0 = undefined,calm)\n    wind_speed                          Wind speed at the time indicated, meter/second\n    total_sky_cover                     Amount of sky dome covered by clouds or obscuring phenonema at time stamp, tenths of sky\n    opaque_sky_cover                    Amount of sky dome covered by clouds or obscuring phenonema that prevent observing the sky at time stamp, tenths of sky\n    visibility                          Horizontal visibility at the time indicated, km\n    ceiling_height                      Height of cloud base above local terrain (7777=unlimited), meter\n    present_weather_observation         Indicator for remaining fields: If 0, then the observed weather codes are taken from the following field. If 9, then missing weather is assumed.\n    present_weather_codes               Present weather code, see [1], chapter 2.9.1.28\n    precipitable_water                  Total precipitable water contained in a column of unit cross section from earth to top of atmosphere, cm\n    aerosol_optical_depth               The broadband aerosol optical depth per unit of air mass due to extinction by aerosol component of atmosphere, unitless\n    snow_depth                          Snow depth in centimeters on the day indicated, (999 = missing data)\n    days_since_last_snowfall            Number of days since last snowfall (maximum value of 88, where 88 = 88 or greater days; 99 = missing data)\n    albedo                              The ratio of reflected solar irradiance to global horizontal irradiance, unitless\n    liquid_precipitation_depth          The amount of liquid precipitation observed at indicated time for the period indicated in the liquid precipitation quantity field, millimeter\n    liquid_precipitation_quantity       The period of accumulation for the liquid precipitation depth field, hour\n    =============================       ==============================================================================================================================================================\n\n    References\n    ----------\n\n    [1] EnergyPlus documentation, Auxiliary Programs\n    https://energyplus.net/documentation.\n    '''\n\n    if filename.startswith('http'):\n        # Attempts to download online EPW file\n        # See comments above for possible online sources\n        request = Request(filename, headers={'User-Agent': (\n            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) '\n            'AppleWebKit/537.36 (KHTML, like Gecko) Chrome/67.0.3396.87 '\n            'Safari/537.36')})\n        response = urlopen(request)\n        csvdata = io.StringIO(response.read().decode(errors='ignore'))\n    else:\n        # Assume it's accessible via the file system\n        csvdata = open(filename, 'r')\n\n    # Read line with metadata\n    firstline = csvdata.readline()\n\n    head = ['loc', 'city', 'state-prov', 'country', 'data_type', 'WMO_code',\n            'latitude', 'longitude', 'TZ', 'altitude']\n    meta = dict(zip(head, firstline.rstrip('\\n').split(\",\")))\n\n    meta['altitude'] = float(meta['altitude'])\n    meta['latitude'] = float(meta['latitude'])\n    meta['longitude'] = float(meta['longitude'])\n    meta['TZ'] = float(meta['TZ'])\n\n    colnames = ['year', 'month', 'day', 'hour', 'minute', 'data_source_unct',\n                'temp_air', 'temp_dew', 'relative_humidity',\n                'atmospheric_pressure', 'etr', 'etrn', 'ghi_infrared', 'ghi',\n                'dni', 'dhi', 'global_hor_illum', 'direct_normal_illum',\n                'diffuse_horizontal_illum', 'zenith_luminance',\n                'wind_direction', 'wind_speed', 'total_sky_cover',\n                'opaque_sky_cover', 'visibility', 'ceiling_height',\n                'present_weather_observation', 'present_weather_codes',\n                'precipitable_water', 'aerosol_optical_depth', 'snow_depth',\n                'days_since_last_snowfall', 'albedo',\n                'liquid_precipitation_depth', 'liquid_precipitation_quantity']\n\n    # We only have to skip 6 rows instead of 7 because we have already used\n    # the realine call above.\n    data = pd.read_csv(csvdata, skiprows=6, header=0, names=colnames)\n\n    # Change to single year if requested\n    if coerce_year is not None:\n        data[\"year\"] = coerce_year\n\n    # create index that supplies correct date and time zone information\n    dts = data[['month', 'day']].astype(str).apply(lambda x: x.str.zfill(2))\n    hrs = (data['hour'] - 1).astype(str).str.zfill(2)\n    dtscat = data['year'].astype(str) + dts['month'] + dts['day'] + hrs\n    idx = pd.to_datetime(dtscat, format='%Y%m%d%H')\n    idx = idx.dt.tz_localize(int(meta['TZ'] * 3600))\n    data.index = idx\n\n    return data, meta"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef from_tmy(cls, tmy_metadata, tmy_data=None, **kwargs):\n        # not complete, but hopefully you get the idea.\n        # might need code to handle the difference between tmy2 and tmy3\n\n        # determine if we're dealing with TMY2 or TMY3 data\n        tmy2 = tmy_metadata.get('City', False)\n\n        latitude = tmy_metadata['latitude']\n        longitude = tmy_metadata['longitude']\n\n        if tmy2:\n            name = tmy_metadata['City']\n        else:\n            name = tmy_metadata['Name']\n\n        tz = tmy_metadata['TZ']\n        altitude = tmy_metadata['altitude']\n\n        new_object = cls(latitude, longitude, tz=tz, altitude=altitude,\n                         name=name, **kwargs)\n\n        # not sure if this should be assigned regardless of input.\n        if tmy_data is not None:\n            new_object.tmy_data = tmy_data\n\n        return new_object", "response": "Create an object based on a dictionary of TMY metadata and optional data."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a DataFrame containing the solar position at the specified times.", "response": "def get_solarposition(self, times, pressure=None, temperature=12,\n                          **kwargs):\n        \"\"\"\n        Uses the :py:func:`solarposition.get_solarposition` function\n        to calculate the solar zenith, azimuth, etc. at this location.\n\n        Parameters\n        ----------\n        times : DatetimeIndex\n        pressure : None, float, or array-like, default None\n            If None, pressure will be calculated using\n            :py:func:`atmosphere.alt2pres` and ``self.altitude``.\n        temperature : None, float, or array-like, default 12\n\n        kwargs\n            passed to :py:func:`solarposition.get_solarposition`\n\n        Returns\n        -------\n        solar_position : DataFrame\n            Columns depend on the ``method`` kwarg, but always include\n            ``zenith`` and ``azimuth``.\n        \"\"\"\n        if pressure is None:\n            pressure = atmosphere.alt2pres(self.altitude)\n\n        return solarposition.get_solarposition(times, latitude=self.latitude,\n                                               longitude=self.longitude,\n                                               altitude=self.altitude,\n                                               pressure=pressure,\n                                               temperature=temperature,\n                                               **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalculates the clear sky estimates of GHI DNI and DHI at this location.", "response": "def get_clearsky(self, times, model='ineichen', solar_position=None,\n                     dni_extra=None, **kwargs):\n        \"\"\"\n        Calculate the clear sky estimates of GHI, DNI, and/or DHI\n        at this location.\n\n        Parameters\n        ----------\n        times: DatetimeIndex\n        model: str, default 'ineichen'\n            The clear sky model to use. Must be one of\n            'ineichen', 'haurwitz', 'simplified_solis'.\n        solar_position : None or DataFrame, default None\n            DataFrame with columns 'apparent_zenith', 'zenith',\n            'apparent_elevation'.\n        dni_extra: None or numeric, default None\n            If None, will be calculated from times.\n\n        kwargs\n            Extra parameters passed to the relevant functions. Climatological\n            values are assumed in many cases. See source code for details!\n\n        Returns\n        -------\n        clearsky : DataFrame\n            Column names are: ``ghi, dni, dhi``.\n        \"\"\"\n        if dni_extra is None:\n            dni_extra = irradiance.get_extra_radiation(times)\n\n        try:\n            pressure = kwargs.pop('pressure')\n        except KeyError:\n            pressure = atmosphere.alt2pres(self.altitude)\n\n        if solar_position is None:\n            solar_position = self.get_solarposition(times, pressure=pressure,\n                                                    **kwargs)\n\n        apparent_zenith = solar_position['apparent_zenith']\n        apparent_elevation = solar_position['apparent_elevation']\n\n        if model == 'ineichen':\n            try:\n                linke_turbidity = kwargs.pop('linke_turbidity')\n            except KeyError:\n                interp_turbidity = kwargs.pop('interp_turbidity', True)\n                linke_turbidity = clearsky.lookup_linke_turbidity(\n                    times, self.latitude, self.longitude,\n                    interp_turbidity=interp_turbidity)\n\n            try:\n                airmass_absolute = kwargs.pop('airmass_absolute')\n            except KeyError:\n                airmass_absolute = self.get_airmass(\n                    times, solar_position=solar_position)['airmass_absolute']\n\n            cs = clearsky.ineichen(apparent_zenith, airmass_absolute,\n                                   linke_turbidity, altitude=self.altitude,\n                                   dni_extra=dni_extra, **kwargs)\n        elif model == 'haurwitz':\n            cs = clearsky.haurwitz(apparent_zenith)\n        elif model == 'simplified_solis':\n            cs = clearsky.simplified_solis(\n                apparent_elevation, pressure=pressure, dni_extra=dni_extra,\n                **kwargs)\n        else:\n            raise ValueError('{} is not a valid clear sky model. Must be '\n                             'one of ineichen, simplified_solis, haurwitz'\n                             .format(model))\n\n        return cs"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the relative and absolute airmass for the given solar position.", "response": "def get_airmass(self, times=None, solar_position=None,\n                    model='kastenyoung1989'):\n        \"\"\"\n        Calculate the relative and absolute airmass.\n\n        Automatically chooses zenith or apparant zenith\n        depending on the selected model.\n\n        Parameters\n        ----------\n        times : None or DatetimeIndex, default None\n            Only used if solar_position is not provided.\n        solar_position : None or DataFrame, default None\n            DataFrame with with columns 'apparent_zenith', 'zenith'.\n        model : str, default 'kastenyoung1989'\n            Relative airmass model\n\n        Returns\n        -------\n        airmass : DataFrame\n            Columns are 'airmass_relative', 'airmass_absolute'\n        \"\"\"\n\n        if solar_position is None:\n            solar_position = self.get_solarposition(times)\n\n        if model in atmosphere.APPARENT_ZENITH_MODELS:\n            zenith = solar_position['apparent_zenith']\n        elif model in atmosphere.TRUE_ZENITH_MODELS:\n            zenith = solar_position['zenith']\n        else:\n            raise ValueError('{} is not a valid airmass model'.format(model))\n\n        airmass_relative = atmosphere.get_relative_airmass(zenith, model)\n\n        pressure = atmosphere.alt2pres(self.altitude)\n        airmass_absolute = atmosphere.get_absolute_airmass(airmass_relative,\n                                                           pressure)\n\n        airmass = pd.DataFrame(index=solar_position.index)\n        airmass['airmass_relative'] = airmass_relative\n        airmass['airmass_absolute'] = airmass_absolute\n\n        return airmass"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_sun_rise_set_transit(self, times, method='pyephem', **kwargs):\n\n        if method == 'pyephem':\n            result = solarposition.sun_rise_set_transit_ephem(\n                times, self.latitude, self.longitude, **kwargs)\n        elif method == 'spa':\n            result = solarposition.sun_rise_set_transit_spa(\n                times, self.latitude, self.longitude, **kwargs)\n        elif method == 'geometric':\n            sr, ss, tr = solarposition.sun_rise_set_transit_geometric(\n                times, self.latitude, self.longitude, **kwargs)\n            result = pd.DataFrame(index=times,\n                                  data={'sunrise': sr,\n                                        'sunset': ss,\n                                        'transit': tr})\n        else:\n            raise ValueError('{} is not a valid method. Must be '\n                             'one of pyephem, spa, geometric'\n                             .format(method))\n        return result", "response": "Calculates sunrise sunset and transit times."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef solar_position_loop(unixtime, loc_args, out):\n    lat = loc_args[0]\n    lon = loc_args[1]\n    elev = loc_args[2]\n    pressure = loc_args[3]\n    temp = loc_args[4]\n    delta_t = loc_args[5]\n    atmos_refract = loc_args[6]\n    sst = loc_args[7]\n    esd = loc_args[8]\n\n    for i in range(unixtime.shape[0]):\n        utime = unixtime[i]\n        jd = julian_day(utime)\n        jde = julian_ephemeris_day(jd, delta_t)\n        jc = julian_century(jd)\n        jce = julian_ephemeris_century(jde)\n        jme = julian_ephemeris_millennium(jce)\n        R = heliocentric_radius_vector(jme)\n        if esd:\n            out[0, i] = R\n            continue\n        L = heliocentric_longitude(jme)\n        B = heliocentric_latitude(jme)\n        Theta = geocentric_longitude(L)\n        beta = geocentric_latitude(B)\n        x0 = mean_elongation(jce)\n        x1 = mean_anomaly_sun(jce)\n        x2 = mean_anomaly_moon(jce)\n        x3 = moon_argument_latitude(jce)\n        x4 = moon_ascending_longitude(jce)\n        delta_psi = longitude_nutation(jce, x0, x1, x2, x3, x4)\n        delta_epsilon = obliquity_nutation(jce, x0, x1, x2, x3, x4)\n        epsilon0 = mean_ecliptic_obliquity(jme)\n        epsilon = true_ecliptic_obliquity(epsilon0, delta_epsilon)\n        delta_tau = aberration_correction(R)\n        lamd = apparent_sun_longitude(Theta, delta_psi, delta_tau)\n        v0 = mean_sidereal_time(jd, jc)\n        v = apparent_sidereal_time(v0, delta_psi, epsilon)\n        alpha = geocentric_sun_right_ascension(lamd, epsilon, beta)\n        delta = geocentric_sun_declination(lamd, epsilon, beta)\n        if sst:\n            out[0, i] = v\n            out[1, i] = alpha\n            out[2, i] = delta\n            continue\n        m = sun_mean_longitude(jme)\n        eot = equation_of_time(m, alpha, delta_psi, epsilon)\n        H = local_hour_angle(v, lon, alpha)\n        xi = equatorial_horizontal_parallax(R)\n        u = uterm(lat)\n        x = xterm(u, lat, elev)\n        y = yterm(u, lat, elev)\n        delta_alpha = parallax_sun_right_ascension(x, xi, H, delta)\n        delta_prime = topocentric_sun_declination(delta, x, y, xi, delta_alpha,\n                                                  H)\n        H_prime = topocentric_local_hour_angle(H, delta_alpha)\n        e0 = topocentric_elevation_angle_without_atmosphere(lat, delta_prime,\n                                                            H_prime)\n        delta_e = atmospheric_refraction_correction(pressure, temp, e0,\n                                                    atmos_refract)\n        e = topocentric_elevation_angle(e0, delta_e)\n        theta = topocentric_zenith_angle(e)\n        theta0 = topocentric_zenith_angle(e0)\n        gamma = topocentric_astronomers_azimuth(H_prime, delta_prime, lat)\n        phi = topocentric_azimuth_angle(gamma)\n        out[0, i] = theta\n        out[1, i] = theta0\n        out[2, i] = e\n        out[3, i] = e0\n        out[4, i] = phi\n        out[5, i] = eot", "response": "Loop through the time array and calculate the solar position"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef solar_position_numba(unixtime, lat, lon, elev, pressure, temp, delta_t,\n                         atmos_refract, numthreads, sst=False, esd=False):\n    \"\"\"Calculate the solar position using the numba compiled functions\n    and multiple threads. Very slow if functions are not numba compiled.\n    \"\"\"\n    # these args are the same for each thread\n    loc_args = np.array([lat, lon, elev, pressure, temp, delta_t,\n                         atmos_refract, sst, esd])\n\n    # construct dims x ulength array to put the results in\n    ulength = unixtime.shape[0]\n    if sst:\n        dims = 3\n    elif esd:\n        dims = 1\n    else:\n        dims = 6\n    result = np.empty((dims, ulength), dtype=np.float64)\n\n    if unixtime.dtype != np.float64:\n        unixtime = unixtime.astype(np.float64)\n\n    if ulength < numthreads:\n        warnings.warn('The number of threads is more than the length of '\n                      'the time array. Only using %s threads.'.format(ulength))\n        numthreads = ulength\n\n    if numthreads <= 1:\n        solar_position_loop(unixtime, loc_args, result)\n        return result\n\n    # split the input and output arrays into numthreads chunks\n    split0 = np.array_split(unixtime, numthreads)\n    split2 = np.array_split(result, numthreads, axis=1)\n    chunks = [[a0, loc_args, split2[i]] for i, a0 in enumerate(split0)]\n    # Spawn one thread per chunk\n    threads = [threading.Thread(target=solar_position_loop, args=chunk)\n               for chunk in chunks]\n    for thread in threads:\n        thread.start()\n    for thread in threads:\n        thread.join()\n    return result", "response": "Calculate the solar position using numba compiled functions\n    and multiple threads."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef earthsun_distance(unixtime, delta_t, numthreads):\n\n    R = solar_position(unixtime, 0, 0, 0, 0, 0, delta_t,\n                       0, numthreads, esd=True)[0]\n\n    return R", "response": "Calculates the distance from the earth to the sun using the NREL SPA algorithm described in [ 1 ]."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef calculate_deltat(year, month):\n\n    plw = 'Deltat is unknown for years before -1999 and after 3000. ' \\\n          'Delta values will be calculated, but the calculations ' \\\n          'are not intended to be used for these years.'\n\n    try:\n        if np.any((year > 3000) | (year < -1999)):\n            warnings.warn(plw)\n    except ValueError:\n        if (year > 3000) | (year < -1999):\n            warnings.warn(plw)\n    except TypeError:\n        return 0\n\n    y = year + (month - 0.5)/12\n\n    deltat = np.where(year < -500,\n\n                      -20+32*((y-1820)/100)**2, 0)\n\n    deltat = np.where((-500 <= year) & (year < 500),\n\n                      10583.6-1014.41*(y/100)\n                      + 33.78311*(y/100)**2\n                      - 5.952053*(y/100)**3\n                      - 0.1798452*(y/100)**4\n                      + 0.022174192*(y/100)**5\n                      + 0.0090316521*(y/100)**6, deltat)\n\n    deltat = np.where((500 <= year) & (year < 1600),\n\n                      1574.2-556.01*((y-1000)/100)\n                      + 71.23472*((y-1000)/100)**2\n                      + 0.319781*((y-1000)/100)**3\n                      - 0.8503463*((y-1000)/100)**4\n                      - 0.005050998*((y-1000)/100)**5\n                      + 0.0083572073*((y-1000)/100)**6, deltat)\n\n    deltat = np.where((1600 <= year) & (year < 1700),\n\n                      120-0.9808*(y-1600)\n                      - 0.01532*(y-1600)**2\n                      + (y-1600)**3/7129, deltat)\n\n    deltat = np.where((1700 <= year) & (year < 1800),\n\n                      8.83+0.1603*(y-1700)\n                      - 0.0059285*(y-1700)**2\n                      + 0.00013336*(y-1700)**3\n                      - (y-1700)**4/1174000, deltat)\n\n    deltat = np.where((1800 <= year) & (year < 1860),\n\n                      13.72-0.332447*(y-1800)\n                      + 0.0068612*(y-1800)**2\n                      + 0.0041116*(y-1800)**3\n                      - 0.00037436*(y-1800)**4\n                      + 0.0000121272*(y-1800)**5\n                      - 0.0000001699*(y-1800)**6\n                      + 0.000000000875*(y-1800)**7, deltat)\n\n    deltat = np.where((1860 <= year) & (year < 1900),\n\n                      7.62+0.5737*(y-1860)\n                      - 0.251754*(y-1860)**2\n                      + 0.01680668*(y-1860)**3\n                      - 0.0004473624*(y-1860)**4\n                      + (y-1860)**5/233174, deltat)\n\n    deltat = np.where((1900 <= year) & (year < 1920),\n\n                      -2.79+1.494119*(y-1900)\n                      - 0.0598939*(y-1900)**2\n                      + 0.0061966*(y-1900)**3\n                      - 0.000197*(y-1900)**4, deltat)\n\n    deltat = np.where((1920 <= year) & (year < 1941),\n\n                      21.20+0.84493*(y-1920)\n                      - 0.076100*(y-1920)**2\n                      + 0.0020936*(y-1920)**3, deltat)\n\n    deltat = np.where((1941 <= year) & (year < 1961),\n\n                      29.07+0.407*(y-1950)\n                      - (y-1950)**2/233\n                      + (y-1950)**3/2547, deltat)\n\n    deltat = np.where((1961 <= year) & (year < 1986),\n\n                      45.45+1.067*(y-1975)\n                      - (y-1975)**2/260\n                      - (y-1975)**3/718, deltat)\n\n    deltat = np.where((1986 <= year) & (year < 2005),\n\n                      63.86+0.3345*(y-2000)\n                      - 0.060374*(y-2000)**2\n                      + 0.0017275*(y-2000)**3\n                      + 0.000651814*(y-2000)**4\n                      + 0.00002373599*(y-2000)**5, deltat)\n\n    deltat = np.where((2005 <= year) & (year < 2050),\n\n                      62.92+0.32217*(y-2000)\n                      + 0.005589*(y-2000)**2, deltat)\n\n    deltat = np.where((2050 <= year) & (year < 2150),\n\n                      -20+32*((y-1820)/100)**2\n                      - 0.5628*(2150-y), deltat)\n\n    deltat = np.where(year >= 2150,\n\n                      -20+32*((y-1820)/100)**2, deltat)\n\n    deltat = deltat.item() if np.isscalar(year) & np.isscalar(month)\\\n        else deltat\n\n    return deltat", "response": "Calculate the difference between Terrestrial Dynamical Time and Universal Time."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncalculate front and back surface plane-of-array irradiance on a fixed tilt or single-axis tracker PV array configuration, and using the open-source \"pvfactors\" package. Please refer to pvfactors online documentation for more details: https://sunpower.github.io/pvfactors/ Parameters ---------- solar_azimuth: numeric Sun's azimuth angles using pvlib's azimuth convention (deg) solar_zenith: numeric Sun's zenith angles (deg) surface_azimuth: numeric Azimuth angle of the front surface of the PV modules, using pvlib's convention (deg) surface_tilt: numeric Tilt angle of the PV modules, going from 0 to 180 (deg) timestamps: datetime or DatetimeIndex List of simulation timestamps dni: numeric Direct normal irradiance (W/m2) dhi: numeric Diffuse horizontal irradiance (W/m2) gcr: float Ground coverage ratio of the pv array pvrow_height: float Height of the pv rows, measured at their center (m) pvrow_width: float Width of the pv rows in the considered 2D plane (m) albedo: float Ground albedo n_pvrows: int, default 3 Number of PV rows to consider in the PV array index_observed_pvrow: int, default 1 Index of the PV row whose incident irradiance will be returned. Indices of PV rows go from 0 to n_pvrows-1. rho_front_pvrow: float, default 0.03 Front surface reflectivity of PV rows rho_back_pvrow: float, default 0.05 Back surface reflectivity of PV rows horizon_band_angle: float, default 15 Elevation angle of the sky dome's diffuse horizon band (deg) run_parallel_calculations: bool, default True pvfactors is capable of using multiprocessing. Use this flag to decide to run calculations in parallel (recommended) or not. n_workers_for_parallel_calcs: int, default None Number of workers to use in the case of parallel calculations. The default value of 'None' will lead to using a value equal to the number of CPU's on the machine running the model. Returns ------- front_poa_irradiance: numeric Calculated incident irradiance on the front surface of the PV modules (W/m2) back_poa_irradiance: numeric Calculated incident irradiance on the back surface of the PV modules (W/m2) df_registries: pandas DataFrame DataFrame containing detailed outputs of the simulation; for instance the shapely geometries, the irradiance components incident on all surfaces of the PV array (for all timestamps), etc. In the pvfactors documentation, this is refered to as the \"surface registry\". References ---------- .. [1] Anoma, Marc Abou, et al. \"View Factor Model and Validation for Bifacial PV and Diffuse Shade on Single-Axis Trackers.\" 44th IEEE Photovoltaic Specialist Conference. 2017.", "response": "def pvfactors_timeseries(\n        solar_azimuth, solar_zenith, surface_azimuth, surface_tilt,\n        timestamps, dni, dhi, gcr, pvrow_height, pvrow_width, albedo,\n        n_pvrows=3, index_observed_pvrow=1,\n        rho_front_pvrow=0.03, rho_back_pvrow=0.05,\n        horizon_band_angle=15.,\n        run_parallel_calculations=True, n_workers_for_parallel_calcs=None):\n    \"\"\"\n    Calculate front and back surface plane-of-array irradiance on\n    a fixed tilt or single-axis tracker PV array configuration, and using\n    the open-source \"pvfactors\" package.\n    Please refer to pvfactors online documentation for more details:\n    https://sunpower.github.io/pvfactors/\n\n    Parameters\n    ----------\n    solar_azimuth: numeric\n        Sun's azimuth angles using pvlib's azimuth convention (deg)\n    solar_zenith: numeric\n        Sun's zenith angles (deg)\n    surface_azimuth: numeric\n        Azimuth angle of the front surface of the PV modules, using pvlib's\n        convention (deg)\n    surface_tilt: numeric\n        Tilt angle of the PV modules, going from 0 to 180 (deg)\n    timestamps: datetime or DatetimeIndex\n        List of simulation timestamps\n    dni: numeric\n        Direct normal irradiance (W/m2)\n    dhi: numeric\n        Diffuse horizontal irradiance (W/m2)\n    gcr: float\n        Ground coverage ratio of the pv array\n    pvrow_height: float\n        Height of the pv rows, measured at their center (m)\n    pvrow_width: float\n        Width of the pv rows in the considered 2D plane (m)\n    albedo: float\n        Ground albedo\n    n_pvrows: int, default 3\n        Number of PV rows to consider in the PV array\n    index_observed_pvrow: int, default 1\n        Index of the PV row whose incident irradiance will be returned. Indices\n        of PV rows go from 0 to n_pvrows-1.\n    rho_front_pvrow: float, default 0.03\n        Front surface reflectivity of PV rows\n    rho_back_pvrow: float, default 0.05\n        Back surface reflectivity of PV rows\n    horizon_band_angle: float, default 15\n        Elevation angle of the sky dome's diffuse horizon band (deg)\n    run_parallel_calculations: bool, default True\n        pvfactors is capable of using multiprocessing. Use this flag to decide\n        to run calculations in parallel (recommended) or not.\n    n_workers_for_parallel_calcs: int, default None\n        Number of workers to use in the case of parallel calculations. The\n        default value of 'None' will lead to using a value equal to the number\n        of CPU's on the machine running the model.\n\n    Returns\n    -------\n    front_poa_irradiance: numeric\n        Calculated incident irradiance on the front surface of the PV modules\n        (W/m2)\n    back_poa_irradiance: numeric\n        Calculated incident irradiance on the back surface of the PV modules\n        (W/m2)\n    df_registries: pandas DataFrame\n        DataFrame containing detailed outputs of the simulation; for\n        instance the shapely geometries, the irradiance components incident on\n        all surfaces of the PV array (for all timestamps), etc.\n        In the pvfactors documentation, this is refered to as the \"surface\n        registry\".\n\n    References\n    ----------\n    .. [1] Anoma, Marc Abou, et al. \"View Factor Model and Validation for\n        Bifacial PV and Diffuse Shade on Single-Axis Trackers.\" 44th IEEE\n        Photovoltaic Specialist Conference. 2017.\n    \"\"\"\n\n    # Convert pandas Series inputs to numpy arrays\n    if isinstance(solar_azimuth, pd.Series):\n        solar_azimuth = solar_azimuth.values\n    if isinstance(solar_zenith, pd.Series):\n        solar_zenith = solar_zenith.values\n    if isinstance(surface_azimuth, pd.Series):\n        surface_azimuth = surface_azimuth.values\n    if isinstance(surface_tilt, pd.Series):\n        surface_tilt = surface_tilt.values\n    if isinstance(dni, pd.Series):\n        dni = dni.values\n    if isinstance(dhi, pd.Series):\n        dhi = dhi.values\n\n    # Import pvfactors functions for timeseries calculations.\n    from pvfactors.timeseries import (calculate_radiosities_parallel_perez,\n                                      calculate_radiosities_serially_perez,\n                                      get_average_pvrow_outputs)\n    idx_slice = pd.IndexSlice\n\n    # Build up pv array configuration parameters\n    pvarray_parameters = {\n        'n_pvrows': n_pvrows,\n        'pvrow_height': pvrow_height,\n        'pvrow_width': pvrow_width,\n        'gcr': gcr,\n        'rho_ground': albedo,\n        'rho_front_pvrow': rho_front_pvrow,\n        'rho_back_pvrow': rho_back_pvrow,\n        'horizon_band_angle': horizon_band_angle\n    }\n\n    # Run pvfactors calculations: either in parallel or serially\n    if run_parallel_calculations:\n        df_registries, df_custom_perez = calculate_radiosities_parallel_perez(\n            pvarray_parameters, timestamps, solar_zenith, solar_azimuth,\n            surface_tilt, surface_azimuth, dni, dhi,\n            n_processes=n_workers_for_parallel_calcs)\n    else:\n        inputs = (pvarray_parameters, timestamps, solar_zenith, solar_azimuth,\n                  surface_tilt, surface_azimuth, dni, dhi)\n        df_registries, df_custom_perez = calculate_radiosities_serially_perez(\n            inputs)\n\n    # Get the average surface outputs\n    df_outputs = get_average_pvrow_outputs(df_registries,\n                                           values=['qinc'],\n                                           include_shading=True)\n\n    # Select the calculated outputs from the pvrow to observe\n    ipoa_front = df_outputs.loc[:, idx_slice[index_observed_pvrow,\n                                             'front', 'qinc']]\n\n    ipoa_back = df_outputs.loc[:, idx_slice[index_observed_pvrow,\n                                            'back', 'qinc']]\n\n    # Set timestamps as index of df_registries for consistency of outputs\n    df_registries = df_registries.set_index('timestamps')\n\n    return ipoa_front, ipoa_back, df_registries"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfiling based sample for the Google Assistant API. Examples: $ python -m audiofileinput -i <input file> -o <output file>", "response": "def main(api_endpoint, credentials,\n         device_model_id, device_id, lang, verbose,\n         input_audio_file, output_audio_file,\n         block_size, grpc_deadline, *args, **kwargs):\n    \"\"\"File based sample for the Google Assistant API.\n\n    Examples:\n      $ python -m audiofileinput -i <input file> -o <output file>\n    \"\"\"\n    # Setup logging.\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n\n    # Load OAuth 2.0 credentials.\n    try:\n        with open(credentials, 'r') as f:\n            credentials = google.oauth2.credentials.Credentials(token=None,\n                                                                **json.load(f))\n            http_request = google.auth.transport.requests.Request()\n            credentials.refresh(http_request)\n    except Exception as e:\n        logging.error('Error loading credentials: %s', e)\n        logging.error('Run google-oauthlib-tool to initialize '\n                      'new OAuth 2.0 credentials.')\n        sys.exit(-1)\n\n    # Create an authorized gRPC channel.\n    grpc_channel = google.auth.transport.grpc.secure_authorized_channel(\n        credentials, http_request, api_endpoint)\n    logging.info('Connecting to %s', api_endpoint)\n\n    # Create gRPC stubs\n    assistant = embedded_assistant_pb2_grpc.EmbeddedAssistantStub(grpc_channel)\n\n    # Generate gRPC requests.\n    def gen_assist_requests(input_stream):\n        dialog_state_in = embedded_assistant_pb2.DialogStateIn(\n            language_code=lang,\n            conversation_state=b''\n        )\n        config = embedded_assistant_pb2.AssistConfig(\n            audio_in_config=embedded_assistant_pb2.AudioInConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=16000,\n            ),\n            audio_out_config=embedded_assistant_pb2.AudioOutConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=16000,\n                volume_percentage=100,\n            ),\n            dialog_state_in=dialog_state_in,\n            device_config=embedded_assistant_pb2.DeviceConfig(\n                device_id=device_id,\n                device_model_id=device_model_id,\n            )\n        )\n        # Send first AssistRequest message with configuration.\n        yield embedded_assistant_pb2.AssistRequest(config=config)\n        while True:\n            # Read user request from file.\n            data = input_stream.read(block_size)\n            if not data:\n                break\n            # Send following AssitRequest message with audio chunks.\n            yield embedded_assistant_pb2.AssistRequest(audio_in=data)\n\n    for resp in assistant.Assist(gen_assist_requests(input_audio_file),\n                                 grpc_deadline):\n        # Iterate on AssistResponse messages.\n        if resp.event_type == END_OF_UTTERANCE:\n            logging.info('End of audio request detected')\n        if resp.speech_results:\n            logging.info('Transcript of user request: \"%s\".',\n                         ' '.join(r.transcript\n                                  for r in resp.speech_results))\n        if len(resp.audio_out.audio_data) > 0:\n            # Write assistant response to supplied file.\n            output_audio_file.write(resp.audio_out.audio_data)\n        if resp.dialog_state_out.supplemental_display_text:\n            logging.info('Assistant display text: \"%s\"',\n                         resp.dialog_state_out.supplemental_display_text)\n        if resp.device_action.device_request_json:\n            device_request = json.loads(resp.device_action.device_request_json)\n            logging.info('Device request: %s', device_request)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process_event(event):\n    if event.type == EventType.ON_CONVERSATION_TURN_STARTED:\n        print()\n\n    print(event)\n\n    if (event.type == EventType.ON_CONVERSATION_TURN_FINISHED and\n            event.args and not event.args['with_follow_on_turn']):\n        print()\n    if event.type == EventType.ON_DEVICE_ACTION:\n        for command, params in event.actions:\n            print('Do command', command, 'with params', str(params))", "response": "Pretty prints events.\n\n    Prints all events that occur with two spaces between each new\n    conversation and a single space between turns of a conversation.\n\n    Args:\n        event(event.Event): The current event to process."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef failed_request_exception(message, r):\n    try:\n        resp = json.loads(r.text)\n        message = '%s: %d\\n%s' % (message, resp['error']['code'],\n                                  resp['error']['message'])\n        return click.ClickException(message)\n    except ValueError:\n        # fallback on raw text response if error is not structured.\n        return click.ClickException('%s: %d\\n%s' % (message,\n                                                    r.status_code,\n                                                    r.text))", "response": "Build ClickException from a failed request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pretty_print_model(devicemodel):\n    PRETTY_PRINT_MODEL = \"\"\"Device Model ID: %(deviceModelId)s\n        Project ID: %(projectId)s\n        Device Type: %(deviceType)s\"\"\"\n    logging.info(PRETTY_PRINT_MODEL % devicemodel)\n    if 'traits' in devicemodel:\n        for trait in devicemodel['traits']:\n            logging.info('        Trait %s' % trait)\n    else:\n        logging.info('No traits')\n    logging.info('')", "response": "Prints out a device model in the terminal by parsing dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints out a device instance in the terminal by parsing dict.", "response": "def pretty_print_device(device):\n    \"\"\"Prints out a device instance in the terminal by parsing dict.\"\"\"\n    logging.info('Device Instance ID: %s' % device['id'])\n    if 'nickname' in device:\n        logging.info('    Nickname: %s' % device['nickname'])\n    if 'modelId' in device:\n        logging.info('    Model: %s' % device['modelId'])\n    logging.info('')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a device model and instance.", "response": "def register(ctx, model, type, trait, manufacturer, product_name, description,\n             device, nickname, client_type):\n    \"\"\"Registers a device model and instance.\n\n    Device model fields can only contain letters, numbers, and the following\n    symbols: period (.), hyphen (-), underscore (_), space ( ) and plus (+).\n    The first character of a field must be a letter or number.\n\n    Device instance fields must start with a letter or number. The device ID\n    can only contain letters, numbers, and the following symbols: period (.),\n    hyphen (-), underscore (_), and plus (+). The device nickname can only\n    contain numbers, letters, and the space ( ) symbol.\n    \"\"\"\n    # cache SESSION and PROJECT_ID\n    # so that we don't re-create them between commands\n    ctx.obj['SESSION'] = google.auth.transport.requests.AuthorizedSession(\n        ctx.obj['CREDENTIALS']\n    )\n    ctx.invoke(register_model,\n               model=model, type=type, trait=trait,\n               manufacturer=manufacturer,\n               product_name=product_name,\n               description=description)\n    ctx.invoke(register_device, device=device, model=model,\n               nickname=nickname, client_type=client_type)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a new device model.", "response": "def register_model(ctx, model, type, trait,\n                   manufacturer, product_name, description):\n    \"\"\"Registers a device model.\n\n    Device model fields can only contain letters, numbers, and the following\n    symbols: period (.), hyphen (-), underscore (_), space ( ) and plus (+).\n    The first character of a field must be a letter or number.\n    \"\"\"\n    session, api_url, project_id = build_client_from_context(ctx)\n    model_base_url = '/'.join([api_url, 'deviceModels'])\n    model_url = '/'.join([model_base_url, model])\n    payload = {\n        'device_model_id': model,\n        'project_id': project_id,\n        'device_type': 'action.devices.types.' + type,\n    }\n    if trait:\n        payload['traits'] = trait\n    if manufacturer:\n        payload.setdefault('manifest', {})['manufacturer'] = manufacturer\n    if product_name:\n        payload.setdefault('manifest', {})['productName'] = product_name\n    if description:\n        payload.setdefault('manifest', {})['deviceDescription'] = description\n    logging.debug(json.dumps(payload))\n    r = session.get(model_url)\n    logging.debug(r.text)\n    if r.status_code == 200:\n        click.echo('Updating existing device model: %s' % model)\n        r = session.put(model_url, data=json.dumps(payload))\n    elif r.status_code in (400, 403, 404):\n        click.echo('Creating new device model')\n        r = session.post(model_base_url, data=json.dumps(payload))\n    else:\n        raise failed_request_exception('Failed to check existing device model',\n                                       r)\n    if r.status_code != 200:\n        raise failed_request_exception('Failed to register model', r)\n    click.echo('Model %s successfully registered' % model)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering a new device instance under an existing device model.", "response": "def register_device(ctx, device, model, nickname, client_type):\n    \"\"\"Registers a device instance under an existing device model.\n\n    Device instance fields must start with a letter or number. The device ID\n    can only contain letters, numbers, and the following symbols: period (.),\n    hyphen (-), underscore (_), and plus (+). The device nickname can only\n    contain numbers, letters, and the space ( ) symbol.\n    \"\"\"\n    session, api_url, project_id = build_client_from_context(ctx)\n    device_base_url = '/'.join([api_url, 'devices'])\n    device_url = '/'.join([device_base_url, device])\n    payload = {\n        'id': device,\n        'model_id': model,\n    }\n    if client_type:\n        payload['client_type'] = 'SDK_' + client_type\n    if nickname:\n        payload['nickname'] = nickname\n\n    logging.debug(json.dumps(payload))\n    r = session.get(device_url)\n    if r.status_code == 200:\n        click.echo('Updating existing device: %s' % device)\n        session.delete(device_url)\n        r = session.post(device_base_url, data=json.dumps(payload))\n    elif r.status_code in (400, 403, 404):\n        click.echo('Creating new device')\n        r = session.post(device_base_url, data=json.dumps(payload))\n    else:\n        raise failed_request_exception('Failed to check existing device', r)\n    if r.status_code != 200:\n        raise failed_request_exception('Failed to register device', r)\n    click.echo('Device instance %s successfully registered' % device)\n    logging.debug(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting all of the information fields for a given device model or instance.", "response": "def get(ctx, resource, id):\n    \"\"\"Gets all of the information (fields) for a given device model or\n    instance.\n    \"\"\"\n    session, api_url, project_id = build_client_from_context(ctx)\n    url = '/'.join([api_url, resource, id])\n    r = session.get(url)\n    if r.status_code != 200:\n        raise failed_request_exception('Failed to get resource', r)\n\n    response = json.loads(r.text)\n    if resource == 'deviceModels':\n        pretty_print_model(response)\n    elif resource == 'devices':\n        pretty_print_device(response)\n    logging.debug(r.text)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(ctx, resource, id):\n    session, api_url, project_id = build_client_from_context(ctx)\n    url = '/'.join([api_url, resource, id])\n    r = session.delete(url)\n    if r.status_code != 200:\n        raise failed_request_exception('failed to delete resource', r)\n    click.echo(r.text)", "response": "Delete given device model or instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Converse(self, request_iterator, context):\n    context.set_code(grpc.StatusCode.UNIMPLEMENTED)\n    context.set_details('Method not implemented!')\n    raise NotImplementedError('Method not implemented!')", "response": "Initiates or continues a conversation with the embedded assistant service."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nlog AssistRequest fields without audio data.", "response": "def log_assist_request_without_audio(assist_request):\n    \"\"\"Log AssistRequest fields without audio data.\"\"\"\n    if logging.getLogger().isEnabledFor(logging.DEBUG):\n        resp_copy = embedded_assistant_pb2.AssistRequest()\n        resp_copy.CopyFrom(assist_request)\n        if len(resp_copy.audio_in) > 0:\n            size = len(resp_copy.audio_in)\n            resp_copy.ClearField('audio_in')\n            logging.debug('AssistRequest: audio_in (%d bytes)',\n                          size)\n            return\n        logging.debug('AssistRequest: %s', resp_copy)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging AssistResponse fields without audio data.", "response": "def log_assist_response_without_audio(assist_response):\n    \"\"\"Log AssistResponse fields without audio data.\"\"\"\n    if logging.getLogger().isEnabledFor(logging.DEBUG):\n        resp_copy = embedded_assistant_pb2.AssistResponse()\n        resp_copy.CopyFrom(assist_response)\n        has_audio_data = (resp_copy.HasField('audio_out') and\n                          len(resp_copy.audio_out.audio_data) > 0)\n        if has_audio_data:\n            size = len(resp_copy.audio_out.audio_data)\n            resp_copy.audio_out.ClearField('audio_data')\n            if resp_copy.audio_out.ListFields():\n                logging.debug('AssistResponse: %s audio_data (%d bytes)',\n                              resp_copy,\n                              size)\n            else:\n                logging.debug('AssistResponse: audio_data (%d bytes)',\n                              size)\n            return\n        logging.debug('AssistResponse: %s', resp_copy)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nregisters a device action handler.", "response": "def command(self, intent):\n        \"\"\"Register a device action handlers.\"\"\"\n        def decorator(fn):\n            self.handlers[intent] = fn\n        return decorator"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsubmit command executions for devices.", "response": "def submit_commands(self, devices, execution):\n        \"\"\"Submit device command executions.\n\n        Returns: a list of concurrent.futures for scheduled executions.\n        \"\"\"\n        fs = []\n        for device in devices:\n            if device[key_id_] != self.device_id:\n                logging.warning('Ignoring command for unknown device: %s'\n                                % device[key_id_])\n                continue\n            if not execution:\n                logging.warning('Ignoring noop execution')\n                continue\n            for command in execution:\n                f = self.executor.submit(\n                    self.dispatch_command, **command\n                )\n                fs.append(f)\n        return fs"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef dispatch_command(self, command, params=None):\n        try:\n            if command in self.handlers:\n                self.handlers[command](**params)\n            else:\n                logging.warning('Unsupported command: %s: %s',\n                                command, params)\n        except Exception as e:\n            logging.warning('Error during command execution',\n                            exc_info=sys.exc_info())\n            raise e", "response": "Dispatch device commands to the appropriate handler."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef normalize_audio_buffer(buf, volume_percentage, sample_width=2):\n    if sample_width != 2:\n        raise Exception('unsupported sample width:', sample_width)\n    scale = math.pow(2, 1.0*volume_percentage/100)-1\n    # Construct array from bytes based on sample_width, multiply by scale\n    # and convert it back to bytes\n    arr = array.array('h', buf)\n    for idx in range(0, len(arr)):\n        arr[idx] = int(arr[idx]*scale)\n    buf = arr.tostring()\n    return buf", "response": "Normalizes the loudness of the audio data in the given buffer."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nalign the buffer with zeros if needed", "response": "def align_buf(buf, sample_width):\n    \"\"\"In case of buffer size not aligned to sample_width pad it with 0s\"\"\"\n    remainder = len(buf) % sample_width\n    if remainder != 0:\n        buf += b'\\0' * (sample_width - remainder)\n    return buf"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read(self, size):\n        now = time.time()\n        missing_dt = self._sleep_until - now\n        if missing_dt > 0:\n            time.sleep(missing_dt)\n        self._sleep_until = time.time() + self._sleep_time(size)\n        data = (self._wavep.readframes(size)\n                if self._wavep\n                else self._fp.read(size))\n        #  When reach end of audio stream, pad remainder with silence (zeros).\n        if not data:\n            return b'\\x00' * size\n        return data", "response": "Read bytes from the stream and block until sample rate is achieved."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef close(self):\n        if self._wavep:\n            self._wavep.close()\n        self._fp.close()", "response": "Close the underlying stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef read(self, size):\n        buf, overflow = self._audio_stream.read(size)\n        if overflow:\n            logging.warning('SoundDeviceStream read overflow (%d, %d)',\n                            size, len(buf))\n        return bytes(buf)", "response": "Read bytes from the stream."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef write(self, buf):\n        underflow = self._audio_stream.write(buf)\n        if underflow:\n            logging.warning('SoundDeviceStream write underflow (size: %d)',\n                            len(buf))\n        return len(buf)", "response": "Write bytes to the stream."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef close(self):\n        if self._audio_stream:\n            self.stop()\n            self._audio_stream.close()\n            self._audio_stream = None", "response": "Close the underlying stream and audio interface."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef start_recording(self):\n        self._recording = True\n        self._stop_recording.clear()\n        self._source.start()", "response": "Start recording from the audio source."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nstop recording from the audio source.", "response": "def stop_recording(self):\n        \"\"\"Stop recording from the audio source.\"\"\"\n        self._stop_recording.set()\n        with self._source_lock:\n            self._source.stop()\n        self._recording = False"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstopping playback from the audio sink.", "response": "def stop_playback(self):\n        \"\"\"Stop playback from the audio sink.\"\"\"\n        self._sink.flush()\n        self._sink.stop()\n        self._playing = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites bytes to the sink.", "response": "def write(self, buf):\n        \"\"\"Write bytes to the sink (if currently playing).\n        \"\"\"\n        buf = align_buf(buf, self._sample_width)\n        buf = normalize_audio_buffer(buf, self.volume_percentage)\n        return self._sink.write(buf)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef assist(self, text_query):\n        def iter_assist_requests():\n            config = embedded_assistant_pb2.AssistConfig(\n                audio_out_config=embedded_assistant_pb2.AudioOutConfig(\n                    encoding='LINEAR16',\n                    sample_rate_hertz=16000,\n                    volume_percentage=0,\n                ),\n                dialog_state_in=embedded_assistant_pb2.DialogStateIn(\n                    language_code=self.language_code,\n                    conversation_state=self.conversation_state,\n                    is_new_conversation=self.is_new_conversation,\n                ),\n                device_config=embedded_assistant_pb2.DeviceConfig(\n                    device_id=self.device_id,\n                    device_model_id=self.device_model_id,\n                ),\n                text_query=text_query,\n            )\n            # Continue current conversation with later requests.\n            self.is_new_conversation = False\n            if self.display:\n                config.screen_out_config.screen_mode = PLAYING\n            req = embedded_assistant_pb2.AssistRequest(config=config)\n            assistant_helpers.log_assist_request_without_audio(req)\n            yield req\n\n        text_response = None\n        html_response = None\n        for resp in self.assistant.Assist(iter_assist_requests(),\n                                          self.deadline):\n            assistant_helpers.log_assist_response_without_audio(resp)\n            if resp.screen_out.data:\n                html_response = resp.screen_out.data\n            if resp.dialog_state_out.conversation_state:\n                conversation_state = resp.dialog_state_out.conversation_state\n                self.conversation_state = conversation_state\n            if resp.dialog_state_out.supplemental_display_text:\n                text_response = resp.dialog_state_out.supplemental_display_text\n        return text_response, html_response", "response": "Send a text request to the Assistant and playback the response."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef main(api_endpoint, credentials, project_id,\n         device_model_id, device_id, device_config,\n         lang, display, verbose,\n         input_audio_file, output_audio_file,\n         audio_sample_rate, audio_sample_width,\n         audio_iter_size, audio_block_size, audio_flush_size,\n         grpc_deadline, once, *args, **kwargs):\n    \"\"\"Samples for the Google Assistant API.\n\n    Examples:\n      Run the sample with microphone input and speaker output:\n\n        $ python -m googlesamples.assistant\n\n      Run the sample with file input and speaker output:\n\n        $ python -m googlesamples.assistant -i <input file>\n\n      Run the sample with file input and output:\n\n        $ python -m googlesamples.assistant -i <input file> -o <output file>\n    \"\"\"\n    # Setup logging.\n    logging.basicConfig(level=logging.DEBUG if verbose else logging.INFO)\n\n    # Load OAuth 2.0 credentials.\n    try:\n        with open(credentials, 'r') as f:\n            credentials = google.oauth2.credentials.Credentials(token=None,\n                                                                **json.load(f))\n            http_request = google.auth.transport.requests.Request()\n            credentials.refresh(http_request)\n    except Exception as e:\n        logging.error('Error loading credentials: %s', e)\n        logging.error('Run google-oauthlib-tool to initialize '\n                      'new OAuth 2.0 credentials.')\n        sys.exit(-1)\n\n    # Create an authorized gRPC channel.\n    grpc_channel = google.auth.transport.grpc.secure_authorized_channel(\n        credentials, http_request, api_endpoint)\n    logging.info('Connecting to %s', api_endpoint)\n\n    # Configure audio source and sink.\n    audio_device = None\n    if input_audio_file:\n        audio_source = audio_helpers.WaveSource(\n            open(input_audio_file, 'rb'),\n            sample_rate=audio_sample_rate,\n            sample_width=audio_sample_width\n        )\n    else:\n        audio_source = audio_device = (\n            audio_device or audio_helpers.SoundDeviceStream(\n                sample_rate=audio_sample_rate,\n                sample_width=audio_sample_width,\n                block_size=audio_block_size,\n                flush_size=audio_flush_size\n            )\n        )\n    if output_audio_file:\n        audio_sink = audio_helpers.WaveSink(\n            open(output_audio_file, 'wb'),\n            sample_rate=audio_sample_rate,\n            sample_width=audio_sample_width\n        )\n    else:\n        audio_sink = audio_device = (\n            audio_device or audio_helpers.SoundDeviceStream(\n                sample_rate=audio_sample_rate,\n                sample_width=audio_sample_width,\n                block_size=audio_block_size,\n                flush_size=audio_flush_size\n            )\n        )\n    # Create conversation stream with the given audio source and sink.\n    conversation_stream = audio_helpers.ConversationStream(\n        source=audio_source,\n        sink=audio_sink,\n        iter_size=audio_iter_size,\n        sample_width=audio_sample_width,\n    )\n\n    if not device_id or not device_model_id:\n        try:\n            with open(device_config) as f:\n                device = json.load(f)\n                device_id = device['id']\n                device_model_id = device['model_id']\n                logging.info(\"Using device model %s and device id %s\",\n                             device_model_id,\n                             device_id)\n        except Exception as e:\n            logging.warning('Device config not found: %s' % e)\n            logging.info('Registering device')\n            if not device_model_id:\n                logging.error('Option --device-model-id required '\n                              'when registering a device instance.')\n                sys.exit(-1)\n            if not project_id:\n                logging.error('Option --project-id required '\n                              'when registering a device instance.')\n                sys.exit(-1)\n            device_base_url = (\n                'https://%s/v1alpha2/projects/%s/devices' % (api_endpoint,\n                                                             project_id)\n            )\n            device_id = str(uuid.uuid1())\n            payload = {\n                'id': device_id,\n                'model_id': device_model_id,\n                'client_type': 'SDK_SERVICE'\n            }\n            session = google.auth.transport.requests.AuthorizedSession(\n                credentials\n            )\n            r = session.post(device_base_url, data=json.dumps(payload))\n            if r.status_code != 200:\n                logging.error('Failed to register device: %s', r.text)\n                sys.exit(-1)\n            logging.info('Device registered: %s', device_id)\n            pathlib.Path(os.path.dirname(device_config)).mkdir(exist_ok=True)\n            with open(device_config, 'w') as f:\n                json.dump(payload, f)\n\n    device_handler = device_helpers.DeviceRequestHandler(device_id)\n\n    @device_handler.command('action.devices.commands.OnOff')\n    def onoff(on):\n        if on:\n            logging.info('Turning device on')\n        else:\n            logging.info('Turning device off')\n\n    @device_handler.command('com.example.commands.BlinkLight')\n    def blink(speed, number):\n        logging.info('Blinking device %s times.' % number)\n        delay = 1\n        if speed == \"SLOWLY\":\n            delay = 2\n        elif speed == \"QUICKLY\":\n            delay = 0.5\n        for i in range(int(number)):\n            logging.info('Device is blinking.')\n            time.sleep(delay)\n\n    with SampleAssistant(lang, device_model_id, device_id,\n                         conversation_stream, display,\n                         grpc_channel, grpc_deadline,\n                         device_handler) as assistant:\n        # If file arguments are supplied:\n        # exit after the first turn of the conversation.\n        if input_audio_file or output_audio_file:\n            assistant.assist()\n            return\n\n        # If no file arguments supplied:\n        # keep recording voice requests using the microphone\n        # and playing back assistant response using the speaker.\n        # When the once flag is set, don't wait for a trigger. Otherwise, wait.\n        wait_for_user_trigger = not once\n        while True:\n            if wait_for_user_trigger:\n                click.pause(info='Press Enter to send a new request...')\n            continue_conversation = assistant.assist()\n            # wait for user trigger if there is no follow-up turn in\n            # the conversation.\n            wait_for_user_trigger = not continue_conversation\n\n            # If we only want one conversation, break.\n            if once and (not continue_conversation):\n                break", "response": "Main function for the Google Assistant API."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef assist(self):\n        continue_conversation = False\n        device_actions_futures = []\n\n        self.conversation_stream.start_recording()\n        logging.info('Recording audio request.')\n\n        def iter_log_assist_requests():\n            for c in self.gen_assist_requests():\n                assistant_helpers.log_assist_request_without_audio(c)\n                yield c\n            logging.debug('Reached end of AssistRequest iteration.')\n\n        # This generator yields AssistResponse proto messages\n        # received from the gRPC Google Assistant API.\n        for resp in self.assistant.Assist(iter_log_assist_requests(),\n                                          self.deadline):\n            assistant_helpers.log_assist_response_without_audio(resp)\n            if resp.event_type == END_OF_UTTERANCE:\n                logging.info('End of audio request detected.')\n                logging.info('Stopping recording.')\n                self.conversation_stream.stop_recording()\n            if resp.speech_results:\n                logging.info('Transcript of user request: \"%s\".',\n                             ' '.join(r.transcript\n                                      for r in resp.speech_results))\n            if len(resp.audio_out.audio_data) > 0:\n                if not self.conversation_stream.playing:\n                    self.conversation_stream.stop_recording()\n                    self.conversation_stream.start_playback()\n                    logging.info('Playing assistant response.')\n                self.conversation_stream.write(resp.audio_out.audio_data)\n            if resp.dialog_state_out.conversation_state:\n                conversation_state = resp.dialog_state_out.conversation_state\n                logging.debug('Updating conversation state.')\n                self.conversation_state = conversation_state\n            if resp.dialog_state_out.volume_percentage != 0:\n                volume_percentage = resp.dialog_state_out.volume_percentage\n                logging.info('Setting volume to %s%%', volume_percentage)\n                self.conversation_stream.volume_percentage = volume_percentage\n            if resp.dialog_state_out.microphone_mode == DIALOG_FOLLOW_ON:\n                continue_conversation = True\n                logging.info('Expecting follow-on query from user.')\n            elif resp.dialog_state_out.microphone_mode == CLOSE_MICROPHONE:\n                continue_conversation = False\n            if resp.device_action.device_request_json:\n                device_request = json.loads(\n                    resp.device_action.device_request_json\n                )\n                fs = self.device_handler(device_request)\n                if fs:\n                    device_actions_futures.extend(fs)\n            if self.display and resp.screen_out.data:\n                system_browser = browser_helpers.system_browser\n                system_browser.display(resp.screen_out.data)\n\n        if len(device_actions_futures):\n            logging.info('Waiting for device executions to complete.')\n            concurrent.futures.wait(device_actions_futures)\n\n        logging.info('Finished playing assistant response.')\n        self.conversation_stream.stop_playback()\n        return continue_conversation", "response": "Send a voice request to the Assistant and playback the response."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields the AssistRequest messages to send to the API.", "response": "def gen_assist_requests(self):\n        \"\"\"Yields: AssistRequest messages to send to the API.\"\"\"\n\n        config = embedded_assistant_pb2.AssistConfig(\n            audio_in_config=embedded_assistant_pb2.AudioInConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=self.conversation_stream.sample_rate,\n            ),\n            audio_out_config=embedded_assistant_pb2.AudioOutConfig(\n                encoding='LINEAR16',\n                sample_rate_hertz=self.conversation_stream.sample_rate,\n                volume_percentage=self.conversation_stream.volume_percentage,\n            ),\n            dialog_state_in=embedded_assistant_pb2.DialogStateIn(\n                language_code=self.language_code,\n                conversation_state=self.conversation_state,\n                is_new_conversation=self.is_new_conversation,\n            ),\n            device_config=embedded_assistant_pb2.DeviceConfig(\n                device_id=self.device_id,\n                device_model_id=self.device_model_id,\n            )\n        )\n        if self.display:\n            config.screen_out_config.screen_mode = PLAYING\n        # Continue current conversation with later requests.\n        self.is_new_conversation = False\n        # The first AssistRequest must contain the AssistConfig\n        # and no audio data.\n        yield embedded_assistant_pb2.AssistRequest(config=config)\n        for data in self.conversation_stream:\n            # Subsequent requests need audio data, but not config.\n            yield embedded_assistant_pb2.AssistRequest(audio_in=data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nperforms a SVD step on the given data X and return reconstructed X and rank achieved.", "response": "def _svd_step(self, X, shrinkage_value, max_rank=None):\n        \"\"\"\n        Returns reconstructed X from low-rank thresholded SVD and\n        the rank achieved.\n        \"\"\"\n        if max_rank:\n            # if we have a max rank then perform the faster randomized SVD\n            (U, s, V) = randomized_svd(\n                X,\n                max_rank,\n                n_iter=self.n_power_iterations)\n        else:\n            # perform a full rank SVD using ARPACK\n            (U, s, V) = np.linalg.svd(\n                X,\n                full_matrices=False,\n                compute_uv=True)\n        s_thresh = np.maximum(s - shrinkage_value, 0)\n        rank = (s_thresh > 0).sum()\n        s_thresh = s_thresh[:rank]\n        U_thresh = U[:, :rank]\n        V_thresh = V[:rank, :]\n        S_thresh = np.diag(s_thresh)\n        X_reconstruction = np.dot(U_thresh, np.dot(S_thresh, V_thresh))\n        return X_reconstruction, rank"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfilling the missing entries with NaN values.", "response": "def fill(\n            self,\n            X,\n            missing_mask,\n            fill_method=None,\n            inplace=False):\n        \"\"\"\n        Parameters\n        ----------\n        X : np.array\n            Data array containing NaN entries\n\n        missing_mask : np.array\n            Boolean array indicating where NaN entries are\n\n        fill_method : str\n            \"zero\": fill missing entries with zeros\n            \"mean\": fill with column means\n            \"median\" : fill with column medians\n            \"min\": fill with min value per column\n            \"random\": fill with gaussian samples according to mean/std of column\n\n        inplace : bool\n            Modify matrix or fill a copy\n        \"\"\"\n        X = check_array(X, force_all_finite=False)\n\n        if not inplace:\n            X = X.copy()\n\n        if not fill_method:\n            fill_method = self.fill_method\n\n        if fill_method not in (\"zero\", \"mean\", \"median\", \"min\", \"random\"):\n            raise ValueError(\"Invalid fill method: '%s'\" % (fill_method))\n        elif fill_method == \"zero\":\n            # replace NaN's with 0\n            X[missing_mask] = 0\n        elif fill_method == \"mean\":\n            self._fill_columns_with_fn(X, missing_mask, np.nanmean)\n        elif fill_method == \"median\":\n            self._fill_columns_with_fn(X, missing_mask, np.nanmedian)\n        elif fill_method == \"min\":\n            self._fill_columns_with_fn(X, missing_mask, np.nanmin)\n        elif fill_method == \"random\":\n            self._fill_columns_with_fn(\n                X,\n                missing_mask,\n                col_fn=generate_random_column_samples)\n        return X"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_input_data(self, X):\n        X = check_array(X, force_all_finite=False)\n        if X.dtype != \"f\" and X.dtype != \"d\":\n            X = X.astype(float)\n\n        self._check_input(X)\n        missing_mask = np.isnan(X)\n        self._check_missing_value_mask(missing_mask)\n        return X, missing_mask", "response": "Check to make sure that the input matrix and its mask of missing values are valid. Returns X and missing mask."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clip(self, X):\n        X = np.asarray(X)\n        if self.min_value is not None:\n            X[X < self.min_value] = self.min_value\n        if self.max_value is not None:\n            X[X > self.max_value] = self.max_value\n        return X", "response": "Clip values to fall within any global or column - wise min or max constraints."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef project_result(self, X):\n        X = np.asarray(X)\n        if self.normalizer is not None:\n            X = self.normalizer.inverse_transform(X)\n        return self.clip(X)", "response": "Project the result of the normalization and clip to the user - specified min max."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nfit the imputer and then transform input X.", "response": "def fit_transform(self, X, y=None):\n        \"\"\"\n        Fit the imputer and then transform input `X`\n\n        Note: all imputations should have a `fit_transform` method,\n        but only some (like IterativeImputer) also support inductive mode\n        using `fit` or `fit_transform` on `X_train` and then `transform`\n        on new `X_test`.\n        \"\"\"\n        X_original, missing_mask = self.prepare_input_data(X)\n        observed_mask = ~missing_mask\n        X = X_original.copy()\n        if self.normalizer is not None:\n            X = self.normalizer.fit_transform(X)\n        X_filled = self.fill(X, missing_mask, inplace=True)\n        if not isinstance(X_filled, np.ndarray):\n            raise TypeError(\n                \"Expected %s.fill() to return NumPy array but got %s\" % (\n                    self.__class__.__name__,\n                    type(X_filled)))\n\n        X_result = self.solve(X_filled, missing_mask)\n        if not isinstance(X_result, np.ndarray):\n            raise TypeError(\n                \"Expected %s.solve() to return NumPy array but got %s\" % (\n                    self.__class__.__name__,\n                    type(X_result)))\n\n        X_result = self.project_result(X=X_result)\n        X_result[observed_mask] = X_original[observed_mask]\n        return X_result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform input X to new X.", "response": "def transform(self, X, y=None):\n        \"\"\"\n        Transform input `X`.\n\n        Note: all imputations should have a `fit_transform` method,\n        but only some (like IterativeImputer) also support inductive mode\n        using `fit` or `fit_transform` on `X_train` and then `transform`\n        on new `X_test`.\n        \"\"\"\n        raise ValueError(\n            \"%s.transform not implemented! This imputation algorithm likely \"\n            \"doesn't support inductive mode. Only %s.fit_transform is \"\n            \"supported at this time.\" % (\n                self.__class__.__name__, self.__class__.__name__))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef sorted_errors(self):\n        for i, (name, mae) in enumerate(\n                sorted(self.mae_dict.items(), key=lambda x: x[1])):\n            yield(i + 1, name, self.mse_dict[name], self.mae_dict[name],)", "response": "Generator for ( rank name MSE MAE ) sorted by increasing MAE"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntesting if x is NaN", "response": "def is_scalar_nan(x):\n    \"\"\"Tests if x is NaN\n    This function is meant to overcome the issue that np.isnan does not allow\n    non-numerical types as input, and that np.nan is not np.float('nan').\n    Parameters\n    ----------\n    x : any type\n    Returns\n    -------\n    boolean\n    Examples\n    --------\n    >>> is_scalar_nan(np.nan)\n    True\n    >>> is_scalar_nan(float(\"nan\"))\n    True\n    >>> is_scalar_nan(None)\n    False\n    >>> is_scalar_nan(\"\")\n    False\n    >>> is_scalar_nan([np.nan])\n    False\n    \"\"\"\n\n    # convert from numpy.bool_ to python bool to ensure that testing\n    # is_scalar_nan(x) is True does not fail.\n    # Redondant np.floating is needed because numbers can't match np.float32\n    # in python 2.\n    return bool(isinstance(x, (numbers.Real, np.floating)) and np.isnan(x))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _get_mask(X, value_to_mask):\n    if is_scalar_nan(value_to_mask):\n        if X.dtype.kind == \"f\":\n            return np.isnan(X)\n        elif X.dtype.kind in (\"i\", \"u\"):\n            # can't have NaNs in integer array.\n            return np.zeros(X.shape, dtype=bool)\n        else:\n            # np.isnan does not work on object dtypes.\n            return _object_dtype_isnan(X)\n    else:\n        # X == value_to_mask with object dytpes does not always perform\n        # element-wise for old versions of numpy\n        return np.equal(X, value_to_mask)", "response": "Compute the boolean mask X == value_to_mask."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _most_frequent(array, extra_value, n_repeat):\n    # Compute the most frequent value in array only\n    if array.size > 0:\n        with warnings.catch_warnings():\n            # stats.mode raises a warning when input array contains objects due\n            # to incapacity to detect NaNs. Irrelevant here since input array\n            # has already been NaN-masked.\n            warnings.simplefilter(\"ignore\", RuntimeWarning)\n            mode = stats.mode(array)\n\n        most_frequent_value = mode[0][0]\n        most_frequent_count = mode[1][0]\n    else:\n        most_frequent_value = 0\n        most_frequent_count = 0\n\n    # Compare to array + [extra_value] * n_repeat\n    if most_frequent_count == 0 and n_repeat == 0:\n        return np.nan\n    elif most_frequent_count < n_repeat:\n        return extra_value\n    elif most_frequent_count > n_repeat:\n        return most_frequent_value\n    elif most_frequent_count == n_repeat:\n        # Ties the breaks. Copy the behaviour of scipy.stats.mode\n        if most_frequent_value < extra_value:\n            return most_frequent_value\n        else:\n            return extra_value", "response": "Compute the most frequent value in a 1d array extended with\n       [ extra_value ] n_repeat where extra_value is assumed to be not part\n       of the array."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fit(self, X, y=None):\n        X = self._validate_input(X)\n\n        # default fill_value is 0 for numerical input and \"missing_value\"\n        # otherwise\n        if self.fill_value is None:\n            if X.dtype.kind in (\"i\", \"u\", \"f\"):\n                fill_value = 0\n            else:\n                fill_value = \"missing_value\"\n        else:\n            fill_value = self.fill_value\n\n        # fill_value should be numerical in case of numerical input\n        if (self.strategy == \"constant\" and\n                X.dtype.kind in (\"i\", \"u\", \"f\") and\n                not isinstance(fill_value, numbers.Real)):\n            raise ValueError(\"'fill_value'={0} is invalid. Expected a \"\n                             \"numerical value when imputing numerical \"\n                             \"data\".format(fill_value))\n\n        if sparse.issparse(X):\n            # missing_values = 0 not allowed with sparse data as it would\n            # force densification\n            if self.missing_values == 0:\n                raise ValueError(\"Imputation not possible when missing_values \"\n                                 \"== 0 and input is sparse. Provide a dense \"\n                                 \"array instead.\")\n            else:\n                self.statistics_ = self._sparse_fit(X,\n                                                    self.strategy,\n                                                    self.missing_values,\n                                                    fill_value)\n        else:\n            self.statistics_ = self._dense_fit(X,\n                                               self.strategy,\n                                               self.missing_values,\n                                               fill_value)\n\n        return self", "response": "Fit the imputer on X."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits the transformer on sparse data.", "response": "def _sparse_fit(self, X, strategy, missing_values, fill_value):\n        \"\"\"Fit the transformer on sparse data.\"\"\"\n        mask_data = _get_mask(X.data, missing_values)\n        n_implicit_zeros = X.shape[0] - np.diff(X.indptr)\n\n        statistics = np.empty(X.shape[1])\n\n        if strategy == \"constant\":\n            # for constant strategy, self.statistcs_ is used to store\n            # fill_value in each column\n            statistics.fill(fill_value)\n\n        else:\n            for i in range(X.shape[1]):\n                column = X.data[X.indptr[i]:X.indptr[i + 1]]\n                mask_column = mask_data[X.indptr[i]:X.indptr[i + 1]]\n                column = column[~mask_column]\n\n                # combine explicit and implicit zeros\n                mask_zeros = _get_mask(column, 0)\n                column = column[~mask_zeros]\n                n_explicit_zeros = mask_zeros.sum()\n                n_zeros = n_implicit_zeros[i] + n_explicit_zeros\n\n                if strategy == \"mean\":\n                    s = column.size + n_zeros\n                    statistics[i] = np.nan if s == 0 else column.sum() / s\n\n                elif strategy == \"median\":\n                    statistics[i] = _get_median(column,\n                                                n_zeros)\n\n                elif strategy == \"most_frequent\":\n                    statistics[i] = _most_frequent(column,\n                                                   0,\n                                                   n_zeros)\n        return statistics"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _dense_fit(self, X, strategy, missing_values, fill_value):\n        mask = _get_mask(X, missing_values)\n        masked_X = ma.masked_array(X, mask=mask)\n\n        # Mean\n        if strategy == \"mean\":\n            mean_masked = np.ma.mean(masked_X, axis=0)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            mean = np.ma.getdata(mean_masked)\n            mean[np.ma.getmask(mean_masked)] = np.nan\n\n            return mean\n\n        # Median\n        elif strategy == \"median\":\n            median_masked = np.ma.median(masked_X, axis=0)\n            # Avoid the warning \"Warning: converting a masked element to nan.\"\n            median = np.ma.getdata(median_masked)\n            median[np.ma.getmaskarray(median_masked)] = np.nan\n\n            return median\n\n        # Most frequent\n        elif strategy == \"most_frequent\":\n            # scipy.stats.mstats.mode cannot be used because it will no work\n            # properly if the first element is masked and if its frequency\n            # is equal to the frequency of the most frequent valid element\n            # See https://github.com/scipy/scipy/issues/2636\n\n            # To be able access the elements by columns\n            X = X.transpose()\n            mask = mask.transpose()\n\n            if X.dtype.kind == \"O\":\n                most_frequent = np.empty(X.shape[0], dtype=object)\n            else:\n                most_frequent = np.empty(X.shape[0])\n\n            for i, (row, row_mask) in enumerate(zip(X[:], mask[:])):\n                row_mask = np.logical_not(row_mask).astype(np.bool)\n                row = row[row_mask]\n                most_frequent[i] = _most_frequent(row, np.nan, 0)\n\n            return most_frequent\n\n        # Constant\n        elif strategy == \"constant\":\n            # for constant strategy, self.statistcs_ is used to store\n            # fill_value in each column\n            return np.full(X.shape[1], fill_value, dtype=X.dtype)", "response": "Fit the transformer on dense data."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef transform(self, X):\n        check_is_fitted(self, 'statistics_')\n\n        X = self._validate_input(X)\n\n        statistics = self.statistics_\n\n        if X.shape[1] != statistics.shape[0]:\n            raise ValueError(\"X has %d features per sample, expected %d\"\n                             % (X.shape[1], self.statistics_.shape[0]))\n\n        # Delete the invalid columns if strategy is not constant\n        if self.strategy == \"constant\":\n            valid_statistics = statistics\n        else:\n            # same as np.isnan but also works for object dtypes\n            invalid_mask = _get_mask(statistics, np.nan)\n            valid_mask = np.logical_not(invalid_mask)\n            valid_statistics = statistics[valid_mask]\n            valid_statistics_indexes = np.flatnonzero(valid_mask)\n\n            if invalid_mask.any():\n                missing = np.arange(X.shape[1])[invalid_mask]\n                if self.verbose:\n                    warnings.warn(\"Deleting features without \"\n                                  \"observed values: %s\" % missing)\n                X = X[:, valid_statistics_indexes]\n\n        # Do actual imputation\n        if sparse.issparse(X):\n            if self.missing_values == 0:\n                raise ValueError(\"Imputation not possible when missing_values \"\n                                 \"== 0 and input is sparse. Provide a dense \"\n                                 \"array instead.\")\n            else:\n                mask = _get_mask(X.data, self.missing_values)\n                indexes = np.repeat(np.arange(len(X.indptr) - 1, dtype=np.int),\n                                    np.diff(X.indptr))[mask]\n\n                X.data[mask] = valid_statistics[indexes].astype(X.dtype,\n                                                                copy=False)\n        else:\n            mask = _get_mask(X, self.missing_values)\n            n_missing = np.sum(mask, axis=0)\n            values = np.repeat(valid_statistics, n_missing)\n            coordinates = np.where(mask.transpose())[::-1]\n\n            X[coordinates] = values\n\n        return X", "response": "Impute all missing values in X."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nimputes a single feature from the others provided.", "response": "def _impute_one_feature(self,\n                            X_filled,\n                            mask_missing_values,\n                            feat_idx,\n                            neighbor_feat_idx,\n                            predictor=None,\n                            fit_mode=True):\n        \"\"\"Impute a single feature from the others provided.\n\n        This function predicts the missing values of one of the features using\n        the current estimates of all the other features. The ``predictor`` must\n        support ``return_std=True`` in its ``predict`` method for this function\n        to work.\n\n        Parameters\n        ----------\n        X_filled : ndarray\n            Input data with the most recent imputations.\n\n        mask_missing_values : ndarray\n            Input data's missing indicator matrix.\n\n        feat_idx : int\n            Index of the feature currently being imputed.\n\n        neighbor_feat_idx : ndarray\n            Indices of the features to be used in imputing ``feat_idx``.\n\n        predictor : object\n            The predictor to use at this step of the round-robin imputation.\n            If ``sample_posterior`` is True, the predictor must support\n            ``return_std`` in its ``predict`` method.\n            If None, it will be cloned from self._predictor.\n\n        fit_mode : boolean, default=True\n            Whether to fit and predict with the predictor or just predict.\n\n        Returns\n        -------\n        X_filled : ndarray\n            Input data with ``X_filled[missing_row_mask, feat_idx]`` updated.\n\n        predictor : predictor with sklearn API\n            The fitted predictor used to impute\n            ``X_filled[missing_row_mask, feat_idx]``.\n        \"\"\"\n\n        # if nothing is missing, just return the default\n        # (should not happen at fit time because feat_ids would be excluded)\n        missing_row_mask = mask_missing_values[:, feat_idx]\n        if not np.any(missing_row_mask):\n            return X_filled, predictor\n\n        if predictor is None and fit_mode is False:\n            raise ValueError(\"If fit_mode is False, then an already-fitted \"\n                             \"predictor should be passed in.\")\n\n        if predictor is None:\n            predictor = clone(self._predictor)\n\n        if fit_mode:\n            X_train = safe_indexing(X_filled[:, neighbor_feat_idx],\n                                    ~missing_row_mask)\n            y_train = safe_indexing(X_filled[:, feat_idx],\n                                    ~missing_row_mask)\n            predictor.fit(X_train, y_train)\n\n        # get posterior samples\n        X_test = safe_indexing(X_filled[:, neighbor_feat_idx],\n                               missing_row_mask)\n        if self.sample_posterior:\n            mus, sigmas = predictor.predict(X_test, return_std=True)\n            good_sigmas = sigmas > 0\n            imputed_values = np.zeros(mus.shape, dtype=X_filled.dtype)\n            imputed_values[~good_sigmas] = mus[~good_sigmas]\n            imputed_values[good_sigmas] = self.random_state_.normal(\n                loc=mus[good_sigmas], scale=sigmas[good_sigmas])\n        else:\n            imputed_values = predictor.predict(X_test)\n\n        # clip the values\n        imputed_values = self.clip(imputed_values)\n\n        # update the feature\n        X_filled[missing_row_mask, feat_idx] = imputed_values\n        return X_filled, predictor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_neighbor_feat_idx(self,\n                               n_features,\n                               feat_idx,\n                               abs_corr_mat):\n        \"\"\"Get a list of other features to predict ``feat_idx``.\n\n        If self.n_nearest_features is less than or equal to the total\n        number of features, then use a probability proportional to the absolute\n        correlation between ``feat_idx`` and each other feature to randomly\n        choose a subsample of the other features (without replacement).\n\n        Parameters\n        ----------\n        n_features : int\n            Number of features in ``X``.\n\n        feat_idx : int\n            Index of the feature currently being imputed.\n\n        abs_corr_mat : ndarray, shape (n_features, n_features)\n            Absolute correlation matrix of ``X``. The diagonal has been zeroed\n            out and each feature has been normalized to sum to 1. Can be None.\n\n        Returns\n        -------\n        neighbor_feat_idx : array-like\n            The features to use to impute ``feat_idx``.\n        \"\"\"\n        if (self.n_nearest_features is not None and\n                self.n_nearest_features < n_features):\n            p = abs_corr_mat[:, feat_idx]\n            neighbor_feat_idx = self.random_state_.choice(\n                np.arange(n_features), self.n_nearest_features, replace=False,\n                p=p)\n        else:\n            inds_left = np.arange(feat_idx)\n            inds_right = np.arange(feat_idx + 1, n_features)\n            neighbor_feat_idx = np.concatenate((inds_left, inds_right))\n        return neighbor_feat_idx", "response": "Get a list of other features to predict feat_idx."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndecide in what order we will update the features. As a homage to the MICE R package, we will have 4 main options of how to order the updates, and use a random order if anything else is specified. Also, this function skips features which have no missing values. Parameters ---------- mask_missing_values : array-like, shape (n_samples, n_features) Input data's missing indicator matrix, where \"n_samples\" is the number of samples and \"n_features\" is the number of features. Returns ------- ordered_idx : ndarray, shape (n_features,) The order in which to impute the features.", "response": "def _get_ordered_idx(self, mask_missing_values):\n        \"\"\"Decide in what order we will update the features.\n\n        As a homage to the MICE R package, we will have 4 main options of\n        how to order the updates, and use a random order if anything else\n        is specified.\n\n        Also, this function skips features which have no missing values.\n\n        Parameters\n        ----------\n        mask_missing_values : array-like, shape (n_samples, n_features)\n            Input data's missing indicator matrix, where \"n_samples\" is the\n            number of samples and \"n_features\" is the number of features.\n\n        Returns\n        -------\n        ordered_idx : ndarray, shape (n_features,)\n            The order in which to impute the features.\n        \"\"\"\n        frac_of_missing_values = mask_missing_values.mean(axis=0)\n        missing_values_idx = np.nonzero(frac_of_missing_values)[0]\n        if self.imputation_order == 'roman':\n            ordered_idx = missing_values_idx\n        elif self.imputation_order == 'arabic':\n            ordered_idx = missing_values_idx[::-1]\n        elif self.imputation_order == 'ascending':\n            n = len(frac_of_missing_values) - len(missing_values_idx)\n            ordered_idx = np.argsort(frac_of_missing_values,\n                                     kind='mergesort')[n:][::-1]\n        elif self.imputation_order == 'descending':\n            n = len(frac_of_missing_values) - len(missing_values_idx)\n            ordered_idx = np.argsort(frac_of_missing_values,\n                                     kind='mergesort')[n:]\n        elif self.imputation_order == 'random':\n            ordered_idx = missing_values_idx\n            self.random_state_.shuffle(ordered_idx)\n        else:\n            raise ValueError(\"Got an invalid imputation order: '{0}'. It must \"\n                             \"be one of the following: 'roman', 'arabic', \"\n                             \"'ascending', 'descending', or \"\n                             \"'random'.\".format(self.imputation_order))\n        return ordered_idx"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets absolute correlation matrix between features X_filled and the most recent imputations.", "response": "def _get_abs_corr_mat(self, X_filled, tolerance=1e-6):\n        \"\"\"Get absolute correlation matrix between features.\n\n        Parameters\n        ----------\n        X_filled : ndarray, shape (n_samples, n_features)\n            Input data with the most recent imputations.\n\n        tolerance : float, optional (default=1e-6)\n            ``abs_corr_mat`` can have nans, which will be replaced\n            with ``tolerance``.\n\n        Returns\n        -------\n        abs_corr_mat : ndarray, shape (n_features, n_features)\n            Absolute correlation matrix of ``X`` at the beginning of the\n            current round. The diagonal has been zeroed out and each feature's\n            absolute correlations with all others have been normalized to sum\n            to 1.\n        \"\"\"\n        n_features = X_filled.shape[1]\n        if (self.n_nearest_features is None or\n                self.n_nearest_features >= n_features):\n            return None\n        abs_corr_mat = np.abs(np.corrcoef(X_filled.T))\n        # np.corrcoef is not defined for features with zero std\n        abs_corr_mat[np.isnan(abs_corr_mat)] = tolerance\n        # ensures exploration, i.e. at least some probability of sampling\n        np.clip(abs_corr_mat, tolerance, None, out=abs_corr_mat)\n        # features are not their own neighbors\n        np.fill_diagonal(abs_corr_mat, 0)\n        # needs to sum to 1 for np.random.choice sampling\n        abs_corr_mat = normalize(abs_corr_mat, norm='l1', axis=0, copy=False)\n        return abs_corr_mat"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms initial imputation for input X.", "response": "def _initial_imputation(self, X):\n        \"\"\"Perform initial imputation for input X.\n\n        Parameters\n        ----------\n        X : ndarray, shape (n_samples, n_features)\n            Input data, where \"n_samples\" is the number of samples and\n            \"n_features\" is the number of features.\n\n        Returns\n        -------\n        Xt : ndarray, shape (n_samples, n_features)\n            Input data, where \"n_samples\" is the number of samples and\n            \"n_features\" is the number of features.\n\n        X_filled : ndarray, shape (n_samples, n_features)\n            Input data with the most recent imputations.\n\n        mask_missing_values : ndarray, shape (n_samples, n_features)\n            Input data's missing indicator matrix, where \"n_samples\" is the\n            number of samples and \"n_features\" is the number of features.\n        \"\"\"\n        # TODO: change False to \"allow-nan\"\n        if is_scalar_nan(self.missing_values):\n            force_all_finite = False # \"allow-nan\"\n        else:\n            force_all_finite = True\n\n        X = check_array(X, dtype=FLOAT_DTYPES, order=\"F\",\n                        force_all_finite=force_all_finite)\n        _check_inputs_dtype(X, self.missing_values)\n\n        mask_missing_values = _get_mask(X, self.missing_values)\n        if self.initial_imputer_ is None:\n            self.initial_imputer_ = _SimpleImputer(\n                                            missing_values=self.missing_values,\n                                            strategy=self.initial_strategy)\n            X_filled = self.initial_imputer_.fit_transform(X)\n        else:\n            X_filled = self.initial_imputer_.transform(X)\n\n        valid_mask = np.flatnonzero(np.logical_not(\n            np.isnan(self.initial_imputer_.statistics_)))\n        Xt = X[:, valid_mask]\n        mask_missing_values = mask_missing_values[:, valid_mask]\n\n        return Xt, X_filled, mask_missing_values"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nfits the imputer on X and returns the transformed X.", "response": "def fit_transform(self, X, y=None):\n        \"\"\"Fits the imputer on X and return the transformed X.\n\n        Parameters\n        ----------\n        X : array-like, shape (n_samples, n_features)\n            Input data, where \"n_samples\" is the number of samples and\n            \"n_features\" is the number of features.\n\n        y : ignored.\n\n        Returns\n        -------\n        Xt : array-like, shape (n_samples, n_features)\n             The imputed input data.\n        \"\"\"\n        self.random_state_ = getattr(self, \"random_state_\",\n                                     check_random_state(self.random_state))\n\n        if self.n_iter < 0:\n            raise ValueError(\n                \"'n_iter' should be a positive integer. Got {} instead.\"\n                .format(self.n_iter))\n\n        if self.predictor is None:\n            if self.sample_posterior:\n                from sklearn.linear_model import BayesianRidge\n                self._predictor = BayesianRidge()\n            else:\n                from sklearn.linear_model import RidgeCV\n                # including a very small alpha to approximate OLS\n                self._predictor = RidgeCV(alphas=np.array([1e-5, 0.1,  1, 10]))\n        else:\n            self._predictor = clone(self.predictor)\n\n        if hasattr(self._predictor, 'random_state'):\n            self._predictor.random_state = self.random_state_\n\n        self._min_value = np.nan if self.min_value is None else self.min_value\n        self._max_value = np.nan if self.max_value is None else self.max_value\n\n        self.initial_imputer_ = None\n        X, Xt, mask_missing_values = self._initial_imputation(X)\n\n        if self.n_iter == 0:\n            return Xt\n\n        # order in which to impute\n        # note this is probably too slow for large feature data (d > 100000)\n        # and a better way would be good.\n        # see: https://goo.gl/KyCNwj and subsequent comments\n        ordered_idx = self._get_ordered_idx(mask_missing_values)\n        self.n_features_with_missing_ = len(ordered_idx)\n\n        abs_corr_mat = self._get_abs_corr_mat(Xt)\n\n        # impute data\n        n_samples, n_features = Xt.shape\n        self.imputation_sequence_ = []\n        if self.verbose > 0:\n            print(\"[IterativeImputer] Completing matrix with shape %s\"\n                  % (X.shape,))\n        start_t = time()\n        for i_rnd in range(self.n_iter):\n            if self.imputation_order == 'random':\n                ordered_idx = self._get_ordered_idx(mask_missing_values)\n\n            for feat_idx in ordered_idx:\n                neighbor_feat_idx = self._get_neighbor_feat_idx(n_features,\n                                                                feat_idx,\n                                                                abs_corr_mat)\n                Xt, predictor = self._impute_one_feature(\n                    Xt, mask_missing_values, feat_idx, neighbor_feat_idx,\n                    predictor=None, fit_mode=True)\n                predictor_triplet = ImputerTriplet(feat_idx,\n                                                   neighbor_feat_idx,\n                                                   predictor)\n                self.imputation_sequence_.append(predictor_triplet)\n\n            if self.verbose > 0:\n                print('[IterativeImputer] Ending imputation round '\n                      '%d/%d, elapsed time %0.2f'\n                      % (i_rnd + 1, self.n_iter, time() - start_t))\n\n        Xt[~mask_missing_values] = X[~mask_missing_values]\n        return Xt"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nimputes all missing values in X. Note that this is stochastic, and that if random_state is not fixed, repeated calls, or permuted input, will yield different results. Parameters ---------- X : array-like, shape = [n_samples, n_features] The input data to complete. Returns ------- Xt : array-like, shape (n_samples, n_features) The imputed input data.", "response": "def transform(self, X):\n        \"\"\"Imputes all missing values in X.\n\n        Note that this is stochastic, and that if random_state is not fixed,\n        repeated calls, or permuted input, will yield different results.\n\n        Parameters\n        ----------\n        X : array-like, shape = [n_samples, n_features]\n            The input data to complete.\n\n        Returns\n        -------\n        Xt : array-like, shape (n_samples, n_features)\n             The imputed input data.\n        \"\"\"\n        check_is_fitted(self, 'initial_imputer_')\n\n        X, Xt, mask_missing_values = self._initial_imputation(X)\n\n        if self.n_iter == 0:\n            return Xt\n\n        imputations_per_round = len(self.imputation_sequence_) // self.n_iter\n        i_rnd = 0\n        if self.verbose > 0:\n            print(\"[IterativeImputer] Completing matrix with shape %s\"\n                  % (X.shape,))\n        start_t = time()\n        for it, predictor_triplet in enumerate(self.imputation_sequence_):\n            Xt, _ = self._impute_one_feature(\n                Xt,\n                mask_missing_values,\n                predictor_triplet.feat_idx,\n                predictor_triplet.neighbor_feat_idx,\n                predictor=predictor_triplet.predictor,\n                fit_mode=False\n            )\n            if not (it + 1) % imputations_per_round:\n                if self.verbose > 1:\n                    print('[IterativeImputer] Ending imputation round '\n                          '%d/%d, elapsed time %0.2f'\n                          % (i_rnd + 1, self.n_iter, time() - start_t))\n                i_rnd += 1\n\n        Xt[~mask_missing_values] = X[~mask_missing_values]\n        return Xt"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _constraints(self, X, missing_mask, S, error_tolerance):\n        ok_mask = ~missing_mask\n        masked_X = cvxpy.multiply(ok_mask, X)\n        masked_S = cvxpy.multiply(ok_mask, S)\n        abs_diff = cvxpy.abs(masked_S - masked_X)\n        close_to_data = abs_diff <= error_tolerance\n        constraints = [close_to_data]\n        if self.require_symmetric_solution:\n            constraints.append(S == S.T)\n\n        if self.min_value is not None:\n            constraints.append(S >= self.min_value)\n\n        if self.max_value is not None:\n            constraints.append(S <= self.max_value)\n\n        return constraints", "response": "Returns a list of constraints that can be applied to the entry point of the logarithmic entry."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _create_objective(self, m, n):\n        # S is the completed matrix\n        shape = (m, n)\n        S = cvxpy.Variable(shape, name=\"S\")\n        norm = cvxpy.norm(S, \"nuc\")\n        objective = cvxpy.Minimize(norm)\n        return S, objective", "response": "Creates the objective function and variable representing the objective function for the current entry point."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef estimate_row_means(\n            self,\n            X,\n            observed,\n            column_means,\n            column_scales):\n        \"\"\"\n        row_center[i] =\n        sum{j in observed[i, :]}{\n            (1 / column_scale[j]) * (X[i, j] - column_center[j])\n        }\n        ------------------------------------------------------------\n        sum{j in observed[i, :]}{1 / column_scale[j]}\n        \"\"\"\n\n        n_rows, n_cols = X.shape\n\n        column_means = np.asarray(column_means)\n        if len(column_means) != n_cols:\n            raise ValueError(\"Expected length %d but got shape %s\" % (\n                n_cols, column_means.shape))\n        X = X - column_means.reshape((1, n_cols))\n        column_weights = 1.0 / column_scales\n        X *= column_weights.reshape((1, n_cols))\n        row_means = np.zeros(n_rows, dtype=X.dtype)\n        row_residual_sums = np.nansum(X, axis=1)\n        for i in range(n_rows):\n            row_mask = observed[i, :]\n            sum_weights = column_weights[row_mask].sum()\n            row_means[i] = row_residual_sums[i] / sum_weights\n        return row_means", "response": "Estimate the row means of the logarithmic entry for each entry in the logarithmic entry."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef estimate_column_means(\n            self,\n            X,\n            observed,\n            row_means,\n            row_scales):\n        \"\"\"\n        column_center[j] =\n        sum{i in observed[:, j]}{\n            (1 / row_scale[i]) * (X[i, j]) - row_center[i])\n        }\n        ------------------------------------------------------------\n        sum{i in observed[:, j]}{1 / row_scale[i]}\n        \"\"\"\n        n_rows, n_cols = X.shape\n        row_means = np.asarray(row_means)\n\n        if len(row_means) != n_rows:\n            raise ValueError(\"Expected length %d but got shape %s\" % (\n                n_rows, row_means.shape))\n        column_means = np.zeros(n_cols, dtype=X.dtype)\n\n        X = X - row_means.reshape((n_rows, 1))\n        row_weights = 1.0 / row_scales\n        X *= row_weights.reshape((n_rows, 1))\n        col_residual_sums = np.nansum(X, axis=0)\n        for j in range(n_cols):\n            col_mask = observed[:, j]\n            sum_weights = row_weights[col_mask].sum()\n            column_means[j] = col_residual_sums[j] / sum_weights\n        return column_means", "response": "Estimate the column means of the log - likelihood of the given data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nestimate the row scales for a single entry in the logarithmic table.", "response": "def estimate_row_scales(\n            self,\n            X_centered,\n            column_scales):\n        \"\"\"\n        row_scale[i]**2 =\n        mean{j in observed[i, :]}{\n            (X[i, j] - row_center[i] - column_center[j]) ** 2\n            --------------------------------------------------\n                        column_scale[j] ** 2\n        }\n        \"\"\"\n        n_rows, n_cols = X_centered.shape\n        column_scales = np.asarray(column_scales)\n        if len(column_scales) != n_cols:\n            raise ValueError(\"Expected length %d but got shape %s\" % (\n                n_cols, column_scales))\n        row_variances = np.nanmean(\n            X_centered ** 2 / (column_scales ** 2).reshape((1, n_cols)),\n            axis=1)\n        row_variances[row_variances == 0] = 1.0\n        assert len(row_variances) == n_rows, \"%d != %d\" % (\n            len(row_variances),\n            n_rows)\n        return np.sqrt(row_variances)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nestimating the column scales of the log - likelihood of the given n - grams.", "response": "def estimate_column_scales(\n            self,\n            X_centered,\n            row_scales):\n        \"\"\"\n        column_scale[j] ** 2 =\n          mean{i in observed[:, j]}{\n            (X[i, j] - row_center[i] - column_center[j]) ** 2\n            -------------------------------------------------\n                        row_scale[i] ** 2\n        }\n        \"\"\"\n        n_rows, n_cols = X_centered.shape\n        row_scales = np.asarray(row_scales)\n\n        if len(row_scales) != n_rows:\n            raise ValueError(\"Expected length %s, got shape %s\" % (\n                n_rows, row_scales.shape,))\n\n        column_variances = np.nanmean(\n            X_centered ** 2 / (row_scales ** 2).reshape((n_rows, 1)),\n            axis=0)\n        column_variances[column_variances == 0] = 1.0\n        assert len(column_variances) == n_cols, \"%d != %d\" % (\n            len(column_variances),\n            n_cols)\n        return np.sqrt(column_variances)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef jacard_similarity_from_nested_dicts(self, nested_dictionaries):\n        sims = {}\n        overlaps = {}\n        weights = {}\n        for a, column_dict_a in nested_dictionaries.items():\n            row_set_a = set(column_dict_a.keys())\n            for b, column_dict_b in nested_dictionaries.items():\n                row_set_b = set(column_dict_b.keys())\n                common_rows = row_set_a.intersection(row_set_b)\n                n_overlap = len(common_rows)\n                overlaps[(a, b)] = n_overlap\n                total = 0.0\n                weight = 0.0\n                for row_name in common_rows:\n                    value_a = column_dict_a[row_name]\n                    value_b = column_dict_b[row_name]\n                    minval = min(value_a, value_b)\n                    maxval = max(value_a, value_b)\n                    total += minval\n                    weight += maxval\n                weights[(a, b)] = weight\n                if weight < self.min_weight_for_similarity:\n                    continue\n                if n_overlap < self.min_count_for_similarity:\n                    continue\n                sims[(a, b)] = total / weight\n        return sims, overlaps, weights", "response": "Compute the continuous Jacard similarity between all pairs of keys in dictionary - of - dictionaries given as an input."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef complete_dict(\n            self,\n            values_dict):\n        \"\"\"\n        Keys of nested dictionaries can be arbitrary objects.\n        \"\"\"\n        if self.orientation != \"rows\":\n            values_dict = transpose_nested_dictionary(values_dict)\n\n        row_keys, column_keys = collect_nested_keys(values_dict)\n        if self.verbose:\n            print(\"[SimilarityWeightedAveraging] # rows = %d\" % (len(row_keys)))\n            print(\"[SimilarityWeightedAveraging] # columns = %d\" % (len(column_keys)))\n        similarities, overlaps, weights = \\\n            self.jacard_similarity_from_nested_dicts(values_dict)\n        if self.verbose:\n            print(\n                \"[SimilarityWeightedAveraging] Computed %d similarities between rows\" % (\n                    len(similarities),))\n        column_to_row_values = reverse_lookup_from_nested_dict(values_dict)\n\n        result = defaultdict(dict)\n\n        exponent = self.similarity_exponent\n        shrinkage_coef = self.shrinkage_coef\n        for i, row_key in enumerate(row_keys):\n            for column_key, value_triplets in column_to_row_values.items():\n                total = 0\n                denom = shrinkage_coef\n                for (other_row_key, y) in value_triplets:\n                    sample_weight = 1.0\n                    sim = similarities.get((row_key, other_row_key), 0)\n                    combined_weight = sim ** exponent\n                    combined_weight *= sample_weight\n                    total += combined_weight * y\n                    denom += combined_weight\n                if denom > shrinkage_coef:\n                    result[row_key][column_key] = total / denom\n        if self.orientation != \"rows\":\n            result = transpose_nested_dictionary(result)\n        return result", "response": "Complete a nested dictionary of similarities."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ngive a nested dictionary that maps to dictionaries return a tuple of outer and inner key indices.", "response": "def nested_key_indices(nested_dict):\n    \"\"\"\n    Give an ordering to the outer and inner keys used in a dictionary that\n    maps to dictionaries.\n    \"\"\"\n    outer_keys, inner_keys = collect_nested_keys(nested_dict)\n    outer_key_indices = {k: i for (i, k) in enumerate(outer_keys)}\n    inner_key_indices = {k: i for (i, k) in enumerate(inner_keys)}\n    return outer_key_indices, inner_key_indices"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncombining the outer and inner keys of nested dictionaries into a single ordering.", "response": "def flattened_nested_key_indices(nested_dict):\n    \"\"\"\n    Combine the outer and inner keys of nested dictionaries into a single\n    ordering.\n    \"\"\"\n    outer_keys, inner_keys = collect_nested_keys(nested_dict)\n    combined_keys = list(sorted(set(outer_keys + inner_keys)))\n    return {k: i for (i, k) in enumerate(combined_keys)}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef array_from_nested_dictionary(\n        nested_dict,\n        array_fn,\n        dtype=\"float32\",\n        square_result=False):\n    \"\"\"\n    Parameters\n    ----------\n    nested_dict : dict\n        Dictionary which contains dictionaries\n\n    array_fn : function\n        Takes shape and dtype as arguments, returns empty array.\n\n    dtype : dtype\n        NumPy dtype of result array\n\n    square_result : bool\n        Combine keys from outer and inner dictionaries.\n\n    Returns array and sorted lists of the outer and inner keys.\n    \"\"\"\n    if square_result:\n        outer_key_indices = inner_key_indices = flattened_nested_key_indices(\n            nested_dict)\n    else:\n        outer_key_indices, inner_key_indices = nested_key_indices(\n            nested_dict)\n\n    n_rows = len(outer_key_indices)\n    n_cols = len(inner_key_indices)\n    shape = (n_rows, n_cols)\n    result = array_fn(shape, dtype)\n    for outer_key, sub_dictionary in nested_dict.items():\n        i = outer_key_indices[outer_key]\n        for inner_key, value in sub_dictionary.items():\n            j = inner_key_indices[inner_key]\n            result[i, j] = value\n    outer_key_list = index_dict_to_sorted_list(outer_key_indices)\n    inner_key_list = index_dict_to_sorted_list(inner_key_indices)\n    return result, outer_key_list, inner_key_list", "response": "Returns an array of the elements of the nested dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a matrix of entries into a dictionary of key - value pairs.", "response": "def matrix_to_pair_dictionary(\n        X, row_keys=None, column_keys=None, filter_fn=None):\n    \"\"\"\n    X : numpy.ndarray\n\n    row_keys : dict\n        Dictionary mapping indices to row names. If omitted then maps each\n        number to its string representation, such as 1 -> \"1\".\n\n    column_keys : dict\n        If omitted and matrix is square, then use the same dictionary\n        as the rows. Otherwise map each column index to its string form.\n\n    filter_fn : function\n        If given then only add elements for which this function returns True.\n    \"\"\"\n    n_rows, n_cols = X.shape\n\n    if row_keys is None:\n        row_keys = {i: i for i in range(n_rows)}\n\n    if column_keys is None:\n        if n_rows == n_cols:\n            column_keys = row_keys\n        else:\n            column_keys = {j: j for j in range(n_cols)}\n\n    if len(row_keys) != n_rows:\n        raise ValueError(\"Need %d row keys but got list of length %d\" % (\n            n_rows,\n            len(row_keys)))\n\n    if len(column_keys) != n_cols:\n        raise ValueError(\"Need %d column keys but got list of length %d\" % (\n            n_cols,\n            len(column_keys)))\n\n    result_dict = {}\n    for i, X_i in enumerate(X):\n        row_key = row_keys[i]\n        for j, X_ij in enumerate(X_i):\n            if filter_fn and not filter_fn(X_ij):\n                continue\n            column_key = column_keys[j]\n            key_pair = (row_key, column_key)\n            result_dict[key_pair] = X_ij\n    return result_dict"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform dictionary from pairs of keys to dict -> float", "response": "def curry_pair_dictionary(key_pair_dict, default_value=0.0):\n    \"\"\"\n    Transform dictionary from pairs of keys to dict -> dict -> float\n    \"\"\"\n    result = defaultdict(dict)\n    for (a, b), value in key_pair_dict.items():\n        result[a][b] = value\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef uncurry_nested_dictionary(curried_dict):\n    result = {}\n    for a, a_dict in curried_dict.items():\n        for b, value in a_dict.items():\n            result[(a, b)] = value\n    return result", "response": "Transform a dictionary from a key - > value pair to a nested dict."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef array_from_pair_dictionary(\n        pair_dict,\n        array_fn,\n        dtype=\"float32\",\n        square_result=False):\n    \"\"\"\n    Convert a dictionary whose keys are pairs (k1, k2) into a sparse\n    or incomplete array.\n\n    Parameters\n    ----------\n    pair_dict : dict\n        Dictionary from pairs of keys to values.\n\n    array_fn : function\n        Takes shape and dtype as arguments, returns empty array.\n\n    dtype : dtype\n        NumPy dtype of result array\n\n    square_result : bool\n        Combine keys from rows and columns\n\n    Returns array and sorted lists of the row and column keys.\n    \"\"\"\n    row_key_set, column_key_set = pair_dict_key_sets(pair_dict)\n\n    if square_result:\n        combined_key_set = row_key_set.union(column_key_set)\n        row_key_list = column_key_list = list(sorted(combined_key_set))\n        row_key_indices = column_key_indices = {\n            k: i for (i, k) in enumerate(row_key_list)\n        }\n    else:\n        row_key_list = list(sorted(row_key_set))\n        column_key_list = list(sorted(column_key_set))\n        row_key_indices = {k: i for (i, k) in enumerate(row_key_list)}\n        column_key_indices = {k: i for (i, k) in enumerate(column_key_list)}\n\n    n_rows = len(row_key_indices)\n    n_cols = len(column_key_indices)\n    shape = (n_rows, n_cols)\n    result = array_fn(shape, dtype)\n    for (row_key, column_key), value in pair_dict.items():\n        i = row_key_indices[row_key]\n        j = column_key_indices[column_key]\n        result[i, j] = value\n    return result, row_key_list, column_key_list", "response": "Convert a dictionary whose keys are pairs k1 k2 into a sparse array."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transpose_nested_dictionary(nested_dict):\n    result = defaultdict(dict)\n    for k1, d in nested_dict.items():\n        for k2, v in d.items():\n            result[k2][k1] = v\n    return result", "response": "Given a nested dictionary from k1 > k2 transpose its outer and inner keys so it maps the keys k1 > k2 and k2 > value\n   "}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a reverse - lookup dictionary mapping each row key to a list of triplets.", "response": "def reverse_lookup_from_nested_dict(values_dict):\n    \"\"\"\n    Create reverse-lookup dictionary mapping each row key to a list of triplets:\n    [(column key, value), ...]\n\n    Parameters\n    ----------\n    nested_values_dict : dict\n        column_key -> row_key -> value\n\n    weights_dict : dict\n        column_key -> row_key -> sample weight\n\n    Returns dictionary mapping row_key -> [(column key, value)]\n    \"\"\"\n    reverse_lookup = defaultdict(list)\n    for column_key, column_dict in values_dict.items():\n        for row_key, value in column_dict.items():\n            entry = (column_key, value)\n            reverse_lookup[row_key].append(entry)\n    return reverse_lookup"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _cells(nb):\n    if nb.nbformat < 4:\n        for ws in nb.worksheets:\n            for cell in ws.cells:\n                yield cell\n    else:\n        for cell in nb.cells:\n            yield cell", "response": "Yield all cells in an nbformat - insensitive manner"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nstrips the outputs from a notebook object", "response": "def strip_output(nb):\n    \"\"\"strip the outputs from a notebook object\"\"\"\n    nb.metadata.pop('signature', None)\n    nb.metadata.pop('widgets', None)\n    for cell in _cells(nb):\n        if 'outputs' in cell:\n            cell['outputs'] = []\n        if 'prompt_number' in cell:\n            cell['prompt_number'] = None\n    return nb"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenforces min <= value <= max", "response": "def _validate_min(self, proposal):\n        \"\"\"Enforce min <= value <= max\"\"\"\n        min = proposal['value']\n        if min > self.max:\n            raise TraitError('Setting min > max')\n        if min > self.value:\n            self.value = min\n        return min"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _validate_max(self, proposal):\n        max = proposal['value']\n        if max < self.min:\n            raise TraitError('setting max < min')\n        if max < self.value:\n            self.value = max\n        return max", "response": "Enforce min <= value <= max"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _validate_value(self, proposal):\n        value = proposal['value']\n        if self.base ** self.min > value or self.base ** self.max < value:\n            value = min(max(value, self.base **  self.min), self.base **  self.max)\n        return value", "response": "Validate the value of the attribute."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_inline_matplotlib_plots():\n    if 'matplotlib' not in sys.modules:\n        # matplotlib hasn't been imported, nothing to do.\n        return\n\n    try:\n        import matplotlib as mpl\n        from ipykernel.pylab.backend_inline import flush_figures\n    except ImportError:\n        return\n\n    if mpl.get_backend() == 'module://ipykernel.pylab.backend_inline':\n        flush_figures()", "response": "Show matplotlib plots immediately if using the inline backend."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef interactive_output(f, controls):\n\n    out = Output()\n    def observer(change):\n        kwargs = {k:v.value for k,v in controls.items()}\n        show_inline_matplotlib_plots()\n        with out:\n            clear_output(wait=True)\n            f(**kwargs)\n            show_inline_matplotlib_plots()\n    for k,w in controls.items():\n        w.observe(observer, 'value')\n    show_inline_matplotlib_plots()\n    observer(None)\n    return out", "response": "Connect widget controls to a function and generate a user interface for the widgets."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmatches a pattern of types in a sequence.", "response": "def _matches(o, pattern):\n    \"\"\"Match a pattern of types in a sequence.\"\"\"\n    if not len(o) == len(pattern):\n        return False\n    comps = zip(o,pattern)\n    return all(isinstance(obj,kind) for obj,kind in comps)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_min_max_value(min, max, value=None, step=None):\n    # Either min and max need to be given, or value needs to be given\n    if value is None:\n        if min is None or max is None:\n            raise ValueError('unable to infer range, value from: ({0}, {1}, {2})'.format(min, max, value))\n        diff = max - min\n        value = min + (diff / 2)\n        # Ensure that value has the same type as diff\n        if not isinstance(value, type(diff)):\n            value = min + (diff // 2)\n    else:  # value is not None\n        if not isinstance(value, Real):\n            raise TypeError('expected a real number, got: %r' % value)\n        # Infer min/max from value\n        if value == 0:\n            # This gives (0, 1) of the correct type\n            vrange = (value, value + 1)\n        elif value > 0:\n            vrange = (-value, 3*value)\n        else:\n            vrange = (3*value, -value)\n        if min is None:\n            min = vrange[0]\n        if max is None:\n            max = vrange[1]\n    if step is not None:\n        # ensure value is on a step\n        tick = int((value - min) / step)\n        value = min + tick * step\n    if not min <= value <= max:\n        raise ValueError('value must be between min and max (min={0}, value={1}, max={2})'.format(min, value, max))\n    return min, max, value", "response": "Return min max value given input values with possible None."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields the names value and default values for a function parameter.", "response": "def _yield_abbreviations_for_parameter(param, kwargs):\n    \"\"\"Get an abbreviation for a function parameter.\"\"\"\n    name = param.name\n    kind = param.kind\n    ann = param.annotation\n    default = param.default\n    not_found = (name, empty, empty)\n    if kind in (Parameter.POSITIONAL_OR_KEYWORD, Parameter.KEYWORD_ONLY):\n        if name in kwargs:\n            value = kwargs.pop(name)\n        elif ann is not empty:\n            warn(\"Using function annotations to implicitly specify interactive controls is deprecated. Use an explicit keyword argument for the parameter instead.\", DeprecationWarning)\n            value = ann\n        elif default is not empty:\n            value = default\n        else:\n            yield not_found\n        yield (name, value, default)\n    elif kind == Parameter.VAR_KEYWORD:\n        # In this case name=kwargs and we yield the items in kwargs with their keys.\n        for k, v in kwargs.copy().items():\n            kwargs.pop(k)\n            yield k, v, empty"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupdate the output widget with the result of the function call.", "response": "def update(self, *args):\n        \"\"\"\n        Call the interact function and update the output widget with\n        the result of the function call.\n\n        Parameters\n        ----------\n        *args : ignored\n            Required for this method to be used as traitlets callback.\n        \"\"\"\n        self.kwargs = {}\n        if self.manual:\n            self.manual_button.disabled = True\n        try:\n            show_inline_matplotlib_plots()\n            with self.out:\n                if self.clear_output:\n                    clear_output(wait=True)\n                for widget in self.kwargs_widgets:\n                    value = widget.get_interact_value()\n                    self.kwargs[widget._kwarg] = value\n                self.result = self.f(**self.kwargs)\n                show_inline_matplotlib_plots()\n                if self.auto_display and self.result is not None:\n                    display(self.result)\n        except Exception as e:\n            ip = get_ipython()\n            if ip is None:\n                self.log.warn(\"Exception in interact callback: %s\", e, exc_info=True)\n            else:\n                ip.showtraceback()\n        finally:\n            if self.manual:\n                self.manual_button.disabled = False"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_abbreviations(self, kwargs):\n        new_kwargs = []\n        try:\n            sig = self.signature()\n        except (ValueError, TypeError):\n            # can't inspect, no info from function; only use kwargs\n            return [ (key, value, value) for key, value in kwargs.items() ]\n\n        for param in sig.parameters.values():\n            for name, value, default in _yield_abbreviations_for_parameter(param, kwargs):\n                if value is empty:\n                    raise ValueError('cannot find widget or abbreviation for argument: {!r}'.format(name))\n                new_kwargs.append((name, value, default))\n        return new_kwargs", "response": "Find the abbreviations for the given function and kwargs."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ngive a sequence of ( name abbrev default ) tuples return a sequence of Widgets.", "response": "def widgets_from_abbreviations(self, seq):\n        \"\"\"Given a sequence of (name, abbrev, default) tuples, return a sequence of Widgets.\"\"\"\n        result = []\n        for name, abbrev, default in seq:\n            widget = self.widget_from_abbrev(abbrev, default)\n            if not (isinstance(widget, ValueWidget) or isinstance(widget, fixed)):\n                if widget is None:\n                    raise ValueError(\"{!r} cannot be transformed to a widget\".format(abbrev))\n                else:\n                    raise TypeError(\"{!r} is not a ValueWidget\".format(widget))\n            if not widget.description:\n                widget.description = name\n            widget._kwarg = name\n            result.append(widget)\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef widget_from_abbrev(cls, abbrev, default=empty):\n        if isinstance(abbrev, ValueWidget) or isinstance(abbrev, fixed):\n            return abbrev\n\n        if isinstance(abbrev, tuple):\n            widget = cls.widget_from_tuple(abbrev)\n            if default is not empty:\n                try:\n                    widget.value = default\n                except Exception:\n                    # ignore failure to set default\n                    pass\n            return widget\n\n        # Try single value\n        widget = cls.widget_from_single_value(abbrev)\n        if widget is not None:\n            return widget\n\n        # Something iterable (list, dict, generator, ...). Note that str and\n        # tuple should be handled before, that is why we check this case last.\n        if isinstance(abbrev, Iterable):\n            widget = cls.widget_from_iterable(abbrev)\n            if default is not empty:\n                try:\n                    widget.value = default\n                except Exception:\n                    # ignore failure to set default\n                    pass\n            return widget\n\n        # No idea...\n        return None", "response": "Build a ValueWidget instance given an abbreviation or Widget."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nmaking widgets from single values which can be used as parameter defaults.", "response": "def widget_from_single_value(o):\n        \"\"\"Make widgets from single values, which can be used as parameter defaults.\"\"\"\n        if isinstance(o, string_types):\n            return Text(value=unicode_type(o))\n        elif isinstance(o, bool):\n            return Checkbox(value=o)\n        elif isinstance(o, Integral):\n            min, max, value = _get_min_max_value(None, None, o)\n            return IntSlider(value=o, min=min, max=max)\n        elif isinstance(o, Real):\n            min, max, value = _get_min_max_value(None, None, o)\n            return FloatSlider(value=o, min=min, max=max)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake widgets from a tuple abbreviation.", "response": "def widget_from_tuple(o):\n        \"\"\"Make widgets from a tuple abbreviation.\"\"\"\n        if _matches(o, (Real, Real)):\n            min, max, value = _get_min_max_value(o[0], o[1])\n            if all(isinstance(_, Integral) for _ in o):\n                cls = IntSlider\n            else:\n                cls = FloatSlider\n            return cls(value=value, min=min, max=max)\n        elif _matches(o, (Real, Real, Real)):\n            step = o[2]\n            if step <= 0:\n                raise ValueError(\"step must be >= 0, not %r\" % step)\n            min, max, value = _get_min_max_value(o[0], o[1], step=step)\n            if all(isinstance(_, Integral) for _ in o):\n                cls = IntSlider\n            else:\n                cls = FloatSlider\n            return cls(value=value, min=min, max=max, step=step)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nmake widgets from an iterable. This should not be done for a string or tuple.", "response": "def widget_from_iterable(o):\n        \"\"\"Make widgets from an iterable. This should not be done for\n        a string or tuple.\"\"\"\n        # Dropdown expects a dict or list, so we convert an arbitrary\n        # iterable to either of those.\n        if isinstance(o, (list, dict)):\n            return Dropdown(options=o)\n        elif isinstance(o, Mapping):\n            return Dropdown(options=list(o.items()))\n        else:\n            return Dropdown(options=list(o))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an interactive function widget for the given function.", "response": "def widget(self, f):\n        \"\"\"\n        Return an interactive function widget for the given function.\n\n        The widget is only constructed, not displayed nor attached to\n        the function.\n\n        Returns\n        -------\n        An instance of ``self.cls`` (typically :class:`interactive`).\n\n        Parameters\n        ----------\n        f : function\n            The function to which the interactive widgets are tied.\n        \"\"\"\n        return self.cls(f, self.opts, **self.kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new instance of _InteractFactory which will apply the options for interactive functions.", "response": "def options(self, **kwds):\n        \"\"\"\n        Change options for interactive functions.\n\n        Returns\n        -------\n        A new :class:`_InteractFactory` which will apply the\n        options when called.\n        \"\"\"\n        opts = dict(self.opts)\n        for k in kwds:\n            try:\n                # Ensure that the key exists because we want to change\n                # existing options, not add new ones.\n                _ = opts[k]\n            except KeyError:\n                raise ValueError(\"invalid option {!r}\".format(k))\n            opts[k] = kwds[k]\n        return type(self)(self.cls, opts, self.kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsetting the title of a container page.", "response": "def set_title(self, index, title):\n        \"\"\"Sets the title of a container page.\n\n        Parameters\n        ----------\n        index : int\n            Index of the container page\n        title : unicode\n            New title\n        \"\"\"\n        # JSON dictionaries have string keys, so we convert index to a string\n        index = unicode_type(int(index))\n        self._titles[index] = title\n        self.send_state('_titles')"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_title(self, index):\n        # JSON dictionaries have string keys, so we convert index to a string\n        index = unicode_type(int(index))\n        if index in self._titles:\n            return self._titles[index]\n        else:\n            return None", "response": "Gets the title of a container page."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef bash(filename):\n    sys.stdout.flush()\n    subprocess.call(\"bash {}\".format(filename), shell=True)", "response": "Runs a bash script in the local directory"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nput the buffers into the state dict.", "response": "def _put_buffers(state, buffer_paths, buffers):\n    \"\"\"The inverse of _remove_buffers, except here we modify the existing dict/lists.\n    Modifying should be fine, since this is used when state comes from the wire.\n    \"\"\"\n    for buffer_path, buffer in zip(buffer_paths, buffers):\n        # we'd like to set say sync_data['x'][0]['y'] = buffer\n        # where buffer_path in this example would be ['x', 0, 'y']\n        obj = state\n        for key in buffer_path[:-1]:\n            obj = obj[key]\n        obj[buffer_path[-1]] = buffer"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _remove_buffers(state):\n    buffer_paths, buffers = [], []\n    state = _separate_buffers(state, [], buffer_paths, buffers)\n    return state, buffer_paths, buffers", "response": "Return a state with buffers removed for binary message parts\n   "}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _buffer_list_equal(a, b):\n    if len(a) != len(b):\n        return False\n    if a == b:\n        return True\n    for ia, ib in zip(a, b):\n        # Check byte equality, since bytes are what is actually synced\n        # NOTE: Simple ia != ib does not always work as intended, as\n        # e.g. memoryview(np.frombuffer(ia, dtype='float32')) !=\n        # memoryview(np.frombuffer(b)), since the format info differs.\n        if PY3:\n            # compare without copying\n            if memoryview(ia).cast('B') != memoryview(ib).cast('B'):\n                return False\n        else:\n            # python 2 doesn't have memoryview.cast, so we may have to copy\n            if isinstance(ia, memoryview) and ia.format != 'B':\n                ia = ia.tobytes()\n            if isinstance(ib, memoryview) and ib.format != 'B':\n                ib = ib.tobytes()\n            if ia != ib:\n                return False\n    return True", "response": "Compare two lists of buffers for equality."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef register(name=''):\n    \"For backwards compatibility, we support @register(name) syntax.\"\n    def reg(widget):\n        \"\"\"A decorator registering a widget class in the widget registry.\"\"\"\n        w = widget.class_traits()\n        Widget.widget_types.register(w['_model_module'].default_value,\n                                    w['_model_module_version'].default_value,\n                                    w['_model_name'].default_value,\n                                    w['_view_module'].default_value,\n                                    w['_view_module_version'].default_value,\n                                    w['_view_name'].default_value,\n                                    widget)\n        return widget\n    if isinstance(name, string_types):\n        import warnings\n        warnings.warn(\"Widget registration using a string name has been deprecated. Widget registration now uses a plain `@register` decorator.\", DeprecationWarning)\n        return reg\n    else:\n        return reg(name)", "response": "For backwards compatibility we support @register ( name ) syntax."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngets a value from the registry", "response": "def get(self, model_module, model_module_version, model_name, view_module, view_module_version, view_name):\n        \"\"\"Get a value\"\"\"\n        module_versions = self._registry[model_module]\n        # The python semver module doesn't work well, for example, it can't do match('3', '*')\n        # so we just take the first model module version.\n        #model_names = next(v for k, v in module_versions.items()\n        #                   if semver.match(model_module_version, k))\n        model_names = list(module_versions.values())[0]\n        view_modules = model_names[model_name]\n        view_versions = view_modules[view_module]\n        # The python semver module doesn't work well, so we just take the first view module version\n        #view_names = next(v for k, v in view_versions.items()\n        #                  if semver.match(view_module_version, k))\n        view_names = list(view_versions.values())[0]\n        widget_class = view_names[view_name]\n        return widget_class"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _call_widget_constructed(widget):\n        if Widget._widget_construction_callback is not None and callable(Widget._widget_construction_callback):\n            Widget._widget_construction_callback(widget)", "response": "Static method called when a widget is constructed."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef handle_comm_opened(comm, msg):\n        version = msg.get('metadata', {}).get('version', '')\n        if version.split('.')[0] != PROTOCOL_VERSION_MAJOR:\n            raise ValueError(\"Incompatible widget protocol versions: received version %r, expected version %r\"%(version, __protocol_version__))\n        data = msg['content']['data']\n        state = data['state']\n\n        # Find the widget class to instantiate in the registered widgets\n        widget_class = Widget.widget_types.get(state['_model_module'],\n                                               state['_model_module_version'],\n                                               state['_model_name'],\n                                               state['_view_module'],\n                                               state['_view_module_version'],\n                                               state['_view_name'])\n        widget = widget_class(comm=comm)\n        if 'buffer_paths' in data:\n            _put_buffers(state, data['buffer_paths'], msg['buffers'])\n        widget.set_state(state)", "response": "Static method called when a widget is constructed."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_manager_state(drop_defaults=False, widgets=None):\n        state = {}\n        if widgets is None:\n            widgets = Widget.widgets.values()\n        for widget in widgets:\n            state[widget.model_id] = widget._get_embed_state(drop_defaults=drop_defaults)\n        return {'version_major': 2, 'version_minor': 0, 'state': state}", "response": "Returns the full state of a widget manager for embedding."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef open(self):\n        if self.comm is None:\n            state, buffer_paths, buffers = _remove_buffers(self.get_state())\n\n            args = dict(target_name='jupyter.widget',\n                        data={'state': state, 'buffer_paths': buffer_paths},\n                        buffers=buffers,\n                        metadata={'version': __protocol_version__}\n                        )\n            if self._model_id is not None:\n                args['comm_id'] = self._model_id\n\n            self.comm = Comm(**args)", "response": "Open a comm to the frontend if one isn t already open."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when the comm is changed.", "response": "def _comm_changed(self, change):\n        \"\"\"Called when the comm is changed.\"\"\"\n        if change['new'] is None:\n            return\n        self._model_id = self.model_id\n\n        self.comm.on_msg(self._handle_msg)\n        Widget.widgets[self.model_id] = self"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nclosing the underlying comm.", "response": "def close(self):\n        \"\"\"Close method.\n\n        Closes the underlying comm.\n        When the comm is closed, all of the widget views are automatically\n        removed from the front-end.\"\"\"\n        if self.comm is not None:\n            Widget.widgets.pop(self.model_id, None)\n            self.comm.close()\n            self.comm = None\n            self._ipython_display_ = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef send_state(self, key=None):\n        state = self.get_state(key=key)\n        if len(state) > 0:\n            if self._property_lock:  # we need to keep this dict up to date with the front-end values\n                for name, value in state.items():\n                    if name in self._property_lock:\n                        self._property_lock[name] = value\n            state, buffer_paths, buffers = _remove_buffers(state)\n            msg = {'method': 'update', 'state': state, 'buffer_paths': buffer_paths}\n            self._send(msg, buffers=buffers)", "response": "Sends the widget state or a piece of it to the front - end if it exists."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the widget state or a piece of it.", "response": "def get_state(self, key=None, drop_defaults=False):\n        \"\"\"Gets the widget state, or a piece of it.\n\n        Parameters\n        ----------\n        key : unicode or iterable (optional)\n            A single property's name or iterable of property names to get.\n\n        Returns\n        -------\n        state : dict of states\n        metadata : dict\n            metadata for each field: {key: metadata}\n        \"\"\"\n        if key is None:\n            keys = self.keys\n        elif isinstance(key, string_types):\n            keys = [key]\n        elif isinstance(key, collections.Iterable):\n            keys = key\n        else:\n            raise ValueError(\"key must be a string, an iterable of keys, or None\")\n        state = {}\n        traits = self.traits()\n        for k in keys:\n            to_json = self.trait_metadata(k, 'to_json', self._trait_to_json)\n            value = to_json(getattr(self, k), self)\n            if not PY3 and isinstance(traits[k], Bytes) and isinstance(value, bytes):\n                value = memoryview(value)\n            if not drop_defaults or not self._compare(value, traits[k].default_value):\n                state[k] = value\n        return state"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_state(self, sync_data):\n        # The order of these context managers is important. Properties must\n        # be locked when the hold_trait_notification context manager is\n        # released and notifications are fired.\n        with self._lock_property(**sync_data), self.hold_trait_notifications():\n            for name in sync_data:\n                if name in self.keys:\n                    from_json = self.trait_metadata(name, 'from_json',\n                                                    self._trait_from_json)\n                    self.set_trait(name, from_json(sync_data[name], self))", "response": "Called when a state is received from the front - end."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_msg(self, callback, remove=False):\n        self._msg_callbacks.register_callback(callback, remove=remove)", "response": "Register a custom msg receive callback."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nregister a callback to be called when a specific item is displayed.", "response": "def on_displayed(self, callback, remove=False):\n        \"\"\"(Un)Register a widget displayed callback.\n\n        Parameters\n        ----------\n        callback: method handler\n            Must have a signature of::\n\n                callback(widget, **kwargs)\n\n            kwargs from display are passed through without modification.\n        remove: bool\n            True if the callback should be unregistered.\"\"\"\n        self._display_callbacks.register_callback(callback, remove=remove)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when a property has changed.", "response": "def notify_change(self, change):\n        \"\"\"Called when a property has changed.\"\"\"\n        # Send the state to the frontend before the user-registered callbacks\n        # are called.\n        name = change['name']\n        if self.comm is not None and self.comm.kernel is not None:\n            # Make sure this isn't information that the front-end just sent us.\n            if name in self.keys and self._should_send_property(name, getattr(self, name)):\n                # Send new state to front-end\n                self.send_state(key=name)\n        super(Widget, self).notify_change(change)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef hold_sync(self):\n        if self._holding_sync is True:\n            yield\n        else:\n            try:\n                self._holding_sync = True\n                yield\n            finally:\n                self._holding_sync = False\n                self.send_state(self._states_to_send)\n                self._states_to_send.clear()", "response": "Hold syncing any state until the outermost context manager exits"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _should_send_property(self, key, value):\n        to_json = self.trait_metadata(key, 'to_json', self._trait_to_json)\n        if key in self._property_lock:\n            # model_state, buffer_paths, buffers\n            split_value = _remove_buffers({ key: to_json(value, self)})\n            split_lock = _remove_buffers({ key: self._property_lock[key]})\n            # A roundtrip conversion through json in the comparison takes care of\n            # idiosyncracies of how python data structures map to json, for example\n            # tuples get converted to lists.\n            if (jsonloads(jsondumps(split_value[0])) == split_lock[0]\n                and split_value[1] == split_lock[1]\n                and _buffer_list_equal(split_value[2], split_lock[2])):\n                return False\n        if self._holding_sync:\n            self._states_to_send.add(key)\n            return False\n        else:\n            return True", "response": "Check if the given key is in the property lock and if so send it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _handle_msg(self, msg):\n        data = msg['content']['data']\n        method = data['method']\n\n        if method == 'update':\n            if 'state' in data:\n                state = data['state']\n                if 'buffer_paths' in data:\n                    _put_buffers(state, data['buffer_paths'], msg['buffers'])\n                self.set_state(state)\n\n        # Handle a state request.\n        elif method == 'request_state':\n            self.send_state()\n\n        # Handle a custom msg from the front-end.\n        elif method == 'custom':\n            if 'content' in data:\n                self._handle_custom_msg(data['content'], msg['buffers'])\n\n        # Catch remainder.\n        else:\n            self.log.error('Unknown front-end to back-end widget msg with method \"%s\"' % method)", "response": "Called when a message is received from the front - end"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall when IPython. display. display is called on the widget.", "response": "def _ipython_display_(self, **kwargs):\n        \"\"\"Called when `IPython.display.display` is called on the widget.\"\"\"\n\n        plaintext = repr(self)\n        if len(plaintext) > 110:\n            plaintext = plaintext[:110] + '\u2026'\n        data = {\n            'text/plain': plaintext,\n        }\n        if self._view_name is not None:\n            # The 'application/vnd.jupyter.widget-view+json' mimetype has not been registered yet.\n            # See the registration process and naming convention at\n            # http://tools.ietf.org/html/rfc6838\n            # and the currently registered mimetypes at\n            # http://www.iana.org/assignments/media-types/media-types.xhtml.\n            data['application/vnd.jupyter.widget-view+json'] = {\n                'version_major': 2,\n                'version_minor': 0,\n                'model_id': self._model_id\n            }\n        display(data, raw=True)\n\n        if self._view_name is not None:\n            self._handle_displayed(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsends a message to the model in the front - end.", "response": "def _send(self, msg, buffers=None):\n        \"\"\"Sends a message to the model in the front-end.\"\"\"\n        if self.comm is not None and self.comm.kernel is not None:\n            self.comm.send(data=msg, buffers=buffers)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _int_doc(cls):\n    def __init__(self, value=None, **kwargs):\n        if value is not None:\n            kwargs['value'] = value\n        super(cls, self).__init__(**kwargs)\n\n    __init__.__doc__ = _int_doc_t\n    cls.__init__ = __init__\n    return cls", "response": "Add int docstring template to class init."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _bounded_int_doc(cls):\n    def __init__(self, value=None, min=None, max=None, step=None, **kwargs):\n        if value is not None:\n            kwargs['value'] = value\n        if min is not None:\n            kwargs['min'] = min\n        if max is not None:\n            kwargs['max'] = max\n        if step is not None:\n            kwargs['step'] = step\n        super(cls, self).__init__(**kwargs)\n\n    __init__.__doc__ = _bounded_int_doc_t\n    cls.__init__ = __init__\n    return cls", "response": "Add bounded int docstring template to class init."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nserialize a Python datetime object to json.", "response": "def datetime_to_json(pydt, manager):\n    \"\"\"Serialize a Python datetime object to json.\n\n    Instantiating a JavaScript Date object with a string assumes that the\n    string is a UTC string, while instantiating it with constructor arguments\n    assumes that it's in local time:\n\n    >>> cdate = new Date('2015-05-12')\n    Mon May 11 2015 20:00:00 GMT-0400 (Eastern Daylight Time)\n    >>> cdate = new Date(2015, 4, 12) // Months are 0-based indices in JS\n    Tue May 12 2015 00:00:00 GMT-0400 (Eastern Daylight Time)\n\n    Attributes of this dictionary are to be passed to the JavaScript Date\n    constructor.\n    \"\"\"\n    if pydt is None:\n        return None\n    else:\n        return dict(\n            year=pydt.year,\n            month=pydt.month - 1,  # Months are 0-based indices in JS\n            date=pydt.day,\n            hours=pydt.hour,       # Hours, Minutes, Seconds and Milliseconds\n            minutes=pydt.minute,   # are plural in JS\n            seconds=pydt.second,\n            milliseconds=pydt.microsecond / 1000\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef date_to_json(pydate, manager):\n    if pydate is None:\n        return None\n    else:\n        return dict(\n            year=pydate.year,\n            month=pydate.month - 1,  # Months are 0-based indices in JS\n            date=pydate.day\n        )", "response": "Serialize a Python date object to a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _find_widget_refs_by_state(widget, state):\n    # Copy keys to allow changes to state during iteration:\n    keys = tuple(state.keys())\n    for key in keys:\n        value = getattr(widget, key)\n        # Trivial case: Direct references to other widgets:\n        if isinstance(value, Widget):\n            yield value\n        # Also check for buried references in known, JSON-able structures\n        # Note: This might miss references buried in more esoteric structures\n        elif isinstance(value, (list, tuple)):\n            for item in value:\n                if isinstance(item, Widget):\n                    yield item\n        elif isinstance(value, dict):\n            for item in value.values():\n                if isinstance(item, Widget):\n                    yield item", "response": "Find references to other widgets in a widget s state"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the embed state of a widget and all other widgets it refers to as well", "response": "def _get_recursive_state(widget, store=None, drop_defaults=False):\n    \"\"\"Gets the embed state of a widget, and all other widgets it refers to as well\"\"\"\n    if store is None:\n        store = dict()\n    state = widget._get_embed_state(drop_defaults=drop_defaults)\n    store[widget.model_id] = state\n\n    # Loop over all values included in state (i.e. don't consider excluded values):\n    for ref in _find_widget_refs_by_state(widget, state['state']):\n        if ref.model_id not in store:\n            _get_recursive_state(ref, store, drop_defaults=drop_defaults)\n    return store"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_resolved_links(store, drop_defaults):\n    for widget_id, widget in Widget.widgets.items(): # go over all widgets\n        if isinstance(widget, Link) and widget_id not in store:\n            if widget.source[0].model_id in store and widget.target[0].model_id in store:\n                store[widget.model_id] = widget._get_embed_state(drop_defaults=drop_defaults)", "response": "Adds the state of any link models between two models in store"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting the state of all widgets and their dependencies.", "response": "def dependency_state(widgets, drop_defaults=True):\n    \"\"\"Get the state of all widgets specified, and their dependencies.\n\n    This uses a simple dependency finder, including:\n     - any widget directly referenced in the state of an included widget\n     - any widget in a list/tuple attribute in the state of an included widget\n     - any widget in a dict attribute in the state of an included widget\n     - any jslink/jsdlink between two included widgets\n    What this alogrithm does not do:\n     - Find widget references in nested list/dict structures\n     - Find widget references in other types of attributes\n\n    Note that this searches the state of the widgets for references, so if\n    a widget reference is not included in the serialized state, it won't\n    be considered as a dependency.\n\n    Parameters\n    ----------\n    widgets: single widget or list of widgets.\n       This function will return the state of every widget mentioned\n       and of all their dependencies.\n    drop_defaults: boolean\n        Whether to drop default values from the widget states.\n\n    Returns\n    -------\n    A dictionary with the state of the widgets and any widget they\n    depend on.\n    \"\"\"\n    # collect the state of all relevant widgets\n    if widgets is None:\n        # Get state of all widgets, no smart resolution needed.\n        state = Widget.get_manager_state(drop_defaults=drop_defaults, widgets=None)['state']\n    else:\n        try:\n            widgets[0]\n        except (IndexError, TypeError):\n            widgets = [widgets]\n        state = {}\n        for widget in widgets:\n            _get_recursive_state(widget, state, drop_defaults)\n        # Add any links between included widgets:\n        add_resolved_links(state, drop_defaults)\n    return state"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the raw data for embedding a list of views.", "response": "def embed_data(views, drop_defaults=True, state=None):\n    \"\"\"Gets data for embedding.\n\n    Use this to get the raw data for embedding if you have special\n    formatting needs.\n\n    Parameters\n    ----------\n    {views_attribute}\n    drop_defaults: boolean\n        Whether to drop default values from the widget states.\n    state: dict or None (default)\n        The state to include. When set to None, the state of all widgets\n        know to the widget manager is included. Otherwise it uses the\n        passed state directly. This allows for end users to include a\n        smaller state, under the responsibility that this state is\n        sufficient to reconstruct the embedded views.\n\n    Returns\n    -------\n    A dictionary with the following entries:\n        manager_state: dict of the widget manager state data\n        view_specs: a list of widget view specs\n    \"\"\"\n    if views is None:\n        views = [w for w in Widget.widgets.values() if isinstance(w, DOMWidget)]\n    else:\n        try:\n            views[0]\n        except (IndexError, TypeError):\n            views = [views]\n\n    if state is None:\n        # Get state of all known widgets\n        state = Widget.get_manager_state(drop_defaults=drop_defaults, widgets=None)['state']\n\n    # Rely on ipywidget to get the default values\n    json_data = Widget.get_manager_state(widgets=[])\n    # but plug in our own state\n    json_data['state'] = state\n\n    view_specs = [w.get_view_spec() for w in views]\n\n    return dict(manager_state=json_data, view_specs=view_specs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef embed_snippet(views,\n                  drop_defaults=True,\n                  state=None,\n                  indent=2,\n                  embed_url=None,\n                  requirejs=True,\n                  cors=True\n                 ):\n    \"\"\"Return a snippet that can be embedded in an HTML file.\n\n    Parameters\n    ----------\n    {views_attribute}\n    {embed_kwargs}\n\n    Returns\n    -------\n    A unicode string with an HTML snippet containing several `<script>` tags.\n    \"\"\"\n\n    data = embed_data(views, drop_defaults=drop_defaults, state=state)\n\n    widget_views = u'\\n'.join(\n        widget_view_template.format(view_spec=escape_script(json.dumps(view_spec)))\n        for view_spec in data['view_specs']\n    )\n\n    if embed_url is None:\n        embed_url = DEFAULT_EMBED_REQUIREJS_URL if requirejs else DEFAULT_EMBED_SCRIPT_URL\n\n    load = load_requirejs_template if requirejs else load_template\n\n    use_cors = ' crossorigin=\"anonymous\"' if cors else ''\n    values = {\n        'load': load.format(embed_url=embed_url, use_cors=use_cors),\n        'json_data': escape_script(json.dumps(data['manager_state'], indent=indent)),\n        'widget_views': widget_views,\n    }\n\n    return snippet_template.format(**values)", "response": "Returns a unicode string that can be embedded in an HTML file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef embed_minimal_html(fp, views, title=u'IPyWidget export', template=None, **kwargs):\n    snippet = embed_snippet(views, **kwargs)\n\n    values = {\n        'title': title,\n        'snippet': snippet,\n    }\n    if template is None:\n        template = html_template\n\n    html_code = template.format(**values)\n\n    # Check if fp is writable:\n    if hasattr(fp, 'write'):\n        fp.write(html_code)\n    else:\n        # Assume fp is a filename:\n        with open(fp, \"w\") as f:\n            f.write(html_code)", "response": "Write a minimal HTML file with widget views embedded."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nstandardizing the options tuple format.", "response": "def _make_options(x):\n    \"\"\"Standardize the options tuple format.\n\n    The returned tuple should be in the format (('label', value), ('label', value), ...).\n\n    The input can be\n    * an iterable of (label, value) pairs\n    * an iterable of values, and labels will be generated\n    \"\"\"\n    # Check if x is a mapping of labels to values\n    if isinstance(x, Mapping):\n        import warnings\n        warnings.warn(\"Support for mapping types has been deprecated and will be dropped in a future release.\", DeprecationWarning)\n        return tuple((unicode_type(k), v) for k, v in x.items())\n\n    # only iterate once through the options.\n    xlist = tuple(x)\n\n    # Check if x is an iterable of (label, value) pairs\n    if all((isinstance(i, (list, tuple)) and len(i) == 2) for i in xlist):\n        return tuple((unicode_type(k), v) for k, v in xlist)\n\n    # Otherwise, assume x is an iterable of values\n    return tuple((unicode_type(i), i) for i in xlist)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nset the values and labels and select the first option if we aren t initializing", "response": "def _propagate_options(self, change):\n        \"Set the values and labels, and select the first option if we aren't initializing\"\n        options = self._options_full\n        self.set_trait('_options_labels', tuple(i[0] for i in options))\n        self._options_values = tuple(i[1] for i in options)\n        if self._initializing_traits_ is not True:\n            if len(options) > 0:\n                if self.index == 0:\n                    # Explicitly trigger the observers to pick up the new value and\n                    # label. Just setting the value would not trigger the observers\n                    # since traitlets thinks the value hasn't changed.\n                    self._notify_trait('index', 0, 0)\n                else:\n                    self.index = 0\n            else:\n                self.index = None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _propagate_index(self, change):\n        \"Propagate changes in index to the value and label properties\"\n        label = self._options_labels[change.new] if change.new is not None else None\n        value = self._options_values[change.new] if change.new is not None else None\n        if self.label is not label:\n            self.label = label\n        if self.value is not value:\n            self.value = value", "response": "Propagate changes in index to the value and label properties"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the range of each proposed index.", "response": "def _validate_index(self, proposal):\n        \"Check the range of each proposed index.\"\n        if all(0 <= i < len(self._options_labels) for i in proposal.value):\n            return proposal.value\n        else:\n            raise TraitError('Invalid selection: index out of bounds')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _validate_value(self, proposal):\n        \"Replace all values with the actual objects in the options list\"\n        try:\n            return tuple(findvalue(self._options_values, i, self.equals) for i in proposal.value)\n        except ValueError:\n            raise TraitError('Invalid selection: value not found')", "response": "Replace all values with the actual objects in the options list"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a class to the top level element of the widget.", "response": "def add_class(self, className):\n        \"\"\"\n        Adds a class to the top level element of the widget.\n\n        Doesn't add the class if it already exists.\n        \"\"\"\n        if className not in self._dom_classes:\n            self._dom_classes = list(self._dom_classes) + [className]\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef remove_class(self, className):\n        if className in self._dom_classes:\n            self._dom_classes = [c for c in self._dom_classes if c != className]\n        return self", "response": "Removes a class from the top level element of the widget."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nregistering the jupyter. widget comm target", "response": "def register_comm_target(kernel=None):\n    \"\"\"Register the jupyter.widget comm target\"\"\"\n    if kernel is None:\n        kernel = get_ipython().kernel\n    kernel.comm_manager.register_target('jupyter.widget', Widget.handle_comm_opened)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsubstituting format strings in class or function docstring", "response": "def doc_subst(snippets):\n    \"\"\" Substitute format strings in class or function docstring \"\"\"\n    def decorator(cls):\n        # Strip the snippets to avoid trailing new lines and whitespace\n        stripped_snippets = {\n            key: snippet.strip() for (key, snippet) in snippets.items()\n        }\n        cls.__doc__ = cls.__doc__.format(**stripped_snippets)\n        return cls\n    return decorator"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget absolute path to resource.", "response": "def base_uri(relative_path=''):\n    \"\"\" Get absolute path to resource, works for dev and for PyInstaller \"\"\"\n    try:\n        # PyInstaller creates a temp folder and stores path in _MEIPASS\n        base_path = sys._MEIPASS\n    except Exception:\n        if 'pytest' in sys.modules:\n            for arg in reversed(sys.argv):\n                path = os.path.realpath(arg)\n\n                if os.path.exists(path):\n                    base_path = path if os.path.isdir(path) else os.path.dirname(path)\n                    break\n        else:\n            base_path = os.path.dirname(os.path.realpath(sys.argv[0]))\n\n    if not os.path.exists(base_path):\n        raise ValueError('Path %s does not exist' % base_path)\n\n    return 'file://%s' % os.path.join(base_path, relative_path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing file type string description file extensions", "response": "def parse_file_type(file_type):\n    '''\n    :param file_type: file type string 'description (*.file_extension1;*.file_extension2)' as required by file filter in create_file_dialog\n    :return: (description, file extensions) tuple\n    '''\n    valid_file_filter = r'^([\\w ]+)\\((\\*(?:\\.(?:\\w+|\\*))*(?:;\\*\\.\\w+)*)\\)$'\n    match = re.search(valid_file_filter, file_type)\n\n    if match:\n        return match.group(1).rstrip(), match.group(2)\n    else:\n        raise ValueError('{0} is not a valid file filter'.format(file_type))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a default application menu that shows the application name and the main menu for the application and adds the application name to the menu items if it s available.", "response": "def _add_app_menu(self):\n        \"\"\"\n        Create a default Cocoa menu that shows 'Services', 'Hide',\n        'Hide Others', 'Show All', and 'Quit'. Will append the application name\n        to some menu items if it's available.\n        \"\"\"\n        # Set the main menu for the application\n        mainMenu = AppKit.NSMenu.alloc().init()\n        self.app.setMainMenu_(mainMenu)\n\n        # Create an application menu and make it a submenu of the main menu\n        mainAppMenuItem = AppKit.NSMenuItem.alloc().init()\n        mainMenu.addItem_(mainAppMenuItem)\n        appMenu = AppKit.NSMenu.alloc().init()\n        mainAppMenuItem.setSubmenu_(appMenu)\n\n        appMenu.addItemWithTitle_action_keyEquivalent_(self._append_app_name(localization[\"cocoa.menu.about\"]), \"orderFrontStandardAboutPanel:\", \"\")\n\n        appMenu.addItem_(AppKit.NSMenuItem.separatorItem())\n\n        # Set the 'Services' menu for the app and create an app menu item\n        appServicesMenu = AppKit.NSMenu.alloc().init()\n        self.app.setServicesMenu_(appServicesMenu)\n        servicesMenuItem = appMenu.addItemWithTitle_action_keyEquivalent_(localization[\"cocoa.menu.services\"], nil, \"\")\n        servicesMenuItem.setSubmenu_(appServicesMenu)\n\n        appMenu.addItem_(AppKit.NSMenuItem.separatorItem())\n\n        # Append the 'Hide', 'Hide Others', and 'Show All' menu items\n        appMenu.addItemWithTitle_action_keyEquivalent_(self._append_app_name(localization[\"cocoa.menu.hide\"]), \"hide:\", \"h\")\n        hideOthersMenuItem = appMenu.addItemWithTitle_action_keyEquivalent_(localization[\"cocoa.menu.hideOthers\"], \"hideOtherApplications:\", \"h\")\n        hideOthersMenuItem.setKeyEquivalentModifierMask_(AppKit.NSAlternateKeyMask | AppKit.NSCommandKeyMask)\n        appMenu.addItemWithTitle_action_keyEquivalent_(localization[\"cocoa.menu.showAll\"], \"unhideAllApplications:\", \"\")\n\n        appMenu.addItem_(AppKit.NSMenuItem.separatorItem())\n\n        # Append a 'Quit' menu item\n        appMenu.addItemWithTitle_action_keyEquivalent_(self._append_app_name(localization[\"cocoa.menu.quit\"]), \"terminate:\", \"q\")"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _add_view_menu(self):\n        mainMenu = self.app.mainMenu()\n\n        # Create an View menu and make it a submenu of the main menu\n        viewMenu = AppKit.NSMenu.alloc().init()\n        viewMenu.setTitle_(localization[\"cocoa.menu.view\"])\n        viewMenuItem = AppKit.NSMenuItem.alloc().init()\n        viewMenuItem.setSubmenu_(viewMenu)\n        mainMenu.addItem_(viewMenuItem)\n\n        # TODO: localization of the Enter fullscreen string has no effect\n        fullScreenMenuItem = viewMenu.addItemWithTitle_action_keyEquivalent_(localization[\"cocoa.menu.fullscreen\"], \"toggleFullScreen:\", \"f\")\n        fullScreenMenuItem.setKeyEquivalentModifierMask_(AppKit.NSControlKeyMask | AppKit.NSCommandKeyMask)", "response": "Create a default View menu that shows Enter Full Screen."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef nscolor_from_hex(hex_string):\n\n        hex_string = hex_string[1:]     # Remove leading hash\n        if len(hex_string) == 3:\n            hex_string = ''.join([c*2 for c in hex_string]) # 3-digit to 6-digit\n\n        hex_int = int(hex_string, 16)\n        rgb = (\n            (hex_int >> 16) & 0xff,     # Red byte\n            (hex_int >> 8) & 0xff,      # Blue byte\n            (hex_int) & 0xff            # Green byte\n        )\n        rgb = [i / 255.0 for i in rgb]      # Normalize to range(0.0, 1.0)\n\n        return AppKit.NSColor.colorWithSRGBRed_green_blue_alpha_(rgb[0], rgb[1], rgb[2], 1.0)", "response": "Convert given hex color to NSColor."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_instance(attr, value):\n        for i in list(BrowserView.instances.values()):\n            try:\n                if getattr(i, attr) == value:\n                    return i\n            except AttributeError:\n                break\n\n        return None", "response": "Return a BrowserView instance by the given attribute and value."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a PyObjCMethodSignature object for given signature string.", "response": "def pyobjc_method_signature(signature_str):\n        \"\"\"\n        Return a PyObjCMethodSignature object for given signature string.\n\n        :param signature_str: A byte string containing the type encoding for the method signature\n        :return: A method signature object, assignable to attributes like __block_signature__\n        :rtype: <type objc._method_signature>\n        \"\"\"\n        _objc_so.PyObjCMethodSignature_WithMetaData.restype = ctypes.py_object\n        return _objc_so.PyObjCMethodSignature_WithMetaData(ctypes.create_string_buffer(signature_str), None, False)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef choose_path():\n    dirs = webview.create_file_dialog(webview.FOLDER_DIALOG)\n    if dirs and len(dirs) > 0:\n        directory = dirs[0]\n        if isinstance(directory, bytes):\n            directory = directory.decode(\"utf-8\")\n\n        response = {\"status\": \"ok\", \"directory\": directory}\n    else:\n        response = {\"status\": \"cancel\"}\n\n    return jsonify(response)", "response": "Invoke a folder selection dialog here\n   "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_ie_mode():\n\n    try:\n        import _winreg as winreg  # Python 2\n    except ImportError:\n        import winreg  # Python 3\n\n    def get_ie_mode():\n        \"\"\"\n        Get the installed version of IE\n        :return:\n        \"\"\"\n        ie_key = winreg.OpenKey(winreg.HKEY_LOCAL_MACHINE, r\"Software\\Microsoft\\Internet Explorer\")\n        try:\n            version, type = winreg.QueryValueEx(ie_key, \"svcVersion\")\n        except:\n            version, type = winreg.QueryValueEx(ie_key, \"Version\")\n\n        winreg.CloseKey(ie_key)\n\n        if version.startswith(\"11\"):\n            value = 0x2AF9\n        elif version.startswith(\"10\"):\n            value = 0x2711\n        elif version.startswith(\"9\"):\n            value = 0x270F\n        elif version.startswith(\"8\"):\n            value = 0x22B8\n        else:\n            value = 0x2AF9  # Set IE11 as default\n\n        return value\n\n    try:\n        browser_emulation = winreg.OpenKey(winreg.HKEY_CURRENT_USER,\n                                           r\"Software\\Microsoft\\Internet Explorer\\Main\\FeatureControl\\FEATURE_BROWSER_EMULATION\",\n                                           0, winreg.KEY_ALL_ACCESS)\n    except WindowsError:\n        browser_emulation = winreg.CreateKeyEx(winreg.HKEY_CURRENT_USER,\n                                               r\"Software\\Microsoft\\Internet Explorer\\Main\\FeatureControl\\FEATURE_BROWSER_EMULATION\",\n                                               0, winreg.KEY_ALL_ACCESS)\n\n    try:\n        dpi_support = winreg.OpenKey(winreg.HKEY_CURRENT_USER,\n                                     r\"Software\\Microsoft\\Internet Explorer\\Main\\FeatureControl\\FEATURE_96DPI_PIXEL\",\n                                     0, winreg.KEY_ALL_ACCESS)\n    except WindowsError:\n        dpi_support = winreg.CreateKeyEx(winreg.HKEY_CURRENT_USER,\n                                               r\"Software\\Microsoft\\Internet Explorer\\Main\\FeatureControl\\FEATURE_96DPI_PIXEL\",\n                                               0, winreg.KEY_ALL_ACCESS)\n\n    mode = get_ie_mode()\n    executable_name = sys.executable.split(\"\\\\\")[-1]\n    winreg.SetValueEx(browser_emulation, executable_name, 0, winreg.REG_DWORD, mode)\n    winreg.CloseKey(browser_emulation)\n\n    winreg.SetValueEx(dpi_support, executable_name, 0, winreg.REG_DWORD, 1)\n    winreg.CloseKey(dpi_support)", "response": "Set the hosted IE mode for the current executable."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _api_call(function):\n    @wraps(function)\n    def wrapper(*args, **kwargs):\n        try:\n            if not _webview_ready.wait(15):\n                raise Exception('Main window failed to start')\n            return function(*args, **kwargs)\n        except NameError:\n            raise Exception('Create a web view window first, before invoking this function')\n        except KeyError as e:\n            try:\n                uid = kwargs['uid']\n            except KeyError:\n                # uid not passed as a keyword arg, assumes it to be last in the arg list\n                uid = args[-1]\n            raise Exception('Cannot call function: No webview exists with uid: {}'.format(uid))\n    return wrapper", "response": "Decorator to call a pywebview API"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_window(title, url=None, js_api=None, width=800, height=600,\n                  resizable=True, fullscreen=False, min_size=(200, 100), strings={}, confirm_quit=False,\n                  background_color='#FFFFFF', text_select=False, frameless=False, debug=False):\n    \"\"\"\n    Create a web view window using a native GUI. The execution blocks after this function is invoked, so other\n    program logic must be executed in a separate thread.\n    :param title: Window title\n    :param url: URL to load\n    :param width: window width. Default is 800px\n    :param height:window height. Default is 600px\n    :param resizable True if window can be resized, False otherwise. Default is True\n    :param fullscreen: True if start in fullscreen mode. Default is False\n    :param min_size: a (width, height) tuple that specifies a minimum window size. Default is 200x100\n    :param strings: a dictionary with localized strings\n    :param confirm_quit: Display a quit confirmation dialog. Default is False\n    :param background_color: Background color as a hex string that is displayed before the content of webview is loaded. Default is white.\n    :param text_select: Allow text selection on page. Default is False.\n    :param frameless: Whether the window should have a frame.\n    :return: The uid of the created window.\n    \"\"\"\n\n    valid_color = r'^#(?:[0-9a-fA-F]{3}){1,2}$'\n    if not re.match(valid_color, background_color):\n        raise ValueError('{0} is not a valid hex triplet color'.format(background_color))\n\n    # Check if starting up from main thread; if not, wait; finally raise exception\n    if current_thread().name == 'MainThread':\n        uid = 'master'\n\n        if not _initialized:\n            _initialize_imports()\n            localization.update(strings)\n    else:\n        uid = 'child_' + uuid4().hex[:8]\n        if not _webview_ready.wait(5):\n            raise Exception('Call create_window from the main thread first')\n\n    _webview_ready.clear()  # Make API calls wait while the new window is created\n    gui.create_window(uid, make_unicode(title), transform_url(url),\n                      width, height, resizable, fullscreen, min_size, confirm_quit,\n                      background_color, debug, js_api, text_select, frameless, _webview_ready)\n\n    if uid == 'master':\n        _webview_ready.clear()\n    else:\n        return uid", "response": "Create a webview window using a native GUI."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_file_dialog(dialog_type=OPEN_DIALOG, directory='', allow_multiple=False, save_filename='', file_types=()):\n    if type(file_types) != tuple and type(file_types) != list:\n        raise TypeError('file_types must be a tuple of strings')\n    for f in file_types:\n        parse_file_type(f)\n\n    if not os.path.exists(directory):\n        directory = ''\n\n    return gui.create_file_dialog(dialog_type, directory, allow_multiple, save_filename, file_types)", "response": "Create a file dialog"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nloading a new content into a previously created WebView window.", "response": "def load_html(content, base_uri=base_uri(), uid='master'):\n    \"\"\"\n    Load a new content into a previously created WebView window. This function must be invoked after WebView windows is\n    created with create_window(). Otherwise an exception is thrown.\n    :param content: Content to load.\n    :param base_uri: Base URI for resolving links. Default is the directory of the application entry point.\n    :param uid: uid of the target instance\n    \"\"\"\n    content = make_unicode(content)\n    gui.load_html(content, base_uri, uid)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nevaluate given JavaScript code and return the result", "response": "def evaluate_js(script, uid='master'):\n    \"\"\"\n    Evaluate given JavaScript code and return the result\n    :param script: The JavaScript code to be evaluated\n    :param uid: uid of the target instance\n    :return: Return value of the evaluated code\n    \"\"\"\n    escaped_script = 'JSON.stringify(eval(\"{0}\"))'.format(escape_string(script))\n    return gui.evaluate_js(escaped_script, uid)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_image(self, idx):\n        im = Image.open('{}/data/images/img_{}.png'.format(self.nyud_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean_bgr\n        in_ = in_.transpose((2,0,1))\n        return in_", "response": "Load input image and preprocess for Caffe"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_label(self, idx):\n        label = scipy.io.loadmat('{}/segmentation/img_{}.mat'.format(self.nyud_dir, idx))['segmentation'].astype(np.uint8)\n        label -= 1  # rotate labels\n        label = label[np.newaxis, ...]\n        return label", "response": "Load label image as 1 x height x width integer array of label indices."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_depth(self, idx):\n        im = Image.open('{}/data/depth/img_{}.png'.format(self.nyud_dir, idx))\n        d = np.array(im, dtype=np.float32)\n        d = np.log(d)\n        d -= self.mean_logd\n        d = d[np.newaxis, ...]\n        return d", "response": "Load depth for NYUDv2 segmentation set."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef load_hha(self, idx):\n        im = Image.open('{}/data/hha/img_{}.png'.format(self.nyud_dir, idx))\n        hha = np.array(im, dtype=np.float32)\n        hha -= self.mean_hha\n        hha = hha.transpose((2,0,1))\n        return hha", "response": "Load HHA features from Gupta et al. ECCV14. m\n       "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_image(self, idx):\n        im = Image.open('{}/Images/spatial_envelope_256x256_static_8outdoorcategories/{}.jpg'.format(self.siftflow_dir, idx))\n        in_ = np.array(im, dtype=np.float32)\n        in_ = in_[:,:,::-1]\n        in_ -= self.mean\n        in_ = in_.transpose((2,0,1))\n        return in_", "response": "Load input image and preprocess for Caffe"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_label(self, idx, label_type=None):\n        if label_type == 'semantic':\n            label = scipy.io.loadmat('{}/SemanticLabels/spatial_envelope_256x256_static_8outdoorcategories/{}.mat'.format(self.siftflow_dir, idx))['S']\n        elif label_type == 'geometric':\n            label = scipy.io.loadmat('{}/GeoLabels/spatial_envelope_256x256_static_8outdoorcategories/{}.mat'.format(self.siftflow_dir, idx))['S']\n            label[label == -1] = 0\n        else:\n            raise Exception(\"Unknown label type: {}. Pick semantic or geometric.\".format(label_type))\n        label = label.astype(np.uint8)\n        label -= 1  # rotate labels so classes start at 0, void is 255\n        label = label[np.newaxis, ...]\n        return label.copy()", "response": "Load label image as 1 x height x width integer array of label indices."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nmake a 2D bilinear kernel suitable for upsampling", "response": "def get_upsampling_weight(in_channels, out_channels, kernel_size):\n    \"\"\"Make a 2D bilinear kernel suitable for upsampling\"\"\"\n    factor = (kernel_size + 1) // 2\n    if kernel_size % 2 == 1:\n        center = factor - 1\n    else:\n        center = factor - 0.5\n    og = np.ogrid[:kernel_size, :kernel_size]\n    filt = (1 - abs(og[0] - center) / factor) * \\\n           (1 - abs(og[1] - center) / factor)\n    weight = np.zeros((in_channels, out_channels, kernel_size, kernel_size),\n                      dtype=np.float64)\n    weight[range(in_channels), range(out_channels), :, :] = filt\n    return torch.from_numpy(weight).float()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncomputing the accuracy score evaluation result.", "response": "def label_accuracy_score(label_trues, label_preds, n_class):\n    \"\"\"Returns accuracy score evaluation result.\n\n      - overall accuracy\n      - mean accuracy\n      - mean IU\n      - fwavacc\n    \"\"\"\n    hist = np.zeros((n_class, n_class))\n    for lt, lp in zip(label_trues, label_preds):\n        hist += _fast_hist(lt.flatten(), lp.flatten(), n_class)\n    acc = np.diag(hist).sum() / hist.sum()\n    with np.errstate(divide='ignore', invalid='ignore'):\n        acc_cls = np.diag(hist) / hist.sum(axis=1)\n    acc_cls = np.nanmean(acc_cls)\n    with np.errstate(divide='ignore', invalid='ignore'):\n        iu = np.diag(hist) / (\n            hist.sum(axis=1) + hist.sum(axis=0) - np.diag(hist)\n        )\n    mean_iu = np.nanmean(iu)\n    freq = hist.sum(axis=1) / hist.sum()\n    fwavacc = (freq[freq > 0] * iu[freq > 0]).sum()\n    return acc, acc_cls, mean_iu, fwavacc"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntransplanting a single object to another object.", "response": "def transplant(new_net, net, suffix=''):\n    \"\"\"\n    Transfer weights by copying matching parameters, coercing parameters of\n    incompatible shape, and dropping unmatched parameters.\n\n    The coercion is useful to convert fully connected layers to their\n    equivalent convolutional layers, since the weights are the same and only\n    the shapes are different.  In particular, equivalent fully connected and\n    convolution layers have shapes O x I and O x I x H x W respectively for O\n    outputs channels, I input channels, H kernel height, and W kernel width.\n\n    Both  `net` to `new_net` arguments must be instantiated `caffe.Net`s.\n    \"\"\"\n    for p in net.params:\n        p_new = p + suffix\n        if p_new not in new_net.params:\n            print 'dropping', p\n            continue\n        for i in range(len(net.params[p])):\n            if i > (len(new_net.params[p_new]) - 1):\n                print 'dropping', p, i\n                break\n            if net.params[p][i].data.shape != new_net.params[p_new][i].data.shape:\n                print 'coercing', p, i, 'from', net.params[p][i].data.shape, 'to', new_net.params[p_new][i].data.shape\n            else:\n                print 'copying', p, ' -> ', p_new, i\n            new_net.params[p_new][i].data.flat = net.params[p][i].data.flat"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef interp(net, layers):\n    for l in layers:\n        m, k, h, w = net.params[l][0].data.shape\n        if m != k and k != 1:\n            print 'input + output channels need to be the same or |output| == 1'\n            raise\n        if h != w:\n            print 'filters need to be square'\n            raise\n        filt = upsample_filt(h)\n        net.params[l][0].data[range(m), range(k), :, :] = filt", "response": "Interpolate the kernels of each layer in layers to bilinear kernels for interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expand_score(new_net, new_layer, net, layer):\n    old_cl = net.params[layer][0].num\n    new_net.params[new_layer][0].data[:old_cl][...] = net.params[layer][0].data\n    new_net.params[new_layer][1].data[0,0,0,:old_cl][...] = net.params[layer][1].data", "response": "Transplant an old score layer s parameters with k < k classes into a new score layer with k < k classes s. t. the first k classes are the old classes."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_label(self, idx):\n        label_400 = scipy.io.loadmat('{}/trainval/{}.mat'.format(self.context_dir, idx))['LabelMap']\n        label = np.zeros_like(label_400, dtype=np.uint8)\n        for idx, l in enumerate(self.labels_59):\n            idx_400 = self.labels_400.index(l) + 1\n            label[label_400 == idx_400] = idx + 1\n        label = label[np.newaxis, ...]\n        return label", "response": "Load label image as 1 x height x width integer array of label indices."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload label image as 1 x height x width integer array of label indices.", "response": "def load_label(self, idx):\n        \"\"\"\n        Load label image as 1 x height x width integer array of label indices.\n        The leading singleton dimension is required by the loss.\n        \"\"\"\n        label = Image.open('{}/SegmentationClass/{}.png'.format(self.dir, idx))\n        label = np.array(label, dtype=np.uint8)\n        label = label[np.newaxis, ...]\n        return label"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ntransfer the color palette to an output mask for visualization.", "response": "def palette(self, label_im):\n        '''\n        Transfer the VOC color palette to an output mask for visualization.\n        '''\n        if label_im.ndim == 3:\n            label_im = label_im[0]\n        label = Image.fromarray(label_im, mode='P')\n        label.palette = copy.copy(self.palette)\n        return label"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef bin_dir(venv):\n    bin_part = 'Scripts' if os.name == 'nt' else 'bin'\n    return os.path.join(venv, bin_part)", "response": "Returns the path to the bin directory for the virtualenv"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninstall the pre - commit hooks.", "response": "def install(\n        config_file, store,\n        overwrite=False, hooks=False, hook_type='pre-commit',\n        skip_on_missing_conf=False,\n):\n    \"\"\"Install the pre-commit hooks.\"\"\"\n    if cmd_output('git', 'config', 'core.hooksPath', retcode=None)[1].strip():\n        logger.error(\n            'Cowardly refusing to install hooks with `core.hooksPath` set.\\n'\n            'hint: `git config --unset-all core.hooksPath`',\n        )\n        return 1\n\n    hook_path, legacy_path = _hook_paths(hook_type)\n\n    mkdirp(os.path.dirname(hook_path))\n\n    # If we have an existing hook, move it to pre-commit.legacy\n    if os.path.lexists(hook_path) and not is_our_script(hook_path):\n        shutil.move(hook_path, legacy_path)\n\n    # If we specify overwrite, we simply delete the legacy file\n    if overwrite and os.path.exists(legacy_path):\n        os.remove(legacy_path)\n    elif os.path.exists(legacy_path):\n        output.write_line(\n            'Running in migration mode with existing hooks at {}\\n'\n            'Use -f to use only pre-commit.'.format(legacy_path),\n        )\n\n    params = {\n        'CONFIG': config_file,\n        'HOOK_TYPE': hook_type,\n        'INSTALL_PYTHON': sys.executable,\n        'SKIP_ON_MISSING_CONFIG': skip_on_missing_conf,\n    }\n\n    with io.open(hook_path, 'w') as hook_file:\n        contents = resource_text('hook-tmpl')\n        before, rest = contents.split(TEMPLATE_START)\n        to_template, after = rest.split(TEMPLATE_END)\n\n        before = before.replace('#!/usr/bin/env python3', shebang())\n\n        hook_file.write(before + TEMPLATE_START)\n        for line in to_template.splitlines():\n            var = line.split()[0]\n            hook_file.write('{} = {!r}\\n'.format(var, params[var]))\n        hook_file.write(TEMPLATE_END + after)\n    make_executable(hook_path)\n\n    output.write_line('pre-commit installed at {}'.format(hook_path))\n\n    # If they requested we install all of the hooks, do so.\n    if hooks:\n        install_hooks(config_file, store)\n\n    return 0"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uninstall(hook_type='pre-commit'):\n    hook_path, legacy_path = _hook_paths(hook_type)\n\n    # If our file doesn't exist or it isn't ours, gtfo.\n    if not os.path.exists(hook_path) or not is_our_script(hook_path):\n        return 0\n\n    os.remove(hook_path)\n    output.write_line('{} uninstalled'.format(hook_type))\n\n    if os.path.exists(legacy_path):\n        os.rename(legacy_path, hook_path)\n        output.write_line('Restored previous hooks to {}'.format(hook_path))\n\n    return 0", "response": "Uninstall the pre - commit hooks."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nnormalizing the command to be a deep - path command", "response": "def normalize_cmd(cmd):\n    \"\"\"Fixes for the following issues on windows\n    - https://bugs.python.org/issue8557\n    - windows does not parse shebangs\n\n    This function also makes deep-path shebangs work just fine\n    \"\"\"\n    # Use PATH to determine the executable\n    exe = normexe(cmd[0])\n\n    # Figure out the shebang from the resulting command\n    cmd = parse_filename(exe) + (exe,) + cmd[1:]\n\n    # This could have given us back another bare executable\n    exe = normexe(cmd[0])\n\n    return (exe,) + cmd[1:]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_hook_message(\n        start,\n        postfix='',\n        end_msg=None,\n        end_len=0,\n        end_color=None,\n        use_color=None,\n        cols=80,\n):\n    \"\"\"Prints a message for running a hook.\n\n    This currently supports three approaches:\n\n    # Print `start` followed by dots, leaving 6 characters at the end\n    >>> print_hook_message('start', end_len=6)\n    start...............................................................\n\n    # Print `start` followed by dots with the end message colored if coloring\n    # is specified and a newline afterwards\n    >>> print_hook_message(\n        'start',\n        end_msg='end',\n        end_color=color.RED,\n        use_color=True,\n    )\n    start...................................................................end\n\n    # Print `start` followed by dots, followed by the `postfix` message\n    # uncolored, followed by the `end_msg` colored if specified and a newline\n    # afterwards\n    >>> print_hook_message(\n        'start',\n        postfix='postfix ',\n        end_msg='end',\n        end_color=color.RED,\n        use_color=True,\n    )\n    start...........................................................postfix end\n    \"\"\"\n    if bool(end_msg) == bool(end_len):\n        raise ValueError('Expected one of (`end_msg`, `end_len`)')\n    if end_msg is not None and (end_color is None or use_color is None):\n        raise ValueError(\n            '`end_color` and `use_color` are required with `end_msg`',\n        )\n\n    if end_len:\n        return start + '.' * (cols - len(start) - end_len - 1)\n    else:\n        return '{}{}{}{}\\n'.format(\n            start,\n            '.' * (cols - len(start) - len(postfix) - len(end_msg) - 1),\n            postfix,\n            color.format_color(end_msg, end_color, use_color),\n        )", "response": "Prints a message for running a hook."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef check_for_cygwin_mismatch():\n    if sys.platform in ('cygwin', 'win32'):  # pragma: no cover (windows)\n        is_cygwin_python = sys.platform == 'cygwin'\n        toplevel = cmd_output('git', 'rev-parse', '--show-toplevel')[1]\n        is_cygwin_git = toplevel.startswith('/')\n\n        if is_cygwin_python ^ is_cygwin_git:\n            exe_type = {True: '(cygwin)', False: '(windows)'}\n            logger.warn(\n                'pre-commit has detected a mix of cygwin python / git\\n'\n                'This combination is not supported, it is likely you will '\n                'receive an error later in the program.\\n'\n                'Make sure to use cygwin git+python while using cygwin\\n'\n                'These can be installed through the cygwin installer.\\n'\n                ' - python {}\\n'\n                ' - git {}\\n'.format(\n                    exe_type[is_cygwin_python], exe_type[is_cygwin_git],\n                ),\n            )", "response": "Check if the current platform is cygwin and if so check if the cygwin version is compatible with the current platform."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmake an archive of a repository in the given destdir.", "response": "def make_archive(name, repo, ref, destdir):\n    \"\"\"Makes an archive of a repository in the given destdir.\n\n    :param text name: Name to give the archive.  For instance foo.  The file\n    that is created will be called foo.tar.gz.\n    :param text repo: Repository to clone.\n    :param text ref: Tag/SHA/branch to check out.\n    :param text destdir: Directory to place archives in.\n    \"\"\"\n    output_path = os.path.join(destdir, name + '.tar.gz')\n    with tmpdir() as tempdir:\n        # Clone the repository to the temporary directory\n        cmd_output('git', 'clone', repo, tempdir)\n        cmd_output('git', 'checkout', ref, cwd=tempdir)\n\n        # We don't want the '.git' directory\n        # It adds a bunch of size to the archive and we don't use it at\n        # runtime\n        rmtree(os.path.join(tempdir, '.git'))\n\n        with tarfile.open(output_path, 'w|gz') as tf:\n            tf.add(tempdir, name)\n\n    return output_path"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_cols(hooks, verbose):\n    if hooks:\n        name_len = max(len(_hook_msg_start(hook, verbose)) for hook in hooks)\n    else:\n        name_len = 0\n\n    cols = name_len + 3 + len(NO_FILES) + 1 + len(SKIPPED)\n    return max(cols, 80)", "response": "Compute the number of columns to display in the log file."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _update_repo(repo_config, store, tags_only):\n    repo_path = store.clone(repo_config['repo'], repo_config['rev'])\n\n    cmd_output('git', 'fetch', cwd=repo_path)\n    tag_cmd = ('git', 'describe', 'origin/master', '--tags')\n    if tags_only:\n        tag_cmd += ('--abbrev=0',)\n    else:\n        tag_cmd += ('--exact',)\n    try:\n        rev = cmd_output(*tag_cmd, cwd=repo_path)[1].strip()\n    except CalledProcessError:\n        tag_cmd = ('git', 'rev-parse', 'origin/master')\n        rev = cmd_output(*tag_cmd, cwd=repo_path)[1].strip()\n\n    # Don't bother trying to update if our rev is the same\n    if rev == repo_config['rev']:\n        return repo_config\n\n    try:\n        path = store.clone(repo_config['repo'], rev)\n        manifest = load_manifest(os.path.join(path, C.MANIFEST_FILE))\n    except InvalidManifestError as e:\n        raise RepositoryCannotBeUpdatedError(six.text_type(e))\n\n    # See if any of our hooks were deleted with the new commits\n    hooks = {hook['id'] for hook in repo_config['hooks']}\n    hooks_missing = hooks - {hook['id'] for hook in manifest}\n    if hooks_missing:\n        raise RepositoryCannotBeUpdatedError(\n            'Cannot update because the tip of master is missing these hooks:\\n'\n            '{}'.format(', '.join(sorted(hooks_missing))),\n        )\n\n    # Construct a new config with the head rev\n    new_config = repo_config.copy()\n    new_config['rev'] = rev\n    return new_config", "response": "Updates a repository to the tip of master."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef autoupdate(config_file, store, tags_only, repos=()):\n    migrate_config(config_file, quiet=True)\n    retv = 0\n    output_repos = []\n    changed = False\n\n    input_config = load_config(config_file)\n\n    for repo_config in input_config['repos']:\n        if (\n            repo_config['repo'] in {LOCAL, META} or\n            # Skip updating any repo_configs that aren't for the specified repo\n            repos and repo_config['repo'] not in repos\n        ):\n            output_repos.append(repo_config)\n            continue\n        output.write('Updating {}...'.format(repo_config['repo']))\n        try:\n            new_repo_config = _update_repo(repo_config, store, tags_only)\n        except RepositoryCannotBeUpdatedError as error:\n            output.write_line(error.args[0])\n            output_repos.append(repo_config)\n            retv = 1\n            continue\n\n        if new_repo_config['rev'] != repo_config['rev']:\n            changed = True\n            output.write_line(\n                'updating {} -> {}.'.format(\n                    repo_config['rev'], new_repo_config['rev'],\n                ),\n            )\n            output_repos.append(new_repo_config)\n        else:\n            output.write_line('already up to date.')\n            output_repos.append(repo_config)\n\n    if changed:\n        output_config = input_config.copy()\n        output_config['repos'] = output_repos\n        _write_new_config_file(config_file, output_config)\n\n    return retv", "response": "Auto - update the pre - commit config to the latest versions of repos."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef orig_py_exe(exe):  # pragma: no cover (platform specific)\n    try:\n        prefix_script = 'import sys; print(sys.real_prefix)'\n        _, prefix, _ = cmd_output(exe, '-c', prefix_script)\n        prefix = prefix.strip()\n    except CalledProcessError:\n        # not created from -mvirtualenv\n        return exe\n\n    if os.name == 'nt':\n        expected = os.path.join(prefix, 'python.exe')\n    else:\n        expected = os.path.join(prefix, 'bin', os.path.basename(exe))\n\n    if os.path.exists(expected):\n        return expected\n    else:\n        return exe", "response": "A python executable that is being used is the original path to the correct location."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nenable virtual terminal processing for the current process.", "response": "def enable_virtual_terminal_processing():\n    \"\"\"As of Windows 10, the Windows console supports (some) ANSI escape\n    sequences, but it needs to be enabled using `SetConsoleMode` first.\n\n    More info on the escape sequences supported:\n    https://msdn.microsoft.com/en-us/library/windows/desktop/mt638032(v=vs.85).aspx\n    \"\"\"\n    stdout = GetStdHandle(STD_OUTPUT_HANDLE)\n    flags = GetConsoleMode(stdout)\n    SetConsoleMode(stdout, flags | ENABLE_VIRTUAL_TERMINAL_PROCESSING)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nformatting text with color.", "response": "def format_color(text, color, use_color_setting):\n    \"\"\"Format text with color.\n\n    Args:\n        text - Text to be formatted with color if `use_color`\n        color - The color start string\n        use_color_setting - Whether or not to color\n    \"\"\"\n    if not use_color_setting:\n        return text\n    else:\n        return '{}{}{}'.format(color, text, NORMAL)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nchoose whether to use color based on the command argument.", "response": "def use_color(setting):\n    \"\"\"Choose whether to use color based on the command argument.\n\n    Args:\n        setting - Either `auto`, `always`, or `never`\n    \"\"\"\n    if setting not in COLOR_CHOICES:\n        raise InvalidColorSetting(setting)\n\n    return (\n        setting == 'always' or\n        (setting == 'auto' and sys.stdout.isatty() and terminal_supports_color)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the default directory for the store.", "response": "def _get_default_directory():\n    \"\"\"Returns the default directory for the Store.  This is intentionally\n    underscored to indicate that `Store.get_default_directory` is the intended\n    way to get this information.  This is also done so\n    `Store.get_default_directory` can be mocked in tests and\n    `_get_default_directory` can be tested.\n    \"\"\"\n    return os.environ.get('PRE_COMMIT_HOME') or os.path.join(\n        os.environ.get('XDG_CACHE_HOME') or os.path.expanduser('~/.cache'),\n        'pre-commit',\n    )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _shallow_clone(self, ref, git_cmd):  # pragma: windows no cover\n\n        git_config = 'protocol.version=2'\n        git_cmd('-c', git_config, 'fetch', 'origin', ref, '--depth=1')\n        git_cmd('checkout', ref)\n        git_cmd(\n            '-c', git_config, 'submodule', 'update', '--init',\n            '--recursive', '--depth=1',\n        )", "response": "Perform a shallow clone of a repository and its submodules"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef clone(self, repo, ref, deps=()):\n\n        if os.path.isdir(repo):\n            repo = os.path.abspath(repo)\n\n        def clone_strategy(directory):\n            env = git.no_git_env()\n\n            def _git_cmd(*args):\n                cmd_output('git', *args, cwd=directory, env=env)\n\n            _git_cmd('init', '.')\n            _git_cmd('remote', 'add', 'origin', repo)\n\n            try:\n                self._shallow_clone(ref, _git_cmd)\n            except CalledProcessError:\n                self._complete_clone(ref, _git_cmd)\n\n        return self._new_repo(repo, ref, deps, clone_strategy)", "response": "Clone the given url and checkout the specific ref."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nyielding an infinite series of compounding values. Each value is produced by multiplying the previous value by the compound rate.", "response": "def compounding(start, stop, compound, t=0.0):\n    \"\"\"Yield an infinite series of compounding values. Each time the\n    generator is called, a value is produced by multiplying the previous\n    value by the compound rate.\n\n    EXAMPLE:\n      >>> sizes = compounding(1., 10., 1.5)\n      >>> assert next(sizes) == 1.\n      >>> assert next(sizes) == 1 * 1.5\n      >>> assert next(sizes) == 1.5 * 1.5\n    \"\"\"\n    curr = float(start)\n    while True:\n        yield _clip(curr, start, stop)\n        curr *= compound"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields an infinite series of values according to Howard and Ruder s slanted triangular learning rate schedule.", "response": "def slanted_triangular(max_rate, num_steps, cut_frac=0.1, ratio=32, decay=1, t=0.0):\n    \"\"\"Yield an infinite series of values according to Howard and Ruder's\n    \"slanted triangular learning rate\" schedule.\n    \"\"\"\n    cut = int(num_steps * cut_frac)\n    while True:\n        t += 1\n        if t < cut:\n            p = t / cut\n        else:\n            p = 1 - ((t - cut) / (cut * (1 / cut_frac - 1)))\n        learn_rate = max_rate * (1 + p * (ratio - 1)) * (1 / ratio)\n        yield learn_rate"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef equal_length(*args):\n    for i, arg in enumerate(args):\n        if not isinstance(arg, Sized):\n            raise ExpectedTypeError(arg, [\"Sized\"])\n        if i >= 1 and len(arg) != len(args[0]):\n            raise DifferentLengthError(args, arg)", "response": "Check that all arguments have the same length."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck that all elements in the sequence have the same dimension on specified axis.", "response": "def equal_axis(*args, **axis):\n    \"\"\"Check that elements have the same dimension on specified axis.\n    \"\"\"\n    axis = axis.get(\"axis\", -1)\n    for i, arg in enumerate(args):\n        if not isinstance(arg, ndarray):\n            raise ExpectedTypeError(arg, [\"ndarray\"])\n        if axis >= 0 and (axis + 1) < arg.shape[axis]:\n            raise ShapeMismatchError(arg.shape[axis], axis, [])\n        if i >= 1 and arg.shape[axis] != args[0].shape[axis]:\n            lengths = [a.shape[axis] for a in args]\n            raise DifferentLengthError(lengths, arg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef has_shape(shape):\n\n    def has_shape_inner(arg_id, args, kwargs):\n        self = args[0]\n        arg = args[arg_id]\n        if not hasattr(arg, \"shape\"):\n            raise ExpectedTypeError(arg, [\"array\"])\n        shape_values = []\n        for dim in shape:\n            if not isinstance(dim, integer_types):\n                dim = getattr(self, dim, None)\n            shape_values.append(dim)\n        if len(shape) != len(arg.shape):\n            raise ShapeMismatchError(arg.shape, tuple(shape_values), shape)\n        for i, dim in enumerate(shape_values):\n            # Allow underspecified dimensions\n            if dim is not None and arg.shape[i] != dim:\n                raise ShapeMismatchError(arg.shape, shape_values, shape)\n\n    return has_shape_inner", "response": "Check that a particular argument is an array with a given shape."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef define_operators(cls, operators):\n        old_ops = dict(cls._operators)\n        for op, func in operators.items():\n            cls._operators[op] = func\n        yield\n        cls._operators = old_ops", "response": "Bind operators to specified functions for the scope of the context of the block."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nchange the device to execute on for the scope of the block.", "response": "def use_device(cls, device):\n        \"\"\"Change the device to execute on for the scope of the block.\"\"\"\n        if device == cls.ops.device:\n            yield\n        else:\n            curr_Ops, curr_ops = (cls.Ops, cls.ops)\n            cls.Ops = get_ops(device)\n            cls.ops = cls.Ops()\n            yield\n            cls.Ops = curr_Ops\n            cls.ops = curr_ops"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nevaluates the log - likelihood of two sets of keys.", "response": "def evaluate(self, X, y):\n        \"\"\"\n        x\n            Must match expected type\n            Must match expected shape\n        y\n            Must match expected type\n        \"\"\"\n        scores = self.ops.flatten(list(self.pipe(X)))\n        if not hasattr(y, \"shape\"):\n            y = self.ops.flatten(y)\n        scores = scores.reshape(y.shape)\n        if len(scores.shape) == 1:\n            correct = ((scores >= 0.5) == (y >= 0.5)).sum()\n        else:\n            correct = (scores.argmax(axis=1) == y.argmax(axis=1)).sum()\n        return correct / y.shape[0]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking we can learn to output a zero vector", "response": "def check_learns_zero_output_rnn(model, sgd, X, Y, initial_hidden=None):\n    \"\"\"Check we can learn to output a zero vector\"\"\"\n    outputs, get_dX = model.begin_update(X, initial_hidden)\n    Yh, h_n = outputs\n    tupleDy = (Yh - Y, h_n)\n    dX = get_dX(tupleDy, sgd=sgd)\n    prev = numpy.abs(Yh.sum())\n    print(prev)\n    for i in range(1000):\n        outputs, get_dX = model.begin_update(X)\n        Yh, h_n = outputs\n        current_sum = numpy.abs(Yh.sum())\n        tupleDy = (Yh - Y, h_n)\n        dX = get_dX(tupleDy, sgd=sgd)  # noqa: F841\n\n    # Should have decreased\n    print(current_sum)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef begin_update(self, x_data, drop=0.0):\n        x_var = torch.autograd.Variable(xp2torch(x_data), requires_grad=True)\n        # Make prediction\n\n        y_var = self._model(x_var)\n\n        def backward_pytorch(dy_data, sgd=None):\n            dy_var = xp2torch(dy_data)\n            torch.autograd.backward((y_var,), grad_tensors=(dy_var,))\n            if sgd is not None:\n                if self._optimizer is None:\n                    self._optimizer = self._create_optimizer(sgd)\n                self._optimizer.step()\n                self._optimizer.zero_grad()\n            return torch2xp(x_var.grad)\n\n        return torch2xp(y_var), backward_pytorch", "response": "Returns the output of the wrapped PyTorch model for the given input and a callback to handle the backward pass."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef begin_update(self, x_data, h_0=None, drop=0.0):\n        x_var = torch.autograd.Variable(xp2torch(x_data), requires_grad=True)\n\n        # Make prediction\n        out, h_n = self._model(x_var, h_0)\n        # Shapes will be:\n        # out = seq_len, batch, hidden_size * num_directions\n        # h_n = num_layers * num_directions, batch, hidden_size\n\n        def backward_pytorch_rnn(d_data, sgd=None):\n            dy_data, _ = d_data\n            dout = xp2torch(dy_data)\n            torch.autograd.backward((out,), grad_tensors=(dout,))\n            if sgd is not None:\n                if self._optimizer is None:\n                    self._optimizer = self._create_optimizer(sgd)\n                self._optimizer.step()\n                self._optimizer.zero_grad()\n            return torch2xp(x_var.grad)\n\n        return (torch2xp(out), h_n), backward_pytorch_rnn", "response": "Begin an update of the PyTorch model."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwraps a function into a layer", "response": "def layerize(begin_update=None, predict=None, *args, **kwargs):\n    \"\"\"Wrap a function into a layer\"\"\"\n    if begin_update is not None:\n        return FunctionLayer(begin_update, predict=predict, *args, **kwargs)\n\n    def wrapper(begin_update):\n        return FunctionLayer(begin_update, *args, **kwargs)\n\n    return wrapper"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef metalayerize(user_func):\n\n    def returned(layers, *args, **kwargs):\n        def begin_update(X, *args, **kwargs):\n            return user_func(layers, X, *args, **kwargs)\n\n        return FunctionLayer(begin_update, *args, **kwargs)\n\n    return returned", "response": "Wrap a function over a sequence of layers and an input into a layer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef noop(*layers):\n\n    def begin_update(X, drop=0.0):\n        return X, lambda D, *a, **k: D\n\n    return begin_update", "response": "Transform a sequences of layers into a null operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncomposes two models f and g such that they become layers of a single feed - forward model that computes g ( f x.", "response": "def chain(*layers):\n    \"\"\"Compose two models `f` and `g` such that they become layers of a single\n    feed-forward model that computes `g(f(x))`.\n\n    Raises exception if their dimensions don't match.\n    \"\"\"\n    if len(layers) == 0:\n        return FeedForward([])\n    elif len(layers) == 1:\n        return layers[0]\n    else:\n        return FeedForward(layers)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing n copies of a layer with distinct weights.", "response": "def clone(orig, n):\n    \"\"\"Construct `n` copies of a layer, with distinct weights.\n\n    i.e. `clone(f, 3)(x)` computes `f(f'(f''(x)))`.\n    \"\"\"\n    if n == 0:\n        return layerize(noop())\n    layers = [orig]\n    for i in range(n - 1):\n        layers.append(copy.deepcopy(orig))\n        layers[-1].set_id()\n    return FeedForward(layers)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef concatenate(*layers):  # pragma: no cover\n    if not layers:\n        return noop()\n    ops = layers[0].ops\n\n    def begin_update(X, *a, **k):\n        forward, backward = split_backward(layers)\n        values = [fwd(X, *a, **k) for fwd in forward]\n\n        output = ops.xp.hstack(values)\n        shapes = [val.shape for val in values]\n\n        def finish_update(gradient, *args, **kwargs):\n            layer_grads = []\n            start = 0\n            for bwd, shape in zip(backward, shapes):\n                end = start + shape[1]\n                if bwd is not None:\n                    d = bwd(\n                        ops.xp.ascontiguousarray(gradient[:, start:end]),\n                        *args,\n                        **kwargs\n                    )\n                    if d is not None and hasattr(X, \"shape\"):\n                        if not layer_grads:\n                            layer_grads.append(d)\n                        else:\n                            layer_grads[-1] += d\n                start = end\n            if layer_grads:\n                return ops.asarray(layer_grads[-1])\n            else:\n                return None\n\n        return output, finish_update\n\n    layer = FunctionLayer(begin_update)\n    layer._layers = list(layers)\n\n    def on_data(self, X, y=None):\n        for layer in self._layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X, y)\n\n    layer.on_data_hooks.append(on_data)\n    return layer", "response": "Concatenate two or more models f g etc."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nseparating a sequence of layers s begin_update methods into two lists of functions that computes the forward values and the other that completes .", "response": "def split_backward(layers):  # pragma: no cover\n    \"\"\"Separate a sequence of layers' `begin_update` methods into two lists of\n    functions: one that computes the forward values, and the other that completes\n    the backward pass. The backward sequence is only populated after the forward\n    functions have been applied.\n    \"\"\"\n    backward = []\n    forward = [sink_return(op.begin_update, backward.append) for op in layers]\n    return forward, backward"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntransforms a function func that returns tuples into a function that returns single values. Call a function sink on the unused values.", "response": "def sink_return(func, sink, splitter=None):  # pragma: no cover\n    \"\"\"Transform a function `func` that returns tuples into a function that returns\n    single values. Call a function `sink` on the unused values.\n    \"\"\"\n\n    def wrap(*args, **kwargs):\n        output = func(*args, **kwargs)\n        if splitter is None:\n            to_keep, to_sink = output\n        else:\n            to_keep, to_sink = splitter(*output)\n        sink(to_sink)\n        return to_keep\n\n    return wrap"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngroup inputs to a layer, so that the layer only has to compute for the unique values. The data is transformed back before output, and the same transformation is applied for the gradient. Effectively, this is a cache local to each minibatch. The uniqued wrapper is useful for word inputs, because common words are seen often, but we may want to compute complicated features for the words, using e.g. character LSTM.", "response": "def uniqued(layer, column=0):\n    \"\"\"Group inputs to a layer, so that the layer only has to compute\n    for the unique values. The data is transformed back before output, and the same\n    transformation is applied for the gradient. Effectively, this is a cache\n    local to each minibatch.\n\n    The uniqued wrapper is useful for word inputs, because common words are\n    seen often, but we may want to compute complicated features for the words,\n    using e.g. character LSTM.\n    \"\"\"\n\n    def uniqued_fwd(X, drop=0.0):\n        keys = X[:, column]\n        keys = layer.ops.xp.ascontiguousarray(keys)\n        if not isinstance(keys, numpy.ndarray):\n            keys = keys.get()\n        uniq_keys, ind, inv, counts = numpy.unique(\n            keys, return_index=True, return_inverse=True, return_counts=True\n        )\n        X_uniq = layer.ops.xp.ascontiguousarray(X[ind])\n        Y_uniq, bp_Y_uniq = layer.begin_update(X_uniq, drop=drop)\n        Y = Y_uniq[inv].reshape((X.shape[0],) + Y_uniq.shape[1:])\n\n        def uniqued_bwd(dY, sgd=None):\n            dY_uniq = layer.ops.allocate(Y_uniq.shape, dtype=\"f\")\n            layer.ops.scatter_add(dY_uniq, layer.ops.asarray(inv, dtype=\"i\"), dY)\n            d_uniques = bp_Y_uniq(dY_uniq, sgd=sgd)\n            if d_uniques is not None:\n                dX = (d_uniques / counts)[inv]\n                return dX\n            else:\n                return None\n\n        return Y, uniqued_bwd\n\n    model = wrap(uniqued_fwd, layer)\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmapping a layer across list items", "response": "def foreach(layer, drop_factor=1.0):\n    \"\"\"Map a layer across list items\"\"\"\n\n    def foreach_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc if len(sent)]\n            subset = [\n                s for s in doc_sents if numpy.random.random() >= drop * drop_factor\n            ]\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def foreach_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, foreach_bwd\n\n    model = wrap(foreach_fwd, layer)\n\n    def _run_foreach_child_hooks(model, X, y):\n        for layer in model._layers:\n            for hook in layer.on_data_hooks:\n                hook(layer, X[0], y[0])\n\n    model.on_data_hooks = [_run_foreach_child_hooks]\n\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nmap a layer across sentences.", "response": "def foreach_sentence(layer, drop_factor=1.0):\n    \"\"\"Map a layer across sentences (assumes spaCy-esque .sents interface)\"\"\"\n\n    def sentence_fwd(docs, drop=0.0):\n        sents = []\n        lengths = []\n        for doc in docs:\n            doc_sents = [sent for sent in doc.sents if len(sent)]\n            subset = [\n                s for s in doc_sents if numpy.random.random() >= drop * drop_factor\n            ]\n            if subset:\n                sents.extend(subset)\n                lengths.append(len(subset))\n            else:\n                numpy.random.shuffle(doc_sents)\n                sents.append(doc_sents[0])\n                lengths.append(1)\n        flat, bp_flat = layer.begin_update(sents, drop=0.0)\n        output = layer.ops.unflatten(flat, lengths)\n\n        def sentence_bwd(d_output, sgd=None):\n            d_flat = layer.ops.flatten(d_output)\n            d_sents = bp_flat(d_flat, sgd=sgd)\n            if d_sents is None:\n                return d_sents\n            else:\n                return layer.ops.unflatten(d_sents, lengths)\n\n        return output, sentence_bwd\n\n    model = wrap(sentence_fwd, layer)\n    return model"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef func_dump(func):\n    '''Serialize user defined function.'''\n    code = marshal.dumps(func.__code__).decode('raw_unicode_escape')\n    defaults = func.__defaults__\n    if func.__closure__:\n        closure = tuple(c.cell_contents for c in func.__closure__)\n    else:\n        closure = None\n    return code, defaults, closure", "response": "Serialize user defined function."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef func_load(code, defaults=None, closure=None, globs=None):\n    '''Deserialize user defined function.'''\n    if isinstance(code, (tuple, list)):  # unpack previous dump\n        code, defaults, closure = code\n    code = marshal.loads(code.encode('raw_unicode_escape'))\n    if globs is None:\n        globs = globals()\n    return python_types.FunctionType(code, globs,\n                                     name=code.co_name,\n                                     argdefs=defaults,\n                                     closure=closure)", "response": "Deserialize user defined function."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef preprocess(ops, nlp, rows, get_ids):\n    Xs = []\n    ys = []\n    for (text1, text2), label in rows:\n        Xs.append((get_ids([nlp(text1)])[0], get_ids([nlp(text2)])[0]))\n        ys.append(label)\n    return Xs, to_categorical(ys, nb_classes=2)", "response": "Parse the texts with spaCy. Make one - hot vectors for the labels."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates a bidirectional LSTM layer.", "response": "def BiLSTM(nO, nI):\n    \"\"\"Create a bidirectional LSTM layer. Args: number out, number in\"\"\"\n    return Bidirectional(LSTM(nO // 2, nI), LSTM(nO // 2, nI))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef LSTM(nO, nI):\n    weights = LSTM_weights(nO, nI)\n    gates = LSTM_gates(weights.ops)\n    return Recurrent(RNN_step(weights, gates))", "response": "Create an LSTM layer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef Bidirectional(l2r, r2l):\n    nO = l2r.nO\n\n    def birnn_fwd(Xs, drop=0.0):\n        l2r_Zs, bp_l2r_Zs = l2r.begin_update(Xs, drop=drop)\n        r2l_Zs, bp_r2l_Zs = r2l.begin_update(\n            [l2r.ops.xp.ascontiguousarray(X[::-1]) for X in Xs]\n        )\n\n        def birnn_bwd(dZs, sgd=None):\n            d_l2r_Zs = []\n            d_r2l_Zs = []\n            for dZ in dZs:\n                l2r_fwd = dZ[:, :nO]\n                r2l_fwd = dZ[:, nO:]\n                d_l2r_Zs.append(l2r.ops.xp.ascontiguousarray(l2r_fwd))\n                d_r2l_Zs.append(l2r.ops.xp.ascontiguousarray(r2l_fwd[::-1]))\n            dXs_l2r = bp_l2r_Zs(d_l2r_Zs, sgd=sgd)\n            dXs_r2l = bp_r2l_Zs(d_r2l_Zs, sgd=sgd)\n            dXs = [dXf + dXb[::-1] for dXf, dXb in zip(dXs_l2r, dXs_r2l)]\n            return dXs\n\n        Zs = [l2r.ops.xp.hstack((Zf, Zb[::-1])) for Zf, Zb in zip(l2r_Zs, r2l_Zs)]\n        return Zs, birnn_bwd\n\n    return wrap(birnn_fwd, l2r, r2l)", "response": "Stitch two LNN models into a bidirectional layer."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef RNN_step(weights, gates):\n\n    def rnn_step_fwd(prevstate_inputs, drop=0.0):\n        prevstate, inputs = prevstate_inputs\n        cell_tm1, hidden_tm1 = prevstate\n\n        acts, bp_acts = weights.begin_update((inputs, hidden_tm1), drop=drop)\n        (cells, hiddens), bp_gates = gates.begin_update((acts, cell_tm1), drop=drop)\n\n        def rnn_step_bwd(d_state_d_hiddens, sgd=None):\n            (d_cells, d_hiddens), d_hiddens = d_state_d_hiddens\n            d_acts, d_cell_tm1 = bp_gates((d_cells, d_hiddens), sgd=sgd)\n            d_inputs, d_hidden_tm1 = bp_acts(d_acts, sgd=sgd)\n            return (d_cell_tm1, d_hidden_tm1), d_inputs\n\n        return ((cells, hiddens), hiddens), rnn_step_bwd\n\n    model = wrap(rnn_step_fwd, weights, gates)\n    model.nO = weights.nO\n    model.nI = weights.nI\n    model.weights = weights\n    model.gates = gates\n    return model", "response": "Create a step model for an RNN given weights and gates functions."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndownload a file from a URL if it not already in the cache.", "response": "def get_file(fname, origin, untar=False, unzip=False,\n             md5_hash=None, cache_subdir='datasets'):\n    '''Downloads a file from a URL if it not already in the cache.\n\n    Passing the MD5 hash will verify the file after download as well as if it is already present in the cache.\n\n    # Arguments\n        fname: name of the file\n        origin: original URL of the file\n        untar: boolean, whether the file should be decompressed\n        md5_hash: MD5 hash of the file for verification\n        cache_subdir: directory being used as the cache\n\n    # Returns\n        Path to the downloaded file\n    '''\n    datadir_base = os.path.expanduser(os.path.join('~', '.keras'))\n    if not os.access(datadir_base, os.W_OK):\n        datadir_base = os.path.join('/tmp', '.keras')\n    datadir = os.path.join(datadir_base, cache_subdir)\n    if not os.path.exists(datadir):\n        os.makedirs(datadir)\n\n    if untar or unzip:\n        untar_fpath = os.path.join(datadir, fname)\n        if unzip:\n            fpath = untar_fpath + '.zip'\n        else:\n            fpath = untar_fpath + '.tar.gz'\n    else:\n        fpath = os.path.join(datadir, fname)\n\n    download = False\n    if os.path.exists(fpath):\n        # file found; verify integrity if a hash was provided\n        if md5_hash is not None:\n            if not validate_file(fpath, md5_hash):\n                print('A local file was found, but it seems to be '\n                      'incomplete or outdated.')\n                download = True\n    else:\n        download = True\n\n    if download:\n        print('Downloading data from', origin)\n        global progbar\n        progbar = None\n\n        def dl_progress(count, block_size, total_size):\n            global progbar\n            if progbar is None:\n                progbar = Progbar(total_size)\n            else:\n                progbar.update(count * block_size)\n\n        error_msg = 'URL fetch failure on {}: {} -- {}'\n        try:\n            try:\n                urlretrieve(origin, fpath, dl_progress)\n            except URLError as e:\n                raise Exception(error_msg.format(origin, e.errno, e.reason))\n            except HTTPError as e:\n                raise Exception(error_msg.format(origin, e.code, e.msg))\n        except (Exception, KeyboardInterrupt) as e:\n            if os.path.exists(fpath):\n                os.remove(fpath)\n            raise\n        progbar = None\n\n    if untar:\n        if not os.path.exists(untar_fpath):\n            print('Untaring file...')\n            tfile = tarfile.open(fpath, 'r:gz')\n            try:\n                tfile.extractall(path=datadir)\n            except (Exception, KeyboardInterrupt) as e:\n                if os.path.exists(untar_fpath):\n                    if os.path.isfile(untar_fpath):\n                        os.remove(untar_fpath)\n                    else:\n                        shutil.rmtree(untar_fpath)\n                raise\n            tfile.close()\n        return untar_fpath\n    elif unzip:\n        if not os.path.exists(untar_fpath):\n            print('Unzipping file...')\n            with zipfile.ZipFile(fpath) as file_:\n                try:\n                    file_.extractall(path=datadir)\n                except (Exception, KeyboardInterrupt) as e:\n                    if os.path.exists(untar_fpath):\n                        if os.path.isfile(untar_fpath):\n                            os.remove(untar_fpath)\n                        else:\n                            shutil.rmtree(untar_fpath)\n                    raise\n        return untar_fpath\n\n    return fpath"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef validate_file(fpath, md5_hash):\n    '''Validates a file against a MD5 hash\n\n    # Arguments\n        fpath: path to the file being validated\n        md5_hash: the MD5 hash being validated against\n\n    # Returns\n        Whether the file is valid\n    '''\n    hasher = hashlib.md5()\n    with open(fpath, 'rb') as f:\n        buf = f.read()\n        hasher.update(buf)\n    if str(hasher.hexdigest()) == str(md5_hash):\n        return True\n    else:\n        return False", "response": "Validates a file against a MD5 hash"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef track_progress(**context):\n    model = context[\"model\"]\n    train_X = context[\"train_X\"]\n    dev_X = context[\"dev_X\"]\n    dev_y = context[\"dev_y\"]\n    n_train = len(train_X)\n    trainer = context[\"trainer\"]\n\n    def each_epoch():\n        global epoch_train_acc, epoch\n        with model.use_params(trainer.optimizer.averages):\n            avg_acc = model.evaluate_logloss(dev_X, dev_y)\n        stats = (avg_acc, float(epoch_train_acc) / n_train, trainer.dropout)\n        print(\"%.3f dev acc, %.3f train acc, %.4f drop\" % stats)\n        epoch_train_acc = 0.0\n        epoch += 1\n\n    return each_epoch", "response": "Print training progress. Called after each epoch."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the texts with spaCy. Make one - hot vectors for the labels.", "response": "def preprocess(ops, nlp, rows, get_ids):\n    \"\"\"Parse the texts with spaCy. Make one-hot vectors for the labels.\"\"\"\n    Xs = []\n    ys = []\n    for (text1, text2), label in rows:\n        Xs.append((get_ids([nlp(text1)])[0], get_ids([nlp(text2)])[0]))\n        ys.append(label)\n    return Xs, ops.asarray(ys, dtype=\"float32\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_cupy_array(arr):\n    if cupy is None:\n        return False\n    elif isinstance(arr, cupy.ndarray):\n        return True\n    else:\n        return False", "response": "Check whether an array is a cupy array"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef prefer_gpu():\n    from ._classes.model import Model\n    from .ops import CupyOps\n\n    if CupyOps.xp is not None:\n        Model.Ops = CupyOps\n        Model.ops = CupyOps()\n        return True\n    else:\n        return False", "response": "Use GPU if it s available. Returns True if so False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\npad a list of sequences with EOL markers.", "response": "def mark_sentence_boundaries(sequences, drop=0.0):  # pragma: no cover\n    \"\"\"Pad sentence sequences with EOL markers.\"\"\"\n    for sequence in sequences:\n        sequence.insert(0, \"-EOL-\")\n        sequence.insert(0, \"-EOL-\")\n        sequence.append(\"-EOL-\")\n        sequence.append(\"-EOL-\")\n    return sequences, None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a text message to the broker.", "response": "def send_text(self, message, opcode=OPCODE_TEXT):\n        \"\"\"\n        Important: Fragmented(=continuation) messages are not supported since\n        their usage cases are limited - when we don't know the payload length.\n        \"\"\"\n\n        # Validate message\n        if isinstance(message, bytes):\n            message = try_decode_UTF8(message)  # this is slower but ensures we have UTF-8\n            if not message:\n                logger.warning(\"Can\\'t send message, message is not valid UTF-8\")\n                return False\n        elif sys.version_info < (3,0) and (isinstance(message, str) or isinstance(message, unicode)):\n            pass\n        elif isinstance(message, str):\n            pass\n        else:\n            logger.warning('Can\\'t send message, message has to be a string or bytes. Given type is %s' % type(message))\n            return False\n\n        header  = bytearray()\n        payload = encode_to_UTF8(message)\n        payload_length = len(payload)\n\n        # Normal payload\n        if payload_length <= 125:\n            header.append(FIN | opcode)\n            header.append(payload_length)\n\n        # Extended payload\n        elif payload_length >= 126 and payload_length <= 65535:\n            header.append(FIN | opcode)\n            header.append(PAYLOAD_LEN_EXT16)\n            header.extend(struct.pack(\">H\", payload_length))\n\n        # Huge extended payload\n        elif payload_length < 18446744073709551616:\n            header.append(FIN | opcode)\n            header.append(PAYLOAD_LEN_EXT64)\n            header.extend(struct.pack(\">Q\", payload_length))\n\n        else:\n            raise Exception(\"Message is too big. Consider breaking it into chunks.\")\n            return\n\n        self.request.send(header + payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getFilepaths(self, filename):\n        return (os.path.join(os.environ['HOME'], filename),\n                os.path.join(self.mackup.mackup_folder, filename))", "response": "Get home and mackup filepaths for given file"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef restore(self):\n        # For each file used by the application\n        for filename in self.files:\n            (home_filepath, mackup_filepath) = self.getFilepaths(filename)\n\n            # If the file exists and is not already pointing to the mackup file\n            # and the folder makes sense on the current platform (Don't sync\n            # any subfolder of ~/Library on GNU/Linux)\n            file_or_dir_exists = (os.path.isfile(mackup_filepath) or\n                                  os.path.isdir(mackup_filepath))\n            pointing_to_mackup = (os.path.islink(home_filepath) and\n                                  os.path.exists(mackup_filepath) and\n                                  os.path.samefile(mackup_filepath,\n                                                   home_filepath))\n            supported = utils.can_file_be_synced_on_current_platform(filename)\n\n            if file_or_dir_exists and not pointing_to_mackup and supported:\n                if self.verbose:\n                    print(\"Restoring\\n  linking {}\\n  to      {} ...\"\n                          .format(home_filepath, mackup_filepath))\n                else:\n                    print(\"Restoring {} ...\".format(filename))\n\n                if self.dry_run:\n                    continue\n\n                # Check if there is already a file in the home folder\n                if os.path.exists(home_filepath):\n                    # Name it right\n                    if os.path.isfile(home_filepath):\n                        file_type = 'file'\n                    elif os.path.isdir(home_filepath):\n                        file_type = 'folder'\n                    elif os.path.islink(home_filepath):\n                        file_type = 'link'\n                    else:\n                        raise ValueError(\"Unsupported file: {}\"\n                                         .format(mackup_filepath))\n\n                    if utils.confirm(\"You already have a {} named {} in your\"\n                                     \" home.\\nDo you want to replace it with\"\n                                     \" your backup ?\"\n                                     .format(file_type, filename)):\n                        utils.delete(home_filepath)\n                        utils.link(mackup_filepath, home_filepath)\n                else:\n                    utils.link(mackup_filepath, home_filepath)\n            elif self.verbose:\n                if os.path.exists(home_filepath):\n                    print(\"Doing nothing\\n  {}\\n  already linked by\\n  {}\"\n                          .format(mackup_filepath, home_filepath))\n                elif os.path.islink(home_filepath):\n                    print(\"Doing nothing\\n  {}\\n  \"\n                          \"is a broken link, you might want to fix it.\"\n                          .format(home_filepath))\n                else:\n                    print(\"Doing nothing\\n  {}\\n  does not exist\"\n                          .format(mackup_filepath))", "response": "Restores the application config files."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef uninstall(self):\n        # For each file used by the application\n        for filename in self.files:\n            (home_filepath, mackup_filepath) = self.getFilepaths(filename)\n\n            # If the mackup file exists\n            if (os.path.isfile(mackup_filepath) or\n                    os.path.isdir(mackup_filepath)):\n                # Check if there is a corresponding file in the home folder\n                if os.path.exists(home_filepath):\n                    if self.verbose:\n                        print(\"Reverting {}\\n  at {} ...\"\n                              .format(mackup_filepath, home_filepath))\n                    else:\n                        print(\"Reverting {} ...\".format(filename))\n\n                    if self.dry_run:\n                        continue\n\n                    # If there is, delete it as we are gonna copy the Dropbox\n                    # one there\n                    utils.delete(home_filepath)\n\n                    # Copy the Dropbox file to the home folder\n                    utils.copy(mackup_filepath, home_filepath)\n            elif self.verbose:\n                print(\"Doing nothing, {} does not exist\"\n                      .format(mackup_filepath))", "response": "Uninstalls the Mackup backup and copies the Dropbox file to the home folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the current environment is usable and has everything s required.", "response": "def check_for_usable_environment(self):\n        \"\"\"Check if the current env is usable and has everything's required.\"\"\"\n        # Do not let the user run Mackup as root\n        if os.geteuid() == 0:\n            utils.error(\"Running Mackup as a superuser is useless and\"\n                        \" dangerous. Don't do it!\")\n\n        # Do we have a folder to put the Mackup folder ?\n        if not os.path.isdir(self._config.path):\n            utils.error(\"Unable to find the storage folder: {}\"\n                        .format(self._config.path))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the current env can be used to restore files.", "response": "def check_for_usable_restore_env(self):\n        \"\"\"Check if the current env can be used to restore files.\"\"\"\n        self.check_for_usable_environment()\n\n        if not os.path.isdir(self.mackup_folder):\n            utils.error(\"Unable to find the Mackup folder: {}\\n\"\n                        \"You might want to back up some files or get your\"\n                        \" storage directory synced first.\"\n                        .format(self.mackup_folder))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates the Mackup home folder if it doesn t exist.", "response": "def create_mackup_home(self):\n        \"\"\"If the Mackup home folder does not exist, create it.\"\"\"\n        if not os.path.isdir(self.mackup_folder):\n            if utils.confirm(\"Mackup needs a directory to store your\"\n                             \" configuration files\\n\"\n                             \"Do you want to create it now? <{}>\"\n                             .format(self.mackup_folder)):\n                os.makedirs(self.mackup_folder)\n            else:\n                utils.error(\"Mackup can't do anything without a home =(\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the list of applications that should be backed up by Mackup.", "response": "def get_apps_to_backup(self):\n        \"\"\"\n        Get the list of applications that should be backed up by Mackup.\n\n        It's the list of allowed apps minus the list of ignored apps.\n\n        Returns:\n            (set) List of application names to back up\n        \"\"\"\n        # Instantiate the app db\n        app_db = appsdb.ApplicationsDatabase()\n\n        # If a list of apps to sync is specify, we only allow those\n        # Or we allow every supported app by default\n        apps_to_backup = self._config.apps_to_sync or app_db.get_app_names()\n\n        # Remove the specified apps to ignore\n        for app_name in self._config.apps_to_ignore:\n            apps_to_backup.discard(app_name)\n\n        return apps_to_backup"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the list of configuration files describing the apps supported by the current application.", "response": "def get_config_files():\n        \"\"\"\n        Return the application configuration files.\n\n        Return a list of configuration files describing the apps supported by\n        Mackup. The files return are absolute full path to those files.\n        e.g. /usr/lib/mackup/applications/bash.cfg\n\n        Only one config file per application should be returned, custom config\n        having a priority over stock config.\n\n        Returns:\n            set of strings.\n        \"\"\"\n        # Configure the config parser\n        apps_dir = os.path.join(os.path.dirname(os.path.realpath(__file__)),\n                                APPS_DIR)\n        custom_apps_dir = os.path.join(os.environ['HOME'], CUSTOM_APPS_DIR)\n\n        # List of stock application config files\n        config_files = set()\n\n        # Temp list of user added app config file names\n        custom_files = set()\n\n        # Get the list of custom application config files first\n        if os.path.isdir(custom_apps_dir):\n            for filename in os.listdir(custom_apps_dir):\n                if filename.endswith('.cfg'):\n                    config_files.add(os.path.join(custom_apps_dir,\n                                                  filename))\n                    # Also add it to the set of custom apps, so that we don't\n                    # add the stock config for the same app too\n                    custom_files.add(filename)\n\n        # Add the default provided app config files, but only if those are not\n        # customized, as we don't want to overwrite custom app config.\n        for filename in os.listdir(apps_dir):\n            if filename.endswith('.cfg') and filename not in custom_files:\n                config_files.add(os.path.join(apps_dir, filename))\n\n        return config_files"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_app_names(self):\n        app_names = set()\n        for name in self.apps:\n            app_names.add(name)\n\n        return app_names", "response": "Return the list of application names that are available in the database."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the list of pretty app names that are available in the database.", "response": "def get_pretty_app_names(self):\n        \"\"\"\n        Return the list of pretty app names that are available in the database.\n\n        Returns:\n            set of str.\n        \"\"\"\n        pretty_app_names = set()\n        for app_name in self.get_app_names():\n            pretty_app_names.add(self.get_name(app_name))\n\n        return pretty_app_names"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef confirm(question):\n    if FORCE_YES:\n        return True\n\n    while True:\n        answer = input(question + ' <Yes|No>').lower()\n\n        if answer == 'yes' or answer == 'y':\n            confirmed = True\n            break\n        if answer == 'no' or answer == 'n':\n            confirmed = False\n            break\n\n    return confirmed", "response": "Ask the user if he really want something to happen."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef delete(filepath):\n    # Some files have ACLs, let's remove them recursively\n    remove_acl(filepath)\n\n    # Some files have immutable attributes, let's remove them recursively\n    remove_immutable_attribute(filepath)\n\n    # Finally remove the files and folders\n    if os.path.isfile(filepath) or os.path.islink(filepath):\n        os.remove(filepath)\n    elif os.path.isdir(filepath):\n        shutil.rmtree(filepath)", "response": "Delete the given file directory or link."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncopies a file or folder from src to dst.", "response": "def copy(src, dst):\n    \"\"\"\n    Copy a file or a folder (recursively) from src to dst.\n\n    For simplicity sake, both src and dst must be absolute path and must\n    include the filename of the file or folder.\n    Also do not include any trailing slash.\n\n    e.g. copy('/path/to/src_file', '/path/to/dst_file')\n    or copy('/path/to/src_folder', '/path/to/dst_folder')\n\n    But not: copy('/path/to/src_file', 'path/to/')\n    or copy('/path/to/src_folder/', '/path/to/dst_folder')\n\n    Args:\n        src (str): Source file or folder\n        dst (str): Destination file or folder\n    \"\"\"\n    assert isinstance(src, str)\n    assert os.path.exists(src)\n    assert isinstance(dst, str)\n\n    # Create the path to the dst file if it does not exists\n    abs_path = os.path.dirname(os.path.abspath(dst))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    # We need to copy a single file\n    if os.path.isfile(src):\n        # Copy the src file to dst\n        shutil.copy(src, dst)\n\n    # We need to copy a whole folder\n    elif os.path.isdir(src):\n        shutil.copytree(src, dst)\n\n    # What the heck is this ?\n    else:\n        raise ValueError(\"Unsupported file: {}\".format(src))\n\n    # Set the good mode to the file or folder recursively\n    chmod(dst)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a link to a target file or folder.", "response": "def link(target, link_to):\n    \"\"\"\n    Create a link to a target file or a folder.\n\n    For simplicity sake, both target and link_to must be absolute path and must\n    include the filename of the file or folder.\n    Also do not include any trailing slash.\n\n    e.g. link('/path/to/file', '/path/to/link')\n\n    But not: link('/path/to/file', 'path/to/')\n    or link('/path/to/folder/', '/path/to/link')\n\n    Args:\n        target (str): file or folder the link will point to\n        link_to (str): Link to create\n    \"\"\"\n    assert isinstance(target, str)\n    assert os.path.exists(target)\n    assert isinstance(link_to, str)\n\n    # Create the path to the link if it does not exists\n    abs_path = os.path.dirname(os.path.abspath(link_to))\n    if not os.path.isdir(abs_path):\n        os.makedirs(abs_path)\n\n    # Make sure the file or folder recursively has the good mode\n    chmod(target)\n\n    # Create the link to target\n    os.symlink(target, link_to)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nthrow an error with the given message and immediately quit.", "response": "def error(message):\n    \"\"\"\n    Throw an error with the given message and immediately quit.\n\n    Args:\n        message(str): The message to display.\n    \"\"\"\n    fail = '\\033[91m'\n    end = '\\033[0m'\n    sys.exit(fail + \"Error: {}\".format(message) + end)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntrying to locate the Dropbox folder.", "response": "def get_dropbox_folder_location():\n    \"\"\"\n    Try to locate the Dropbox folder.\n\n    Returns:\n        (str) Full path to the current Dropbox folder\n    \"\"\"\n    host_db_path = os.path.join(os.environ['HOME'], '.dropbox/host.db')\n    try:\n        with open(host_db_path, 'r') as f_hostdb:\n            data = f_hostdb.read().split()\n    except IOError:\n        error(\"Unable to find your Dropbox install =(\")\n    dropbox_home = base64.b64decode(data[1]).decode()\n\n    return dropbox_home"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_google_drive_folder_location():\n    gdrive_db_path = 'Library/Application Support/Google/Drive/sync_config.db'\n    yosemite_gdrive_db_path = ('Library/Application Support/Google/Drive/'\n                               'user_default/sync_config.db')\n    yosemite_gdrive_db = os.path.join(os.environ['HOME'],\n                                      yosemite_gdrive_db_path)\n    if os.path.isfile(yosemite_gdrive_db):\n        gdrive_db_path = yosemite_gdrive_db\n\n    googledrive_home = None\n\n    gdrive_db = os.path.join(os.environ['HOME'], gdrive_db_path)\n    if os.path.isfile(gdrive_db):\n        con = sqlite3.connect(gdrive_db)\n        if con:\n            cur = con.cursor()\n            query = (\"SELECT data_value \"\n                     \"FROM data \"\n                     \"WHERE entry_key = 'local_sync_root_path';\")\n            cur.execute(query)\n            data = cur.fetchone()\n            googledrive_home = str(data[0])\n            con.close()\n\n    if not googledrive_home:\n        error(\"Unable to find your Google Drive install =(\")\n\n    return googledrive_home", "response": "Try to locate the Google Drive folder."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_box_folder_location():\n    box_prefs_path = ('Library/Application Support/Box/Box Sync/'\n                      'sync_root_folder.txt')\n    box_home = None\n\n    box_prefs = os.path.join(os.environ['HOME'], box_prefs_path)\n    try:\n        with open(box_prefs, 'r') as sync_path:\n            data = sync_path.read()\n            box_home = data\n    except IOError:\n        error(\"Unable to find your Box prefs =(\")\n\n    return box_home", "response": "Try to locate the Box folder."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_copy_folder_location():\n    copy_settings_path = 'Library/Application Support/Copy Agent/config.db'\n    copy_home = None\n\n    copy_settings = os.path.join(os.environ['HOME'], copy_settings_path)\n\n    if os.path.isfile(copy_settings):\n        database = sqlite3.connect(copy_settings)\n        if database:\n            cur = database.cursor()\n            query = (\"SELECT value \"\n                     \"FROM config2 \"\n                     \"WHERE option = 'csmRootPath';\")\n            cur.execute(query)\n            data = cur.fetchone()\n            copy_home = str(data[0])\n            cur.close()\n\n    if not copy_home:\n        error(\"Unable to find your Copy install =(\")\n\n    return copy_home", "response": "Try to locate the Copy folder."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntry to locate the iCloud Drive folder.", "response": "def get_icloud_folder_location():\n    \"\"\"\n    Try to locate the iCloud Drive folder.\n\n    Returns:\n        (str) Full path to the iCloud Drive folder.\n    \"\"\"\n    yosemite_icloud_path = '~/Library/Mobile Documents/com~apple~CloudDocs/'\n\n    icloud_home = os.path.expanduser(yosemite_icloud_path)\n\n    if not os.path.isdir(icloud_home):\n        error('Unable to find your iCloud Drive =(')\n\n    return str(icloud_home)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a process with the given name is running.", "response": "def is_process_running(process_name):\n    \"\"\"\n    Check if a process with the given name is running.\n\n    Args:\n        (str): Process name, e.g. \"Sublime Text\"\n\n    Returns:\n        (bool): True if the process is running\n    \"\"\"\n    is_running = False\n\n    # On systems with pgrep, check if the given process is running\n    if os.path.isfile('/usr/bin/pgrep'):\n        dev_null = open(os.devnull, 'wb')\n        returncode = subprocess.call(['/usr/bin/pgrep', process_name],\n                                     stdout=dev_null)\n        is_running = bool(returncode == 0)\n\n    return is_running"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove the ACL of the file or folder at the given path.", "response": "def remove_acl(path):\n    \"\"\"\n    Remove the ACL of the file or folder located on the given path.\n\n    Also remove the ACL of any file and folder below the given one,\n    recursively.\n\n    Args:\n        path (str): Path to the file or folder to remove the ACL for,\n                    recursively.\n    \"\"\"\n    # Some files have ACLs, let's remove them recursively\n    if (platform.system() == constants.PLATFORM_DARWIN and\n            os.path.isfile('/bin/chmod')):\n        subprocess.call(['/bin/chmod', '-R', '-N', path])\n    elif ((platform.system() == constants.PLATFORM_LINUX) and\n            os.path.isfile('/bin/setfacl')):\n        subprocess.call(['/bin/setfacl', '-R', '-b', path])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the given path can be synced locally on the current platform.", "response": "def can_file_be_synced_on_current_platform(path):\n    \"\"\"\n    Check if the given path can be synced locally.\n\n    Check if it makes sense to sync the file at the given path on the current\n    platform.\n    For now we don't sync any file in the ~/Library folder on GNU/Linux.\n    There might be other exceptions in the future.\n\n    Args:\n        (str): Path to the file or folder to check. If relative, prepend it\n               with the home folder.\n               'abc' becomes '~/abc'\n               '/def' stays '/def'\n\n    Returns:\n        (bool): True if given file can be synced\n    \"\"\"\n    can_be_synced = True\n\n    # If the given path is relative, prepend home\n    fullpath = os.path.join(os.environ['HOME'], path)\n\n    # Compute the ~/Library path on OS X\n    # End it with a slash because we are looking for this specific folder and\n    # not any file/folder named LibrarySomething\n    library_path = os.path.join(os.environ['HOME'], 'Library/')\n\n    if platform.system() == constants.PLATFORM_LINUX:\n        if fullpath.startswith(library_path):\n            can_be_synced = False\n\n    return can_be_synced"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef fullpath(self):\n        return str(os.path.join(self.path, self.directory))", "response": "Returns the full path to the Mackup configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _setup_parser(self, filename=None):\n        assert isinstance(filename, str) or filename is None\n\n        # If we are not overriding the config filename\n        if not filename:\n            filename = MACKUP_CONFIG_FILE\n\n        parser = configparser.SafeConfigParser(allow_no_value=True)\n        parser.read(os.path.join(os.path.join(os.environ['HOME'], filename)))\n\n        return parser", "response": "Setup the ConfigParser instance to parse the config file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _warn_on_old_config(self):\n        # Is an old setion is in the config file ?\n        old_sections = ['Allowed Applications', 'Ignored Applications']\n        for old_section in old_sections:\n            if self._parser.has_section(old_section):\n                error(\"Old config file detected. Aborting.\\n\"\n                      \"\\n\"\n                      \"An old section (e.g. [Allowed Applications]\"\n                      \" or [Ignored Applications] has been detected\"\n                      \" in your {} file.\\n\"\n                      \"I'd rather do nothing than do something you\"\n                      \" do not want me to do.\\n\"\n                      \"\\n\"\n                      \"Please read the up to date documentation on\"\n                      \" <https://github.com/lra/mackup> and migrate\"\n                      \" your configuration file.\"\n                      .format(MACKUP_CONFIG_FILE))", "response": "Warn the user if an old config format is detected."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _parse_engine(self):\n        if self._parser.has_option('storage', 'engine'):\n            engine = str(self._parser.get('storage', 'engine'))\n        else:\n            engine = ENGINE_DROPBOX\n\n        assert isinstance(engine, str)\n\n        if engine not in [ENGINE_DROPBOX,\n                          ENGINE_GDRIVE,\n                          ENGINE_COPY,\n                          ENGINE_ICLOUD,\n                          ENGINE_BOX,\n                          ENGINE_FS]:\n            raise ConfigError('Unknown storage engine: {}'.format(engine))\n\n        return str(engine)", "response": "Parse the storage engine in the config."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the storage path in the config.", "response": "def _parse_path(self):\n        \"\"\"\n        Parse the storage path in the config.\n\n        Returns:\n            str\n        \"\"\"\n        if self.engine == ENGINE_DROPBOX:\n            path = get_dropbox_folder_location()\n        elif self.engine == ENGINE_GDRIVE:\n            path = get_google_drive_folder_location()\n        elif self.engine == ENGINE_COPY:\n            path = get_copy_folder_location()\n        elif self.engine == ENGINE_ICLOUD:\n            path = get_icloud_folder_location()\n        elif self.engine == ENGINE_BOX:\n            path = get_box_folder_location()\n        elif self.engine == ENGINE_FS:\n            if self._parser.has_option('storage', 'path'):\n                cfg_path = self._parser.get('storage', 'path')\n                path = os.path.join(os.environ['HOME'], cfg_path)\n            else:\n                raise ConfigError(\"The required 'path' can't be found while\"\n                                  \" the 'file_system' engine is used.\")\n\n        return str(path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nparse the storage directory in the config.", "response": "def _parse_directory(self):\n        \"\"\"\n        Parse the storage directory in the config.\n\n        Returns:\n            str\n        \"\"\"\n        if self._parser.has_option('storage', 'directory'):\n            directory = self._parser.get('storage', 'directory')\n            # Don't allow CUSTOM_APPS_DIR as a storage directory\n            if directory == CUSTOM_APPS_DIR:\n                raise ConfigError(\"{} cannot be used as a storage directory.\"\n                                  .format(CUSTOM_APPS_DIR))\n        else:\n            directory = MACKUP_BACKUP_PATH\n\n        return str(directory)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nparses the applications to ignore in the config file.", "response": "def _parse_apps_to_ignore(self):\n        \"\"\"\n        Parse the applications to ignore in the config.\n\n        Returns:\n            set\n        \"\"\"\n        # We ignore nothing by default\n        apps_to_ignore = set()\n\n        # Is the \"[applications_to_ignore]\" in the cfg file ?\n        section_title = 'applications_to_ignore'\n        if self._parser.has_section(section_title):\n            apps_to_ignore = set(self._parser.options(section_title))\n\n        return apps_to_ignore"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing the applications to backup in the config file.", "response": "def _parse_apps_to_sync(self):\n        \"\"\"\n        Parse the applications to backup in the config.\n\n        Returns:\n            set\n        \"\"\"\n        # We allow nothing by default\n        apps_to_sync = set()\n\n        # Is the \"[applications_to_sync]\" section in the cfg file ?\n        section_title = 'applications_to_sync'\n        if self._parser.has_section(section_title):\n            apps_to_sync = set(self._parser.options(section_title))\n\n        return apps_to_sync"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef load_template(path_or_buffer):\n    '''Build tabula-py option from template file\n\n    Args:\n        file_like_obj: File like object of Tabula app template\n\n    Returns:\n        `obj`:dict: tabula-py options\n    '''\n\n    from itertools import groupby\n    from operator import itemgetter\n\n    path_or_buffer = _stringify_path(path_or_buffer)\n\n    if is_file_like(path_or_buffer):\n        templates = json.load(path_or_buffer)\n    else:\n        with open(path_or_buffer, 'r') as f:\n            templates = json.load(f)\n\n    options = []\n\n    grouper = itemgetter('page', 'extraction_method')\n\n    for key, grp in groupby(sorted(templates, key=grouper), grouper):\n        tmp_options = [_convert_template_option(e) for e in grp]\n\n        if len(tmp_options) == 1:\n            options.append(tmp_options[0])\n            continue\n\n        option = tmp_options[0]\n        areas = [e.get('area') for e in tmp_options]\n        option['area'] = areas\n        option['multiple_tables'] = True\n        options.append(option)\n\n    return options", "response": "Build tabula - py option from template file"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _convert_template_option(template):\n    '''Convert Tabula app template to tabula-py option\n\n    Args:\n        template (dict): Tabula app template\n\n    Returns:\n        `obj`:dict: tabula-py option\n    '''\n\n    option = {}\n    extraction_method = template.get('extraction_method')\n    if extraction_method == 'guess':\n        option['guess'] = True\n    elif extraction_method == 'lattice':\n        option['lattice'] = True\n    elif extraction_method == 'stream':\n        option['stream'] = True\n\n    option['pages'] = template.get('page')\n    option['area'] = [round(template['y1'], 3), round(template['x1'], 3), round(template['y2'], 3), round(template['x2'], 3)]\n\n    return option", "response": "Convert Tabula app template to tabula - py option."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls tabula - java with the given lists of Java options and tabula - py options and return the output.", "response": "def _run(java_options, options, path=None, encoding='utf-8'):\n    \"\"\"Call tabula-java with the given lists of Java options and tabula-py\n    options, as well as an optional path to pass to tabula-java as a regular\n    argument and an optional encoding to use for any required output sent to\n    stderr.\n\n    tabula-py options are translated into tabula-java options, see\n    :func:`build_options` for more information.\n    \"\"\"\n    # Workaround to enforce the silent option. See:\n    # https://github.com/tabulapdf/tabula-java/issues/231#issuecomment-397281157\n    if 'silent' in options:\n        java_options.extend((\n            '-Dorg.slf4j.simpleLogger.defaultLogLevel=off',\n            '-Dorg.apache.commons.logging.Log=org.apache.commons.logging.impl.NoOpLog',\n        ))\n\n    built_options = build_options(options)\n    args = [\"java\"] + java_options + [\"-jar\", _jar_path()] + built_options\n    if path:\n        args.append(path)\n\n    try:\n        return subprocess.check_output(args)\n    except FileNotFoundError as e:\n        raise JavaNotFoundError(JAVA_NOT_FOUND_ERROR)\n    except subprocess.CalledProcessError as e:\n        sys.stderr.write(\"Error: {}\\n\".format(e.output.decode(encoding)))\n        raise"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_pdf(input_path,\n             output_format='dataframe',\n             encoding='utf-8',\n             java_options=None,\n             pandas_options=None,\n             multiple_tables=False, **kwargs):\n    '''Read tables in PDF.\n\n    Args:\n        input_path (file like obj):\n            File like object of tareget PDF file.\n        output_format (str, optional):\n            Output format of this function (dataframe or json)\n        encoding (str, optional):\n            Encoding type for pandas. Default is 'utf-8'\n        java_options (list, optional):\n            Set java options like `-Xmx256m`.\n        pandas_options (dict, optional):\n            Set pandas options like {'header': None}.\n        multiple_tables (bool, optional):\n            This is experimental option. It enables to handle multple tables within a page.\n            Note: If `multiple_tables` option is enabled, tabula-py uses not `pd.read_csv()`,\n             but `pd.DataFrame()`. Make sure to pass appropreate `pandas_options`.\n        kwargs (dict):\n            Dictionary of option for tabula-java. Details are shown in `build_options()`\n\n    Returns:\n        Extracted pandas DataFrame or list.\n    '''\n\n    if output_format == 'dataframe':\n        kwargs.pop('format', None)\n\n    elif output_format == 'json':\n        kwargs['format'] = 'JSON'\n\n    if multiple_tables:\n        kwargs['format'] = 'JSON'\n\n    if java_options is None:\n        java_options = []\n\n    elif isinstance(java_options, str):\n        java_options = shlex.split(java_options)\n\n    # to prevent tabula-py from stealing focus on every call on mac\n    if platform.system() == 'Darwin':\n        r = 'java.awt.headless'\n        if not any(filter(r.find, java_options)):\n            java_options = java_options + ['-Djava.awt.headless=true']\n\n    if encoding == 'utf-8':\n        r = 'file.encoding'\n        if not any(filter(r.find, java_options)):\n            java_options = java_options + ['-Dfile.encoding=UTF8']\n\n    path, temporary = localize_file(input_path)\n\n    if not os.path.exists(path):\n        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n\n    try:\n        output = _run(java_options, kwargs, path, encoding)\n    finally:\n        if temporary:\n            os.unlink(path)\n\n    if len(output) == 0:\n        return\n\n    if pandas_options is None:\n        pandas_options = {}\n\n    fmt = kwargs.get('format')\n    if fmt == 'JSON':\n        if multiple_tables:\n            return _extract_from(json.loads(output.decode(encoding)), pandas_options)\n\n        else:\n            return json.loads(output.decode(encoding))\n\n    else:\n        pandas_options['encoding'] = pandas_options.get('encoding', encoding)\n\n        try:\n            return pd.read_csv(io.BytesIO(output), **pandas_options)\n\n        except pd.errors.ParserError as e:\n            message = \"Error failed to create DataFrame with different column tables.\\n\"\n            message += \"Try to set `multiple_tables=True` or set `names` option for `pandas_options`. \\n\"\n\n            raise CSVParseError(message, e)", "response": "Read tables in PDF file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef read_pdf_with_template(\n        input_path,\n        template_path,\n        pandas_options=None,\n        encoding='utf-8',\n        java_options=None,\n        **kwargs):\n    '''Read tables in PDF.\n\n    Args:\n        input_path (file like obj):\n            File like object of tareget PDF file.\n        template_path (file_like_obj):\n            File like object for Tabula app template.\n        pandas_options (dict, optional):\n            Set pandas options like {'header': None}.\n        encoding (str, optional):\n            Encoding type for pandas. Default is 'utf-8'\n        java_options (list, optional):\n            Set java options like `-Xmx256m`.\n        kwargs (dict):\n            Dictionary of option for tabula-java. Details are shown in `build_options()`\n\n    Returns:\n        Extracted pandas DataFrame or list.\n    '''\n\n    from itertools import chain\n\n    options = load_template(template_path)\n    dataframes = []\n\n    for option in options:\n        _df = read_pdf(\n            input_path, pandas_options=pandas_options,\n            encoding=encoding, java_options=java_options,\n            **dict(kwargs, **option))\n        if isinstance(_df, list):\n            dataframes.extend(_df)\n        else:\n            dataframes.append(_df)\n\n    return dataframes", "response": "Read tables in PDF file with template."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_into(input_path, output_path, output_format='csv', java_options=None, **kwargs):\n    '''Convert tables from PDF into a file.\n\n    Args:\n        input_path (file like obj):\n            File like object of tareget PDF file.\n        output_path (str):\n            File path of output file.\n        output_format (str, optional):\n            Output format of this function (csv, json or tsv). Default: csv\n        java_options (list, optional):\n            Set java options like `-Xmx256m`.\n        kwargs (dict):\n            Dictionary of option for tabula-java. Details are shown in `build_options()`\n\n    Returns:\n        Nothing. Output file will be saved into `output_path`\n    '''\n\n    if output_path is None or len(output_path) is 0:\n        raise AttributeError(\"'output_path' shoud not be None or empty\")\n\n    kwargs['output_path'] = output_path\n    kwargs['format'] = _extract_format_for_conversion(output_format)\n\n    if java_options is None:\n        java_options = []\n\n    elif isinstance(java_options, str):\n        java_options = shlex.split(java_options)\n\n    path, temporary = localize_file(input_path)\n\n    if not os.path.exists(path):\n        raise FileNotFoundError(errno.ENOENT, os.strerror(errno.ENOENT), path)\n\n    try:\n        _run(java_options, kwargs, path)\n    finally:\n        if temporary:\n            os.unlink(path)", "response": "Convert tables from PDF into a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef convert_into_by_batch(input_dir, output_format='csv', java_options=None, **kwargs):\n    '''Convert tables from PDFs in a directory.\n\n    Args:\n        input_dir (str):\n            Directory path.\n        output_format (str, optional):\n            Output format of this function (csv, json or tsv)\n        java_options (list, optional):\n            Set java options like `-Xmx256m`.\n        kwargs (dict):\n            Dictionary of option for tabula-java. Details are shown in `build_options()`\n\n    Returns:\n        Nothing. Outputs are saved into the same directory with `input_dir`\n    '''\n\n    if input_dir is None or not os.path.isdir(input_dir):\n        raise AttributeError(\"'input_dir' shoud be directory path\")\n\n    kwargs['format'] = _extract_format_for_conversion(output_format)\n\n    if java_options is None:\n        java_options = []\n\n    elif isinstance(java_options, str):\n        java_options = shlex.split(java_options)\n\n    # Option for batch\n    kwargs['batch'] = input_dir\n\n    _run(java_options, kwargs)", "response": "Convert tables from PDFs in a directory."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _extract_from(raw_json, pandas_options=None):\n    '''Extract tables from json.\n\n    Args:\n        raw_json (list):\n            Decoded list from tabula-java JSON.\n        pandas_options (dict optional):\n            pandas options for `pd.DataFrame()`\n    '''\n\n    data_frames = []\n    if pandas_options is None:\n        pandas_options = {}\n\n    columns = pandas_options.pop('columns', None)\n    columns, header_line_number = _convert_pandas_csv_options(pandas_options, columns)\n\n    for table in raw_json:\n        list_data = [[np.nan if not e['text'] else e['text'] for e in row] for row in table['data']]\n        _columns = columns\n\n        if isinstance(header_line_number, int) and not columns:\n            _columns = list_data.pop(header_line_number)\n            _columns = ['' if e is np.nan else e for e in _columns]\n\n        data_frames.append(pd.DataFrame(data=list_data, columns=_columns, **pandas_options))\n\n    return data_frames", "response": "Extract tables from json."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ntranslating pd. read_csv options into pd. DataFrame especially for header.", "response": "def _convert_pandas_csv_options(pandas_options, columns):\n    ''' Translate `pd.read_csv()` options into `pd.DataFrame()` especially for header.\n\n    Args:\n        pandas_option (dict):\n            pandas options like {'header': None}.\n        columns (list):\n            list of column name.\n    '''\n\n    _columns = pandas_options.pop('names', columns)\n    header = pandas_options.pop('header', None)\n    pandas_options.pop('encoding', None)\n\n    if header == 'infer':\n        header_line_number = 0 if not bool(_columns) else None\n    else:\n        header_line_number = header\n\n    return _columns, header_line_number"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nbuild options for tabula - java", "response": "def build_options(kwargs=None):\n    '''Build options for tabula-java\n\n    Args:\n        options (str, optional):\n            Raw option string for tabula-java.\n        pages (str, int, :obj:`list` of :obj:`int`, optional):\n            An optional values specifying pages to extract from. It allows\n            `str`,`int`, :obj:`list` of :obj:`int`.\n            Example: '1-2,3', 'all' or [1,2]\n        guess (bool, optional):\n            Guess the portion of the page to analyze per page. Default `True`\n        area (:obj:`list` of :obj:`float` or :obj:`list` of :obj:`list` of :obj:`float`, optional):\n            Portion of the page to analyze(top,left,bottom,right).\n            Example: [269.875,12.75,790.5,561] or [[12.1,20.5,30.1,50.2],[1.0,3.2,10.5,40.2]].\n            Default is entire page\n        relative_area (bool, optional):\n            If all area values are between 0-100 (inclusive) and preceded by '%', input will be taken as\n            % of actual height or width of the page. Default False.\n        lattice (bool, optional):\n            Force PDF to be extracted using lattice-mode extraction\n            (if there are ruling lines separating each cell, as in a PDF of an\n            Excel spreadsheet)\n        stream (bool, optional):\n            Force PDF to be extracted using stream-mode extraction\n            (if there are no ruling lines separating each cell, as in a PDF of an\n             Excel spreadsheet)\n        password (str, optional):\n            Password to decrypt document. Default is empty\n        silent (bool, optional):\n            Suppress all stderr output.\n        columns (list, optional):\n            X coordinates of column boundaries.\n            Example: [10.1, 20.2, 30.3]\n        format (str, optional):\n            Format for output file or extracted object. (CSV, TSV, JSON)\n        batch (str, optional):\n            Convert all .pdfs in the provided directory. This argument should be direcotry.\n        output_path (str, optional):\n            Output file path. File format of it is depends on `format`.\n            Same as `--outfile` option of tabula-java.\n\n    Returns:\n        `obj`:list: Built list of options\n    '''\n\n    __options = []\n    if kwargs is None:\n        kwargs = {}\n    options = kwargs.get('options', '')\n    # handle options described in string for backward compatibility\n    __options += shlex.split(options)\n\n    DEPRECATED_OPTIONS = set(['spreadsheet', 'nospreadsheet'])\n    for option in set(kwargs.keys()) & DEPRECATED_OPTIONS:\n        deprecated_option(option)\n\n    # parse options\n    pages = kwargs.get('pages', 1)\n    if pages:\n        __pages = pages\n        if isinstance(pages, int):\n            __pages = str(pages)\n        elif type(pages) in [list, tuple]:\n            __pages = \",\".join(map(str, pages))\n\n        __options += [\"--pages\", __pages]\n\n    area = kwargs.get('area')\n    relative_area = kwargs.get('relative_area')\n    multiple_areas = False\n    if area:\n        __area = area\n        if type(area) in [list, tuple]:\n            # Check if nested list or tuple for multiple areas\n            if any(type(e) in [list, tuple] for e in area):\n                for e in area:\n                    __area = \"{percent}{area_str}\".format(percent='%' if relative_area else '', area_str=\",\".join(map(str, e)))\n                    __options += [\"--area\", __area]\n                    multiple_areas = True\n\n            else:\n                __area = \"{percent}{area_str}\".format(percent='%' if relative_area else '', area_str=\",\".join(map(str, area)))\n                __options += [\"--area\", __area]\n\n    guess = kwargs.get('guess', True)\n\n    lattice = kwargs.get('lattice') or kwargs.get('spreadsheet')\n    if lattice:\n        guess = False\n        __options.append(\"--lattice\")\n\n    stream = kwargs.get('stream') or kwargs.get('nospreadsheet')\n    if stream:\n        guess = False\n        __options.append(\"--stream\")\n\n    if guess and not multiple_areas:\n        __options.append(\"--guess\")\n\n    fmt = kwargs.get('format')\n    if fmt:\n        __options += [\"--format\", fmt]\n\n    output_path = kwargs.get('output_path')\n    if output_path:\n        __options += [\"--outfile\", output_path]\n\n    columns = kwargs.get('columns')\n    if columns:\n        __columns = \",\".join(map(str, columns))\n        __options += [\"--columns\", __columns]\n\n    password = kwargs.get('password')\n    if password:\n        __options += [\"--password\", password]\n\n    batch = kwargs.get('batch')\n    if batch:\n        __options += [\"--batch\", batch]\n\n    silent = kwargs.get('silent')\n    if silent:\n        __options.append(\"--silent\")\n\n    return __options"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nensuring localize target file.", "response": "def localize_file(path_or_buffer):\n    '''Ensure localize target file.\n\n    If the target file is remote, this function fetches into local storage.\n\n    Args:\n        path (str):\n            File path or file like object or URL of target file.\n\n    Returns:\n        filename (str): file name in local storage\n        temporary_file_flag (bool): temporary file flag\n    '''\n\n    path_or_buffer = _stringify_path(path_or_buffer)\n\n    if _is_url(path_or_buffer):\n        req = urlopen(path_or_buffer)\n        filename = os.path.basename(req.geturl())\n        if os.path.splitext(filename)[-1] is not \".pdf\":\n            pid = os.getpid()\n            filename = \"{0}.pdf\".format(pid)\n\n        with open(filename, 'wb') as f:\n            shutil.copyfileobj(req, f)\n\n        return filename, True\n\n    elif is_file_like(path_or_buffer):\n        pid = os.getpid()\n        filename = \"{0}.pdf\".format(pid)\n\n        with open(filename, 'wb') as f:\n            shutil.copyfileobj(path_or_buffer, f)\n\n        return filename, True\n\n    # File path case\n    else:\n        return os.path.expanduser(path_or_buffer), False"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _stringify_path(path_or_buffer):\n    '''Convert path like object to string\n\n    Args:\n        path_or_buffer: object to be converted\n\n    Returns:\n        string_path_or_buffer: maybe string version of path_or_buffer\n    '''\n\n    try:\n        import pathlib\n        _PATHLIB_INSTALLED = True\n    except ImportError:\n        _PATHLIB_INSTALLED = False\n\n    if hasattr(path_or_buffer, '__fspath__'):\n        return path_or_buffer.__fspath__()\n\n    if _PATHLIB_INSTALLED and isinstance(path_or_buffer, pathlib.Path):\n        return text_type(path_or_buffer)\n\n    return path_or_buffer", "response": "Convert path like object to string version of path_or_buffer"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nextracts the file from the given zipfile and store the permissions of the file.", "response": "def _extract_file(self, zf, info, extract_dir):\n        \"\"\"\n        the zipfile module does not restore file permissions\n        so we'll do it manually\n        \"\"\"\n        out_path = os.path.join(extract_dir, info.filename)\n        out_path = os.path.abspath(out_path)\n        if not out_path.startswith(extract_dir):\n            raise ValueError(\n                \"malicious zipfile, %s outside of extract_dir %s\" %\n                (info.filename, extract_dir))\n\n        zf.extract(info.filename, path=extract_dir)\n\n        # not sure why zipfiles store the perms 16 bits away but they do\n        perm = info.external_attr >> 16\n        os.chmod(out_path, perm)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a Tor process that can be used to connect to the Tor.", "response": "def get_tor(reactor,\n            launch_tor=False,\n            tor_control_port=None,\n            timing=None,\n            stderr=sys.stderr):\n    \"\"\"\n    If launch_tor=True, I will try to launch a new Tor process, ask it\n    for its SOCKS and control ports, and use those for outbound\n    connections (and inbound onion-service listeners, if necessary).\n\n    Otherwise if tor_control_port is provided, I will attempt to connect\n    to an existing Tor's control port at the endpoint it specifies.  I'll\n    ask that Tor for its SOCKS port.\n\n    With no arguments, I will try to connect to an existing Tor's control\n    port at the usual places: [unix:/var/run/tor/control,\n    tcp:127.0.0.1:9051, tcp:127.0.0.1:9151].  If any are successful, I'll\n    ask that Tor for its SOCKS port.  If none are successful, I'll\n    attempt to do SOCKS to the usual places: [tcp:127.0.0.1:9050,\n    tcp:127.0.0.1:9150].\n\n    If I am unable to make a SOCKS connection, the initial connection to\n    the Rendezvous Server will fail, and the program will terminate.\n\n    Control-port connections can only succeed if I can authenticate (by\n    reading a cookie file named by the Tor process), so the current user\n    must have permission to read that file (either they started Tor, e.g.\n    TorBrowser, or they are in a unix group that's been given access,\n    e.g. debian-tor).\n    \"\"\"\n    # rationale: launching a new Tor takes a long time, so only do it if\n    # the user specifically asks for it with --launch-tor. Using an\n    # existing Tor should be much faster, but still requires general\n    # permission via --tor.\n\n    if not txtorcon:\n        raise errors.NoTorError()\n\n    if not isinstance(launch_tor, bool):  # note: False is int\n        raise TypeError(\"launch_tor= must be boolean\")\n    if not isinstance(tor_control_port, (type(\"\"), type(None))):\n        raise TypeError(\"tor_control_port= must be str or None\")\n    assert tor_control_port != \"\"\n    if launch_tor and tor_control_port is not None:\n        raise ValueError(\"cannot combine --launch-tor and --tor-control-port=\")\n    timing = timing or DebugTiming()\n\n    # Connect to an existing Tor, or create a new one. If we need to\n    # launch an onion service, then we need a working control port (and\n    # authentication cookie). If we're only acting as a client, we don't\n    # need the control port.\n\n    if launch_tor:\n        print(\n            \" launching a new Tor process, this may take a while..\",\n            file=stderr)\n        with timing.add(\"launch tor\"):\n            tor = yield txtorcon.launch(reactor,\n                                        # data_directory=,\n                                        # tor_binary=,\n                                        )\n    elif tor_control_port:\n        with timing.add(\"find tor\"):\n            control_ep = clientFromString(reactor, tor_control_port)\n            tor = yield txtorcon.connect(reactor, control_ep)  # might raise\n            print(\n                \" using Tor via control port at %s\" % tor_control_port,\n                file=stderr)\n    else:\n        # Let txtorcon look through a list of usual places. If that fails,\n        # we'll arrange to attempt the default SOCKS port\n        with timing.add(\"find tor\"):\n            try:\n                tor = yield txtorcon.connect(reactor)\n                print(\" using Tor via default control port\", file=stderr)\n            except Exception:\n                # TODO: make this more specific. I think connect() is\n                # likely to throw a reactor.connectTCP -type error, like\n                # ConnectionFailed or ConnectionRefused or something\n                print(\n                    \" unable to find default Tor control port, using SOCKS\",\n                    file=stderr)\n                tor = SocksOnlyTor(reactor)\n    directlyProvides(tor, _interfaces.ITorManager)\n    returnValue(tor)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a Magic Wormhole and communicate through it.", "response": "def wormhole(context, dump_timing, transit_helper, relay_url, appid):\n    \"\"\"\n    Create a Magic Wormhole and communicate through it.\n\n    Wormholes are created by speaking the same magic CODE in two\n    different places at the same time.  Wormholes are secure against\n    anyone who doesn't use the same code.\n    \"\"\"\n    context.obj = cfg = Config()\n    cfg.appid = appid\n    cfg.relay_url = relay_url\n    cfg.transit_helper = transit_helper\n    cfg.dump_timing = dump_timing"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _dispatch_command(reactor, cfg, command):\n    cfg.timing.add(\"command dispatch\")\n    cfg.timing.add(\n        \"import\", when=start, which=\"top\").finish(when=top_import_finish)\n\n    try:\n        yield maybeDeferred(command)\n    except (WrongPasswordError, NoTorError) as e:\n        msg = fill(\"ERROR: \" + dedent(e.__doc__))\n        print(msg, file=cfg.stderr)\n        raise SystemExit(1)\n    except (WelcomeError, UnsendableFileError, KeyFormatError) as e:\n        msg = fill(\"ERROR: \" + dedent(e.__doc__))\n        print(msg, file=cfg.stderr)\n        print(six.u(\"\"), file=cfg.stderr)\n        print(six.text_type(e), file=cfg.stderr)\n        raise SystemExit(1)\n    except TransferError as e:\n        print(u\"TransferError: %s\" % six.text_type(e), file=cfg.stderr)\n        raise SystemExit(1)\n    except ServerConnectionError as e:\n        msg = fill(\"ERROR: \" + dedent(e.__doc__)) + \"\\n\"\n        msg += \"(relay URL was %s)\\n\" % e.url\n        msg += six.text_type(e)\n        print(msg, file=cfg.stderr)\n        raise SystemExit(1)\n    except Exception as e:\n        # this prints a proper traceback, whereas\n        # traceback.print_exc() just prints a TB to the \"yield\"\n        # line above ...\n        Failure().printTraceback(file=cfg.stderr)\n        print(u\"ERROR:\", six.text_type(e), file=cfg.stderr)\n        raise SystemExit(1)\n\n    cfg.timing.add(\"exit\")\n    if cfg.dump_timing:\n        cfg.timing.write(cfg.dump_timing, cfg.stderr)", "response": "Internal helper that dispatches a command to the current node."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsend a text message file or directory", "response": "def send(cfg, **kwargs):\n    \"\"\"Send a text message, file, or directory\"\"\"\n    for name, value in kwargs.items():\n        setattr(cfg, name, value)\n    with cfg.timing.add(\"import\", which=\"cmd_send\"):\n        from . import cmd_send\n\n    return go(cmd_send.send, cfg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef receive(cfg, code, **kwargs):\n    for name, value in kwargs.items():\n        setattr(cfg, name, value)\n    with cfg.timing.add(\"import\", which=\"cmd_receive\"):\n        from . import cmd_receive\n    if len(code) == 1:\n        cfg.code = code[0]\n    elif len(code) > 1:\n        print(\"Pass either no code or just one code; you passed\"\n              \" {}: {}\".format(len(code), ', '.join(code)))\n        raise SystemExit(1)\n    else:\n        cfg.code = None\n\n    return go(cmd_receive.receive, cfg)", "response": "Receive a text message file or directory"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ninvite a public - key to a user", "response": "def ssh_invite(ctx, code_length, user, **kwargs):\n    \"\"\"\n    Add a public-key to a ~/.ssh/authorized_keys file\n    \"\"\"\n    for name, value in kwargs.items():\n        setattr(ctx.obj, name, value)\n    from . import cmd_ssh\n    ctx.obj.code_length = code_length\n    ctx.obj.ssh_user = user\n    return go(cmd_ssh.invite, ctx.obj)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ssh_accept(cfg, code, key_file, yes, **kwargs):\n\n    for name, value in kwargs.items():\n        setattr(cfg, name, value)\n    from . import cmd_ssh\n    kind, keyid, pubkey = cmd_ssh.find_public_key(key_file)\n    print(\"Sending public key type='{}' keyid='{}'\".format(kind, keyid))\n    if yes is not True:\n        click.confirm(\n            \"Really send public key '{}' ?\".format(keyid), abort=True)\n    cfg.public_key = (kind, keyid, pubkey)\n    cfg.code = code\n\n    return go(cmd_ssh.accept, cfg)", "response": "Send your SSH public - key to a specific SSH key file"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nallocating a TCP port on localhost. This briefly listens on the port in question then closes it right away.", "response": "def allocate_tcp_port():\n    \"\"\"Return an (integer) available TCP port on localhost. This briefly\n    listens on the port in question, then closes it right away.\"\"\"\n    # We want to bind() the socket but not listen(). Twisted (in\n    # tcp.Port.createInternetSocket) would do several other things:\n    # non-blocking, close-on-exec, and SO_REUSEADDR. We don't need\n    # non-blocking because we never listen on it, and we don't need\n    # close-on-exec because we close it right away. So just add SO_REUSEADDR.\n    s = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    if platformType == \"posix\" and sys.platform != \"cygwin\":\n        s.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n    s.bind((\"127.0.0.1\", 0))\n    port = s.getsockname()[1]\n    s.close()\n    return port"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _not_forever(self, timeout, d):\n        t = self._reactor.callLater(timeout, d.cancel)\n\n        def _done(res):\n            if t.active():\n                t.cancel()\n            return res\n\n        d.addBoth(_done)\n        return d", "response": "Cancel the deferred if the timer fires first."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef receive(reactor,\n            appid,\n            relay_url,\n            code,\n            use_tor=False,\n            launch_tor=False,\n            tor_control_port=None,\n            on_code=None):\n    \"\"\"\n    This is a convenience API which returns a Deferred that callbacks\n    with a single chunk of data from another wormhole (and then closes\n    the wormhole). Under the hood, it's just using an instance\n    returned from :func:`wormhole.wormhole`. This is similar to the\n    `wormhole receive` command.\n\n    :param unicode appid: our application ID\n\n    :param unicode relay_url: the relay URL to use\n\n    :param unicode code: a pre-existing code to use, or None\n\n    :param bool use_tor: True if we should use Tor, False to not use it (None\n                         for default)\n\n    :param on_code: if not None, this is called when we have a code (even if\n                    you passed in one explicitly)\n    :type on_code: single-argument callable\n    \"\"\"\n    tor = None\n    if use_tor:\n        tor = yield get_tor(reactor, launch_tor, tor_control_port)\n        # For now, block everything until Tor has started. Soon: launch\n        # tor in parallel with everything else, make sure the Tor object\n        # can lazy-provide an endpoint, and overlap the startup process\n        # with the user handing off the wormhole code\n\n    wh = wormhole.create(appid, relay_url, reactor, tor=tor)\n    if code is None:\n        wh.allocate_code()\n        code = yield wh.get_code()\n    else:\n        wh.set_code(code)\n    # we'll call this no matter what, even if you passed in a code --\n    # maybe it should be only in the 'if' block above?\n    if on_code:\n        on_code(code)\n    data = yield wh.get_message()\n    data = json.loads(data.decode(\"utf-8\"))\n    offer = data.get('offer', None)\n    if not offer:\n        raise Exception(\"Do not understand response: {}\".format(data))\n    msg = None\n    if 'message' in offer:\n        msg = offer['message']\n        wh.send_message(\n            json.dumps({\n                \"answer\": {\n                    \"message_ack\": \"ok\"\n                }\n            }).encode(\"utf-8\"))\n\n    else:\n        raise Exception(\"Unknown offer type: {}\".format(offer.keys()))\n\n    yield wh.close()\n    returnValue(msg)", "response": "Receive a single message from another wormhole."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef table(self, name=DEFAULT_TABLE, **options):\n\n        if name in self._table_cache:\n            return self._table_cache[name]\n\n        table_class = options.pop('table_class', self._cls_table)\n        table = table_class(self._cls_storage_proxy(self._storage, name), name, **options)\n\n        self._table_cache[name] = table\n\n        return table", "response": "Get access to a specific table."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef purge_table(self, name):\n        if name in self._table_cache:\n            del self._table_cache[name]\n\n        proxy = StorageProxy(self._storage, name)\n        proxy.purge_table()", "response": "Purge a specific table from the database."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef insert(self, document):\n\n        doc_id = self._get_doc_id(document)\n        data = self._read()\n        data[doc_id] = dict(document)\n        self._write(data)\n\n        return doc_id", "response": "Insert a new document into the table."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ninserting multiple documents into the table.", "response": "def insert_multiple(self, documents):\n        \"\"\"\n        Insert multiple documents into the table.\n\n        :param documents: a list of documents to insert\n        :returns: a list containing the inserted documents' IDs\n        \"\"\"\n\n        doc_ids = []\n        data = self._read()\n\n        for doc in documents:\n            doc_id = self._get_doc_id(doc)\n            doc_ids.append(doc_id)\n\n            data[doc_id] = dict(doc)\n\n        self._write(data)\n\n        return doc_ids"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, cond=None, doc_ids=None, eids=None):\n        doc_ids = _get_doc_ids(doc_ids, eids)\n\n        if cond is None and doc_ids is None:\n            raise RuntimeError('Use purge() to remove all documents')\n\n        return self.process_elements(\n            lambda data, doc_id: data.pop(doc_id),\n            cond, doc_ids\n        )", "response": "Remove all matching documents."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nupdates all matching documents to have a given set of fields.", "response": "def update(self, fields, cond=None, doc_ids=None, eids=None):\n        \"\"\"\n        Update all matching documents to have a given set of fields.\n\n        :param fields: the fields that the matching documents will have\n                       or a method that will update the documents\n        :type fields: dict | dict -> None\n        :param cond: which documents to update\n        :type cond: query\n        :param doc_ids: a list of document IDs\n        :type doc_ids: list\n        :returns: a list containing the updated document's ID\n        \"\"\"\n        doc_ids = _get_doc_ids(doc_ids, eids)\n\n        if callable(fields):\n            return self.process_elements(\n                lambda data, doc_id: fields(data[doc_id]),\n                cond, doc_ids\n            )\n        else:\n            return self.process_elements(\n                lambda data, doc_id: data[doc_id].update(fields),\n                cond, doc_ids\n            )"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite back a list of documents by doc_id.", "response": "def write_back(self, documents, doc_ids=None, eids=None):\n        \"\"\"\n        Write back documents by doc_id\n\n        :param documents: a list of document to write back\n        :param doc_ids: a list of document IDs which need to be written back\n        :returns: a list of document IDs that have been written\n        \"\"\"\n        doc_ids = _get_doc_ids(doc_ids, eids)\n\n        if doc_ids is not None and not len(documents) == len(doc_ids):\n            raise ValueError(\n                'The length of documents and doc_ids is not match.')\n\n        if doc_ids is None:\n            doc_ids = [doc.doc_id for doc in documents]\n\n        # Since this function will write docs back like inserting, to ensure\n        # here only process existing or removed instead of inserting new,\n        # raise error if doc_id exceeded the last.\n        if len(doc_ids) > 0 and max(doc_ids) > self._last_id:\n            raise IndexError(\n                'ID exceeds table length, use existing or removed doc_id.')\n\n        data = self._read()\n\n        # Document specified by ID\n        documents.reverse()\n        for doc_id in doc_ids:\n            data[doc_id] = dict(documents.pop())\n\n        self._write(data)\n\n        return doc_ids"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates a document if it exist - insert it otherwise.", "response": "def upsert(self, document, cond):\n        \"\"\"\n        Update a document, if it exist - insert it otherwise.\n\n        Note: this will update *all* documents matching the query.\n\n        :param document: the document to insert or the fields to update\n        :param cond: which document to look for\n        :returns: a list containing the updated document's ID\n        \"\"\"\n        updated_docs = self.update(document, cond)\n\n        if updated_docs:\n            return updated_docs\n        else:\n            return [self.insert(document)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches for all documents matching a condition.", "response": "def search(self, cond):\n        \"\"\"\n        Search for all documents matching a 'where' cond.\n\n        :param cond: the condition to check against\n        :type cond: Query\n\n        :returns: list of matching documents\n        :rtype: list[Element]\n        \"\"\"\n\n        if cond in self._query_cache:\n            return self._query_cache.get(cond, [])[:]\n\n        docs = [doc for doc in self.all() if cond(doc)]\n        self._query_cache[cond] = docs\n\n        return docs[:]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget exactly one document specified by a query or and ID.", "response": "def get(self, cond=None, doc_id=None, eid=None):\n        \"\"\"\n        Get exactly one document specified by a query or and ID.\n\n        Returns ``None`` if the document doesn't exist\n\n        :param cond: the condition to check against\n        :type cond: Query\n\n        :param doc_id: the document's ID\n\n        :returns: the document or None\n        :rtype: Element | None\n        \"\"\"\n        doc_id = _get_doc_id(doc_id, eid)\n\n        # Cannot use process_elements here because we want to return a\n        # specific document\n\n        if doc_id is not None:\n            # Document specified by ID\n            return self._read().get(doc_id, None)\n\n        # Document specified by condition\n        for doc in self.all():\n            if cond(doc):\n                return doc"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef contains(self, cond=None, doc_ids=None, eids=None):\n        doc_ids = _get_doc_ids(doc_ids, eids)\n\n        if doc_ids is not None:\n            # Documents specified by ID\n            return any(self.get(doc_id=doc_id) for doc_id in doc_ids)\n\n        # Document specified by condition\n        return self.get(cond) is not None", "response": "Check if the database contains a document with the specified condition or a specific ID."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nflushes all unwritten data to disk.", "response": "def flush(self):\n        \"\"\"\n        Flush all unwritten data to disk.\n        \"\"\"\n        if self._cache_modified_count > 0:\n            self.storage.write(self.cache)\n            self._cache_modified_count = 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef matches(self, regex, flags=0):\n        return self._generate_test(\n            lambda value: re.match(regex, value, flags),\n            ('matches', self._path, regex)\n        )", "response": "Run a regex test against a dict value."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nrun a regex test against a dict value", "response": "def search(self, regex, flags=0):\n        \"\"\"\n        Run a regex test against a dict value (only substring string has to\n        match).\n\n        >>> Query().f1.search(r'^\\w+$')\n\n        :param regex: The regular expression to use for matching\n        \"\"\"\n        return self._generate_test(\n            lambda value: re.search(regex, value, flags),\n            ('search', self._path, regex)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nchecks if a condition is met by any document in a list", "response": "def any(self, cond):\n        \"\"\"\n        Check if a condition is met by any document in a list,\n        where a condition can also be a sequence (e.g. list).\n\n        >>> Query().f1.any(Query().f2 == 1)\n\n        Matches::\n\n            {'f1': [{'f2': 1}, {'f2': 0}]}\n\n        >>> Query().f1.any([1, 2, 3])\n\n        Matches::\n\n            {'f1': [1, 2]}\n            {'f1': [3, 4, 5]}\n\n        :param cond: Either a query that at least one document has to match or\n                     a list of which at least one document has to be contained\n                     in the tested document.\n        \"\"\"\n        if callable(cond):\n            def _cmp(value):\n                return is_sequence(value) and any(cond(e) for e in value)\n\n        else:\n            def _cmp(value):\n                return is_sequence(value) and any(e in cond for e in value)\n\n        return self._generate_test(\n            lambda value: _cmp(value),\n            ('any', self._path, freeze(cond))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef one_of(self, items):\n        return self._generate_test(\n            lambda value: value in items,\n            ('one_of', self._path, freeze(items))\n        )", "response": "Check if the value is contained in a list or generator."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef setobjattr(obj, key, value):\n    try:\n        setattr(obj, key, int(value))\n    except ValueError:\n        try:\n            setattr(obj, key, float(value))\n        except ValueError:\n            # string if not number\n            try:\n                setattr(obj, key, str(value))\n            except UnicodeEncodeError:\n                setattr(obj, key, value)", "response": "Sets an object attribute with the correct data type."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_scoreboard(year, month, day):\n    try:\n        data = urlopen(BASE_URL.format(year, month, day) + 'scoreboard.xml')\n    except HTTPError:\n        data = os.path.join(PWD, 'default.xml')\n    return data", "response": "Return the game file for a certain day matching certain criteria."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_box_score(game_id):\n    year, month, day = get_date_from_game_id(game_id)\n    try:\n        return urlopen(GAME_URL.format(year, month, day, game_id,\n                                       'boxscore.xml'))\n    except HTTPError:\n        raise ValueError('Could not find a game with that id.')", "response": "Return the box score file of a game with matching id."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the innings file of a game with matching id.", "response": "def get_innings(game_id):\n    \"\"\"Return the innings file of a game with matching id.\"\"\"\n    year, month, day = get_date_from_game_id(game_id)\n    try:\n        return urlopen(INNINGS_URL.format(year, month, day, game_id))\n    except HTTPError:\n        raise ValueError('Could not find a game with that id.')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_standings(date):\n    try:\n        return urlopen(STANDINGS_URL.format(date.year,\n                                            date.strftime('%Y/%m/%d')))\n    except HTTPError:\n        ValueError('Could not find the standings file. '\n                   'mlb.com does not provide the file that '\n                   'mlbgame needs to perform this operation.')", "response": "Return the standings file for current date."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_historical_standings(date):\n    try:\n        url = STANDINGS_HISTORICAL_URL.format(date.year,\n                                              date.strftime('%Y/%m/%d'))\n        return urlopen(url)\n    except HTTPError:\n        ValueError('Could not find standings for that date.')", "response": "Return the historical standings file for a given date."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef scoreboard(year, month, day, home=None, away=None):\n    # get data\n    data = mlbgame.data.get_scoreboard(year, month, day)\n    # parse data\n    parsed = etree.parse(data)\n    root = parsed.getroot()\n    games = {}\n    output = {}\n    # loop through games\n    for game in root:\n        if game.tag == 'data':\n            return []\n        # get team names\n        teams = game.findall('team')\n        home_name = teams[0].attrib['name']\n        away_name = teams[1].attrib['name']\n        # check if teams match parameters\n        if (home_name == home and home is not None) \\\n                or (away_name == away and away is not None) \\\n                or (away is None and home is None):\n            # throw all the data into a complicated dictionary\n            game_tag = game.tag\n            game_data = game.find('game')\n            game_id = game_data.attrib['id']\n            game_league = game_data.attrib['league']\n            game_status = game_data.attrib['status']\n            game_start_time = game_data.attrib['start_time']\n            home_team_data = teams[0].find('gameteam')\n            home_team = home_name\n            home_team_runs = int(home_team_data.attrib['R'])\n            home_team_hits = int(home_team_data.attrib['H'])\n            home_team_errors = int(home_team_data.attrib['E'])\n            away_team_data = teams[1].find('gameteam')\n            away_team = away_name\n            away_team_runs = int(away_team_data.attrib['R'])\n            away_team_hits = int(away_team_data.attrib['H'])\n            away_team_errors = int(away_team_data.attrib['E'])\n            # check type of game\n            if game_tag == 'go_game' or game_tag == 'ig_game':\n                try:\n                    w_pitcher_data = game.find('w_pitcher')\n                    w_pitcher = w_pitcher_data.find('pitcher').attrib['name']\n                    w_pitcher_wins = int(w_pitcher_data.attrib['wins'])\n                    w_pitcher_losses = int(w_pitcher_data.attrib['losses'])\n                except Exception:\n                    w_pitcher = \"\"\n                    w_pitcher_wins = 0\n                    w_pitcher_losses = 0\n                try:\n                    l_pitcher_data = game.find('l_pitcher')\n                    l_pitcher = l_pitcher_data.find('pitcher').attrib['name']\n                    l_pitcher_wins = int(l_pitcher_data.attrib['wins'])\n                    l_pitcher_losses = int(l_pitcher_data.attrib['losses'])\n                except Exception:\n                    l_pitcher = \"\"\n                    l_pitcher_wins = 0\n                    l_pitcher_losses = 0\n                try:\n                    sv_pitcher_data = game.find('sv_pitcher')\n                    sv_pitcher = sv_pitcher_data.find('pitcher').attrib['name']\n                    sv_pitcher_saves = int(sv_pitcher_data.attrib['saves'])\n                except Exception:\n                    sv_pitcher = \"\"\n                    sv_pitcher_saves = 0\n                output = {\n                    'game_id': game_id,\n                    'game_tag': game_tag,\n                    'game_league': game_league,\n                    'game_status': game_status,\n                    'game_start_time': game_start_time,\n                    'home_team': home_team,\n                    'home_team_runs': home_team_runs,\n                    'home_team_hits': home_team_hits,\n                    'home_team_errors': home_team_errors,\n                    'away_team': away_team,\n                    'away_team_runs': away_team_runs,\n                    'away_team_hits': away_team_hits,\n                    'away_team_errors': away_team_errors,\n                    'w_pitcher': w_pitcher,\n                    'w_pitcher_wins': w_pitcher_wins,\n                    'w_pitcher_losses': w_pitcher_losses,\n                    'l_pitcher': l_pitcher,\n                    'l_pitcher_wins': l_pitcher_wins,\n                    'l_pitcher_losses': l_pitcher_losses,\n                    'sv_pitcher': sv_pitcher,\n                    'sv_pitcher_saves': sv_pitcher_saves\n                }\n            # games that were not played\n            elif game_tag == 'sg_game':\n                try:\n                    p_pitcher_data = game.findall('p_pitcher')\n                    p_pitcher_home_data = p_pitcher_data[0]\n                    p_pitcher_home = p_pitcher_home_data.find(\n                        'pitcher').attrib['name']\n                    p_pitcher_home_wins = int(p_pitcher_home_data.\n                                              attrib['wins'])\n                    p_pitcher_home_losses = int(p_pitcher_home_data.\n                                                attrib['losses'])\n                    p_pitcher_away_data = p_pitcher_data[1]\n                    p_pitcher_away = p_pitcher_away_data.find(\n                        'pitcher').attrib['name']\n                    p_pitcher_away_wins = int(p_pitcher_away_data.\n                                              attrib['wins'])\n                    p_pitcher_away_losses = int(p_pitcher_away_data.\n                                                attrib['losses'])\n                except Exception:\n                    p_pitcher_home = ''\n                    p_pitcher_home_wins = 0\n                    p_pitcher_home_losses = 0\n                    p_pitcher_away = ''\n                    p_pitcher_away_wins = 0\n                    p_pitcher_away_losses = 0\n                output = {\n                    'game_id': game_id,\n                    'game_tag': game_tag,\n                    'game_league': game_league,\n                    'game_status': game_status,\n                    'game_start_time': game_start_time,\n                    'home_team': home_team,\n                    'home_team_runs': home_team_runs,\n                    'home_team_hits': home_team_hits,\n                    'home_team_errors': home_team_errors,\n                    'away_team': away_team,\n                    'away_team_runs': away_team_runs,\n                    'away_team_hits': away_team_hits,\n                    'away_team_errors': away_team_errors,\n                    'p_pitcher_home': p_pitcher_home,\n                    'p_pitcher_home_wins': p_pitcher_home_wins,\n                    'p_pitcher_home_losses': p_pitcher_home_losses,\n                    'p_pitcher_away': p_pitcher_away,\n                    'p_pitcher_away_wins': p_pitcher_away_wins,\n                    'p_pitcher_away_losses': p_pitcher_away_losses\n                }\n            # put this dictionary into the larger dictionary\n            games[game_id] = output\n    return games", "response": "Return the scoreboard information for games matching the parameters\n    as a dictionary."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef box_score(game_id):\n    # get data\n    data = mlbgame.data.get_box_score(game_id)\n    # parse data\n    parsed = etree.parse(data)\n    root = parsed.getroot()\n    linescore = root.find('linescore')\n    result = dict()\n    result['game_id'] = game_id\n    # loop through innings and add them to output\n    for x in linescore:\n        inning = x.attrib['inning']\n        home = value_to_int(x.attrib, 'home')\n        away = value_to_int(x.attrib, 'away')\n        result[int(inning)] = {'home': home, 'away': away}\n    return result", "response": "Gets the box score information for the game with matching id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef value_to_int(attrib, key):\n    val = attrib.get(key, 0)\n    if isinstance(val, str):\n        if val.isspace() or val == '':\n            return 0\n    return val", "response": "Converts a value to an integer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef overview(game_id):\n    output = {}\n    # get data\n    overview = mlbgame.data.get_overview(game_id)\n    # parse data\n    overview_root = etree.parse(overview).getroot()\n\n    try:\n        output = add_raw_box_score_attributes(output, game_id)\n    except ValueError:\n        pass\n\n    # get overview attributes\n    for x in overview_root.attrib:\n        output[x] = overview_root.attrib[x]\n\n    # Get probable starter attributes if they exist\n    home_pitcher_tree = overview_root.find('home_probable_pitcher')\n    if home_pitcher_tree is not None:\n        output.update(build_namespaced_attributes(\n            'home_probable_pitcher', home_pitcher_tree))\n    else:\n        output.update(build_probable_starter_defaults('home'))\n\n    away_pitcher_tree = overview_root.find('away_probable_pitcher')\n    if away_pitcher_tree is not None:\n        output.update(build_namespaced_attributes(\n            'away_probable_pitcher', away_pitcher_tree))\n    else:\n        output.update(build_probable_starter_defaults('away'))\n\n    return output", "response": "Gets the overview information for the game with matching id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting player and coach information for the game with matching id.", "response": "def players(game_id):\n    \"\"\"Gets player/coach/umpire information for the game with matching id.\"\"\"\n    # get data\n    data = mlbgame.data.get_players(game_id)\n    # parse data\n    parsed = etree.parse(data)\n    root = parsed.getroot()\n\n    output = {}\n    output['game_id'] = game_id\n\n    # get player/coach data\n    for team in root.findall('team'):\n        type = team.attrib['type'] + \"_team\"\n        # the type is either home_team or away_team\n        output[type] = {}\n        output[type]['players'] = []\n        output[type]['coaches'] = []\n\n        for p in team.findall('player'):\n            player = {}\n            for key in p.keys():\n                player[key] = p.get(key)\n            output[type]['players'].append(player)\n\n        for c in team.findall('coach'):\n            coach = {}\n            for key in c.keys():\n                coach[key] = c.get(key)\n            output[type]['coaches'].append(coach)\n\n    # get umpire data\n    output['umpires'] = []\n    for u in root.find('umpires').findall('umpire'):\n        umpire = {}\n        for key in u.keys():\n            umpire[key] = u.get(key)\n        output['umpires'].append(umpire)\n\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nprint the object as a scoreboard.", "response": "def print_scoreboard(self):\n        \"\"\"Print object as a scoreboard.\"\"\"\n        output = ''\n        # parallel dictionaries with innings and scores\n        innings = []\n        away = []\n        home = []\n        for x in self:\n            innings.append(x['inning'])\n            away.append(x['away'])\n            home.append(x['home'])\n        # go through all the information and make a nice output\n        # that looks like a scoreboard\n        output += 'Inning\\t'\n        for x in innings:\n            output += str(x) + ' '\n        output += '\\n'\n        for x in innings:\n            output += '---'\n        output += '\\nAway\\t' + self.__enumerate_scoreboard(away)\n        output += '\\nHome\\t' + self.__enumerate_scoreboard(home)\n        return output"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of games for a certain day.", "response": "def day(year, month, day, home=None, away=None):\n    \"\"\"Return a list of games for a certain day.\n\n    If the home and away team are the same,\n    it will return the game(s) for that team.\n    \"\"\"\n    # get the days per month\n    daysinmonth = calendar.monthrange(year, month)[1]\n    # do not even try to get data if day is too high\n    if daysinmonth < day:\n        return []\n    # get data\n    data = mlbgame.game.scoreboard(year, month, day, home=home, away=away)\n    return [mlbgame.game.GameScoreboard(data[x]) for x in data]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef games(years, months=None, days=None, home=None, away=None):\n    # put in data if months and days are not specified\n    if months is None:\n        months = list(range(1, 13))\n    if days is None:\n        days = list(range(1, 32))\n    results = []\n    # check if lists, if not make lists\n    # allows users to input either numbers or lists\n    if not isinstance(years, list):\n        years = [years]\n    if not isinstance(months, list):\n        months = [months]\n    if not isinstance(days, list):\n        days = [days]\n    for i in years:\n        for y in months:\n            # get the days in a month\n            daysinmonth = calendar.monthrange(i, y)[1]\n            for x in days:\n                if daysinmonth >= x:\n                    # use the day function to get data for each day in range\n                    game = day(i, y, x, home=home, away=away)\n                    if game:\n                        results.append(game)\n    return results", "response": "Return a list of lists of games for multiple days."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns box score for game matching the game id.", "response": "def box_score(game_id):\n    \"\"\"Return box score for game matching the game id.\"\"\"\n    # get box score data\n    data = mlbgame.game.box_score(game_id)\n    # create object with data\n    obj = mlbgame.game.GameBoxScore(data)\n    return obj"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef player_stats(game_id):\n    # get information for that game\n    data = mlbgame.stats.player_stats(game_id)\n    return mlbgame.stats.Stats(data, game_id, True)", "response": "Return dictionary of player stats for game matching the game id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn dictionary of team stats for game matching the game id.", "response": "def team_stats(game_id):\n    \"\"\"Return dictionary of team stats for game matching the game id.\"\"\"\n    # get data\n    data = mlbgame.stats.team_stats(game_id)\n    return mlbgame.stats.Stats(data, game_id, False)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn list of Inning objects for game matching the game id.", "response": "def game_events(game_id, innings_endpoint=False):\n    \"\"\"Return list of Inning objects for game matching the game id.\n\n    Using `inning_endpoints=True` will result in objects with\n    additional, undocumented data properties, but also objects\n    that may be missing properties expected by the user.\n\n    `innings_endpoint`: bool, use more detailed `innings` API endpoint\n    \"\"\"\n    data = mlbgame.events.game_events(game_id, innings_endpoint)\n    return [mlbgame.events.Inning(data[x], x) for x in data]"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn ImportantDates object that contains MLB important dates", "response": "def important_dates(year=None):\n    \"\"\"Return ImportantDates object that contains MLB important dates\"\"\"\n    year = datetime.now().year if not year else year\n    data = mlbgame.info.important_dates(year)\n    return mlbgame.info.ImportantDates(data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef roster(team_id):\n    data = mlbgame.info.roster(team_id)\n    return mlbgame.info.Roster(data)", "response": "Return a Roster object that contains roster info for a team"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef standings(date=datetime.now()):\n    data = mlbgame.info.standings(date)\n    return mlbgame.info.Standings(data)", "response": "Return Standings object that contains info for a given date"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns BroadcastInfo object that containts informationon the television and radio broadcasts for the team_id and year", "response": "def broadcast_info(team_id, date=datetime.now()):\n    \"\"\"Return BroadcastInfo object that containts information\n    about the television and radio broadcasts for the team_id\n    and year\"\"\"\n    data = mlbgame.info.broadcast_info(team_id, date)\n    return [mlbgame.info.BroadcastInfo(x) for x in data]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn dictionary of individual stats of a game with matching id.", "response": "def player_stats(game_id):\n    \"\"\"Return dictionary of individual stats of a game with matching id.\n\n       The additional pitching/batting is mostly the same stats, except it\n       contains some useful stats such as groundouts/flyouts per pitcher\n       (go/ao). MLB decided to have two box score files, thus we return the\n       data from both.\n    \"\"\"\n    # get data from data module\n    box_score = mlbgame.data.get_box_score(game_id)\n    box_score_tree = etree.parse(box_score).getroot()\n    # get pitching and batting info\n    pitching = box_score_tree.findall('pitching')\n    batting = box_score_tree.findall('batting')\n    # get parsed stats\n    pitching_info = __player_stats_info(pitching, 'pitcher')\n    batting_info = __player_stats_info(batting, 'batter')\n    # rawboxscore not available after 2018\n    try:\n        raw_box_score = mlbgame.data.get_raw_box_score(game_id)\n        raw_box_score_tree = etree.parse(raw_box_score).getroot()\n        additional_stats = __raw_player_stats_info(raw_box_score_tree)\n        addl_home_pitching = additional_stats[0]['pitchers']\n        addl_home_batting = additional_stats[0]['batters']\n        addl_away_pitching = additional_stats[1]['pitchers']\n        addl_away_batting = additional_stats[1]['batters']\n\n        output = {\n            'home_pitching': pitching_info[0],\n            'away_pitching': pitching_info[1],\n            'home_batting': batting_info[0],\n            'away_batting': batting_info[1],\n            'home_additional_pitching': addl_home_pitching,\n            'away_additional_pitching': addl_away_pitching,\n            'home_additional_batting': addl_home_batting,\n            'away_additional_batting': addl_away_batting\n        }\n    except etree.XMLSyntaxError:\n        output = {\n            'home_pitching': pitching_info[0],\n            'away_pitching': pitching_info[1],\n            'home_batting': batting_info[0],\n            'away_batting': batting_info[1],\n        }\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef team_stats(game_id):\n    # get data from data module\n    box_score = mlbgame.data.get_box_score(game_id)\n    raw_box_score = mlbgame.data.get_raw_box_score(game_id)\n    # parse XML\n    box_score_tree = etree.parse(box_score).getroot()\n    raw_box_score_tree = etree.parse(raw_box_score).getroot()\n    # get pitching and batting ingo\n    pitching = box_score_tree.findall('pitching')\n    batting = box_score_tree.findall('batting')\n    # dictionary for output\n    output = {}\n    output = __team_stats_info(pitching, output, 'pitching')\n    output = __team_stats_info(batting, output, 'batting')\n    output = __raw_team_stats_info(raw_box_score_tree, output)\n    return output", "response": "Return team stats of a game with matching id."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the xml object corresponding to the league Only designed for internal use", "response": "def __get_league_object():\n    \"\"\"Returns the xml object corresponding to the league\n\n    Only designed for internal use\"\"\"\n    # get data\n    data = mlbgame.data.get_properties()\n    # return league object\n    return etree.parse(data).getroot().find('leagues').find('league')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of league information", "response": "def league_info():\n    \"\"\"Returns a dictionary of league information\"\"\"\n    league = __get_league_object()\n    output = {}\n    for x in league.attrib:\n        output[x] = league.attrib[x]\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of team information dictionaries", "response": "def team_info():\n    \"\"\"Returns a list of team information dictionaries\"\"\"\n    teams = __get_league_object().find('teams').findall('team')\n    output = []\n    for team in teams:\n        info = {}\n        for x in team.attrib:\n            info[x] = team.attrib[x]\n        output.append(info)\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a dictionary of important dates for a given year", "response": "def important_dates(year):\n    \"\"\"Returns a dictionary of important dates\"\"\"\n    output = {}\n    data = mlbgame.data.get_important_dates(year)\n    important_dates = etree.parse(data).getroot().\\\n        find('queryResults').find('row')\n    try:\n        for x in important_dates.attrib:\n            output[x] = important_dates.attrib[x]\n    except AttributeError:\n        raise ValueError('Unable to find important dates for {}.'.format(year))\n    return output"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef broadcast_info(team_id, date=datetime.now()):\n    year = date.year\n    game_date = date.strftime('%Y-%m-%dT00:00:00')\n    data = mlbgame.data.get_broadcast_info(team_id, year)\n    schedule = json.loads(data.read().decode('utf-8'))\n    schedule = schedule['mlb_broadcast_info']['queryResults']['row']\n    return [g for g in schedule if g['game_date'] == game_date]", "response": "Returns a dictionary of broadcast information for a given team during a given season"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef roster(team_id):\n    data = mlbgame.data.get_roster(team_id)\n    parsed = json.loads(data.read().decode('utf-8'))\n    players = parsed['roster_40']['queryResults']['row']\n    return {'players': players, 'team_id': team_id}", "response": "Returns a dictionary of roster information for team id"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef nice_output(self):\n        dates = [\n            str_format('Opening Day {0}: {1}.',\n                       [self.year, date_format(self.first_date_seas)]),\n            str_format('Last day of the 1st half: {0}.',\n                       [date_format(self.last_date_1sth)]),\n            str_format('{0} All Star Game: {1}.',\n                       [self.year, date_format(self.all_star_date)]),\n            str_format('First day of the 2nd half: {}.',\n                       [date_format(self.first_date_2ndh)]),\n            str_format('Last day of the {0} season: {1}.',\n                       [self.year, date_format(self.last_date_seas)]),\n            str_format('{0} Playoffs start: {1}.',\n                       [self.year, date_format(self.playoffs_start_date)]),\n            str_format('{0} Playoffs end: {1}.',\n                       [self.year, date_format(self.playoffs_end_date)])\n        ]\n        return '\\n'.join(dates)", "response": "Return a string for printing"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef game_events(game_id, innings_endpoint=False):\n    # get data from data module\n    if not innings_endpoint:\n        data = mlbgame.data.get_game_events(game_id)\n        endpoint = 'game_events'\n    else:\n        data = mlbgame.data.get_innings(game_id)\n        endpoint = 'innings'\n    # parse XML\n    parsed = etree.parse(data)\n    root = parsed.getroot()\n    # empty output file\n    output = {}\n    # loop through innings\n    innings = root.findall('inning')\n    for x in innings:\n        output[x.attrib['num']] = {\n            'top': __inning_info(x, 'top', endpoint),\n            'bottom': __inning_info(x, 'bottom', endpoint)\n        }\n    return output", "response": "Return dictionary of events for a game with matching id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_platform():\n    plat = platform_detect.platform_detect()\n    if plat == platform_detect.RASPBERRY_PI:\n        # Check for version 1 or 2 of the pi.\n        version = platform_detect.pi_version()\n        if version == 1:\n            from . import Raspberry_Pi\n            return Raspberry_Pi\n        elif version == 2:\n            from . import Raspberry_Pi_2\n            return Raspberry_Pi_2\n        elif version == 3:\n            \"\"\"Use Pi 2 driver even though running on Pi 3\"\"\"\n            from . import Raspberry_Pi_2\n            return Raspberry_Pi_2\n        else:\n            raise RuntimeError('No driver for detected Raspberry Pi version available!')\n    elif plat == platform_detect.BEAGLEBONE_BLACK:\n        from . import Beaglebone_Black\n        return Beaglebone_Black\n    else:\n        raise RuntimeError('Unknown platform.')", "response": "Return a DHT platform interface for the currently detected platform."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read(sensor, pin, platform=None):\n    if sensor not in SENSORS:\n        raise ValueError('Expected DHT11, DHT22, or AM2302 sensor value.')\n    if platform is None:\n        platform = get_platform()\n    return platform.read(sensor, pin)", "response": "Read DHT sensor on specified pin and return a tuple of humidity and temperature."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreading DHT sensor of specified sensor type and pin and return a tuple of humidity and temperature.", "response": "def read_retry(sensor, pin, retries=15, delay_seconds=2, platform=None):\n    \"\"\"Read DHT sensor of specified sensor type (DHT11, DHT22, or AM2302) on\n    specified pin and return a tuple of humidity (as a floating point value\n    in percent) and temperature (as a floating point value in Celsius).\n    Unlike the read function, this read_retry function will attempt to read\n    multiple times (up to the specified max retries) until a good reading can be\n    found. If a good reading cannot be found after the amount of retries, a tuple\n    of (None, None) is returned. The delay between retries is by default 2\n    seconds, but can be overridden.\n    \"\"\"\n    for i in range(retries):\n        humidity, temperature = read(sensor, pin, platform)\n        if humidity is not None and temperature is not None:\n            return (humidity, temperature)\n        time.sleep(delay_seconds)\n    return (None, None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to Google Docs spreadsheet and return the first worksheet.", "response": "def login_open_sheet(oauth_key_file, spreadsheet):\n    \"\"\"Connect to Google Docs spreadsheet and return the first worksheet.\"\"\"\n    try:\n        scope =  ['https://spreadsheets.google.com/feeds']\n        credentials = ServiceAccountCredentials.from_json_keyfile_name(oauth_key_file, scope)\n        gc = gspread.authorize(credentials)\n        worksheet = gc.open(spreadsheet).sheet1\n        return worksheet\n    except Exception as ex:\n        print('Unable to login and get spreadsheet.  Check OAuth credentials, spreadsheet name, and make sure spreadsheet is shared to the client_email address in the OAuth .json file!')\n        print('Google sheet login failed with error:', ex)\n        sys.exit(1)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetecting if running on the Raspberry Pi or Beaglebone Black and return the platform type. Will return RASPBERRY_PI BEAGLEBONE_BLACK or UNKNOWN.", "response": "def platform_detect():\n    \"\"\"Detect if running on the Raspberry Pi or Beaglebone Black and return the\n    platform type.  Will return RASPBERRY_PI, BEAGLEBONE_BLACK, or UNKNOWN.\"\"\"\n    # Handle Raspberry Pi\n    pi = pi_version()\n    if pi is not None:\n        return RASPBERRY_PI\n\n    # Handle Beaglebone Black\n    # TODO: Check the Beaglebone Black /proc/cpuinfo value instead of reading\n    # the platform.\n    plat = platform.platform()\n    if plat.lower().find('armv7l-with-debian') > -1:\n        return BEAGLEBONE_BLACK\n    elif plat.lower().find('armv7l-with-ubuntu') > -1:\n        return BEAGLEBONE_BLACK\n    elif plat.lower().find('armv7l-with-glibc2.4') > -1:\n        return BEAGLEBONE_BLACK\n    elif plat.lower().find('armv7l-with-arch') > -1:\n        return BEAGLEBONE_BLACK\n\n    # Couldn't figure out the platform, just return unknown.\n    return UNKNOWN"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndetecting the revision number of a Raspberry Pi useful for changing the revision number of a Raspberry Pi.", "response": "def pi_revision():\n    \"\"\"Detect the revision number of a Raspberry Pi, useful for changing\n    functionality like default I2C bus based on revision.\"\"\"\n    # Revision list available at: http://elinux.org/RPi_HardwareHistory#Board_Revision_History\n    with open('/proc/cpuinfo', 'r') as infile:\n        for line in infile:\n            # Match a line of the form \"Revision : 0002\" while ignoring extra\n            # info in front of the revsion (like 1000 when the Pi was over-volted).\n            match = re.match('Revision\\s+:\\s+.*(\\w{4})$', line, flags=re.IGNORECASE)\n            if match and match.group(1) in ['0000', '0002', '0003']:\n                # Return revision 1 if revision ends with 0000, 0002 or 0003.\n                return 1\n            elif match:\n                # Assume revision 2 if revision ends with any other 4 chars.\n                return 2\n        # Couldn't find the revision, throw an exception.\n        raise RuntimeError('Could not determine Raspberry Pi revision.')"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling getting the password", "response": "def handle_password(user, password):  # pragma: no cover\n    \"\"\" Handles getting the password\"\"\"\n    if password is None:\n        try:\n            password = keyring.get_password(\"yagmail\", user)\n        except NameError as e:\n            print(\n                \"'keyring' cannot be loaded. Try 'pip install keyring' or continue without. See https://github.com/kootenpv/yagmail\"\n            )\n            raise e\n        if password is None:\n            import getpass\n\n            password = getpass.getpass(\"Password for <{0}>: \".format(user))\n            answer = \"\"\n            # Python 2 fix\n            while answer != \"y\" and answer != \"n\":\n                prompt_string = \"Save username and password in keyring? [y/n]: \"\n                # pylint: disable=undefined-variable\n                try:\n                    answer = raw_input(prompt_string).strip()\n                except NameError:\n                    answer = input(prompt_string).strip()\n            if answer == \"y\":\n                register(user, password)\n    return password"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nhandles the targets addresses adding aliases when defined", "response": "def resolve_addresses(user, useralias, to, cc, bcc):\n    \"\"\" Handle the targets addresses, adding aliases when defined \"\"\"\n    addresses = {\"recipients\": []}\n    if to is not None:\n        make_addr_alias_target(to, addresses, \"To\")\n    elif cc is not None and bcc is not None:\n        make_addr_alias_target([user, useralias], addresses, \"To\")\n    else:\n        addresses[\"recipients\"].append(user)\n    if cc is not None:\n        make_addr_alias_target(cc, addresses, \"Cc\")\n    if bcc is not None:\n        make_addr_alias_target(bcc, addresses, \"Bcc\")\n    return addresses"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef main():\n    import argparse\n\n    parser = argparse.ArgumentParser(description='Send a (g)mail with yagmail.')\n    subparsers = parser.add_subparsers(dest=\"command\")\n    oauth = subparsers.add_parser('oauth')\n    oauth.add_argument(\n        '--user', '-u', required=True, help='The gmail username to register oauth2 for'\n    )\n    oauth.add_argument(\n        '--file', '-f', required=True, help='The filepath to store the oauth2 credentials'\n    )\n    parser.add_argument('-to', '-t', help='Send an email to address \"TO\"', nargs='+')\n    parser.add_argument('-subject', '-s', help='Subject of email', nargs='+')\n    parser.add_argument('-contents', '-c', help='Contents to send', nargs='+')\n    parser.add_argument('-attachments', '-a', help='Attachments to attach', nargs='+')\n    parser.add_argument('-user', '-u', help='Username')\n    parser.add_argument('-oauth2', '-o', help='OAuth2 file path')\n    parser.add_argument(\n        '-password', '-p', help='Preferable to use keyring rather than password here'\n    )\n    args = parser.parse_args()\n    args.contents = (\n        args.contents if args.contents else (sys.stdin.read() if not sys.stdin.isatty() else None)\n    )\n    if args.command == \"oauth\":\n        user = args.user\n        SMTP(args.user, oauth2_file=args.file)\n        print(\"Succesful.\")\n    else:\n        yag = SMTP(args.user, args.password, oauth2_file=args.oauth2)\n        yag.send(\n            to=args.to, subject=args.subject, contents=args.contents, attachments=args.attachments\n        )", "response": "This is the main function that is run from commandline with yagmail."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare_message(user, useralias, addresses, subject, contents, attachments, headers, encoding, newline_to_break=True):\n    # check if closed!!!!!! XXX\n    \"\"\" Prepare a MIME message \"\"\"\n    if isinstance(contents, text_type):\n        contents = [contents]\n    if isinstance(attachments, text_type):\n        attachments = [attachments]\n\n    # merge contents and attachments for now.\n    if attachments is not None:\n        for a in attachments:\n            if not os.path.isfile(a):\n                raise TypeError(\"'{0}' is not a valid filepath\".format(a))\n        contents = attachments if contents is None else contents + attachments\n\n    has_included_images, content_objects = prepare_contents(contents, encoding)\n    msg = MIMEMultipart()\n    if headers is not None:\n        # Strangely, msg does not have an update method, so then manually.\n        for k, v in headers.items():\n            msg[k] = v\n    if headers is None or \"Date\" not in headers:\n        msg[\"Date\"] = formatdate()\n\n    msg_alternative = MIMEMultipart(\"alternative\")\n    msg_related = MIMEMultipart(\"related\")\n    msg_related.attach(\"-- HTML goes here --\")\n    msg.attach(msg_alternative)\n    add_subject(msg, subject)\n    add_recipients_headers(user, useralias, msg, addresses)\n    htmlstr = \"\"\n    altstr = []\n    if has_included_images:\n        msg.preamble = \"This message is best displayed using a MIME capable email reader.\"\n\n    if contents is not None:\n        for content_object, content_string in zip(content_objects, contents):\n            if content_object[\"main_type\"] == \"image\":\n                # all image objects need base64 encoding, so do it now\n                email.encoders.encode_base64(content_object[\"mime_object\"])\n                # aliased image {'path' : 'alias'}\n                if isinstance(content_string, dict) and len(content_string) == 1:\n                    for key in content_string:\n                        hashed_ref = str(abs(hash(key)))\n                        alias = content_string[key]\n                    # pylint: disable=undefined-loop-variable\n                    content_string = key\n                else:\n                    alias = os.path.basename(str(content_string))\n                    hashed_ref = str(abs(hash(alias)))\n\n                # TODO: I should probably remove inline now that there is \"attachments\"\n                # if string is `inline`, inline, else, attach\n                # pylint: disable=unidiomatic-typecheck\n                if type(content_string) == inline:\n                    htmlstr += '<img src=\"cid:{0}\" title=\"{1}\"/>'.format(hashed_ref, alias)\n                    content_object[\"mime_object\"].add_header(\n                        \"Content-ID\", \"<{0}>\".format(hashed_ref)\n                    )\n                    altstr.append(\"-- img {0} should be here -- \".format(alias))\n                    # inline images should be in related MIME block\n                    msg_related.attach(content_object[\"mime_object\"])\n                else:\n                    # non-inline images get attached like any other attachment\n                    msg.attach(content_object[\"mime_object\"])\n\n            else:\n                if content_object[\"encoding\"] == \"base64\":\n                    email.encoders.encode_base64(content_object[\"mime_object\"])\n                    msg.attach(content_object[\"mime_object\"])\n                elif content_object[\"sub_type\"] not in [\"html\", \"plain\"]:\n                    msg.attach(content_object[\"mime_object\"])\n                else:\n                    if newline_to_break:\n                        content_string = content_string.replace(\"\\n\", \"<br>\")\n                    try:\n                        htmlstr += \"<div>{0}</div>\".format(content_string)\n                    except UnicodeEncodeError:\n                        htmlstr += u\"<div>{0}</div>\".format(content_string)\n                    altstr.append(content_string)\n\n    msg_related.get_payload()[0] = MIMEText(htmlstr, \"html\", _charset=encoding)\n    msg_alternative.attach(MIMEText(\"\\n\".join(altstr), _charset=encoding))\n    msg_alternative.attach(msg_related)\n    return msg", "response": "Prepare a MIME message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef validate_email_with_regex(email_address):\n    if not re.match(VALID_ADDRESS_REGEXP, email_address):\n        emsg = 'Emailaddress \"{}\" is not valid according to RFC 2822 standards'.format(\n            email_address)\n        raise YagInvalidEmailAddress(emsg)\n    # apart from the standard, I personally do not trust email addresses without dot.\n    if \".\" not in email_address and \"localhost\" not in email_address.lower():\n        raise YagInvalidEmailAddress(\"Missing dot in emailaddress\")", "response": "Validate an email address with a regular expression."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(\n        self,\n        to=None,\n        subject=None,\n        contents=None,\n        attachments=None,\n        cc=None,\n        bcc=None,\n        preview_only=False,\n        headers=None,\n        newline_to_break=True,\n    ):\n        \"\"\" Use this to send an email with gmail\"\"\"\n        self.login()\n        recipients, msg_string = self.prepare_send(\n            to, subject, contents, attachments, cc, bcc, headers, newline_to_break\n        )\n        if preview_only:\n            return (recipients, msg_string)\n        return self._attempt_send(recipients, msg_string)", "response": "Send an email with gmail"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_unsent(self):\n        for i in range(len(self.unsent)):\n            recipients, msg_string = self.unsent.pop(i)\n            self._attempt_send(recipients, msg_string)", "response": "Send all unsent email to all the users who have not sent them."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncloses the connection to the SMTP server.", "response": "def close(self):\n        \"\"\" Close the connection to the SMTP server \"\"\"\n        self.is_closed = True\n        try:\n            self.smtp.quit()\n        except (TypeError, AttributeError, smtplib.SMTPServerDisconnected):\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlogs-in to the SMTP server using the given password.", "response": "def _login(self, password):\n        \"\"\"\n        Login to the SMTP server using password. `login` only needs to be manually run when the\n        connection to the SMTP server was closed by the user.\n        \"\"\"\n        self.smtp = self.connection(self.host, self.port, **self.kwargs)\n        self.smtp.set_debuglevel(self.debuglevel)\n        if self.starttls:\n            self.smtp.ehlo()\n            if self.starttls is True:\n                self.smtp.starttls()\n            else:\n                self.smtp.starttls(**self.starttls)\n            self.smtp.ehlo()\n        self.is_closed = False\n        if not self.smtp_skip_login:\n            password = self.handle_password(self.user, password)\n            self.smtp.login(self.user, password)\n        self.log.info(\"Connected to SMTP @ %s:%s as %s\", self.host, self.port, self.user)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncancels all requests in the queue so we can exit.", "response": "def cancel_queue(self):\n        \"\"\"\n        Cancel all requests in the queue so we can exit.\n        \"\"\"\n        q = list(self.queue)\n        self.queue = []\n        log.debug(\"Canceling requests: {}\".format(q))\n        for req in q:\n            req.response = APIServerNotRunningErrorResponse()\n        for req in q:\n            req.signal()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndispatching any queued requests.", "response": "def dispatch_queue(self):\n        \"\"\"\n        Dispatch any queued requests.\n\n        Called by the debugger when it stops.\n        \"\"\"\n        self.queue_lock.acquire()\n        q = list(self.queue)\n        self.queue = []\n        self.queue_lock.release()\n        log.debug(\"Dispatching requests: {}\".format(q))\n        for req in q:\n            req.response = self.dispatch_request(req)\n        for req in q:\n            req.signal()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndispatch a request object.", "response": "def dispatch_request(self, req):\n        \"\"\"\n        Dispatch a request object.\n        \"\"\"\n        log.debug(\"Dispatching request: {}\".format(str(req)))\n\n        # make sure it's valid\n        res = None\n        try:\n            req.validate()\n        except MissingFieldError as e:\n            res = APIMissingFieldErrorResponse(str(e))\n\n        # dispatch the request\n        if not res:\n            try:\n                res = req.dispatch()\n            except Exception as e:\n                msg = \"Exception raised while dispatching request: {}\".format(repr(e))\n                log.exception(msg)\n                res = APIGenericErrorResponse(msg)\n\n        log.debug(\"Response: {}\".format(str(res)))\n\n        return res"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_request(self, request):\n        # default to an empty response error\n        res = APIEmptyResponseErrorResponse()\n\n        # perform the request\n        log.debug(\"Client sending request: \" + str(request))\n        response = self.session.post(self.url, data=str(request))\n        data = response.text\n        if response.status_code != 200:\n            res = APIGenericErrorResponse(response.text)\n        elif data and len(data) > 0:\n            log.debug('Client received message: ' + data)\n\n            try:\n                # parse the response data\n                generic_response = APIResponse(data=data)\n\n                # if there's an error, return an error response\n                if generic_response.is_error:\n                    res = APIErrorResponse(data=data)\n                else:\n                    # success; generate a proper response\n                    plugin = voltron.plugin.pm.api_plugin_for_request(request.request)\n                    if plugin and plugin.response_class:\n                        # found a plugin for the request we sent, use its response type\n                        res = plugin.response_class(data=data)\n                    else:\n                        # didn't find a plugin, just return the generic APIResponse we already generated\n                        res = generic_response\n            except Exception as e:\n                log.exception('Exception parsing message: ' + str(e))\n                log.error('Invalid message: ' + data)\n        else:\n            res = APIEmptyResponseErrorResponse()\n\n        return res", "response": "Send a request to the server and return the response or subclass instance."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send_requests(self, *args):\n        threads = [ClientThread(self, req) for req in args]\n        for t in threads:\n            t.start()\n        for t in threads:\n            t.join()\n        exceptions = [t.exception for t in threads if t.exception]\n        if len(exceptions):\n            raise exceptions[0]\n        return [t.response for t in threads]", "response": "Send a set of requests."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef perform_request(self, request_type, *args, **kwargs):\n        # create a request\n        req = api_request(request_type, *args, **kwargs)\n\n        # send it\n        res = self.send_request(req)\n\n        return res", "response": "Create and send a request."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef run(self, build_requests=None, callback=None):\n        if callback:\n            self.callback = callback\n        if build_requests:\n            self.build_requests = build_requests\n\n        def normalise_requests_err(e):\n            try:\n                msg = e.message.args[1].strerror\n            except:\n                try:\n                    msg = e.message.args[0]\n                except:\n                    msg = str(e)\n            return msg\n\n        while not self.done:\n            try:\n\n                # get the server version info\n                if not self.server_version:\n                    self.server_version = self.perform_request('version')\n\n                    # if the server supports async mode, use it, as some views may only work in async mode\n                    if self.server_version.capabilities and 'async' in self.server_version.capabilities:\n                        self.update()\n                        self.block = False\n                    elif self.supports_blocking:\n                        self.block = True\n                    else:\n                        raise BlockingNotSupportedError(\"Debugger requires blocking mode\")\n\n                if self.block:\n                    # synchronous requests\n                    self.update()\n                else:\n                    # async requests, block using a null request until the debugger stops again\n                    res = self.perform_request('null', block=True)\n                    if res.is_success:\n                        self.server_version = res\n                        self.update()\n            except ConnectionError as e:\n                self.callback(error='Error: {}'.format(normalise_requests_err(e)))\n                self.server_version = None\n                time.sleep(1)", "response": "Run the debugger in a loop."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nstarting the client thread.", "response": "def start(self, build_requests=None, callback=None):\n        \"\"\"\n        Run the client using a background thread.\n        \"\"\"\n        if callback:\n            self.callback = callback\n        if build_requests:\n            self.build_requests = build_requests\n\n        # spin off requester thread\n        self.sw = threading.Thread(target=self.run)\n        self.sw.start()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef client_side(func):\n    def inner(*args, **kwargs):\n        if args and hasattr(args[0], 'is_server') and voltron.debugger:\n            raise ClientSideOnlyException(\"This method can only be called on a client-side instance\")\n        return func(*args, **kwargs)\n    return inner", "response": "Decorator to designate an API method applicable only to client - side availabe items."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a transmission - safe dictionary representation of the API message properties.", "response": "def to_dict(self):\n        \"\"\"\n        Return a transmission-safe dictionary representation of the API message properties.\n        \"\"\"\n        d = {field: getattr(self, field) for field in self._top_fields if hasattr(self, field)}\n\n        # set values of data fields\n        d['data'] = {}\n        for field in self._fields:\n            if hasattr(self, field):\n                # base64 encode the field for transmission if necessary\n                if field in self._encode_fields:\n                    val = getattr(self, field)\n                    if val:\n                        val = cast_s(base64.b64encode(cast_b(val)))\n                    d['data'][field] = val\n                else:\n                    d['data'][field] = getattr(self, field)\n\n        return d"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ninitialises an API message from a transmission - safe dictionary.", "response": "def from_dict(self, d):\n        \"\"\"\n        Initialise an API message from a transmission-safe dictionary.\n        \"\"\"\n        for key in d:\n            if key == 'data':\n                for dkey in d['data']:\n                    if dkey in self._encode_fields:\n                        setattr(self, str(dkey), base64.b64decode(d['data'][dkey]))\n                    else:\n                        setattr(self, str(dkey), d['data'][dkey])\n            else:\n                setattr(self, str(key), d[key])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef from_json(self, data):\n        try:\n            d = json.loads(data)\n        except ValueError:\n            raise InvalidMessageException()\n        self.from_dict(d)", "response": "Initialise an API message from a JSON representation."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef validate(self):\n        required_fields = list(filter(lambda x: self._fields[x], self._fields.keys()))\n        for field in (self._top_fields + required_fields):\n            if not hasattr(self, field) or hasattr(self, field) and getattr(self, field) == None:\n                raise MissingFieldError(field)", "response": "Validate the message.\n\n        Ensure all the required fields are present and not None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nwaiting for the request to be dispatched.", "response": "def wait(self):\n        \"\"\"\n        Wait for the request to be dispatched.\n        \"\"\"\n        self.wait_event = threading.Event()\n        timeout = int(self.timeout) if self.timeout else None\n        self.timed_out = not self.wait_event.wait(timeout)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef lock_host(func, *args, **kwargs):\n    def inner(self, *args, **kwargs):\n        self.host_lock.acquire()\n        try:\n            res = func(self, *args, **kwargs)\n            self.host_lock.release()\n        except Exception as e:\n            self.host_lock.release()\n            raise e\n        return res\n    return inner", "response": "A decorator that acquires a lock before accessing the debugger host."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef target_exists(self, target_id=0):\n        try:\n            target = self._target(target_id=target_id)\n        except Exception as e:\n            log.error(\"Exception checking if target exists: {} {}\".format(type(e), e))\n            return False\n        return target is not None", "response": "Returns True or False indicating whether or not the specified target exists and valid."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef target_is_valid(self, target_id=0):\n        try:\n            target = self._target(target_id=target_id)\n        except:\n            return False\n        return target['state'] != \"invalid\"", "response": "Returns True or False indicating whether or not the specified target is present and valid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn True or False indicating whether or not the specified target is busy.", "response": "def target_is_busy(self, target_id=0):\n        \"\"\"\n        Returns True or False indicating whether or not the specified\n        target is busy.\n\n        `target_id` is a target ID (or None for the first target)\n        \"\"\"\n        try:\n            target = self._target(target_id=target_id)\n        except:\n            raise NoSuchTargetException()\n        return target['state'] == \"running\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef api_request(request, *args, **kwargs):\n    plugin = pm.api_plugin_for_request(request)\n    if plugin and plugin.request_class:\n        req = plugin.request_class(*args, **kwargs)\n    else:\n        raise Exception(\"Invalid request type\")\n    return req", "response": "Create an API request."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nregisters a new plugin with the PluginManager.", "response": "def register_plugin(self, plugin):\n        \"\"\"\n        Register a new plugin with the PluginManager.\n\n        `plugin` is a subclass of scruffy's Plugin class.\n\n        This is called by __init__(), but may also be called by the debugger\n        host to load a specific plugin at runtime.\n        \"\"\"\n        if hasattr(plugin, 'initialise'):\n            plugin.initialise()\n        if self.valid_api_plugin(plugin):\n            log.debug(\"Registering API plugin: {}\".format(plugin))\n            self._api_plugins[plugin.request] = plugin()\n        elif self.valid_debugger_plugin(plugin):\n            log.debug(\"Registering debugger plugin: {}\".format(plugin))\n            self._debugger_plugins[plugin.host] = plugin()\n        elif self.valid_view_plugin(plugin):\n            log.debug(\"Registering view plugin: {}\".format(plugin))\n            self._view_plugins[plugin.name] = plugin()\n        elif self.valid_web_plugin(plugin):\n            log.debug(\"Registering web plugin: {}\".format(plugin))\n            self._web_plugins[plugin.name] = plugin()\n        elif self.valid_command_plugin(plugin):\n            log.debug(\"Registering command plugin: {}\".format(plugin))\n            self._command_plugins[plugin.name] = plugin()\n            if voltron.debugger:\n                voltron.debugger.register_command_plugin(plugin.name, plugin.command_class)\n        else:\n            log.debug(\"Ignoring invalid plugin: {}\".format(plugin))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef valid_api_plugin(self, plugin):\n        if (issubclass(plugin, APIPlugin)       and\n            hasattr(plugin, 'plugin_type')      and plugin.plugin_type == 'api' and\n            hasattr(plugin, 'request')          and plugin.request != None and\n            hasattr(plugin, 'request_class')    and plugin.request_class != None and\n            hasattr(plugin, 'response_class')   and plugin.response_class != None):\n            return True\n        return False", "response": "Validate an API plugin ensuring it is an API plugin and has the necessary fields present."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef valid_debugger_plugin(self, plugin):\n        if (issubclass(plugin, DebuggerAdaptorPlugin) and\n            hasattr(plugin, 'plugin_type')      and plugin.plugin_type == 'debugger' and\n            hasattr(plugin, 'host')             and plugin.host != None):\n            return True\n        return False", "response": "Validate a debugger plugin and return a boolean indicating if it is a debugger plugin and has the necessary fields present."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nvalidating a view plugin ensuring it is a view plugin and has the necessary fields present.", "response": "def valid_view_plugin(self, plugin):\n        \"\"\"\n        Validate a view plugin, ensuring it is a view plugin and has the\n        necessary fields present.\n\n        `plugin` is a subclass of scruffy's Plugin class.\n        \"\"\"\n        if (issubclass(plugin, ViewPlugin)      and\n            hasattr(plugin, 'plugin_type')      and plugin.plugin_type == 'view' and\n            hasattr(plugin, 'name')             and plugin.name != None and\n            hasattr(plugin, 'view_class')       and plugin.view_class != None):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates a web plugin ensuring it is a web plugin and has the necessary fields present.", "response": "def valid_web_plugin(self, plugin):\n        \"\"\"\n        Validate a web plugin, ensuring it is a web plugin and has the\n        necessary fields present.\n\n        `plugin` is a subclass of scruffy's Plugin class.\n        \"\"\"\n        if (issubclass(plugin, WebPlugin)      and\n            hasattr(plugin, 'plugin_type')      and plugin.plugin_type == 'web' and\n            hasattr(plugin, 'name')             and plugin.name != None):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nvalidates a command plugin ensuring it is a command plugin and has the necessary fields present.", "response": "def valid_command_plugin(self, plugin):\n        \"\"\"\n        Validate a command plugin, ensuring it is a command plugin and has the\n        necessary fields present.\n\n        `plugin` is a subclass of scruffy's Plugin class.\n        \"\"\"\n        if (issubclass(plugin, CommandPlugin)   and\n            hasattr(plugin, 'plugin_type')      and plugin.plugin_type == 'command' and\n            hasattr(plugin, 'name')             and plugin.name != None):\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks if Voltron is installed on MacOS and Linux.", "response": "def check_install():\n    \"\"\"\n    Try to detect the two most common installation errors:\n\n    1. Installing on macOS using a Homebrew version of Python\n    2. Installing on Linux using Python 2 when GDB is linked with Python 3\n    \"\"\"\n    if platform.system() == 'Darwin' and sys.executable != '/usr/bin/python':\n        print(\"*\" * 79)\n        print(textwrap.fill(\n            \"WARNING: You are not using the version of Python included with \"\n            \"macOS. If you intend to use Voltron with the LLDB included \"\n            \"with Xcode, or GDB installed with Homebrew, it will not work \"\n            \"unless it is installed using the system's default Python. If \"\n            \"you intend to use Voltron with a debugger installed by some \"\n            \"other method, it may be safe to ignore this warning. See the \"\n            \"following documentation for more detailed installation \"\n            \"instructions: \"\n            \"https://github.com/snare/voltron/wiki/Installation\", 79))\n        print(\"*\" * 79)\n    elif platform.system() == 'Linux':\n        try:\n            output = check_output([\n                \"gdb\", \"-batch\", \"-q\", \"--nx\", \"-ex\",\n                \"pi print(sys.version_info.major)\"\n            ]).decode(\"utf-8\")\n            gdb_python = int(output)\n\n            if gdb_python != sys.version_info.major:\n                print(\"*\" * 79)\n                print(textwrap.fill(\n                    \"WARNING: You are installing Voltron using Python {0}.x \"\n                    \"and GDB is linked with Python {1}.x. GDB will not be \"\n                    \"able to load Voltron. Please install using Python {1} \"\n                    \"if you intend to use Voltron with the copy of GDB that \"\n                    \"is installed. See the following documentation for more \"\n                    \"detailed installation instructions: \"\n                    \"https://github.com/snare/voltron/wiki/Installation\"\n                    .format(sys.version_info.major, gdb_python), 79))\n                print(\"*\" * 79)\n        except:\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning the view event loop.", "response": "def run(self):\n        \"\"\"\n        Run the view event loop.\n        \"\"\"\n        def render(results=[], error=None):\n            if len(results) and not results[0].timed_out:\n                self.render(results)\n            elif error:\n                self.do_render(error=error)\n\n        # start the client\n        self.client.start(self.build_requests, render)\n\n        # handle keyboard input\n        try:\n            with self.t.cbreak():\n                val = ''\n                while not self.done:\n                    val = self.t.inkey(timeout=1)\n                    if val:\n                        self.handle_key(val)\n        except KeyboardInterrupt:\n            self.exit()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef handle_key(self, key):\n        try:\n            func = None\n            if key.is_sequence:\n                try:\n                    func = self.config.keymap[key.name]\n                except:\n                    try:\n                        func = self.config.keymap[key.code]\n                    except:\n                        func = self.config.keymap[str(key)]\n            else:\n                func = self.config.keymap[str(key)]\n\n            if func in self.valid_key_funcs:\n                getattr(self, func)()\n        except:\n            raise", "response": "Handle a keypress. Concrete subclasses can implement this method if\n        custom keypresses need to be handled other than for exit and scrolling."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_cast_status(self, cast_status):\n        casts = self._casts\n        group_members = self._mz.members\n        for member_uuid in group_members:\n            if member_uuid not in casts:\n                continue\n            for listener in list(casts[member_uuid]['listeners']):\n                listener.multizone_new_cast_status(\n                    self._group_uuid, cast_status)", "response": "Handle reception of a new CastStatus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new_media_status(self, media_status):\n        casts = self._casts\n        group_members = self._mz.members\n        for member_uuid in group_members:\n            if member_uuid not in casts:\n                continue\n            for listener in list(casts[member_uuid]['listeners']):\n                listener.multizone_new_media_status(\n                    self._group_uuid, media_status)", "response": "Handle reception of a new MediaStatus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef new_connection_status(self, conn_status):\n        if conn_status.status == CONNECTION_STATUS_CONNECTED:\n            self._mz.update_members()\n        if (conn_status.status == CONNECTION_STATUS_DISCONNECTED or\n                conn_status.status == CONNECTION_STATUS_LOST):\n            self._mz.reset_members()", "response": "Handle reception of a new ConnectionStatus."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef multizone_member_added(self, member_uuid):\n        casts = self._casts\n        if member_uuid not in casts:\n            casts[member_uuid] = {'listeners': [],\n                                  'groups': set()}\n        casts[member_uuid]['groups'].add(self._group_uuid)\n        for listener in list(casts[member_uuid]['listeners']):\n            listener.added_to_multizone(self._group_uuid)", "response": "Handle added audio group member."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef multizone_member_removed(self, member_uuid):\n        casts = self._casts\n        if member_uuid not in casts:\n            casts[member_uuid] = {'listeners': [], 'groups': set()}\n        casts[member_uuid]['groups'].discard(self._group_uuid)\n        for listener in list(casts[member_uuid]['listeners']):\n            listener.removed_from_multizone(self._group_uuid)", "response": "Handle removed audio group member."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a multizone to the cache", "response": "def add_multizone(self, group_cast):\n        \"\"\" Start managing a group \"\"\"\n        self._groups[str(group_cast.uuid)] = {\n            'chromecast': group_cast,\n            'listener': Listener(group_cast, self._casts),\n            'members': set()}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef remove_multizone(self, group_uuid):\n        group_uuid = str(group_uuid)\n        group = self._groups.pop(group_uuid, None)\n        # Inform all group members that they are no longer members\n        if group is not None:\n            group['listener']._mz.reset_members()  # noqa: E501 pylint: disable=protected-access\n        for member in self._casts.values():\n            member['groups'].discard(group_uuid)", "response": "Stop managing a multizone"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef register_listener(self, member_uuid, listener):\n        member_uuid = str(member_uuid)\n        if member_uuid not in self._casts:\n            self._casts[member_uuid] = {'listeners': [],\n                                        'groups': set()}\n        self._casts[member_uuid]['listeners'].append(listener)", "response": "Register a listener for audio group changes of a given cast uuid."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when a multizone message is received.", "response": "def receive_message(self, message, data):  # noqa: E501 pylint: disable=too-many-return-statements\n        \"\"\" Called when a multizone message is received. \"\"\"\n        if data[MESSAGE_TYPE] == TYPE_DEVICE_ADDED:\n            uuid = data['device']['deviceId']\n            name = data['device']['name']\n            self._add_member(uuid, name)\n            return True\n\n        if data[MESSAGE_TYPE] == TYPE_DEVICE_REMOVED:\n            uuid = data['deviceId']\n            self._remove_member(uuid)\n            return True\n\n        if data[MESSAGE_TYPE] == TYPE_DEVICE_UPDATED:\n            uuid = data['device']['deviceId']\n            name = data['device']['name']\n            self._add_member(uuid, name)\n            return True\n\n        if data[MESSAGE_TYPE] == TYPE_MULTIZONE_STATUS:\n            members = data['status']['devices']\n            members = \\\n                {member['deviceId']: member['name'] for member in members}\n            removed_members = \\\n                list(set(self._members.keys())-set(members.keys()))\n            added_members = list(set(members.keys())-set(self._members.keys()))\n            _LOGGER.debug(\"(%s) Added members %s, Removed members: %s\",\n                          self._uuid, added_members, removed_members)\n\n            for uuid in removed_members:\n                self._remove_member(uuid)\n            for uuid in added_members:\n                self._add_member(uuid, members[uuid])\n\n            for listener in list(self._status_listeners):\n                listener.multizone_status_received()\n\n            return True\n\n        if data[MESSAGE_TYPE] == TYPE_SESSION_UPDATED:\n            # A temporary group has been formed\n            return True\n\n        if data[MESSAGE_TYPE] == TYPE_CASTING_GROUPS:\n            # Answer to GET_CASTING_GROUPS\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_chromecast_from_host(host, tries=None, retry_wait=None, timeout=None,\n                              blocking=True):\n    \"\"\"Creates a Chromecast object from a zeroconf host.\"\"\"\n    # Build device status from the mDNS info, this information is\n    # the primary source and the remaining will be fetched\n    # later on.\n    ip_address, port, uuid, model_name, friendly_name = host\n    _LOGGER.debug(\"_get_chromecast_from_host %s\", host)\n    cast_type = CAST_TYPES.get(model_name.lower(),\n                               CAST_TYPE_CHROMECAST)\n    device = DeviceStatus(\n        friendly_name=friendly_name, model_name=model_name,\n        manufacturer=None, uuid=uuid, cast_type=cast_type,\n    )\n    return Chromecast(host=ip_address, port=port, device=device, tries=tries,\n                      timeout=timeout, retry_wait=retry_wait,\n                      blocking=blocking)", "response": "Creates a Chromecast object from a zeroconf host."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _get_chromecast_from_service(services, tries=None, retry_wait=None,\n                                 timeout=None, blocking=True):\n    \"\"\"Creates a Chromecast object from a zeroconf service.\"\"\"\n    # Build device status from the mDNS service name info, this\n    # information is the primary source and the remaining will be\n    # fetched later on.\n    services, zconf, uuid, model_name, friendly_name = services\n    _LOGGER.debug(\"_get_chromecast_from_service %s\", services)\n    cast_type = CAST_TYPES.get(model_name.lower(),\n                               CAST_TYPE_CHROMECAST)\n    device = DeviceStatus(\n        friendly_name=friendly_name, model_name=model_name,\n        manufacturer=None, uuid=uuid, cast_type=cast_type,\n    )\n    return Chromecast(host=None, device=device, tries=tries, timeout=timeout,\n                      retry_wait=retry_wait, blocking=blocking,\n                      services=services, zconf=zconf)", "response": "Creates a Chromecast object from a zeroconf service."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of Chromecast objects.", "response": "def get_chromecasts(tries=None, retry_wait=None, timeout=None,\n                    blocking=True, callback=None):\n    \"\"\"\n    Searches the network for chromecast devices.\n\n    If blocking = True, returns a list of discovered chromecast devices.\n    If blocking = False, triggers a callback for each discovered chromecast,\n                         and returns a function which can be executed to stop\n                         discovery.\n\n    May return an empty list if no chromecasts were found.\n\n    Tries is specified if you want to limit the number of times the\n    underlying socket associated with your Chromecast objects will\n    retry connecting if connection is lost or it fails to connect\n    in the first place. The number of seconds spent between each retry\n    can be defined by passing the retry_wait parameter, the default is\n    to wait 5 seconds.\n    \"\"\"\n    if blocking:\n        # Thread blocking chromecast discovery\n        hosts = discover_chromecasts()\n        cc_list = []\n        for host in hosts:\n            try:\n                cc_list.append(_get_chromecast_from_host(\n                    host, tries=tries, retry_wait=retry_wait, timeout=timeout,\n                    blocking=blocking))\n            except ChromecastConnectionError:  # noqa\n                pass\n        return cc_list\n    else:\n        # Callback based chromecast discovery\n        if not callable(callback):\n            raise ValueError(\n                \"Nonblocking discovery requires a callback function.\")\n\n        def internal_callback(name):\n            \"\"\"Called when zeroconf has discovered a new chromecast.\"\"\"\n            try:\n                callback(_get_chromecast_from_host(\n                    listener.services[name], tries=tries,\n                    retry_wait=retry_wait, timeout=timeout, blocking=blocking))\n            except ChromecastConnectionError:  # noqa\n                pass\n\n        def internal_stop():\n            \"\"\"Stops discovery of new chromecasts.\"\"\"\n            stop_discovery(browser)\n\n        listener, browser = start_discovery(internal_callback)\n        return internal_stop"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns whether the CEC data should be ignored.", "response": "def ignore_cec(self):\n        \"\"\" Returns whether the CEC data should be ignored. \"\"\"\n        return self.device is not None and \\\n            any([fnmatch.fnmatchcase(self.device.friendly_name, pattern)\n                 for pattern in IGNORE_CEC])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning if there is currently an app running.", "response": "def is_idle(self):\n        \"\"\" Returns if there is currently an app running. \"\"\"\n        return (self.status is None or\n                self.app_id in (None, IDLE_APP_ID) or\n                (not self.status.is_active_input and not self.ignore_cec))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef new_cast_status(self, status):\n        self.status = status\n        if status:\n            self.status_event.set()", "response": "Called when a new status is received from the Chromecast."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef start_app(self, app_id, force_launch=False):\n        self.logger.info(\"Starting app %s\", app_id)\n\n        self.socket_client.receiver_controller.launch_app(app_id, force_launch)", "response": "Start an app on the Chromecast."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nincrements the volume by delta unless it is already maxed.", "response": "def volume_up(self, delta=0.1):\n        \"\"\" Increment volume by 0.1 (or delta) unless it is already maxed.\n        Returns the new volume.\n\n        \"\"\"\n        if delta <= 0:\n            raise ValueError(\n                \"volume delta must be greater than zero, not {}\".format(delta))\n        return self.set_volume(self.status.volume_level + delta)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef volume_down(self, delta=0.1):\n        if delta <= 0:\n            raise ValueError(\n                \"volume delta must be greater than zero, not {}\".format(delta))\n        return self.set_volume(self.status.volume_level - delta)", "response": "Decrement the volume by delta unless it is already 0. Returns the new volume."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef wait(self, timeout=None):\n        if not self.socket_client.isAlive():\n            self.socket_client.start()\n        self.status_event.wait(timeout=timeout)", "response": "Waits until the cast device is ready for communication."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef disconnect(self, timeout=None, blocking=True):\n        self.socket_client.disconnect()\n        if blocking:\n            self.join(timeout=timeout)", "response": "Disconnects the chromecast and waits for it to terminate."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses a PB2 message into JSON format.", "response": "def _json_from_message(message):\n    \"\"\" Parses a PB2 message into JSON format. \"\"\"\n    try:\n        return json.loads(message.payload_utf8)\n    except ValueError:\n        logger = logging.getLogger(__name__)\n        logger.warning(\"Ignoring invalid json in namespace %s: %s\",\n                       message.namespace, message.payload_utf8)\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a string representation of a PB2 message.", "response": "def _message_to_string(message, data=None):\n    \"\"\" Gives a string representation of a PB2 message. \"\"\"\n    if data is None:\n        data = _json_from_message(message)\n\n    return \"Message {} from {} to {}: {}\".format(\n        message.namespace, message.source_id, message.destination_id, data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new socket with OS - specific parameters.", "response": "def new_socket():\n    \"\"\"\n    Create a new socket with OS-specific parameters\n\n    Try to set SO_REUSEPORT for BSD-flavored systems if it's an option.\n    Catches errors if not.\n    \"\"\"\n    new_sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n    new_sock.setsockopt(socket.SOL_SOCKET, socket.SO_REUSEADDR, 1)\n\n    try:\n        # noinspection PyUnresolvedReferences\n        reuseport = socket.SO_REUSEPORT\n    except AttributeError:\n        pass\n    else:\n        try:\n            new_sock.setsockopt(socket.SOL_SOCKET, reuseport, 1)\n        except (OSError, socket.error) as err:\n            # OSError on python 3, socket.error on python 2\n            if err.errno != errno.ENOPROTOOPT:\n                raise\n\n    return new_sock"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ninitialize a socket to a Chromecast.", "response": "def initialize_connection(self):  # noqa: E501 pylint:disable=too-many-statements, too-many-branches\n        \"\"\"Initialize a socket to a Chromecast, retrying as necessary.\"\"\"\n        tries = self.tries\n\n        if self.socket is not None:\n            self.socket.close()\n            self.socket = None\n\n        # Make sure nobody is blocking.\n        for callback in self._request_callbacks.values():\n            callback['event'].set()\n\n        self.app_namespaces = []\n        self.destination_id = None\n        self.session_id = None\n        self._request_id = 0\n        self._request_callbacks = {}\n        self._open_channels = []\n\n        self.connecting = True\n        retry_log_fun = self.logger.error\n\n        # Dict keeping track of individual retry delay for each named service\n        retries = {}\n\n        def mdns_backoff(service, retry):\n            \"\"\"Exponentional backoff for service name mdns lookups.\"\"\"\n            now = time.time()\n            retry['next_retry'] = now + retry['delay']\n            retry['delay'] = min(retry['delay']*2, 300)\n            retries[service] = retry\n\n        while not self.stop.is_set() and (tries is None or tries > 0):  # noqa: E501 pylint:disable=too-many-nested-blocks\n            # Prune retries dict\n            retries = {key: retries[key] for key in self.services if (\n                key is not None and key in retries)}\n\n            for service in self.services.copy():\n                now = time.time()\n                retry = retries.get(\n                    service, {'delay': self.retry_wait, 'next_retry': now})\n                # If we're connecting to a named service, check if it's time\n                if service and now < retry['next_retry']:\n                    continue\n                try:\n                    self.socket = new_socket()\n                    self.socket.settimeout(self.timeout)\n                    self._report_connection_status(\n                        ConnectionStatus(CONNECTION_STATUS_CONNECTING,\n                                         NetworkAddress(self.host, self.port)))\n                    # Resolve the service name. If service is None, we're\n                    # connecting directly to a host name or IP-address\n                    if service:\n                        host = None\n                        port = None\n                        service_info = get_info_from_service(service,\n                                                             self.zconf)\n                        host, port = get_host_from_service_info(service_info)\n                        if host and port:\n                            try:\n                                self.fn = service_info.properties[b'fn']\\\n                                    .decode('utf-8')\n                            except (AttributeError, KeyError, UnicodeError):\n                                pass\n                            self.logger.debug(\n                                \"[%s:%s] Resolved service %s to %s:%s\",\n                                self.fn or self.host, self.port, service, host,\n                                port)\n                            self.host = host\n                            self.port = port\n                        else:\n                            self.logger.debug(\n                                \"[%s:%s] Failed to resolve service %s\",\n                                self.fn or self.host, self.port, service)\n                            self._report_connection_status(\n                                ConnectionStatus(\n                                    CONNECTION_STATUS_FAILED_RESOLVE,\n                                    NetworkAddress(service, None)))\n                            mdns_backoff(service, retry)\n                            # If zeroconf fails to receive the necessary data,\n                            # try next service\n                            continue\n\n                    self.logger.debug(\"[%s:%s] Connecting to %s:%s\",\n                                      self.fn or self.host, self.port,\n                                      self.host, self.port)\n                    self.socket.connect((self.host, self.port))\n                    self.socket = ssl.wrap_socket(self.socket)\n                    self.connecting = False\n                    self._force_recon = False\n                    self._report_connection_status(\n                        ConnectionStatus(CONNECTION_STATUS_CONNECTED,\n                                         NetworkAddress(self.host, self.port)))\n                    self.receiver_controller.update_status()\n                    self.heartbeat_controller.ping()\n                    self.heartbeat_controller.reset()\n\n                    self.logger.debug(\"[%s:%s] Connected!\",\n                                      self.fn or self.host, self.port)\n                    return\n                except OSError as err:\n                    self.connecting = True\n                    if self.stop.is_set():\n                        self.logger.error(\n                            \"[%s:%s] Failed to connect: %s. \"\n                            \"aborting due to stop signal.\",\n                            self.fn or self.host, self.port, err)\n                        raise ChromecastConnectionError(\"Failed to connect\")\n\n                    self._report_connection_status(\n                        ConnectionStatus(CONNECTION_STATUS_FAILED,\n                                         NetworkAddress(self.host, self.port)))\n                    if service is not None:\n                        retry_log_fun(\n                            \"[%s:%s] Failed to connect to service %s\"\n                            \", retrying in %.1fs\",\n                            self.fn or self.host, self.port,\n                            service, retry['delay'])\n                        mdns_backoff(service, retry)\n                    else:\n                        retry_log_fun(\n                            \"[%s:%s] Failed to connect, retrying in %.1fs\",\n                            self.fn or self.host, self.port, self.retry_wait)\n                    retry_log_fun = self.logger.debug\n\n            # Only sleep if we have another retry remaining\n            if tries is None or tries > 1:\n                self.logger.debug(\n                    \"[%s:%s] Not connected, sleeping for %.1fs. Services: %s\",\n                    self.fn or self.host, self.port,\n                    self.retry_wait, self.services)\n                time.sleep(self.retry_wait)\n\n            if tries:\n                tries -= 1\n\n        self.stop.set()\n        self.logger.error(\"[%s:%s] Failed to connect. No retries.\",\n                          self.fn or self.host, self.port)\n        raise ChromecastConnectionError(\"Failed to connect\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconnects to Chromecast device.", "response": "def connect(self):\n        \"\"\" Connect socket connection to Chromecast device.\n\n            Must only be called if the worker thread will not be started.\n        \"\"\"\n        try:\n            self.initialize_connection()\n        except ChromecastConnectionError:\n            self._report_connection_status(\n                ConnectionStatus(CONNECTION_STATUS_DISCONNECTED,\n                                 NetworkAddress(self.host, self.port)))\n            return"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nregister a new handler.", "response": "def register_handler(self, handler):\n        \"\"\" Register a new namespace handler. \"\"\"\n        self._handlers[handler.namespace] = handler\n\n        handler.registered(self)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncalling when a new cast status has been received.", "response": "def new_cast_status(self, cast_status):\n        \"\"\" Called when a new cast status has been received. \"\"\"\n        new_channel = self.destination_id != cast_status.transport_id\n\n        if new_channel:\n            self.disconnect_channel(self.destination_id)\n\n        self.app_namespaces = cast_status.namespaces\n        self.destination_id = cast_status.transport_id\n        self.session_id = cast_status.session_id\n\n        if new_channel:\n            # If any of the namespaces of the new app are supported\n            # we will automatically connect to it to receive updates\n            for namespace in self.app_namespaces:\n                if namespace in self._handlers:\n                    self._ensure_channel_connected(self.destination_id)\n                    self._handlers[namespace].channel_connected()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconnects to the cast and start polling the socket.", "response": "def run(self):\n        \"\"\" Connect to the cast and start polling the socket. \"\"\"\n        try:\n            self.initialize_connection()\n        except ChromecastConnectionError:\n            self._report_connection_status(\n                ConnectionStatus(CONNECTION_STATUS_DISCONNECTED,\n                                 NetworkAddress(self.host, self.port)))\n            return\n\n        self.heartbeat_controller.reset()\n        self._force_recon = False\n        logging.debug(\"Thread started...\")\n        while not self.stop.is_set():\n\n            if self.run_once() == 1:\n                break\n\n        # Clean up\n        self._cleanup()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef run_once(self):\n        # pylint: disable=too-many-branches, too-many-return-statements\n\n        try:\n            if not self._check_connection():\n                return 0\n        except ChromecastConnectionError:\n            return 1\n\n        # poll the socket\n        can_read, _, _ = select.select([self.socket], [], [], self.polltime)\n\n        # read messages from chromecast\n        message = data = None\n        if self.socket in can_read and not self._force_recon:\n            try:\n                message = self._read_message()\n            except InterruptLoop as exc:\n                if self.stop.is_set():\n                    self.logger.info(\n                        \"[%s:%s] Stopped while reading message, \"\n                        \"disconnecting.\",\n                        self.fn or self.host, self.port)\n                else:\n                    self.logger.error(\n                        \"[%s:%s] Interruption caught without being stopped: \"\n                        \"%s\",\n                        self.fn or self.host, self.port, exc)\n                return 1\n            except ssl.SSLError as exc:\n                if exc.errno == ssl.SSL_ERROR_EOF:\n                    if self.stop.is_set():\n                        return 1\n                raise\n            except socket.error:\n                self._force_recon = True\n                self.logger.error('[%s:%s] Error reading from socket.',\n                                  self.fn or self.host, self.port)\n            else:\n                data = _json_from_message(message)\n        if not message:\n            return 0\n\n        # If we are stopped after receiving a message we skip the message\n        # and tear down the connection\n        if self.stop.is_set():\n            return 1\n\n        # See if any handlers will accept this message\n        self._route_message(message, data)\n\n        if REQUEST_ID in data:\n            callback = self._request_callbacks.pop(data[REQUEST_ID], None)\n            if callback is not None:\n                event = callback['event']\n                callback['response'] = data\n                function = callback['function']\n                event.set()\n                if function:\n                    function(data)\n\n        return 0", "response": "This method is called by the main loop when you want to run the main loop."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _check_connection(self):\n        # check if connection is expired\n        reset = False\n        if self._force_recon:\n            self.logger.warning(\n                \"[%s:%s] Error communicating with socket, resetting \"\n                \"connection\",\n                self.fn or self.host, self.port)\n            reset = True\n\n        elif self.heartbeat_controller.is_expired():\n            self.logger.warning(\n                \"[%s:%s] Heartbeat timeout, resetting connection\",\n                self.fn or self.host, self.port)\n            reset = True\n\n        if reset:\n            for channel in self._open_channels:\n                self.disconnect_channel(channel)\n            self._report_connection_status(\n                ConnectionStatus(CONNECTION_STATUS_LOST,\n                                 NetworkAddress(self.host, self.port)))\n            try:\n                self.initialize_connection()\n            except ChromecastConnectionError:\n                self.stop.set()\n            return False\n        return True", "response": "Checks if the connection is active and if not reconnects it."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nroute a message to any handlers on the message namespace", "response": "def _route_message(self, message, data):\n        \"\"\" Route message to any handlers on the message namespace \"\"\"\n        # route message to handlers\n        if message.namespace in self._handlers:\n\n            # debug messages\n            if message.namespace != NS_HEARTBEAT:\n                self.logger.debug(\n                    \"[%s:%s] Received: %s\", self.fn or self.host, self.port,\n                    _message_to_string(message, data))\n\n            # message handlers\n            try:\n                handled = \\\n                    self._handlers[message.namespace].receive_message(\n                        message, data)\n\n                if not handled:\n                    if data.get(REQUEST_ID) not in self._request_callbacks:\n                        self.logger.debug(\n                            \"[%s:%s] Message unhandled: %s\",\n                            self.fn or self.host, self.port,\n                            _message_to_string(message, data))\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\n                    (\"[%s:%s] Exception caught while sending message to \"\n                     \"controller %s: %s\"), self.fn or self.host, self.port,\n                    type(self._handlers[message.namespace]).__name__,\n                    _message_to_string(message, data))\n\n        else:\n            self.logger.debug(\n                \"[%s:%s] Received unknown namespace: %s\",\n                self.fn or self.host, self.port,\n                _message_to_string(message, data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _report_connection_status(self, status):\n        for listener in self._connection_listeners:\n            try:\n                self.logger.debug(\"[%s:%s] connection listener: %x (%s)\",\n                                  self.fn or self.host, self.port,\n                                  id(listener), type(listener).__name__)\n                listener.new_connection_status(status)\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\n                    \"[%s:%s] Exception thrown when calling connection \"\n                    \"listener\", self.fn or self.host, self.port)", "response": "Report a change in the connection status to all connection listeners."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nread bytes from the socket.", "response": "def _read_bytes_from_socket(self, msglen):\n        \"\"\" Read bytes from the socket. \"\"\"\n        chunks = []\n        bytes_recd = 0\n        while bytes_recd < msglen:\n            if self.stop.is_set():\n                raise InterruptLoop(\"Stopped while reading from socket\")\n            try:\n                chunk = self.socket.recv(min(msglen - bytes_recd, 2048))\n                if chunk == b'':\n                    raise socket.error(\"socket connection broken\")\n                chunks.append(chunk)\n                bytes_recd += len(chunk)\n            except socket.timeout:\n                continue\n            except ssl.SSLError as exc:\n                # Support older ssl implementations which does not raise\n                # socket.timeout on timeouts\n                if _is_ssl_timeout(exc):\n                    continue\n                raise\n        return b''.join(chunks)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nread a message from the socket and converts it to a message.", "response": "def _read_message(self):\n        \"\"\" Reads a message from the socket and converts it to a message. \"\"\"\n        # first 4 bytes is Big-Endian payload length\n        payload_info = self._read_bytes_from_socket(4)\n        read_len = unpack(\">I\", payload_info)[0]\n\n        # now read the payload\n        payload = self._read_bytes_from_socket(read_len)\n\n        # pylint: disable=no-member\n        message = cast_channel_pb2.CastMessage()\n        message.ParseFromString(payload)\n\n        return message"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsends a message to the Chromecast.", "response": "def send_message(self, destination_id, namespace, data,\n                     inc_session_id=False, callback_function=False,\n                     no_add_request_id=False, force=False):\n        \"\"\" Send a message to the Chromecast. \"\"\"\n\n        # namespace is a string containing namespace\n        # data is a dict that will be converted to json\n        # wait_for_response only works if we have a request id\n\n        # If channel is not open yet, connect to it.\n        self._ensure_channel_connected(destination_id)\n\n        request_id = None\n        if not no_add_request_id:\n            request_id = self._gen_request_id()\n            data[REQUEST_ID] = request_id\n\n        if inc_session_id:\n            data[SESSION_ID] = self.session_id\n\n        # pylint: disable=no-member\n        msg = cast_channel_pb2.CastMessage()\n\n        msg.protocol_version = msg.CASTV2_1_0\n        msg.source_id = self.source_id\n        msg.destination_id = destination_id\n        msg.payload_type = cast_channel_pb2.CastMessage.STRING\n        msg.namespace = namespace\n        msg.payload_utf8 = _json_to_payload(data)\n\n        # prepend message with Big-Endian 4 byte payload size\n        be_size = pack(\">I\", msg.ByteSize())\n\n        # Log all messages except heartbeat\n        if msg.namespace != NS_HEARTBEAT:\n            self.logger.debug(\"[%s:%s] Sending: %s\",\n                              self.fn or self.host, self.port,\n                              _message_to_string(msg, data))\n\n        if not force and self.stop.is_set():\n            raise PyChromecastStopped(\"Socket client's thread is stopped.\")\n        if not self.connecting and not self._force_recon:\n            try:\n                if not no_add_request_id and callback_function:\n                    self._request_callbacks[request_id] = {\n                        'event': threading.Event(),\n                        'response': None,\n                        'function': callback_function,\n                    }\n                self.socket.sendall(be_size + msg.SerializeToString())\n            except socket.error:\n                self._request_callbacks.pop(request_id, None)\n                self._force_recon = True\n                self.logger.info('[%s:%s] Error writing to socket.',\n                                 self.fn or self.host, self.port)\n        else:\n            raise NotConnected(\"Chromecast \" + self.host + \":\" +\n                               str(self.port) + \" is connecting...\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nsend a message to the platform.", "response": "def send_platform_message(self, namespace, message, inc_session_id=False,\n                              callback_function_param=False):\n        \"\"\" Helper method to send a message to the platform. \"\"\"\n        return self.send_message(PLATFORM_DESTINATION_ID, namespace, message,\n                                 inc_session_id, callback_function_param)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send_app_message(self, namespace, message, inc_session_id=False,\n                         callback_function_param=False):\n        \"\"\" Helper method to send a message to current running app. \"\"\"\n        if namespace not in self.app_namespaces:\n            raise UnsupportedNamespace(\n                (\"Namespace {} is not supported by current app. \"\n                 \"Supported are {}\").format(namespace,\n                                            \", \".join(self.app_namespaces)))\n\n        return self.send_message(self.destination_id, namespace, message,\n                                 inc_session_id, callback_function_param)", "response": "Send a message to the current running app."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nensure we opened a channel to destination_id.", "response": "def _ensure_channel_connected(self, destination_id):\n        \"\"\" Ensure we opened a channel to destination_id. \"\"\"\n        if destination_id not in self._open_channels:\n            self._open_channels.append(destination_id)\n\n            self.send_message(\n                destination_id, NS_CONNECTION,\n                {MESSAGE_TYPE: TYPE_CONNECT,\n                 'origin': {},\n                 'userAgent': 'PyChromecast',\n                 'senderInfo': {\n                     'sdkType': 2,\n                     'version': '15.605.1.3',\n                     'browserVersion': \"44.0.2403.30\",\n                     'platform': 4,\n                     'systemVersion': 'Macintosh; Intel Mac OS X10_10_3',\n                     'connectionType': 1}},\n                no_add_request_id=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndisconnects a channel with destination_id.", "response": "def disconnect_channel(self, destination_id):\n        \"\"\" Disconnect a channel with destination_id. \"\"\"\n        if destination_id in self._open_channels:\n            try:\n                self.send_message(\n                    destination_id, NS_CONNECTION,\n                    {MESSAGE_TYPE: TYPE_CLOSE, 'origin': {}},\n                    no_add_request_id=True, force=True)\n            except NotConnected:\n                pass\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\"[%s:%s] Exception\",\n                                      self.fn or self.host, self.port)\n\n            self._open_channels.remove(destination_id)\n\n            self.handle_channel_disconnected()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles a channel being disconnected.", "response": "def handle_channel_disconnected(self):\n        \"\"\" Handles a channel being disconnected. \"\"\"\n        for namespace in self.app_namespaces:\n            if namespace in self._handlers:\n                self._handlers[namespace].channel_disconnected()\n\n        self.app_namespaces = []\n        self.destination_id = None\n        self.session_id = None"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling when a connection message is received.", "response": "def receive_message(self, message, data):\n        \"\"\" Called when a connection message is received. \"\"\"\n        if self._socket_client.is_stopped:\n            return True\n\n        if data[MESSAGE_TYPE] == TYPE_CLOSE:\n            # The cast device is asking us to acknowledge closing this channel.\n            self._socket_client.disconnect_channel(message.source_id)\n\n            # Schedule a status update so that a channel is created.\n            self._socket_client.receiver_controller.update_status()\n\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall when a heartbeat message is received.", "response": "def receive_message(self, message, data):\n        \"\"\" Called when a heartbeat message is received. \"\"\"\n        if self._socket_client.is_stopped:\n            return True\n\n        if data[MESSAGE_TYPE] == TYPE_PING:\n            try:\n                self._socket_client.send_message(\n                    PLATFORM_DESTINATION_ID, self.namespace,\n                    {MESSAGE_TYPE: TYPE_PONG}, no_add_request_id=True)\n            except PyChromecastStopped:\n                self._socket_client.logger.debug(\n                    \"Heartbeat error when sending response, \"\n                    \"Chromecast connection has stopped\")\n\n            return True\n\n        elif data[MESSAGE_TYPE] == TYPE_PONG:\n            self.reset()\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending a ping message.", "response": "def ping(self):\n        \"\"\" Send a ping message. \"\"\"\n        self.last_ping = time.time()\n        try:\n            self.send_message({MESSAGE_TYPE: TYPE_PING})\n        except NotConnected:\n            self._socket_client.logger.error(\"Chromecast is disconnected. \" +\n                                             \"Cannot ping until reconnected.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_expired(self):\n        if time.time() - self.last_ping > HB_PING_TIME:\n            self.ping()\n\n        return (time.time() - self.last_pong) > HB_PING_TIME + HB_PONG_TIME", "response": "Indicates if the connection has expired."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling when a receiver message has been received.", "response": "def receive_message(self, message, data):\n        \"\"\" Called when a receiver-message has been received. \"\"\"\n        if data[MESSAGE_TYPE] == TYPE_RECEIVER_STATUS:\n            self._process_get_status(data)\n\n            return True\n\n        elif data[MESSAGE_TYPE] == TYPE_LAUNCH_ERROR:\n            self._process_launch_error(data)\n\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsends a message to the Chromecast to update the status of the current Chromecast.", "response": "def update_status(self, callback_function_param=False):\n        \"\"\" Sends a message to the Chromecast to update the status. \"\"\"\n        self.logger.debug(\"Receiver:Updating status\")\n        self.send_message({MESSAGE_TYPE: TYPE_GET_STATUS},\n                          callback_function=callback_function_param)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nlaunch an app on the Chromecast.", "response": "def launch_app(self, app_id, force_launch=False, callback_function=False):\n        \"\"\" Launches an app on the Chromecast.\n\n            Will only launch if it is not currently running unless\n            force_launch=True. \"\"\"\n\n        if not force_launch and self.app_id is None:\n            self.update_status(lambda response:\n                               self._send_launch_message(app_id, force_launch,\n                                                         callback_function))\n        else:\n            self._send_launch_message(app_id, force_launch, callback_function)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef stop_app(self, callback_function_param=False):\n        self.logger.info(\"Receiver:Stopping current app '%s'\", self.app_id)\n        return self.send_message(\n            {MESSAGE_TYPE: 'STOP'},\n            inc_session_id=True, callback_function=callback_function_param)", "response": "Stops the current running app on the Chromecast."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_volume(self, volume):\n        volume = min(max(0, volume), 1)\n        self.logger.info(\"Receiver:setting volume to %.1f\", volume)\n        self.send_message({MESSAGE_TYPE: 'SET_VOLUME',\n                           'volume': {'level': volume}})\n        return volume", "response": "Allows to set volume. Should be value between 0.. 1. Returns the new volume."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing a STATUS message and returns a CastStatus object.", "response": "def _parse_status(data, cast_type):\n        \"\"\"\n        Parses a STATUS message and returns a CastStatus object.\n\n        :type data: dict\n        :param cast_type: Type of Chromecast.\n        :rtype: CastStatus\n        \"\"\"\n        data = data.get('status', {})\n\n        volume_data = data.get('volume', {})\n\n        try:\n            app_data = data['applications'][0]\n        except KeyError:\n            app_data = {}\n\n        is_audio = cast_type in (CAST_TYPE_AUDIO, CAST_TYPE_GROUP)\n\n        status = CastStatus(\n            data.get('isActiveInput', None if is_audio else False),\n            data.get('isStandBy', None if is_audio else True),\n            volume_data.get('level', 1.0),\n            volume_data.get('muted', False),\n            app_data.get(APP_ID),\n            app_data.get('displayName'),\n            [item['name'] for item in app_data.get('namespaces', [])],\n            app_data.get(SESSION_ID),\n            app_data.get('transportId'),\n            app_data.get('statusText', '')\n        )\n        return status"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _process_get_status(self, data):\n        status = self._parse_status(data, self.cast_type)\n        is_new_app = self.app_id != status.app_id and self.app_to_launch\n        self.status = status\n\n        self.logger.debug(\"Received status: %s\", self.status)\n        self._report_status()\n\n        if is_new_app and self.app_to_launch == self.app_id:\n            self.app_to_launch = None\n            self.app_launch_event.set()\n            if self.app_launch_event_function:\n                self.logger.debug(\"Start app_launch_event_function...\")\n                self.app_launch_event_function()\n                self.app_launch_event_function = None", "response": "Processes a STATUS message and notifies listeners."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreports the current status to all status listeners.", "response": "def _report_status(self):\n        \"\"\" Reports the current status to all listeners. \"\"\"\n        for listener in self._status_listeners:\n            try:\n                listener.new_cast_status(self.status)\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\n                    \"Exception thrown when calling cast status listener\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _parse_launch_error(data):\n        return LaunchFailure(\n            data.get(ERROR_REASON, None),\n            data.get(APP_ID),\n            data.get(REQUEST_ID),\n        )", "response": "Parses a LAUNCH_ERROR message and returns a LaunchFailure object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nprocessing a LAUNCH_ERROR message and notifies listeners.", "response": "def _process_launch_error(self, data):\n        \"\"\"\n        Processes a received LAUNCH_ERROR message and notifies listeners.\n        \"\"\"\n        launch_failure = self._parse_launch_error(data)\n        self.launch_failure = launch_failure\n\n        if self.app_to_launch:\n            self.app_to_launch = None\n            self.app_launch_event.set()\n\n        self.logger.debug(\"Launch status: %s\", launch_failure)\n\n        for listener in self._launch_error_listeners:\n            try:\n                listener.new_launch_error(launch_failure)\n            except Exception:  # pylint: disable=broad-except\n                self.logger.exception(\n                    \"Exception thrown when calling launch error listener\")"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncalls when controller is destroyed.", "response": "def tear_down(self):\n        \"\"\" Called when controller is destroyed. \"\"\"\n        super(ReceiverController, self).tear_down()\n\n        self.status = None\n        self.launch_failure = None\n        self.app_to_launch = None\n        self.app_launch_event.clear()\n\n        self._status_listeners[:] = []"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nloads a URL and launches the app.", "response": "def load_url(self, url, force=False, reload_seconds=0,\n                 callback_function=None):\n        \"\"\"\n        Starts loading a URL with an optional reload time\n        in seconds.\n\n        Setting force to True may load pages which block\n        iframe embedding, but will prevent reload from\n        working and will cause calls to load_url()\n        to reload the app.\n        \"\"\"\n        def launch_callback():\n            \"\"\"Loads requested URL after app launched.\"\"\"\n            should_reload = not force and reload_seconds not in (0, None)\n            reload_milliseconds = (0 if not should_reload\n                                   else reload_seconds * 1000)\n            msg = {\n                \"url\": url,\n                \"force\": force,\n                \"reload\": should_reload,\n                \"reload_time\": reload_milliseconds\n            }\n\n            self.send_message(msg, inc_session_id=True,\n                              callback_function=callback_function)\n\n        self.launch(callback_function=launch_callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_status(host, services, zconf, path):\n\n    if not host:\n        for service in services.copy():\n            service_info = get_info_from_service(service, zconf)\n            host, _ = get_host_from_service_info(service_info)\n            if host:\n                _LOGGER.debug(\"Resolved service %s to %s\", service, host)\n                break\n\n    req = CC_SESSION.get(FORMAT_BASE_URL.format(host) + path, timeout=10)\n\n    req.raise_for_status()\n\n    # The Requests library will fall back to guessing the encoding in case\n    # no encoding is specified in the response headers - which is the case\n    # for the Chromecast.\n    # The standard mandates utf-8 encoding, let's fall back to that instead\n    # if no encoding is provided, since the autodetection does not always\n    # provide correct results.\n    if req.encoding is None:\n        req.encoding = 'utf-8'\n\n    return req.json()", "response": "Fetch status from Chromecast."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the status of a single device in a virtual environment.", "response": "def get_device_status(host, services=None, zconf=None):\n    \"\"\"\n    :param host: Hostname or ip to fetch status from\n    :type host: str\n    :return: The device status as a named tuple.\n    :rtype: pychromecast.dial.DeviceStatus or None\n    \"\"\"\n\n    try:\n        status = _get_status(\n            host, services, zconf, \"/setup/eureka_info?options=detail\")\n\n        friendly_name = status.get('name', \"Unknown Chromecast\")\n        model_name = \"Unknown model name\"\n        manufacturer = \"Unknown manufacturer\"\n        if 'detail' in status:\n            model_name = status['detail'].get('model_name', model_name)\n            manufacturer = status['detail'].get('manufacturer', manufacturer)\n\n        udn = status.get('ssdp_udn', None)\n\n        cast_type = CAST_TYPES.get(model_name.lower(),\n                                   CAST_TYPE_CHROMECAST)\n\n        uuid = None\n        if udn:\n            uuid = UUID(udn.replace('-', ''))\n\n        return DeviceStatus(friendly_name, model_name, manufacturer,\n                            uuid, cast_type)\n\n    except (requests.exceptions.RequestException, OSError, ValueError):\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_multizone_status(host, services=None, zconf=None):\n\n    try:\n        status = status = _get_status(\n            host, services, zconf, \"/setup/eureka_info?params=multizone\")\n\n        dynamic_groups = []\n        if 'multizone' in status and 'dynamic_groups' in status['multizone']:\n            for group in status['multizone']['dynamic_groups']:\n                name = group.get('name', \"Unknown group name\")\n                udn = group.get('uuid', None)\n                uuid = None\n                if udn:\n                    uuid = UUID(udn.replace('-', ''))\n                dynamic_groups.append(MultizoneInfo(name, uuid))\n\n        groups = []\n        if 'multizone' in status and 'groups' in status['multizone']:\n            for group in status['multizone']['groups']:\n                name = group.get('name', \"Unknown group name\")\n                udn = group.get('uuid', None)\n                uuid = None\n                if udn:\n                    uuid = UUID(udn.replace('-', ''))\n                groups.append(MultizoneInfo(name, uuid))\n\n        return MultizoneStatus(dynamic_groups, groups)\n\n    except (requests.exceptions.RequestException, OSError, ValueError):\n        return None", "response": "Get the multizone status from the specified host."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nstart a session if it is not yet initialized.", "response": "def start_session_if_none(self):\n        \"\"\"\n        Starts a session it is not yet initialized.\n        \"\"\"\n        if not (self._screen_id and self._session):\n            self.update_screen_id()\n            self._session = YouTubeSession(screen_id=self._screen_id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nplay video(video_id) now. This ignores the current play queue order. :param video_id: YouTube video id(http://youtube.com/watch?v=video_id) :param playlist_id: youtube.com/watch?v=video_id&list=playlist_id", "response": "def play_video(self, video_id, playlist_id=None):\n        \"\"\"\n        Play video(video_id) now. This ignores the current play queue order.\n        :param video_id: YouTube video id(http://youtube.com/watch?v=video_id)\n        :param playlist_id: youtube.com/watch?v=video_id&list=playlist_id\n        \"\"\"\n        self.start_session_if_none()\n        self._session.play_video(video_id, playlist_id)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update_screen_id(self):\n        self.status_update_event.clear()\n        # This gets the screenId but always throws. Couldn't find a better way.\n        try:\n            self.send_message({MESSAGE_TYPE: TYPE_GET_SCREEN_ID})\n        except UnsupportedNamespace:\n            pass\n        self.status_update_event.wait()\n        self.status_update_event.clear()", "response": "Sends a getMdxSessionStatus to get the screenId and waits for response."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncall when a media message is received.", "response": "def receive_message(self, message, data):\n        \"\"\" Called when a media message is received. \"\"\"\n        if data[MESSAGE_TYPE] == TYPE_STATUS:\n            self._process_status(data.get(\"data\"))\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprocess latest status update.", "response": "def _process_status(self, status):\n        \"\"\" Process latest status update. \"\"\"\n        self._screen_id = status.get(ATTR_SCREEN_ID)\n        self.status_update_event.set()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nstarts discovery of chromecasts on the network.", "response": "def start_discovery(add_callback=None, remove_callback=None):\n    \"\"\"\n    Start discovering chromecasts on the network.\n\n    This method will start discovering chromecasts on a separate thread. When\n    a chromecast is discovered, the callback will be called with the\n    discovered chromecast's zeroconf name. This is the dictionary key to find\n    the chromecast metadata in listener.services.\n\n    This method returns the CastListener object and the zeroconf ServiceBrowser\n    object. The CastListener object will contain information for the discovered\n    chromecasts. To stop discovery, call the stop_discovery method with the\n    ServiceBrowser object.\n    \"\"\"\n    listener = CastListener(add_callback, remove_callback)\n    service_browser = False\n    try:\n        service_browser = zeroconf.ServiceBrowser(zeroconf.Zeroconf(),\n                                                  \"_googlecast._tcp.local.\",\n                                                  listener)\n    except (zeroconf.BadTypeInNameException,\n            NotImplementedError,\n            OSError,\n            socket.error,\n            zeroconf.NonUniqueNameException):\n        pass\n\n    return listener, service_browser"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndiscovers chromecasts on the network.", "response": "def discover_chromecasts(max_devices=None, timeout=DISCOVER_TIMEOUT):\n    \"\"\" Discover chromecasts on the network. \"\"\"\n    from threading import Event\n    browser = False\n    try:\n        # pylint: disable=unused-argument\n        def callback(name):\n            \"\"\"Called when zeroconf has discovered a new chromecast.\"\"\"\n            if max_devices is not None and listener.count >= max_devices:\n                discover_complete.set()\n\n        discover_complete = Event()\n        listener, browser = start_discovery(callback)\n\n        # Wait for the timeout or the maximum number of devices\n        discover_complete.wait(timeout)\n\n        return listener.devices\n    except Exception:  # pylint: disable=broad-except\n        raise\n    finally:\n        if browser is not False:\n            stop_discovery(browser)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nresolving service_info from service.", "response": "def get_info_from_service(service, zconf):\n    \"\"\" Resolve service_info from service. \"\"\"\n    service_info = None\n    try:\n        service_info = zconf.get_service_info('_googlecast._tcp.local.',\n                                              service)\n        if service_info:\n            _LOGGER.debug(\n                \"get_info_from_service resolved service %s to service_info %s\",\n                service, service_info)\n    except IOError:\n        pass\n    return service_info"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting hostname or IP from service_info.", "response": "def get_host_from_service_info(service_info):\n    \"\"\" Get hostname or IP from service_info. \"\"\"\n    host = None\n    port = None\n    if (service_info and service_info.port and\n            (service_info.server or service_info.address)):\n        if service_info.address:\n            host = socket.inet_ntoa(service_info.address)\n        else:\n            host = service_info.server.lower()\n        port = service_info.port\n    return (host, port)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove a service from the collection.", "response": "def remove_service(self, zconf, typ, name):\n        \"\"\" Remove a service from the collection. \"\"\"\n        _LOGGER.debug(\"remove_service %s, %s\", typ, name)\n        service = self.services.pop(name, None)\n\n        if not service:\n            _LOGGER.debug(\"remove_service unknown %s, %s\", typ, name)\n            return\n\n        if self.remove_callback:\n            self.remove_callback(name, service)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a service to the collection.", "response": "def add_service(self, zconf, typ, name):\n        \"\"\" Add a service to the collection. \"\"\"\n        service = None\n        tries = 0\n        _LOGGER.debug(\"add_service %s, %s\", typ, name)\n        while service is None and tries < 4:\n            try:\n                service = zconf.get_service_info(typ, name)\n            except IOError:\n                # If the zeroconf fails to receive the necessary data we abort\n                # adding the service\n                break\n            tries += 1\n\n        if not service:\n            _LOGGER.debug(\"add_service failed to add %s, %s\", typ, name)\n            return\n\n        def get_value(key):\n            \"\"\"Retrieve value and decode to UTF-8.\"\"\"\n            value = service.properties.get(key.encode('utf-8'))\n\n            if value is None or isinstance(value, str):\n                return value\n            return value.decode('utf-8')\n\n        ips = zconf.cache.entries_with_name(service.server.lower())\n        host = repr(ips[0]) if ips else service.server\n\n        model_name = get_value('md')\n        uuid = get_value('id')\n        friendly_name = get_value('fn')\n\n        if uuid:\n            uuid = UUID(uuid)\n\n        self.services[name] = (host, service.port, uuid, model_name,\n                               friendly_name)\n\n        if self.add_callback:\n            self.add_callback(name)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlaunching Spotify application. Will raise a LaunchError exception if there is no response from the Spotify app within timeout seconds.", "response": "def launch_app(self, timeout=10):\n        \"\"\"\n        Launch Spotify application.\n\n        Will raise a LaunchError exception if there is no response from the\n        Spotify app within timeout seconds.\n        \"\"\"\n\n        def callback():\n            \"\"\"Callback function\"\"\"\n            self.send_message({\"type\": TYPE_STATUS,\n                               \"credentials\": self.access_token,\n                               \"expiresIn\": self.expires})\n\n        self.launch(callback_function=callback)\n\n        # Need to wait for Spotify to be launched on Chromecast completely\n        while not self.is_launched and timeout:\n            time.sleep(1)\n            timeout -= 1\n\n        if not self.is_launched:\n            raise LaunchError(\n                \"Timeout when waiting for status response from Spotify app\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef launch(self, callback_function=None):\n        self._check_registered()\n\n        self._socket_client.receiver_controller.launch_app(\n            self.supporting_app_id, callback_function=callback_function)", "response": "Launches the app related to the controller."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls when a controller is registered.", "response": "def registered(self, socket_client):\n        \"\"\" Called when a controller is registered. \"\"\"\n        self._socket_client = socket_client\n\n        if self.target_platform:\n            self._message_func = self._socket_client.send_platform_message\n        else:\n            self._message_func = self._socket_client.send_app_message"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_message(self, data, inc_session_id=False,\n                     callback_function=None):\n        \"\"\"\n        Send a message on this namespace to the Chromecast.\n\n        Will raise a NotConnected exception if not connected.\n        \"\"\"\n        self._check_registered()\n\n        if not self.target_platform and \\\n           self.namespace not in self._socket_client.app_namespaces:\n            if self.supporting_app_id is not None:\n                self.launch()\n\n            else:\n                raise UnsupportedNamespace(\n                    (\"Namespace {} is not supported by running\"\n                     \"application.\").format(self.namespace))\n\n        return self._message_func(\n            self.namespace, data, inc_session_id, callback_function)", "response": "Send a message to the Chromecast."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning calculated current seek time of media in seconds", "response": "def adjusted_current_time(self):\n        \"\"\" Returns calculated current seek time of media in seconds \"\"\"\n        if self.player_state == MEDIA_PLAYER_STATE_PLAYING:\n            # Add time since last update\n            return (self.current_time +\n                    (datetime.utcnow() - self.last_updated).total_seconds())\n        # Not playing, return last reported seek time\n        return self.current_time"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef images(self):\n        return [\n            MediaImage(item.get('url'), item.get('height'), item.get('width'))\n            for item in self.media_metadata.get('images', [])\n        ]", "response": "Return a list of MediaImage objects for this media."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef update(self, data):\n        if not data.get('status', []):\n            return\n\n        status_data = data['status'][0]\n        media_data = status_data.get('media') or {}\n        volume_data = status_data.get('volume', {})\n\n        self.current_time = status_data.get('currentTime', self.current_time)\n        self.content_id = media_data.get('contentId', self.content_id)\n        self.content_type = media_data.get('contentType', self.content_type)\n        self.duration = media_data.get('duration', self.duration)\n        self.stream_type = media_data.get('streamType', self.stream_type)\n        self.idle_reason = status_data.get('idleReason', self.idle_reason)\n        self.media_session_id = status_data.get(\n            'mediaSessionId', self.media_session_id)\n        self.playback_rate = status_data.get(\n            'playbackRate', self.playback_rate)\n        self.player_state = status_data.get('playerState', self.player_state)\n        self.supported_media_commands = status_data.get(\n            'supportedMediaCommands', self.supported_media_commands)\n        self.volume_level = volume_data.get('level', self.volume_level)\n        self.volume_muted = volume_data.get('muted', self.volume_muted)\n        self.media_custom_data = media_data.get(\n            'customData', self.media_custom_data)\n        self.media_metadata = media_data.get('metadata', self.media_metadata)\n        self.subtitle_tracks = media_data.get('tracks', self.subtitle_tracks)\n        self.current_subtitle_tracks = status_data.get(\n            'activeTrackIds', self.current_subtitle_tracks)\n        self.last_updated = datetime.utcnow()", "response": "Update the internal state of the internal state of the class."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling when a message is received.", "response": "def receive_message(self, message, data):\n        \"\"\" Called when a media message is received. \"\"\"\n        if data[MESSAGE_TYPE] == TYPE_MEDIA_STATUS:\n            self._process_media_status(data)\n\n            return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a command to the Chromecast on media channel.", "response": "def _send_command(self, command):\n        \"\"\" Send a command to the Chromecast on media channel. \"\"\"\n        if self.status is None or self.status.media_session_id is None:\n            self.logger.warning(\n                \"%s command requested but no session is active.\",\n                command[MESSAGE_TYPE])\n            return\n\n        command['mediaSessionId'] = self.status.media_session_id\n\n        self.send_message(command, inc_session_id=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef thumbnail(self):\n        if not self.status:\n            return None\n\n        images = self.status.images\n\n        return images[0].url if images else None", "response": "Return the url of the current playing item thumbnail."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nprocesses a STATUS message.", "response": "def _process_media_status(self, data):\n        \"\"\" Processes a STATUS message. \"\"\"\n        self.status.update(data)\n\n        self.logger.debug(\"Media:Received status %s\", data)\n\n        # Update session active threading event\n        if self.status.media_session_id is None:\n            self.session_active_event.clear()\n        else:\n            self.session_active_event.set()\n\n        self._fire_status_changed()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _fire_status_changed(self):\n        for listener in self._status_listeners:\n            try:\n                listener.new_media_status(self.status)\n            except Exception:  # pylint: disable=broad-except\n                _LOGGER.exception(\"Exception thrown when calling media status \"\n                                  \"callback\")", "response": "Tells listeners of a status changed."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nplay a media on the Chromecast.", "response": "def play_media(self, url, content_type, title=None, thumb=None,\n                   current_time=0, autoplay=True,\n                   stream_type=STREAM_TYPE_BUFFERED,\n                   metadata=None, subtitles=None, subtitles_lang='en-US',\n                   subtitles_mime='text/vtt', subtitle_id=1):\n        \"\"\"\n        Plays media on the Chromecast. Start default media receiver if not\n        already started.\n\n        Parameters:\n        url: str - url of the media.\n        content_type: str - mime type. Example: 'video/mp4'.\n        title: str - title of the media.\n        thumb: str - thumbnail image url.\n        current_time: float - seconds from the beginning of the media\n            to start playback.\n        autoplay: bool - whether the media will automatically play.\n        stream_type: str - describes the type of media artifact as one of the\n            following: \"NONE\", \"BUFFERED\", \"LIVE\".\n        subtitles: str - url of subtitle file to be shown on chromecast.\n        subtitles_lang: str - language for subtitles.\n        subtitles_mime: str - mimetype of subtitles.\n        subtitle_id: int - id of subtitle to be loaded.\n        metadata: dict - media metadata object, one of the following:\n            GenericMediaMetadata, MovieMediaMetadata, TvShowMediaMetadata,\n            MusicTrackMediaMetadata, PhotoMediaMetadata.\n\n        Docs:\n        https://developers.google.com/cast/docs/reference/messages#MediaData\n        \"\"\"\n        # pylint: disable=too-many-locals\n        def app_launched_callback():\n            \"\"\"Plays media after chromecast has switched to requested app.\"\"\"\n            self._send_start_play_media(\n                url, content_type, title, thumb, current_time, autoplay,\n                stream_type, metadata, subtitles, subtitles_lang,\n                subtitles_mime, subtitle_id)\n\n        receiver_ctrl = self._socket_client.receiver_controller\n        receiver_ctrl.launch_app(self.app_id,\n                                 callback_function=app_launched_callback)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn all possible app ids.", "response": "def get_possible_app_ids():\n    \"\"\" Returns all possible app ids. \"\"\"\n\n    try:\n        req = requests.get(\n            \"https://clients3.google.com/cast/chromecast/device/baseconfig\")\n        data = json.loads(req.text[4:])\n\n        return [app['app_id'] for app in data['applications']] + \\\n            data[\"enabled_app_ids\"]\n\n    except ValueError:\n        # If json fails to parse\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_app_config(app_id):\n    try:\n        req = requests.get(\n            (\"https://clients3.google.com/\"\n             \"cast/chromecast/device/app?a={}\").format(app_id))\n\n        return json.loads(req.text[4:]) if req.status_code == 200 else {}\n\n    except ValueError:\n        # If json fails to parse\n        return {}", "response": "Get specific configuration for app_id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_contents(self, path):\n        '''\n        Loads the contents of the file specified by path\n\n        Args:\n            path (string): The relative or absolute path to the file to\n                be loaded.  If the path is relative, then it is combined\n                with the base_path to generate a full path string\n\n        Returns:\n            string: The contents of the file as a string\n\n        Raises:\n            ConfigurationError: If the file cannot be loaded\n        '''\n        try:\n            if not os.path.exists(path):\n                raise ConfigurationError('specified path does not exist %s' % path)\n\n            with open(path) as f:\n                data = f.read()\n\n            return data\n\n        except (IOError, OSError) as exc:\n            raise ConfigurationError('error trying to load file contents: %s' % exc)", "response": "Loads the contents of the file specified by path and returns it as a string"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef abspath(self, path):\n        '''\n        Transform the path to an absolute path\n\n        Args:\n            path (string): The path to transform to an absolute path\n\n        Returns:\n            string: The absolute path to the file\n        '''\n        if not path.startswith(os.path.sep) or path.startswith('~'):\n            path = os.path.expanduser(os.path.join(self.base_path, path))\n        return path", "response": "Transform the path to an absolute path"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nloading the contents of a file into a dict and returns it.", "response": "def load_file(self, path, objtype=None, encoding='utf-8'):\n        '''\n        Load the file specified by path\n\n        This method will first try to load the file contents from cache and\n        if there is a cache miss, it will load the contents from disk\n\n        Args:\n            path (string): The full or relative path to the file to be loaded\n\n            encoding (string): The file contents text encoding\n\n            objtype (object): The object type of the file contents.  This\n                is used to type check the deserialized content against the\n                contents loaded from disk.\n                Ignore serializing if objtype is string_types\n\n        Returns:\n            object: The deserialized file contents which could be either a\n                string object or a dict object\n\n        Raises:\n            ConfigurationError:\n        '''\n        path = self.abspath(path)\n        debug('file path is %s' % path)\n\n        if path in self._cache:\n            return self._cache[path]\n\n        try:\n            debug('cache miss, attempting to load file from disk: %s' % path)\n            contents = parsed_data = self.get_contents(path)\n            if encoding:\n                parsed_data = contents.encode(encoding)\n        except ConfigurationError as exc:\n            debug(exc)\n            raise\n        except UnicodeEncodeError:\n            raise ConfigurationError('unable to encode file contents')\n\n        if objtype is not string_types:\n            for deserializer in (self._load_json, self._load_yaml):\n                parsed_data = deserializer(contents)\n                if parsed_data:\n                    break\n\n            if objtype and not isinstance(parsed_data, objtype):\n                debug('specified file %s is not of type %s' % (path, objtype))\n                raise ConfigurationError('invalid file serialization type for contents')\n\n        self._cache[path] = parsed_data\n        return parsed_data"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef prepare(self):\n        # ansible_path = find_executable('ansible')\n        # if ansible_path is None or not os.access(ansible_path, os.X_OK):\n        #     raise ConfigurationError(\"Ansible not found. Make sure that it is installed.\")\n        if self.private_data_dir is None:\n            raise ConfigurationError(\"Runner Base Directory is not defined\")\n        if self.module and self.playbook:\n            raise ConfigurationError(\"Only one of playbook and module options are allowed\")\n        if not os.path.exists(self.artifact_dir):\n            os.makedirs(self.artifact_dir, mode=0o700)\n        if self.directory_isolation_path is not None:\n            self.directory_isolation_path = tempfile.mkdtemp(prefix='runner_di_', dir=self.directory_isolation_path)\n            if os.path.exists(self.project_dir):\n                output.debug(\"Copying directory tree from {} to {} for working directory isolation\".format(self.project_dir,\n                                                                                                           self.directory_isolation_path))\n                copy_tree(self.project_dir, self.directory_isolation_path, preserve_symlinks=True)\n\n        self.prepare_inventory()\n        self.prepare_env()\n        self.prepare_command()\n\n        if self.execution_mode == ExecutionMode.ANSIBLE_PLAYBOOK and self.playbook is None:\n            raise ConfigurationError(\"Runner playbook required when running ansible-playbook\")\n        elif self.execution_mode == ExecutionMode.ANSIBLE and self.module is None:\n            raise ConfigurationError(\"Runner module required when running ansible\")\n        elif self.execution_mode == ExecutionMode.NONE:\n            raise ConfigurationError(\"No executable for runner to run\")\n\n        # write the SSH key data into a fifo read by ssh-agent\n        if self.ssh_key_data:\n            self.ssh_key_path = os.path.join(self.artifact_dir, 'ssh_key_data')\n            open_fifo_write(self.ssh_key_path, self.ssh_key_data)\n            self.command = self.wrap_args_with_ssh_agent(self.command, self.ssh_key_path)\n\n        # Use local callback directory\n        callback_dir = self.env.get('AWX_LIB_DIRECTORY', os.getenv('AWX_LIB_DIRECTORY'))\n        if callback_dir is None:\n            callback_dir = os.path.join(os.path.split(os.path.abspath(__file__))[0],\n                                        \"callbacks\")\n        python_path = self.env.get('PYTHONPATH', os.getenv('PYTHONPATH', ''))\n        if python_path and not python_path.endswith(':'):\n            python_path += ':'\n        self.env['ANSIBLE_CALLBACK_PLUGINS'] = callback_dir\n        if 'AD_HOC_COMMAND_ID' in self.env:\n            self.env['ANSIBLE_STDOUT_CALLBACK'] = 'minimal'\n        else:\n            self.env['ANSIBLE_STDOUT_CALLBACK'] = 'awx_display'\n        self.env['ANSIBLE_RETRY_FILES_ENABLED'] = 'False'\n        self.env['ANSIBLE_HOST_KEY_CHECKING'] = 'False'\n        self.env['AWX_ISOLATED_DATA_DIR'] = self.artifact_dir\n\n        self.env['PYTHONPATH'] = python_path + callback_dir\n        if self.roles_path:\n            self.env['ANSIBLE_ROLES_PATH'] = ':'.join(self.roles_path)\n\n        if self.process_isolation:\n            self.command = self.wrap_args_with_process_isolation(self.command)\n\n        if self.fact_cache_type == 'jsonfile':\n            self.env['ANSIBLE_CACHE_PLUGIN'] = 'jsonfile'\n            self.env['ANSIBLE_CACHE_PLUGIN_CONNECTION'] = self.fact_cache", "response": "Prepares the ansible and ansible - ssh - agent and returns a dictionary of the appropriate class attributes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef prepare_inventory(self):\n        if self.inventory is None:\n            self.inventory  = os.path.join(self.private_data_dir, \"inventory\")", "response": "Prepare the inventory for the current user."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef prepare_env(self):\n        try:\n            passwords = self.loader.load_file('env/passwords', Mapping)\n            self.expect_passwords = {\n                re.compile(pattern, re.M): password\n                for pattern, password in iteritems(passwords)\n            }\n        except ConfigurationError:\n            output.debug('Not loading passwords')\n            self.expect_passwords = dict()\n        self.expect_passwords[pexpect.TIMEOUT] = None\n        self.expect_passwords[pexpect.EOF] = None\n\n        try:\n            # seed env with existing shell env\n            self.env = os.environ.copy()\n            envvars = self.loader.load_file('env/envvars', Mapping)\n            if envvars:\n                self.env.update({k:six.text_type(v) for k, v in envvars.items()})\n            if self.envvars and isinstance(self.envvars, dict):\n                self.env.update({k:six.text_type(v) for k, v in self.envvars.items()})\n        except ConfigurationError:\n            output.debug(\"Not loading environment vars\")\n            # Still need to pass default environment to pexpect\n            self.env = os.environ.copy()\n\n        try:\n            self.settings = self.loader.load_file('env/settings', Mapping)\n        except ConfigurationError:\n            output.debug(\"Not loading settings\")\n            self.settings = dict()\n\n        try:\n            self.ssh_key_data = self.loader.load_file('env/ssh_key', string_types)\n        except ConfigurationError:\n            output.debug(\"Not loading ssh key\")\n            self.ssh_key_data = None\n\n        self.idle_timeout = self.settings.get('idle_timeout', None)\n        self.job_timeout = self.settings.get('job_timeout', None)\n        self.pexpect_timeout = self.settings.get('pexpect_timeout', 5)\n\n        self.process_isolation = self.settings.get('process_isolation', self.process_isolation)\n        self.process_isolation_executable = self.settings.get('process_isolation_executable', self.process_isolation_executable)\n        self.process_isolation_path = self.settings.get('process_isolation_path', self.process_isolation_path)\n        self.process_isolation_hide_paths = self.settings.get('process_isolation_hide_paths', self.process_isolation_hide_paths)\n        self.process_isolation_show_paths = self.settings.get('process_isolation_show_paths', self.process_isolation_show_paths)\n        self.process_isolation_ro_paths = self.settings.get('process_isolation_ro_paths', self.process_isolation_ro_paths)\n\n        self.pexpect_use_poll = self.settings.get('pexpect_use_poll', True)\n        self.suppress_ansible_output = self.settings.get('suppress_ansible_output', self.quiet)\n        self.directory_isolation_cleanup = bool(self.settings.get('directory_isolation_cleanup', True))\n\n        if 'AD_HOC_COMMAND_ID' in self.env or not os.path.exists(self.project_dir):\n            self.cwd = self.private_data_dir\n        else:\n            if self.directory_isolation_path is not None:\n                self.cwd = self.directory_isolation_path\n            else:\n                self.cwd = self.project_dir\n\n        if 'fact_cache' in self.settings:\n            if 'fact_cache_type' in self.settings:\n                if self.settings['fact_cache_type'] == 'jsonfile':\n                    self.fact_cache = os.path.join(self.artifact_dir, self.settings['fact_cache'])\n            else:\n                self.fact_cache = os.path.join(self.artifact_dir, self.settings['fact_cache'])", "response": "Prepares the environment for the current instance of ansible - runner."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef prepare_command(self):\n        try:\n            cmdline_args = self.loader.load_file('args', string_types)\n            self.command = shlex.split(cmdline_args.decode('utf-8'))\n            self.execution_mode = ExecutionMode.RAW\n        except ConfigurationError:\n            self.command = self.generate_ansible_command()", "response": "Prepares the command for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngenerating the command that will be used by the method.", "response": "def generate_ansible_command(self):\n        \"\"\"\n        Given that the ``RunnerConfig`` preparation methods have been run to gather the inputs this method\n        will generate the ``ansible`` or ``ansible-playbook`` command that will be used by the\n        :py:class:`ansible_runner.runner.Runner` object to start the process\n        \"\"\"\n        if self.binary is not None:\n            base_command = self.binary\n            self.execution_mode = ExecutionMode.RAW\n        elif self.module is not None:\n            base_command = 'ansible'\n            self.execution_mode = ExecutionMode.ANSIBLE\n        else:\n            base_command = 'ansible-playbook'\n            self.execution_mode = ExecutionMode.ANSIBLE_PLAYBOOK\n\n        exec_list = [base_command]\n\n        try:\n            cmdline_args = self.loader.load_file('env/cmdline', string_types, encoding=None)\n            args = shlex.split(cmdline_args)\n            exec_list.extend(args)\n        except ConfigurationError:\n            pass\n\n        if isinstance(self.inventory, list):\n            for i in self.inventory:\n                exec_list.append(\"-i\")\n                exec_list.append(i)\n        else:\n            exec_list.append(\"-i\")\n            exec_list.append(self.inventory)\n\n        if self.limit is not None:\n            exec_list.append(\"--limit\")\n            exec_list.append(self.limit)\n\n        if self.loader.isfile('env/extravars'):\n            exec_list.extend(['-e', '@{}'.format(self.loader.abspath('env/extravars'))])\n        if isinstance(self.extra_vars, dict) and self.extra_vars:\n            exec_list.extend(\n                [\n                    '-e',\n                    '%s' % ' '.join(\n                        [\"{}=\\\"{}\\\"\".format(k, self.extra_vars[k]) for k in self.extra_vars]\n                    )\n                ]\n            )\n        if self.verbosity:\n            v = 'v' * self.verbosity\n            exec_list.append('-{}'.format(v))\n\n        if self.tags:\n            exec_list.extend(['--tags', '{}'.format(self.tags)])\n\n        if self.skip_tags:\n            exec_list.extend(['--skip-tags', '{}'.format(self.skip_tags)])\n\n        if self.forks:\n            exec_list.extend(['--forks', '{}'.format(self.forks)])\n\n        # Other parameters\n        if self.execution_mode == ExecutionMode.ANSIBLE_PLAYBOOK:\n            exec_list.append(self.playbook)\n        elif self.execution_mode == ExecutionMode.ANSIBLE:\n            exec_list.append(\"-m\")\n            exec_list.append(self.module)\n\n            if self.module_args is not None:\n                exec_list.append(\"-a\")\n                exec_list.append(self.module_args)\n\n            if self.host_pattern is not None:\n                exec_list.append(self.host_pattern)\n\n        return exec_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncreates a temporary directory for process isolation to use.", "response": "def build_process_isolation_temp_dir(self):\n        '''\n        Create a temporary directory for process isolation to use.\n        '''\n        path = tempfile.mkdtemp(prefix='ansible_runner_pi_', dir=self.process_isolation_path)\n        os.chmod(path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)\n        atexit.register(shutil.rmtree, path)\n        return path"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrap existing command line with bwrap to restrict access to process isolation.", "response": "def wrap_args_with_process_isolation(self, args):\n        '''\n        Wrap existing command line with bwrap to restrict access to:\n         - self.process_isolation_path (generally, /tmp) (except for own /tmp files)\n        '''\n        cwd = os.path.realpath(self.cwd)\n        pi_temp_dir = self.build_process_isolation_temp_dir()\n        new_args = [self.process_isolation_executable or 'bwrap', '--unshare-pid', '--dev-bind', '/', '/', '--proc', '/proc']\n\n        for path in sorted(set(self.process_isolation_hide_paths or [])):\n            if not os.path.exists(path):\n                logger.debug('hide path not found: {0}'.format(path))\n                continue\n            path = os.path.realpath(path)\n            if os.path.isdir(path):\n                new_path = tempfile.mkdtemp(dir=pi_temp_dir)\n                os.chmod(new_path, stat.S_IRUSR | stat.S_IWUSR | stat.S_IXUSR)\n            else:\n                handle, new_path = tempfile.mkstemp(dir=pi_temp_dir)\n                os.close(handle)\n                os.chmod(new_path, stat.S_IRUSR | stat.S_IWUSR)\n            new_args.extend(['--bind', '{0}'.format(new_path), '{0}'.format(path)])\n\n        if self.private_data_dir:\n            show_paths = [self.private_data_dir]\n        else:\n            show_paths = [cwd]\n\n        for path in sorted(set(self.process_isolation_ro_paths or [])):\n            if not os.path.exists(path):\n                logger.debug('read-only path not found: {0}'.format(path))\n                continue\n            path = os.path.realpath(path)\n            new_args.extend(['--ro-bind', '{0}'.format(path),  '{0}'.format(path)])\n\n        show_paths.extend(self.process_isolation_show_paths or [])\n        for path in sorted(set(show_paths)):\n            if not os.path.exists(path):\n                logger.debug('show path not found: {0}'.format(path))\n                continue\n            path = os.path.realpath(path)\n            new_args.extend(['--bind', '{0}'.format(path), '{0}'.format(path)])\n\n        if self.execution_mode == ExecutionMode.ANSIBLE_PLAYBOOK:\n            # playbook runs should cwd to the SCM checkout dir\n            if self.directory_isolation_path is not None:\n                new_args.extend(['--chdir', os.path.realpath(self.directory_isolation_path)])\n            else:\n                new_args.extend(['--chdir', self.project_dir])\n        elif self.execution_mode == ExecutionMode.ANSIBLE:\n            # ad-hoc runs should cwd to the root of the private data dir\n            new_args.extend(['--chdir', os.path.realpath(self.private_data_dir)])\n\n        new_args.extend(args)\n        return new_args"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wrap_args_with_ssh_agent(self, args, ssh_key_path, ssh_auth_sock=None, silence_ssh_add=False):\n        if ssh_key_path:\n            ssh_add_command = args2cmdline('ssh-add', ssh_key_path)\n            if silence_ssh_add:\n                ssh_add_command = ' '.join([ssh_add_command, '2>/dev/null'])\n            cmd = ' && '.join([ssh_add_command,\n                               args2cmdline('rm', '-f', ssh_key_path),\n                               args2cmdline(*args)])\n            args = ['ssh-agent']\n            if ssh_auth_sock:\n                args.extend(['-a', ssh_auth_sock])\n            args.extend(['sh', '-c', cmd])\n        return args", "response": "Given an existing command line and parameterization this will return the same command line wrapped with the ssh - agent command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef configure():\n    '''\n    Configures the logging facility\n\n    This function will setup an initial logging facility for handling display\n    and debug outputs.  The default facility will send display messages to\n    stdout and the default debug facility will do nothing.\n\n    :returns: None\n    '''\n    root_logger = logging.getLogger()\n    root_logger.addHandler(logging.NullHandler())\n    root_logger.setLevel(99)\n\n    _display_logger.setLevel(70)\n    _debug_logger.setLevel(10)\n\n    display_handlers = [h.get_name() for h in _display_logger.handlers]\n\n    if 'stdout' not in display_handlers:\n        stdout_handler = logging.StreamHandler(sys.stdout)\n        stdout_handler.set_name('stdout')\n        formatter = logging.Formatter('%(message)s')\n        stdout_handler.setFormatter(formatter)\n        _display_logger.addHandler(stdout_handler)", "response": "Configures the logging facility\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ninvoke for every Ansible event to collect stdout with the event data and store it for later use", "response": "def event_callback(self, event_data):\n        '''\n        Invoked for every Ansible event to collect stdout with the event data and store it for\n        later use\n        '''\n        self.last_stdout_update = time.time()\n        job_events_path = os.path.join(self.config.artifact_dir, 'job_events')\n        if not os.path.exists(job_events_path):\n            os.mkdir(job_events_path, 0o700)\n        if 'uuid' in event_data:\n            filename = '{}-partial.json'.format(event_data['uuid'])\n            partial_filename = os.path.join(self.config.artifact_dir,\n                                            'job_events',\n                                            filename)\n            full_filename = os.path.join(self.config.artifact_dir,\n                                         'job_events',\n                                         '{}-{}.json'.format(event_data['counter'],\n                                                             event_data['uuid']))\n            try:\n                event_data.update(dict(runner_ident=str(self.config.ident)))\n                try:\n                    with codecs.open(partial_filename, 'r', encoding='utf-8') as read_file:\n                        partial_event_data = json.load(read_file)\n                    event_data.update(partial_event_data)\n                    if self.remove_partials:\n                        os.remove(partial_filename)\n                except IOError:\n                    debug(\"Failed to open ansible stdout callback plugin partial data file {}\".format(partial_filename))\n                if self.event_handler is not None:\n                    should_write = self.event_handler(event_data)\n                else:\n                    should_write = True\n                for plugin in ansible_runner.plugins:\n                    ansible_runner.plugins[plugin].event_handler(self.config, event_data)\n                if should_write:\n                    with codecs.open(full_filename, 'w', encoding='utf-8') as write_file:\n                        os.chmod(full_filename, stat.S_IRUSR | stat.S_IWUSR)\n                        json.dump(event_data, write_file)\n            except IOError as e:\n                debug(\"Failed writing event data: {}\".format(e))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nlaunching the Ansible task configured in self. config and returns once the invocation is complete.", "response": "def run(self):\n        '''\n        Launch the Ansible task configured in self.config (A RunnerConfig object), returns once the\n        invocation is complete\n        '''\n        self.status_callback('starting')\n        stdout_filename = os.path.join(self.config.artifact_dir, 'stdout')\n        command_filename = os.path.join(self.config.artifact_dir, 'command')\n\n        try:\n            os.makedirs(self.config.artifact_dir, mode=0o700)\n        except OSError as exc:\n            if exc.errno == errno.EEXIST and os.path.isdir(self.config.artifact_dir):\n                pass\n            else:\n                raise\n        os.close(os.open(stdout_filename, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))\n\n        command = [a.decode('utf-8') if six.PY2 else a for a in self.config.command]\n        with codecs.open(command_filename, 'w', encoding='utf-8') as f:\n            os.chmod(command_filename, stat.S_IRUSR | stat.S_IWUSR)\n            json.dump(\n                {'command': command,\n                 'cwd': self.config.cwd,\n                 'env': self.config.env}, f, ensure_ascii=False\n            )\n\n        if self.config.ident is not None:\n            cleanup_artifact_dir(os.path.join(self.config.artifact_dir, \"..\"), self.config.rotate_artifacts)\n\n        stdout_handle = codecs.open(stdout_filename, 'w', encoding='utf-8')\n        stdout_handle = OutputEventFilter(stdout_handle, self.event_callback, self.config.suppress_ansible_output, output_json=self.config.json_mode)\n\n        if not isinstance(self.config.expect_passwords, collections.OrderedDict):\n            # We iterate over `expect_passwords.keys()` and\n            # `expect_passwords.values()` separately to map matched inputs to\n            # patterns and choose the proper string to send to the subprocess;\n            # enforce usage of an OrderedDict so that the ordering of elements in\n            # `keys()` matches `values()`.\n            expect_passwords = collections.OrderedDict(self.config.expect_passwords)\n        password_patterns = list(expect_passwords.keys())\n        password_values = list(expect_passwords.values())\n\n        # pexpect needs all env vars to be utf-8 encoded bytes\n        # https://github.com/pexpect/pexpect/issues/512\n\n        # Use a copy so as not to cause problems when serializing the job_env.\n        env = {\n            ensure_str(k): ensure_str(v) if k != 'PATH' and isinstance(v, six.text_type) else v\n            for k, v in self.config.env.items()\n        }\n\n        self.status_callback('running')\n        self.last_stdout_update = time.time()\n        try:\n            child = pexpect.spawn(\n                command[0],\n                command[1:],\n                cwd=self.config.cwd,\n                env=env,\n                ignore_sighup=True,\n                encoding='utf-8',\n                echo=False,\n                use_poll=self.config.pexpect_use_poll,\n            )\n            child.logfile_read = stdout_handle\n        except pexpect.exceptions.ExceptionPexpect as e:\n            child = collections.namedtuple(\n                'MissingProcess', 'exitstatus isalive'\n            )(\n                exitstatus=127,\n                isalive=lambda: False\n            )\n\n            def _decode(x):\n                return x.decode('utf-8') if six.PY2 else x\n\n            # create the events directory (the callback plugin won't run, so it\n            # won't get created)\n            events_directory = os.path.join(self.config.artifact_dir, 'job_events')\n            if not os.path.exists(events_directory):\n                os.mkdir(events_directory, 0o700)\n            stdout_handle.write(_decode(str(e)))\n            stdout_handle.write(_decode('\\n'))\n\n        job_start = time.time()\n        while child.isalive():\n            result_id = child.expect(password_patterns,\n                                     timeout=self.config.pexpect_timeout,\n                                     searchwindowsize=100)\n            password = password_values[result_id]\n            if password is not None:\n                child.sendline(password)\n                self.last_stdout_update = time.time()\n            if self.cancel_callback:\n                try:\n                    self.canceled = self.cancel_callback()\n                except Exception as e:\n                    # TODO: logger.exception('Could not check cancel callback - cancelling immediately')\n                    #if isinstance(extra_update_fields, dict):\n                    #    extra_update_fields['job_explanation'] = \"System error during job execution, check system logs\"\n                    raise CallbackError(\"Exception in Cancel Callback: {}\".format(e))\n            if self.config.job_timeout and not self.canceled and (time.time() - job_start) > self.config.job_timeout:\n                self.timed_out = True\n                # if isinstance(extra_update_fields, dict):\n                #     extra_update_fields['job_explanation'] = \"Job terminated due to timeout\"\n            if self.canceled or self.timed_out or self.errored:\n                Runner.handle_termination(child.pid, is_cancel=self.canceled)\n            if self.config.idle_timeout and (time.time() - self.last_stdout_update) > self.config.idle_timeout:\n                Runner.handle_termination(child.pid, is_cancel=False)\n                self.timed_out = True\n\n        stdout_handle.flush()\n        stdout_handle.close()\n\n        if self.canceled:\n            self.status_callback('canceled')\n        elif child.exitstatus == 0 and not self.timed_out:\n            self.status_callback('successful')\n        elif self.timed_out:\n            self.status_callback('timeout')\n        else:\n            self.status_callback('failed')\n        self.rc = child.exitstatus if not (self.timed_out or self.canceled) else 254\n        for filename, data in [\n            ('status', self.status),\n            ('rc', self.rc),\n        ]:\n            artifact_path = os.path.join(self.config.artifact_dir, filename)\n            if not os.path.exists(artifact_path):\n                os.close(os.open(artifact_path, os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR))\n            with open(artifact_path, 'w') as f:\n                f.write(str(data))\n        if self.config.directory_isolation_path and self.config.directory_isolation_cleanup:\n            shutil.rmtree(self.config.directory_isolation_path)\n        if self.finished_callback is not None:\n            try:\n                self.finished_callback(self)\n            except Exception as e:\n                raise CallbackError(\"Exception in Finished Callback: {}\".format(e))\n        return self.status, self.rc"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef stdout(self):\n        '''\n        Returns an open file handle to the stdout representing the Ansible run\n        '''\n        stdout_path = os.path.join(self.config.artifact_dir, 'stdout')\n        if not os.path.exists(stdout_path):\n            raise AnsibleRunnerException(\"stdout missing\")\n        return open(os.path.join(self.config.artifact_dir, 'stdout'), 'r')", "response": "Returns an open file handle to the stdout representing the Ansible run\n       "}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef stats(self):\n        '''\n        Returns the final high level stats from the Ansible run\n\n        Example:\n            {'dark': {}, 'failures': {}, 'skipped': {}, 'ok': {u'localhost': 2}, 'processed': {u'localhost': 1}}\n        '''\n        last_event = list(filter(lambda x: 'event' in x and x['event'] == 'playbook_on_stats',\n                                 self.events))\n        if not last_event:\n            return None\n        last_event = last_event[0]['event_data']\n        return dict(skipped=last_event['skipped'],\n                    ok=last_event['ok'],\n                    dark=last_event['dark'],\n                    failures=last_event['failures'],\n                    processed=last_event['processed'])", "response": "Returns the final high level stats from the Ansible run\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef host_events(self, host):\n        '''\n        Given a host name, this will return all task events executed on that host\n        '''\n        all_host_events = filter(lambda x: 'event_data' in x and 'host' in x['event_data'] and x['event_data']['host'] == host,\n                                 self.events)\n        return all_host_events", "response": "This will return all task events executed on that host"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef handle_termination(cls, pid, is_cancel=True):\n        '''\n        Internal method to terminate a subprocess spawned by `pexpect` representing an invocation of runner.\n\n        :param pid:       the process id of the running the job.\n        :param is_cancel: flag showing whether this termination is caused by\n                          instance's cancel_flag.\n        '''\n        try:\n            main_proc = psutil.Process(pid=pid)\n            child_procs = main_proc.children(recursive=True)\n            for child_proc in child_procs:\n                try:\n                    os.kill(child_proc.pid, signal.SIGKILL)\n                except (TypeError, OSError):\n                    pass\n            os.kill(main_proc.pid, signal.SIGKILL)\n        except (TypeError, psutil.Error, OSError):\n            try:\n                os.kill(pid, signal.SIGKILL)\n            except (OSError):\n                pass", "response": "Internal method to terminate a subprocess spawned by pexpect representing an invocation of the job."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the entire fact cache only if the fact_cache_type is jsonfile", "response": "def get_fact_cache(self, host):\n        '''\n        Get the entire fact cache only if the fact_cache_type is 'jsonfile'\n        '''\n        if self.config.fact_cache_type != 'jsonfile':\n            raise Exception('Unsupported fact cache type.  Only \"jsonfile\" is supported for reading and writing facts from ansible-runner')\n        fact_cache = os.path.join(self.config.fact_cache, host)\n        if os.path.exists(fact_cache):\n            with open(fact_cache) as f:\n                return json.loads(f.read())\n        return {}"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nsets the entire fact cache data only if the fact_cache_type is jsonfile", "response": "def set_fact_cache(self, host, data):\n        '''\n        Set the entire fact cache data only if the fact_cache_type is 'jsonfile'\n        '''\n        if self.config.fact_cache_type != 'jsonfile':\n            raise Exception('Unsupported fact cache type.  Only \"jsonfile\" is supported for reading and writing facts from ansible-runner')\n        fact_cache = os.path.join(self.config.fact_cache, host)\n        if not os.path.exists(os.path.dirname(fact_cache)):\n            os.makedirs(os.path.dirname(fact_cache), mode=0o700)\n        with open(fact_cache, 'w') as f:\n            return f.write(json.dumps(data))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef init_runner(**kwargs):\n    '''\n    Initialize the Runner() instance\n\n    This function will properly initialize both run() and run_async()\n    functions in the same way and return a value instance of Runner.\n\n    See parameters given to :py:func:`ansible_runner.interface.run`\n    '''\n    dump_artifacts(kwargs)\n\n    debug = kwargs.pop('debug', None)\n    logfile = kwargs.pop('logfile', None)\n\n    if not kwargs.pop(\"ignore_logging\", True):\n        output.configure()\n        if debug in (True, False):\n            output.set_debug('enable' if debug is True else 'disable')\n\n        if logfile:\n            output.set_logfile(logfile)\n\n    if kwargs.get(\"process_isolation\", False):\n        check_isolation_executable_installed(kwargs.get(\"process_isolation_executable\", \"bwrap\"))\n\n    event_callback_handler = kwargs.pop('event_handler', None)\n    status_callback_handler = kwargs.pop('status_handler', None)\n    cancel_callback = kwargs.pop('cancel_callback', None)\n    finished_callback = kwargs.pop('finished_callback',  None)\n\n    rc = RunnerConfig(**kwargs)\n    rc.prepare()\n\n    return Runner(rc,\n                  event_handler=event_callback_handler,\n                  status_handler=status_callback_handler,\n                  cancel_callback=cancel_callback,\n                  finished_callback=finished_callback)", "response": "Initialize the Runner instance."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef run_async(**kwargs):\n    '''\n    Runs an Ansible Runner task in the background which will start immediately. Returns the thread object and a Runner object.\n\n    This uses the same parameters as :py:func:`ansible_runner.interface.run`\n\n    :returns: A tuple containing a :py:class:`threading.Thread` object and a :py:class:`ansible_runner.runner.Runner` object\n    '''\n    r = init_runner(**kwargs)\n    runner_thread = threading.Thread(target=r.run)\n    runner_thread.start()\n    return runner_thread, r", "response": "Runs an Ansible Runner task in the background which will start immediately. Returns the thread object and a Runner object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the object is a playbook object and returns if it is not a list", "response": "def isplaybook(obj):\n    '''\n    Inspects the object and returns if it is a playbook\n\n    Args:\n        obj (object): The object to be inspected by this function\n\n    Returns:\n        boolean: True if the object is a list and False if it is not\n    '''\n    return isinstance(obj, Iterable) and (not isinstance(obj, string_types) and not isinstance(obj, Mapping))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef check_isolation_executable_installed(isolation_executable):\n    '''\n    Check that proot is installed.\n    '''\n    cmd = [isolation_executable, '--version']\n    try:\n        proc = subprocess.Popen(cmd, stdout=subprocess.PIPE,\n                                stderr=subprocess.PIPE)\n        proc.communicate()\n        return bool(proc.returncode == 0)\n    except (OSError, ValueError) as e:\n        if isinstance(e, ValueError) or getattr(e, 'errno', 1) != 2:  # ENOENT, no such file or directory\n            raise RuntimeError('bwrap unavailable for unexpected reason.')\n        return False", "response": "Check that the isolation executable is installed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nwrite the artifact to disk at the specified path.", "response": "def dump_artifact(obj, path, filename=None):\n    '''\n    Write the artifact to disk at the specified path\n\n    Args:\n        obj (string): The string object to be dumped to disk in the specified\n            path.  The artifact filename will be automatically created\n\n        path (string): The full path to the artifacts data directory.\n\n        filename (string, optional): The name of file to write the artifact to.\n            If the filename is not provided, then one will be generated.\n\n    Returns:\n        string: The full path filename for the artifact that was generated\n    '''\n    p_sha1 = None\n\n    if not os.path.exists(path):\n        os.makedirs(path, mode=0o700)\n    else:\n        p_sha1 = hashlib.sha1()\n        p_sha1.update(obj.encode(encoding='UTF-8'))\n\n    if filename is None:\n        fd, fn = tempfile.mkstemp(dir=path)\n    else:\n        fn = os.path.join(path, filename)\n\n    if os.path.exists(fn):\n        c_sha1 = hashlib.sha1()\n        with open(fn) as f:\n            contents = f.read()\n        c_sha1.update(contents.encode(encoding='UTF-8'))\n\n    if not os.path.exists(fn) or p_sha1.hexdigest() != c_sha1.hexdigest():\n        lock_fp = os.path.join(path, '.artifact_write_lock')\n        lock_fd = os.open(lock_fp, os.O_RDWR | os.O_CREAT, stat.S_IRUSR | stat.S_IWUSR)\n        fcntl.lockf(lock_fd, fcntl.LOCK_EX)\n\n        try:\n            with open(fn, 'w') as f:\n                os.chmod(fn, stat.S_IRUSR)\n                f.write(str(obj))\n        finally:\n            fcntl.lockf(lock_fd, fcntl.LOCK_UN)\n            os.close(lock_fd)\n            os.remove(lock_fp)\n\n    return fn"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef dump_artifacts(kwargs):\n    '''\n    Introspect the kwargs and dump objects to disk\n    '''\n    private_data_dir = kwargs.get('private_data_dir')\n    if not private_data_dir:\n        private_data_dir = tempfile.mkdtemp()\n        kwargs['private_data_dir'] = private_data_dir\n\n    if not os.path.exists(private_data_dir):\n        raise ValueError('private_data_dir path is either invalid or does not exist')\n\n    if 'role' in kwargs:\n        role = {'name': kwargs.pop('role')}\n        if 'role_vars' in kwargs:\n            role['vars'] = kwargs.pop('role_vars')\n\n        play = [{'hosts': kwargs.pop('hosts', 'all'), 'roles': [role]}]\n\n        if kwargs.pop('role_skip_facts', False):\n            play[0]['gather_facts'] = False\n\n        kwargs['playbook'] = play\n\n        if 'envvars' not in kwargs:\n            kwargs['envvars'] = {}\n\n        roles_path = kwargs.pop('roles_path', None)\n        if not roles_path:\n            roles_path = os.path.join(private_data_dir, 'roles')\n        else:\n            roles_path += ':{}'.format(os.path.join(private_data_dir, 'roles'))\n\n        kwargs['envvars']['ANSIBLE_ROLES_PATH'] = roles_path\n\n    obj = kwargs.get('playbook')\n    if obj and isplaybook(obj):\n        path = os.path.join(private_data_dir, 'project')\n        kwargs['playbook'] = dump_artifact(json.dumps(obj), path, 'main.json')\n\n    obj = kwargs.get('inventory')\n    if obj and isinventory(obj):\n        path = os.path.join(private_data_dir, 'inventory')\n        if isinstance(obj, Mapping):\n            kwargs['inventory'] = dump_artifact(json.dumps(obj), path, 'hosts.json')\n        elif isinstance(obj, string_types):\n            if not os.path.exists(obj):\n                kwargs['inventory'] = dump_artifact(obj, path, 'hosts')\n\n    for key in ('envvars', 'extravars', 'passwords', 'settings'):\n        obj = kwargs.get(key)\n        if obj and not os.path.exists(os.path.join(private_data_dir, 'env', key)):\n            path = os.path.join(private_data_dir, 'env')\n            dump_artifact(json.dumps(obj), path, key)\n            kwargs.pop(key)\n\n    for key in ('ssh_key', 'cmdline'):\n        obj = kwargs.get(key)\n        if obj and not os.path.exists(os.path.join(private_data_dir, 'env', key)):\n            path = os.path.join(private_data_dir, 'env')\n            dump_artifact(str(kwargs[key]), path, key)\n            kwargs.pop(key)", "response": "Introspect the kwargs and dump objects to disk\n   "}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open_fifo_write(path, data):\n    '''open_fifo_write opens the fifo named pipe in a new thread.\n    This blocks the thread until an external process (such as ssh-agent)\n    reads data from the pipe.\n    '''\n    os.mkfifo(path, stat.S_IRUSR | stat.S_IWUSR)\n    threading.Thread(target=lambda p, d: open(p, 'wb').write(d),\n                     args=(path, data)).start()", "response": "Open a new fifo named pipe and write data to it."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef encode_pdf_date(d: datetime) -> str:\n\n    # The formatting of %Y is not consistent as described in\n    # https://bugs.python.org/issue13305 and underspecification in libc.\n    # So explicitly format the year with leading zeros\n    s = \"{:04d}\".format(d.year)\n    s += d.strftime(r'%m%d%H%M%S')\n    tz = d.strftime('%z')\n    if tz:\n        sign, tz_hours, tz_mins = tz[0], tz[1:3], tz[3:5]\n        s += \"{}{}'{}'\".format(sign, tz_hours, tz_mins)\n    return s", "response": "Encode Python datetime object as PDF date string."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndecodes a pdfmark date to a Python datetime object.", "response": "def decode_pdf_date(s: str) -> datetime:\n    \"\"\"Decode a pdfmark date to a Python datetime object\n\n    A pdfmark date is a string in a paritcular format. See the pdfmark\n    Reference for the specification.\n    \"\"\"\n    if isinstance(s, String):\n        s = str(s)\n    if s.startswith('D:'):\n        s = s[2:]\n\n    # Literal Z00'00', is incorrect but found in the wild,\n    # probably made by OS X Quartz -- standardize\n    if s.endswith(\"Z00'00'\"):\n        s = s.replace(\"Z00'00'\", '+0000')\n    elif s.endswith('Z'):\n        s = s.replace('Z', '+0000')\n    s = s.replace(\"'\", \"\")  # Remove apos from PDF time strings\n    try:\n        return datetime.strptime(s, r'%Y%m%d%H%M%S%z')\n    except ValueError:\n        return datetime.strptime(s, r'%Y%m%d%H%M%S')"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef load_from_docinfo(self, docinfo, delete_missing=False, raise_failure=False):\n        for uri, shortkey, docinfo_name, converter in self.DOCINFO_MAPPING:\n            qname = QName(uri, shortkey)\n            # docinfo might be a dict or pikepdf.Dictionary, so lookup keys\n            # by str(Name)\n            val = docinfo.get(str(docinfo_name))\n            if val is None:\n                if delete_missing and qname in self:\n                    del self[qname]\n                continue\n            try:\n                val = str(val)\n                if converter:\n                    val = converter.xmp_from_docinfo(val)\n                if not val:\n                    continue\n                self[qname] = val\n            except (ValueError, AttributeError) as e:\n                msg = \"The metadata field {} could not be copied to XMP\".format(\n                    docinfo_name\n                )\n                if raise_failure:\n                    raise ValueError(msg) from e\n                else:\n                    warn(msg)", "response": "Populate the XMP metadata object with the contents of a DocumentInfo object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates the PDF s DocumentInfo dictionary to match XMP metadata.", "response": "def _update_docinfo(self):\n        \"\"\"Update the PDF's DocumentInfo dictionary to match XMP metadata\n\n        The standard mapping is described here:\n            https://www.pdfa.org/pdfa-metadata-xmp-rdf-dublin-core/\n        \"\"\"\n        self._pdf.docinfo  # Touch object to ensure it exists\n        for uri, element, docinfo_name, converter in self.DOCINFO_MAPPING:\n            qname = QName(uri, element)\n            try:\n                value = self[qname]\n            except KeyError:\n                if docinfo_name in self._pdf.docinfo:\n                    del self._pdf.docinfo[docinfo_name]\n                continue\n            if converter:\n                try:\n                    value = converter.docinfo_from_xmp(value)\n                except ValueError:\n                    warn(\n                        \"The DocumentInfo field {} could not be updated from XMP\".format(\n                            docinfo_name\n                        )\n                    )\n                    value = None\n            if value is None:\n                if docinfo_name in self._pdf.docinfo:\n                    del self._pdf.docinfo[docinfo_name]\n                continue\n            value = re_xml_illegal_chars.sub('', value)\n            try:\n                # Try to save pure ASCII\n                self._pdf.docinfo[docinfo_name] = value.encode('ascii')\n            except UnicodeEncodeError:\n                # qpdf will serialize this as a UTF-16 with BOM string\n                self._pdf.docinfo[docinfo_name] = value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nserialize our changes back to the PDF in memory", "response": "def _apply_changes(self):\n        \"\"\"Serialize our changes back to the PDF in memory\n\n        Depending how we are initialized, leave our metadata mark and producer.\n        \"\"\"\n        if self.mark:\n            self[QName(XMP_NS_XMP, 'MetadataDate')] = datetime.now().isoformat()\n            self[QName(XMP_NS_PDF, 'Producer')] = 'pikepdf ' + pikepdf_version\n        xml = self._get_xml_bytes()\n        self._pdf.Root.Metadata = Stream(self._pdf, xml)\n        self._pdf.Root.Metadata[Name.Type] = Name.Metadata\n        self._pdf.Root.Metadata[Name.Subtype] = Name.XML\n        if self.sync_docinfo:\n            self._update_docinfo()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nconverts name to an XML QName e. g. pdf : Producer -> 1. 3", "response": "def _qname(self, name):\n        \"\"\"Convert name to an XML QName\n\n        e.g. pdf:Producer -> {http://ns.adobe.com/pdf/1.3/}Producer\n        \"\"\"\n        if isinstance(name, QName):\n            return name\n        if not isinstance(name, str):\n            raise TypeError(\"{} must be str\".format(name))\n        if name == '':\n            return name\n        if name.startswith('{'):\n            return name\n        prefix, tag = name.split(':', maxsplit=1)\n        uri = self.NS[prefix]\n        return QName(uri, tag)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a fully qualified XML name find a prefix that is appropriate for the given URI.", "response": "def _prefix_from_uri(self, uriname):\n        \"\"\"Given a fully qualified XML name, find a prefix\n\n        e.g. {http://ns.adobe.com/pdf/1.3/}Producer -> pdf:Producer\n        \"\"\"\n        uripart, tag = uriname.split('}', maxsplit=1)\n        uri = uripart.replace('{', '')\n        return self.REVERSE_NS[uri] + ':' + tag"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngather the sub - elements attached to a node into set and list respectively.", "response": "def _get_subelements(self, node):\n        \"\"\"Gather the sub-elements attached to a node\n\n        Gather rdf:Bag and and rdf:Seq into set and list respectively. For\n        alternate languages values, take the first language only for\n        simplicity.\n        \"\"\"\n        items = node.find('rdf:Alt', self.NS)\n        if items is not None:\n            try:\n                return items[0].text\n            except IndexError:\n                return ''\n\n        for xmlcontainer, container, insertfn in XMP_CONTAINERS:\n            items = node.find('rdf:{}'.format(xmlcontainer), self.NS)\n            if items is None:\n                continue\n            result = container()\n            for item in items:\n                insertfn(result, item.text)\n            return result\n        return ''"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_elements(self, name=''):\n        qname = self._qname(name)\n        rdf = self._get_rdf_root()\n        for rdfdesc in rdf.findall('rdf:Description[@rdf:about=\"\"]', self.NS):\n            if qname and qname in rdfdesc.keys():\n                yield (rdfdesc, qname, rdfdesc.get(qname), rdf)\n            elif not qname:\n                for k, v in rdfdesc.items():\n                    if v:\n                        yield (rdfdesc, k, v, rdf)\n            xpath = qname if name else '*'\n            for node in rdfdesc.findall(xpath, self.NS):\n                if node.text and node.text.strip():\n                    yield (node, None, node.text, rdfdesc)\n                    continue\n                values = self._get_subelements(node)\n                yield (node, None, values, rdfdesc)", "response": "Get elements from XMP and yield them."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pdfa_status(self):\n        key_part = QName(XMP_NS_PDFA_ID, 'part')\n        key_conformance = QName(XMP_NS_PDFA_ID, 'conformance')\n        try:\n            return self[key_part] + self[key_conformance]\n        except KeyError:\n            return ''", "response": "Returns the status of the PDF / A conformance level claimed by this PDF or False if the PDF is truly PDF or truly PDF."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef mode(self):\n        m = ''\n        if self.indexed:\n            m = 'P'\n        elif self.bits_per_component == 1:\n            m = '1'\n        elif self.bits_per_component == 8:\n            if self.colorspace == '/DeviceRGB':\n                m = 'RGB'\n            elif self.colorspace == '/DeviceGray':\n                m = 'L'\n            elif self.colorspace == '/DeviceCMYK':\n                m = 'CMYK'\n        if m == '':\n            raise NotImplementedError(\"Not sure how to handle PDF image of this type\")\n        return m", "response": "Returns the mode of the image in the PIL file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef palette(self):\n\n        if not self.indexed:\n            return None\n        _idx, base, hival, lookup = None, None, None, None\n        try:\n            _idx, base, hival, lookup = self._colorspaces\n        except ValueError as e:\n            raise ValueError('Not sure how to interpret this palette') from e\n        base = str(base)\n        hival = int(hival)\n        lookup = bytes(lookup)\n        if not base in self.SIMPLE_COLORSPACES:\n            raise NotImplementedError(\"not sure how to interpret this palette\")\n        if base == '/DeviceRGB':\n            base = 'RGB'\n        elif base == '/DeviceGray':\n            base = 'L'\n        return base, lookup", "response": "Retrieves the color palette for this image\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _from_pil_image(cls, *, pdf, page, name, image):  # pragma: no cover\n\n        data = image.tobytes()\n\n        imstream = Stream(pdf, data)\n        imstream.Type = Name('/XObject')\n        imstream.Subtype = Name('/Image')\n        if image.mode == 'RGB':\n            imstream.ColorSpace = Name('/DeviceRGB')\n        elif image.mode in ('1', 'L'):\n            imstream.ColorSpace = Name('/DeviceGray')\n        imstream.BitsPerComponent = 1 if image.mode == '1' else 8\n        imstream.Width = image.width\n        imstream.Height = image.height\n\n        page.Resources.XObject[name] = imstream\n\n        return cls(imstream)", "response": "Create a new image object from a PIL image."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nattempt to extract the image directly to a usable image file.", "response": "def _extract_direct(self, *, stream):\n        \"\"\"Attempt to extract the image directly to a usable image file\n\n        If there is no way to extract the image without decompressing or\n        transcoding then raise an exception. The type and format of image\n        generated will vary.\n\n        Args:\n            stream: Writable stream to write data to\n        \"\"\"\n\n        def normal_dct_rgb():\n            # Normal DCTDecode RGB images have the default value of\n            # /ColorTransform 1 and are actually in YUV. Such a file can be\n            # saved as a standard JPEG. RGB JPEGs without YUV conversion can't\n            # be saved as JPEGs, and are probably bugs. Some software in the\n            # wild actually produces RGB JPEGs in PDFs (probably a bug).\n            DEFAULT_CT_RGB = 1\n            ct = self.filter_decodeparms[0][1].get('/ColorTransform', DEFAULT_CT_RGB)\n            return self.mode == 'RGB' and ct == DEFAULT_CT_RGB\n\n        def normal_dct_cmyk():\n            # Normal DCTDecode CMYKs have /ColorTransform 0 and can be saved.\n            # There is a YUVK colorspace but CMYK JPEGs don't generally use it\n            DEFAULT_CT_CMYK = 0\n            ct = self.filter_decodeparms[0][1].get('/ColorTransform', DEFAULT_CT_CMYK)\n            return self.mode == 'CMYK' and ct == DEFAULT_CT_CMYK\n\n        if self.filters == ['/CCITTFaxDecode']:\n            data = self.obj.read_raw_bytes()\n            stream.write(self._generate_ccitt_header(data))\n            stream.write(data)\n            return '.tif'\n        elif self.filters == ['/DCTDecode'] and (\n            self.mode == 'L' or normal_dct_rgb() or normal_dct_cmyk()\n        ):\n            buffer = self.obj.get_raw_stream_buffer()\n            stream.write(buffer)\n            return '.jpg'\n\n        raise UnsupportedImageTypeError()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _extract_to_stream(self, *, stream):\n\n        try:\n            return self._extract_direct(stream=stream)\n        except UnsupportedImageTypeError:\n            pass\n\n        im = self._extract_transcoded()\n        if im:\n            im.save(stream, format='png')\n            return '.png'\n\n        raise UnsupportedImageTypeError(repr(self))", "response": "Attempt to extract the image directly to a usable image file and save it to the stream."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef extract_to(self, *, stream=None, fileprefix=''):\n\n        if bool(stream) == bool(fileprefix):\n            raise ValueError(\"Cannot set both stream and fileprefix\")\n        if stream:\n            return self._extract_to_stream(stream=stream)\n\n        bio = BytesIO()\n        extension = self._extract_to_stream(stream=bio)\n        bio.seek(0)\n        filepath = Path(Path(fileprefix).name + extension)\n        with filepath.open('wb') as target:\n            copyfileobj(bio, target)\n        return str(filepath)", "response": "Attempt to extract the image directly to a usable image file file\n            is returned."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef as_pil_image(self):\n        from PIL import Image\n\n        try:\n            bio = BytesIO()\n            self._extract_direct(stream=bio)\n            bio.seek(0)\n            return Image.open(bio)\n        except UnsupportedImageTypeError:\n            pass\n\n        im = self._extract_transcoded()\n        if not im:\n            raise UnsupportedImageTypeError(repr(self))\n\n        return im", "response": "Extract the image as a Pillow Image using decompression as necessary\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ngenerate a CCITT G3 or G4 header from the PDF metadata.", "response": "def _generate_ccitt_header(self, data):\n        \"\"\"Construct a CCITT G3 or G4 header from the PDF metadata\"\"\"\n        # https://stackoverflow.com/questions/2641770/\n        # https://www.itu.int/itudoc/itu-t/com16/tiff-fx/docs/tiff6.pdf\n\n        if not self.decode_parms:\n            raise ValueError(\"/CCITTFaxDecode without /DecodeParms\")\n\n        if self.decode_parms[0].get(\"/K\", 1) < 0:\n            ccitt_group = 4  # Pure two-dimensional encoding (Group 4)\n        else:\n            ccitt_group = 3\n        black_is_one = self.decode_parms[0].get(\"/BlackIs1\", False)\n        white_is_zero = 1 if black_is_one else 0\n\n        img_size = len(data)\n        tiff_header_struct = '<' + '2s' + 'H' + 'L' + 'H' + 'HHLL' * 8 + 'L'\n        # fmt: off\n        tiff_header = struct.pack(\n            tiff_header_struct,\n            b'II',  # Byte order indication: Little endian\n            42,  # Version number (always 42)\n            8,  # Offset to first IFD\n            8,  # Number of tags in IFD\n            256, 4, 1, self.width,  # ImageWidth, LONG, 1, width\n            257, 4, 1, self.height,  # ImageLength, LONG, 1, length\n            258, 3, 1, 1,  # BitsPerSample, SHORT, 1, 1\n            259, 3, 1, ccitt_group,  # Compression, SHORT, 1, 4 = CCITT Group 4 fax encoding\n            262, 3, 1, int(white_is_zero),  # Thresholding, SHORT, 1, 0 = WhiteIsZero\n            273, 4, 1, struct.calcsize(tiff_header_struct),  # StripOffsets, LONG, 1, length of header\n            278, 4, 1, self.height,\n            279, 4, 1, img_size,  # StripByteCounts, LONG, 1, size of image\n            0  # last IFD\n        )\n        # fmt: on\n        return tiff_header"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndisplay hook for IPython / Jupyter", "response": "def _repr_png_(self):\n        \"\"\"Display hook for IPython/Jupyter\"\"\"\n        b = BytesIO()\n        im = self.as_pil_image()\n        im.save(b, 'PNG')\n        return b.getvalue()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef augments(cls_cpp):\n\n    def class_augment(cls, cls_cpp=cls_cpp):\n        for name, fn in inspect.getmembers(cls, inspect.isfunction):\n            fn.__qualname__ = fn.__qualname__.replace(cls.__name__, cls_cpp.__name__)\n            setattr(cls_cpp, name, fn)\n        for name, fn in inspect.getmembers(cls, inspect.isdatadescriptor):\n            setattr(cls_cpp, name, fn)\n\n        def block_init(self):\n            # Prevent initialization of the support class\n            raise NotImplementedError(self.__class__.__name__ + '.__init__')\n\n        cls.__init__ = block_init\n        return cls\n\n    return class_augment", "response": "Augment a Python class with methods defined in the support class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nconstructing a single page PDF from the provided page in memory", "response": "def _single_page_pdf(page):\n    \"\"\"Construct a single page PDF from the provided page in memory\"\"\"\n    pdf = Pdf.new()\n    pdf.pages.append(page)\n    bio = BytesIO()\n    pdf.save(bio)\n    bio.seek(0)\n    return bio.read()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nuses mupdf draw to rasterize the PDF in the memory buffer", "response": "def _mudraw(buffer, fmt):\n    \"\"\"Use mupdf draw to rasterize the PDF in the memory buffer\"\"\"\n    with NamedTemporaryFile(suffix='.pdf') as tmp_in:\n        tmp_in.write(buffer)\n        tmp_in.seek(0)\n        tmp_in.flush()\n\n        proc = run(\n            ['mudraw', '-F', fmt, '-o', '-', tmp_in.name], stdout=PIPE, stderr=PIPE\n        )\n        if proc.stderr:\n            raise RuntimeError(proc.stderr.decode())\n        return proc.stdout"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rotated(self, angle_degrees_ccw):\n        angle = angle_degrees_ccw / 180.0 * pi\n        c, s = cos(angle), sin(angle)\n        return self @ PdfMatrix((c, s, -s, c, 0, 0))", "response": "Concatenates a rotation matrix on this matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef shorthand(self):\n        return (self.a, self.b, self.c, self.d, self.e, self.f)", "response": "Return the 6 - tuple that describes this matrix"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nencodes this matrix in binary suitable for including in a PDF", "response": "def encode(self):\n        \"\"\"Encode this matrix in binary suitable for including in a PDF\"\"\"\n        return '{:.6f} {:.6f} {:.6f} {:.6f} {:.6f} {:.6f}'.format(\n            self.a, self.b, self.c, self.d, self.e, self.f\n        ).encode()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_content_stream(page_or_stream, operators=''):\n\n    if not isinstance(page_or_stream, Object):\n        raise TypeError(\"stream must a PDF object\")\n\n    if (\n        page_or_stream._type_code != ObjectType.stream\n        and page_or_stream.get('/Type') != '/Page'\n    ):\n        raise TypeError(\"parse_content_stream called on page or stream object\")\n\n    try:\n        if page_or_stream.get('/Type') == '/Page':\n            page = page_or_stream\n            instructions = page._parse_page_contents_grouped(operators)\n        else:\n            stream = page_or_stream\n            instructions = Object._parse_stream_grouped(stream, operators)\n    except PdfError as e:\n        # This is the error message for qpdf >= 7.0. It was different in 6.x\n        # but we no longer support 6.x\n        if 'ignoring non-stream while parsing' in str(e):\n            raise TypeError(\"parse_content_stream called on non-stream Object\")\n        raise e from e\n\n    return instructions", "response": "Parses a PDF content stream into a sequence of instructions that describe where to render the text and graphics in a PDF."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nchecking if this page print text.", "response": "def has_text(self):\n        \"\"\"Check if this page print text\n\n        Search the content stream for any of the four text showing operators.\n        We ignore text positioning operators because some editors might\n        generate maintain these even if text is deleted etc.\n\n        This cannot detect raster text (text in a bitmap), text rendered as\n        curves. It also cannot determine if the text is visible to the user.\n\n        :return: True if there is text\n        \"\"\"\n        text_showing_operators = \"\"\"TJ \" ' Tj\"\"\"\n        text_showing_insts = parse_content_stream(self.obj, text_showing_operators)\n        if len(text_showing_insts) > 0:\n            return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngroups Tokens that have beginning and end.", "response": "def _group_matching(tlist, cls):\n    \"\"\"Groups Tokens that have beginning and end.\"\"\"\n    opens = []\n    tidx_offset = 0\n    for idx, token in enumerate(list(tlist)):\n        tidx = idx - tidx_offset\n\n        if token.is_whitespace:\n            # ~50% of tokens will be whitespace. Will checking early\n            # for them avoid 3 comparisons, but then add 1 more comparison\n            # for the other ~50% of tokens...\n            continue\n\n        if token.is_group and not isinstance(token, cls):\n            # Check inside previously grouped (i.e. parenthesis) if group\n            # of different type is inside (i.e., case). though ideally  should\n            # should check for all open/close tokens at once to avoid recursion\n            _group_matching(token, cls)\n            continue\n\n        if token.match(*cls.M_OPEN):\n            opens.append(tidx)\n\n        elif token.match(*cls.M_CLOSE):\n            try:\n                open_idx = opens.pop()\n            except IndexError:\n                # this indicates invalid sql and unbalanced tokens.\n                # instead of break, continue in case other \"valid\" groups exist\n                continue\n            close_idx = tidx\n            tlist.group_tokens(cls, open_idx, close_idx)\n            tidx_offset += close_idx - open_idx"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef group_order(tlist):\n    tidx, token = tlist.token_next_by(t=T.Keyword.Order)\n    while token:\n        pidx, prev_ = tlist.token_prev(tidx)\n        if imt(prev_, i=sql.Identifier, t=T.Number):\n            tlist.group_tokens(sql.Identifier, pidx, tidx)\n            tidx = pidx\n        tidx, token = tlist.token_next_by(t=T.Keyword.Order, idx=tidx)", "response": "Group together Identifier and Asc / Desc token"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngrouping together tokens that are joined by a middle token. i. e. x < y.", "response": "def _group(tlist, cls, match,\n           valid_prev=lambda t: True,\n           valid_next=lambda t: True,\n           post=None,\n           extend=True,\n           recurse=True\n           ):\n    \"\"\"Groups together tokens that are joined by a middle token. i.e. x < y\"\"\"\n\n    tidx_offset = 0\n    pidx, prev_ = None, None\n    for idx, token in enumerate(list(tlist)):\n        tidx = idx - tidx_offset\n\n        if token.is_whitespace:\n            continue\n\n        if recurse and token.is_group and not isinstance(token, cls):\n            _group(token, cls, match, valid_prev, valid_next, post, extend)\n\n        if match(token):\n            nidx, next_ = tlist.token_next(tidx)\n            if prev_ and valid_prev(prev_) and valid_next(next_):\n                from_idx, to_idx = post(tlist, pidx, tidx, nidx)\n                grp = tlist.group_tokens(cls, from_idx, to_idx, extend=extend)\n\n                tidx_offset += to_idx - from_idx\n                pidx, prev_ = from_idx, grp\n                continue\n\n        pidx, prev_ = tidx, token"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nresetting the filter attributes to its default values", "response": "def _reset(self):\n        \"\"\"Set the filter attributes to its default values\"\"\"\n        self._in_declare = False\n        self._is_create = False\n        self._begin_depth = 0\n\n        self.consume_ws = False\n        self.tokens = []\n        self.level = 0"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchanging the split level of a log entry.", "response": "def _change_splitlevel(self, ttype, value):\n        \"\"\"Get the new split level (increase, decrease or remain equal)\"\"\"\n\n        # parenthesis increase/decrease a level\n        if ttype is T.Punctuation and value == '(':\n            return 1\n        elif ttype is T.Punctuation and value == ')':\n            return -1\n        elif ttype not in T.Keyword: # if normal token return\n            return 0\n\n        # Everything after here is ttype = T.Keyword\n        # Also to note, once entered an If statement you are done and basically\n        # returning\n        unified = value.upper()\n\n        # three keywords begin with CREATE, but only one of them is DDL\n        # DDL Create though can contain more words such as \"or replace\"\n        if ttype is T.Keyword.DDL and unified.startswith('CREATE'):\n            self._is_create = True\n            return 0\n\n        # can have nested declare inside of being...\n        if unified == 'DECLARE' and self._is_create and self._begin_depth == 0:\n            self._in_declare = True\n            return 1\n\n        if unified == 'BEGIN':\n            self._begin_depth += 1\n            if self._is_create:\n                # FIXME(andi): This makes no sense.\n                return 1\n            return 0\n\n        # Should this respect a preceding BEGIN?\n        # In CASE ... WHEN ... END this results in a split level -1.\n        # Would having multiple CASE WHEN END and a Assignment Operator\n        # cause the statement to cut off prematurely?\n        if unified == 'END':\n            self._begin_depth = max(0, self._begin_depth - 1)\n            return -1\n\n        if (unified in ('IF', 'FOR', 'WHILE')\n                and self._is_create and self._begin_depth > 0):\n            return 1\n\n        if unified in ('END IF', 'END FOR', 'END WHILE'):\n            return -1\n\n        # Default\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef process(self, stream):\n        EOS_TTYPE = T.Whitespace, T.Comment.Single\n\n        # Run over all stream tokens\n        for ttype, value in stream:\n            # Yield token if we finished a statement and there's no whitespaces\n            # It will count newline token as a non whitespace. In this context\n            # whitespace ignores newlines.\n            # why don't multi line comments also count?\n            if self.consume_ws and ttype not in EOS_TTYPE:\n                yield sql.Statement(self.tokens)\n\n                # Reset filter and prepare to process next statement\n                self._reset()\n\n            # Change current split level (increase, decrease or remain equal)\n            self.level += self._change_splitlevel(ttype, value)\n\n            # Append the token to the current statement\n            self.tokens.append(sql.Token(ttype, value))\n\n            # Check if we get the end of a statement\n            if self.level <= 0 and ttype is T.Punctuation and value == ';':\n                self.consume_ws = True\n\n        # Yield pending statement (if any)\n        if self.tokens:\n            yield sql.Statement(self.tokens)", "response": "Yields the tokens from the given stream."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nvalidating the options and returns a new object.", "response": "def validate_options(options):\n    \"\"\"Validates options.\"\"\"\n    kwcase = options.get('keyword_case')\n    if kwcase not in [None, 'upper', 'lower', 'capitalize']:\n        raise SQLParseError('Invalid value for keyword_case: '\n                            '{0!r}'.format(kwcase))\n\n    idcase = options.get('identifier_case')\n    if idcase not in [None, 'upper', 'lower', 'capitalize']:\n        raise SQLParseError('Invalid value for identifier_case: '\n                            '{0!r}'.format(idcase))\n\n    ofrmt = options.get('output_format')\n    if ofrmt not in [None, 'sql', 'python', 'php']:\n        raise SQLParseError('Unknown output format: '\n                            '{0!r}'.format(ofrmt))\n\n    strip_comments = options.get('strip_comments', False)\n    if strip_comments not in [True, False]:\n        raise SQLParseError('Invalid value for strip_comments: '\n                            '{0!r}'.format(strip_comments))\n\n    space_around_operators = options.get('use_space_around_operators', False)\n    if space_around_operators not in [True, False]:\n        raise SQLParseError('Invalid value for use_space_around_operators: '\n                            '{0!r}'.format(space_around_operators))\n\n    strip_ws = options.get('strip_whitespace', False)\n    if strip_ws not in [True, False]:\n        raise SQLParseError('Invalid value for strip_whitespace: '\n                            '{0!r}'.format(strip_ws))\n\n    truncate_strings = options.get('truncate_strings')\n    if truncate_strings is not None:\n        try:\n            truncate_strings = int(truncate_strings)\n        except (ValueError, TypeError):\n            raise SQLParseError('Invalid value for truncate_strings: '\n                                '{0!r}'.format(truncate_strings))\n        if truncate_strings <= 1:\n            raise SQLParseError('Invalid value for truncate_strings: '\n                                '{0!r}'.format(truncate_strings))\n        options['truncate_strings'] = truncate_strings\n        options['truncate_char'] = options.get('truncate_char', '[...]')\n\n    indent_columns = options.get('indent_columns', False)\n    if indent_columns not in [True, False]:\n        raise SQLParseError('Invalid value for indent_columns: '\n                            '{0!r}'.format(indent_columns))\n    elif indent_columns:\n        options['reindent'] = True  # enforce reindent\n    options['indent_columns'] = indent_columns\n\n    reindent = options.get('reindent', False)\n    if reindent not in [True, False]:\n        raise SQLParseError('Invalid value for reindent: '\n                            '{0!r}'.format(reindent))\n    elif reindent:\n        options['strip_whitespace'] = True\n\n    reindent_aligned = options.get('reindent_aligned', False)\n    if reindent_aligned not in [True, False]:\n        raise SQLParseError('Invalid value for reindent_aligned: '\n                            '{0!r}'.format(reindent))\n    elif reindent_aligned:\n        options['strip_whitespace'] = True\n\n    indent_after_first = options.get('indent_after_first', False)\n    if indent_after_first not in [True, False]:\n        raise SQLParseError('Invalid value for indent_after_first: '\n                            '{0!r}'.format(indent_after_first))\n    options['indent_after_first'] = indent_after_first\n\n    indent_tabs = options.get('indent_tabs', False)\n    if indent_tabs not in [True, False]:\n        raise SQLParseError('Invalid value for indent_tabs: '\n                            '{0!r}'.format(indent_tabs))\n    elif indent_tabs:\n        options['indent_char'] = '\\t'\n    else:\n        options['indent_char'] = ' '\n\n    indent_width = options.get('indent_width', 2)\n    try:\n        indent_width = int(indent_width)\n    except (TypeError, ValueError):\n        raise SQLParseError('indent_width requires an integer')\n    if indent_width < 1:\n        raise SQLParseError('indent_width requires a positive integer')\n    options['indent_width'] = indent_width\n\n    wrap_after = options.get('wrap_after', 0)\n    try:\n        wrap_after = int(wrap_after)\n    except (TypeError, ValueError):\n        raise SQLParseError('wrap_after requires an integer')\n    if wrap_after < 0:\n        raise SQLParseError('wrap_after requires a positive integer')\n    options['wrap_after'] = wrap_after\n\n    comma_first = options.get('comma_first', False)\n    if comma_first not in [True, False]:\n        raise SQLParseError('comma_first requires a boolean value')\n    options['comma_first'] = comma_first\n\n    right_margin = options.get('right_margin')\n    if right_margin is not None:\n        try:\n            right_margin = int(right_margin)\n        except (TypeError, ValueError):\n            raise SQLParseError('right_margin requires an integer')\n        if right_margin < 10:\n            raise SQLParseError('right_margin requires an integer > 10')\n    options['right_margin'] = right_margin\n\n    return options"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsplit a string on all unquoted newlines.", "response": "def split_unquoted_newlines(stmt):\n    \"\"\"Split a string on all unquoted newlines.\n\n    Unlike str.splitlines(), this will ignore CR/LF/CR+LF if the requisite\n    character is inside of a string.\"\"\"\n    text = text_type(stmt)\n    lines = SPLIT_REGEX.split(text)\n    outputlines = ['']\n    for line in lines:\n        if not line:\n            continue\n        elif LINE_MATCH.match(line):\n            outputlines.append('')\n        else:\n            outputlines[-1] += line\n    return outputlines"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef recurse(*cls):\n    def wrap(f):\n        def wrapped_f(tlist):\n            for sgroup in tlist.get_sublists():\n                if not isinstance(sgroup, cls):\n                    wrapped_f(sgroup)\n            f(tlist)\n\n        return wrapped_f\n\n    return wrap", "response": "Decorator to help with recursion over a sequence of objects."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef imt(token, i=None, m=None, t=None):\n    clss = i\n    types = [t, ] if t and not isinstance(t, list) else t\n    mpatterns = [m, ] if m and not isinstance(m, list) else m\n\n    if token is None:\n        return False\n    elif clss and isinstance(token, clss):\n        return True\n    elif mpatterns and any(token.match(*pattern) for pattern in mpatterns):\n        return True\n    elif types and any(token.ttype in ttype for ttype in types):\n        return True\n    else:\n        return False", "response": "Helper function to simplify comparisons Instance Match and TokenType\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses sql statements from a file - like object.", "response": "def parsestream(stream, encoding=None):\n    \"\"\"Parses sql statements from file-like object.\n\n    :param stream: A file-like object.\n    :param encoding: The encoding of the stream contents (optional).\n    :returns: A generator of :class:`~sqlparse.sql.Statement` instances.\n    \"\"\"\n    stack = engine.FilterStack()\n    stack.enable_grouping()\n    return stack.run(stream, encoding)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nformat the SQL statement according to the given options.", "response": "def format(sql, encoding=None, **options):\n    \"\"\"Format *sql* according to *options*.\n\n    Available options are documented in :ref:`formatting`.\n\n    In addition to the formatting options this function accepts the\n    keyword \"encoding\" which determines the encoding of the statement.\n\n    :returns: The formatted SQL statement as string.\n    \"\"\"\n    stack = engine.FilterStack()\n    options = formatter.validate_options(options)\n    stack = formatter.build_filter_stack(stack, options)\n    stack.postprocess.append(filters.SerializerUnicode())\n    return u''.join(stack.run(sql, encoding))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsplitting a SQL string into a list of strings.", "response": "def split(sql, encoding=None):\n    \"\"\"Split *sql* into single statements.\n\n    :param sql: A string containing one or more SQL statements.\n    :param encoding: The encoding of the statement (optional).\n    :returns: A list of strings.\n    \"\"\"\n    stack = engine.FilterStack()\n    return [text_type(stmt).strip() for stmt in stack.run(sql, encoding)]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_tokens(text, encoding=None):\n        if isinstance(text, file_types):\n            text = text.read()\n\n        if isinstance(text, text_type):\n            pass\n        elif isinstance(text, bytes):\n            if encoding:\n                text = text.decode(encoding)\n            else:\n                try:\n                    text = text.decode('utf-8')\n                except UnicodeDecodeError:\n                    text = text.decode('unicode-escape')\n        else:\n            raise TypeError(u\"Expected text or file-like object, got {!r}\".\n                            format(type(text)))\n\n        iterable = enumerate(text)\n        for pos, char in iterable:\n            for rexmatch, action in SQL_REGEX:\n                m = rexmatch(text, pos)\n\n                if not m:\n                    continue\n                elif isinstance(action, tokens._TokenType):\n                    yield action, m.group()\n                elif callable(action):\n                    yield action(m.group())\n\n                consume(iterable, m.end() - pos - 1)\n                break\n            else:\n                yield tokens.Error, char", "response": "Yields tokens from text."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nyields all tokens up to token but excluding current.", "response": "def _flatten_up_to_token(self, token):\n        \"\"\"Yields all tokens up to token but excluding current.\"\"\"\n        if token.is_group:\n            token = next(token.flatten())\n\n        for t in self._curr_stmt.flatten():\n            if t == token:\n                break\n            yield t"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_type(self):\n        first_token = self.token_first(skip_cm=True)\n        if first_token is None:\n            # An \"empty\" statement that either has not tokens at all\n            # or only whitespace tokens.\n            return 'UNKNOWN'\n\n        elif first_token.ttype in (T.Keyword.DML, T.Keyword.DDL):\n            return first_token.normalized\n\n        elif first_token.ttype == T.Keyword.CTE:\n            # The WITH keyword should be followed by either an Identifier or\n            # an IdentifierList containing the CTE definitions;  the actual\n            # DML keyword (e.g. SELECT, INSERT) will follow next.\n            fidx = self.token_index(first_token)\n            tidx, token = self.token_next(fidx, skip_ws=True)\n            if isinstance(token, (Identifier, IdentifierList)):\n                _, dml_keyword = self.token_next(tidx, skip_ws=True)\n\n                if dml_keyword is not None \\\n                        and dml_keyword.ttype == T.Keyword.DML:\n                    return dml_keyword.normalized\n\n        # Hmm, probably invalid syntax, so return unknown.\n        return 'UNKNOWN'", "response": "Returns the type of a statement."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if this identifier contains a wildcard.", "response": "def is_wildcard(self):\n        \"\"\"Return ``True`` if this identifier contains a wildcard.\"\"\"\n        _, token = self.token_next_by(t=T.Wildcard)\n        return token is not None"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_typecast(self):\n        midx, marker = self.token_next_by(m=(T.Punctuation, '::'))\n        nidx, next_ = self.token_next(midx, skip_ws=False)\n        return next_.value if next_ else None", "response": "Returns the typecast or None of this object as a string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_ordering(self):\n        _, ordering = self.token_next_by(t=T.Keyword.Order)\n        return ordering.normalized if ordering else None", "response": "Returns the ordering or None as uppercase string."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_array_indices(self):\n\n        for token in self.tokens:\n            if isinstance(token, SquareBrackets):\n                # Use [1:-1] index to discard the square brackets\n                yield token.tokens[1:-1]", "response": "Returns an iterator of index token lists"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_identifiers(self):\n        for token in self.tokens:\n            if not (token.is_whitespace or token.match(T.Punctuation, ',')):\n                yield token", "response": "Returns the identifiers. Whitespaces and punctuations are not included in this generator."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_cases(self, skip_ws=False):\n        CONDITION = 1\n        VALUE = 2\n\n        ret = []\n        mode = CONDITION\n\n        for token in self.tokens:\n            # Set mode from the current statement\n            if token.match(T.Keyword, 'CASE'):\n                continue\n\n            elif skip_ws and token.ttype in T.Whitespace:\n                continue\n\n            elif token.match(T.Keyword, 'WHEN'):\n                ret.append(([], []))\n                mode = CONDITION\n\n            elif token.match(T.Keyword, 'THEN'):\n                mode = VALUE\n\n            elif token.match(T.Keyword, 'ELSE'):\n                ret.append((None, []))\n                mode = VALUE\n\n            elif token.match(T.Keyword, 'END'):\n                mode = None\n\n            # First condition without preceding WHEN\n            if mode and not ret:\n                ret.append(([], []))\n\n            # Append token depending of the current mode\n            if mode == CONDITION:\n                ret[-1][0].append(token)\n\n            elif mode == VALUE:\n                ret[-1][1].append(token)\n\n        # Return cases list\n        return ret", "response": "Returns a list of 2 - tuples. If an ELSE exists condition is None."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a list of parameters.", "response": "def get_parameters(self):\n        \"\"\"Return a list of parameters.\"\"\"\n        parenthesis = self.tokens[-1]\n        for token in parenthesis.tokens:\n            if isinstance(token, IdentifierList):\n                return token.get_identifiers()\n            elif imt(token, i=(Function, Identifier), t=T.Literal):\n                return [token, ]\n        return []"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nturn all the output devices on.", "response": "def on(self):\n        \"\"\"\n        Turn all the output devices on.\n        \"\"\"\n        for device in self:\n            if isinstance(device, (OutputDevice, CompositeOutputDevice)):\n                device.on()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef off(self):\n        for device in self:\n            if isinstance(device, (OutputDevice, CompositeOutputDevice)):\n                device.off()", "response": "Turn all the output devices off."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef toggle(self):\n        for device in self:\n            if isinstance(device, (OutputDevice, CompositeOutputDevice)):\n                device.toggle()", "response": "Toggle all the output devices."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef on(self, *args):\n        self._stop_blink()\n        if args:\n            for index in args:\n                self[index].on()\n        else:\n            super(LEDBoard, self).on()", "response": "Turn all the LEDs on."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nturns all the LEDs on and off.", "response": "def off(self, *args):\n        \"\"\"\n        If no arguments are specified, turn all the LEDs off. If arguments are\n        specified, they must be the indexes of the LEDs you wish to turn off.\n        For example::\n\n            from gpiozero import LEDBoard\n\n            leds = LEDBoard(2, 3, 4, 5)\n            leds.on()      # turn on all LEDs\n            leds.off(0)    # turn off the first LED (pin 2)\n            leds.off(-1)   # turn off the last LED (pin 5)\n            leds.off(1, 2) # turn off the middle LEDs (pins 3 and 4)\n            leds.on()      # turn on all LEDs\n\n        If :meth:`blink` is currently active, it will be stopped first.\n\n        :param int args:\n            The index(es) of the LED(s) to turn off. If no indexes are\n            specified turn off all LEDs.\n        \"\"\"\n        self._stop_blink()\n        if args:\n            for index in args:\n                self[index].off()\n        else:\n            super(LEDBoard, self).off()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntoggle the state of all the items in the specified items.", "response": "def toggle(self, *args):\n        \"\"\"\n        If no arguments are specified, toggle the state of all LEDs. If\n        arguments are specified, they must be the indexes of the LEDs you wish\n        to toggle. For example::\n\n            from gpiozero import LEDBoard\n\n            leds = LEDBoard(2, 3, 4, 5)\n            leds.toggle(0)   # turn on the first LED (pin 2)\n            leds.toggle(-1)  # turn on the last LED (pin 5)\n            leds.toggle()    # turn the first and last LED off, and the\n                             # middle pair on\n\n        If :meth:`blink` is currently active, it will be stopped first.\n\n        :param int args:\n            The index(es) of the LED(s) to toggle. If no indexes are specified\n            toggle the state of all LEDs.\n        \"\"\"\n        self._stop_blink()\n        if args:\n            for index in args:\n                self[index].toggle()\n        else:\n            super(LEDBoard, self).toggle()"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes all LEDs fade in and out repeatedly. Note that this method will only work if the *pwm* parameter was :data:`True` at construction time. :param float fade_in_time: Number of seconds to spend fading in. Defaults to 1. :param float fade_out_time: Number of seconds to spend fading out. Defaults to 1. :type n: int or None :param n: Number of times to blink; :data:`None` (the default) means forever. :param bool background: If :data:`True` (the default), start a background thread to continue blinking and return immediately. If :data:`False`, only return when the blink is finished (warning: the default value of *n* will result in this method never returning).", "response": "def pulse(self, fade_in_time=1, fade_out_time=1, n=None, background=True):\n        \"\"\"\n        Make all LEDs fade in and out repeatedly. Note that this method will\n        only work if the *pwm* parameter was :data:`True` at construction time.\n\n        :param float fade_in_time:\n            Number of seconds to spend fading in. Defaults to 1.\n\n        :param float fade_out_time:\n            Number of seconds to spend fading out. Defaults to 1.\n\n        :type n: int or None\n        :param n:\n            Number of times to blink; :data:`None` (the default) means forever.\n\n        :param bool background:\n            If :data:`True` (the default), start a background thread to\n            continue blinking and return immediately. If :data:`False`, only\n            return when the blink is finished (warning: the default value of\n            *n* will result in this method never returning).\n        \"\"\"\n        on_time = off_time = 0\n        self.blink(\n            on_time, off_time, fade_in_time, fade_out_time, n, background)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef value(self):\n        result = sum(led.value for led in self)\n        if self[0].value < self[-1].value:\n            result = -result\n        return result / len(self)", "response": "The value of the internal system."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef lit_count(self):\n        lit_value = self.value * len(self)\n        if not isinstance(self[0], PWMLED):\n            lit_value = int(lit_value)\n        return lit_value", "response": "Returns the number of LEDs on the bar graph actually lit up."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef left(self, speed=1):\n        self.right_motor.forward(speed)\n        self.left_motor.backward(speed)", "response": "Make the robot turn left by running the right motor forward and left motor backward."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmaking the robot turn right by running the left motor forward and right motor backward.", "response": "def right(self, speed=1):\n        \"\"\"\n        Make the robot turn right by running the left motor forward and right\n        motor backward.\n\n        :param float speed:\n            Speed at which to drive the motors, as a value between 0 (stopped)\n            and 1 (full speed). The default is 1.\n        \"\"\"\n        self.left_motor.forward(speed)\n        self.right_motor.backward(speed)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef backward(self, speed=1):\n        self.left_motor.backward(speed)\n        self.right_motor.backward(speed)", "response": "Drive the robot backward by running both motors backward."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreverses the robot s current motor directions.", "response": "def reverse(self):\n        \"\"\"\n        Reverse the robot's current motor directions. If the robot is currently\n        running full speed forward, it will run full speed backward. If the\n        robot is turning left at half-speed, it will turn right at half-speed.\n        If the robot is currently stopped it will remain stopped.\n        \"\"\"\n        self.left_motor.value = -self.left_motor.value\n        self.right_motor.value = -self.right_motor.value"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef reserve_pins(self, requester, *pins):\n        with self._res_lock:\n            for pin in pins:\n                for reserver_ref in self._reservations[pin]:\n                    reserver = reserver_ref()\n                    if reserver is not None and requester._conflicts_with(reserver):\n                        raise GPIOPinInUse('pin %s is already in use by %r' %\n                                           (pin, reserver))\n                self._reservations[pin].append(ref(requester))", "response": "Reserves the specified pins for the given user."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreleases the reservation of reserver against pins.", "response": "def release_pins(self, reserver, *pins):\n        \"\"\"\n        Releases the reservation of *reserver* against *pins*.  This is\n        typically called during :meth:`~gpiozero.Device.close` to clean up\n        reservations taken during construction. Releasing a reservation that is\n        not currently held will be silently ignored (to permit clean-up after\n        failed / partial construction).\n        \"\"\"\n        with self._res_lock:\n            for pin in pins:\n                self._reservations[pin] = [\n                    ref for ref in self._reservations[pin]\n                    if ref() not in (reserver, None) # may as well clean up dead refs\n                    ]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef pin(self, spec, pin_class=None, **kwargs):\n        if pin_class is None:\n            pin_class = self.pin_class\n        n = self.pi_info.to_gpio(spec)\n        try:\n            pin = self.pins[n]\n        except KeyError:\n            pin = pin_class(self, n, **kwargs)\n            self.pins[n] = pin\n        else:\n            # Ensure the pin class expected supports PWM (or not)\n            if issubclass(pin_class, MockPWMPin) != isinstance(pin, MockPWMPin):\n                raise ValueError('pin %d is already in use as a %s' % (n, pin.__class__.__name__))\n        return pin", "response": "Returns a new instance of the specified pin class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pi_info(revision=None):\n    if revision is None:\n        if Device.pin_factory is None:\n            Device.pin_factory = Device._default_pin_factory()\n        result = Device.pin_factory.pi_info\n        if result is None:\n            raise PinUnknownPi('The default pin_factory is not attached to a Pi')\n        else:\n            return result\n    else:\n        if isinstance(revision, bytes):\n            revision = revision.decode('ascii')\n        if isinstance(revision, str):\n            revision = int(revision, base=16)\n        else:\n            # be nice to people passing an int (or something numeric anyway)\n            revision = int(revision)\n        return PiBoardInfo.from_revision(revision)", "response": "Returns a PiBoardInfo instance containing information about the specified revision of the Raspberry Pi."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the set of physical pins that support the specified function.", "response": "def physical_pins(self, function):\n        \"\"\"\n        Return the physical pins supporting the specified *function* as tuples\n        of ``(header, pin_number)`` where *header* is a string specifying the\n        header containing the *pin_number*. Note that the return value is a\n        :class:`set` which is not indexable. Use :func:`physical_pin` if you\n        are expecting a single return value.\n\n        :param str function:\n            The pin function you wish to search for. Usually this is something\n            like \"GPIO9\" for Broadcom GPIO pin 9, or \"GND\" for all the pins\n            connecting to electrical ground.\n        \"\"\"\n        return {\n            (header, pin.number)\n            for (header, info) in self.headers.items()\n            for pin in info.pins.values()\n            if pin.function == function\n            }"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the physical pin that supports the specified function.", "response": "def physical_pin(self, function):\n        \"\"\"\n        Return the physical pin supporting the specified *function*. If no pins\n        support the desired *function*, this function raises :exc:`PinNoPins`.\n        If multiple pins support the desired *function*, :exc:`PinMultiplePins`\n        will be raised (use :func:`physical_pins` if you expect multiple pins\n        in the result, such as for electrical ground).\n\n        :param str function:\n            The pin function you wish to search for. Usually this is something\n            like \"GPIO9\" for Broadcom GPIO pin 9.\n        \"\"\"\n        result = self.physical_pins(function)\n        if len(result) > 1:\n            raise PinMultiplePins('multiple pins can be used for %s' % function)\n        elif result:\n            return result.pop()\n        else:\n            raise PinNoPins('no pins can be used for %s' % function)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a bool indicating whether a physical pull - up is attached to the specified function.", "response": "def pulled_up(self, function):\n        \"\"\"\n        Returns a bool indicating whether a physical pull-up is attached to\n        the pin supporting the specified *function*. Either :exc:`PinNoPins`\n        or :exc:`PinMultiplePins` may be raised if the function is not\n        associated with a single pin.\n\n        :param str function:\n            The pin function you wish to determine pull-up for. Usually this is\n            something like \"GPIO9\" for Broadcom GPIO pin 9.\n        \"\"\"\n        try:\n            header, number = self.physical_pin(function)\n        except PinNoPins:\n            return False\n        else:\n            return self.headers[header].pins[number].pull_up"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nparsing a string representing a Broadcom GPIO port number and returns the equivalent Broadcom GPIO port number.", "response": "def to_gpio(self, spec):\n        \"\"\"\n        Parses a pin *spec*, returning the equivalent Broadcom GPIO port number\n        or raising a :exc:`ValueError` exception if the spec does not represent\n        a GPIO port.\n\n        The *spec* may be given in any of the following forms:\n\n        * An integer, which will be accepted as a GPIO number\n        * 'GPIOn' where n is the GPIO number\n        * 'WPIn' where n is the `wiringPi`_ pin number\n        * 'BCMn' where n is the GPIO number (alias of GPIOn)\n        * 'BOARDn' where n is the physical pin number on the main header\n        * 'h:n' where h is the header name and n is the physical pin number\n          (for example J8:5 is physical pin 5 on header J8, which is the main\n          header on modern Raspberry Pis)\n\n        .. _wiringPi: http://wiringpi.com/pins/\n        \"\"\"\n        if isinstance(spec, int):\n            if not 0 <= spec < 54:\n                raise PinInvalidPin('invalid GPIO port %d specified '\n                                    '(range 0..53) ' % spec)\n            return spec\n        else:\n            if isinstance(spec, bytes):\n                spec = spec.decode('ascii')\n            spec = spec.upper()\n            if spec.isdigit():\n                return self.to_gpio(int(spec))\n            if spec.startswith('GPIO') and spec[4:].isdigit():\n                return self.to_gpio(int(spec[4:]))\n            elif spec.startswith('BCM') and spec[3:].isdigit():\n                return self.to_gpio(int(spec[3:]))\n            elif spec.startswith('WPI') and spec[3:].isdigit():\n                main_head = 'P1' if 'P1' in self.headers else 'J8'\n                try:\n                    return self.to_gpio({\n                        0:  '%s:11' % main_head,\n                        1:  '%s:12' % main_head,\n                        2:  '%s:13' % main_head,\n                        3:  '%s:15' % main_head,\n                        4:  '%s:16' % main_head,\n                        5:  '%s:18' % main_head,\n                        6:  '%s:22' % main_head,\n                        7:  '%s:7'  % main_head,\n                        8:  '%s:3'  % main_head,\n                        9:  '%s:5'  % main_head,\n                        10: '%s:24' % main_head,\n                        11: '%s:26' % main_head,\n                        12: '%s:19' % main_head,\n                        13: '%s:21' % main_head,\n                        14: '%s:23' % main_head,\n                        15: '%s:8'  % main_head,\n                        16: '%s:10' % main_head,\n                        17: 'P5:3',\n                        18: 'P5:4',\n                        19: 'P5:5',\n                        20: 'P5:6',\n                        21: '%s:29' % main_head,\n                        22: '%s:31' % main_head,\n                        23: '%s:33' % main_head,\n                        24: '%s:35' % main_head,\n                        25: '%s:37' % main_head,\n                        26: '%s:32' % main_head,\n                        27: '%s:36' % main_head,\n                        28: '%s:38' % main_head,\n                        29: '%s:40' % main_head,\n                        30: '%s:27' % main_head,\n                        31: '%s:28' % main_head,\n                        }[int(spec[3:])])\n                except KeyError:\n                    raise PinInvalidPin('%s is not a valid wiringPi pin' % spec)\n            elif ':' in spec:\n                header, pin = spec.split(':', 1)\n                if pin.isdigit():\n                    try:\n                        header = self.headers[header]\n                    except KeyError:\n                        raise PinInvalidPin(\n                            'there is no %s header on this Pi' % header)\n                    try:\n                        function = header.pins[int(pin)].function\n                    except KeyError:\n                        raise PinInvalidPin(\n                            'no such pin %s on header %s' % (pin, header.name))\n                    if function.startswith('GPIO') and function[4:].isdigit():\n                        return self.to_gpio(int(function[4:]))\n                    else:\n                        raise PinInvalidPin('%s is not a GPIO pin' % spec)\n            elif spec.startswith('BOARD') and spec[5:].isdigit():\n                main_head = ({'P1', 'J8', 'SODIMM'} & set(self.headers)).pop()\n                return self.to_gpio('%s:%s' % (main_head, spec[5:]))\n            raise PinInvalidPin('%s is not a valid pin spec' % spec)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn an SPI interface for the specified port and device.", "response": "def spi(self, **spi_args):\n        \"\"\"\n        Returns an SPI interface, for the specified SPI *port* and *device*, or\n        for the specified pins (*clock_pin*, *mosi_pin*, *miso_pin*, and\n        *select_pin*).  Only one of the schemes can be used; attempting to mix\n        *port* and *device* with pin numbers will raise\n        :exc:`~gpiozero.SPIBadArgs`.\n\n        If the pins specified match the hardware SPI pins (clock on GPIO11,\n        MOSI on GPIO10, MISO on GPIO9, and chip select on GPIO8 or GPIO7), and\n        the spidev module can be imported, a hardware based interface (using\n        spidev) will be returned. Otherwise, a software based interface will be\n        returned which will use simple bit-banging to communicate.\n\n        Both interfaces have the same API, support clock polarity and phase\n        attributes, and can handle half and full duplex communications, but the\n        hardware interface is significantly faster (though for many things this\n        doesn't matter).\n        \"\"\"\n        spi_args, kwargs = self._extract_spi_args(**spi_args)\n        shared = 'shared' if kwargs.pop('shared', False) else 'exclusive'\n        if kwargs:\n            raise SPIBadArgs(\n                'unrecognized keyword argument %s' % kwargs.popitem()[0])\n        for port, pins in SPI_HARDWARE_PINS.items():\n            if all((\n                    spi_args['clock_pin']  == pins['clock'],\n                    spi_args['mosi_pin']   == pins['mosi'],\n                    spi_args['miso_pin']   == pins['miso'],\n                    spi_args['select_pin'] in pins['select'],\n                    )):\n                try:\n                    return self.spi_classes[('hardware', shared)](\n                        self, port=port,\n                        device=pins['select'].index(spi_args['select_pin'])\n                        )\n                except Exception as e:\n                    warnings.warn(\n                        SPISoftwareFallback(\n                            'failed to initialize hardware SPI, falling back to '\n                            'software (error was: %s)' % str(e)))\n                    break\n        return self.spi_classes[('software', shared)](self, **spi_args)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nextracts the SPI arguments from the given set of keyword arguments and returns a tuple of the two arguments.", "response": "def _extract_spi_args(self, **kwargs):\n        \"\"\"\n        Given a set of keyword arguments, splits it into those relevant to SPI\n        implementations and all the rest. SPI arguments are augmented with\n        defaults and converted into the pin format (from the port/device\n        format) if necessary.\n\n        Returns a tuple of ``(spi_args, other_args)``.\n        \"\"\"\n        dev_defaults = {\n            'port': 0,\n            'device': 0,\n            }\n        default_hw = SPI_HARDWARE_PINS[dev_defaults['port']]\n        pin_defaults = {\n            'clock_pin':  default_hw['clock'],\n            'mosi_pin':   default_hw['mosi'],\n            'miso_pin':   default_hw['miso'],\n            'select_pin': default_hw['select'][dev_defaults['device']],\n            }\n        spi_args = {\n            key: value for (key, value) in kwargs.items()\n            if key in pin_defaults or key in dev_defaults\n            }\n        kwargs = {\n            key: value for (key, value) in kwargs.items()\n            if key not in spi_args\n            }\n        if not spi_args:\n            spi_args = pin_defaults\n        elif set(spi_args) <= set(pin_defaults):\n            spi_args = {\n                key: self.pi_info.to_gpio(spi_args.get(key, default))\n                for key, default in pin_defaults.items()\n                }\n        elif set(spi_args) <= set(dev_defaults):\n            spi_args = {\n                key: spi_args.get(key, default)\n                for key, default in dev_defaults.items()\n                }\n            try:\n                selected_hw = SPI_HARDWARE_PINS[spi_args['port']]\n            except KeyError:\n                raise SPIBadArgs(\n                    'port %d is not a valid SPI port' % spi_args['port'])\n            try:\n                selected_hw['select'][spi_args['device']]\n            except IndexError:\n                raise SPIBadArgs(\n                    'device must be in the range 0..%d' %\n                    len(selected_hw['select']))\n            spi_args = {\n                key: value if key != 'select_pin' else selected_hw['select'][spi_args['device']]\n                for key, value in pin_defaults.items()\n                }\n        else:\n            raise SPIBadArgs(\n                'you must either specify port and device, or clock_pin, '\n                'mosi_pin, miso_pin, and select_pin; combinations of the two '\n                'schemes (e.g. port and clock_pin) are not permitted')\n        return spi_args, kwargs"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _call_when_changed(self, ticks, state):\n        method = self._when_changed()\n        if method is None:\n            self.when_changed = None\n        else:\n            method(ticks, state)", "response": "Calls the _when_changed method of the related object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _int_to_words(self, pattern):\n        try:\n            bits_required = int(ceil(log(pattern, 2))) + 1\n        except ValueError:\n            # pattern == 0 (technically speaking, no bits are required to\n            # transmit the value zero ;)\n            bits_required = 1\n        shifts = range(0, bits_required, self._spi.bits_per_word)[::-1]\n        mask = 2 ** self._spi.bits_per_word - 1\n        return [(pattern >> shift) & mask for shift in shifts]", "response": "Given an integer number return a sequence of individual words that make up the pattern."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a sequence of words which each fit in the internal SPI interface s number of bits per word returns the value obtained by concatenating each word into a single bit - string.", "response": "def _words_to_int(self, words, expected_bits=None):\n        \"\"\"\n        Given a sequence of words which each fit in the internal SPI\n        interface's number of bits per word, returns the value obtained by\n        concatenating each word into a single bit-string.\n\n        If *expected_bits* is specified, it limits the size of the output to\n        the specified number of bits (by masking off bits above the expected\n        number). If unspecified, no limit will be applied.\n        \"\"\"\n        if expected_bits is None:\n            expected_bits = len(words) * self._spi.bits_per_word\n        shifts = range(0, expected_bits, self._spi.bits_per_word)[::-1]\n        mask = 2 ** expected_bits - 1\n        return reduce(or_, (word << shift for word, shift in zip(words, shifts))) & mask"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef transfer(self, data, clock_phase=False, lsb_first=False, bits_per_word=8):\n        result = []\n        with self.lock:\n            # See https://en.wikipedia.org/wiki/Serial_Peripheral_Interface_Bus\n            # (specifically the section \"Example of bit-banging the master\n            # protocol\") for a simpler C implementation of this which ignores\n            # clock polarity, phase, variable word-size, and multiple input\n            # words\n            if lsb_first:\n                shift = operator.lshift\n                init_mask = 1\n            else:\n                shift = operator.rshift\n                init_mask = 1 << (bits_per_word - 1)\n            for write_word in data:\n                mask = init_mask\n                read_word = 0\n                for _ in range(bits_per_word):\n                    if self.mosi is not None:\n                        self.mosi.value = bool(write_word & mask)\n                    # read bit on clock activation\n                    self.clock.on()\n                    if not clock_phase:\n                        if self.miso is not None and self.miso.value:\n                            read_word |= mask\n                    # read bit on clock deactivation\n                    self.clock.off()\n                    if clock_phase:\n                        if self.miso is not None and self.miso.value:\n                            read_word |= mask\n                    mask = shift(mask, 1)\n                result.append(read_word)\n        return result", "response": "Writes data to the SPI interface and reads an equivalent number of words."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a boolean indicating if the host returned a single ping and if the host returned a single ping and the user expects to change the current value.", "response": "def value(self):\n        \"\"\"\n        Returns :data:`True` if the host returned a single ping, and\n        :data:`False` otherwise.\n        \"\"\"\n        # XXX This is doing a DNS lookup every time it's queried; should we\n        # call gethostbyname in the constructor and ping that instead (good\n        # for consistency, but what if the user *expects* the host to change\n        # address?)\n        with io.open(os.devnull, 'wb') as devnull:\n            try:\n                subprocess.check_call(\n                    ['ping', '-c1', self.host],\n                    stdout=devnull, stderr=devnull)\n            except subprocess.CalledProcessError:\n                return False\n            else:\n                return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef temperature(self):\n        with io.open(self.sensor_file, 'r') as f:\n            return float(f.readline().strip()) / 1000", "response": "Returns the current CPU temperature in degrees celsius."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef value(self):\n        temp_range = self.max_temp - self.min_temp\n        return (self.temperature - self.min_temp) / temp_range", "response": "Returns the current CPU temperature as a value between 0. 0 and 1. 0"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_average(self):\n        with io.open(self.load_average_file, 'r') as f:\n            file_columns = f.readline().strip().split()\n            return float(file_columns[self._load_average_file_column])", "response": "Returns the current load average."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the current load average as a value between 0. 0 and 1. 0 respectively.", "response": "def value(self):\n        \"\"\"\n        Returns the current load average as a value between 0.0 (representing\n        the *min_load_average* value) and 1.0 (representing the\n        *max_load_average* value). These default to 0.0 and 1.0 respectively.\n        \"\"\"\n        load_average_range = self.max_load_average - self.min_load_average\n        return (self.load_average - self.min_load_average) / load_average_range"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a boolean value indicating whether the system clock reads between the start_time and the end_time of the a .", "response": "def value(self):\n        \"\"\"\n        Returns :data:`True` when the system clock reads between\n        :attr:`start_time` and :attr:`end_time`, and :data:`False` otherwise.\n        If :attr:`start_time` is greater than :attr:`end_time` (indicating a\n        period that crosses midnight), then this returns :data:`True` when the\n        current time is greater than :attr:`start_time` or less than\n        :attr:`end_time`.\n        \"\"\"\n        now = datetime.utcnow().time() if self.utc else datetime.now().time()\n        if self.start_time < self.end_time:\n            return self.start_time <= now <= self.end_time\n        else:\n            return not self.end_time < now < self.start_time"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the current usage as a value between 0. 0 and 1. 0 by dividing by 100.", "response": "def value(self):\n        \"\"\"\n        Returns the current disk usage as a value between 0.0 and 1.0 by\n        dividing :attr:`usage` by 100.\n        \"\"\"\n        # This slightly convoluted calculation is equivalent to df's \"Use%\";\n        # it calculates the percentage of FS usage as a proportion of the\n        # space available to *non-root users*. Technically this means it can\n        # exceed 100% (when FS is filled to the point that only root can write\n        # to it), hence the clamp.\n        vfs = os.statvfs(self.filesystem)\n        used = vfs.f_blocks - vfs.f_bfree\n        total = used + vfs.f_bavail\n        return min(1.0, used / total)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef inverted(values, input_min=0, input_max=1):\n    values = _normalize(values)\n    if input_min >= input_max:\n        raise ValueError('input_min must be smaller than input_max')\n    for v in values:\n        yield input_min + input_max - v", "response": "Yields the inversion of the supplied values."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef scaled(values, output_min, output_max, input_min=0, input_max=1):\n    values = _normalize(values)\n    if input_min >= input_max:\n        raise ValueError('input_min must be smaller than input_max')\n    input_size = input_max - input_min\n    output_size = output_max - output_min\n    for v in values:\n        yield (((v - input_min) / input_size) * output_size) + output_min", "response": "Returns a generator that yields all items in values that lie between output_min and output_max."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a generator that yields items from values from output_min to output_max.", "response": "def clamped(values, output_min=0, output_max=1):\n    \"\"\"\n    Returns *values* clamped from *output_min* to *output_max*, i.e. any items\n    less than *output_min* will be returned as *output_min* and any items\n    larger than *output_max* will be returned as *output_max* (these default to\n    0 and 1 respectively). For example::\n\n        from gpiozero import PWMLED, MCP3008\n        from gpiozero.tools import clamped\n        from signal import pause\n\n        led = PWMLED(4)\n        pot = MCP3008(channel=0)\n\n        led.source = clamped(pot, 0.5, 1.0)\n\n        pause()\n    \"\"\"\n    values = _normalize(values)\n    if output_min >= output_max:\n        raise ValueError('output_min must be smaller than output_max')\n    for v in values:\n        yield min(max(v, output_min), output_max)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a generator that yields the values quantized to steps increments.", "response": "def quantized(values, steps, input_min=0, input_max=1):\n    \"\"\"\n    Returns *values* quantized to *steps* increments. All items in *values* are\n    assumed to be between *input_min* and *input_max* (which default to 0 and\n    1 respectively), and the output will be in the same range.\n\n    For example, to quantize values between 0 and 1 to 5 \"steps\" (0.0, 0.25,\n    0.5, 0.75, 1.0)::\n\n        from gpiozero import PWMLED, MCP3008\n        from gpiozero.tools import quantized\n        from signal import pause\n\n        led = PWMLED(4)\n        pot = MCP3008(channel=0)\n\n        led.source = quantized(pot, 4)\n\n        pause()\n    \"\"\"\n    values = _normalize(values)\n    if steps < 1:\n        raise ValueError(\"steps must be 1 or larger\")\n    if input_min >= input_max:\n        raise ValueError('input_min must be smaller than input_max')\n    input_size = input_max - input_min\n    for v in scaled(values, 0, 1, input_min, input_max):\n        yield ((int(v * steps) / steps) * input_size) + input_min"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef booleanized(values, min_value, max_value, hysteresis=0):\n    values = _normalize(values)\n    if min_value >= max_value:\n        raise ValueError('min_value must be smaller than max_value')\n    min_value = float(min_value)\n    max_value = float(max_value)\n    if hysteresis < 0:\n        raise ValueError(\"hysteresis must be 0 or larger\")\n    else:\n        hysteresis = float(hysteresis)\n    if (max_value - min_value) <= hysteresis:\n        raise ValueError('The gap between min_value and max_value must be '\n                         'larger than hysteresis')\n    last_state = None\n    for v in values:\n        if v < min_value:\n            new_state = 'below'\n        elif v > max_value:\n            new_state = 'above'\n        else:\n            new_state = 'in'\n        switch = False\n        if last_state == None or not hysteresis:\n            switch = True\n        elif new_state == last_state:\n            pass\n        else: # new_state != last_state\n            if last_state == 'below' and new_state == 'in':\n                switch = v >= min_value + hysteresis\n            elif last_state == 'in' and new_state == 'below':\n                switch = v < min_value - hysteresis\n            elif last_state == 'in' and new_state == 'above':\n                switch = v > max_value + hysteresis\n            elif last_state == 'above' and new_state == 'in':\n                switch = v <= max_value - hysteresis\n            else: # above->below or below->above\n                switch = True\n        if switch:\n            last_state = new_state\n        yield last_state == 'in'", "response": "Returns True if each item in values is in a boolean state between min_value and max_value and False otherwise."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef all_values(*values):\n    print(\"here\")\n    values = [_normalize(v) for v in values]\n    for v in zip(*values):\n        yield all(v)", "response": "Returns the logical conjunction of all supplied values."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef any_values(*values):\n    values = [_normalize(v) for v in values]\n    for v in zip(*values):\n        yield any(v)", "response": "Returns the logical disjunction of all supplied values."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the mean of all supplied values. One or more values can be .", "response": "def averaged(*values):\n    \"\"\"\n    Returns the mean of all supplied values. One or more *values* can be\n    specified. For example, to light a :class:`~gpiozero.PWMLED` as the average\n    of several potentiometers connected to an :class:`~gpiozero.MCP3008` ADC::\n\n        from gpiozero import MCP3008, PWMLED\n        from gpiozero.tools import averaged\n        from signal import pause\n\n        pot1 = MCP3008(channel=0)\n        pot2 = MCP3008(channel=1)\n        pot3 = MCP3008(channel=2)\n        led = PWMLED(4)\n\n        led.source = averaged(pot1, pot2, pot3)\n\n        pause()\n    \"\"\"\n    values = [_normalize(v) for v in values]\n    for v in zip(*values):\n        yield mean(v)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef summed(*values):\n    values = [_normalize(v) for v in values]\n    for v in zip(*values):\n        yield sum(v)", "response": "Returns the sum of all supplied values."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the product of all supplied values.", "response": "def multiplied(*values):\n    \"\"\"\n    Returns the product of all supplied values. One or more *values* can be\n    specified. For example, to light a :class:`~gpiozero.PWMLED` as the product\n    (i.e. multiplication) of several potentiometers connected to an\n    :class:`~gpiozero.MCP3008`\n    ADC::\n\n        from gpiozero import MCP3008, PWMLED\n        from gpiozero.tools import multiplied\n        from signal import pause\n\n        pot1 = MCP3008(channel=0)\n        pot2 = MCP3008(channel=1)\n        pot3 = MCP3008(channel=2)\n        led = PWMLED(4)\n\n        led.source = multiplied(pot1, pot2, pot3)\n\n        pause()\n    \"\"\"\n    values = [_normalize(v) for v in values]\n    def _product(it):\n        p = 1\n        for n in it:\n            p *= n\n        return p\n    for v in zip(*values):\n        yield _product(v)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nyield the next qsize items from the given list of values.", "response": "def queued(values, qsize):\n    \"\"\"\n    Queues up readings from *values* (the number of readings queued is\n    determined by *qsize*) and begins yielding values only when the queue is\n    full. For example, to \"cascade\" values along a sequence of LEDs::\n\n        from gpiozero import LEDBoard, Button\n        from gpiozero.tools import queued\n        from signal import pause\n\n        leds = LEDBoard(5, 6, 13, 19, 26)\n        btn = Button(17)\n\n        for i in range(4):\n            leds[i].source = queued(leds[i + 1], 5)\n            leds[i].source_delay = 0.01\n\n        leds[4].source = btn\n\n        pause()\n    \"\"\"\n    values = [_normalize(v) for v in values]\n    if qsize < 1:\n        raise ValueError(\"qsize must be 1 or larger\")\n    q = []\n    it = iter(values)\n    try:\n        for i in range(qsize):\n            q.append(next(it))\n        for i in cycle(range(qsize)):\n            yield q[i]\n            q[i] = next(it)\n    except StopIteration:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn a generator that yields the average of the readings in the last qsize values.", "response": "def smoothed(values, qsize, average=mean):\n    \"\"\"\n    Queues up readings from *values* (the number of readings queued is\n    determined by *qsize*) and begins yielding the *average* of the last\n    *qsize* values when the queue is full. The larger the *qsize*, the more the\n    values are smoothed. For example, to smooth the analog values read from an\n    ADC::\n\n        from gpiozero import MCP3008\n        from gpiozero.tools import smoothed\n\n        adc = MCP3008(channel=0)\n\n        for value in smoothed(adc, 5):\n            print(value)\n    \"\"\"\n    values = _normalize(values)\n    if qsize < 1:\n        raise ValueError(\"qsize must be 1 or larger\")\n    q = []\n    it = iter(values)\n    try:\n        for i in range(qsize):\n            q.append(next(it))\n        for i in cycle(range(qsize)):\n            yield average(q)\n            q[i] = next(it)\n    except StopIteration:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_delayed(values, delay):\n    values = _normalize(values)\n    if delay < 0:\n        raise ValueError(\"delay must be 0 or larger\")\n    for v in values:\n        sleep(delay)\n        yield v", "response": "Yields the items from the given iterable."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef post_delayed(values, delay):\n    values = _normalize(values)\n    if delay < 0:\n        raise ValueError(\"delay must be 0 or larger\")\n    for v in values:\n        yield v\n        sleep(delay)", "response": "Yields the items from the given iterable after a delay."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef pre_periodic_filtered(values, block, repeat_after):\n    values = _normalize(values)\n    if block < 1:\n        raise ValueError(\"block must be 1 or larger\")\n    if repeat_after < 0:\n        raise ValueError(\"repeat_after must be 0 or larger\")\n    it = iter(values)\n    try:\n        if repeat_after == 0:\n            for _ in range(block):\n                next(it)\n            while True:\n                yield next(it)\n        else:\n            while True:\n                for _ in range(block):\n                    next(it)\n                for _ in range(repeat_after):\n                    yield next(it)\n    except StopIteration:\n        pass", "response": "Yields the items from the given list of items in the specified block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nyield the next repeat_after items from the given list of items.", "response": "def post_periodic_filtered(values, repeat_after, block):\n    \"\"\"\n    After every *repeat_after* items, blocks the next *block* items from\n    *values*. Note that unlike :func:`pre_periodic_filtered`, *repeat_after*\n    can't be 0. For example, to block every tenth item read from an ADC::\n\n        from gpiozero import MCP3008\n        from gpiozero.tools import post_periodic_filtered\n\n        adc = MCP3008(channel=0)\n\n        for value in post_periodic_filtered(adc, 9, 1):\n            print(value)\n    \"\"\"\n    values = _normalize(values)\n    if repeat_after < 1:\n        raise ValueError(\"repeat_after must be 1 or larger\")\n    if block < 1:\n        raise ValueError(\"block must be 1 or larger\")\n    it = iter(values)\n    try:\n        while True:\n            for _ in range(repeat_after):\n                yield next(it)\n            for _ in range(block):\n                next(it)\n    except StopIteration:\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sin_values(period=360):\n    angles = (2 * pi * i / period for i in range(period))\n    for a in cycle(angles):\n        yield sin(a)", "response": "Generates an infinite source of values representing a sine wave."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngenerates an infinite source of values representing a cosine wave.", "response": "def cos_values(period=360):\n    \"\"\"\n    Provides an infinite source of values representing a cosine wave (from -1\n    to +1) which repeats every *period* values. For example, to produce a\n    \"siren\" effect with a couple of LEDs that repeats once a second::\n\n        from gpiozero import PWMLED\n        from gpiozero.tools import cos_values, scaled, inverted\n        from signal import pause\n\n        red = PWMLED(2)\n        blue = PWMLED(3)\n\n        red.source_delay = 0.01\n        blue.source_delay = red.source_delay\n        red.source = scaled(cos_values(100), 0, 1, -1, 1)\n        blue.source = inverted(red)\n\n        pause()\n\n    If you require a different range than -1 to +1, see :func:`scaled`.\n    \"\"\"\n    angles = (2 * pi * i / period for i in range(period))\n    for a in cycle(angles):\n        yield cos(a)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nprovide an infinite source of values representing a triangle wave (from 0 to 1 and back again) which repeats every *period* values. For example, to pulse an LED once a second:: from gpiozero import PWMLED from gpiozero.tools import ramping_values from signal import pause red = PWMLED(2) red.source_delay = 0.01 red.source = ramping_values(100) pause() If you require a wider range than 0 to 1, see :func:`scaled`.", "response": "def ramping_values(period=360):\n    \"\"\"\n    Provides an infinite source of values representing a triangle wave (from 0\n    to 1 and back again) which repeats every *period* values. For example, to\n    pulse an LED once a second::\n\n        from gpiozero import PWMLED\n        from gpiozero.tools import ramping_values\n        from signal import pause\n\n        red = PWMLED(2)\n\n        red.source_delay = 0.01\n        red.source = ramping_values(100)\n\n        pause()\n\n    If you require a wider range than 0 to 1, see :func:`scaled`.\n    \"\"\"\n    step = 2 / period\n    value = 0\n    while True:\n        yield value\n        value += step\n        if isclose(value, 1, abs_tol=1e-9):\n            value = 1\n            step *= -1\n        elif isclose(value, 0, abs_tol=1e-9):\n            value = 0\n            step *= -1\n        elif value > 1 or value < 0:\n            step *= -1\n            value += step"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nturn the state of the user s in - memory device.", "response": "def toggle(self):\n        \"\"\"\n        Reverse the state of the device. If it's on, turn it off; if it's off,\n        turn it on.\n        \"\"\"\n        with self._lock:\n            if self.is_active:\n                self.off()\n            else:\n                self.on()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef blink(\n            self, on_time=1, off_time=1, fade_in_time=0, fade_out_time=0,\n            n=None, background=True):\n        \"\"\"\n        Make the device turn on and off repeatedly.\n\n        :param float on_time:\n            Number of seconds on. Defaults to 1 second.\n\n        :param float off_time:\n            Number of seconds off. Defaults to 1 second.\n\n        :param float fade_in_time:\n            Number of seconds to spend fading in. Defaults to 0.\n\n        :param float fade_out_time:\n            Number of seconds to spend fading out. Defaults to 0.\n\n        :type n: int or None\n        :param n:\n            Number of times to blink; :data:`None` (the default) means forever.\n\n        :param bool background:\n            If :data:`True` (the default), start a background thread to\n            continue blinking and return immediately. If :data:`False`, only\n            return when the blink is finished (warning: the default value of\n            *n* will result in this method never returning).\n        \"\"\"\n        self._stop_blink()\n        self._blink_thread = GPIOThread(\n            target=self._blink_device,\n            args=(on_time, off_time, fade_in_time, fade_out_time, n)\n        )\n        self._blink_thread.start()\n        if not background:\n            self._blink_thread.join()\n            self._blink_thread = None", "response": "Blink the specified number of times."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef play(self, tone):\n        if tone is None:\n            self.value = None\n        else:\n            if not isinstance(tone, Tone):\n                tone = Tone(tone)\n            freq = tone.frequency\n            if self.min_tone.frequency <= tone <= self.max_tone.frequency:\n                self.pwm_device.pin.frequency = freq\n                self.pwm_device.value = 0.5\n            else:\n                raise ValueError(\"tone is out of the device's range\")", "response": "Play the given tone."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef tone(self):\n        if self.pwm_device.pin.frequency is None:\n            return None\n        else:\n            return Tone.from_frequency(self.pwm_device.pin.frequency)", "response": "Returns the current tone of the current buzzer."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef value(self):\n        if self.pwm_device.pin.frequency is None:\n            return None\n        else:\n            try:\n                return log2(\n                    self.pwm_device.pin.frequency / self.mid_tone.frequency\n                ) / self.octaves\n            except ZeroDivisionError:\n                return 0.0", "response": "Returns the value of the buzzer as a value between - 1 and 1."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntoggle the state of the device.", "response": "def toggle(self):\n        \"\"\"\n        Toggle the state of the device. If the device is currently off\n        (:attr:`value` is ``(0, 0, 0)``), this changes it to \"fully\" on\n        (:attr:`value` is ``(1, 1, 1)``).  If the device has a specific color,\n        this method inverts the color.\n        \"\"\"\n        r, g, b = self.value\n        self.value = (1 - r, 1 - g, 1 - b)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nblink the specified object to the specified color.", "response": "def blink(\n            self, on_time=1, off_time=1, fade_in_time=0, fade_out_time=0,\n            on_color=(1, 1, 1), off_color=(0, 0, 0), n=None, background=True):\n        \"\"\"\n        Make the device turn on and off repeatedly.\n\n        :param float on_time:\n            Number of seconds on. Defaults to 1 second.\n\n        :param float off_time:\n            Number of seconds off. Defaults to 1 second.\n\n        :param float fade_in_time:\n            Number of seconds to spend fading in. Defaults to 0. Must be 0 if\n            *pwm* was :data:`False` when the class was constructed\n            (:exc:`ValueError` will be raised if not).\n\n        :param float fade_out_time:\n            Number of seconds to spend fading out. Defaults to 0. Must be 0 if\n            *pwm* was :data:`False` when the class was constructed\n            (:exc:`ValueError` will be raised if not).\n\n        :type on_color: ~colorzero.Color or tuple\n        :param on_color:\n            The color to use when the LED is \"on\". Defaults to white.\n\n        :type off_color: ~colorzero.Color or tuple\n        :param off_color:\n            The color to use when the LED is \"off\". Defaults to black.\n\n        :type n: int or None\n        :param n:\n            Number of times to blink; :data:`None` (the default) means forever.\n\n        :param bool background:\n            If :data:`True` (the default), start a background thread to\n            continue blinking and return immediately. If :data:`False`, only\n            return when the blink is finished (warning: the default value of\n            *n* will result in this method never returning).\n        \"\"\"\n        if isinstance(self._leds[0], LED):\n            if fade_in_time:\n                raise ValueError('fade_in_time must be 0 with non-PWM RGBLEDs')\n            if fade_out_time:\n                raise ValueError('fade_out_time must be 0 with non-PWM RGBLEDs')\n        self._stop_blink()\n        self._blink_thread = GPIOThread(\n            target=self._blink_device,\n            args=(\n                on_time, off_time, fade_in_time, fade_out_time,\n                on_color, off_color, n\n            )\n        )\n        self._blink_thread.start()\n        if not background:\n            self._blink_thread.join()\n            self._blink_thread = None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndrive the motor forwards.", "response": "def forward(self, speed=1):\n        \"\"\"\n        Drive the motor forwards.\n\n        :param float speed:\n            The speed at which the motor should turn. Can be any value between\n            0 (stopped) and the default 1 (maximum speed) if *pwm* was\n            :data:`True` when the class was constructed (and only 0 or 1 if\n            not).\n        \"\"\"\n        if not 0 <= speed <= 1:\n            raise ValueError('forward speed must be between 0 and 1')\n        if isinstance(self.forward_device, DigitalOutputDevice):\n            if speed not in (0, 1):\n                raise ValueError(\n                    'forward speed must be 0 or 1 with non-PWM Motors')\n        self.backward_device.off()\n        self.forward_device.value = speed"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef forward(self, speed=1):\n        if isinstance(self.enable_device, DigitalOutputDevice):\n            if speed not in (0, 1):\n                raise ValueError(\n                    'forward speed must be 0 or 1 with non-PWM Motors')\n        self.enable_device.off()\n        self.phase_device.off()\n        self.enable_device.value = speed", "response": "Drive the motor forwards."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef backward(self, speed=1):\n        if isinstance(self.enable_device, DigitalOutputDevice):\n            if speed not in (0, 1):\n                raise ValueError(\n                    'backward speed must be 0 or 1 with non-PWM Motors')\n        self.enable_device.off()\n        self.phase_device.on()\n        self.enable_device.value = speed", "response": "Drive the motor backwards."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef pulse_width(self):\n        if self.pwm_device.pin.frequency is None:\n            return None\n        else:\n            return self.pwm_device.pin.state * self.frame_width", "response": "Returns the pulse width controlling the servo."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the angle of the servo as an angle measured in degrees.", "response": "def angle(self):\n        \"\"\"\n        The position of the servo as an angle measured in degrees. This will\n        only be accurate if :attr:`min_angle` and :attr:`max_angle` have been\n        set appropriately in the constructor.\n\n        This can also be the special value :data:`None` indicating that the\n        servo is currently \"uncontrolled\", i.e. that no control signal is being\n        sent.  Typically this means the servo's position remains unchanged, but\n        that it can be moved by hand.\n        \"\"\"\n        result = self._get_value()\n        if result is None:\n            return None\n        else:\n            # NOTE: Why round(n, 12) here instead of 14? Angle ranges can be\n            # much larger than -1..1 so we need a little more rounding to\n            # smooth off the rough corners!\n            return round(\n                self._angular_range *\n                ((result - self._min_value) / self._value_range) +\n                self._min_angle, 12)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nconstruct a Tone from a MIDI note.", "response": "def from_midi(cls, midi_note):\n        \"\"\"\n        Construct a :class:`Tone` from a MIDI note, which must be an integer\n        in the range 0 to 127. For reference, A4 (`concert A`_ typically used\n        for tuning) is MIDI note #69.\n\n        .. _concert A: https://en.wikipedia.org/wiki/Concert_pitch\n        \"\"\"\n        midi = int(midi_note)\n        if 0 <= midi_note < 128:\n            A4_midi = 69\n            A4_freq = 440\n            return cls.from_frequency(A4_freq * 2 ** ((midi - A4_midi) / 12))\n        raise ValueError('invalid MIDI note: %r' % midi)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct a Tone object from a musical note. Note is a string representing a MIDI - formatted MIDI tuning note.", "response": "def from_note(cls, note):\n        \"\"\"\n        Construct a :class:`Tone` from a musical note which must consist of\n        a capital letter A through G, followed by an optional semi-tone\n        modifier (\"b\" for flat, \"#\" for sharp, or their Unicode equivalents),\n        followed by an octave number (0 through 9).\n\n        For example `concert A`_, the typical tuning note at 440Hz, would be\n        represented as \"A4\". One semi-tone above this would be \"A#4\" or\n        alternatively \"Bb4\". Unicode representations of sharp and flat are also\n        accepted.\n        \"\"\"\n        if isinstance(note, bytes):\n            note = note.decode('ascii')\n        if isinstance(note, str):\n            match = Tone.regex.match(note)\n            if match:\n                octave = int(match.group('octave')) + 1\n                return cls.from_midi(\n                    Tone.tones.index(match.group('note')) +\n                    Tone.semitones[match.group('semi')] +\n                    octave * 12)\n        raise ValueError('invalid note specification: %r' % note)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef from_frequency(cls, freq):\n        if 0 < freq <= 20000:\n            return super(Tone, cls).__new__(cls, freq)\n        raise ValueError('invalid frequency: %.2f' % freq)", "response": "Construct a Tone object from a frequency specified inHz_."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef midi(self):\n        result = int(round(12 * log2(self.frequency / 440) + 69))\n        if 0 <= result < 128:\n            return result\n        raise ValueError('%f is outside the MIDI note range' % self.frequency)", "response": "Return the MIDI note that corresponds to this tone s frequency."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the nearest note to the tone s frequency.", "response": "def note(self):\n        \"\"\"\n        Return the (nearest) note to the tone's frequency. This will be a\n        string in the form accepted by :meth:`from_note`. If the frequency is\n        outside the range represented by this format (\"A0\" is approximately\n        27.5Hz, and \"G9\" is approximately 12.5Khz) a :exc:`ValueError`\n        exception will be raised.\n        \"\"\"\n        offset = self.midi - 60  # self.midi - A4_midi + Tone.tones.index('A')\n        index = offset % 12      # offset % len(Tone.tones)\n        octave = 4 + offset // 12\n        if 0 <= octave <= 9:\n            return (\n                Tone.tones[index] +\n                ('#' if Tone.tones[index] == Tone.tones[index - 1] else '') +\n                str(octave)\n            )\n        raise ValueError('%f is outside the notation range' % self.frequency)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the length of time that the device has been active for.", "response": "def active_time(self):\n        \"\"\"\n        The length of time (in seconds) that the device has been active for.\n        When the device is inactive, this is :data:`None`.\n        \"\"\"\n        if self._active_event.is_set():\n            return self.pin_factory.ticks_diff(self.pin_factory.ticks(),\n                                               self._last_changed)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the time that the device has been inactive for.", "response": "def inactive_time(self):\n        \"\"\"\n        The length of time (in seconds) that the device has been inactive for.\n        When the device is active, this is :data:`None`.\n        \"\"\"\n        if self._inactive_event.is_set():\n            return self.pin_factory.ticks_diff(self.pin_factory.ticks(),\n                                               self._last_changed)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the length of time that the device has been held for.", "response": "def held_time(self):\n        \"\"\"\n        The length of time (in seconds) that the device has been held for.\n        This is counted from the first execution of the :attr:`when_held` event\n        rather than when the device activated, in contrast to\n        :attr:`~EventsMixin.active_time`. If the device is not currently held,\n        this is :data:`None`.\n        \"\"\"\n        if self._held_from is not None:\n            return self.pin_factory.ticks_diff(self.pin_factory.ticks(),\n                                               self._held_from)\n        else:\n            return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _call_when_changed(self, ticks=None, state=None):\n        super(LocalPiPin, self)._call_when_changed(\n            self._factory.ticks() if ticks is None else ticks,\n            self.state if state is None else state)", "response": "Override to provide default ticks from the local Pi factory."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nsets a default proxy for all socksocket objects.", "response": "def set_default_proxy(proxy_type=None, addr=None, port=None, rdns=True, username=None, password=None):\n    \"\"\"\n    set_default_proxy(proxy_type, addr[, port[, rdns[, username, password]]])\n\n    Sets a default proxy which all further socksocket objects will use,\n    unless explicitly changed. All parameters are as for socket.set_proxy().\n    \"\"\"\n    socksocket.default_proxy = (proxy_type, addr, port, rdns,\n                                username.encode() if username else None,\n                                password.encode() if password else None)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a new connection to a SOCKS proxy.", "response": "def create_connection(dest_pair, proxy_type=None, proxy_addr=None,\n                      proxy_port=None, proxy_rdns=True,\n                      proxy_username=None, proxy_password=None,\n                      timeout=None, source_address=None,\n                      socket_options=None):\n    \"\"\"create_connection(dest_pair, *[, timeout], **proxy_args) -> socket object\n\n    Like socket.create_connection(), but connects to proxy\n    before returning the socket object.\n\n    dest_pair - 2-tuple of (IP/hostname, port).\n    **proxy_args - Same args passed to socksocket.set_proxy() if present.\n    timeout - Optional socket timeout value, in seconds.\n    source_address - tuple (host, port) for the socket to bind to as its source\n    address before connecting (only for compatibility)\n    \"\"\"\n    # Remove IPv6 brackets on the remote address and proxy address.\n    remote_host, remote_port = dest_pair\n    if remote_host.startswith('['):\n        remote_host = remote_host.strip('[]')\n    if proxy_addr and proxy_addr.startswith('['):\n        proxy_addr = proxy_addr.strip('[]')\n\n    err = None\n\n    # Allow the SOCKS proxy to be on IPv4 or IPv6 addresses.\n    for r in socket.getaddrinfo(proxy_addr, proxy_port, 0, socket.SOCK_STREAM):\n        family, socket_type, proto, canonname, sa = r\n        sock = None\n        try:\n            sock = socksocket(family, socket_type, proto)\n\n            if socket_options:\n                for opt in socket_options:\n                    sock.setsockopt(*opt)\n\n            if isinstance(timeout, (int, float)):\n                sock.settimeout(timeout)\n\n            if proxy_type:\n                sock.set_proxy(proxy_type, proxy_addr, proxy_port, proxy_rdns,\n                               proxy_username, proxy_password)\n            if source_address:\n                sock.bind(source_address)\n\n            sock.connect((remote_host, remote_port))\n            return sock\n\n        except (socket.error, ProxyConnectionError) as e:\n            err = e\n            if sock:\n                sock.close()\n                sock = None\n\n    if err:\n        raise err\n\n    raise socket.error(\"gai returned empty list.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreads all the bytes from the file object.", "response": "def _readall(self, file, count):\n        \"\"\"\n        Receive EXACTLY the number of bytes requested from the file object.\n        Blocks until the required number of bytes have been received.\n        \"\"\"\n        data = b\"\"\n        while len(data) < count:\n            d = file.read(count - len(data))\n            if not d:\n                raise GeneralProxyError(\"Connection closed unexpectedly\")\n            data += d\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_proxy(self, proxy_type=None, addr=None, port=None, rdns=True, username=None, password=None):\n        self.proxy = (proxy_type, addr, port, rdns,\n                      username.encode() if username else None,\n                      password.encode() if password else None)", "response": "Sets the proxy to be used for the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nbinding to a local address.", "response": "def bind(self, *pos, **kw):\n        \"\"\"\n        Implements proxy connection for UDP sockets,\n        which happens during the bind() phase.\n        \"\"\"\n        proxy_type, proxy_addr, proxy_port, rdns, username, password = self.proxy\n        if not proxy_type or self.type != socket.SOCK_DGRAM:\n            return _orig_socket.bind(self, *pos, **kw)\n\n        if self._proxyconn:\n            raise socket.error(EINVAL, \"Socket already bound to an address\")\n        if proxy_type != SOCKS5:\n            msg = \"UDP only supported by SOCKS5 proxy type\"\n            raise socket.error(EOPNOTSUPP, msg)\n        super(socksocket, self).bind(*pos, **kw)\n\n        # Need to specify actual local port because\n        # some relays drop packets if a port of zero is specified.\n        # Avoid specifying host address in case of NAT though.\n        _, port = self.getsockname()\n        dst = (\"0\", port)\n\n        self._proxyconn = _orig_socket()\n        proxy = self._proxy_addr()\n        self._proxyconn.connect(proxy)\n\n        UDP_ASSOCIATE = b\"\\x03\"\n        _, relay = self._SOCKS5_request(self._proxyconn, UDP_ASSOCIATE, dst)\n\n        # The relay is most likely on the same host as the SOCKS proxy,\n        # but some proxies return a private IP address (10.x.y.z)\n        host, _ = proxy\n        _, port = relay\n        super(socksocket, self).connect((host, port))\n        super(socksocket, self).settimeout(self._timeout)\n        self.proxy_sockname = (\"0.0.0.0\", 0)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _negotiate_SOCKS5(self, *dest_addr):\n        CONNECT = b\"\\x01\"\n        self.proxy_peername, self.proxy_sockname = self._SOCKS5_request(self,\n            CONNECT, dest_addr)", "response": "Negotiates a SOCKS5 connection through a SOCKS5 server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _SOCKS5_request(self, conn, cmd, dst):\n        proxy_type, addr, port, rdns, username, password = self.proxy\n\n        writer = conn.makefile(\"wb\")\n        reader = conn.makefile(\"rb\", 0)  # buffering=0 renamed in Python 3\n        try:\n            # First we'll send the authentication packages we support.\n            if username and password:\n                # The username/password details were supplied to the\n                # set_proxy method so we support the USERNAME/PASSWORD\n                # authentication (in addition to the standard none).\n                writer.write(b\"\\x05\\x02\\x00\\x02\")\n            else:\n                # No username/password were entered, therefore we\n                # only support connections with no authentication.\n                writer.write(b\"\\x05\\x01\\x00\")\n\n            # We'll receive the server's response to determine which\n            # method was selected\n            writer.flush()\n            chosen_auth = self._readall(reader, 2)\n\n            if chosen_auth[0:1] != b\"\\x05\":\n                # Note: string[i:i+1] is used because indexing of a bytestring\n                # via bytestring[i] yields an integer in Python 3\n                raise GeneralProxyError(\"SOCKS5 proxy server sent invalid data\")\n\n            # Check the chosen authentication method\n\n            if chosen_auth[1:2] == b\"\\x02\":\n                # Okay, we need to perform a basic username/password\n                # authentication.\n                writer.write(b\"\\x01\" + chr(len(username)).encode()\n                             + username\n                             + chr(len(password)).encode()\n                             + password)\n                writer.flush()\n                auth_status = self._readall(reader, 2)\n                if auth_status[0:1] != b\"\\x01\":\n                    # Bad response\n                    raise GeneralProxyError(\"SOCKS5 proxy server sent invalid data\")\n                if auth_status[1:2] != b\"\\x00\":\n                    # Authentication failed\n                    raise SOCKS5AuthError(\"SOCKS5 authentication failed\")\n\n                # Otherwise, authentication succeeded\n\n            # No authentication is required if 0x00\n            elif chosen_auth[1:2] != b\"\\x00\":\n                # Reaching here is always bad\n                if chosen_auth[1:2] == b\"\\xFF\":\n                    raise SOCKS5AuthError(\"All offered SOCKS5 authentication methods were rejected\")\n                else:\n                    raise GeneralProxyError(\"SOCKS5 proxy server sent invalid data\")\n\n            # Now we can request the actual connection\n            writer.write(b\"\\x05\" + cmd + b\"\\x00\")\n            resolved = self._write_SOCKS5_address(dst, writer)\n            writer.flush()\n\n            # Get the response\n            resp = self._readall(reader, 3)\n            if resp[0:1] != b\"\\x05\":\n                raise GeneralProxyError(\"SOCKS5 proxy server sent invalid data\")\n\n            status = ord(resp[1:2])\n            if status != 0x00:\n                # Connection failed: server returned an error\n                error = SOCKS5_ERRORS.get(status, \"Unknown error\")\n                raise SOCKS5Error(\"{0:#04x}: {1}\".format(status, error))\n\n            # Get the bound address/port\n            bnd = self._read_SOCKS5_address(reader)\n\n            super(socksocket, self).settimeout(self._timeout)\n            return (resolved, bnd)\n        finally:\n            reader.close()\n            writer.close()", "response": "Send SOCKS5 request to the specified SOCKS5 server."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _write_SOCKS5_address(self, addr, file):\n        host, port = addr\n        proxy_type, _, _, rdns, username, password = self.proxy\n        family_to_byte = {socket.AF_INET: b\"\\x01\", socket.AF_INET6: b\"\\x04\"}\n\n        # If the given destination address is an IP address, we'll\n        # use the IP address request even if remote resolving was specified.\n        # Detect whether the address is IPv4/6 directly.\n        for family in (socket.AF_INET, socket.AF_INET6):\n            try:\n                addr_bytes = socket.inet_pton(family, host)\n                file.write(family_to_byte[family] + addr_bytes)\n                host = socket.inet_ntop(family, addr_bytes)\n                file.write(struct.pack(\">H\", port))\n                return host, port\n            except socket.error:\n                continue\n\n        # Well it's not an IP number, so it's probably a DNS name.\n        if rdns:\n            # Resolve remotely\n            host_bytes = host.encode('idna')\n            file.write(b\"\\x03\" + chr(len(host_bytes)).encode() + host_bytes)\n        else:\n            # Resolve locally\n            addresses = socket.getaddrinfo(host, port, socket.AF_UNSPEC, socket.SOCK_STREAM, socket.IPPROTO_TCP, socket.AI_ADDRCONFIG)\n            # We can't really work out what IP is reachable, so just pick the\n            # first.\n            target_addr = addresses[0]\n            family = target_addr[0]\n            host = target_addr[4][0]\n\n            addr_bytes = socket.inet_pton(family, host)\n            file.write(family_to_byte[family] + addr_bytes)\n            host = socket.inet_ntop(family, addr_bytes)\n        file.write(struct.pack(\">H\", port))\n        return host, port", "response": "Write the SOCKS5 address to file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _negotiate_SOCKS4(self, dest_addr, dest_port):\n        proxy_type, addr, port, rdns, username, password = self.proxy\n\n        writer = self.makefile(\"wb\")\n        reader = self.makefile(\"rb\", 0)  # buffering=0 renamed in Python 3\n        try:\n            # Check if the destination address provided is an IP address\n            remote_resolve = False\n            try:\n                addr_bytes = socket.inet_aton(dest_addr)\n            except socket.error:\n                # It's a DNS name. Check where it should be resolved.\n                if rdns:\n                    addr_bytes = b\"\\x00\\x00\\x00\\x01\"\n                    remote_resolve = True\n                else:\n                    addr_bytes = socket.inet_aton(socket.gethostbyname(dest_addr))\n\n            # Construct the request packet\n            writer.write(struct.pack(\">BBH\", 0x04, 0x01, dest_port))\n            writer.write(addr_bytes)\n\n            # The username parameter is considered userid for SOCKS4\n            if username:\n                writer.write(username)\n            writer.write(b\"\\x00\")\n\n            # DNS name if remote resolving is required\n            # NOTE: This is actually an extension to the SOCKS4 protocol\n            # called SOCKS4A and may not be supported in all cases.\n            if remote_resolve:\n                writer.write(dest_addr.encode('idna') + b\"\\x00\")\n            writer.flush()\n\n            # Get the response from the server\n            resp = self._readall(reader, 8)\n            if resp[0:1] != b\"\\x00\":\n                # Bad data\n                raise GeneralProxyError(\"SOCKS4 proxy server sent invalid data\")\n\n            status = ord(resp[1:2])\n            if status != 0x5A:\n                # Connection failed: server returned an error\n                error = SOCKS4_ERRORS.get(status, \"Unknown error\")\n                raise SOCKS4Error(\"{0:#04x}: {1}\".format(status, error))\n\n            # Get the bound address/port\n            self.proxy_sockname = (socket.inet_ntoa(resp[4:]), struct.unpack(\">H\", resp[2:4])[0])\n            if remote_resolve:\n                self.proxy_peername = socket.inet_ntoa(addr_bytes), dest_port\n            else:\n                self.proxy_peername = dest_addr, dest_port\n        finally:\n            reader.close()\n            writer.close()", "response": "Negotiates a SOCKS4 connection through a SOCKS4 server."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _negotiate_HTTP(self, dest_addr, dest_port):\n        proxy_type, addr, port, rdns, username, password = self.proxy\n\n        # If we need to resolve locally, we do this now\n        addr = dest_addr if rdns else socket.gethostbyname(dest_addr)\n\n        http_headers = [\n            b\"CONNECT \" + addr.encode('idna') + b\":\" + str(dest_port).encode() + b\" HTTP/1.1\",\n            b\"Host: \" + dest_addr.encode('idna')\n        ]\n\n        if username and password:\n            http_headers.append(b\"Proxy-Authorization: basic \" + b64encode(username + b\":\" + password))\n\n        http_headers.append(b\"\\r\\n\")\n\n        self.sendall(b\"\\r\\n\".join(http_headers))\n\n        # We just need the first line to check if the connection was successful\n        fobj = self.makefile()\n        status_line = fobj.readline()\n        fobj.close()\n\n        if not status_line:\n            raise GeneralProxyError(\"Connection closed unexpectedly\")\n\n        try:\n            proto, status_code, status_msg = status_line.split(\" \", 2)\n        except ValueError:\n            raise GeneralProxyError(\"HTTP proxy server sent invalid response\")\n\n        if not proto.startswith(\"HTTP/\"):\n            raise GeneralProxyError(\"Proxy server does not appear to be an HTTP proxy\")\n\n        try:\n            status_code = int(status_code)\n        except ValueError:\n            raise HTTPError(\"HTTP proxy server did not return a valid HTTP status\")\n\n        if status_code != 200:\n            error = \"{0}: {1}\".format(status_code, status_msg)\n            if status_code in (400, 403, 405):\n                # It's likely that the HTTP proxy server does not support the CONNECT tunneling method\n                error += (\"\\n[*] Note: The HTTP proxy server may not be supported by PySocks\"\n                          \" (must be a CONNECT tunnel proxy)\")\n            raise HTTPError(error)\n\n        self.proxy_sockname = (b\"0.0.0.0\", 0)\n        self.proxy_peername = addr, dest_port", "response": "Negotiates a connection through an HTTP proxy server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef connect(self, dest_pair):\n        if len(dest_pair) != 2 or dest_pair[0].startswith(\"[\"):\n            # Probably IPv6, not supported -- raise an error, and hope\n            # Happy Eyeballs (RFC6555) makes sure at least the IPv4\n            # connection works...\n            raise socket.error(\"PySocks doesn't support IPv6: %s\" % str(dest_pair))\n\n        dest_addr, dest_port = dest_pair\n\n        if self.type == socket.SOCK_DGRAM:\n            if not self._proxyconn:\n                self.bind((\"\", 0))\n            dest_addr = socket.gethostbyname(dest_addr)\n\n            # If the host address is INADDR_ANY or similar, reset the peer\n            # address so that packets are received from any peer\n            if dest_addr == \"0.0.0.0\" and not dest_port:\n                self.proxy_peername = None\n            else:\n                self.proxy_peername = (dest_addr, dest_port)\n            return\n\n        proxy_type, proxy_addr, proxy_port, rdns, username, password = self.proxy\n\n        # Do a minimal input check first\n        if (not isinstance(dest_pair, (list, tuple))\n                or len(dest_pair) != 2\n                or not dest_addr\n                or not isinstance(dest_port, int)):\n            raise GeneralProxyError(\"Invalid destination-connection (host, port) pair\")\n\n\n        # We set the timeout here so that we don't hang in connection or during\n        # negotiation.\n        super(socksocket, self).settimeout(self._timeout)\n\n        if proxy_type is None:\n            # Treat like regular socket object\n            self.proxy_peername = dest_pair\n            super(socksocket, self).settimeout(self._timeout)\n            super(socksocket, self).connect((dest_addr, dest_port))\n            return\n\n        proxy_addr = self._proxy_addr()\n\n        try:\n            # Initial connection to proxy server.\n            super(socksocket, self).connect(proxy_addr)\n\n        except socket.error as error:\n            # Error while connecting to proxy\n            self.close()\n            proxy_addr, proxy_port = proxy_addr\n            proxy_server = \"{0}:{1}\".format(proxy_addr, proxy_port)\n            printable_type = PRINTABLE_PROXY_TYPES[proxy_type]\n\n            msg = \"Error connecting to {0} proxy {1}\".format(printable_type,\n                                                           proxy_server)\n            log.debug(\"%s due to: %s\", msg, error)\n            raise ProxyConnectionError(msg, error)\n\n        else:\n            # Connected to proxy server, now negotiate\n            try:\n                # Calls negotiate_{SOCKS4, SOCKS5, HTTP}\n                negotiate = self._proxy_negotiators[proxy_type]\n                negotiate(self, dest_addr, dest_port)\n            except socket.error as error:\n                # Wrap socket errors\n                self.close()\n                raise GeneralProxyError(\"Socket error\", error)\n            except ProxyError:\n                # Protocol error while negotiating with proxy\n                self.close()\n                raise", "response": "Connects to the specified destination through a proxy."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _proxy_addr(self):\n        proxy_type, proxy_addr, proxy_port, rdns, username, password = self.proxy\n        proxy_port = proxy_port or DEFAULT_PORTS.get(proxy_type)\n        if not proxy_port:\n            raise GeneralProxyError(\"Invalid proxy type\")\n        return proxy_addr, proxy_port", "response": "Return proxy address to connect to as tuple object\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the PSL source.", "response": "def _parse(self, source, accept_encoded_idn, only_icann=False):\n        \"\"\" PSL parser core \"\"\"\n\n        publicsuffix = set()\n        maxlabel = 0\n        section_is_icann = None\n\n        if isinstance(source, decodablestr):\n            source = source.splitlines()\n\n        ln = 0\n        for line in source:\n            ln += 1\n            if only_icann:\n                ul = u(line).rstrip()\n                if ul == \"// ===BEGIN ICANN DOMAINS===\":\n                    section_is_icann = True\n                    continue\n                elif ul == \"// ===END ICANN DOMAINS===\":\n                    section_is_icann = False\n                    continue\n                if not section_is_icann:\n                    continue\n\n            s = u(line).lower().split(\" \")[0].rstrip()\n            if s == \"\" or s.startswith(\"//\"):\n                continue\n\n            maxlabel = max(maxlabel, s.count(\".\") + 1)\n            publicsuffix.add(s)\n            if accept_encoded_idn:\n                e = encode_idn(s.lstrip(\"!\"))\n                if s[0] == \"!\":\n                    publicsuffix.add(\"!\" + e)\n                else:\n                    publicsuffix.add(e)\n\n        self._publicsuffix = frozenset(publicsuffix)\n        self._maxlabel = maxlabel"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the shortest suffix assigned for an individual.", "response": "def privatesuffix(self, domain, accept_unknown=None):\n        \"\"\" Return shortest suffix assigned for an individual.\n\n        domain: str or unicode to parse. (Required)\n        accept_unknown: bool, assume unknown TLDs to be public suffix. (Default: object default)\n\n        Return None if domain has invalid format.\n        Return None if domain has no private part.\n        \"\"\"\n\n        if accept_unknown is None:\n            accept_unknown = self.accept_unknown\n\n        if not isinstance(domain, basestr):\n            raise TypeError()\n\n        labels = domain.lower().rsplit(\".\", self._maxlabel + 2)\n        ll = len(labels)\n\n        if \"\\0\" in domain or \"\" in labels:\n            # not a valid domain\n            return None\n\n        if ll <= 1:\n            # is TLD\n            return None\n\n        # skip labels longer than rules\n        for i in range(max(0, ll - self._maxlabel), ll):\n            s = \".\".join(labels[i:])\n\n            if i > 0 and (\"!*.\" + s) in self._publicsuffix:\n                return \".\".join(labels[i-1:])\n\n            if (\"!\" + s) in self._publicsuffix:\n                # exact private match\n                return s\n\n            if i > 0 and (\"*.\" + s) in self._publicsuffix:\n                if i <= 1:\n                    # domain is publicsuffix\n                    return None\n                else:\n                    return \".\".join(labels[i-2:])\n\n            if s in self._publicsuffix:\n                if i > 0:\n                    return \".\".join(labels[i-1:])\n                else:\n                    # domain is publicsuffix\n                    return None\n\n        else:\n            # no match found\n            if self.accept_unknown and ll >= 2:\n                return \".\".join(labels[-2:])\n            else:\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef privateparts(self, domain):\n        s = self.privatesuffix(domain)\n        if s is None:\n            return None\n        else:\n            # I know the domain is valid and ends with private suffix\n            pre = domain[0:-(len(s)+1)]\n            if pre == \"\":\n                return (s,)\n            else:\n                return tuple(pre.split(\".\") + [s])", "response": "Return tuple of labels and private suffix."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns so - called subdomain of specified depth in the private suffix.", "response": "def subdomain(self, domain, depth):\n        \"\"\" Return so-called subdomain of specified depth in the private suffix. \"\"\"\n        p = self.privateparts(domain)\n        if p is None or depth > len(p) - 1:\n            return None\n        else:\n            return \".\".join(p[-(depth+1):])"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef updatePSL(psl_file=PSLFILE):\n    if requests is None:\n        raise Exception(\"Please install python-requests http(s) library. $ sudo pip install requests\")\n\n\n    r = requests.get(PSLURL)\n    if r.status_code != requests.codes.ok or len(r.content) == 0:\n        raise Exception(\"Could not download PSL from \" + PSLURL)\n\n    lastmod = r.headers.get(\"last-modified\", None)\n    f = open(psl_file + \".swp\", \"wb\")\n    f.write(r.content)\n    f.close()\n\n    with open(psl_file + \".swp\", \"rb\") as f:\n        psl = PublicSuffixList(f)\n\n    os.rename(psl_file + \".swp\", psl_file)\n    if lastmod:\n        t = time.mktime(parsedate(lastmod))\n        os.utime(psl_file, (t, t))\n\n    print(\"PSL updated\")\n    if lastmod:\n        print(\"last-modified: \" + lastmod)", "response": "Updates a local copy of the PSL file with the list of public suffixes."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ssl_context(private_key, certificate):\n    if (\n        certificate\n        and os.path.isfile(certificate)\n        and private_key\n        and os.path.isfile(private_key)\n    ):\n        context = ssl.SSLContext(ssl.PROTOCOL_TLSv1_2)\n        context.load_cert_chain(certificate, private_key)\n        return context\n    return None", "response": "Get ssl context from private key and certificate paths."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef init_app(self, app):\n        app.config.setdefault(\"SSLIFY_SUBDOMAINS\", False)\n        app.config.setdefault(\"SSLIFY_PERMANENT\", False)\n        app.config.setdefault(\"SSLIFY_SKIPS\", None)\n\n        self.hsts_include_subdomains = (\n            self.hsts_include_subdomains or app.config[\"SSLIFY_SUBDOMAINS\"]\n        )\n        self.permanent = self.permanent or self.app.config[\"SSLIFY_PERMANENT\"]\n        self.skip_list = self.skip_list or self.app.config[\"SSLIFY_SKIPS\"]\n\n        app.before_request(self.redirect_to_ssl)\n        app.after_request(self.set_hsts_header)", "response": "Configures the specified Flask app to enforce SSL."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef set_hsts_header(self, response):\n        # Should we add STS header?\n        if request.is_secure and not self.skip:\n            response.headers.setdefault(\"Strict-Transport-Security\", self.hsts_header)\n        return response", "response": "Adds HSTS header to each response."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef csrf_protect_all_post_and_cross_origin_requests():\n    success = None\n\n    if is_cross_origin(request):\n        logger.warning(\"Received cross origin request. Aborting\")\n        abort(403)\n    if request.method in [\"POST\", \"PUT\"]:\n        token = session.get(\"csrf_token\")\n        if token == request.form.get(\"csrf_token\"):\n            return success\n\n        elif token == request.environ.get(\"HTTP_X_CSRFTOKEN\"):\n            return success\n\n        else:\n            logger.warning(\"Received invalid csrf token. Aborting\")\n            abort(403)", "response": "protects all POST PUT and HTTP_X_CSRFTOKEN requests"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_cross_origin(request):\n    origin = request.environ.get(\"HTTP_ORIGIN\")\n    host = request.environ.get(\"HTTP_HOST\")\n    if origin is None:\n        # origin is sometimes omitted by the browser when origin and host are equal\n        return False\n\n    if origin.startswith(\"http://\"):\n        origin = origin.replace(\"http://\", \"\")\n    elif origin.startswith(\"https://\"):\n        origin = origin.replace(\"https://\", \"\")\n    return host != origin", "response": "Compare headers HOST and ORIGIN and return True if they are not equal"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef csrf_protect(f):\n\n    @wraps(f)\n    def wrapper(*args, **kwargs):\n        token = session.get(\"csrf_token\", None)\n        if token is None or token != request.environ.get(\"HTTP_X_CSRFTOKEN\"):\n            logger.warning(\"Received invalid csrf token. Aborting\")\n            abort(403)\n        # call original request handler\n        return f(*args, **kwargs)\n\n    return wrapper", "response": "A decorator to add csrf protection by validating the X_CSRFTOKEN csrf_token in request header"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nrunning the server of the gdb gui", "response": "def setup_backend(\n    serve=True,\n    host=DEFAULT_HOST,\n    port=DEFAULT_PORT,\n    debug=False,\n    open_browser=True,\n    browsername=None,\n    testing=False,\n    private_key=None,\n    certificate=None,\n    LLDB=False,\n):\n    \"\"\"Run the server of the gdb gui\"\"\"\n    app.config[\"LLDB\"] = LLDB\n\n    kwargs = {}\n    ssl_context = get_ssl_context(private_key, certificate)\n    if ssl_context:\n        # got valid ssl context\n        # force everything through https\n        SSLify(app)\n        # pass ssl_context to flask\n        kwargs[\"ssl_context\"] = ssl_context\n\n    url = \"%s:%s\" % (host, port)\n    if kwargs.get(\"ssl_context\"):\n        protocol = \"https://\"\n        url_with_prefix = \"https://\" + url\n    else:\n        protocol = \"http://\"\n        url_with_prefix = \"http://\" + url\n\n    if debug:\n        async_mode = \"eventlet\"\n    else:\n        async_mode = \"gevent\"\n\n    socketio.server_options[\"async_mode\"] = async_mode\n    try:\n        socketio.init_app(app)\n    except Exception:\n        print(\n            'failed to initialize socketio app with async mode \"%s\". Continuing with async mode \"threading\".'\n            % async_mode\n        )\n        socketio.server_options[\"async_mode\"] = \"threading\"\n        socketio.init_app(app)\n\n    if testing is False:\n        if host == DEFAULT_HOST:\n            url = (DEFAULT_HOST, port)\n        else:\n            try:\n                url = (socket.gethostbyname(socket.gethostname()), port)\n            except Exception:\n                url = (host, port)\n\n        if open_browser is True and debug is False:\n            browsertext = repr(browsername) if browsername else \"default browser\"\n            args = (browsertext,) + url\n            text = (\"Opening gdbgui with %s at \" + protocol + \"%s:%d\") % args\n            print(colorize(text))\n            b = webbrowser.get(browsername) if browsername else webbrowser\n            b.open(url_with_prefix)\n        else:\n            print(colorize(\"View gdbgui at %s%s:%d\" % (protocol, url[0], url[1])))\n\n        print(\"exit gdbgui by pressing CTRL+C\")\n\n        try:\n            socketio.run(\n                app,\n                debug=debug,\n                port=int(port),\n                host=host,\n                extra_files=get_extra_files(),\n                **kwargs\n            )\n        except KeyboardInterrupt:\n            # Process was interrupted by ctrl+c on keyboard, show message\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef dbprint(*args):\n    if app and app.debug:\n        if USING_WINDOWS:\n            print(\"DEBUG: \" + \" \".join(args))\n\n        else:\n            CYELLOW2 = \"\\33[93m\"\n            NORMAL = \"\\033[0m\"\n            print(CYELLOW2 + \"DEBUG: \" + \" \".join(args) + NORMAL)", "response": "print only if app. debug is truthy"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending message to all clients", "response": "def send_msg_to_clients(client_ids, msg, error=False):\n    \"\"\"Send message to all clients\"\"\"\n    if error:\n        stream = \"stderr\"\n    else:\n        stream = \"stdout\"\n\n    response = [{\"message\": None, \"type\": \"console\", \"payload\": msg, \"stream\": stream}]\n\n    for client_id in client_ids:\n        logger.info(\"emiting message to websocket client id \" + client_id)\n        socketio.emit(\n            \"gdb_response\", response, namespace=\"/gdb_listener\", room=client_id\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_and_forward_gdb_output():\n\n    while True:\n        socketio.sleep(0.05)\n        controllers_to_remove = []\n        controller_items = _state.controller_to_client_ids.items()\n        for controller, client_ids in controller_items:\n            try:\n                try:\n                    response = controller.get_gdb_response(\n                        timeout_sec=0, raise_error_on_timeout=False\n                    )\n                except NoGdbProcessError:\n                    response = None\n                    send_msg_to_clients(\n                        client_ids,\n                        \"The underlying gdb process has been killed. This tab will no longer function as expected.\",\n                        error=True,\n                    )\n                    controllers_to_remove.append(controller)\n\n                if response:\n                    for client_id in client_ids:\n                        logger.info(\n                            \"emiting message to websocket client id \" + client_id\n                        )\n                        socketio.emit(\n                            \"gdb_response\",\n                            response,\n                            namespace=\"/gdb_listener\",\n                            room=client_id,\n                        )\n                else:\n                    # there was no queued response from gdb, not a problem\n                    pass\n\n            except Exception:\n                logger.error(traceback.format_exc())\n\n        for controller in controllers_to_remove:\n            _state.remove_gdb_controller(controller)", "response": "A task that runs on a different thread and emits websocket messages\n    of gdb responses"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef gdbgui():\n    interpreter = \"lldb\" if app.config[\"LLDB\"] else \"gdb\"\n    gdbpid = request.args.get(\"gdbpid\", 0)\n    initial_gdb_user_command = request.args.get(\"initial_gdb_user_command\", \"\")\n\n    add_csrf_token_to_session()\n\n    THEMES = [\"monokai\", \"light\"]\n    # fmt: off\n    initial_data = {\n        \"csrf_token\": session[\"csrf_token\"],\n        \"gdbgui_version\": __version__,\n        \"gdbpid\": gdbpid,\n        \"initial_gdb_user_command\": initial_gdb_user_command,\n        \"interpreter\": interpreter,\n        \"initial_binary_and_args\": app.config[\"initial_binary_and_args\"],\n        \"p\": pbkdf2_hex(str(app.config.get(\"l\")), \"Feo8CJol\")\n        if app.config.get(\"l\")\n        else \"\",\n        \"project_home\": app.config[\"project_home\"],\n        \"remap_sources\": app.config[\"remap_sources\"],\n        \"rr\": app.config[\"rr\"],\n        \"show_gdbgui_upgrades\": app.config[\"show_gdbgui_upgrades\"],\n        \"themes\": THEMES,\n        \"signals\": SIGNAL_NAME_TO_OBJ,\n        \"using_windows\": USING_WINDOWS,\n    }\n    # fmt: on\n\n    return render_template(\n        \"gdbgui.html\",\n        version=__version__,\n        debug=app.debug,\n        interpreter=interpreter,\n        initial_data=initial_data,\n        themes=THEMES,\n    )", "response": "Render the main gdbgui interface"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting last modified unix time for a given file", "response": "def get_last_modified_unix_sec():\n    \"\"\"Get last modified unix time for a given file\"\"\"\n    path = request.args.get(\"path\")\n    if path and os.path.isfile(path):\n        try:\n            last_modified = os.path.getmtime(path)\n            return jsonify({\"path\": path, \"last_modified_unix_sec\": last_modified})\n\n        except Exception as e:\n            return client_error({\"message\": \"%s\" % e, \"path\": path})\n\n    else:\n        return client_error({\"message\": \"File not found: %s\" % path, \"path\": path})"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef read_file():\n    path = request.args.get(\"path\")\n    start_line = int(request.args.get(\"start_line\"))\n    end_line = int(request.args.get(\"end_line\"))\n\n    start_line = max(1, start_line)  # make sure it's not negative\n\n    try:\n        highlight = json.loads(request.args.get(\"highlight\", \"true\"))\n    except Exception as e:\n        if app.debug:\n            print(\"Raising exception since debug is on\")\n            raise e\n\n        else:\n            highlight = (\n                True\n            )  # highlight argument was invalid for some reason, default to true\n\n    if path and os.path.isfile(path):\n        try:\n            last_modified = os.path.getmtime(path)\n            with open(path, \"r\") as f:\n                raw_source_code_list = f.read().split(\"\\n\")\n                num_lines_in_file = len(raw_source_code_list)\n                end_line = min(\n                    num_lines_in_file, end_line\n                )  # make sure we don't try to go too far\n\n                # if leading lines are '', then the lexer will strip them out, but we want\n                # to preserve blank lines. Insert a space whenever we find a blank line.\n                for i in range((start_line - 1), (end_line)):\n                    if raw_source_code_list[i] == \"\":\n                        raw_source_code_list[i] = \" \"\n                raw_source_code_lines_of_interest = raw_source_code_list[\n                    (start_line - 1) : (end_line)\n                ]\n            try:\n                lexer = get_lexer_for_filename(path)\n            except Exception:\n                lexer = None\n\n            if lexer and highlight:\n                highlighted = True\n                # convert string into tokens\n                tokens = lexer.get_tokens(\"\\n\".join(raw_source_code_lines_of_interest))\n                # format tokens into nice, marked up list of html\n                formatter = (\n                    htmllistformatter.HtmlListFormatter()\n                )  # Don't add newlines after each line\n                source_code = formatter.get_marked_up_list(tokens)\n            else:\n                highlighted = False\n                source_code = raw_source_code_lines_of_interest\n\n            return jsonify(\n                {\n                    \"source_code_array\": source_code,\n                    \"path\": path,\n                    \"last_modified_unix_sec\": last_modified,\n                    \"highlighted\": highlighted,\n                    \"start_line\": start_line,\n                    \"end_line\": end_line,\n                    \"num_lines_in_file\": num_lines_in_file,\n                }\n            )\n\n        except Exception as e:\n            return client_error({\"message\": \"%s\" % e})\n\n    else:\n        return client_error({\"message\": \"File not found: %s\" % path})", "response": "Read a file and return its contents as an array"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning True if user may need to turn shell off due", "response": "def warn_startup_with_shell_off(platform, gdb_args):\n    \"\"\"return True if user may need to turn shell off\n    if mac OS version is 16 (sierra) or higher, may need to set shell off due\n    to os's security requirements\n    http://stackoverflow.com/questions/39702871/gdb-kind-of-doesnt-work-on-macos-sierra\n    \"\"\"\n    darwin_match = re.match(\"darwin-(\\d+)\\..*\", platform)\n    on_darwin = darwin_match is not None and int(darwin_match.groups()[0]) >= 16\n    if on_darwin:\n        shell_is_off = \"startup-with-shell off\" in gdb_args\n        return not shell_is_off\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef main_base_ramp(self) -> \"Ramp\":\n        if hasattr(self, \"cached_main_base_ramp\"):\n            return self.cached_main_base_ramp\n        self.cached_main_base_ramp = min(\n            {ramp for ramp in self.game_info.map_ramps if len(ramp.upper2_for_ramp_wall) == 2},\n            key=(lambda r: self.start_location.distance_to(r.top_center)),\n        )\n        return self.cached_main_base_ramp", "response": "Returns the main - base ramp instance of the closest main - ramp to start location."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef expansion_locations(self) -> Dict[Point2, Units]:\n        # RESOURCE_SPREAD_THRESHOLD = 144\n        RESOURCE_SPREAD_THRESHOLD = 225\n        geysers = self.state.vespene_geyser\n        all_resources = self.state.resources\n\n        # Group nearby minerals together to form expansion locations\n        resource_groups = []\n        for mf in all_resources:\n            mf_height = self.get_terrain_height(mf.position)\n            for cluster in resource_groups:\n                # bases on standard maps dont have more than 10 resources\n                if len(cluster) == 10:\n                    continue\n                if mf.position._distance_squared(\n                    cluster[0].position\n                ) < RESOURCE_SPREAD_THRESHOLD and mf_height == self.get_terrain_height(cluster[0].position):\n                    cluster.append(mf)\n                    break\n            else:  # not found\n                resource_groups.append([mf])\n        # Filter out bases with only one mineral field\n        resource_groups = [cluster for cluster in resource_groups if len(cluster) > 1]\n        # distance offsets from a gas geysir\n        offsets = [(x, y) for x in range(-9, 10) for y in range(-9, 10) if 75 >= x ** 2 + y ** 2 >= 49]\n        centers = {}\n        # for every resource group:\n        for resources in resource_groups:\n            # possible expansion points\n            # resources[-1] is a gas geysir which always has (x.5, y.5) coordinates, just like an expansion\n            possible_points = (\n                Point2((offset[0] + resources[-1].position.x, offset[1] + resources[-1].position.y))\n                for offset in offsets\n            )\n            # filter out points that are too near\n            possible_points = [\n                point\n                for point in possible_points\n                if all(point.distance_to(resource) >= (7 if resource in geysers else 6) for resource in resources)\n            ]\n            # choose best fitting point\n            result = min(possible_points, key=lambda p: sum(p.distance_to(resource) for resource in resources))\n            centers[result] = resources\n        \"\"\" Returns dict with center of resources as key, resources (mineral field, vespene geyser) as value \"\"\"\n        return centers", "response": "Return a list of possible expansion locations for all possible minerals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of available abilities for the given units.", "response": "async def get_available_abilities(self, units: Union[List[Unit], Units], ignore_resource_requirements=False) -> List[List[AbilityId]]:\n        \"\"\" Returns available abilities of one or more units. \"\"\"\n        # right know only checks cooldown, energy cost, and whether the ability has been researched\n        return await self._client.query_available_abilities(units, ignore_resource_requirements)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfinding next expansion location.", "response": "async def get_next_expansion(self) -> Optional[Point2]:\n        \"\"\"Find next expansion location.\"\"\"\n\n        closest = None\n        distance = math.inf\n        for el in self.expansion_locations:\n            def is_near_to_expansion(t):\n                return t.position.distance_to(el) < self.EXPANSION_GAP_THRESHOLD\n\n            if any(map(is_near_to_expansion, self.townhalls)):\n                # already taken\n                continue\n\n            startp = self._game_info.player_start_location\n            d = await self._client.query_pathing(startp, el)\n            if d is None:\n                continue\n\n            if d < distance:\n                distance = d\n                closest = el\n\n        return closest"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def distribute_workers(self):\n\n        # TODO:\n        # OPTIMIZE: Assign idle workers smarter\n        # OPTIMIZE: Never use same worker mutltiple times\n        owned_expansions = self.owned_expansions\n        worker_pool = []\n        actions = []\n\n        for idle_worker in self.workers.idle:\n            mf = self.state.mineral_field.closest_to(idle_worker)\n            actions.append(idle_worker.gather(mf))\n\n        for location, townhall in owned_expansions.items():\n            workers = self.workers.closer_than(20, location)\n            actual = townhall.assigned_harvesters\n            ideal = townhall.ideal_harvesters\n            excess = actual - ideal\n            if actual > ideal:\n                worker_pool.extend(workers.random_group_of(min(excess, len(workers))))\n                continue\n        for g in self.geysers:\n            workers = self.workers.closer_than(5, g)\n            actual = g.assigned_harvesters\n            ideal = g.ideal_harvesters\n            excess = actual - ideal\n            if actual > ideal:\n                worker_pool.extend(workers.random_group_of(min(excess, len(workers))))\n                continue\n\n        for g in self.geysers:\n            actual = g.assigned_harvesters\n            ideal = g.ideal_harvesters\n            deficit = ideal - actual\n\n            for _ in range(deficit):\n                if worker_pool:\n                    w = worker_pool.pop()\n                    if len(w.orders) == 1 and w.orders[0].ability.id is AbilityId.HARVEST_RETURN:\n                        actions.append(w.move(g))\n                        actions.append(w.return_resource(queue=True))\n                    else:\n                        actions.append(w.gather(g))\n\n        for location, townhall in owned_expansions.items():\n            actual = townhall.assigned_harvesters\n            ideal = townhall.ideal_harvesters\n\n            deficit = ideal - actual\n            for x in range(0, deficit):\n                if worker_pool:\n                    w = worker_pool.pop()\n                    mf = self.state.mineral_field.closest_to(townhall)\n                    if len(w.orders) == 1 and w.orders[0].ability.id is AbilityId.HARVEST_RETURN:\n                        actions.append(w.move(townhall))\n                        actions.append(w.return_resource(queue=True))\n                        actions.append(w.gather(mf, queue=True))\n                    else:\n                        actions.append(w.gather(mf))\n\n        await self.do_actions(actions)", "response": "Distributes workers across all the bases taken."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nlists of expansions owned by the player.", "response": "def owned_expansions(self):\n        \"\"\"List of expansions owned by the player.\"\"\"\n\n        owned = {}\n        for el in self.expansion_locations:\n            def is_near_to_expansion(t):\n                return t.position.distance_to(el) < self.EXPANSION_GAP_THRESHOLD\n\n            th = next((x for x in self.townhalls if is_near_to_expansion(x)), None)\n            if th:\n                owned[el] = th\n\n        return owned"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef can_feed(self, unit_type: UnitTypeId) -> bool:\n        required = self._game_data.units[unit_type.value]._proto.food_required\n        return required == 0 or self.supply_left >= required", "response": "Checks if you have enough free supply to build the unit"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ntests if the player has enough resources to build a unit or cast an ability.", "response": "def can_afford(self, item_id: Union[UnitTypeId, UpgradeId, AbilityId], check_supply_cost: bool=True) -> \"CanAffordWrapper\":\n        \"\"\"Tests if the player has enough resources to build a unit or cast an ability.\"\"\"\n        enough_supply = True\n        if isinstance(item_id, UnitTypeId):\n            unit = self._game_data.units[item_id.value]\n            cost = self._game_data.calculate_ability_cost(unit.creation_ability)\n            if check_supply_cost:\n                enough_supply = self.can_feed(item_id)\n        elif isinstance(item_id, UpgradeId):\n            cost = self._game_data.upgrades[item_id.value].cost\n        else:\n            cost = self._game_data.calculate_ability_cost(item_id)\n\n        return CanAffordWrapper(cost.minerals <= self.minerals, cost.vespene <= self.vespene, enough_supply)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def can_cast(self, unit: Unit, ability_id: AbilityId, target: Optional[Union[Unit, Point2, Point3]]=None, only_check_energy_and_cooldown: bool=False, cached_abilities_of_unit: List[AbilityId]=None) -> bool:\n        assert isinstance(unit, Unit)\n        assert isinstance(ability_id, AbilityId)\n        assert isinstance(target, (type(None), Unit, Point2, Point3))\n        # check if unit has enough energy to cast or if ability is on cooldown\n        if cached_abilities_of_unit:\n            abilities = cached_abilities_of_unit\n        else:\n            abilities = (await self.get_available_abilities([unit]))[0]\n\n        if ability_id in abilities:\n            if only_check_energy_and_cooldown:\n                return True\n            cast_range = self._game_data.abilities[ability_id.value]._proto.cast_range\n            ability_target = self._game_data.abilities[ability_id.value]._proto.target\n            # Check if target is in range (or is a self cast like stimpack)\n            if ability_target == 1 or ability_target == Target.PointOrNone.value and isinstance(target, (Point2, Point3)) and unit.distance_to(target) <= cast_range: # cant replace 1 with \"Target.None.value\" because \".None\" doesnt seem to be a valid enum name\n                return True\n            # Check if able to use ability on a unit\n            elif ability_target in {Target.Unit.value, Target.PointOrUnit.value} and isinstance(target, Unit) and unit.distance_to(target) <= cast_range:\n                return True\n            # Check if able to use ability on a position\n            elif ability_target in {Target.Point.value, Target.PointOrUnit.value} and isinstance(target, (Point2, Point3)) and unit.distance_to(target) <= cast_range:\n                return True\n        return False", "response": "Tests if a unit has an ability available and enough energy to cast it."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef select_build_worker(self, pos: Union[Unit, Point2, Point3], force: bool=False) -> Optional[Unit]:\n\n        workers = self.workers.closer_than(20, pos) or self.workers\n        for worker in workers.prefer_close_to(pos).prefer_idle:\n            if not worker.orders or len(worker.orders) == 1 and worker.orders[0].ability.id in {AbilityId.MOVE,\n                                                                                                AbilityId.HARVEST_GATHER,\n                                                                                                AbilityId.HARVEST_RETURN}:\n                return worker\n\n        return workers.random if force else None", "response": "Select a worker to build a bulding with."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def can_place(self, building: Union[AbilityData, AbilityId, UnitTypeId], position: Point2) -> bool:\n\n        assert isinstance(building, (AbilityData, AbilityId, UnitTypeId))\n\n        if isinstance(building, UnitTypeId):\n            building = self._game_data.units[building.value].creation_ability\n        elif isinstance(building, AbilityId):\n            building = self._game_data.abilities[building.value]\n\n        r = await self._client.query_building_placement(building, [position])\n        return r[0] == ActionResult.Success", "response": "Tests if a building can be placed in the given location."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def find_placement(self, building: UnitTypeId, near: Union[Unit, Point2, Point3], max_distance: int=20, random_alternative: bool=True, placement_step: int=2) -> Optional[Point2]:\n\n        assert isinstance(building, (AbilityId, UnitTypeId))\n        assert isinstance(near, Point2)\n\n        if isinstance(building, UnitTypeId):\n            building = self._game_data.units[building.value].creation_ability\n        else:  # AbilityId\n            building = self._game_data.abilities[building.value]\n\n        if await self.can_place(building, near):\n            return near\n\n        if max_distance == 0:\n            return None\n\n        for distance in range(placement_step, max_distance, placement_step):\n            possible_positions = [Point2(p).offset(near).to2 for p in (\n                    [(dx, -distance) for dx in range(-distance, distance + 1, placement_step)] +\n                    [(dx, distance) for dx in range(-distance, distance + 1, placement_step)] +\n                    [(-distance, dy) for dy in range(-distance, distance + 1, placement_step)] +\n                    [(distance, dy) for dy in range(-distance, distance + 1, placement_step)]\n            )]\n            res = await self._client.query_building_placement(building, possible_positions)\n            possible = [p for r, p in zip(res, possible_positions) if r == ActionResult.Success]\n            if not possible:\n                continue\n\n            if random_alternative:\n                return random.choice(possible)\n            else:\n                return min(possible, key=lambda p: p.distance_to(near))\n        return None", "response": "Finds a placement location for building."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nchecks if an upgrade is already being researched and return the new version of the new version.", "response": "def already_pending_upgrade(self, upgrade_type: UpgradeId) -> Union[int, float]:\n        \"\"\" Check if an upgrade is being researched\n        Return values:\n        0: not started\n        0 < x < 1: researching\n        1: finished\n        \"\"\"\n        assert isinstance(upgrade_type, UpgradeId)\n        if upgrade_type in self.state.upgrades:\n            return 1\n        level = None\n        if \"LEVEL\" in upgrade_type.name:\n            level = upgrade_type.name[-1]\n        creationAbilityID = self._game_data.upgrades[upgrade_type.value].research_ability.id\n        for structure in self.units.structure.ready:\n            for order in structure.orders:\n                if order.ability.id is creationAbilityID:\n                    if level and order.ability.button_name[-1] != level:\n                        return 0\n                    return order.progress\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef already_pending(self, unit_type: Union[UpgradeId, UnitTypeId], all_units: bool=False) -> int:\n\n        # TODO / FIXME: SCV building a structure might be counted as two units\n\n        if isinstance(unit_type, UpgradeId):\n            return self.already_pending_upgrade(unit_type)\n            \n        ability = self._game_data.units[unit_type.value].creation_ability\n\n        amount = len(self.units(unit_type).not_ready)\n\n        if all_units:\n            amount += sum([o.ability == ability for u in self.units for o in u.orders])\n        else:\n            amount += sum([o.ability == ability for w in self.workers for o in w.orders])\n            amount += sum([egg.orders[0].ability == ability for egg in self.units(UnitTypeId.EGG)])\n\n        return amount", "response": "Returns a number of buildings or workers or build queues of a given unit type."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def chat_send(self, message: str):\n        assert isinstance(message, str)\n        await self._client.chat_send(message, False)", "response": "Send a chat message."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_terrain_height(self, pos: Union[Point2, Point3, Unit]) -> int:\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.terrain_height[pos]", "response": "Returns the terrain height at a position. Caution is not anywhere near a unit s z - coordinate."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef in_placement_grid(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.placement_grid[pos] != 0", "response": "Returns True if you can place something at a position."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn True if a unit can pass through a grid point.", "response": "def in_pathing_grid(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        \"\"\" Returns True if a unit can pass through a grid point. \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self._game_info.pathing_grid[pos] == 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn True if the point is visible on a grid point.", "response": "def is_visible(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        \"\"\" Returns True if you have vision on a grid point. \"\"\"\n        # more info: https://github.com/Blizzard/s2client-proto/blob/9906df71d6909511907d8419b33acc1a3bd51ec0/s2clientprotocol/spatial.proto#L19\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self.state.visibility[pos] == 2"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns True if there is creep on the grid point.", "response": "def has_creep(self, pos: Union[Point2, Point3, Unit]) -> bool:\n        \"\"\" Returns True if there is creep on the grid point. \"\"\"\n        assert isinstance(pos, (Point2, Point3, Unit))\n        pos = pos.position.to2.rounded\n        return self.state.creep[pos] != 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprepare game and player data for game start.", "response": "def _prepare_start(self, client, player_id, game_info, game_data):\n        \"\"\"Ran until game start to set game and player data.\"\"\"\n        self._client: \"Client\" = client\n        self._game_info: \"GameInfo\" = game_info\n        self._game_data: GameData = game_data\n\n        self.player_id: int = player_id\n        self.race: Race = Race(self._game_info.player_races[self.player_id])\n        self._units_previous_map: dict = dict()\n        self.units: Units = Units([], game_data)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\npreparing the first step extra preparations. Must not be called before _prepare_step.", "response": "def _prepare_first_step(self):\n        \"\"\"First step extra preparations. Must not be called before _prepare_step.\"\"\"\n        if self.townhalls:\n            self._game_info.player_start_location = self.townhalls.first.position\n        self._game_info.map_ramps = self._game_info._find_ramps()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _prepare_step(self, state):\n        self.state: GameState = state\n        # Required for events\n        self._units_previous_map.clear()\n        for unit in self.units:\n            self._units_previous_map[unit.tag] = unit\n\n        self.units: Units = state.own_units\n        self.workers: Units = self.units(race_worker[self.race])\n        self.townhalls: Units = self.units(race_townhalls[self.race])\n        self.geysers: Units = self.units(race_gas[self.race])\n\n        self.minerals: Union[float, int] = state.common.minerals\n        self.vespene: Union[float, int] = state.common.vespene\n        self.supply_used: Union[float, int] = state.common.food_used\n        self.supply_cap: Union[float, int] = state.common.food_cap\n        self.supply_left: Union[float, int] = self.supply_cap - self.supply_used\n        # reset cached values\n        self.cached_known_enemy_structures = None\n        self.cached_known_enemy_units = None", "response": "Set attributes from new state before on_step."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def issue_events(self):\n        await self._issue_unit_dead_events()\n        await self._issue_unit_added_events()\n        for unit in self.units.structure:\n            await self._issue_building_complete_event(unit)", "response": "Issues events from the unit structure to the builders."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef type_id(self) -> UnitTypeId:\n        unit_type = self._proto.unit_type\n        if unit_type not in self._game_data.unit_types:\n            self._game_data.unit_types[unit_type] = UnitTypeId(unit_type)\n        return self._game_data.unit_types[unit_type]", "response": "Return the unit type id for this unit type."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the ground dps of the weapon.", "response": "def ground_dps(self) -> Union[int, float]:\n        \"\"\" Does not include upgrades \"\"\"\n        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Ground.value, TargetType.Any.value}),\n                None,\n            )\n            if weapon:\n                return (weapon.damage * weapon.attacks) / weapon.speed\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the ground range of the weapon.", "response": "def ground_range(self) -> Union[int, float]:\n        \"\"\" Does not include upgrades \"\"\"\n        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Ground.value, TargetType.Any.value}),\n                None,\n            )\n            if weapon:\n                return weapon.range\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef can_attack_air(self) -> bool:\n        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Air.value, TargetType.Any.value}),\n                None,\n            )\n            return weapon is not None\n        return False", "response": "Returns True if the user can attack air."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the air dps of the weapon.", "response": "def air_dps(self) -> Union[int, float]:\n        \"\"\" Does not include upgrades \"\"\"\n        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Air.value, TargetType.Any.value}),\n                None,\n            )\n            if weapon:\n                return (weapon.damage * weapon.attacks) / weapon.speed\n        return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef air_range(self) -> Union[int, float]:\n        if self._weapons:\n            weapon = next(\n                (weapon for weapon in self._weapons if weapon.type in {TargetType.Air.value, TargetType.Any.value}),\n                None,\n            )\n            if weapon:\n                return weapon.range\n        return 0", "response": "Returns the air range of the weapons."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a tuple of form bonus damage armor type if unit does bonus damage against armor type", "response": "def bonus_damage(self):\n        \"\"\" Returns a tuple of form 'bonus damage, armor type' if unit does bonus damage against armor type\n        Light = 1; Armored = 2; Biological = 3; Mechanical = 4; Robotic = 5; Psionic = 6; Massive = 7;\n        Structure = 8; Hover = 9; Heroic = 10; Summoned = 11 \"\"\"\n        # TODO Consider unit with ability attacks like Oracle, Thor, Baneling\n        if self._weapons:\n            for weapon in self._weapons:\n                if weapon.damage_bonus:\n                    b = weapon.damage_bonus[0]\n                    return b.bonus, b.attribute"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef health_percentage(self) -> Union[int, float]:\n        if self._proto.health_max == 0:\n            return 0\n        return self._proto.health / self._proto.health_max", "response": "Returns the percentage of the current health."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nusing the 2d distance between self and p.", "response": "def distance_to(self, p: Union[\"Unit\", Point2, Point3], bot: \"BotAI\" = None) -> Union[int, float]:\n        \"\"\" Using the 2d distance between self and p. To calculate the 3d distance,\n        use unit.position3d.distance_to(p) \"\"\"\n        if bot and isinstance(p, Unit):\n            index = bot.distances_tag_dict\n            return (bot.unit_distances_dict[index[self.tag]][index[p.tag]]) ** 0.5\n        return self.position.distance_to_point2(p.position)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef weapon_cooldown(self) -> Union[int, float]:\n        if self.can_attack:\n            return self._proto.weapon_cooldown\n        return -1", "response": "Returns some time until the unit can fire again."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef passengers(self) -> Set[\"PassengerUnit\"]:\n        return {PassengerUnit(unit, self._game_data) for unit in self._proto.passengers}", "response": "The set of passengers in this game."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef target_in_range(self, target: \"Unit\", bonus_distance: Union[int, float] = 0) -> bool:\n        if self.can_attack_ground and not target.is_flying:\n            unit_attack_range = self.ground_range\n        elif self.can_attack_air and (target.is_flying or target.type_id == UnitTypeId.COLOSSUS):\n            unit_attack_range = self.air_range\n        else:\n            unit_attack_range = -1\n        return (\n            self.position._distance_squared(target.position)\n            <= (self.radius + target.radius + unit_attack_range - bonus_distance) ** 2\n        )", "response": "Returns True if the target is within the range of the current unit."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef is_carrying_minerals(self) -> bool:\n        return any(\n            buff.value in self._proto.buff_ids\n            for buff in {BuffId.CARRYMINERALFIELDMINERALS, BuffId.CARRYHIGHYIELDMINERALFIELDMINERALS}\n        )", "response": "Checks if a worker or MULE is carrying minerals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a worker is carrying vespene.", "response": "def is_carrying_vespene(self) -> bool:\n        \"\"\" Checks if a worker is carrying vespene. \"\"\"\n        return any(\n            buff.value in self._proto.buff_ids\n            for buff in {\n                BuffId.CARRYHARVESTABLEVESPENEGEYSERGAS,\n                BuffId.CARRYHARVESTABLEVESPENEGEYSERGASPROTOSS,\n                BuffId.CARRYHARVESTABLEVESPENEGEYSERGASZERG,\n            }\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef is_patrolling(self) -> bool:\n        return self.orders and self.orders[0].ability.id is AbilityId.PATROL", "response": "Checks if a unit is patrolling."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if a unit is on its way to a mineral field or vespene geyser to mine.", "response": "def is_gathering(self) -> bool:\n        \"\"\" Checks if a unit is on its way to a mineral field / vespene geyser to mine. \"\"\"\n        return self.orders and self.orders[0].ability.id is AbilityId.HARVEST_GATHER"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if a unit is returning from mineral field or vespene geyser to deliver resources to townhall.", "response": "def is_returning(self) -> bool:\n        \"\"\" Checks if a unit is returning from mineral field / vespene geyser to deliver resources to townhall. \"\"\"\n        return self.orders and self.orders[0].ability.id is AbilityId.HARVEST_RETURN"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nchecking if the basket is collecting.", "response": "def is_collecting(self) -> bool:\n        \"\"\" Combines the two properties above. \"\"\"\n        return self.orders and self.orders[0].ability.id in {AbilityId.HARVEST_GATHER, AbilityId.HARVEST_RETURN}"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck if the unit is an SCV that is currently building.", "response": "def is_constructing_scv(self) -> bool:\n        \"\"\" Checks if the unit is an SCV that is currently building. \"\"\"\n        return self.orders and self.orders[0].ability.id in {\n            AbilityId.TERRANBUILD_ARMORY,\n            AbilityId.TERRANBUILD_BARRACKS,\n            AbilityId.TERRANBUILD_BUNKER,\n            AbilityId.TERRANBUILD_COMMANDCENTER,\n            AbilityId.TERRANBUILD_ENGINEERINGBAY,\n            AbilityId.TERRANBUILD_FACTORY,\n            AbilityId.TERRANBUILD_FUSIONCORE,\n            AbilityId.TERRANBUILD_GHOSTACADEMY,\n            AbilityId.TERRANBUILD_MISSILETURRET,\n            AbilityId.TERRANBUILD_REFINERY,\n            AbilityId.TERRANBUILD_SENSORTOWER,\n            AbilityId.TERRANBUILD_STARPORT,\n            AbilityId.TERRANBUILD_SUPPLYDEPOT,\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the target tag of the first order unit or position.", "response": "def order_target(self) -> Optional[Union[int, Point2]]:\n        \"\"\" Returns the target tag (if it is a Unit) or Point2 (if it is a Position)\n        from the first order, returns None if the unit is idle \"\"\"\n        if self.orders:\n            if isinstance(self.orders[0].target, int):\n                return self.orders[0].target\n            else:\n                return Point2.from_proto(self.orders[0].target)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef research(self, upgrade, *args, **kwargs):\n        return self(self._game_data.upgrades[upgrade.value].research_ability.id, *args, **kwargs)", "response": "Research the game for the given upgrade."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nfiltering units that are in attack range of the given unit in parameter bonus_distance.", "response": "def in_attack_range_of(self, unit: Unit, bonus_distance: Union[int, float] = 0) -> \"Units\":\n        \"\"\" Filters units that are in attack range of the unit in parameter \"\"\"\n        return self.filter(lambda x: unit.target_in_range(x, bonus_distance=bonus_distance))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the distance between the closest unit from this group to the target unit", "response": "def closest_distance_to(self, position: Union[Unit, Point2, Point3]) -> Union[int, float]:\n        \"\"\" Returns the distance between the closest unit from this group to the target unit \"\"\"\n        assert self.exists\n        if isinstance(position, Unit):\n            position = position.position\n        return position.distance_to_closest(\n            [u.position for u in self]\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef furthest_distance_to(self, position: Union[Unit, Point2, Point3]) -> Union[int, float]:\n        assert self.exists\n        if isinstance(position, Unit):\n            position = position.position\n        return position.distance_to_furthest([u.position for u in self])", "response": "Returns the distance between the furthest unit from this group to the target unit"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef sorted_by_distance_to(self, position: Union[Unit, Point2], reverse: bool = False) -> \"Units\":\n        if len(self) in [0, 1]:\n            return self\n        position = position.position\n        return self.sorted(keyfn=lambda unit: unit.position._distance_squared(position), reverse=reverse)", "response": "Returns a list of units sorted by distance to a given position."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef tags_not_in(self, other: Union[Set[int], List[int], Dict[int, Any]]) -> \"Units\":\n        # example: self.units(QUEEN).tags_not_in(self.queen_tags_assigned_to_do_injects)\n        if isinstance(other, list):\n            other = set(other)\n        return self.filter(lambda unit: unit.tag not in other)", "response": "Returns a new Units with units with their tags not in other."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef of_type(self, other: Union[UnitTypeId, Set[UnitTypeId], List[UnitTypeId], Dict[UnitTypeId, Any]]) -> \"Units\":\n        # example: self.units.of_type([ZERGLING, ROACH, HYDRALISK, BROODLORD])\n        if isinstance(other, UnitTypeId):\n            other = {other}\n        if isinstance(other, list):\n            other = set(other)\n        return self.filter(lambda unit: unit.type_id in other)", "response": "Returns a new UnitCollection with only units of a specific type."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a new UnitCollection with the same tech as the given one.", "response": "def same_tech(self, other: Union[UnitTypeId, Set[UnitTypeId], List[UnitTypeId], Dict[UnitTypeId, Any]]) -> \"Units\":\n        \"\"\" Usage:\n        'self.units.same_tech(UnitTypeId.COMMANDCENTER)' or 'self.units.same_tech(UnitTypeId.ORBITALCOMMAND)'\n        returns all CommandCenter, CommandCenterFlying, OrbitalCommand, OrbitalCommandFlying, PlanetaryFortress\n        This also works with a set/list/dict parameter, e.g. 'self.units.same_tech({UnitTypeId.COMMANDCENTER, UnitTypeId.SUPPLYDEPOT})'\n        Untested: This should return the equivalents for Hatchery, WarpPrism, Observer, Overseer, SupplyDepot and others\n        \"\"\"\n        if isinstance(other, UnitTypeId):\n            other = {other}\n        tech_alias_types = set(other)\n        for unitType in other:\n            tech_alias = self.game_data.units[unitType.value].tech_alias\n            if tech_alias:\n                for same in tech_alias:\n                    tech_alias_types.add(same)\n        return self.filter(\n            lambda unit: unit.type_id in tech_alias_types\n            or unit._type_data.tech_alias is not None\n            and any(same in tech_alias_types for same in unit._type_data.tech_alias)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef same_unit(self, other: Union[UnitTypeId, Set[UnitTypeId], List[UnitTypeId], Dict[UnitTypeId, Any]]) -> \"Units\":\n        if isinstance(other, UnitTypeId):\n            other = {other}\n        unit_alias_types = set(other)\n        for unitType in other:\n            unit_alias = self.game_data.units[unitType.value].unit_alias\n            if unit_alias:\n                unit_alias_types.add(unit_alias)\n        return self.filter(\n            lambda unit: unit.type_id in unit_alias_types\n            or unit._type_data.unit_alias is not None\n            and unit._type_data.unit_alias in unit_alias_types\n        )", "response": "Return a new list of Units with the same unit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the center point of all units in this list.", "response": "def center(self) -> Point2:\n        \"\"\" Returns the central point of all units in this list \"\"\"\n        assert self\n        pos = Point2(\n            (\n                sum([unit.position.x for unit in self]) / self.amount,\n                sum([unit.position.y for unit in self]) / self.amount,\n            )\n        )\n        return pos"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def on_step(self, iteration):\n        self.combinedActions = []\n\n        \"\"\"\n        -  depots when low on remaining supply\n        - townhalls contains commandcenter and orbitalcommand\n        - self.units(TYPE).not_ready.amount selects all units of that type, filters incomplete units, and then counts the amount\n        - self.already_pending(TYPE) counts how many units are queued - but in this bot below you will find a slightly different already_pending function which only counts units queued (but not in construction)\n        \"\"\"\n        if self.supply_left < 5 and self.townhalls.exists and self.supply_used >= 14 and self.can_afford(UnitTypeId.SUPPLYDEPOT) and self.units(UnitTypeId.SUPPLYDEPOT).not_ready.amount + self.already_pending(UnitTypeId.SUPPLYDEPOT) < 1:\n            ws = self.workers.gathering\n            if ws: # if workers found\n                w = ws.furthest_to(ws.center)\n                loc = await self.find_placement(UnitTypeId.SUPPLYDEPOT, w.position, placement_step=3)\n                if loc: # if a placement location was found\n                    # build exactly on that location\n                    self.combinedActions.append(w.build(UnitTypeId.SUPPLYDEPOT, loc))\n\n        # lower all depots when finished\n        for depot in self.units(UnitTypeId.SUPPLYDEPOT).ready:\n            self.combinedActions.append(depot(AbilityId.MORPH_SUPPLYDEPOT_LOWER))\n\n        # morph commandcenter to orbitalcommand\n        if self.units(UnitTypeId.BARRACKS).ready.exists and self.can_afford(UnitTypeId.ORBITALCOMMAND): # check if orbital is affordable\n            for cc in self.units(UnitTypeId.COMMANDCENTER).idle: # .idle filters idle command centers\n                self.combinedActions.append(cc(AbilityId.UPGRADETOORBITAL_ORBITALCOMMAND))\n\n        # expand if we can afford and have less than 2 bases\n        if 1 <= self.townhalls.amount < 2 and self.already_pending(UnitTypeId.COMMANDCENTER) == 0 and self.can_afford(UnitTypeId.COMMANDCENTER):\n            # get_next_expansion returns the center of the mineral fields of the next nearby expansion\n            next_expo = await self.get_next_expansion()\n            # from the center of mineral fields, we need to find a valid place to place the command center\n            location = await self.find_placement(UnitTypeId.COMMANDCENTER, next_expo, placement_step=1)\n            if location:\n                # now we \"select\" (or choose) the nearest worker to that found location\n                w = self.select_build_worker(location)\n                if w and self.can_afford(UnitTypeId.COMMANDCENTER):\n                    # the worker will be commanded to build the command center\n                    error = await self.do(w.build(UnitTypeId.COMMANDCENTER, location))\n                    if error:\n                        print(error)\n\n        # make up to 4 barracks if we can afford them\n        # check if we have a supply depot (tech requirement) before trying to make barracks\n        if self.units.of_type([UnitTypeId.SUPPLYDEPOT, UnitTypeId.SUPPLYDEPOTLOWERED, UnitTypeId.SUPPLYDEPOTDROP]).ready.exists and self.units(UnitTypeId.BARRACKS).amount + self.already_pending(UnitTypeId.BARRACKS) < 4 and self.can_afford(UnitTypeId.BARRACKS):\n            ws = self.workers.gathering\n            if ws and self.townhalls.exists: # need to check if townhalls.amount > 0 because placement is based on townhall location\n                w = ws.furthest_to(ws.center)\n                # I chose placement_step 4 here so there will be gaps between barracks hopefully\n                loc = await self.find_placement(UnitTypeId.BARRACKS, self.townhalls.random.position, placement_step=4)\n                if loc:\n                    self.combinedActions.append(w.build(UnitTypeId.BARRACKS, loc))\n\n        # build refineries (on nearby vespene) when at least one barracks is in construction\n        if self.units(UnitTypeId.BARRACKS).amount > 0 and self.already_pending(UnitTypeId.REFINERY) < 1:\n            for th in self.townhalls:\n                vgs = self.state.vespene_geyser.closer_than(10, th)\n                for vg in vgs:\n                    if await self.can_place(UnitTypeId.REFINERY, vg.position) and self.can_afford(UnitTypeId.REFINERY):\n                        ws = self.workers.gathering\n                        if ws.exists: # same condition as above\n                            w = ws.closest_to(vg)\n                            # caution: the target for the refinery has to be the vespene geyser, not its position!\n                            self.combinedActions.append(w.build(UnitTypeId.REFINERY, vg))\n\n\n        # make scvs until 18, usually you only need 1:1 mineral:gas ratio for reapers, but if you don't lose any then you will need additional depots (mule income should take care of that)\n        # stop scv production when barracks is complete but we still have a command cender (priotize morphing to orbital command)\n        if self.can_afford(UnitTypeId.SCV) and self.supply_left > 0 and self.units(UnitTypeId.SCV).amount < 18 and (self.units(UnitTypeId.BARRACKS).ready.amount < 1 and self.units(UnitTypeId.COMMANDCENTER).idle.exists or self.units(UnitTypeId.ORBITALCOMMAND).idle.exists):\n            for th in self.townhalls.idle:\n                self.combinedActions.append(th.train(UnitTypeId.SCV))\n\n        # make reapers if we can afford them and we have supply remaining\n        if self.can_afford(UnitTypeId.REAPER) and self.supply_left > 0:\n            # loop through all idle barracks\n            for rax in self.units(UnitTypeId.BARRACKS).idle:\n                self.combinedActions.append(rax.train(UnitTypeId.REAPER))\n\n        # send workers to mine from gas\n        if iteration % 25 == 0:\n            await self.distribute_workers()\n\n        # reaper micro\n        for r in self.units(UnitTypeId.REAPER):\n\n            # move to range 15 of closest unit if reaper is below 20 hp and not regenerating\n            enemyThreatsClose = self.known_enemy_units.filter(lambda x: x.can_attack_ground).closer_than(15, r) # threats that can attack the reaper\n            if r.health_percentage < 2/5 and enemyThreatsClose.exists:\n                retreatPoints = self.neighbors8(r.position, distance=2) | self.neighbors8(r.position, distance=4)\n                # filter points that are pathable\n                retreatPoints = {x for x in retreatPoints if self.inPathingGrid(x)}\n                if retreatPoints:\n                    closestEnemy = enemyThreatsClose.closest_to(r)\n                    retreatPoint = closestEnemy.position.furthest(retreatPoints)\n                    self.combinedActions.append(r.move(retreatPoint))\n                    continue # continue for loop, dont execute any of the following\n\n            # reaper is ready to attack, shoot nearest ground unit\n            enemyGroundUnits = self.known_enemy_units.not_flying.closer_than(5, r) # hardcoded attackrange of 5\n            if r.weapon_cooldown == 0 and enemyGroundUnits.exists:\n                enemyGroundUnits = enemyGroundUnits.sorted(lambda x: x.distance_to(r))\n                closestEnemy = enemyGroundUnits[0]\n                self.combinedActions.append(r.attack(closestEnemy))\n                continue # continue for loop, dont execute any of the following\n            \n            # attack is on cooldown, check if grenade is on cooldown, if not then throw it to furthest enemy in range 5\n            reaperGrenadeRange = self._game_data.abilities[AbilityId.KD8CHARGE_KD8CHARGE.value]._proto.cast_range\n            enemyGroundUnitsInGrenadeRange = self.known_enemy_units.not_structure.not_flying.exclude_type([UnitTypeId.LARVA, UnitTypeId.EGG]).closer_than(reaperGrenadeRange, r)\n            if enemyGroundUnitsInGrenadeRange.exists and (r.is_attacking or r.is_moving):\n                # if AbilityId.KD8CHARGE_KD8CHARGE in abilities, we check that to see if the reaper grenade is off cooldown\n                abilities = (await self.get_available_abilities(r))\n                enemyGroundUnitsInGrenadeRange = enemyGroundUnitsInGrenadeRange.sorted(lambda x: x.distance_to(r), reverse=True)\n                furthestEnemy = None\n                for enemy in enemyGroundUnitsInGrenadeRange:\n                    if await self.can_cast(r, AbilityId.KD8CHARGE_KD8CHARGE, enemy, cached_abilities_of_unit=abilities):\n                        furthestEnemy = enemy\n                        break\n                if furthestEnemy:\n                    self.combinedActions.append(r(AbilityId.KD8CHARGE_KD8CHARGE, furthestEnemy))\n                    continue # continue for loop, don't execute any of the following\n\n            # move towards to max unit range if enemy is closer than 4\n            enemyThreatsVeryClose = self.known_enemy_units.filter(lambda x: x.can_attack_ground).closer_than(4.5, r) # hardcoded attackrange minus 0.5\n            # threats that can attack the reaper\n            if r.weapon_cooldown != 0 and enemyThreatsVeryClose.exists:\n                retreatPoints = self.neighbors8(r.position, distance=2) | self.neighbors8(r.position, distance=4)               \n                # filter points that are pathable by a reaper\n                retreatPoints = {x for x in retreatPoints if self.inPathingGrid(x)}\n                if retreatPoints:\n                    closestEnemy = enemyThreatsVeryClose.closest_to(r)\n                    retreatPoint = max(retreatPoints, key=lambda x: x.distance_to(closestEnemy) - x.distance_to(r))\n                    # retreatPoint = closestEnemy.position.furthest(retreatPoints)\n                    self.combinedActions.append(r.move(retreatPoint))\n                    continue # continue for loop, don't execute any of the following\n\n            # move to nearest enemy ground unit/building because no enemy unit is closer than 5\n            allEnemyGroundUnits = self.known_enemy_units.not_flying\n            if allEnemyGroundUnits.exists:\n                closestEnemy = allEnemyGroundUnits.closest_to(r)\n                self.combinedActions.append(r.move(closestEnemy))\n                continue # continue for loop, don't execute any of the following\n\n            # move to random enemy start location if no enemy buildings have been seen\n            self.combinedActions.append(r.move(random.choice(self.enemy_start_locations)))\n            \n        # manage idle scvs, would be taken care by distribute workers aswell\n        if self.townhalls.exists:\n            for w in self.workers.idle:\n                th = self.townhalls.closest_to(w)\n                mfs = self.state.mineral_field.closer_than(10, th)\n                if mfs:\n                    mf = mfs.closest_to(w)\n                    self.combinedActions.append(w.gather(mf))\n\n        # manage orbital energy and drop mules\n        for oc in self.units(UnitTypeId.ORBITALCOMMAND).filter(lambda x: x.energy >= 50):\n            mfs = self.state.mineral_field.closer_than(10, oc)\n            if mfs:\n                mf = max(mfs, key=lambda x:x.mineral_contents)\n                self.combinedActions.append(oc(AbilityId.CALLDOWNMULE_CALLDOWNMULE, mf))\n\n        # when running out of mineral fields near command center, fly to next base with minerals\n\n        # execuite actions\n        await self.do_actions(self.combinedActions)", "response": "This method is called by the main loop when the iteration is done."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nleaving the midst game.", "response": "async def leave(self):\n        \"\"\" You can use 'await self._client.leave()' to surrender midst game. \"\"\"\n        is_resign = self._game_result is None\n\n        if is_resign:\n            # For all clients that can leave, result of leaving the game either\n            # loss, or the client will ignore the result\n            self._game_result = {self._player_id: Result.Defeat}\n\n        try:\n            await self._execute(leave_game=sc_pb.RequestLeaveGame())\n        except ProtocolError:\n            if is_resign:\n                raise"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nquerying the pathing of a single resource.", "response": "async def query_pathing(\n        self, start: Union[Unit, Point2, Point3], end: Union[Point2, Point3]\n    ) -> Optional[Union[int, float]]:\n        \"\"\" Caution: returns 0 when path not found \"\"\"\n        assert isinstance(start, (Point2, Unit))\n        assert isinstance(end, Point2)\n        if isinstance(start, Point2):\n            result = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(\n                            start_pos=common_pb.Point2D(x=start.x, y=start.y),\n                            end_pos=common_pb.Point2D(x=end.x, y=end.y),\n                        )\n                    ]\n                )\n            )\n        else:\n            result = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(unit_tag=start.tag, end_pos=common_pb.Point2D(x=end.x, y=end.y))\n                    ]\n                )\n            )\n        distance = float(result.query.pathing[0].distance)\n        if distance <= 0.0:\n            return None\n        return distance"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def query_pathings(self, zipped_list: List[List[Union[Unit, Point2, Point3]]]) -> List[Union[float, int]]:\n        assert zipped_list, \"No zipped_list\"\n        assert isinstance(zipped_list, list), f\"{type(zipped_list)}\"\n        assert isinstance(zipped_list[0], list), f\"{type(zipped_list[0])}\"\n        assert len(zipped_list[0]) == 2, f\"{len(zipped_list[0])}\"\n        assert isinstance(zipped_list[0][0], (Point2, Unit)), f\"{type(zipped_list[0][0])}\"\n        assert isinstance(zipped_list[0][1], Point2), f\"{type(zipped_list[0][1])}\"\n        if isinstance(zipped_list[0][0], Point2):\n            results = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(\n                            start_pos=common_pb.Point2D(x=p1.x, y=p1.y), end_pos=common_pb.Point2D(x=p2.x, y=p2.y)\n                        )\n                        for p1, p2 in zipped_list\n                    ]\n                )\n            )\n        else:\n            results = await self._execute(\n                query=query_pb.RequestQuery(\n                    pathing=[\n                        query_pb.RequestQueryPathing(unit_tag=p1.tag, end_pos=common_pb.Point2D(x=p2.x, y=p2.y))\n                        for p1, p2 in zipped_list\n                    ]\n                )\n            )\n        results = [float(d.distance) for d in results.query.pathing]\n        return results", "response": "Query the pathings of the given list of units."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nquery available abilities of a set of units.", "response": "async def query_available_abilities(\n        self, units: Union[List[Unit], \"Units\"], ignore_resource_requirements: bool = False\n    ) -> List[List[AbilityId]]:\n        \"\"\" Query abilities of multiple units \"\"\"\n        if not isinstance(units, list):\n            \"\"\" Deprecated, accepting a single unit may be removed in the future, query a list of units instead \"\"\"\n            assert isinstance(units, Unit)\n            units = [units]\n            input_was_a_list = False\n        else:\n            input_was_a_list = True\n        assert units\n        result = await self._execute(\n            query=query_pb.RequestQuery(\n                abilities=[query_pb.RequestQueryAvailableAbilities(unit_tag=unit.tag) for unit in units],\n                ignore_resource_requirements=ignore_resource_requirements,\n            )\n        )\n        \"\"\" Fix for bots that only query a single unit \"\"\"\n        if not input_was_a_list:\n            return [[AbilityId(a.ability_id) for a in b.abilities] for b in result.query.abilities][0]\n        return [[AbilityId(a.ability_id) for a in b.abilities] for b in result.query.abilities]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def chat_send(self, message: str, team_only: bool):\n        ch = ChatChannel.Team if team_only else ChatChannel.Broadcast\n        await self._execute(\n            action=sc_pb.RequestAction(\n                actions=[sc_pb.Action(action_chat=sc_pb.ActionChat(channel=ch.value, message=message))]\n            )\n        )", "response": "Sends a message to the chat"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\nasync def debug_create_unit(self, unit_spawn_commands: List[List[Union[UnitTypeId, int, Point2, Point3]]]):\n        assert isinstance(unit_spawn_commands, list)\n        assert unit_spawn_commands\n        assert isinstance(unit_spawn_commands[0], list)\n        assert len(unit_spawn_commands[0]) == 4\n        assert isinstance(unit_spawn_commands[0][0], UnitTypeId)\n        assert unit_spawn_commands[0][1] > 0  # careful, in realtime=True this function may create more units\n        assert isinstance(unit_spawn_commands[0][2], (Point2, Point3))\n        assert 1 <= unit_spawn_commands[0][3] <= 2\n\n        await self._execute(\n            debug=sc_pb.RequestDebug(\n                debug=[\n                    debug_pb.DebugCommand(\n                        create_unit=debug_pb.DebugCreateUnit(\n                            unit_type=unit_type.value,\n                            owner=owner_id,\n                            pos=common_pb.Point2D(x=position.x, y=position.y),\n                            quantity=amount_of_units,\n                        )\n                    )\n                    for unit_type, amount_of_units, position, owner_id in unit_spawn_commands\n                ]\n            )\n        )", "response": "This function will create a new unit in the map."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove the camera to the target position", "response": "async def move_camera(self, position: Union[Unit, Point2, Point3]):\n        \"\"\" Moves camera to the target position \"\"\"\n        assert isinstance(position, (Unit, Point2, Point3))\n        if isinstance(position, Unit):\n            position = position.position\n        await self._execute(\n            action=sc_pb.RequestAction(\n                actions=[\n                    sc_pb.Action(\n                        action_raw=raw_pb.ActionRaw(\n                            camera_move=raw_pb.ActionRawCameraMove(\n                                center_world_space=common_pb.Point(x=position.x, y=position.y)\n                            )\n                        )\n                    )\n                ]\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nmove the camera to the target position using the spatial aciton interface.", "response": "async def move_camera_spatial(self, position: Union[Point2, Point3]):\n        \"\"\" Moves camera to the target position using the spatial aciton interface \"\"\"\n        from s2clientprotocol import spatial_pb2 as spatial_pb\n        assert isinstance(position, (Point2, Point3))\n        action = sc_pb.Action(\n            action_render=spatial_pb.ActionSpatial(\n                camera_move=spatial_pb.ActionSpatialCameraMove(\n                    center_minimap=common_pb.PointI(x=position.x, y=position.y)\n                )\n            )\n        )\n        await self._execute(action=sc_pb.RequestAction(actions=[action]))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsends debug text to the user.", "response": "async def debug_text(self, texts: Union[str, list], positions: Union[list, set], color=(0, 255, 0), size_px=16):\n        \"\"\" Deprecated, may be removed soon \"\"\"\n        if isinstance(positions, (set, list)):\n            if not positions:\n                return\n\n            if isinstance(texts, str):\n                texts = [texts] * len(positions)\n            assert len(texts) == len(positions)\n\n            await self._execute(\n                debug=sc_pb.RequestDebug(\n                    debug=[\n                        debug_pb.DebugCommand(\n                            draw=debug_pb.DebugDraw(\n                                text=[\n                                    debug_pb.DebugText(\n                                        text=t,\n                                        color=debug_pb.Color(r=color[0], g=color[1], b=color[2]),\n                                        world_pos=common_pb.Point(x=p.x, y=p.y, z=getattr(p, \"z\", 10)),\n                                        size=size_px,\n                                    )\n                                    for t, p in zip(texts, positions)\n                                ]\n                            )\n                        )\n                    ]\n                )\n            )\n        else:\n            await self.debug_text([texts], [positions], color)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndraws a text in the top left corner of the screen.", "response": "def debug_text_simple(self, text: str):\n        \"\"\" Draws a text in the top left corner of the screen (up to a max of 6 messages it seems). Don't forget to add 'await self._client.send_debug'. \"\"\"\n        self._debug_texts.append(self.to_debug_message(text))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef debug_text_screen(self, text: str, pos: Union[Point2, Point3, tuple, list], color=None, size: int = 8):\n        assert len(pos) >= 2\n        assert 0 <= pos[0] <= 1\n        assert 0 <= pos[1] <= 1\n        pos = Point2((pos[0], pos[1]))\n        self._debug_texts.append(self.to_debug_message(text, color, pos, size))", "response": "Draws a text on the screen with coordinates 0 < x y < 1."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndraw a text at a point.", "response": "def debug_text_world(self, text: str, pos: Union[Unit, Point2, Point3], color=None, size: int = 8):\n        \"\"\" Draws a text at Point3 position. Don't forget to add 'await self._client.send_debug'.\n        To grab a unit's 3d position, use unit.position3d\n        Usually the Z value of a Point3 is between 8 and 14 (except for flying units)\n        \"\"\"\n        if isinstance(pos, Point2) and not isinstance(pos, Point3):  # a Point3 is also a Point2\n            pos = Point3((pos.x, pos.y, 0))\n        self._debug_texts.append(self.to_debug_message(text, color, pos, size))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndraw a line from p0 to p1. Don t forget to add await self. _client. send_debug.", "response": "def debug_line_out(self, p0: Union[Unit, Point2, Point3], p1: Union[Unit, Point2, Point3], color=None):\n        \"\"\" Draws a line from p0 to p1. Don't forget to add 'await self._client.send_debug'. \"\"\"\n        self._debug_lines.append(\n            debug_pb.DebugLine(\n                line=debug_pb.Line(p0=self.to_debug_point(p0), p1=self.to_debug_point(p1)),\n                color=self.to_debug_color(color),\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndrawing a box with p_min and p_max as corners. Don t forget to add await self. _client. send_debug.", "response": "def debug_box_out(self, p_min: Union[Unit, Point2, Point3], p_max: Union[Unit, Point2, Point3], color=None):\n        \"\"\" Draws a box with p_min and p_max as corners. Don't forget to add 'await self._client.send_debug'. \"\"\"\n        self._debug_boxes.append(\n            debug_pb.DebugBox(\n                min=self.to_debug_point(p_min), max=self.to_debug_point(p_max), color=self.to_debug_color(color)\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndrawing a sphere at point p with radius r.", "response": "def debug_sphere_out(self, p: Union[Unit, Point2, Point3], r: Union[int, float], color=None):\n        \"\"\" Draws a sphere at point p with radius r. Don't forget to add 'await self._client.send_debug'. \"\"\"\n        self._debug_spheres.append(\n            debug_pb.DebugSphere(p=self.to_debug_point(p), r=r, color=self.to_debug_color(color))\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending the debug draw command to the user.", "response": "async def send_debug(self):\n        \"\"\" Sends the debug draw execution. Put this after your debug creation functions. \"\"\"\n        await self._execute(\n            debug=sc_pb.RequestDebug(\n                debug=[\n                    debug_pb.DebugCommand(\n                        draw=debug_pb.DebugDraw(\n                            text=self._debug_texts if self._debug_texts else None,\n                            lines=self._debug_lines if self._debug_lines else None,\n                            boxes=self._debug_boxes if self._debug_boxes else None,\n                            spheres=self._debug_spheres if self._debug_spheres else None,\n                        )\n                    )\n                ]\n            )\n        )\n        self._debug_texts.clear()\n        self._debug_lines.clear()\n        self._debug_boxes.clear()\n        self._debug_spheres.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef to_debug_color(self, color):\n        if color is None:\n            return debug_pb.Color(r=255, g=255, b=255)\n        else:\n            r = getattr(color, \"r\", getattr(color, \"x\", 255))\n            g = getattr(color, \"g\", getattr(color, \"y\", 255))\n            b = getattr(color, \"b\", getattr(color, \"z\", 255))\n            if max(r, g, b) <= 1:\n                r *= 255\n                g *= 255\n                b *= 255\n\n            return debug_pb.Color(r=int(r), g=int(g), b=int(b))", "response": "Convert color from hex to debug color"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a point to a debug point.", "response": "def to_debug_point(self, point: Union[Unit, Point2, Point3]) -> common_pb.Point:\n        \"\"\" Helper function for point conversion \"\"\"\n        if isinstance(point, Unit):\n            point = point.position3d\n        return common_pb.Point(x=point.x, y=point.y, z=getattr(point, \"z\", 0))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _distance_squared(self, p2: \"Point2\") -> Union[int, float]:\n        return (self[0] - p2[0]) ** 2 + (self[1] - p2[1]) ** 2", "response": "Function used to take the square root as the distance is proportionally the same."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef sort_by_distance(self, ps: Union[\"Units\", List[\"Point2\"]]) -> List[\"Point2\"]:\n        if len(ps) == 1:\n            return ps[0]\n        # if ps and all(isinstance(p, Point2) for p in ps):\n        #     return sorted(ps, key=lambda p: self._distance_squared(p))\n        return sorted(ps, key=lambda p: self._distance_squared(p.position))", "response": "This function sorts the target points by distance."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef closest(self, ps: Union[\"Units\", List[\"Point2\"], Set[\"Point2\"]]) -> Union[\"Unit\", \"Point2\"]:\n        assert ps\n        if len(ps) == 1:\n            return ps[0]\n        closest_distance_squared = math.inf\n        for p2 in ps:\n            p2pos = p2\n            if not isinstance(p2pos, Point2):\n                p2pos = p2.position\n            distance = (self[0] - p2pos[0]) ** 2 + (self[1] - p2pos[1]) ** 2\n            if distance < closest_distance_squared:\n                closest_distance_squared = distance\n                closest_element = p2\n        return closest_element", "response": "This function returns the closest unit to the given set of points."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef distance_to_closest(self, ps: Union[\"Units\", List[\"Point2\"], Set[\"Point2\"]]) -> Union[int, float]:\n        assert ps\n        closest_distance_squared = math.inf\n        for p2 in ps:\n            if not isinstance(p2, Point2):\n                p2 = p2.position\n            distance = (self[0] - p2[0]) ** 2 + (self[1] - p2[1]) ** 2\n            if distance < closest_distance_squared:\n                closest_distance_squared = distance\n        return closest_distance_squared ** 0.5", "response": "This function returns the distance to the closest point in the set of points."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef distance_to_furthest(self, ps: Union[\"Units\", List[\"Point2\"], Set[\"Point2\"]]) -> Union[int, float]:\n        assert ps\n        furthest_distance_squared = -math.inf\n        for p2 in ps:\n            if not isinstance(p2, Point2):\n                p2 = p2.position\n            distance = (self[0] - p2[0]) ** 2 + (self[1] - p2[1]) ** 2\n            if furthest_distance_squared < distance:\n                furthest_distance_squared = distance\n        return furthest_distance_squared ** 0.5", "response": "This function returns the distance to the furthest of the given set of points."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsquare distance to a point.", "response": "def distance2_to(self, other: \"Point2\"):\n        \"\"\"Squared distance to a point.\"\"\"\n        assert isinstance(other, Point2)\n        return (self[0] - other[0]) ** 2 + (self[1] - other[1]) ** 2"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the intersection of two circles.", "response": "def circle_intersection(self, p: \"Point2\", r: Union[int, float]) -> Set[\"Point2\"]:\n        \"\"\" self is point1, p is point2, r is the radius for circles originating in both points\n        Used in ramp finding \"\"\"\n        assert self != p\n        distanceBetweenPoints = self.distance_to(p)\n        assert r > distanceBetweenPoints / 2\n        # remaining distance from center towards the intersection, using pythagoras\n        remainingDistanceFromCenter = (r ** 2 - (distanceBetweenPoints / 2) ** 2) ** 0.5\n        # center of both points\n        offsetToCenter = Point2(((p.x - self.x) / 2, (p.y - self.y) / 2))\n        center = self.offset(offsetToCenter)\n\n        # stretch offset vector in the ratio of remaining distance from center to intersection\n        vectorStretchFactor = remainingDistanceFromCenter / (distanceBetweenPoints / 2)\n        v = offsetToCenter\n        offsetToCenterStretched = Point2((v.x * vectorStretchFactor, v.y * vectorStretchFactor))\n\n        # rotate vector by 90\u00b0 and -90\u00b0\n        vectorRotated1 = Point2((offsetToCenterStretched.y, -offsetToCenterStretched.x))\n        vectorRotated2 = Point2((-offsetToCenterStretched.y, offsetToCenterStretched.x))\n        intersect1 = center.offset(vectorRotated1)\n        intersect2 = center.offset(vectorRotated2)\n        return {intersect1, intersect2}"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef direction_vector(self, other: \"Point2\") -> \"Point2\":\n        return self.__class__((_sign(other.x - self.x), _sign(other.y - self.y)))", "response": "Converts a vector to a direction that can face vertically horizontally or diagonal or be zero."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the central point for the set of points in a.", "response": "def center(a: Union[Set[\"Point2\"], List[\"Point2\"]]) -> \"Point2\":\n        \"\"\" Returns the central point for points in list \"\"\"\n        s = Point2((0, 0))\n        for p in a:\n            s += p\n        return s / len(a)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the upper points of a ramp.", "response": "def upper(self) -> Set[Point2]:\n        \"\"\" Returns the upper points of a ramp. \"\"\"\n        max_height = max([self.height_at(p) for p in self._points])\n        return {p for p in self._points if self.height_at(p) == max_height}"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef upper2_for_ramp_wall(self) -> Set[Point2]:\n        if len(self.upper) > 5:\n            # NOTE: this was way too slow on large ramps\n            return set()  # HACK: makes this work for now\n            # FIXME: please do\n\n        upper2 = sorted(list(self.upper), key=lambda x: x.distance_to(self.bottom_center), reverse=True)\n        while len(upper2) > 2:\n            upper2.pop()\n        return set(upper2)", "response": "Returns the 2 upper ramp points of the main base ramp required for the supply depot and barracks placement properties used in this file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the barracks position in the middle of the 2 depots.", "response": "def barracks_in_middle(self) -> Point2:\n        \"\"\" Barracks position in the middle of the 2 depots \"\"\"\n        if len(self.upper2_for_ramp_wall) == 2:\n            points = self.upper2_for_ramp_wall\n            p1 = points.pop().offset((self.x_offset, self.y_offset))\n            p2 = points.pop().offset((self.x_offset, self.y_offset))\n            # Offset from top point to barracks center is (2, 1)\n            intersects = p1.circle_intersection(p2, 5 ** 0.5)\n            anyLowerPoint = next(iter(self.lower))\n            return max(intersects, key=lambda p: p.distance_to(anyLowerPoint))\n        raise Exception(\"Not implemented. Trying to access a ramp that has a wrong amount of upper points.\")"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding the 2 depot positions on the outside of the ramp.", "response": "def corner_depots(self) -> Set[Point2]:\n        \"\"\" Finds the 2 depot positions on the outside \"\"\"\n        if len(self.upper2_for_ramp_wall) == 2:\n            points = self.upper2_for_ramp_wall\n            p1 = points.pop().offset((self.x_offset, self.y_offset))  # still an error with pixelmap?\n            p2 = points.pop().offset((self.x_offset, self.y_offset))\n            center = p1.towards(p2, p1.distance_to(p2) / 2)\n            depotPosition = self.depot_in_middle\n            # Offset from middle depot to corner depots is (2, 1)\n            intersects = center.circle_intersection(depotPosition, 5 ** 0.5)\n            return intersects\n        raise Exception(\"Not implemented. Trying to access a ramp that has a wrong amount of upper points.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef barracks_can_fit_addon(self) -> bool:\n        # https://i.imgur.com/4b2cXHZ.png\n        if len(self.upper2_for_ramp_wall) == 2:\n            return self.barracks_in_middle.x + 1 > max(self.corner_depots, key=lambda depot: depot.x).x\n        raise Exception(\"Not implemented. Trying to access a ramp that has a wrong amount of upper points.\")", "response": "Test if a barracks can fit an addon at natural ramp."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef barracks_correct_placement(self) -> Point2:\n        if len(self.upper2_for_ramp_wall) == 2:\n            if self.barracks_can_fit_addon:\n                return self.barracks_in_middle\n            else:\n                return self.barracks_in_middle.offset((-2, 0))\n        raise Exception(\"Not implemented. Trying to access a ramp that has a wrong amount of upper points.\")", "response": "Corrected placement so that an addon can fit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfinding ramps by comparing heights.", "response": "def _find_ramps(self) -> List[Ramp]:\n        \"\"\"Calculate (self.pathing_grid - self.placement_grid) (for sets) and then find ramps by comparing heights.\"\"\"\n        rampDict = {\n            Point2((x, y)): self.pathing_grid[(x, y)] == 0 and self.placement_grid[(x, y)] == 0\n            for x in range(self.pathing_grid.width)\n            for y in range(self.pathing_grid.height)\n        }\n\n        rampPoints = {p for p in rampDict if rampDict[p]}  # filter only points part of ramp\n        rampGroups = self._find_groups(rampPoints)\n        return [Ramp(group, self) for group in rampGroups]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef split_camel_case(text) -> list:\n    return list(reduce(\n        lambda a, b: (a + [b] if b.isupper() else a[:-1] + [a[-1] + b]),\n        text,\n        []\n    ))", "response": "Splits words from CamelCase text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef tech_alias(self) -> Optional[List[UnitTypeId]]:\n        \"\"\" Building tech equality, e.g. Hive is the same as Lair and Hatchery \"\"\"\n        return_list = []\n        for tech_alias in self._proto.tech_alias:\n            if tech_alias in self._game_data.units:\n                return_list.append(UnitTypeId(tech_alias))\n        \"\"\" For Hive, this returns [UnitTypeId.Hatchery, UnitTypeId.Lair] \"\"\"\n        \"\"\" For SCV, this returns None \"\"\"\n        if return_list:\n            return return_list\n        return None", "response": "Return the list of tech aliases that are defined in the protocol."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unit_alias(self) -> Optional[UnitTypeId]:\n        if self._proto.unit_alias == 0:\n            return None\n        if self._proto.unit_alias not in self._game_data.units:\n            return None\n        \"\"\" For flying OrbitalCommand, this returns UnitTypeId.OrbitalCommand \"\"\"\n        return UnitTypeId(self._proto.unit_alias)", "response": "Returns the UnitTypeId for the given alias."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef property_cache_once_per_frame(f):\n    f.frame = -1\n    f.cache = None\n\n    @wraps(f)\n    def inner(self):\n        if f.frame != self.state.game_loop:\n            f.frame = self.state.game_loop\n            f.cache = None\n        if f.cache is None:\n            f.cache = f(self)\n        return f.cache\n\n    return property(inner)", "response": "A decorator that caches the return value for one game loop and clears it if it is accessed in a different game loop."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlaunching the analysis. this will start a total of `max_fetcher` threads.", "response": "def go(self):\n        \"\"\"\n        Launch the analysis.\n\n        this will start a total of `max_fetcher` threads.\n        \"\"\"\n        myandro = self.settings[\"my\"]\n\n        def worker(idx, q):\n            \"\"\"\n            Worker Thread\n            \"\"\"\n            l.debug(\"Running worker-%d\" % idx)\n\n            while True:\n                a, d, dx, axmlobj, arscobj = None, None, None, None, None\n                try:\n                    filename, fileraw = q.get()\n                    id_file = zlib.adler32(fileraw)\n\n                    l.debug(\"(worker-%d) get %s %d\" % (idx, filename, id_file))\n\n                    # FIXME: If the try/catch crashes before this line, there\n                    # will be no logf to put into finish.\n                    logf = self.settings[\"log\"](id_file, filename)\n\n                    is_analysis_dex, is_analysis_adex = True, True\n                    l.debug(\"(worker-%d) filtering file %d\" % (idx, id_file))\n                    # TODO: This information should probably also go into the logf?\n                    filter_file_ret, filter_file_type = myandro.filter_file(logf, fileraw)\n\n                    if filter_file_ret:\n                        l.debug(\"(worker-%d) analysis %s\" % (id_file, filter_file_type))\n\n                        if filter_file_type == \"APK\":\n                            a = myandro.create_apk(logf, fileraw)\n                            is_analysis_dex = myandro.analysis_apk(logf, a)\n                            # TODO: Support multidex here\n                            fileraw = a.get_dex()\n                            filter_file_type = androconf.is_android_raw(fileraw)\n\n                        elif filter_file_type == \"AXML\":\n                            axmlobj = myandro.create_axml(logf, fileraw)\n                            # TODO: the return value of analysis_axml is not checked\n                            myandro.analysis_axml(logf, axmlobj)\n\n                        elif filter_file_type == \"ARSC\":\n                            arscobj = myandro.create_arsc(logf, fileraw)\n                            # TODO: the return value of analysis_arsc is not checked\n                            myandro.analysis_arsc(logf, arscobj)\n\n                        if is_analysis_dex and filter_file_type == \"DEX\":\n                            d = myandro.create_dex(logf, fileraw)\n                            is_analysis_adex = myandro.analysis_dex(logf, d)\n\n                        elif is_analysis_dex and filter_file_type == \"DEY\":\n                            d = myandro.create_dey(logf, fileraw)\n                            is_analysis_adex = myandro.analysis_dey(logf, d)\n\n                        if is_analysis_adex and d:\n                            # TODO: Support multidex here\n                            dx = myandro.create_adex(logf, d)\n                            # TODO: The return value of analysis_adex is not checked\n                            myandro.analysis_adex(logf, dx)\n\n                        # TODO: this is called also if AXML or ARSC is set...\n                        myandro.analysis_app(logf, a, d, dx)\n                        myandro.finish(logf)\n\n                except Exception as why:\n                    myandro.crash(logf, why)\n                    # FIXME: finish is called here in any case of an exception\n                    # but is only called if filter_file_ret is true above.\n                    myandro.finish(logf)\n\n                del a, d, dx, axmlobj, arscobj\n                q.task_done()\n\n        q = queue.Queue(self.settings[\"max_fetcher\"])\n\n        for i in range(self.settings[\"max_fetcher\"]):\n            t = threading.Thread(target=worker, args=[i, q])\n            t.daemon = True\n            t.start()\n\n        # FIXME: Busy waiting with sleep...\n        terminated = True\n        while terminated:\n            terminated = myandro.fetcher(q)\n\n            try:\n                if terminated:\n                    time.sleep(10)\n            except KeyboardInterrupt:\n                terminated = False\n\n        q.join()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef filter_file(self, log, fileraw):\n        file_type = androconf.is_android_raw(fileraw)\n        if file_type in [\"APK\", \"DEX\", \"DEY\", \"AXML\", \"ARSC\"]:\n            return True, file_type\n        return False, None", "response": "This method is called to filter the file based on the file type"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_adex(self, log, dexobj):\n        vm_analysis = analysis.Analysis(dexobj)\n        vm_analysis.create_xref()\n        return vm_analysis", "response": "This method creates an Analysis object which corresponds to a unique app\n       ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef build_path(graph, node1, node2, path=None):\n    if path is None:\n        path = []\n    if node1 is node2:\n        return path\n    path.append(node2)\n    for pred in graph.all_preds(node2):\n        if pred in path:\n            continue\n        build_path(graph, node1, pred, path)\n    return path", "response": "Builds the path from node1 to node2."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmerge the inner class of a class.", "response": "def merge_inner(clsdict):\n    \"\"\"\n    Merge the inner class(es) of a class:\n    e.g class A { ... } class A$foo{ ... } class A$bar{ ... }\n    ==> class A { class foo{...} class bar{...} ... }\n    \"\"\"\n    samelist = False\n    done = {}\n    while not samelist:\n        samelist = True\n        classlist = list(clsdict.keys())\n        for classname in classlist:\n            parts_name = classname.rsplit('$', 1)\n            if len(parts_name) > 1:\n                mainclass, innerclass = parts_name\n                innerclass = innerclass[:-1]  # remove ';' of the name\n                mainclass += ';'\n                if mainclass in clsdict:\n                    clsdict[mainclass].add_subclass(innerclass,\n                                                    clsdict[classname])\n                    clsdict[classname].name = innerclass\n                    done[classname] = clsdict[classname]\n                    del clsdict[classname]\n                    samelist = False\n                elif mainclass in done:\n                    cls = done[mainclass]\n                    cls.add_subclass(innerclass, clsdict[classname])\n                    clsdict[classname].name = innerclass\n                    done[classname] = done[mainclass]\n                    del clsdict[classname]\n                    samelist = False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve the java type of a descriptor.", "response": "def get_type(atype, size=None):\n    \"\"\"\n    Retrieve the java type of a descriptor (e.g : I)\n    \"\"\"\n    res = TYPE_DESCRIPTOR.get(atype)\n    if res is None:\n        if atype[0] == 'L':\n            if atype.startswith('Ljava/lang'):\n                res = atype[1:-1].lstrip('java/lang/').replace('/', '.')\n            else:\n                res = atype[1:-1].replace('/', '.')\n        elif atype[0] == '[':\n            if size is None:\n                res = '%s[]' % get_type(atype[1:])\n            else:\n                res = '{}[{}]'.format(get_type(atype[1:]), size)\n        else:\n            res = atype\n            logger.debug('Unknown descriptor: \"%s\".', atype)\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_params_type(descriptor):\n    params = descriptor.split(')')[0][1:].split()\n    if params:\n        return [param for param in params]\n    return []", "response": "Return the parameters type of a descriptor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_png(cls_name, meth_name, graph, dir_name='graphs2'):\n    m_name = ''.join(x for x in meth_name if x.isalnum())\n    name = ''.join((cls_name.split('/')[-1][:-1], '#', m_name))\n    graph.draw(name, dir_name)", "response": "Creates a PNG from a given graph."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef read(filename, binary=True):\n    with open(filename, 'rb' if binary else 'r') as f:\n        return f.read()", "response": "Open and read a file\n   "}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the string representation of a X509 Certificate name object in a human readable form.", "response": "def get_certificate_name_string(name, short=False, delimiter=', '):\n    \"\"\"\n    Format the Name type of a X509 Certificate in a human readable form.\n\n    :param name: Name object to return the DN from\n    :param short: Use short form (default: False)\n    :param delimiter: Delimiter string or character between two parts (default: ', ')\n\n    :type name: dict or :class:`asn1crypto.x509.Name`\n    :type short: boolean\n    :type delimiter: str\n\n    :rtype: str\n    \"\"\"\n    if isinstance(name, asn1crypto.x509.Name):\n        name = name.native\n\n    # For the shortform, we have a lookup table\n    # See RFC4514 for more details\n    _ = {\n        'business_category': (\"businessCategory\", \"businessCategory\"),\n        'serial_number': (\"serialNumber\", \"serialNumber\"),\n        'country_name': (\"C\", \"countryName\"),\n        'postal_code': (\"postalCode\", \"postalCode\"),\n        'state_or_province_name': (\"ST\", \"stateOrProvinceName\"),\n        'locality_name': (\"L\", \"localityName\"),\n        'street_address': (\"street\", \"streetAddress\"),\n        'organization_name': (\"O\", \"organizationName\"),\n        'organizational_unit_name': (\"OU\", \"organizationalUnitName\"),\n        'title': (\"title\", \"title\"),\n        'common_name': (\"CN\", \"commonName\"),\n        'initials': (\"initials\", \"initials\"),\n        'generation_qualifier': (\"generationQualifier\", \"generationQualifier\"),\n        'surname': (\"SN\", \"surname\"),\n        'given_name': (\"GN\", \"givenName\"),\n        'name': (\"name\", \"name\"),\n        'pseudonym': (\"pseudonym\", \"pseudonym\"),\n        'dn_qualifier': (\"dnQualifier\", \"dnQualifier\"),\n        'telephone_number': (\"telephoneNumber\", \"telephoneNumber\"),\n        'email_address': (\"E\", \"emailAddress\"),\n        'domain_component': (\"DC\", \"domainComponent\"),\n        'name_distinguisher': (\"nameDistinguisher\", \"nameDistinguisher\"),\n        'organization_identifier': (\"organizationIdentifier\", \"organizationIdentifier\"),\n    }\n    return delimiter.join([\"{}={}\".format(_.get(attr, (attr, attr))[0 if short else 1], name[attr]) for attr in name])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nshow color - picker dialog to select color.", "response": "def onColorPicker(self):\n        \"\"\"\n        Show color-picker dialog to select color.\n\n        Qt will use the native dialog by default.\n\n        \"\"\"\n        dlg = QtGui.QColorDialog(QtGui.QColor(self._color), None)\n\n        # if self._color:\n        #    dlg.setCurrentColor(QtGui.QColor(self._color))\n\n        if dlg.exec_():\n            self.setColor(dlg.currentColor().name())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nanalyzes an android application and setup all stuff for a more quickly APK.", "response": "def AnalyzeAPK(_file, session=None, raw=False):\n    \"\"\"\n    Analyze an android application and setup all stuff for a more quickly\n    analysis!\n    If session is None, no session is used at all. This is the default\n    behaviour.\n    If you like to continue your work later, it might be a good idea to use a\n    session.\n    A default session can be created by using :meth:`~get_default_session`.\n\n    :param _file: the filename of the android application or a buffer which represents the application\n    :type _file: string (for filename) or bytes (for raw)\n    :param session: A session (default: None)\n    :param raw: boolean if raw bytes are supplied instead of a filename\n    :rtype: return the :class:`~androguard.core.bytecodes.apk.APK`, list of :class:`~androguard.core.bytecodes.dvm.DalvikVMFormat`, and :class:`~androguard.core.analysis.analysis.Analysis` objects\n    \"\"\"\n    log.debug(\"AnalyzeAPK\")\n\n    if session:\n        log.debug(\"Using existing session {}\".format(session))\n        if raw:\n            data = _file\n            filename = hashlib.md5(_file).hexdigest()\n        else:\n            with open(_file, \"rb\") as fd:\n                data = fd.read()\n                filename = _file\n\n        digest = session.add(filename, data)\n        return session.get_objects_apk(filename, digest)\n    else:\n        log.debug(\"Analysing without session\")\n        a = APK(_file, raw=raw)\n        # FIXME: probably it is not necessary to keep all DalvikVMFormats, as\n        # they are already part of Analysis. But when using sessions, it works\n        # this way...\n        d = []\n        dx = Analysis()\n        for dex in a.get_all_dex():\n            df = DalvikVMFormat(dex, using_api=a.get_target_sdk_version())\n            dx.add(df)\n            d.append(df)\n            df.set_decompiler(decompiler.DecompilerDAD(d, dx))\n\n        dx.create_xref()\n\n        return a, d, dx"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef AnalyzeDex(filename, session=None):\n    log.debug(\"AnalyzeDex\")\n\n    if not session:\n        session = get_default_session()\n\n    with open(filename, \"rb\") as fd:\n        data = fd.read()\n\n    return session.addDEX(filename, data)", "response": "Analyze an android dex file and setup all stuff for a more quickly analysis"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nanalyze an android odex file and setup all stuff for a more quickly analysis", "response": "def AnalyzeODex(filename, session=None):\n    \"\"\"\n    Analyze an android odex file and setup all stuff for a more quickly analysis !\n\n    :param filename: the filename of the android dex file or a buffer which represents the dex file\n    :type filename: string\n    :param session: The Androguard Session to add the ODex to (default: None)\n\n    :rtype: return a tuple of (sha256hash, :class:`DalvikOdexVMFormat`, :class:`Analysis`)\n    \"\"\"\n    log.debug(\"AnalyzeODex\")\n\n    if not session:\n        session = get_default_session()\n\n    with open(filename, \"rb\") as fd:\n        data = fd.read()\n\n    return session.addDEY(filename, data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun the decompiler on a specific analysis of a single object.", "response": "def RunDecompiler(d, dx, decompiler_name):\n    \"\"\"\n    Run the decompiler on a specific analysis\n\n    :param d: the DalvikVMFormat object\n    :type d: :class:`DalvikVMFormat` object\n    :param dx: the analysis of the format\n    :type dx: :class:`VMAnalysis` object\n    :param decompiler: the type of decompiler to use (\"dad\", \"dex2jad\", \"ded\")\n    :type decompiler: string\n    \"\"\"\n    if decompiler_name is not None:\n        log.debug(\"Decompiler ...\")\n        decompiler_name = decompiler_name.lower()\n        # TODO put this into the configuration object and make it more dynamic\n        # e.g. detect new decompilers and so on...\n        if decompiler_name == \"dex2jad\":\n            d.set_decompiler(decompiler.DecompilerDex2Jad(\n                d,\n                androconf.CONF[\"BIN_DEX2JAR\"],\n                androconf.CONF[\"BIN_JAD\"],\n                androconf.CONF[\"TMP_DIRECTORY\"]))\n        elif decompiler_name == \"dex2fernflower\":\n            d.set_decompiler(decompiler.DecompilerDex2Fernflower(\n                d,\n                androconf.CONF[\"BIN_DEX2JAR\"],\n                androconf.CONF[\"BIN_FERNFLOWER\"],\n                androconf.CONF[\"OPTIONS_FERNFLOWER\"],\n                androconf.CONF[\"TMP_DIRECTORY\"]))\n        elif decompiler_name == \"ded\":\n            d.set_decompiler(decompiler.DecompilerDed(\n                d,\n                androconf.CONF[\"BIN_DED\"],\n                androconf.CONF[\"TMP_DIRECTORY\"]))\n        elif decompiler_name == \"jadx\":\n            d.set_decompiler(decompiler.DecompilerJADX(d, dx, jadx=androconf.CONF[\"BIN_JADX\"]))\n        else:\n            d.set_decompiler(decompiler.DecompilerDAD(d, dx))"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsigning an APK file using jarsigner", "response": "def sign_apk(filename, keystore, storepass):\n    \"\"\"\n    Use jarsigner to sign an APK file.\n\n    :param filename: APK file on disk to sign (path)\n    :param keystore: path to keystore\n    :param storepass: your keystorage passphrase\n    \"\"\"\n    from subprocess import Popen, PIPE, STDOUT\n    # TODO use apksigner instead of jarsigner\n    cmd = Popen([androconf.CONF[\"BIN_JARSIGNER\"], \"-sigalg\", \"MD5withRSA\",\n                 \"-digestalg\", \"SHA1\", \"-storepass\", storepass, \"-keystore\",\n                 keystore, filename, \"alias_name\"],\n                stdout=PIPE,\n                stderr=STDOUT)\n    stdout, stderr = cmd.communicate()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nclean a filename version which has no characters in it which are forbidden.", "response": "def clean_file_name(filename, unique=True, replace=\"_\", force_nt=False):\n    \"\"\"\n    Return a filename version, which has no characters in it which are forbidden.\n    On Windows these are for example <, /, ?, ...\n\n    The intention of this function is to allow distribution of files to different OSes.\n\n    :param filename: string to clean\n    :param unique: check if the filename is already taken and append an integer to be unique (default: True)\n    :param replace: replacement character. (default: '_')\n    :param force_nt: Force shortening of paths like on NT systems (default: False)\n    :return: clean string\n    \"\"\"\n\n    if re.match(r'[<>:\"/\\\\|?* .\\x00-\\x1f]', replace):\n        raise ValueError(\"replacement character is not allowed!\")\n\n    path, fname = os.path.split(filename)\n    # For Windows see: https://msdn.microsoft.com/en-us/library/windows/desktop/aa365247(v=vs.85).aspx\n    # Other operating systems seems to be more tolerant...\n\n    # Not allowed filenames, attach replace character if necessary\n    if re.match(r'(CON|PRN|AUX|NUL|COM[1-9]|LPT[1-9])', fname):\n        fname += replace\n\n    # reserved characters\n    fname = re.sub(r'[<>:\"/\\\\|?*\\x00-\\x1f]', replace, fname)\n    # Do not end with dot or space\n    fname = re.sub(r'[ .]$', replace, fname)\n\n    if force_nt or os.name == 'nt':\n        PATH_MAX_LENGTH = 230  # give extra space for other stuff...\n        # Check filename length limit, usually a problem on older Windows versions\n        if len(fname) > PATH_MAX_LENGTH:\n            if \".\" in fname:\n                f, ext = fname.rsplit(\".\", 1)\n                fname = \"{}.{}\".format(f[:PATH_MAX_LENGTH-(len(ext)+1)], ext)\n            else:\n                fname = fname[:PATH_MAX_LENGTH]\n\n        # Special behaviour... On Windows, there is also a problem with the maximum path length in explorer.exe\n        # maximum length is limited to 260 chars, so use 250 to have room for other stuff\n        if len(os.path.abspath(os.path.join(path, fname))) > 250:\n            fname = fname[:250 - (len(os.path.abspath(path)) + 1)]\n\n    if unique:\n        counter = 0\n        origname = fname\n        while os.path.isfile(os.path.join(path, fname)):\n            if \".\" in fname:\n                # assume extension\n                f, ext = origname.rsplit(\".\", 1)\n                fname = \"{}_{}.{}\".format(f, counter, ext)\n            else:\n                fname = \"{}_{}\".format(origname, counter)\n            counter += 1\n\n    return os.path.join(path, fname)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload the Permissions for the given API level.", "response": "def load_permissions(apilevel, permtype='permissions'):\n    \"\"\"\n    Load the Permissions for the given apilevel.\n\n    The permissions lists are generated using this tool: https://github.com/U039b/aosp_permissions_extraction\n\n    Has a fallback to select the maximum or minimal available API level.\n    For example, if 28 is requested but only 26 is available, 26 is returned.\n    If 5 is requested but 16 is available, 16 is returned.\n\n    If an API level is requested which is in between of two API levels we got,\n    the lower level is returned. For example, if 5,6,7,10 is available and 8 is\n    requested, 7 is returned instead.\n\n    :param apilevel:  integer value of the API level\n    :param permtype: either load permissions (:code:`'permissions'`) or\n    permission groups (:code:`'groups'`)\n    :return: a dictionary of {Permission Name: {Permission info}\n    \"\"\"\n\n    if permtype not in ['permissions', 'groups']:\n        raise ValueError(\"The type of permission list is not known.\")\n\n    # Usually apilevel is supplied as string...\n    apilevel = int(apilevel)\n\n    root = os.path.dirname(os.path.realpath(__file__))\n    permissions_file = os.path.join(root, \"aosp_permissions\", \"permissions_{}.json\".format(apilevel))\n\n    levels = filter(lambda x: re.match(r'^permissions_\\d+\\.json$', x), os.listdir(os.path.join(root, \"aosp_permissions\")))\n    levels = list(map(lambda x: int(x[:-5].split('_')[1]), levels))\n\n    if not levels:\n        log.error(\"No Permissions available, can not load!\")\n        return {}\n\n    log.debug(\"Available API levels: {}\".format(\", \".join(map(str, sorted(levels)))))\n\n    if not os.path.isfile(permissions_file):\n        if apilevel > max(levels):\n            log.warning(\"Requested API level {} is larger than maximum we have, returning API level {} instead.\".format(apilevel, max(levels)))\n            return load_permissions(max(levels), permtype)\n        if apilevel < min(levels):\n            log.warning(\"Requested API level {} is smaller than minimal we have, returning API level {} instead.\".format(apilevel, max(levels)))\n            return load_permissions(min(levels), permtype)\n\n        # Missing level between existing ones, return the lower level\n        lower_level = max(filter(lambda x: x < apilevel, levels))\n        log.warning(\"Requested API Level could not be found, using {} instead\".format(lower_level))\n        return load_permissions(lower_level, permtype)\n\n    with open(permissions_file, \"r\") as fp:\n        return json.load(fp)[permtype]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nload the API and Permission mappings for the requested API level.", "response": "def load_permission_mappings(apilevel):\n    \"\"\"\n    Load the API/Permission mapping for the requested API level.\n    If the requetsed level was not found, None is returned.\n\n    :param apilevel: integer value of the API level, i.e. 24 for Android 7.0\n    :return: a dictionary of {MethodSignature: [List of Permissions]}\n    \"\"\"\n    root = os.path.dirname(os.path.realpath(__file__))\n    permissions_file = os.path.join(root, \"api_permission_mappings\", \"permissions_{}.json\".format(apilevel))\n\n    if not os.path.isfile(permissions_file):\n        return {}\n\n    with open(permissions_file, \"r\") as fp:\n        return json.load(fp)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef simplify(graph):\n    redo = True\n    while redo:\n        redo = False\n        node_map = {}\n        to_update = set()\n        for node in graph.nodes[:]:\n            if node.type.is_stmt and node in graph:\n                sucs = graph.all_sucs(node)\n                if len(sucs) != 1:\n                    continue\n                suc = sucs[0]\n                if len(node.get_ins()) == 0:\n                    if any(pred.type.is_switch\n                           for pred in graph.all_preds(node)):\n                        continue\n                    if node is suc:\n                        continue\n                    node_map[node] = suc\n\n                    for pred in graph.all_preds(node):\n                        pred.update_attribute_with(node_map)\n                        if node not in graph.sucs(pred):\n                            graph.add_catch_edge(pred, suc)\n                            continue\n                        graph.add_edge(pred, suc)\n                    redo = True\n                    if node is graph.entry:\n                        graph.entry = suc\n                    graph.remove_node(node)\n                elif (suc.type.is_stmt and len(graph.all_preds(suc)) == 1 and\n                          not (suc in graph.catch_edges) and not (\n                            (node is suc) or (suc is graph.entry))):\n                    ins_to_merge = suc.get_ins()\n                    node.add_ins(ins_to_merge)\n                    for var in suc.var_to_declare:\n                        node.add_variable_declaration(var)\n                    new_suc = graph.sucs(suc)[0]\n                    if new_suc:\n                        graph.add_edge(node, new_suc)\n                    for exception_suc in graph.catch_edges.get(suc, []):\n                        graph.add_catch_edge(node, exception_suc)\n                    redo = True\n                    graph.remove_node(suc)\n            else:\n                to_update.add(node)\n        for node in to_update:\n            node.update_attribute_with(node_map)", "response": "Simplify the CFG by merging and deleting statement nodes when possible."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef dom_lt(graph):\n\n    def _dfs(v, n):\n        semi[v] = n = n + 1\n        vertex[n] = label[v] = v\n        ancestor[v] = 0\n        for w in graph.all_sucs(v):\n            if not semi[w]:\n                parent[w] = v\n                n = _dfs(w, n)\n            pred[w].add(v)\n        return n\n\n    def _compress(v):\n        u = ancestor[v]\n        if ancestor[u]:\n            _compress(u)\n            if semi[label[u]] < semi[label[v]]:\n                label[v] = label[u]\n            ancestor[v] = ancestor[u]\n\n    def _eval(v):\n        if ancestor[v]:\n            _compress(v)\n            return label[v]\n        return v\n\n    def _link(v, w):\n        ancestor[w] = v\n\n    parent, ancestor, vertex = {}, {}, {}\n    label, dom = {}, {}\n    pred, bucket = defaultdict(set), defaultdict(set)\n\n    # Step 1:\n    semi = {v: 0 for v in graph.nodes}\n    n = _dfs(graph.entry, 0)\n    for i in range(n, 1, -1):\n        w = vertex[i]\n        # Step 2:\n        for v in pred[w]:\n            u = _eval(v)\n            y = semi[w] = min(semi[w], semi[u])\n        bucket[vertex[y]].add(w)\n        pw = parent[w]\n        _link(pw, w)\n        # Step 3:\n        bpw = bucket[pw]\n        while bpw:\n            v = bpw.pop()\n            u = _eval(v)\n            dom[v] = u if semi[u] < semi[v] else pw\n    # Step 4:\n    for i in range(2, n + 1):\n        w = vertex[i]\n        dw = dom[w]\n        if dw != vertex[semi[w]]:\n            dom[w] = dom[dw]\n    dom[graph.entry] = None\n    return dom", "response": "Dominator algorithm from Lengauer - Tarjan"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nconstruct a CFG from a basic block.", "response": "def construct(start_block, vmap, exceptions):\n    \"\"\"\n    Constructs a CFG\n\n    :param androguard.core.analysis.analysis.DVMBasicBlock start_block: The startpoint\n    :param vmap: variable mapping\n    :param exceptions: list of androguard.core.analysis.analysis.ExceptionAnalysis\n\n    :rtype: Graph\n    \"\"\"\n    bfs_blocks = bfs(start_block)\n\n    graph = Graph()\n    gen_ret = GenInvokeRetName()\n\n    # Construction of a mapping of basic blocks into Nodes\n    block_to_node = {}\n\n    exceptions_start_block = []\n    for exception in exceptions:\n        for _, _, block in exception.exceptions:\n            exceptions_start_block.append(block)\n\n    for block in bfs_blocks:\n        node = make_node(graph, block, block_to_node, vmap, gen_ret)\n        graph.add_node(node)\n\n    graph.entry = block_to_node[start_block]\n    del block_to_node, bfs_blocks\n\n    graph.compute_rpo()\n    graph.number_ins()\n\n    for node in graph.rpo:\n        preds = [pred for pred in graph.all_preds(node) if pred.num < node.num]\n        if preds and all(pred.in_catch for pred in preds):\n            node.in_catch = True\n\n    # Create a list of Node which are 'return' node\n    # There should be one and only one node of this type\n    # If this is not the case, try to continue anyway by setting the exit node\n    # to the one which has the greatest RPO number (not necessarily the case)\n    lexit_nodes = [node for node in graph if node.type.is_return]\n\n    if len(lexit_nodes) > 1:\n        # Not sure that this case is possible...\n        logger.error('Multiple exit nodes found !')\n        graph.exit = graph.rpo[-1]\n    elif len(lexit_nodes) < 1:\n        # A method can have no return if it has throw statement(s) or if its\n        # body is a while(1) whitout break/return.\n        logger.debug('No exit node found !')\n    else:\n        graph.exit = lexit_nodes[0]\n\n    return graph"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef remove_node(self, node):\n        preds = self.reverse_edges.get(node, [])\n        for pred in preds:\n            self.edges[pred].remove(node)\n\n        succs = self.edges.get(node, [])\n        for suc in succs:\n            self.reverse_edges[suc].remove(node)\n\n        exc_preds = self.reverse_catch_edges.pop(node, [])\n        for pred in exc_preds:\n            self.catch_edges[pred].remove(node)\n\n        exc_succs = self.catch_edges.pop(node, [])\n        for suc in exc_succs:\n            self.reverse_catch_edges[suc].remove(node)\n\n        self.nodes.remove(node)\n        if node in self.rpo:\n            self.rpo.remove(node)\n        del node", "response": "Removes the node from the graph removes also all connections."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the RPO traversal of the tree.", "response": "def compute_rpo(self):\n        \"\"\"\n        Number the nodes in reverse post order.\n        An RPO traversal visit as many predecessors of a node as possible\n        before visiting the node itself.\n        \"\"\"\n        nb = len(self.nodes) + 1\n        for node in self.post_order():\n            node.num = nb - node.po\n        self.rpo = sorted(self.nodes, key=lambda n: n.num)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef post_order(self):\n        def _visit(n, cnt):\n            visited.add(n)\n            for suc in self.all_sucs(n):\n                if suc not in visited:\n                    for cnt, s in _visit(suc, cnt):\n                        yield cnt, s\n            n.po = cnt\n            yield cnt + 1, n\n\n        visited = set()\n        for _, node in _visit(self.entry, 1):\n            yield node", "response": "Yields the nodes of the graph in post - order."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwrites the current graph as a PNG file.", "response": "def draw(self, name, dname, draw_branches=True):\n        \"\"\"\n        Writes the current graph as a PNG file\n\n        :param str name: filename (without .png)\n        :param str dname: directory of the output png\n        :param draw_branches:\n        :return:\n        \"\"\"\n        from pydot import Dot, Edge\n        import os\n\n        g = Dot()\n        g.set_node_defaults(color='lightgray',\n                            style='filled',\n                            shape='box',\n                            fontname='Courier',\n                            fontsize='10')\n        for node in sorted(self.nodes, key=lambda x: x.num):\n            if draw_branches and node.type.is_cond:\n                g.add_edge(Edge(str(node), str(node.true), color='green'))\n                g.add_edge(Edge(str(node), str(node.false), color='red'))\n            else:\n                for suc in self.sucs(node):\n                    g.add_edge(Edge(str(node), str(suc), color='blue'))\n            for except_node in self.catch_edges.get(node, []):\n                g.add_edge(Edge(str(node),\n                                str(except_node),\n                                color='black',\n                                style='dashed'))\n\n        g.write(os.path.join(dname, '%s.png' % name), format='png')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the AndroidManifest. xml file and save it to file.", "response": "def axml(input_, output, file_, resource):\n    \"\"\"\n    Parse the AndroidManifest.xml.\n\n    Parsing is either direct or from a given APK and prints in XML format or\n    saves to file.\n\n    This tool can also be used to process any AXML encoded file, for example\n    from the layout directory.\n\n    Example:\n\n    \\b\n        $ androguard axml AndroidManifest.xml\n    \"\"\"\n    if file_ is not None and input_ is not None:\n        print(\"Can not give --input and positional argument! \"\n              \"Please use only one of them!\")\n        sys.exit(1)\n\n    if file_ is None and input_ is None:\n        print(\"Give one file to decode!\")\n        sys.exit(1)\n\n    if file_ is not None:\n        androaxml_main(file_, output, resource)\n    elif input_ is not None:\n        androaxml_main(input_, output, resource)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef arsc(input_,\n         file_,\n         output,\n         package,\n         locale,\n         type_,\n         id_,\n         list_packages,\n         list_locales,\n         list_types):\n    \"\"\"\n    Decode resources.arsc either directly from a given file or from an APK.\n\n    Example:\n\n    \\b\n        $ androguard arsc app.apk\n    \"\"\"\n    from androguard.core import androconf\n    from androguard.core.bytecodes import apk\n\n    if file_ and input_:\n        print(\"Can not give --input and positional argument! \"\n              \"Please use only one of them!\",\n              file=sys.stderr)\n        sys.exit(1)\n\n    if not input_ and not file_:\n        print(\"Give one file to decode!\", file=sys.stderr)\n        sys.exit(1)\n\n    if input_:\n        fname = input_\n    else:\n        fname = file_\n\n    ret_type = androconf.is_android(fname)\n    if ret_type == \"APK\":\n        a = apk.APK(fname)\n        arscobj = a.get_android_resources()\n        if not arscobj:\n            print(\"The APK does not contain a resources file!\", file=sys.stderr)\n            sys.exit(0)\n    elif ret_type == \"ARSC\":\n        with open(fname, 'rb') as fp:\n            arscobj = apk.ARSCParser(fp.read())\n            if not arscobj:\n                print(\"The resources file seems to be invalid!\", file=sys.stderr)\n                sys.exit(1)\n    else:\n        print(\"Unknown file type!\", file=sys.stderr)\n        sys.exit(1)\n\n    if id_:\n        # Strip the @, if any\n        if id_[0] == \"@\":\n            id_ = id_[1:]\n        try:\n            i_id = int(id_, 16)\n        except ValueError:\n            print(\"ID '{}' could not be parsed! have you supplied the correct hex ID?\".format(id_))\n            sys.exit(1)\n\n        name = arscobj.get_resource_xml_name(i_id)\n        if not name:\n            print(\"Specified resource was not found!\")\n            sys.exit(1)\n\n        print(\"@{:08x} resolves to '{}'\".format(i_id, name))\n        print()\n\n        # All the information is in the config.\n        # we simply need to get the actual value of the entry\n        for config, entry in arscobj.get_resolved_res_configs(i_id):\n            print(\"{} = '{}'\".format(config.get_qualifier() if not config.is_default() else \"<default>\", entry))\n\n        sys.exit(0)\n\n    if list_packages:\n        print(\"\\n\".join(arscobj.get_packages_names()))\n        sys.exit(0)\n\n    if list_locales:\n        for p in arscobj.get_packages_names():\n            print(\"In Package:\", p)\n            print(\"\\n\".join(map(lambda x: \"  \\\\x00\\\\x00\"\n                                if x == \"\\x00\\x00\"\n                                else \"  {}\".format(x),\n                                     sorted(arscobj.get_locales(p)))))\n        sys.exit(0)\n\n    if list_types:\n        for p in arscobj.get_packages_names():\n            print(\"In Package:\", p)\n            for locale in sorted(arscobj.get_locales(p)):\n                print(\"  In Locale: {}\".format(\"\\\\x00\\\\x00\"\n                      if locale == \"\\x00\\x00\" else locale))\n                print(\"\\n\".join(map(\"    {}\".format,\n                                    sorted(arscobj.get_types(p, locale)))))\n        sys.exit(0)\n\n    androarsc_main(arscobj,\n                   outp=output,\n                   package=package,\n                   typ=type_,\n                   locale=locale)", "response": "Decode resources. arsc directly from a given file."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating a call graph and export it into a graph format. classnames are found in the type \"Lfoo/bar/bla;\". Example: \\b $ androguard cg APK", "response": "def cg(output,\n       show,\n       verbose,\n       classname,\n       methodname,\n       descriptor,\n       accessflag,\n       no_isolated,\n       apk):\n    \"\"\"\n    Create a call graph and export it into a graph format.\n\n    classnames are found in the type \"Lfoo/bar/bla;\".\n\n    Example:\n\n    \\b\n        $ androguard cg APK\n    \"\"\"\n    androcg_main(verbose=verbose,\n                 APK=apk,\n                 classname=classname,\n                 methodname=methodname,\n                 descriptor=descriptor,\n                 accessflag=accessflag,\n                 no_isolated=no_isolated,\n                 show=show,\n                 output=output)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef sign(hash_, print_all_hashes, show, apk):\n    androsign_main(apk, hash_, print_all_hashes, show)", "response": "Return the fingerprint(s) of all certificates inside an APK."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef apkid(apks):\n    import json\n    import logging\n    logging.getLogger(\"androguard.axml\").setLevel(logging.ERROR)\n    results = dict()\n    for apk in apks:\n        results[apk] = androguard.core.bytecodes.apk.get_apkid(apk)\n    print(json.dumps(results, indent=2))", "response": "Return the packageName versionCode versionName per APK as JSON."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nload a module by name and search path", "response": "def load_module(module_name, file_path):\n    \"\"\"\n    Load a module by name and search path\n\n    Returns None if Module could not be loaded.\n    \"\"\"\n    if sys.version_info >= (3,5,):\n        import importlib.util\n\n        spec = importlib.util.spec_from_file_location(module_name, file_path)\n        if not spec:\n            return\n\n        module = importlib.util.module_from_spec(spec)\n        spec.loader.exec_module(module)\n\n        return module\n    else:\n        import imp\n        mod = imp.load_source(module_name, file_path)\n        return mod"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef showStatus(self, msg):\n        log.debug(msg)\n        self.statusBar().showMessage(msg)", "response": "Helper function to display a message in the status bar."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef setupEmptyTree(self):\n        if hasattr(self, \"tree\"):\n            del self.tree\n        self.tree = QtWidgets.QTreeWidget(self)\n        self.tree.header().close()", "response": "Setup empty tree at startup."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nopens a source window for the current class and if it does not already open it creates a new one and sets the current tab to the source window s title and title.", "response": "def openSourceWindow(self, current_class, method=None):\n        \"\"\"Main function to open a decompile source window\n           It checks if it already opened and open that tab,\n           otherwise, initialize a new window.\n        \"\"\"\n        log.debug(\"openSourceWindow for %s\" % current_class)\n\n        sourcewin = self.getMeOpenedWindowIfExists(current_class.current_title + \"(S)\")\n        if not sourcewin:\n            current_filename = self.session.get_filename_by_class(current_class)\n            current_digest = self.session.get_digest_by_class(current_class)\n\n            sourcewin = SourceWindow(win=self,\n                                     current_class=current_class,\n                                     current_title=current_class.current_title + \"(S)\",\n                                     current_filename=current_filename,\n                                     current_digest=current_digest,\n                                     session=self.session)\n            sourcewin.reload_java_sources()\n            self.central.addTab(sourcewin, sourcewin.title)\n            self.central.setTabToolTip(self.central.indexOf(sourcewin),\n                                       sourcewin.title)\n\n        if method:\n            sourcewin.browse_to_method(method)\n\n        self.central.setCurrentWidget(sourcewin)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef name_to_androguard(n):\n    if n == \"\":\n        return \"\"\n    is_array = \"\"\n    # FIXME what about n-dimensional arrays?\n    if n.startswith(\"[\"):\n        is_array = \"[\"\n        n = n[1:]\n    elif n.endswith(\"[]\"):\n        # Another special array type...\n        # Probably a bug? See\n        if n[:-2] in TYPE_DESCRIPTOR:\n            return \"[{}\".format(n[0])\n        else:\n            n = n[:-2]\n            is_array = \"[\"\n    if n in R_TYPE_DESCRIPTOR:\n        return \"{}{}\".format(is_array, R_TYPE_DESCRIPTOR[n])\n    else:\n        # assume class\n        return \"{}L{};\".format(is_array, n.replace(\".\", \"/\"))", "response": "Convert a object or primitive name into an androguard syntax."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nconverting a line of axplorer format into androguard method signature + permission", "response": "def convert_name(s):\n    \"\"\"\n    Converts a line of axplorer format into androguard method signature + permission\n    :param s:\n    :return:\n    \"\"\"\n    m = re.compile(r\"^(.*)\\.(.*)\\((.*)\\)(.*)  ::  (.*)$\")\n    res = m.search(s)\n    if res:\n        clname, methodname, all_args, ret, perm = res.groups()\n        args = \" \".join(map(name_to_androguard, all_args.split(\",\")))\n\n        clname = name_to_androguard(clname)\n        ret = name_to_androguard(ret)\n\n        # perm is actually a comma separated list of permissions\n        return \"{}-{}-({}){}\".format(clname, methodname, args, ret), perm.split(\", \")\n    else:\n        raise ValueError(\"what?\")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngenerating the permission mappings from an AndroidManifest. xml file.", "response": "def generate_mappings(axplorerdir=\"libraries/axplorer\", outfolder=\"androguard/core/api_specific_resources\"):\n    \"\"\"\n    Generate the permission mappings from a axplorer root dir into a given folder.\n    For each API Level, separate json file will be created.\n\n    :param axplorerdir: path to the axplorer dir\n    :param outfolder: path to the folder where the resulting json files are put\n    \"\"\"\n    res = dict()\n    for root, dirs, files in os.walk(os.path.join(axplorerdir, \"permissions\")):\n        for fi in files:\n            if fi.startswith(\"cp-map-\"):\n                # We currently do not parse those files\n                print(\"ignored {}\".format(fi))\n                continue\n            elif fi.startswith(\"framework-map-\") or fi.startswith(\"sdk-map-\"):\n                sdk_version = fi.rsplit(\"-\", 1)[1][:-4]\n                print(\"Found file:\", fi, \"for API level:\", sdk_version)\n                if sdk_version not in res:\n                    res[sdk_version] = defaultdict(list)\n                with open(os.path.join(root, fi), \"r\") as f:\n                    for line in f.read().splitlines():\n                        meth, perm = convert_name(line)\n                        for p in perm:\n                            res[sdk_version][meth].append(p)\n\n    for api, v in res.items():\n        with open(os.path.join(outfolder, \"api_permission_mappings\", \"permissions_{}.json\".format(api)), \"w\") as fp:\n            json.dump(v, fp, indent=\"    \")\n\n    # Next, we generate the permission lists, based on the AndroidManifest.xml files.\n    # Thise files typically reside in the platform_framework_base repository\n    # in the folder \"master/core/res/\". This AndroidManifest.xml file contains\n    # all the permissions that are defined by the android system.\n    # Of course, there are even more files (platform packages)\n    # but the question is always, if these should be put into this list as well...\n    # In this case, we collect all permissions that are extracted by axplorer as well.\n    res = defaultdict(dict)\n    XMLNS = '{http://schemas.android.com/apk/res/android}'\n\n    re_api = re.compile(r\".*manifests[\\\\/]api-([0-9]+)\")\n    for root, dirs, files in os.walk(os.path.join(axplorerdir, \"manifests\")):\n        for fi in files:\n            reres = re_api.match(root)\n            if not reres:\n                continue\n            api = int(reres[1])\n            p = os.path.join(root, fi)\n\n            with open(p, \"rb\") as f:\n                tree = etree.XML(f.read())\n            matches = tree.xpath('permission')\n\n            def _get_attrib(elem, attr):\n                if XMLNS + attr in elem.attrib:\n                    return elem.attrib[XMLNS + attr]\n                else:\n                    return \"\"\n\n            for match in matches:\n                name = match.attrib[XMLNS + \"name\"]\n                d = dict(permissionGroup=_get_attrib(match, \"permissionGroup\"),\n                         description=_get_attrib(match, \"description\"),\n                         protectionLevel=_get_attrib(match, \"protectionLevel\"),\n                         label=_get_attrib(match, \"label\"))\n\n                if name in res[api]:\n                    print(\"Potential collision of permission in api {}: {}\".format(api, name))\n                res[api][name] = d\n\n    for api, v in res.items():\n        print(\"Permissions for API: {}, found {} permissions\".format(api, len(v)))\n        with open(os.path.join(outfolder, \"aosp_permissions\", \"permissions_{}.json\".format(api)), \"w\") as fp:\n            json.dump(v, fp, indent=\"    \")"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the class with the given name.", "response": "def get_class(self, class_name):\n        \"\"\"\n        Return the :class:`DvClass` with the given name\n\n        The name is partially matched against the known class names and the first result is returned.\n        For example, the input `foobar` will match on Lfoobar/bla/foo;\n\n        :param str class_name:\n        :return: the class matching on the name\n        :rtype: DvClass\n        \"\"\"\n        for name, klass in self.classes.items():\n            # TODO why use the name partially?\n            if class_name in name:\n                if isinstance(klass, DvClass):\n                    return klass\n                dvclass = self.classes[name] = DvClass(klass, self.vma)\n                return dvclass"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef process(self):\n        for name, klass in self.classes.items():\n            logger.debug('Processing class: %s', name)\n            if isinstance(klass, DvClass):\n                klass.process()\n            else:\n                dvclass = self.classes[name] = DvClass(klass, self.vma)\n                dvclass.process()", "response": "Processes all classes inside the machine."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nrunning process and show_source after each other.", "response": "def process_and_show(self):\n        \"\"\"\n        Run :meth:`process` and :meth:`show_source` after each other.\n        \"\"\"\n        for name, klass in sorted(self.classes.items()):\n            logger.debug('Processing class: %s', name)\n            if not isinstance(klass, DvClass):\n                klass = DvClass(klass, self.vma)\n            klass.process()\n            klass.show_source()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_ast(self):\n        ret = dict()\n        for name, cls in sorted(self.classes.items()):\n            logger.debug('Processing class: %s', name)\n            if not isinstance(cls, DvClass):\n                cls = DvClass(cls, self.vma)\n            cls.process(doAST=True)\n            ret[name] = cls.get_ast()\n        return ret", "response": "Processes each class with AST enabled and returns a dictionary with all single ASTs\n        Classnames as keys."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef disable_colors():\n    for i in CONF[\"COLORS\"]:\n        if isinstance(CONF[\"COLORS\"][i], dict):\n            for j in CONF[\"COLORS\"][i]:\n                CONF[\"COLORS\"][i][j] = Color.normal\n        else:\n            CONF[\"COLORS\"][i] = Color.normal", "response": "Disable colors from the output."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef remove_colors():\n    for i in CONF[\"COLORS\"]:\n        if isinstance(CONF[\"COLORS\"][i], dict):\n            for j in CONF[\"COLORS\"][i]:\n                CONF[\"COLORS\"][i][j] = \"\"\n        else:\n            CONF[\"COLORS\"][i] = \"\"", "response": "Remove colors from the output"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef is_android_raw(raw):\n    val = None\n\n    # We do not check for META-INF/MANIFEST.MF,\n    # as you also want to analyze unsigned APKs...\n    # AndroidManifest.xml should be in every APK.\n    # classes.dex and resources.arsc are not required!\n    # if raw[0:2] == b\"PK\" and b'META-INF/MANIFEST.MF' in raw:\n    # TODO this check might be still invalid. A ZIP file with stored APK inside would match as well.\n    # probably it would be better to rewrite this and add more sanity checks.\n    if raw[0:2] == b\"PK\" and b'AndroidManifest.xml' in raw:\n        val = \"APK\"\n    elif raw[0:3] == b\"dex\":\n        val = \"DEX\"\n    elif raw[0:3] == b\"dey\":\n        val = \"DEY\"\n    elif raw[0:4] == b\"\\x03\\x00\\x08\\x00\" or raw[0:4] == b\"\\x00\\x00\\x08\\x00\":\n        val = \"AXML\"\n    elif raw[0:4] == b\"\\x02\\x00\\x0C\\x00\":\n        val = \"ARSC\"\n\n    return val", "response": "Returns a string that describes the type of file for common Android base files."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nenable log messages on stdout and set the logging level.", "response": "def show_logging(level=logging.INFO):\n    \"\"\"\n    enable log messages on stdout\n\n    We will catch all messages here! From all loggers...\n    \"\"\"\n    logger = logging.getLogger()\n\n    h = logging.StreamHandler(stream=sys.stderr)\n    h.setFormatter(logging.Formatter(fmt=\"[%(levelname)-8s] %(name)s: %(message)s\"))\n\n    logger.addHandler(h)\n    logger.setLevel(level)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef rrmdir(directory):\n    for root, dirs, files in os.walk(directory, topdown=False):\n        for name in files:\n            os.remove(os.path.join(root, name))\n        for name in dirs:\n            os.rmdir(os.path.join(root, name))\n    os.rmdir(directory)", "response": "Recursivly delete a directory containing all the iCal tables and the iCal tables."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nconverts a color string into a tuple.", "response": "def make_color_tuple(color):\n    \"\"\"\n    turn something like \"#000000\" into 0,0,0\n    or \"#FFFFFF into \"255,255,255\"\n    \"\"\"\n    R = color[1:3]\n    G = color[3:5]\n    B = color[5:7]\n\n    R = int(R, 16)\n    G = int(G, 16)\n    B = int(B, 16)\n\n    return R, G, B"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nwrap for interpolate_tuple that accepts colors as html (\"#CCCCC\") and such as html (\"#CCCCC\" )", "response": "def color_range(startcolor, goalcolor, steps):\n    \"\"\"\n    wrapper for interpolate_tuple that accepts colors as html (\"#CCCCC\" and such)\n    \"\"\"\n    start_tuple = make_color_tuple(startcolor)\n    goal_tuple = make_color_tuple(goalcolor)\n\n    return interpolate_tuple(start_tuple, goal_tuple, steps)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef load_api_specific_resource_module(resource_name, api=None):\n    loader = dict(aosp_permissions=load_permissions,\n                  api_permission_mappings=load_permission_mappings)\n\n    if resource_name not in loader:\n        raise InvalidResourceError(\"Invalid Resource '{}', not in [{}]\".format(resource_name, \", \".join(loader.keys())))\n\n    if not api:\n        api = CONF[\"DEFAULT_API\"]\n\n    ret = loader[resource_name](api)\n\n    if ret == {}:\n        # No API mapping found, return default\n        log.warning(\"API mapping for API level {} was not found! \"\n                    \"Returning default, which is API level {}\".format(api, CONF['DEFAULT_API']))\n        ret = loader[resource_name](CONF['DEFAULT_API'])\n\n    return ret", "response": "Load the module from the JSON files and return a dict which might be empty\n   "}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndraw the current entry in the current color.", "response": "def draw(self):\n        qp = QtGui.QPainter()\n        qp.begin(self.qpix)\n\n        qp.fillRect(0, 0, self.width, self.height, self.backgroundBrush)\n        qp.setPen(self.textPen)\n        qp.setFont(self.font)\n\n        cemu = ConsoleEmulator(qp, self.height // self.fontHeight, self.width // self.fontWidth)\n\n        dword = self.dataModel.getDWORD(self.viewMode.getCursorAbsolutePosition(), asString=True)\n        if dword is None:\n            dword = '----'\n\n        sd = 'DWORD: {}'.format(dword)\n\n        pos = 'POS: {:08x}'.format(self.viewMode.getCursorAbsolutePosition())\n\n        qword = self.dataModel.getQWORD(self.viewMode.getCursorAbsolutePosition(), asString=True)\n        if qword is None:\n            qword = '----'\n        sq = 'QWORD: {}'.format(qword)\n\n        byte = self.dataModel.getBYTE(self.viewMode.getCursorAbsolutePosition(), asString=True)\n        if byte is None:\n            byte = '-'\n\n        sb = 'BYTE: {}'.format(byte)\n\n        cemu.writeAt(1, 0, pos)\n        cemu.writeAt(17, 0, sd)\n        cemu.writeAt(35, 0, sq)\n        cemu.writeAt(62, 0, sb)\n\n        qp.drawLine(15 * self.fontWidth + 5, 0, 15 * self.fontWidth + 5, 50)\n        qp.drawLine(33 * self.fontWidth + 5, 0, 33 * self.fontWidth + 5, 50)\n        qp.drawLine(59 * self.fontWidth + 5, 0, 59 * self.fontWidth + 5, 50)\n        qp.drawLine(71 * self.fontWidth + 5, 0, 71 * self.fontWidth + 5, 50)\n\n        if self.viewMode.selector.getCurrentSelection():\n            u, v = self.viewMode.selector.getCurrentSelection()\n            if u != v:\n                pen = QtGui.QPen(QtGui.QColor(51, 153, 255), 0, QtCore.Qt.SolidLine)\n                qp.setPen(pen)\n\n                cemu.writeAt(73, 0, 'Selection: ')\n                cemu.write('{:x}:{}'.format(u, v - u))\n        else:\n            pen = QtGui.QPen(QtGui.QColor(128, 128, 128), 0, QtCore.Qt.SolidLine)\n            qp.setPen(pen)\n\n            cemu.writeAt(73, 0, '<no selection>')\n\n        \"\"\"\n        qp.drawLine(self.fontWidth*(len(pos) + 1) + 15, 0, self.fontWidth*(len(pos) + 1) + 15, 50)\n        qp.drawLine(self.fontWidth*(len(pos + sd) + 1) + 3*15, 0, self.fontWidth*(len(pos + sd) + 1) + 3*15, 50)\n        qp.drawLine(self.fontWidth*(len(pos + sd + sq) + 1) + 5*15, 0, self.fontWidth*(len(pos + sd + sq) + 1) + 5*15, 50)\n        qp.drawLine(self.fontWidth*(len(pos + sd + sq + sb) + 1) + 8*15, 0, self.fontWidth*(len(pos + sd + sq + sb) + 1) + 8*15, 50)\n        \"\"\"\n        # qp.drawLine(270, 0, 270, 50)\n        # qp.drawLine(480, 0, 480, 50)\n        # qp.drawLine(570, 0, 570, 50)\n        \"\"\"\n        # position\n        qp.drawText(0 + 5, self.fontHeight, pos)\n        # separator\n        qp.drawLine(120, 0, 120, 50)\n\n        # dword\n        qp.drawText(130 + 5, self.fontHeight, sd)\n        # separator\n        qp.drawLine(270, 0, 270, 50)\n\n        # qword\n        qp.drawText(280 + 5, self.fontHeight, sq)\n        # separator\n        qp.drawLine(480, 0, 480, 50)\n\n        # byte\n        qp.drawText(490 + 5, self.fontHeight, sb)\n        # separator\n        qp.drawLine(570, 0, 570, 50)\n        \"\"\"\n\n        qp.end()\n\n        pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates the DU chain of the instruction located at loc.", "response": "def update_chain(graph, loc, du, ud):\n    \"\"\"\n    Updates the DU chain of the instruction located at loc such that there is\n    no more reference to it so that we can remove it.\n    When an instruction is found to be dead (i.e it has no side effect, and the\n    register defined is not used) we have to update the DU chain of all the\n    variables that may me used by the dead instruction.\n    \"\"\"\n    ins = graph.get_ins_from_loc(loc)\n    for var in ins.get_used_vars():\n        # We get the definition points of the current variable\n        for def_loc in set(ud[var, loc]):\n            # We remove the use of the variable at loc from the DU chain of\n            # the variable definition located at def_loc\n            du[var, def_loc].remove(loc)\n            ud[var, loc].remove(def_loc)\n            if not ud.get((var, loc)):\n                ud.pop((var, loc))\n            # If the DU chain of the defined variable is now empty, this means\n            # that we may have created a new dead instruction, so we check that\n            # the instruction has no side effect and we update the DU chain of\n            # the new dead instruction, and we delete it.\n            # We also make sure that def_loc is not < 0. This is the case when\n            # the current variable is a method parameter.\n            if def_loc >= 0 and not du[var, def_loc]:\n                du.pop((var, def_loc))\n                def_ins = graph.get_ins_from_loc(def_loc)\n                if def_ins.is_call():\n                    def_ins.remove_defined_var()\n                elif def_ins.has_side_effect():\n                    continue\n                else:\n                    update_chain(graph, def_loc, du, ud)\n                    graph.remove_ins(def_loc)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nrun a dead code elimination pass. Instructions are checked to be dead. If it is the case, we remove them and we update the DU & UD chains of its variables to check for further dead instructions.", "response": "def dead_code_elimination(graph, du, ud):\n    \"\"\"\n    Run a dead code elimination pass.\n    Instructions are checked to be dead. If it is the case, we remove them and\n    we update the DU & UD chains of its variables to check for further dead\n    instructions.\n    \"\"\"\n    for node in graph.rpo:\n        for i, ins in node.get_loc_with_ins():\n            reg = ins.get_lhs()\n            if reg is not None:\n                # If the definition is not used, we check that the instruction\n                # has no side effect. If there is one and this is a call, we\n                # remove only the unused defined variable. else, this is\n                # something like an array access, so we do nothing.\n                # Otherwise (no side effect) we can remove the instruction from\n                # the node.\n                if (reg, i) not in du:\n                    if ins.is_call():\n                        ins.remove_defined_var()\n                    elif ins.has_side_effect():\n                        continue\n                    else:\n                        # We can delete the instruction. First update the DU\n                        # chain of the variables used by the instruction to\n                        # `let them know` that they are not used anymore by the\n                        # deleted instruction.\n                        # Then remove the instruction.\n                        update_chain(graph, i, du, ud)\n                        graph.remove_ins(i)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nclears the path between two locations loc1 and loc2.", "response": "def clear_path(graph, reg, loc1, loc2):\n    \"\"\"\n    Check that the path from loc1 to loc2 is clear.\n    We have to check that there is no side effect between the two location\n    points. We also have to check that the variable `reg` is not redefined\n    along one of the possible pathes from loc1 to loc2.\n    \"\"\"\n    logger.debug('clear_path: reg(%s), loc1(%s), loc2(%s)', reg, loc1, loc2)\n    node1 = graph.get_node_from_loc(loc1)\n    node2 = graph.get_node_from_loc(loc2)\n    # If both instructions are in the same node, we only have to check that the\n    # path is clear inside the node\n    if node1 is node2:\n        return clear_path_node(graph, reg, loc1 + 1, loc2)\n\n    # If instructions are in different nodes, we also have to check the nodes\n    # in the path between the two locations.\n    if not clear_path_node(graph, reg, loc1 + 1, node1.ins_range[1]):\n        return False\n    path = build_path(graph, node1, node2)\n    for node in path:\n        locs = node.ins_range\n        end_loc = loc2 if (locs[0] <= loc2 <= locs[1]) else locs[1]\n        if not clear_path_node(graph, reg, locs[0], end_loc):\n            return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\npropagate the temporary registers between instructions and remove them.", "response": "def register_propagation(graph, du, ud):\n    \"\"\"\n    Propagate the temporary registers between instructions and remove them if\n    necessary.\n    We process the nodes of the graph in reverse post order. For each\n    instruction in the node, we look at the variables that it uses. For each of\n    these variables we look where it is defined and if we can replace it with\n    its definition.\n    We have to be careful to the side effects some instructions may have.\n    To do the propagation, we use the computed DU and UD chains.\n    \"\"\"\n    change = True\n    while change:\n        change = False\n        for node in graph.rpo:\n            for i, ins in node.get_loc_with_ins():\n                logger.debug('Treating instruction %d: %s', i, ins)\n                logger.debug('  Used vars: %s', ins.get_used_vars())\n                for var in ins.get_used_vars():\n                    # Get the list of locations this variable is defined at.\n                    locs = ud[var, i]\n                    logger.debug('    var %s defined in lines %s', var, locs)\n                    # If the variable is uniquely defined for this instruction\n                    # it may be eligible for propagation.\n                    if len(locs) != 1:\n                        continue\n\n                    loc = locs[0]\n                    # Methods parameters are defined with a location < 0.\n                    if loc < 0:\n                        continue\n                    orig_ins = graph.get_ins_from_loc(loc)\n                    logger.debug('     -> %s', orig_ins)\n                    logger.debug('     -> DU(%s, %s) = %s', var, loc,\n                                 du[var, loc])\n\n                    # We defined some instructions as not propagable.\n                    # Actually this is the case only for array creation\n                    # (new foo[x])\n                    if not orig_ins.is_propagable():\n                        logger.debug('    %s not propagable...', orig_ins)\n                        continue\n\n                    if not orig_ins.get_rhs().is_const():\n                        # We only try to propagate constants and definition\n                        # points which are used at only one location.\n                        if len(du[var, loc]) > 1:\n                            logger.debug('       => variable has multiple uses'\n                                         ' and is not const => skip')\n                            continue\n\n                        # We check that the propagation is safe for all the\n                        # variables that are used in the instruction.\n                        # The propagation is not safe if there is a side effect\n                        # along the path from the definition of the variable\n                        # to its use in the instruction, or if the variable may\n                        # be redifined along this path.\n                        safe = True\n                        orig_ins_used_vars = orig_ins.get_used_vars()\n                        logger.debug('    variables used by the original '\n                                     'instruction: %s', orig_ins_used_vars)\n                        for var2 in orig_ins_used_vars:\n                            # loc is the location of the defined variable\n                            # i is the location of the current instruction\n                            if not clear_path(graph, var2, loc, i):\n                                safe = False\n                                break\n                        if not safe:\n                            logger.debug('Propagation NOT SAFE')\n                            continue\n\n                    # We also check that the instruction itself is\n                    # propagable. If the instruction has a side effect it\n                    # cannot be propagated if there is another side effect\n                    # along the path\n                    if orig_ins.has_side_effect():\n                        if not clear_path(graph, None, loc, i):\n                            logger.debug('        %s has side effect and the '\n                                         'path is not clear !', orig_ins)\n                            continue\n\n                    logger.debug('     => Modification of the instruction!')\n                    logger.debug('      - BEFORE: %s', ins)\n                    ins.replace(var, orig_ins.get_rhs())\n                    logger.debug('      -> AFTER: %s', ins)\n                    logger.debug('\\t UD(%s, %s) : %s', var, i, ud[var, i])\n                    ud[var, i].remove(loc)\n                    logger.debug('\\t    -> %s', ud[var, i])\n                    if len(ud[var, i]) == 0:\n                        ud.pop((var, i))\n                    for var2 in orig_ins.get_used_vars():\n                        # We update the UD chain of the variables we\n                        # propagate. We also have to take the\n                        # definition points of all the variables used\n                        # by the instruction and update the DU chain\n                        # with this information.\n                        old_ud = ud.get((var2, loc))\n                        logger.debug('\\t  ud(%s, %s) = %s', var2, loc, old_ud)\n                        # If the instruction use the same variable\n                        # multiple times, the second+ time the ud chain\n                        # will be None because already treated.\n                        if old_ud is None:\n                            continue\n                        ud[var2, i].extend(old_ud)\n                        logger.debug('\\t  - ud(%s, %s) = %s', var2, i,\n                                     ud[var2, i])\n                        ud.pop((var2, loc))\n\n                        for def_loc in old_ud:\n                            du[var2, def_loc].remove(loc)\n                            du[var2, def_loc].append(i)\n\n                    new_du = du[var, loc]\n                    logger.debug('\\t new_du(%s, %s): %s', var, loc, new_du)\n                    new_du.remove(i)\n                    logger.debug('\\t    -> %s', new_du)\n                    if not new_du:\n                        logger.debug('\\t  REMOVING INS %d', loc)\n                        du.pop((var, loc))\n                        graph.remove_ins(loc)\n                        change = True"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nbuilds the Def - Use and Use - Def chains of the variables of the current method.", "response": "def build_def_use(graph, lparams):\n    \"\"\"\n    Builds the Def-Use and Use-Def (DU/UD) chains of the variables of the\n    method.\n    \"\"\"\n    analysis = reach_def_analysis(graph, lparams)\n\n    UD = defaultdict(list)\n    for node in graph.rpo:\n        for i, ins in node.get_loc_with_ins():\n            for var in ins.get_used_vars():\n                # var not in analysis.def_to_loc: test that the register\n                # exists. It is possible that it is not the case, when a\n                # variable is of a type which is stored on multiple registers\n                # e.g: a 'double' stored in v3 is also present in v4, so a call\n                # to foo(v3), will in fact call foo(v3, v4).\n                if var not in analysis.def_to_loc:\n                    continue\n                ldefs = analysis.defs[node]\n                prior_def = -1\n                for v in ldefs.get(var, set()):\n                    if prior_def < v < i:\n                        prior_def = v\n                if prior_def >= 0:\n                    UD[var, i].append(prior_def)\n                else:\n                    intersect = analysis.def_to_loc[var].intersection(\n                        analysis.R[node])\n                    UD[var, i].extend(intersect)\n    DU = defaultdict(list)\n    for var_loc, defs_loc in UD.items():\n        var, loc = var_loc\n        for def_loc in defs_loc:\n            DU[var, def_loc].append(loc)\n\n    return UD, DU"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncomputing the intervals of the graph.", "response": "def intervals(graph):\n    \"\"\"\n    Compute the intervals of the graph\n    Returns\n    interval_graph: a graph of the intervals of G\n    interv_heads: a dict of (header node, interval)\n    \"\"\"\n    interval_graph = Graph()  # graph of intervals\n    heads = [graph.entry]  # list of header nodes\n    interv_heads = {}  # interv_heads[i] = interval of header i\n    processed = {i: False for i in graph}\n    edges = defaultdict(list)\n\n    while heads:\n        head = heads.pop(0)\n\n        if not processed[head]:\n            processed[head] = True\n            interv_heads[head] = Interval(head)\n\n            # Check if there is a node which has all its predecessor in the\n            # current interval. If there is, add that node to the interval and\n            # repeat until all the possible nodes have been added.\n            change = True\n            while change:\n                change = False\n                for node in graph.rpo[1:]:\n                    if all(\n                                    p in interv_heads[head] for p in graph.all_preds(node)):\n                        change |= interv_heads[head].add_node(node)\n\n            # At this stage, a node which is not in the interval, but has one\n            # of its predecessor in it, is the header of another interval. So\n            # we add all such nodes to the header list.\n            for node in graph:\n                if node not in interv_heads[head] and node not in heads:\n                    if any(\n                                    p in interv_heads[head] for p in graph.all_preds(node)):\n                        edges[interv_heads[head]].append(node)\n                        assert (node not in heads)\n                        heads.append(node)\n\n            interval_graph.add_node(interv_heads[head])\n            interv_heads[head].compute_end(graph)\n\n    # Edges is a mapping of 'Interval -> [header nodes of interval successors]'\n    for interval, heads in edges.items():\n        for head in heads:\n            interval_graph.add_edge(interval, interv_heads[head])\n\n    interval_graph.entry = graph.entry.interval\n    if graph.exit:\n        interval_graph.exit = graph.exit.interval\n\n    return interval_graph, interv_heads"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ncomputes the derived sequence of a graph G", "response": "def derived_sequence(graph):\n    \"\"\"\n    Compute the derived sequence of the graph G\n    The intervals of G are collapsed into nodes, intervals of these nodes are\n    built, and the process is repeated iteratively until we obtain a single\n    node (if the graph is not irreducible)\n    \"\"\"\n    deriv_seq = [graph]\n    deriv_interv = []\n    single_node = False\n\n    while not single_node:\n\n        interv_graph, interv_heads = intervals(graph)\n        deriv_interv.append(interv_heads)\n\n        single_node = len(interv_graph) == 1\n        if not single_node:\n            deriv_seq.append(interv_graph)\n\n        graph = interv_graph\n        graph.compute_rpo()\n\n    return deriv_seq, deriv_interv"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the Java source code of a whole class", "response": "def get_source_class(self, _class):\n        \"\"\"\n        Return the Java source code of a whole class\n\n        :param _class: `ClassDefItem` object, to get the source from\n        :return:\n        \"\"\"\n        if not _class.get_name() in self.classes:\n            return \"\"\n        return self.classes[_class.get_name()]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ntest if any class in a DalvikVMObject uses ASCII Obfuscation.", "response": "def is_ascii_obfuscation(vm):\n    \"\"\"\n    Tests if any class inside a DalvikVMObject\n    uses ASCII Obfuscation (e.g. UTF-8 Chars in Classnames)\n\n    :param vm: `DalvikVMObject`\n    :return: True if ascii obfuscation otherwise False\n    \"\"\"\n    for classe in vm.get_classes():\n        if is_ascii_problem(classe.get_name()):\n            return True\n        for method in classe.get_methods():\n            if is_ascii_problem(method.get_name()):\n                return True\n    return False"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef show(self):\n        args, ret = self.method.get_descriptor()[1:].split(\")\")\n        if self.code:\n            # We patch the descriptor here and add the registers, if code is available\n            args = args.split(\" \")\n\n            reg_len = self.code.get_registers_size()\n            nb_args = len(args)\n\n            start_reg = reg_len - nb_args\n            args = [\"{} v{}\".format(a, start_reg + i) for i, a in enumerate(args)]\n\n        print(\"METHOD {} {} {} ({}){}\".format(\n              self.method.get_class_name(),\n              self.method.get_access_flags_string(),\n              self.method.get_name(),\n              \", \".join(args), ret))\n        bytecode.PrettyShow(self, self.basic_blocks.gets(), self.method.notes)", "response": "Prints the content of this method to stdout."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns True if the method seems to be an Android API method.", "response": "def is_android_api(self):\n        \"\"\"\n        Returns True if the method seems to be an Android API method.\n\n        This method might be not very precise unless an list of known API methods\n        is given.\n\n        :return: boolean\n        \"\"\"\n        if not self.is_external():\n            # Method must be external to be an API\n            return False\n\n        # Packages found at https://developer.android.com/reference/packages.html\n        api_candidates = [\"Landroid/\", \"Lcom/android/internal/util\", \"Ldalvik/\", \"Ljava/\", \"Ljavax/\", \"Lorg/apache/\",\n                          \"Lorg/json/\", \"Lorg/w3c/dom/\", \"Lorg/xml/sax\", \"Lorg/xmlpull/v1/\", \"Ljunit/\"]\n\n        if self.apilist:\n            # FIXME: This will not work... need to introduce a name for lookup (like EncodedMethod.__str__ but without\n            # the offset! Such a name is also needed for the lookup in permissions\n            return self.method.get_name() in self.apilist\n        else:\n            for candidate in api_candidates:\n                if self.method.get_class_name().startswith(candidate):\n                    return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef GetMethod(self, name, descriptor):\n        warnings.warn(\"deprecated, use get_method instead. This function might be removed in a later release!\", DeprecationWarning)\n        return self.get_method(name, descriptor)", "response": "Deprecated. Use GetMethod instead."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_method(self, name, descriptor):\n        key = name + str(descriptor)\n        if key not in self.methods:\n            self.methods[key] = ExternalMethod(self.name, name, descriptor)\n\n        return self.methods[key]", "response": "Get the method by name and descriptor."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndetermining if the current class is an Android API class.", "response": "def is_android_api(self):\n        \"\"\"\n        Tries to guess if the current class is an Android API class.\n\n        This might be not very precise unless an apilist is given, with classes that\n        are in fact known APIs.\n        Such a list might be generated by using the android.jar files.\n\n        :return: boolean\n        \"\"\"\n\n        # Packages found at https://developer.android.com/reference/packages.html\n        api_candidates = [\"Landroid/\", \"Lcom/android/internal/util\", \"Ldalvik/\", \"Ljava/\", \"Ljavax/\", \"Lorg/apache/\",\n                          \"Lorg/json/\", \"Lorg/w3c/dom/\", \"Lorg/xml/sax\", \"Lorg/xmlpull/v1/\", \"Ljunit/\"]\n\n        if not self.is_external():\n            # API must be external\n            return False\n\n        if self.apilist:\n            return self.orig_class.get_name() in self.apilist\n        else:\n            for candidate in api_candidates:\n                if self.orig_class.get_name().startswith(candidate):\n                    return True\n\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsearches for the given method name and descriptor and return a fake ExternalMethod if required.", "response": "def get_fake_method(self, name, descriptor):\n        \"\"\"\n        Search for the given method name and descriptor\n        and return a fake (ExternalMethod) if required.\n\n        :param name: name of the method\n        :param descriptor: descriptor of the method, for example `'(I I I)V'`\n        :return: :class:`ExternalMethod`\n        \"\"\"\n        if self.external:\n            # An external class can only generate the methods on demand\n            return self.orig_class.get_method(name, descriptor)\n\n        # We are searching an unknown method in this class\n        # It could be something that the class herits\n        key = name + str(descriptor)\n        if key not in self._inherits_methods:\n            self._inherits_methods[key] = ExternalMethod(self.orig_class.get_name(), name, descriptor)\n        return self._inherits_methods[key]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a Field Read to this class", "response": "def AddFXrefRead(self, method, classobj, field):\n        \"\"\"\n        Add a Field Read to this class\n\n        :param method:\n        :param classobj:\n        :param field:\n        :return:\n        \"\"\"\n        if field not in self._fields:\n            self._fields[field] = FieldClassAnalysis(field)\n        self._fields[field].AddXrefRead(classobj, method)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nadding a Field Write to this class", "response": "def AddFXrefWrite(self, method, classobj, field):\n        \"\"\"\n        Add a Field Write to this class\n\n        :param method:\n        :param classobj:\n        :param field:\n        :return:\n        \"\"\"\n        if field not in self._fields:\n            self._fields[field] = FieldClassAnalysis(field)\n        self._fields[field].AddXrefWrite(classobj, method)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef AddXrefTo(self, ref_kind, classobj, methodobj, offset):\n        self.xrefto[classobj].add((ref_kind, methodobj, offset))", "response": "Adds a crossreference to another class."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef AddXrefFrom(self, ref_kind, classobj, methodobj, offset):\n        self.xreffrom[classobj].add((ref_kind, methodobj, offset))", "response": "Adds a crossreference from this class."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nadds a DalvikVMFormat to this Analysis", "response": "def add(self, vm):\n        \"\"\"\n        Add a DalvikVMFormat to this Analysis\n\n        :param vm: :class:`dvm.DalvikVMFormat` to add to this Analysis\n        \"\"\"\n        self.vms.append(vm)\n        for current_class in vm.get_classes():\n            self.classes[current_class.get_name()] = ClassAnalysis(current_class)\n\n        for method in vm.get_methods():\n            self.methods[method] = MethodAnalysis(vm, method)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates Class Method String and Field crossreferences for all classes in the Analysis.", "response": "def create_xref(self):\n        \"\"\"\n        Create Class, Method, String and Field crossreferences\n        for all classes in the Analysis.\n\n        If you are using multiple DEX files, this function must\n        be called when all DEX files are added.\n        If you call the function after every DEX file, the\n        crossreferences might be wrong!\n        \"\"\"\n        log.debug(\"Creating Crossreferences (XREF)\")\n        tic = time.time()\n\n        # TODO on concurrent runs, we probably need to clean up first,\n        # or check that we do not write garbage.\n\n        # TODO multiprocessing\n        for c in self._get_all_classes():\n            self._create_xref(c)\n\n        log.info(\"End of creating cross references (XREF)\")\n        log.info(\"run time: {:0d}min {:02d}s\".format(*divmod(int(time.time() - tic), 60)))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate the xref for the current class.", "response": "def _create_xref(self, current_class):\n        \"\"\"\n        Create the xref for `current_class`\n\n        There are four steps involved in getting the xrefs:\n        * Xrefs for classes\n        *       for method calls\n        *       for string usage\n        *       for field manipulation\n\n        All these information are stored in the *Analysis Objects.\n\n        Note that this might be quite slow, as all instructions are parsed.\n\n        :param androguard.core.bytecodes.dvm.ClassDefItem current_class: The class to create xrefs for\n        \"\"\"\n        cur_cls_name = current_class.get_name()\n\n        log.debug(\"Creating XREF/DREF for %s\" % cur_cls_name)\n        for current_method in current_class.get_methods():\n            log.debug(\"Creating XREF for %s\" % current_method)\n\n            off = 0\n            for instruction in current_method.get_instructions():\n                op_value = instruction.get_op_value()\n\n                # 1) check for class calls: const-class (0x1c), new-instance (0x22)\n                if op_value in [0x1c, 0x22]:\n                    idx_type = instruction.get_ref_kind()\n                    # type_info is the string like 'Ljava/lang/Object;'\n                    type_info = instruction.cm.vm.get_cm_type(idx_type)\n\n                    # Internal xref related to class manipulation\n                    # FIXME should the xref really only set if the class is in self.classes? If an external class is added later, it will be added too!\n                    # See https://github.com/androguard/androguard/blob/d720ebf2a9c8e2a28484f1c81fdddbc57e04c157/androguard/core/analysis/analysis.py#L806\n                    # Before the check would go for internal classes only!\n                    # FIXME: effectively ignoring calls to itself - do we want that?\n                    if type_info != cur_cls_name:\n                        if type_info not in self.classes:\n                            # Create new external class\n                            self.classes[type_info] = ClassAnalysis(ExternalClass(type_info))\n\n                        cur_cls = self.classes[cur_cls_name]\n                        oth_cls = self.classes[type_info]\n\n                        # FIXME: xref_to does not work here! current_method is wrong, as it is not the target!\n                        cur_cls.AddXrefTo(REF_TYPE(op_value), oth_cls, current_method, off)\n                        oth_cls.AddXrefFrom(REF_TYPE(op_value), cur_cls, current_method, off)\n\n                # 2) check for method calls: invoke-* (0x6e ... 0x72), invoke-xxx/range (0x74 ... 0x78)\n                elif (0x6e <= op_value <= 0x72) or (0x74 <= op_value <= 0x78):\n                    idx_meth = instruction.get_ref_kind()\n                    method_info = instruction.cm.vm.get_cm_method(idx_meth)\n                    if method_info:\n                        class_info = method_info[0]\n\n                        method_item = None\n                        # TODO: should create get_method_descriptor inside Analysis\n                        for vm in self.vms:\n                            method_item = vm.get_method_descriptor(method_info[0], method_info[1], ''.join(method_info[2]))\n                            if method_item:\n                                break\n\n                        if not method_item:\n                            # Seems to be an external class, create it first\n                            # Beware: if not all DEX files are loaded at the time create_xref runs\n                            # you will run into problems!\n                            if method_info[0] not in self.classes:\n                                self.classes[method_info[0]] = ClassAnalysis(ExternalClass(method_info[0]))\n                            method_item = self.classes[method_info[0]].get_fake_method(method_info[1], method_info[2])\n\n                        self.classes[cur_cls_name].AddMXrefTo(current_method, self.classes[class_info], method_item, off)\n                        self.classes[class_info].AddMXrefFrom(method_item, self.classes[cur_cls_name], current_method, off)\n\n                        # Internal xref related to class manipulation\n                        if class_info in self.classes and class_info != cur_cls_name:\n                            self.classes[cur_cls_name].AddXrefTo(REF_TYPE(op_value), self.classes[class_info], method_item, off)\n                            self.classes[class_info].AddXrefFrom(REF_TYPE(op_value), self.classes[cur_cls_name], current_method, off)\n\n                # 3) check for string usage: const-string (0x1a), const-string/jumbo (0x1b)\n                elif 0x1a <= op_value <= 0x1b:\n                    string_value = instruction.cm.vm.get_cm_string(instruction.get_ref_kind())\n                    if string_value not in self.strings:\n                        self.strings[string_value] = StringAnalysis(string_value)\n\n                    # TODO: The bytecode offset is stored for classes but not here?\n                    self.strings[string_value].AddXrefFrom(self.classes[cur_cls_name], current_method)\n\n                # TODO maybe we should add a step 3a) here and check for all const fields. You can then xref for integers etc!\n                # But: This does not work, as const fields are usually optimized internally to const calls...\n\n                # 4) check for field usage: i*op (0x52 ... 0x5f), s*op (0x60 ... 0x6d)\n                elif 0x52 <= op_value <= 0x6d:\n                    idx_field = instruction.get_ref_kind()\n                    field_info = instruction.cm.vm.get_cm_field(idx_field)\n                    field_item = instruction.cm.vm.get_field_descriptor(field_info[0], field_info[2], field_info[1])\n                    # TODO: The bytecode offset is stored for classes but not here?\n                    if field_item:\n                        if (0x52 <= op_value <= 0x58) or (0x60 <= op_value <= 0x66):\n                            # read access to a field\n                            self.classes[cur_cls_name].AddFXrefRead(current_method, self.classes[cur_cls_name], field_item)\n                        else:\n                            # write access to a field\n                            self.classes[cur_cls_name].AddFXrefWrite(current_method, self.classes[cur_cls_name], field_item)\n\n                off += instruction.get_length()"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nsearch for a method in all classes in this analysis and return the corresponding method object.", "response": "def get_method_by_name(self, class_name, method_name, method_descriptor):\n        \"\"\"\n        Search for a :class:`EncodedMethod` in all classes in this analysis\n\n        :param class_name: name of the class, for example 'Ljava/lang/Object;'\n        :param method_name: name of the method, for example 'onCreate'\n        :param method_descriptor: descriptor, for example '(I I Ljava/lang/String)V\n        :return: :class:`EncodedMethod` or None if method was not found\n        \"\"\"\n        if class_name in self.classes:\n            for method in self.classes[class_name].get_vm_class().get_methods():\n                if method.get_name() == method_name and method.get_descriptor() == method_descriptor:\n                    return method\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the crossreferencing object for a given method.", "response": "def get_method_analysis(self, method):\n        \"\"\"\n        Returns the crossreferencing object for a given Method.\n\n        Beware: the similar named function :meth:`~get_method()` will return\n        a :class:`MethodAnalysis` object, while this function returns a :class:`MethodClassAnalysis` object!\n\n        This Method will only work after a run of :meth:`~create_xref()`\n\n        :param method: :class:`EncodedMethod`\n        :return: :class:`MethodClassAnalysis` for the given method or None, if method was not found\n        \"\"\"\n        class_analysis = self.get_class_analysis(method.get_class_name())\n        if class_analysis:\n            return class_analysis.get_method_analysis(method)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_method_analysis_by_name(self, class_name, method_name, method_descriptor):\n        method = self.get_method_by_name(class_name, method_name, method_descriptor)\n        if method:\n            return self.get_method_analysis(method)\n        return None", "response": "Returns the crossreferencing object for a given method."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the FieldAnalysis for a given field name", "response": "def get_field_analysis(self, field):\n        \"\"\"\n        Get the FieldAnalysis for a given fieldname\n\n        :param field: TODO\n        :return: :class:`FieldClassAnalysis`\n        \"\"\"\n        class_analysis = self.get_class_analysis(field.get_class_name())\n        if class_analysis:\n            return class_analysis.get_field_analysis(field)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_methods(self):\n        for c in self.classes.values():\n            for m in c.get_methods():\n                yield m", "response": "Returns a list of MethodClassAnalysis objects"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_fields(self):\n        for c in self.classes.values():\n            for f in c.get_fields():\n                yield f", "response": "Returns a list of FieldAnalysis objects"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding classes by name using regular expression", "response": "def find_classes(self, name=\".*\", no_external=False):\n        \"\"\"\n        Find classes by name, using regular expression\n        This method will return all ClassAnalysis Object that match the name of\n        the class.\n\n        :param name: regular expression for class name (default \".*\")\n        :param no_external: Remove external classes from the output (default False)\n        :rtype: generator of `ClassAnalysis`\n        \"\"\"\n        for cname, c in self.classes.items():\n            if no_external and isinstance(c.get_vm_class(), ExternalClass):\n                continue\n            if re.match(name, cname):\n                yield c"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds a method by name using regular expression. This method will return all MethodClassAnalysis objects, which match the classname, methodname, descriptor and accessflags of the method. :param classname: regular expression for the classname :param methodname: regular expression for the method name :param descriptor: regular expression for the descriptor :param accessflags: regular expression for the accessflags :param no_external: Remove external method from the output (default False) :rtype: generator of `MethodClassAnalysis`", "response": "def find_methods(self, classname=\".*\", methodname=\".*\", descriptor=\".*\",\n            accessflags=\".*\", no_external=False):\n        \"\"\"\n        Find a method by name using regular expression.\n        This method will return all MethodClassAnalysis objects, which match the\n        classname, methodname, descriptor and accessflags of the method.\n\n        :param classname: regular expression for the classname\n        :param methodname: regular expression for the method name\n        :param descriptor: regular expression for the descriptor\n        :param accessflags: regular expression for the accessflags\n        :param no_external: Remove external method from the output (default False)\n        :rtype: generator of `MethodClassAnalysis`\n        \"\"\"\n        for cname, c in self.classes.items():\n            if re.match(classname, cname):\n                for m in c.get_methods():\n                    z = m.get_method()\n                    # TODO is it even possible that an internal class has\n                    # external methods? Maybe we should check for ExternalClass\n                    # instead...\n                    if no_external and isinstance(z, ExternalMethod):\n                        continue\n                    if re.match(methodname, z.get_name()) and \\\n                       re.match(descriptor, z.get_descriptor()) and \\\n                       re.match(accessflags, z.get_access_flags_string()):\n                        yield m"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfinds strings by regex", "response": "def find_strings(self, string=\".*\"):\n        \"\"\"\n        Find strings by regex\n\n        :param string: regular expression for the string to search for\n        :rtype: generator of `StringAnalysis`\n        \"\"\"\n        for s, sa in self.strings.items():\n            if re.match(string, s):\n                yield sa"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfinding fields by regex", "response": "def find_fields(self, classname=\".*\", fieldname=\".*\", fieldtype=\".*\", accessflags=\".*\"):\n        \"\"\"\n        find fields by regex\n\n        :param classname: regular expression of the classname\n        :param fieldname: regular expression of the fieldname\n        :param fieldtype: regular expression of the fieldtype\n        :param accessflags: regular expression of the access flags\n        :rtype: generator of `FieldClassAnalysis`\n        \"\"\"\n        for cname, c in self.classes.items():\n            if re.match(classname, cname):\n                for f in c.get_fields():\n                    z = f.get_field()\n                    if re.match(fieldname, z.get_name()) and \\\n                       re.match(fieldtype, z.get_descriptor()) and \\\n                       re.match(accessflags, z.get_access_flags_string()):\n                        yield f"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_call_graph(self, classname=\".*\", methodname=\".*\", descriptor=\".*\",\n                       accessflags=\".*\", no_isolated=False, entry_points=[]):\n        \"\"\"\n        Generate a directed graph based on the methods found by the filters applied.\n        The filters are the same as in\n        :meth:`~androguard.core.analaysis.analaysis.Analysis.find_methods`\n\n        A networkx.DiGraph is returned, containing all edges only once!\n        that means, if a method calls some method twice or more often, there will\n        only be a single connection.\n\n        :param classname: regular expression of the classname (default: \".*\")\n        :param fieldname: regular expression of the fieldname (default: \".*\")\n        :param fieldtype: regular expression of the fieldtype (default: \".*\")\n        :param accessflags: regular expression of the access flags (default: \".*\")\n        :param no_isolated: remove isolated nodes from the graph, e.g. methods which do not call anything (default: False)\n        :param entry_points: A list of classes that are marked as entry point\n\n        :rtype: DiGraph\n        \"\"\"\n\n        def _add_node(G, method):\n            \"\"\"\n            Wrapper to add methods to a graph\n            \"\"\"\n            if method not in G.node:\n                G.add_node(method,\n                           external=isinstance(method, ExternalMethod),\n                           entrypoint=method.get_class_name() in entry_points,\n                           native=\"native\" in method.get_access_flags_string(),\n                           public=\"public\" in method.get_access_flags_string(),\n                           static=\"static\" in method.get_access_flags_string(),\n                           )\n\n        CG = nx.DiGraph()\n\n        # Note: If you create the CG from many classes at the same time, the drawing\n        # will be a total mess...\n        for m in self.find_methods(classname=classname, methodname=methodname,\n                                   descriptor=descriptor, accessflags=accessflags):\n            orig_method = m.get_method()\n            log.info(\"Found Method --> {}\".format(orig_method))\n\n            if no_isolated and len(m.get_xref_to()) == 0:\n                log.info(\"Skipped {}, because if has no xrefs\".format(orig_method))\n                continue\n\n            _add_node(CG, orig_method)\n\n            for other_class, callee, offset in m.get_xref_to():\n                _add_node(CG, callee)\n\n                # As this is a DiGraph and we are not interested in duplicate edges,\n                # check if the edge is already in the edge set.\n                # If you need all calls, you probably want to check out MultiDiGraph\n                if not CG.has_edge(orig_method, callee):\n                    CG.add_edge(orig_method, callee)\n\n        return CG", "response": "Generate a directed graph of all methods that are called by the filter applied."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef create_ipython_exports(self):\n        # TODO: it would be fun to have the classes organized like the packages. I.e. you could do dx.CLASS_xx.yyy.zzz\n        for cls in self.get_classes():\n            name = \"CLASS_\" + bytecode.FormatClassToPython(cls.name)\n            if hasattr(self, name):\n                log.warning(\"Already existing class {}!\".format(name))\n            setattr(self, name, cls)\n\n            for meth in cls.get_methods():\n                method_name = meth.name\n                if method_name in [\"<init>\", \"<clinit>\"]:\n                    _, method_name = bytecode.get_package_class_name(cls.name)\n\n                # FIXME this naming schema is not very good... but to describe a method uniquely, we need all of it\n                mname = \"METH_\" + method_name + \"_\" + bytecode.FormatDescriptorToPython(meth.access) + \"_\" + bytecode.FormatDescriptorToPython(meth.descriptor)\n                if hasattr(cls, mname):\n                    log.warning(\"already existing method: {} at class {}\".format(mname, name))\n                setattr(cls, mname, meth)\n\n            # FIXME: syntetic classes produce problems here.\n            # If the field name is the same in the parent as in the syntetic one, we can only add one!\n            for field in cls.get_fields():\n                mname = \"FIELD_\" + bytecode.FormatNameToPython(field.name)\n                if hasattr(cls, mname):\n                    log.warning(\"already existing field: {} at class {}\".format(mname, name))\n                setattr(cls, mname, field)", "response": "Create an iPython export for all classes methods and fields."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_permissions(self, apilevel=None):\n\n        # TODO maybe have the API level loading in the __init__ method and pass the APK as well?\n        permmap = load_api_specific_resource_module('api_permission_mappings', apilevel)\n        if not permmap:\n            raise ValueError(\"No permission mapping found! Is one available? \"\n                             \"The requested API level was '{}'\".format(apilevel))\n\n        for cls in self.get_external_classes():\n            for meth_analysis in cls.get_methods():\n                meth = meth_analysis.get_method()\n                if meth.permission_api_name in permmap:\n                    yield meth_analysis, permmap[meth.permission_api_name]", "response": "Returns the permissions and API methods for this object."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the usage of a permission inside the Analysis.", "response": "def get_permission_usage(self, permission, apilevel=None):\n        \"\"\"\n        Find the usage of a permission inside the Analysis.\n\n        example::\n            from androguard.misc import AnalyzeAPK\n            a, d, dx = AnalyzeAPK(\"somefile.apk\")\n\n            for meth in dx.get_permission_usage('android.permission.SEND_SMS', a.get_effective_target_sdk_version()):\n                print(\"Using API method {}\".format(meth))\n                print(\"used in:\")\n                for _, m, _ in meth.get_xref_from():\n                    print(m.full_name)\n\n        .. note::\n            The permission mappings might be incomplete! See also :meth:`get_permissions`.\n\n        :param permission: the name of the android permission (usually 'android.permission.XXX')\n        :param apilevel: the requested API level or None for default\n        :return: yields :class:`MethodClassAnalysis` objects for all using API methods\n        \"\"\"\n\n        # TODO maybe have the API level loading in the __init__ method and pass the APK as well?\n        permmap = load_api_specific_resource_module('api_permission_mappings', apilevel)\n        if not permmap:\n            raise ValueError(\"No permission mapping found! Is one available? \"\n                             \"The requested API level was '{}'\".format(apilevel))\n\n        apis = {k for k, v in permmap.items() if permission in v}\n        if not apis:\n            raise ValueError(\"No API methods could be found which use the permission. \"\n                             \"Does the permission exists? You requested: '{}'\".format(permission))\n\n        for cls in self.get_external_classes():\n            for meth_analysis in cls.get_methods():\n                meth = meth_analysis.get_method()\n                if meth.permission_api_name in apis:\n                    yield meth_analysis"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef Save(session, filename=None):\n\n    if not filename:\n        filename = \"androguard_session_{:%Y-%m-%d_%H%M%S}.ag\".format(datetime.datetime.now())\n\n    if os.path.isfile(filename):\n        log.warning(\"{} already exists, overwriting!\")\n\n    # Setting the recursion limit according to the documentation:\n    # https://docs.python.org/3/library/pickle.html#what-can-be-pickled-and-unpickled\n    #\n    # Some larger APKs require a high recursion limit.\n    # Tested to be above 35000 for some files, setting to 50k to be sure.\n    # You might want to set this even higher if you encounter problems\n    reclimit = sys.getrecursionlimit()\n    sys.setrecursionlimit(50000)\n    saved = False\n    try:\n        with open(filename, \"wb\") as fd:\n            pickle.dump(session, fd)\n        saved = True\n    except RecursionError:\n        log.exception(\"Recursion Limit hit while saving. \"\n                      \"Current Recursion limit: {}. \"\n                      \"Please report this error!\".format(sys.getrecursionlimit()))\n        # Remove partially written file\n        os.unlink(filename)\n\n    sys.setrecursionlimit(reclimit)\n    return filename if saved else None", "response": "Save a session object to a file."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef show(self):\n        print(\"APKs in Session: {}\".format(len(self.analyzed_apk)))\n        for d, a in self.analyzed_apk.items():\n            print(\"\\t{}: {}\".format(d, a))\n\n        print(\"DEXs in Session: {}\".format(len(self.analyzed_dex)))\n        for d, dex in self.analyzed_dex.items():\n            print(\"\\t{}: {}\".format(d, dex))\n\n        print(\"Analysis in Session: {}\".format(len(self.analyzed_vms)))\n        for d, a in self.analyzed_vms.items():\n            print(\"\\t{}: {}\".format(d, a))", "response": "Prints information about the current session."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nadding an APK file to the Session and run analysis on it.", "response": "def addAPK(self, filename, data):\n        \"\"\"\n        Add an APK file to the Session and run analysis on it.\n\n        :param filename: (file)name of APK file\n        :param data: binary data of the APK file\n        :return: a tuple of SHA256 Checksum and APK Object\n        \"\"\"\n        digest = hashlib.sha256(data).hexdigest()\n        log.debug(\"add APK:%s\" % digest)\n        apk = APK(data, True)\n        self.analyzed_apk[digest] = [apk]\n        self.analyzed_files[filename].append(digest)\n        self.analyzed_digest[digest] = filename\n\n        dx = Analysis()\n        self.analyzed_vms[digest] = dx\n\n        for dex in apk.get_all_dex():\n            # we throw away the output... FIXME?\n            self.addDEX(filename, dex, dx)\n\n        log.debug(\"added APK:%s\" % digest)\n        return digest, apk"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef addDEX(self, filename, data, dx=None):\n        digest = hashlib.sha256(data).hexdigest()\n        log.debug(\"add DEX:%s\" % digest)\n\n        log.debug(\"Parsing format ...\")\n        d = DalvikVMFormat(data)\n        log.debug(\"added DEX:%s\" % digest)\n\n        self.analyzed_files[filename].append(digest)\n        self.analyzed_digest[digest] = filename\n\n        self.analyzed_dex[digest] = d\n\n        if dx is None:\n            dx = Analysis()\n\n        dx.add(d)\n        dx.create_xref()\n\n        # TODO: If multidex: this will called many times per dex, even if already set\n        for d in dx.vms:\n            # TODO: allow different decompiler here!\n            d.set_decompiler(DecompilerDAD(d, dx))\n            d.set_vmanalysis(dx)\n        self.analyzed_vms[digest] = dx\n\n        if self.export_ipython:\n            log.debug(\"Exporting in ipython\")\n            d.create_python_export()\n\n        return digest, d, dx", "response": "Add a DEX file to the Session and run analysis."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding a file to the session.", "response": "def add(self, filename, raw_data=None, dx=None):\n        \"\"\"\n        Generic method to add a file to the session.\n\n        This is the main method to use when adding files to a Session!\n\n        If an APK file is supplied, all DEX files are analyzed too.\n        For DEX and ODEX files, only this file is analyzed (what else should be\n        analyzed).\n\n        Returns the SHA256 of the analyzed file.\n\n        :param filename: filename to load\n        :param raw_data: bytes of the file, or None to load the file from filename\n        :param dx: An already exiting :class:`~androguard.core.analysis.analysis.Analysis` object\n        :return: the sha256 of the file or None on failure\n        \"\"\"\n        if not raw_data:\n            log.debug(\"Loading file from '{}'\".format(filename))\n            with open(filename, \"rb\") as fp:\n                raw_data = fp.read()\n\n        ret = androconf.is_android_raw(raw_data)\n        log.debug(\"Found filetype: '{}'\".format(ret))\n        if not ret:\n            return None\n\n        if ret == \"APK\":\n            digest, _ = self.addAPK(filename, raw_data)\n        elif ret == \"DEX\":\n            digest, _, _ = self.addDEX(filename, raw_data, dx)\n        elif ret == \"DEY\":\n            digest, _, _ = self.addDEY(filename, raw_data, dx)\n        else:\n            return None\n\n        return digest"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn all Java Classes from the DEX objects as an array of DEX files.", "response": "def get_classes(self):\n        \"\"\"\n        Returns all Java Classes from the DEX objects as an array of DEX files.\n        \"\"\"\n        for idx, digest in enumerate(self.analyzed_vms):\n            dx = self.analyzed_vms[digest]\n            for vm in dx.vms:\n                filename = self.analyzed_digest[digest]\n                yield idx, filename, digest, vm.get_classes()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_analysis(self, current_class):\n        for digest in self.analyzed_vms:\n            dx = self.analyzed_vms[digest]\n            if dx.is_class_present(current_class.get_name()):\n                return dx\n        return None", "response": "Returns the object containing the current_class."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the filename of the DEX file where the class is in.", "response": "def get_filename_by_class(self, current_class):\n        \"\"\"\n        Returns the filename of the DEX file where the class is in.\n\n        Returns the first filename this class was present.\n        For example, if you analyzed an APK, this should return the filename of\n        the APK and not of the DEX file.\n\n        :param current_class: ClassDefItem\n        :returns: None if class was not found or the filename\n        \"\"\"\n        for digest, dx in self.analyzed_vms.items():\n            if dx.is_class_present(current_class.get_name()):\n                return self.analyzed_digest[digest]\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the SHA256 hash of the object containing the ClassDefItem that is contained in the class containing the current_class. Returns None if the class is not present.", "response": "def get_digest_by_class(self, current_class):\n        \"\"\"\n        Return the SHA256 hash of the object containing the ClassDefItem\n\n        Returns the first digest this class was present.\n        For example, if you analyzed an APK, this should return the digest of\n        the APK and not of the DEX file.\n        \"\"\"\n        for digest, dx in self.analyzed_vms.items():\n            if dx.is_class_present(current_class.get_name()):\n                return digest\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields all StringAnalysis objects for all unique Analysis objects", "response": "def get_strings(self):\n        \"\"\"\n        Yields all StringAnalysis for all unique Analysis objects\n        \"\"\"\n        seen = []\n        for digest, dx in self.analyzed_vms.items():\n            if dx in seen:\n                continue\n            seen.append(dx)\n            yield digest, self.analyzed_digest[digest], dx.get_strings_analysis()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_nb_strings(self):\n        nb = 0\n        seen = []\n        for digest, dx in self.analyzed_vms.items():\n            if dx in seen:\n                continue\n            seen.append(dx)\n            nb += len(dx.get_strings_analysis())\n        return nb", "response": "Return the total number of strings in all Analysis objects in all Analysis objects"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns APK DalvikVMFormat and Analysis of a specified APK.", "response": "def get_objects_apk(self, filename=None, digest=None):\n        \"\"\"\n        Returns APK, DalvikVMFormat and Analysis of a specified APK.\n\n        You must specify either `filename` or `digest`.\n        It is possible to use both, but in this case only `digest` is used.\n\n        example::\n\n            s = Session()\n            digest = s.add(\"some.apk\")\n            a, d, dx = s.get_objects_apk(digest=digest)\n\n        example::\n\n            s = Session()\n            filename = \"some.apk\"\n            digest = s.add(filename)\n            a, d, dx = s.get_objects_apk(filename=filename)\n\n        :param filename: the filename of the APK file, only used of digest is None\n        :param digest: the sha256 hash, as returned by :meth:`add` for the APK\n        :returns: a tuple of (APK, [DalvikVMFormat], Analysis)\n        \"\"\"\n        if not filename and not digest:\n            raise ValueError(\"Must give at least filename or digest!\")\n\n        if digest is None:\n            digests = self.analyzed_files.get(filename)\n            # Negate to reduce tree\n            if not digests:\n                return None, None, None\n            digest = digests[0]\n\n        a = self.analyzed_apk[digest][0]\n        dx = self.analyzed_vms[digest]\n        return a, dx.vms, dx"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_objects_dex(self):\n        # TODO: there is no variant like get_objects_apk\n        for digest, d in self.analyzed_dex.items():\n            yield digest, d, self.analyzed_vms[digest]", "response": "Yields all dex objects inclduing their Analysis objects\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fill(self):\n        log.debug(\"Fill classes tree\")\n\n        for idx, filename, digest, classes in self.session.get_classes():\n            for c in sorted(classes, key=lambda c: c.name):\n                sig = Signature(c)\n                path_node = self.root_path_node\n\n                path = None\n                if not sig.class_path:\n                    path = '.'\n                    if path not in path_node[0]:\n                        path_node[0][path] = (\n                            {}, HashableQTreeWidgetItem(path_node[1]))\n                        path_node[0][path][1].setText(0, path)\n                    path_node = path_node[0][path]\n                else:\n                    # Namespaces\n                    for path in sig.class_path:\n                        if path not in path_node[0]:\n                            path_node[0][path] = (\n                                {}, HashableQTreeWidgetItem(path_node[1]))\n                            path_node[0][path][1].setText(0, path)\n                        path_node = path_node[0][path]\n\n                # Class\n                path_node[0][path] = ({}, HashableQTreeWidgetItem(path_node[1]))\n\n                class_name = sig.class_name\n\n                if idx > 0:\n                    class_name += \"@%d\" % idx\n\n                c.current_title = class_name\n                self._reverse_cache[path_node[0][path][1]] = (c, filename,\n                                                              digest)\n\n                path_node[0][path][1].setText(0, class_name)", "response": "Parse all the paths and build a tree using the insertion method."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntrying to parse additional attributes and dump them into a string", "response": "def _dump_additional_attributes(additional_attributes):\n    \"\"\" try to parse additional attributes, but ends up to hexdump if the scheme is unknown \"\"\"\n\n    attributes_raw = io.BytesIO(additional_attributes)\n    attributes_hex = binascii.hexlify(additional_attributes)\n\n    if not len(additional_attributes):\n        return attributes_hex\n\n    len_attribute, = unpack('<I', attributes_raw.read(4))\n    if len_attribute != 8:\n        return attributes_hex\n\n    attr_id, = unpack('<I', attributes_raw.read(4))\n    if attr_id != APK._APK_SIG_ATTR_V2_STRIPPING_PROTECTION:\n        return attributes_hex\n        \n    scheme_id, = unpack('<I', attributes_raw.read(4))\n\n    return \"stripping protection set, scheme %d\" % scheme_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints the contents of a certificate in a list of strings.", "response": "def show_Certificate(cert, short=False):\n    \"\"\"\n        Print Fingerprints, Issuer and Subject of an X509 Certificate.\n\n        :param cert: X509 Certificate to print\n        :param short: Print in shortform for DN (Default: False)\n\n        :type cert: :class:`asn1crypto.x509.Certificate`\n        :type short: Boolean\n    \"\"\"\n    print(\"SHA1 Fingerprint: {}\".format(cert.sha1_fingerprint))\n    print(\"SHA256 Fingerprint: {}\".format(cert.sha256_fingerprint))\n    print(\"Issuer: {}\".format(get_certificate_name_string(cert.issuer.native, short=short)))\n    print(\"Subject: {}\".format(get_certificate_name_string(cert.subject.native, short=short)))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef ensure_final_value(packageName, arsc, value):\n    if value:\n        returnValue = value\n        if value[0] == '@':\n            # TODO: @packagename:DEADBEEF is not supported here!\n            try:  # can be a literal value or a resId\n                res_id = int('0x' + value[1:], 16)\n                res_id = arsc.get_id(packageName, res_id)[1]\n                returnValue = arsc.get_string(packageName, res_id)[1]\n            except (ValueError, TypeError):\n                pass\n        return returnValue\n    return ''", "response": "Ensure incoming value is always the value and not the Android \"resId aka\n    Resource ID."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_apkid(apkfile):\n    if not os.path.exists(apkfile):\n        log.error(\"'{apkfile}' does not exist!\".format(apkfile=apkfile))\n\n    appid = None\n    versionCode = None\n    versionName = None\n    with zipfile.ZipFile(apkfile) as apk:\n        with apk.open('AndroidManifest.xml') as manifest:\n            axml = AXMLParser(manifest.read())\n            count = 0\n            while axml.is_valid():\n                _type = next(axml)\n                count += 1\n                if _type == START_TAG:\n                    for i in range(0, axml.getAttributeCount()):\n                        name = axml.getAttributeName(i)\n                        _type = axml.getAttributeValueType(i)\n                        _data = axml.getAttributeValueData(i)\n                        value = format_value(_type, _data, lambda _: axml.getAttributeValue(i))\n                        if appid is None and name == 'package':\n                            appid = value\n                        elif versionCode is None and name == 'versionCode':\n                            if value.startswith('0x'):\n                                versionCode = str(int(value, 16))\n                            else:\n                                versionCode = value\n                        elif versionName is None and name == 'versionName':\n                            versionName = value\n\n                    if axml.name == 'manifest':\n                        break\n                elif _type == END_TAG or _type == TEXT or _type == END_DOCUMENT:\n                    raise RuntimeError('{path}: <manifest> must be the first element in AndroidManifest.xml'\n                                       .format(path=apkfile))\n\n    if not versionName or versionName[0] == '@':\n        a = APK(apkfile)\n        versionName = ensure_final_value(a.package, a.get_android_resources(), a.get_androidversion_name())\n    if not versionName:\n        versionName = ''  # versionName is expected to always be a str\n\n    return appid, versionCode, versionName.strip('\\0')", "response": "Read the appid versionCode and versionName from an APK file."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the app name of the APK.", "response": "def get_app_name(self):\n        \"\"\"\n        Return the appname of the APK\n\n        This name is read from the AndroidManifest.xml\n        using the application android:label.\n        If no label exists, the android:label of the main activity is used.\n\n        If there is also no main activity label, an empty string is returned.\n\n        :rtype: :class:`str`\n        \"\"\"\n\n        app_name = self.get_attribute_value('application', 'label')\n        if app_name is None:\n            activities = self.get_main_activities()\n            main_activity_name = None\n            if len(activities) > 0:\n                main_activity_name = activities.pop()\n\n            # FIXME: would need to use _format_value inside get_attribute_value for each returned name!\n            # For example, as the activity name might be foobar.foo.bar but inside the activity it is only .bar\n            app_name = self.get_attribute_value('activity', 'label', name=main_activity_name)\n\n        if app_name is None:\n            # No App name set\n            # TODO return packagename instead?\n            log.warning(\"It looks like that no app name is set for the main activity!\")\n            return \"\"\n\n        if app_name.startswith(\"@\"):\n            res_parser = self.get_android_resources()\n            if not res_parser:\n                # TODO: What should be the correct return value here?\n                return app_name\n\n            res_id, package = res_parser.parse_id(app_name)\n\n            # If the package name is the same as the APK package,\n            # we should be able to resolve the ID.\n            if package and package != self.get_package():\n                if package == 'android':\n                    # TODO: we can not resolve this, as we lack framework-res.apk\n                    # one exception would be when parsing framework-res.apk directly.\n                    log.warning(\"Resource ID with android package name encountered! \"\n                                \"Will not resolve, framework-res.apk would be required.\")\n                    return app_name\n                else:\n                    # TODO should look this up, might be in the resources\n                    log.warning(\"Resource ID with Package name '{}' encountered! Will not resolve\".format(package))\n                    return app_name\n\n            try:\n                app_name = res_parser.get_resolved_res_configs(\n                    res_id,\n                    ARSCResTableConfig.default_config())[0][1]\n            except Exception as e:\n                log.warning(\"Exception selecting app name: %s\" % e)\n        return app_name"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_app_icon(self, max_dpi=65536):\n        main_activity_name = self.get_main_activity()\n\n        app_icon = self.get_attribute_value(\n            'activity', 'icon', name=main_activity_name)\n\n        if not app_icon:\n            app_icon = self.get_attribute_value('application', 'icon')\n\n        res_parser = self.get_android_resources()\n        if not res_parser:\n            # Can not do anything below this point to resolve...\n            return None\n\n        if not app_icon:\n            res_id = res_parser.get_res_id_by_key(self.package, 'mipmap', 'ic_launcher')\n            if res_id:\n                app_icon = \"@%x\" % res_id\n\n        if not app_icon:\n            res_id = res_parser.get_res_id_by_key(self.package, 'drawable', 'ic_launcher')\n            if res_id:\n                app_icon = \"@%x\" % res_id\n\n        if not app_icon:\n            # If the icon can not be found, return now\n            return None\n\n        if app_icon.startswith(\"@\"):\n            res_id = int(app_icon[1:], 16)\n            candidates = res_parser.get_resolved_res_configs(res_id)\n\n            app_icon = None\n            current_dpi = -1\n\n            try:\n                for config, file_name in candidates:\n                    dpi = config.get_density()\n                    if current_dpi < dpi <= max_dpi:\n                        app_icon = file_name\n                        current_dpi = dpi\n            except Exception as e:\n                log.warning(\"Exception selecting app icon: %s\" % e)\n\n        return app_icon", "response": "Return the first app icon file name which density is not greater than max_dpi."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the filetype guessed for a buffer.", "response": "def _get_file_magic_name(self, buffer):\n        \"\"\"\n        Return the filetype guessed for a buffer\n        :param buffer: bytes\n        :returns: str of filetype\n        \"\"\"\n        default = \"Unknown\"\n\n        # Faster way, test once, return default.\n        if self.__no_magic:\n            return default\n\n        try:\n            # Magic is optional\n            import magic\n        except ImportError:\n            self.__no_magic = True\n            log.warning(\"No Magic library was found on your system.\")\n            return default\n        except TypeError as e:\n            self.__no_magic = True\n            log.warning(\"It looks like you have the magic python package installed but not the magic library itself!\")\n            log.warning(\"Error from magic library: %s\", e)\n            log.warning(\"Please follow the installation instructions at https://github.com/ahupp/python-magic/#installation\")\n            return default\n\n        try:\n            # There are several implementations of magic,\n            # unfortunately all called magic\n            # We use this one: https://github.com/ahupp/python-magic/\n            getattr(magic, \"MagicException\")\n        except AttributeError:\n            self.__no_magic = True\n            log.warning(\"Not the correct Magic library was found on your system. Please install python-magic!\")\n            return default\n\n        try:\n            ftype = magic.from_buffer(buffer[:1024])\n        except magic.MagicError as e:\n            log.exception(\"Error getting the magic type: %s\", e)\n            return default\n\n        if not ftype:\n            return default\n        else:\n            return self._patch_magic(buffer, ftype)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the files inside the APK with their associated types.", "response": "def get_files_types(self):\n        \"\"\"\n        Return the files inside the APK with their associated types (by using python-magic)\n\n        At the same time, the CRC32 are calculated for the files.\n\n        :rtype: a dictionnary\n        \"\"\"\n        if self._files == {}:\n            # Generate File Types / CRC List\n            for i in self.get_files():\n                buffer = self._get_crc32(i)\n                self._files[i] = self._get_file_magic_name(buffer)\n\n        return self._files"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _patch_magic(self, buffer, orig):\n        if (\"Zip\" in orig) or ('(JAR)' in orig) and androconf.is_android_raw(buffer) == 'APK':\n            return \"Android application package file\"\n\n        return orig", "response": "Patch the magic number to match the mime libraries"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate and compares the CRC32 and returns the raw buffer.", "response": "def _get_crc32(self, filename):\n        \"\"\"\n        Calculates and compares the CRC32 and returns the raw buffer.\n\n        The CRC32 is added to `files_crc32` dictionary, if not present.\n\n        :param filename: filename inside the zipfile\n        :rtype: bytes\n        \"\"\"\n        buffer = self.zip.read(filename)\n        if filename not in self.files_crc32:\n            self.files_crc32[filename] = crc32(buffer)\n            if self.files_crc32[filename] != self.zip.getinfo(filename).CRC:\n                log.error(\"File '{}' has different CRC32 after unpacking! \"\n                          \"Declared: {:08x}, Calculated: {:08x}\".format(filename,\n                                                                        self.zip.getinfo(filename).CRC,\n                                                                        self.files_crc32[filename]))\n        return buffer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_files_crc32(self):\n        if self.files_crc32 == {}:\n            for i in self.get_files():\n                self._get_crc32(i)\n\n        return self.files_crc32", "response": "Calculates and returns a dictionary of filenames and CRC32s"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the files inside the APK with their associated types and crc32", "response": "def get_files_information(self):\n        \"\"\"\n        Return the files inside the APK with their associated types and crc32\n\n        :rtype: str, str, int\n        \"\"\"\n        for k in self.get_files():\n            yield k, self.get_files_types()[k], self.get_files_crc32()[k]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the raw data of the specified filename inside the APK", "response": "def get_file(self, filename):\n        \"\"\"\n        Return the raw data of the specified filename\n        inside the APK\n\n        :rtype: bytes\n        \"\"\"\n        try:\n            return self.zip.read(filename)\n        except KeyError:\n            raise FileNotPresent(filename)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the names of all DEX files in the APK.", "response": "def get_dex_names(self):\n        \"\"\"\n        Return the names of all DEX files found in the APK.\n        This method only accounts for \"offical\" dex files, i.e. all files\n        in the root directory of the APK named classes.dex or classes[0-9]+.dex\n\n        :rtype: a list of str\n        \"\"\"\n        dexre = re.compile(r\"classes(\\d*).dex\")\n        return filter(lambda x: dexre.match(x), self.get_files())"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nyields all the attribute values in xml files which match with the tag name and the specific attribute name.", "response": "def get_all_attribute_value(\n        self, tag_name, attribute, format_value=True, **attribute_filter\n    ):\n        \"\"\"\n        Yields all the attribute values in xml files which match with the tag name and the specific attribute\n\n        :param str tag_name: specify the tag name\n        :param str attribute: specify the attribute\n        :param bool format_value: specify if the value needs to be formatted with packagename\n        \"\"\"\n        tags = self.find_tags(tag_name, **attribute_filter)\n        for tag in tags:\n            value = tag.get(attribute) or tag.get(self._ns(attribute))\n            if value is not None:\n                if format_value:\n                    yield self._format_value(value)\n                else:\n                    yield value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the value of the attribute in xml files which matches the tag name and the specific attribute name.", "response": "def get_attribute_value(\n        self, tag_name, attribute, format_value=False, **attribute_filter\n    ):\n        \"\"\"\n        Return the attribute value in xml files which matches the tag name and the specific attribute\n\n        :param str tag_name: specify the tag name\n        :param str attribute: specify the attribute\n        :param bool format_value: specify if the value needs to be formatted with packagename\n        \"\"\"\n\n        for value in self.get_all_attribute_value(\n                tag_name, attribute, format_value, **attribute_filter):\n            if value is not None:\n                return value"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the value of the android prefixed attribute in a specific tag.", "response": "def get_value_from_tag(self, tag, attribute):\n        \"\"\"\n        Return the value of the android prefixed attribute in a specific tag.\n\n        This function will always try to get the attribute with a android: prefix first,\n        and will try to return the attribute without the prefix, if the attribute could not be found.\n        This is useful for some broken AndroidManifest.xml, where no android namespace is set,\n        but could also indicate malicious activity (i.e. wrongly repackaged files).\n        A warning is printed if the attribute is found without a namespace prefix.\n\n        If you require to get the exact result you need to query the tag directly:\n\n        example::\n            >>> from lxml.etree import Element\n            >>> tag = Element('bar', nsmap={'android': 'http://schemas.android.com/apk/res/android'})\n            >>> tag.set('{http://schemas.android.com/apk/res/android}foobar', 'barfoo')\n            >>> tag.set('name', 'baz')\n            # Assume that `a` is some APK object\n            >>> a.get_value_from_tag(tag, 'name')\n            'baz'\n            >>> tag.get('name')\n            'baz'\n            >>> tag.get('foobar')\n            None\n            >>> a.get_value_from_tag(tag, 'foobar')\n            'barfoo'\n\n        :param lxml.etree.Element tag: specify the tag element\n        :param str attribute: specify the attribute name\n        :returns: the attribute's value, or None if the attribute is not present\n        \"\"\"\n\n        # TODO: figure out if both android:name and name tag exist which one to give preference:\n        # currently we give preference for the namespace one and fallback to the un-namespaced\n        value = tag.get(self._ns(attribute))\n        if value is None:\n            value = tag.get(attribute)\n\n            if value:\n                # If value is still None, the attribute could not be found, thus is not present\n                log.warning(\"Failed to get the attribute '{}' on tag '{}' with namespace. \"\n                            \"But found the same attribute without namespace!\".format(attribute, tag.tag))\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of all the matched tags in all available xmls.", "response": "def find_tags(self, tag_name, **attribute_filter):\n        \"\"\"\n        Return a list of all the matched tags in all available xml\n\n        :param str tag: specify the tag name\n        \"\"\"\n        all_tags = [\n            self.find_tags_from_xml(\n                i, tag_name, **attribute_filter\n            )\n            for i in self.xml\n        ]\n        return [tag for tag_list in all_tags for tag in tag_list]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef find_tags_from_xml(\n        self, xml_name, tag_name, **attribute_filter\n    ):\n        \"\"\"\n        Return a list of all the matched tags in a specific xml\n        w\n        :param str xml_name: specify from which xml to pick the tag from\n        :param str tag_name: specify the tag name\n        \"\"\"\n        xml = self.xml[xml_name]\n        if xml is None:\n            return []\n        if xml.tag == tag_name:\n            if self.is_tag_matched(\n                xml.tag, **attribute_filter\n            ):\n                return [xml]\n            return []\n        tags = xml.findall(\".//\" + tag_name)\n        return [\n            tag for tag in tags if self.is_tag_matched(\n                tag, **attribute_filter\n            )\n        ]", "response": "Return a list of all the matched tags in a specific xml"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef is_tag_matched(self, tag, **attribute_filter):\n        if len(attribute_filter) <= 0:\n            return True\n        for attr, value in attribute_filter.items():\n            _value = self.get_value_from_tag(tag, attr)\n            if _value != value:\n                return False\n        return True", "response": "r Returns True if the attributes match in attribute filter."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning names of the main activities in the AndroidManifest. xml file.", "response": "def get_main_activities(self):\n        \"\"\"\n        Return names of the main activities\n\n        These values are read from the AndroidManifest.xml\n\n        :rtype: a set of str\n        \"\"\"\n        x = set()\n        y = set()\n\n        for i in self.xml:\n            if self.xml[i] is None:\n                continue\n            activities_and_aliases = self.xml[i].findall(\".//activity\") + \\\n                                     self.xml[i].findall(\".//activity-alias\")\n\n            for item in activities_and_aliases:\n                # Some applications have more than one MAIN activity.\n                # For example: paid and free content\n                activityEnabled = item.get(self._ns(\"enabled\"))\n                if activityEnabled == \"false\":\n                    continue\n\n                for sitem in item.findall(\".//action\"):\n                    val = sitem.get(self._ns(\"name\"))\n                    if val == \"android.intent.action.MAIN\":\n                        activity = item.get(self._ns(\"name\"))\n                        if activity is not None:\n                            x.add(item.get(self._ns(\"name\")))\n                        else:\n                            log.warning('Main activity without name')\n\n                for sitem in item.findall(\".//category\"):\n                    val = sitem.get(self._ns(\"name\"))\n                    if val == \"android.intent.category.LAUNCHER\":\n                        activity = item.get(self._ns(\"name\"))\n                        if activity is not None:\n                            y.add(item.get(self._ns(\"name\")))\n                        else:\n                            log.warning('Launcher activity without name')\n\n        return x.intersection(y)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the name of the main activity in the AndroidManifest. xml", "response": "def get_main_activity(self):\n        \"\"\"\n        Return the name of the main activity\n\n        This value is read from the AndroidManifest.xml\n\n        :rtype: str\n        \"\"\"\n        activities = self.get_main_activities()\n        if len(activities) > 0:\n            return self._format_value(activities.pop())\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_intent_filters(self, itemtype, name):\n        d = {\"action\": [], \"category\": []}\n\n        for i in self.xml:\n            # TODO: this can probably be solved using a single xpath\n            for item in self.xml[i].findall(\".//\" + itemtype):\n                if self._format_value(item.get(self._ns(\"name\"))) == name:\n                    for sitem in item.findall(\".//intent-filter\"):\n                        for ssitem in sitem.findall(\"action\"):\n                            if ssitem.get(self._ns(\"name\")) not in d[\"action\"]:\n                                d[\"action\"].append(ssitem.get(self._ns(\"name\")))\n                        for ssitem in sitem.findall(\"category\"):\n                            if ssitem.get(self._ns(\"name\")) not in d[\"category\"]:\n                                d[\"category\"].append(ssitem.get(self._ns(\"name\")))\n\n        if not d[\"action\"]:\n            del d[\"action\"]\n\n        if not d[\"category\"]:\n            del d[\"category\"]\n\n        return d", "response": "Find intent filters for a given item and name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_uses_implied_permission_list(self):\n        target_sdk_version = self.get_effective_target_sdk_version()\n\n        READ_CALL_LOG = 'android.permission.READ_CALL_LOG'\n        READ_CONTACTS = 'android.permission.READ_CONTACTS'\n        READ_EXTERNAL_STORAGE = 'android.permission.READ_EXTERNAL_STORAGE'\n        READ_PHONE_STATE = 'android.permission.READ_PHONE_STATE'\n        WRITE_CALL_LOG = 'android.permission.WRITE_CALL_LOG'\n        WRITE_CONTACTS = 'android.permission.WRITE_CONTACTS'\n        WRITE_EXTERNAL_STORAGE = 'android.permission.WRITE_EXTERNAL_STORAGE'\n\n        implied = []\n\n        implied_WRITE_EXTERNAL_STORAGE = False\n        if target_sdk_version < 4:\n            if WRITE_EXTERNAL_STORAGE not in self.permissions:\n                implied.append([WRITE_EXTERNAL_STORAGE, None])\n                implied_WRITE_EXTERNAL_STORAGE = True\n            if READ_PHONE_STATE not in self.permissions:\n                implied.append([READ_PHONE_STATE, None])\n\n        if (WRITE_EXTERNAL_STORAGE in self.permissions or implied_WRITE_EXTERNAL_STORAGE) \\\n           and READ_EXTERNAL_STORAGE not in self.permissions:\n            maxSdkVersion = None\n            for name, version in self.uses_permissions:\n                if name == WRITE_EXTERNAL_STORAGE:\n                    maxSdkVersion = version\n                    break\n            implied.append([READ_EXTERNAL_STORAGE, maxSdkVersion])\n\n        if target_sdk_version < 16:\n            if READ_CONTACTS in self.permissions \\\n               and READ_CALL_LOG not in self.permissions:\n                implied.append([READ_CALL_LOG, None])\n            if WRITE_CONTACTS in self.permissions \\\n               and WRITE_CALL_LOG not in self.permissions:\n                implied.append([WRITE_CALL_LOG, None])\n\n        return implied", "response": "Return all permissions implied by the target SDK or other permissions."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_details_permissions(self):\n        l = {}\n\n        for i in self.permissions:\n            if i in self.permission_module:\n                x = self.permission_module[i]\n                l[i] = [x[\"protectionLevel\"], x[\"label\"], x[\"description\"]]\n            else:\n                # FIXME: the permission might be signature, if it is defined by the app itself!\n                l[i] = [\"normal\", \"Unknown permission from android reference\",\n                        \"Unknown permission from android reference\"]\n        return l", "response": "Return details about the permission and the permission module."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the requested permissions declared within AOSP project.", "response": "def get_requested_aosp_permissions(self):\n        \"\"\"\n        Returns requested permissions declared within AOSP project.\n\n        This includes several other permissions as well, which are in the platform apps.\n\n        :rtype: list of str\n        \"\"\"\n        aosp_permissions = []\n        all_permissions = self.get_permissions()\n        for perm in all_permissions:\n            if perm in list(self.permission_module.keys()):\n                aosp_permissions.append(perm)\n        return aosp_permissions"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn requested aosp permissions with details.", "response": "def get_requested_aosp_permissions_details(self):\n        \"\"\"\n        Returns requested aosp permissions with details.\n\n        :rtype: dictionary\n        \"\"\"\n        l = {}\n        for i in self.permissions:\n            try:\n                l[i] = self.permission_module[i]\n            except KeyError:\n                # if we have not found permission do nothing\n                continue\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_requested_third_party_permissions(self):\n        third_party_permissions = []\n        all_permissions = self.get_permissions()\n        for perm in all_permissions:\n            if perm not in list(self.permission_module.keys()):\n                third_party_permissions.append(perm)\n        return third_party_permissions", "response": "Returns list of requested permissions not declared within AOSP project."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the effective targetSdkVersion always returns int > 0.", "response": "def get_effective_target_sdk_version(self):\n        \"\"\"\n            Return the effective targetSdkVersion, always returns int > 0.\n\n            If the targetSdkVersion is not set, it defaults to 1.  This is\n            set based on defaults as defined in:\n            https://developer.android.com/guide/topics/manifest/uses-sdk-element.html\n\n            :rtype: int\n        \"\"\"\n        target_sdk_version = self.get_target_sdk_version()\n        if not target_sdk_version:\n            target_sdk_version = self.get_min_sdk_version()\n        try:\n            return int(target_sdk_version)\n        except (ValueError, TypeError):\n            return 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_certificate_der(self, filename):\n        pkcs7message = self.get_file(filename)\n\n        pkcs7obj = cms.ContentInfo.load(pkcs7message)\n        cert = pkcs7obj['content']['certificates'][0].chosen.dump()\n        return cert", "response": "Returns the DER coded X. 509 certificate from the signature file."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a X.509 certificate object by giving the name in the apk file", "response": "def get_certificate(self, filename):\n        \"\"\"\n        Return a X.509 certificate object by giving the name in the apk file\n\n        :param filename: filename of the signature file in the APK\n        :returns: a :class:`Certificate` certificate\n        \"\"\"\n        cert = self.get_certificate_der(filename)\n        certificate = x509.Certificate.load(cert)\n\n        return certificate"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef new_zip(self, filename, deleted_files=None, new_files={}):\n        zout = zipfile.ZipFile(filename, 'w')\n\n        for item in self.zip.infolist():\n            # Block one: deleted_files, or deleted_files and new_files\n            if deleted_files is not None:\n                if re.match(deleted_files, item.filename) is None:\n                    # if the regex of deleted_files doesn't match the filename\n                    if new_files is not False:\n                        if item.filename in new_files:\n                            # and if the filename is in new_files\n                            zout.writestr(item, new_files[item.filename])\n                            continue\n                    # Otherwise, write the original file.\n                    buffer = self.zip.read(item.filename)\n                    zout.writestr(item, buffer)\n            # Block two: deleted_files is None, new_files is not empty\n            elif new_files is not False:\n                if item.filename in new_files:\n                    zout.writestr(item, new_files[item.filename])\n                else:\n                    buffer = self.zip.read(item.filename)\n                    zout.writestr(item, buffer)\n            # Block three: deleted_files is None, new_files is empty.\n            # Just write out the default zip\n            else:\n                buffer = self.zip.read(item.filename)\n                zout.writestr(item, buffer)\n        zout.close()", "response": "Create a new zip file containing the current version of the file."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef parse_signatures_or_digests(self, digest_bytes):\n\n        if not len(digest_bytes):\n            return []\n        \n        digests = []\n        block = io.BytesIO(digest_bytes)\n\n        data_len = self.read_uint32_le(block)\n        while block.tell() < data_len:\n\n            algorithm_id = self.read_uint32_le(block)\n            digest_len = self.read_uint32_le(block)\n            digest = block.read(digest_len)\n\n            digests.append((algorithm_id, digest))\n\n        return digests", "response": "Parse signatures or digests"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse_v3_signing_block(self):\n\n        self._v3_signing_data = []\n\n        # calling is_signed_v3 should also load the signature, if any\n        if not self.is_signed_v3():\n            return\n\n        block_bytes = self._v2_blocks[self._APK_SIG_KEY_V3_SIGNATURE]\n        block = io.BytesIO(block_bytes)\n        view = block.getvalue()\n\n        # V3 signature Block data format:\n        #\n        # * signer:\n        #    * signed data:\n        #        * digests:\n        #            * signature algorithm ID (uint32)\n        #            * digest (length-prefixed) \n        #        * certificates\n        #        * minSDK\n        #        * maxSDK\n        #        * additional attributes\n        #    * minSDK\n        #    * maxSDK\n        #    * signatures\n        #    * publickey\n        size_sequence = self.read_uint32_le(block)\n        if size_sequence + 4 != len(block_bytes):\n            raise BrokenAPKError(\"size of sequence and blocksize does not match\")\n\n        while block.tell() < len(block_bytes):\n            off_signer = block.tell()\n            size_signer = self.read_uint32_le(block)\n\n            # read whole signed data, since we might to parse\n            # content within the signed data, and mess up offset\n            len_signed_data = self.read_uint32_le(block)\n            signed_data_bytes = block.read(len_signed_data)\n            signed_data = io.BytesIO(signed_data_bytes)\n\n            # Digests\n            len_digests = self.read_uint32_le(signed_data)\n            raw_digests = signed_data.read(len_digests)\n            digests = self.parse_signatures_or_digests(raw_digests)\n\n\n            # Certs\n            certs = []\n            len_certs = self.read_uint32_le(signed_data)\n            start_certs = signed_data.tell()\n            while signed_data.tell() < start_certs + len_certs:\n\n                len_cert = self.read_uint32_le(signed_data)\n                cert = signed_data.read(len_cert)\n                certs.append(cert)\n\n            # versions\n            signed_data_min_sdk = self.read_uint32_le(signed_data)\n            signed_data_max_sdk = self.read_uint32_le(signed_data)\n\n            # Addional attributes\n            len_attr = self.read_uint32_le(signed_data)\n            attr = signed_data.read(len_attr)\n\n            signed_data_object = APKV3SignedData()\n            signed_data_object._bytes = signed_data_bytes\n            signed_data_object.digests = digests\n            signed_data_object.certificates = certs\n            signed_data_object.additional_attributes = attr\n            signed_data_object.minSDK = signed_data_min_sdk\n            signed_data_object.maxSDK = signed_data_max_sdk\n\n            # versions (should be the same as signed data's versions)\n            signer_min_sdk = self.read_uint32_le(block)\n            signer_max_sdk = self.read_uint32_le(block)\n\n            # Signatures\n            len_sigs = self.read_uint32_le(block)\n            raw_sigs = block.read(len_sigs)\n            sigs = self.parse_signatures_or_digests(raw_sigs)\n\n            # PublicKey\n            len_publickey = self.read_uint32_le(block)\n            publickey = block.read(len_publickey)\n\n            signer = APKV3Signer()\n            signer._bytes = view[off_signer:off_signer+size_signer]\n            signer.signed_data = signed_data_object\n            signer.signatures = sigs\n            signer.public_key = publickey\n            signer.minSDK = signer_min_sdk\n            signer.maxSDK = signer_max_sdk\n\n            self._v3_signing_data.append(signer)", "response": "Parse the V3 signing block and extract all features that are present in the v2 file."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nparse the V2 signing block and extract all features that are present in the signature key.", "response": "def parse_v2_signing_block(self):\n        \"\"\"\n        Parse the V2 signing block and extract all features\n        \"\"\"\n\n        self._v2_signing_data = []\n\n        # calling is_signed_v2 should also load the signature\n        if not self.is_signed_v2():\n            return\n\n        block_bytes = self._v2_blocks[self._APK_SIG_KEY_V2_SIGNATURE]\n        block = io.BytesIO(block_bytes)\n        view = block.getvalue()\n\n        # V2 signature Block data format:\n        #\n        # * signer:\n        #    * signed data:\n        #        * digests:\n        #            * signature algorithm ID (uint32)\n        #            * digest (length-prefixed) \n        #        * certificates\n        #        * additional attributes\n        #    * signatures\n        #    * publickey\n\n        size_sequence = self.read_uint32_le(block)\n        if size_sequence + 4 != len(block_bytes):\n            raise BrokenAPKError(\"size of sequence and blocksize does not match\")\n\n        while block.tell() < len(block_bytes):\n            off_signer = block.tell()\n            size_signer = self.read_uint32_le(block)\n\n            # read whole signed data, since we might to parse\n            # content within the signed data, and mess up offset\n            len_signed_data = self.read_uint32_le(block)\n            signed_data_bytes = block.read(len_signed_data)\n            signed_data = io.BytesIO(signed_data_bytes)\n\n            # Digests\n            len_digests = self.read_uint32_le(signed_data)\n            raw_digests = signed_data.read(len_digests)\n            digests = self.parse_signatures_or_digests(raw_digests)\n\n            # Certs\n            certs = []\n            len_certs = self.read_uint32_le(signed_data)\n            start_certs = signed_data.tell()\n            while signed_data.tell() < start_certs + len_certs:\n                len_cert = self.read_uint32_le(signed_data)\n                cert = signed_data.read(len_cert)\n                certs.append(cert)\n\n            # Additional attributes\n            len_attr = self.read_uint32_le(signed_data)\n            attributes = signed_data.read(len_attr)\n\n            signed_data_object = APKV2SignedData()\n            signed_data_object._bytes = signed_data_bytes\n            signed_data_object.digests = digests\n            signed_data_object.certificates = certs\n            signed_data_object.additional_attributes = attributes\n\n            # Signatures\n            len_sigs = self.read_uint32_le(block)\n            raw_sigs = block.read(len_sigs)\n            sigs = self.parse_signatures_or_digests(raw_sigs)\n\n            # PublicKey\n            len_publickey = self.read_uint32_le(block)\n            publickey = block.read(len_publickey)\n\n            signer = APKV2Signer()\n            signer._bytes = view[off_signer:off_signer+size_signer]\n            signer.signed_data = signed_data_object\n            signer.signatures = sigs\n            signer.public_key = publickey\n\n            self._v2_signing_data.append(signer)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_public_keys_der_v3(self):\n\n        if self._v3_signing_data == None:\n            self.parse_v3_signing_block()\n\n        public_keys = []\n\n        for signer in self._v3_signing_data:\n            public_keys.append(signer.public_key)\n\n        return public_keys", "response": "Return a list of DER coded X. 509 public keys from the v3 signature block"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of DER coded X. 509 public keys from the v3 signature block", "response": "def get_public_keys_der_v2(self):\n        \"\"\"\n        Return a list of DER coded X.509 public keys from the v3 signature block\n        \"\"\"\n\n        if self._v2_signing_data == None:\n            self.parse_v2_signing_block()\n\n        public_keys = []\n\n        for signer in self._v2_signing_data:\n            public_keys.append(signer.public_key)\n\n        return public_keys"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a list of DER coded X.509 certificates from the v3 signature block.", "response": "def get_certificates_der_v3(self):\n        \"\"\"\n        Return a list of DER coded X.509 certificates from the v3 signature block\n        \"\"\"\n\n        if self._v3_signing_data == None:\n            self.parse_v3_signing_block()\n\n        certs = []\n        for signed_data in [signer.signed_data for signer in self._v3_signing_data]:\n            for cert in signed_data.certificates:\n                certs.append(cert)\n\n        return certs"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_certificates_der_v2(self):\n\n        if self._v2_signing_data == None:\n            self.parse_v2_signing_block()\n\n        certs = []\n        for signed_data in [signer.signed_data for signer in self._v2_signing_data]:\n            for cert in signed_data.certificates:\n                certs.append(cert)\n\n        return certs", "response": "Return a list of DER coded X.509 certificates from the v3 signature block"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of x509. Certificate objects which are found in the META - INF folder v1 signing.", "response": "def get_certificates_v1(self):\n        \"\"\"\n        Return a list of :class:`asn1crypto.x509.Certificate` which are found\n        in the META-INF folder (v1 signing).\n        Note that we simply extract all certificates regardless of the signer.\n        Therefore this is just a list of all certificates found in all signers.\n        \"\"\"\n        certs = []\n        for x in self.get_signature_names():\n            certs.append(x509.Certificate.load(self.get_certificate_der(x)))\n\n        return certs"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a list of unique ASN. 1. 1 certificates in v1 v2 and v3 signing ArcGIS.", "response": "def get_certificates(self):\n        \"\"\"\n        Return a list of unique :class:`asn1crypto.x509.Certificate` which are found\n        in v1, v2 and v3 signing\n        Note that we simply extract all certificates regardless of the signer.\n        Therefore this is just a list of all certificates found in all signers.\n        \"\"\"\n        fps = []\n        certs = []\n        for x in self.get_certificates_v1() + self.get_certificates_v2() + self.get_certificates_v3():\n            if x.sha256 not in fps:\n                fps.append(x.sha256)\n                certs.append(x)\n        return certs"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of the signature file names matching a Signature", "response": "def get_signature_names(self):\n        \"\"\"\n        Return a list of the signature file names (v1 Signature / JAR\n        Signature)\n\n        :rtype: List of filenames matching a Signature\n        \"\"\"\n        signature_expr = re.compile(r\"^(META-INF/)(.*)(\\.RSA|\\.EC|\\.DSA)$\")\n        signatures = []\n\n        for i in self.get_files():\n            if signature_expr.search(i):\n                if \"{}.SF\".format(i.rsplit(\".\", 1)[0]) in self.get_files():\n                    signatures.append(i)\n                else:\n                    log.warning(\"v1 signature file {} missing .SF file - Partial signature!\".format(i))\n\n        return signatures"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_signatures(self):\n        signature_expr = re.compile(r\"^(META-INF/)(.*)(\\.RSA|\\.EC|\\.DSA)$\")\n        signature_datas = []\n\n        for i in self.get_files():\n            if signature_expr.search(i):\n                signature_datas.append(self.get_file(i))\n\n        return signature_datas", "response": "Return a list of the data of the signature files. Only v1 JAR Signing."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nexports analysis method to dot format", "response": "def method2dot(mx, colors=None):\n    \"\"\"\n    Export analysis method to dot format\n\n    :param mx: :class:`~androguard.core.analysis.analysis.MethodAnalysis`\n    :param colors: dict of colors to use, if colors is None the default colors are used\n\n    :returns: a string which contains the dot graph\n    \"\"\"\n\n    if not colors:\n        colors = {\n            \"true_branch\": \"green\",\n            \"false_branch\": \"red\",\n            \"default_branch\": \"purple\",\n            \"jump_branch\": \"blue\",\n            \"bg_idx\": \"lightgray\",\n            \"idx\": \"blue\",\n            \"bg_start_idx\": \"yellow\",\n            \"bg_instruction\": \"lightgray\",\n            \"instruction_name\": \"black\",\n            \"instructions_operands\": \"yellow\",\n            \"raw\": \"red\",\n            \"string\": \"red\",\n            \"literal\": \"green\",\n            \"offset\": \"#4000FF\",\n            \"method\": \"#DF3A01\",\n            \"field\": \"#088A08\",\n            \"type\": \"#0000FF\",\n            \"registers_range\": (\"#999933\", \"#6666FF\")\n        }\n\n    node_tpl = \"\\nstruct_%s [label=<\\n<TABLE BORDER=\\\"0\\\" CELLBORDER=\\\"0\\\" CELLSPACING=\\\"3\\\">\\n%s</TABLE>>];\\n\"\n    label_tpl = \"<TR><TD ALIGN=\\\"LEFT\\\" BGCOLOR=\\\"%s\\\"> <FONT FACE=\\\"Times-Bold\\\" color=\\\"%s\\\">%x</FONT> </TD><TD ALIGN=\\\"LEFT\\\" BGCOLOR=\\\"%s\\\"> <FONT FACE=\\\"Times-Bold\\\" color=\\\"%s\\\">%s </FONT> %s </TD></TR>\\n\"\n    link_tpl = \"<TR><TD PORT=\\\"%s\\\"></TD></TR>\\n\"\n\n    edges_html = \"\"\n    blocks_html = \"\"\n\n    method = mx.get_method()\n    sha256 = hashlib.sha256(bytearray(\"{}{}{}\".format(\n        mx.get_method().get_class_name(), mx.get_method().get_name(),\n        mx.get_method().get_descriptor()), \"UTF-8\")).hexdigest()\n\n    registers = {}\n    if method.get_code():\n        for DVMBasicMethodBlock in mx.basic_blocks.gets():\n            for DVMBasicMethodBlockInstruction in DVMBasicMethodBlock.get_instructions():\n                operands = DVMBasicMethodBlockInstruction.get_operands(0)\n                for register in operands:\n                    if register[0] == 0:\n                        if register[1] not in registers:\n                            registers[register[1]] = 0\n                        registers[register[1]] += 1\n#        for i in range(method.get_code().get_registers_size()):\n#            registers[i] = 0\n\n    if registers:\n        registers_colors = color_range(colors[\"registers_range\"][0],\n                                       colors[\"registers_range\"][1],\n                                       len(registers))\n        for i in registers:\n            registers[i] = registers_colors.pop(0)\n\n    new_links = []\n\n    for DVMBasicMethodBlock in mx.basic_blocks.gets():\n        ins_idx = DVMBasicMethodBlock.start\n        block_id = hashlib.md5(bytearray(sha256 + DVMBasicMethodBlock.get_name(), \"UTF-8\")).hexdigest()\n\n        content = link_tpl % 'header'\n\n        for DVMBasicMethodBlockInstruction in DVMBasicMethodBlock.get_instructions():\n            if DVMBasicMethodBlockInstruction.get_op_value(\n            ) == 0x2b or DVMBasicMethodBlockInstruction.get_op_value() == 0x2c:\n                new_links.append((DVMBasicMethodBlock, ins_idx,\n                                  DVMBasicMethodBlockInstruction.get_ref_off() * 2 + ins_idx))\n            elif DVMBasicMethodBlockInstruction.get_op_value() == 0x26:\n                new_links.append((DVMBasicMethodBlock, ins_idx,\n                                  DVMBasicMethodBlockInstruction.get_ref_off() * 2 + ins_idx))\n\n            operands = DVMBasicMethodBlockInstruction.get_operands(ins_idx)\n            output = \", \".join(mx.get_vm().get_operand_html(\n                i, registers, colors, escape, textwrap.wrap) for i in operands)\n\n            formatted_operands = DVMBasicMethodBlockInstruction.get_formatted_operands(\n            )\n            if formatted_operands:\n                output += \" ; %s\" % str(formatted_operands)\n\n            bg_idx = colors[\"bg_idx\"]\n            if ins_idx == 0 and \"bg_start_idx\" in colors:\n                bg_idx = colors[\"bg_start_idx\"]\n\n            content += label_tpl % (\n                bg_idx, colors[\"idx\"], ins_idx, colors[\"bg_instruction\"],\n                colors[\"instruction_name\"],\n                DVMBasicMethodBlockInstruction.get_name(), output)\n\n            ins_idx += DVMBasicMethodBlockInstruction.get_length()\n            last_instru = DVMBasicMethodBlockInstruction\n\n        # all blocks from one method parsed\n        # updating dot HTML content\n        content += link_tpl % 'tail'\n        blocks_html += node_tpl % (block_id, content)\n\n        # Block edges color treatment (conditional branchs colors)\n        val = colors[\"true_branch\"]\n        if len(DVMBasicMethodBlock.childs) > 1:\n            val = colors[\"false_branch\"]\n        elif len(DVMBasicMethodBlock.childs) == 1:\n            val = colors[\"jump_branch\"]\n\n        values = None\n        if (last_instru.get_op_value() == 0x2b or\n                last_instru.get_op_value() == 0x2c\n           ) and len(DVMBasicMethodBlock.childs) > 1:\n            val = colors[\"default_branch\"]\n            values = [\"default\"]\n            values.extend(DVMBasicMethodBlock.get_special_ins(\n                ins_idx - last_instru.get_length()).get_values())\n\n        # updating dot edges\n        for DVMBasicMethodBlockChild in DVMBasicMethodBlock.childs:\n            label_edge = \"\"\n\n            if values:\n                label_edge = values.pop(0)\n\n            child_id = hashlib.md5(\n                bytearray(sha256 + DVMBasicMethodBlockChild[-1].get_name(), \"UTF-8\")).hexdigest()\n            edges_html += \"struct_{}:tail -> struct_{}:header  [color=\\\"{}\\\", label=\\\"{}\\\"];\\n\".format(\n                block_id, child_id, val, label_edge)\n            # color switch\n            if val == colors[\"false_branch\"]:\n                val = colors[\"true_branch\"]\n            elif val == colors[\"default_branch\"]:\n                val = colors[\"true_branch\"]\n\n        exception_analysis = DVMBasicMethodBlock.get_exception_analysis()\n        if exception_analysis:\n            for exception_elem in exception_analysis.exceptions:\n                exception_block = exception_elem[-1]\n                if exception_block:\n                    exception_id = hashlib.md5(\n                        bytearray(sha256 + exception_block.get_name(), \"UTF-8\")).hexdigest()\n                    edges_html += \"struct_{}:tail -> struct_{}:header  [color=\\\"{}\\\", label=\\\"{}\\\"];\\n\".format(\n                        block_id, exception_id, \"black\", exception_elem[0])\n\n    for link in new_links:\n        DVMBasicMethodBlock = link[0]\n        DVMBasicMethodBlockChild = mx.basic_blocks.get_basic_block(link[2])\n\n        if DVMBasicMethodBlockChild:\n            block_id = hashlib.md5(bytearray(sha256 + DVMBasicMethodBlock.get_name(\n            ), \"UTF-8\")).hexdigest()\n            child_id = hashlib.md5(bytearray(sha256 + DVMBasicMethodBlockChild.get_name(\n            ), \"UTF-8\")).hexdigest()\n\n            edges_html += \"struct_{}:tail -> struct_{}:header  [color=\\\"{}\\\", label=\\\"data(0x{:x}) to @0x{:x}\\\", style=\\\"dashed\\\"];\\n\".format(\n                block_id, child_id, \"yellow\", link[1], link[2])\n\n    method_label = method.get_class_name() + \".\" + method.get_name(\n    ) + \"->\" + method.get_descriptor()\n\n    method_information = method.get_information()\n    if method_information:\n        method_label += \"\\\\nLocal registers v%d ... v%d\" % (\n            method_information[\"registers\"][0],\n            method_information[\"registers\"][1])\n        if \"params\" in method_information:\n            for register, rtype in method_information[\"params\"]:\n                method_label += \"\\\\nparam v%d = %s\" % (register, rtype)\n        method_label += \"\\\\nreturn = %s\" % (method_information[\"return\"])\n\n    return {'name': method_label, 'nodes': blocks_html, 'edges': edges_html}"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef method2format(output, _format=\"png\", mx=None, raw=None):\n    # pydot is optional!\n    import pydot\n\n    buff = \"digraph {\\n\"\n    buff += \"graph [rankdir=TB]\\n\"\n    buff += \"node [shape=plaintext]\\n\"\n\n    if raw:\n        data = raw\n    else:\n        data = method2dot(mx)\n\n    # subgraphs cluster\n    buff += \"subgraph cluster_{} \".format(hashlib.md5(bytearray(output, \"UTF-8\")).hexdigest())\n    buff += \"{\\n\"\n    buff += \"label=\\\"{}\\\"\\n\".format(data['name'])\n    buff += data['nodes']\n    buff += \"}\\n\"\n\n    # subgraphs edges\n    buff += data['edges']\n    buff += \"}\\n\"\n\n    d = pydot.graph_from_dot_data(buff)\n    if d:\n        for g in d:\n            getattr(g, \"write_\" + _format.lower())(output)", "response": "Export method to a specific file format"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nexporting method to a png file format", "response": "def method2png(output, mx, raw=False):\n    \"\"\"\n    Export method to a png file format\n\n    :param output: output filename\n    :type output: string\n    :param mx: specify the MethodAnalysis object\n    :type mx: :class:`MethodAnalysis` object\n    :param raw: use directly a dot raw buffer\n    :type raw: string\n    \"\"\"\n    buff = raw\n    if not raw:\n        buff = method2dot(mx)\n\n    method2format(output, \"png\", mx, buff)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef method2jpg(output, mx, raw=False):\n    buff = raw\n    if not raw:\n        buff = method2dot(mx)\n\n    method2format(output, \"jpg\", mx, buff)", "response": "This function exports the method to a jpg file format"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets a JSON representation of a DEX file", "response": "def vm2json(vm):\n    \"\"\"\n    Get a JSON representation of a DEX file\n\n    :param vm: :class:`~androguard.core.bytecodes.dvm.DalvikVMFormat`\n    :return:\n    \"\"\"\n    d = {\"name\": \"root\", \"children\": []}\n\n    for _class in vm.get_classes():\n        c_class = {\"name\": _class.get_name(), \"children\": []}\n\n        for method in _class.get_methods():\n            c_method = {\"name\": method.get_name(), \"children\": []}\n\n            c_class[\"children\"].append(c_method)\n\n        d[\"children\"].append(c_class)\n\n    return json.dumps(d)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nconvert a object to a bytearray or call get_raw of the object.", "response": "def object_to_bytes(obj):\n    \"\"\"\n    Convert a object to a bytearray or call get_raw() of the object\n    if no useful type was found.\n    \"\"\"\n    if isinstance(obj, str):\n        return bytearray(obj, \"UTF-8\")\n    elif isinstance(obj, bool):\n        return bytearray()\n    elif isinstance(obj, int):\n        return pack(\"<L\", obj)\n    elif obj is None:\n        return bytearray()\n    elif isinstance(obj, bytearray):\n        return obj\n    else:\n        return obj.get_raw()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransform a typed class name into a form which can be used as a python attribute", "response": "def FormatClassToPython(i):\n    \"\"\"\n    Transform a typed class name into a form which can be used as a python\n    attribute\n\n    example::\n\n        >>> FormatClassToPython('Lfoo/bar/foo/Barfoo$InnerClass;')\n        'Lfoo_bar_foo_Barfoo_InnerClass'\n\n    :param i: classname to transform\n    :rtype: str\n    \"\"\"\n    i = i[:-1]\n    i = i.replace(\"/\", \"_\")\n    i = i.replace(\"$\", \"_\")\n\n    return i"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the package and class name in a typed variant name.", "response": "def get_package_class_name(name):\n    \"\"\"\n    Return package and class name in a java variant from a typed variant name.\n\n    If no package could be found, the package is an empty string.\n\n    example::\n\n        >>> get_package_class_name('Ljava/lang/Object;')\n        ('java.lang', 'Object')\n\n    :param name: the name\n    :rtype: tuple\n    :return:\n    \"\"\"\n    if name[0] != 'L' and name[-1] != ';':\n        raise ValueError(\"The name '{}' does not look like a typed name!\".format(name))\n\n    name = name[1:-1]\n    if '/' not in name:\n        return '', name\n\n    package, clsname = name.rsplit('/', 1)\n    package = package.replace('/', '.')\n\n    return package, clsname"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ntransform a ( method ) name into a form which can be used as a python attribute", "response": "def FormatNameToPython(i):\n    \"\"\"\n    Transform a (method) name into a form which can be used as a python\n    attribute\n\n    example::\n\n        >>> FormatNameToPython('<clinit>')\n        'clinit'\n\n    :param i: name to transform\n    :rtype: str\n    \"\"\"\n\n    i = i.replace(\"<\", \"\")\n    i = i.replace(\">\", \"\")\n    i = i.replace(\"$\", \"_\")\n\n    return i"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef FormatDescriptorToPython(i):\n\n    i = i.replace(\"/\", \"_\")\n    i = i.replace(\";\", \"\")\n    i = i.replace(\"[\", \"\")\n    i = i.replace(\"(\", \"\")\n    i = i.replace(\")\", \"\")\n    i = i.replace(\" \", \"\")\n    i = i.replace(\"$\", \"\")\n\n    return i", "response": "Formats a descriptor into a form which can be used as a python attribute\n    example ::\n    = > LJ_LZZZV >>> FormatDescriptorToPython = > LJ_LZZV >>> FormatDescriptorToPython = > LJ_LZZV >>> FormatDescriptorToPython = > LJ_LZZV >>> FormatDescriptorToPython = > LJ_LZZV >>> FormatDescriptorToPython = > LJ_LZ"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef read_b(self, size):\n        return self.__buff[self.__idx:self.__idx + size]", "response": "Read bytes with length size without incrementing the current offset"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef readat(self, off):\n        if isinstance(off, SV):\n            off = off.value\n\n        return self.__buff[off:]", "response": "Reads all bytes from the start of off until the end of the buffer."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef read(self, size):\n        if isinstance(size, SV):\n            size = size.value\n\n        buff = self.__buff[self.__idx:self.__idx + size]\n        self.__idx += size\n\n        return buff", "response": "Reads from the current offset a total number of bytes\n        and increments the offset by size bytes\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the current buffer to filename", "response": "def save(self, filename):\n        \"\"\"\n        Save the current buffer to `filename`\n\n        Exisiting files with the same name will be overwritten.\n\n        :param str filename: the name of the file to save to\n        \"\"\"\n        with open(filename, \"wb\") as fd:\n            fd.write(self.__buff)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreading a null terminated string from a file - like object.", "response": "def read_null_terminated_string(f):\n    \"\"\"\n    Read a null terminated string from a file-like object.\n\n    :param f: file-like object\n    :rtype: bytearray\n    \"\"\"\n    x = bytearray()\n    while True:\n        z = f.read(1)\n        if ord(z) == 0:\n            return x\n        else:\n            x.append(ord(z))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ntransforms an access flag field to the corresponding string", "response": "def get_access_flags_string(value):\n    \"\"\"\n    Transform an access flag field to the corresponding string\n\n    :param value: the value of the access flags\n    :type value: int\n\n    :rtype: string\n    \"\"\"\n    flags = []\n    for k, v in ACCESS_FLAGS.items():\n        if (k & value) == k:\n            flags.append(v)\n\n    return \" \".join(flags)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nread a signed LEB128 at the current position of the buffer.", "response": "def readsleb128(buff):\n    \"\"\"\n    Read a signed LEB128 at the current position of the buffer.\n\n    :param buff: a file like object\n    :return: decoded sLEB128\n    \"\"\"\n    result = 0\n    shift = 0\n\n    for x in range(0, 5):\n        cur = get_byte(buff)\n        result |= (cur & 0x7f) << shift\n        shift += 7\n\n        if not cur & 0x80:\n            bit_left = max(32 - shift, 0)\n            result = result << bit_left\n            if result > 0x7fffffff:\n                result = (0x7fffffff & result) - 0x80000000\n            result = result >> bit_left\n            break\n\n    return result"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndetermines the next offset inside the bytecode of a method.", "response": "def determineNext(i, cur_idx, m):\n    \"\"\"\n    Determine the next offsets inside the bytecode of an :class:`EncodedMethod`.\n    The offsets are calculated in number of bytes from the start of the method.\n    Note, that offsets inside the bytecode are denoted in 16bit units but this method returns actual bytes!\n\n    Offsets inside the opcode are counted from the beginning of the opcode.\n\n    The returned type is a list, as branching opcodes will have multiple paths.\n    `if` and `switch` opcodes will return more than one item in the list, while\n    `throw`, `return` and `goto` opcodes will always return a list with length one.\n\n    An offset of -1 indicates that the method is exited, for example by `throw` or `return`.\n\n    If the entered opcode is not branching or jumping, an empty list is returned.\n\n    :param Instruction i: the current Instruction\n    :param int cur_idx: Index of the instruction\n    :param EncodedMethod m: the current method\n    :return:\n    :rtype: list\n    \"\"\"\n    op_value = i.get_op_value()\n\n    if (op_value == 0x27) or (0x0e <= op_value <= 0x11):\n        # throw + return*\n        return [-1]\n    elif 0x28 <= op_value <= 0x2a:\n        # all kind of 'goto'\n        off = i.get_ref_off() * 2\n        return [off + cur_idx]\n    elif 0x32 <= op_value <= 0x3d:\n        # all kind of 'if'\n        off = i.get_ref_off() * 2\n        return [cur_idx + i.get_length(), off + cur_idx]\n    elif op_value in (0x2b, 0x2c):\n        # packed/sparse switch\n        # Code flow will continue after the switch command\n        x = [cur_idx + i.get_length()]\n\n        # The payload must be read at the offset position\n        code = m.get_code().get_bc()\n        off = i.get_ref_off() * 2\n\n        # See DEX bytecode documentation:\n        # \"the instructions must be located on even-numbered bytecode offsets (that is, 4-byte aligned).\n        # In order to meet this requirement, dex generation tools must\n        # emit an extra nop instruction as a spacer if such an instruction would otherwise be unaligned.\"\n        padding = (off + cur_idx) % 4\n        if padding != 0:\n            log.warning(\"Switch payload not aligned, assume stuff and add {} bytes...\".format(padding))\n        data = code.get_ins_off(off + cur_idx + padding)\n\n        # TODO: some malware points to invalid code\n        # Does Android ignores the nop and searches for the switch payload?\n        # So we make sure that this is a switch payload\n        if data and (isinstance(data, PackedSwitch) or isinstance(data, SparseSwitch)):\n            for target in data.get_targets():\n                x.append(target * 2 + cur_idx)\n        else:\n            log.warning(\"Could not determine payload of switch command at offset {} inside {}! \"\n                        \"Possibly broken bytecode?\".format(cur_idx, m))\n\n        return x\n    return []"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the list of exceptions inside the method.", "response": "def determineException(vm, m):\n    \"\"\"\n    Returns try-catch handler inside the method.\n\n    :param vm: a :class:`~DalvikVMFormat`\n    :param m: a :class:`~EncodedMethod`\n    :return:\n    \"\"\"\n    # no exceptions !\n    if m.get_code().get_tries_size() <= 0:\n        return []\n\n    h_off = {}\n\n    handler_catch_list = m.get_code().get_handlers()\n\n    for try_item in m.get_code().get_tries():\n        offset_handler = try_item.get_handler_off(\n        ) + handler_catch_list.get_off()\n        if offset_handler in h_off:\n            h_off[offset_handler].append([try_item])\n        else:\n            h_off[offset_handler] = []\n            h_off[offset_handler].append([try_item])\n\n    # print m.get_name(), \"\\t HANDLER_CATCH_LIST SIZE\", handler_catch_list.size, handler_catch_list.get_offset()\n    for handler_catch in handler_catch_list.get_list():\n        if handler_catch.get_off() not in h_off:\n            continue\n\n        for i in h_off[handler_catch.get_off()]:\n            i.append(handler_catch)\n\n    exceptions = []\n    # print m.get_name(), h_off\n    for i in h_off:\n        for value in h_off[i]:\n            try_value = value[0]\n\n            z = [try_value.get_start_addr() * 2,\n                 (try_value.get_start_addr() * 2) +\n                 (try_value.get_insn_count() * 2) - 1]\n\n            handler_catch = value[1]\n            if handler_catch.get_size() <= 0:\n                z.append([\"Ljava/lang/Throwable;\",\n                          handler_catch.get_catch_all_addr() * 2])\n\n            for handler in handler_catch.get_handlers():\n                z.append([vm.get_cm_type(handler.get_type_idx()),\n                          handler.get_addr() * 2])\n\n            exceptions.append(z)\n\n    # print m.get_name(), exceptions\n    return exceptions"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_kind(cm, kind, value):\n    if kind == KIND_METH:\n        method = cm.get_method_ref(value)\n        class_name = method.get_class_name()\n        name = method.get_name()\n        descriptor = method.get_descriptor()\n\n        return \"{}->{}{}\".format(class_name, name, descriptor)\n\n    elif kind == KIND_STRING:\n        return repr(cm.get_string(value))\n\n    elif kind == KIND_RAW_STRING:\n        return cm.get_string(value)\n\n    elif kind == KIND_FIELD:\n        class_name, proto, field_name = cm.get_field(value)\n        return \"{}->{} {}\".format(class_name, field_name, proto)\n\n    elif kind == KIND_TYPE:\n        return cm.get_type(value)\n\n    elif kind == VTABLE_OFFSET:\n        return \"vtable[0x%x]\" % value\n\n    elif kind == FIELD_OFFSET:\n        return \"field[0x%x]\" % value\n\n    elif kind == INLINE_METHOD:\n        buff = \"inline[0x%x]\" % value\n\n        # FIXME: depends of the android version ...\n        if len(INLINE_METHODS) > value:\n            elem = INLINE_METHODS[value]\n            buff += \" {}->{}{}\".format(elem[0], elem[1], elem[2])\n\n        return buff\n\n    return None", "response": "Get the value of the kind argument"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_unicode(self):\n        s = mutf8.decode(self.data)\n        if len(s) != self.utf16_size:\n            raise ValueError(\"UTF16 Length does not match!\")\n\n        # Return a UTF16 String\n        return s", "response": "Returns an Unicode String that is the actual string of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the class name of the field.", "response": "def get_class_name(self):\n        \"\"\"\n        Return the class name of the field\n\n        :rtype: string\n        \"\"\"\n        if self.class_idx_value is None:\n            self.class_idx_value = self.CM.get_type(self.class_idx)\n\n        return self.class_idx_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the prototype of the method", "response": "def get_proto(self):\n        \"\"\"\n        Return the prototype of the method\n\n        :rtype: string\n        \"\"\"\n        if self.proto_idx_value is None:\n            self.proto_idx_value = self.CM.get_proto(self.proto_idx)\n\n        return self.proto_idx_value"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show(self):\n        bytecode._PrintSubBanner(\"Field Information\")\n        bytecode._PrintDefault(\"{}->{} {} [access_flags={}]\\n\".format(\n            self.get_class_name(), self.get_name(), self.get_descriptor(),\n            self.get_access_flags_string()))\n\n        init_value = self.get_init_value()\n        if init_value is not None:\n            bytecode._PrintDefault(\"\\tinit value: %s\\n\" %\n                                   str(init_value.get_value()))", "response": "Display the information about the field."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef each_params_by_register(self, nb, proto):\n        bytecode._PrintSubBanner(\"Params\")\n\n        ret = proto.split(')')\n        params = ret[0][1:].split()\n        if params:\n            bytecode._PrintDefault(\"- local registers: v%d...v%d\\n\" %\n                                   (0, nb - len(params) - 1))\n            j = 0\n            for i in range(nb - len(params), nb):\n                bytecode._PrintDefault(\"- v%d: %s\\n\" % (i, get_type(params[j])))\n                j += 1\n        else:\n            bytecode._PrintDefault(\"local registers: v%d...v%d\\n\" % (0, nb - 1))\n\n        bytecode._PrintDefault(\"- return: %s\\n\" % get_type(ret[1]))\n        bytecode._PrintSubBanner()", "response": "This method prints out the usage of the parameters of a method."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_short_string(self):\n        def _fmt_classname(cls):\n            arr = \"\"\n            # Test for arrays\n            while cls.startswith(\"[\"):\n                arr += \"[\"\n                cls = cls[1:]\n\n            # is a object type\n            if cls.startswith(\"L\"):\n                cls = cls[1:-1]\n            # only return last element\n            if \"/\" in cls:\n                cls = cls.rsplit(\"/\", 1)[1]\n            return arr + cls\n\n        clsname = _fmt_classname(self.get_class_name())\n\n        param, ret = self.get_descriptor()[1:].split(\")\")\n        params = map(_fmt_classname, param.split(\" \"))\n        desc = \"({}){}\".format(\" \".join(params), _fmt_classname(ret))\n\n        return \"{cls} {meth} {desc}\".format(cls=clsname, meth=self.get_name(), desc=desc)", "response": "Returns a shorter formatted String which encodes this method."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ndisplay the information about the method and its associated class.", "response": "def show(self):\n        \"\"\"\n        Display the information (with a pretty print) about the method\n        \"\"\"\n        self.show_info()\n        self.show_notes()\n        if self.code:\n            self.each_params_by_register(self.code.get_registers_size(), self.get_descriptor())\n            self.code.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef show_notes(self):\n        if self.notes:\n            bytecode._PrintSubBanner(\"Notes\")\n            for i in self.notes:\n                bytecode._PrintNote(i)\n            bytecode._PrintSubBanner()", "response": "Display the notes about the method."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_instruction(self, idx, off=None):\n        if self.code is not None:\n            return self.code.get_bc().get_instruction(idx, off)\n        return None", "response": "Get a particular instruction by using the index of the address if specified"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_methods(self):\n        return [x\n                for x in self.direct_methods] + [x\n                                                 for x in self.virtual_methods]", "response": "Return a list of all methods that are available for this resource."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_fields(self):\n        return [x for x in self.static_fields] + [x\n                                                  for x in self.instance_fields]", "response": "Return static and instance fields"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_raw(self):\n        buff = bytearray()\n        buff += writesleb128(self.size)\n        for i in self.handlers:\n            buff += i.get_raw()\n\n        if self.size <= 0:\n            buff += writeuleb128(self.catch_all_addr)\n\n        return buff", "response": "Returns a bytearray containing the raw data of the current object."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a bytearray containing the raw data of the object.", "response": "def get_raw(self):\n        \"\"\"\n        :rtype: bytearray\n        \"\"\"\n        buff = bytearray()\n        buff += self.get_obj()\n        for i in self.list:\n            buff += i.get_raw()\n        return buff"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the kind argument of the instruction", "response": "def get_kind(self):\n        \"\"\"\n        Return the 'kind' argument of the instruction\n\n        :rtype: int\n        \"\"\"\n        if self.OP > 0xff:\n            if self.OP >= 0xf2ff:\n                return DALVIK_OPCODES_OPTIMIZED[self.OP][1][1]\n            return DALVIK_OPCODES_EXTENDED_WIDTH[self.OP][1][1]\n        return DALVIK_OPCODES_FORMAT[self.OP][1][1]"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_name(self):\n        if self.OP > 0xff:\n            if self.OP >= 0xf2ff:\n                return DALVIK_OPCODES_OPTIMIZED[self.OP][1][0]\n            return DALVIK_OPCODES_EXTENDED_WIDTH[self.OP][1][0]\n        return DALVIK_OPCODES_FORMAT[self.OP][1][0]", "response": "Returns the name of the instruction\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef show_buff(self, pos):\n        buff = self.get_name() + \" \"\n\n        for i in range(0, len(self.data)):\n            buff += \"\\\\x%02x\" % self.data[i]\n        return buff", "response": "Return the display of the instruction\n\n       "}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_hex(self):\n\n        s = binascii.hexlify(self.get_raw()).decode(\"ascii\")\n        return \" \".join(s[i:i + 2] for i in range(0, len(s), 2))", "response": "Returns a HEX String separated by spaces every byte\n       "}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the keys of the instruction", "response": "def get_keys(self):\n        \"\"\"\n        Return the keys of the instruction\n\n        :rtype: a list of long\n        \"\"\"\n        return [(self.first_key + i) for i in range(0, len(self.targets))]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_output(self, idx=-1):\n        return \" \".join(\"%x\" % (self.first_key + i)\n                        for i in range(0, len(self.targets)))", "response": "Returns an additional output of the instruction\n\nOID."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the display of the instruction", "response": "def show_buff(self, pos):\n        \"\"\"\n        Return the display of the instruction\n\n        :rtype: string\n        \"\"\"\n        buff = self.get_name() + \" \"\n        buff += \"%x:\" % self.first_key\n\n        for i in self.targets:\n            buff += \" %x\" % i\n\n        return buff"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef set_insn(self, insn):\n        self.insn = insn\n        self.size = len(self.insn)", "response": "Set the instruction to disassemble the new buffer"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nget the instructions of the current entry.", "response": "def get_instructions(self):\n        \"\"\"\n        Get the instructions\n\n        :rtype: a generator of each :class:`Instruction` (or a cached list of instructions if you have setup instructions)\n        \"\"\"\n        # it is possible to a cache for instructions (avoid a new disasm)\n        if self.cached_instructions is None:\n            lsa = LinearSweepAlgorithm()\n            ins = lsa.get_instructions(self.CM, self.size, self.insn,\n                                          self.idx)\n            self.cached_instructions = list(ins)\n\n        for i in self.cached_instructions:\n            yield i"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add_inote(self, msg, idx, off=None):\n        if off is not None:\n            idx = self.off_to_pos(off)\n\n        if idx not in self.notes:\n            self.notes[idx] = []\n\n        self.notes[idx].append(msg)", "response": "Add a message to a specific instruction by using the index of the address if specified"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a specific instruction by using the index of the address if specified.", "response": "def get_instruction(self, idx, off=None):\n        \"\"\"\n        Get a particular instruction by using (default) the index of the address if specified\n\n        :param idx: index of the instruction (the position in the list of the instruction)\n        :type idx: int\n        :param off: address of the instruction\n        :type off: int\n\n        :rtype: an :class:`Instruction` object\n        \"\"\"\n        if off is not None:\n            idx = self.off_to_pos(off)\n        if self.cached_instructions is None:\n            self.get_instructions()\n        return self.cached_instructions[idx]"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef off_to_pos(self, off):\n        idx = 0\n        nb = 0\n        for i in self.get_instructions():\n            if idx == off:\n                return nb\n            nb += 1\n            idx += i.get_length()\n        return -1", "response": "Get the position of an instruction by using the address\n           "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a particular instruction by using the address off.", "response": "def get_ins_off(self, off):\n        \"\"\"\n        Get a particular instruction by using the address\n\n        :param off: address of the instruction\n        :type off: int\n\n        :rtype: an :class:`Instruction` object\n        \"\"\"\n        idx = 0\n        for i in self.get_instructions():\n            if idx == off:\n                return i\n            idx += i.get_length()\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef show(self):\n        off = 0\n        for n, i in enumerate(self.get_instructions()):\n            print(\"{:8d} (0x{:08x}) {:04x} {:30} {}\".format(n, off, i.get_op_value(), i.get_name(), i.get_output(self.idx)))\n            off += i.get_length()", "response": "Display this object s information."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the raw buffer of this object", "response": "def get_raw(self):\n        \"\"\"\n        Return the raw buffer of this object\n\n        :rtype: bytearray\n        \"\"\"\n        buff = bytearray()\n        for i in self.get_instructions():\n            buff += i.get_raw()\n        return buff"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_raw(self):\n        code_raw = self.code.get_raw()\n        self.insns_size = (len(code_raw) // 2) + (len(code_raw) % 2)\n\n        buff = bytearray()\n        buff += pack(\"<H\", self.registers_size) + \\\n                pack(\"<H\", self.ins_size) + \\\n                pack(\"<H\", self.outs_size) + \\\n                pack(\"<H\", self.tries_size) + \\\n                pack(\"<I\", self.debug_info_off) + \\\n                pack(\"<I\", self.insns_size) + \\\n                code_raw\n\n        if self.tries_size > 0:\n            if (self.insns_size % 2 == 1):\n                buff += pack(\"<H\", self.padding)\n\n            for i in self.tries:\n                buff += i.get_raw()\n            buff += self.handlers.get_raw()\n\n        return buff", "response": "Get the reconstructed code as bytearray"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a string from the string table at index idx", "response": "def get_string(self, idx):\n        \"\"\"\n        Return a string from the string table at index `idx`\n\n        :param int idx: index in the string section\n        \"\"\"\n        if idx in self.hook_strings:\n            return self.hook_strings[idx]\n\n        try:\n            off = self.__manage_item[TypeMapItem.STRING_ID_ITEM][idx].get_string_data_off()\n        except IndexError:\n            log.warning(\"unknown string item @ %d\" % idx)\n            return \"AG:IS: invalid string\"\n\n        try:\n            if self.recode_ascii_string:\n                if self.recode_ascii_string_meth:\n                    return self.recode_ascii_string_meth(\n                        self.__strings_off[off].get())\n                return self.get_ascii_string(self.__strings_off[off].get())\n            return self.__strings_off[off].get()\n        except KeyError:\n            log.warning(\"unknown string item @ 0x%x(%d)\" % (off, idx))\n            return \"AG:IS: invalid string\""}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_raw_string(self, idx):\n        try:\n            off = self.__manage_item[TypeMapItem.STRING_ID_ITEM][idx].get_string_data_off()\n        except IndexError:\n            log.warning(\"unknown string item @ %d\" % idx)\n            return \"AG:IS: invalid string\"\n\n        try:\n            return self.__strings_off[off].get()\n        except KeyError:\n            log.warning(\"unknown string item @ 0x%x(%d)\" % (off, idx))\n            return \"AG:IS: invalid string\"", "response": "Return the raw string from the string table at the given index."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_type(self, idx):\n        _type = self.get_type_ref(idx)\n        if _type == -1:\n            return \"AG:ITI: invalid type\"\n        return self.get_string(_type)", "response": "Return the resolved type name based on the index\n       "}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nprint with a pretty display the MapList object", "response": "def show(self):\n        \"\"\"\n        Print with a pretty display the MapList object\n        \"\"\"\n        bytecode._Print(\"MAP_LIST SIZE\", self.size)\n        for i in self.map_item:\n            if i.item != self:\n                # FIXME this does not work for CodeItems!\n                # as we do not have the method analysis here...\n                i.show()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nflushes all caches of the given class.", "response": "def _flush(self):\n        \"\"\"\n        Flush all caches\n        Might be used after classes, methods or fields are added.\n        \"\"\"\n        self.classes_names = None\n        self.__cache_methods = None\n        self.__cached_methods_idx = None\n        self.__cache_fields = None\n\n        # cache methods and fields as well, otherwise the decompiler is quite slow\n        self.__cache_all_methods = None\n        self.__cache_all_fields = None"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the names of the classes in this instance.", "response": "def get_classes_names(self, update=False):\n        \"\"\"\n        Return the names of classes\n\n        :param update: True indicates to recompute the list.\n                       Maybe needed after using a MyClass.set_name().\n        :rtype: a list of string\n        \"\"\"\n        if self.classes_names is None or update:\n            self.classes_names = [i.get_name() for i in self.get_classes()]\n        return self.classes_names"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a specific class by name", "response": "def get_class(self, name):\n        \"\"\"\n        Return a specific class\n\n        :param name: the name of the class\n\n        :rtype: a :class:`ClassDefItem` object\n        \"\"\"\n        for i in self.get_classes():\n            if i.get_name() == name:\n                return i\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_method(self, name):\n        # TODO could use a generator here\n        prog = re.compile(name)\n        l = []\n        for i in self.get_classes():\n            for j in i.get_methods():\n                if prog.match(j.get_name()):\n                    l.append(j)\n        return l", "response": "Return a list of all methods which corresponds to the regexp name"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of all fields which corresponds to the regexp name", "response": "def get_field(self, name):\n        \"\"\"\n        Return a list all fields which corresponds to the regexp\n\n        :param name: the name of the field (a python regexp)\n\n        :rtype: a list with all :class:`EncodedField` objects\n        \"\"\"\n        # TODO could use a generator here\n        prog = re.compile(name)\n        l = []\n        for i in self.get_classes():\n            for j in i.get_fields():\n                if prog.match(j.get_name()):\n                    l.append(j)\n        return l"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_fields(self):\n        if self.__cache_all_fields is None:\n            self.__cache_all_fields = []\n            for i in self.get_classes():\n                for j in i.get_fields():\n                    self.__cache_all_fields.append(j)\n        return self.__cache_all_fields", "response": "Return all field objects in this object"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning all methods in this class.", "response": "def get_methods(self):\n        \"\"\"\n        Return all method objects\n\n        :rtype: a list of :class:`EncodedMethod` objects\n        \"\"\"\n        if self.__cache_all_methods is None:\n            self.__cache_all_methods = []\n            for i in self.get_classes():\n                for j in i.get_methods():\n                    self.__cache_all_methods.append(j)\n        return self.__cache_all_methods"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_method_by_idx(self, idx):\n        if self.__cached_methods_idx is None:\n            self.__cached_methods_idx = {}\n            for i in self.get_classes():\n                for j in i.get_methods():\n                    self.__cached_methods_idx[j.get_method_idx()] = j\n\n        try:\n            return self.__cached_methods_idx[idx]\n        except KeyError:\n            return None", "response": "Return a specific method by using an index."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the specific method descriptor for a specific class and method.", "response": "def get_method_descriptor(self, class_name, method_name, descriptor):\n        \"\"\"\n        Return the specific method\n\n        :param class_name: the class name of the method\n        :type class_name: string\n        :param method_name: the name of the method\n        :type method_name: string\n        :param descriptor: the descriptor of the method\n        :type descriptor: string\n\n        :rtype: None or a :class:`EncodedMethod` object\n        \"\"\"\n        key = class_name + method_name + descriptor\n\n        if self.__cache_methods is None:\n            self.__cache_methods = {}\n            for i in self.get_classes():\n                for j in i.get_methods():\n                    self.__cache_methods[j.get_class_name() + j.get_name() +\n                                         j.get_descriptor()] = j\n\n        return self.__cache_methods.get(key)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_methods_descriptor(self, class_name, method_name):\n        l = []\n        for i in self.get_classes():\n            if i.get_name() == class_name:\n                for j in i.get_methods():\n                    if j.get_name() == method_name:\n                        l.append(j)\n\n        return l", "response": "Returns the specific methods of the class."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_methods_class(self, class_name):\n        l = []\n        for i in self.get_classes():\n            for j in i.get_methods():\n                if class_name == j.get_class_name():\n                    l.append(j)\n\n        return l", "response": "Return all methods of a specific class"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_fields_class(self, class_name):\n        l = []\n        for i in self.get_classes():\n            for j in i.get_fields():\n                if class_name == j.get_class_name():\n                    l.append(j)\n\n        return l", "response": "Return all fields of a specific class"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn the specific field descriptor", "response": "def get_field_descriptor(self, class_name, field_name, descriptor):\n        \"\"\"\n        Return the specific field\n\n        :param class_name: the class name of the field\n        :type class_name: string\n        :param field_name: the name of the field\n        :type field_name: string\n        :param descriptor: the descriptor of the field\n        :type descriptor: string\n\n        :rtype: None or a :class:`EncodedField` object\n        \"\"\"\n\n        key = class_name + field_name + descriptor\n\n        if self.__cache_fields is None:\n            self.__cache_fields = {}\n            for i in self.get_classes():\n                for j in i.get_fields():\n                    self.__cache_fields[j.get_class_name() + j.get_name() +\n                                        j.get_descriptor()] = j\n\n        return self.__cache_fields.get(key)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn all target strings matched the regular_expressions", "response": "def get_regex_strings(self, regular_expressions):\n        \"\"\"\n        Return all target strings matched the regex\n\n        :param regular_expressions: the python regex\n        :type regular_expressions: string\n\n        :rtype: a list of strings matching the regex expression\n        \"\"\"\n        str_list = []\n        if regular_expressions.count is None:\n            return None\n        for i in self.get_strings():\n            if re.match(regular_expressions, i):\n                str_list.append(i)\n        return str_list"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef create_python_export(self):\n        setattr(self, \"C\", ExportObject())\n\n        for _class in self.get_classes():\n            self._create_python_export_class(_class)", "response": "Create the python namespace for this object."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef disassemble(self, offset, size):\n        for i in DCode(\n                self.CM, offset, size,\n                self.get_buff()[offset:offset + size]).get_instructions():\n            yield i", "response": "Disassemble a given offset in the DEX file returning a generator of the instruction objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nconverting a path such as android. support. v4. app. ActivityCompat to Landroid. support. v4. app. ActivityCompat. Set the name of the class.", "response": "def classdot2class(path):\n    \"\"\" Convert a path such as 'android.support.v4.app.ActivityCompat'\n        into a string 'Landroid/support/v4/app/ActivityCompat'\n        so we can change name of a class by d.CLASS_Landroid_support_v4_app_ActivityCompat.set_name(new_name)\n    \"\"\"\n    if path[0] == 'L' and path[-1] == ';':\n        log.debug(\"WARNING: %s already a Lclass; name\" % path)\n        return path\n\n    new_name = 'L' + path.replace('.', '/') + ';'\n    return new_name"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_xrefs_list_from_element(cls, element):\n\n        xref_items = element.XREFfrom.items\n        log.debug(\"%d XREFs found\" % len(xref_items))\n        xrefs = []\n        for xref_item in xref_items:\n            class_ = xref_item[0].get_class_name()\n            method_ = xref_item[0].get_name()\n            descriptor_ = xref_item[0].get_descriptor()\n            xrefs.append(classmethod2display(class_, method_, descriptor_))\n        return xrefs", "response": "Helper for get_xrefs_list\n\n           element is a ClassDefItem or MethodDefItem\n          "}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef plot(cg):\n    from androguard.core.analysis.analysis import ExternalMethod\n    import matplotlib.pyplot as plt\n    import networkx as nx\n    pos = nx.spring_layout(cg)\n\n    internal = []\n    external = []\n\n    for n in cg.node:\n        if isinstance(n, ExternalMethod):\n            external.append(n)\n        else:\n            internal.append(n)\n\n    nx.draw_networkx_nodes(cg, pos=pos, node_color='r', nodelist=internal)\n    nx.draw_networkx_nodes(cg, pos=pos, node_color='b', nodelist=external)\n    nx.draw_networkx_edges(cg, pos, arrow=True)\n    nx.draw_networkx_labels(cg, pos=pos,\n                            labels={x: \"{} {}\".format(x.get_class_name(),\n                                                      x.get_name())\n                                    for x in cg.edge})\n    plt.draw()\n    plt.show()", "response": "Plots the call graph using matplotlib."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _write_gml(G, path):\n    import networkx as nx\n    return nx.write_gml(G, path, stringizer=str)", "response": "Wrapper around nx. write_gml"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload and load an APK or DEX file and return a new APK or DEX file", "response": "def androlyze_main(session, filename):\n    \"\"\"\n    Start an interactive shell\n\n    :param session: Session file to load\n    :param filename: File to analyze, can be APK or DEX (or ODEX)\n    \"\"\"\n    from androguard.core.androconf import ANDROGUARD_VERSION, CONF\n    from IPython.terminal.embed import InteractiveShellEmbed\n    from traitlets.config import Config\n    from androguard.misc import init_print_colors\n    from androguard.session import Session, Load\n    from colorama import Fore\n    import colorama\n    import atexit\n\n    # Import commonly used classes, for further usage...\n    from androguard.core.bytecodes.apk import APK\n    from androguard.core.bytecodes.dvm import DalvikVMFormat\n    from androguard.core.analysis.analysis import Analysis\n\n    colorama.init()\n\n    if session:\n        print(\"Restoring session '{}'...\".format(session))\n        s = CONF['SESSION'] = Load(session)\n        print(\"Successfully restored {}\".format(s))\n        # TODO Restore a, d, dx etc...\n    else:\n        s = CONF[\"SESSION\"] = Session(export_ipython=True)\n\n    if filename:\n        (\"Loading apk {}...\".format(os.path.basename(filename)))\n        print(\"Please be patient, this might take a while.\")\n\n        filetype = androconf.is_android(filename)\n\n        print(\"Found the provided file is of type '{}'\".format(filetype))\n\n        if filetype not in ['DEX', 'DEY', 'APK']:\n            print(Fore.RED + \"This file type is not supported by androlyze for auto loading right now!\" + Fore.RESET, file=sys.stderr)\n            print(\"But your file is still available:\")\n            print(\">>> filename\")\n            print(repr(filename))\n            print()\n\n        else:\n            with open(filename, \"rb\") as fp:\n                raw = fp.read()\n\n            h = s.add(apk, raw)\n            print(\"Added file to session: SHA256::{}\".format(h))\n\n            if filetype == 'APK':\n                print(\"Loaded APK file...\")\n                a, d, dx = s.get_objects_apk(digest=h)\n\n                print(\">>> a\")\n                print(a)\n                print(\">>> d\")\n                print(d)\n                print(\">>> dx\")\n                print(dx)\n                print()\n            elif filetype in ['DEX', 'DEY']:\n                print(\"Loaded DEX file...\")\n                for h_, d, dx in s.get_objects_dex():\n                    if h == h_:\n                        break\n                print(\">>> d\")\n                print(d)\n                print(\">>> dx\")\n                print(dx)\n                print()\n\n    def shutdown_hook():\n        \"\"\"Save the session on exit, if wanted\"\"\"\n        if not s.isOpen():\n            return\n\n        try:\n            res = input(\"Do you want to save the session? (y/[n])?\").lower()\n        except (EOFError, KeyboardInterrupt):\n            pass\n        else:\n            if res == \"y\":\n                # TODO: if we already started from a session, probably we want to save it under the same name...\n                # TODO: be able to take any filename you want\n                fname = s.save()\n                print(\"Saved Session to file: '{}'\".format(fname))\n\n    cfg = Config()\n    _version_string = \"Androguard version {}\".format(ANDROGUARD_VERSION)\n    ipshell = InteractiveShellEmbed(config=cfg, banner1=\"{} started\"\n                                    .format(_version_string))\n    atexit.register(shutdown_hook)\n    init_print_colors()\n    ipshell()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef decode(b):\n    res = \"\"\n\n    b = iter(bytearray(b))\n\n    for x in b:\n        if x >> 7 == 0:\n            # Single char:\n            res += chr(x & 0x7f)\n        elif x >> 5 == 0b110:\n            # 2 byte Multichar\n            b2 = next(b)\n            if b2 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Second byte of 2 byte sequence does not looks right.\")\n\n            res += chr((x & 0x1f) << 6 | b2 & 0x3f)\n        elif x >> 4 == 0b1110:\n            # 3 byte Multichar\n            b2 = next(b)\n            b3 = next(b)\n            if b2 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Second byte of 3 byte sequence does not looks right.\")\n            if b3 >> 6 != 0b10:\n                raise UnicodeDecodeError(\"Third byte of 3 byte sequence does not looks right.\")\n\n            res += chr((x & 0xf) << 12 | (b2 & 0x3f) << 6 | b3 & 0x3f)\n        else:\n            raise UnicodeDecodeError(\"Could not decode byte\")\n\n    return res", "response": "Decodes a byte array into a MUTF - 8 string."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreorganizes a string in such a way that surrogates are printable and lonely surrogates are escaped and 32bit surrogates are escaped.", "response": "def patch_string(s):\n    \"\"\"\n    Reorganize a String in such a way that surrogates are printable\n    and lonely surrogates are escaped.\n\n    :param s: input string\n    :return: string with escaped lonely surrogates and 32bit surrogates\n    \"\"\"\n    res = ''\n    it = PeekIterator(s)\n    for c in it:\n        if (ord(c) >> 10) == 0b110110:\n            # High surrogate\n            # Check for the next\n            n = it.peek()\n            if n and (ord(n) >> 10) == 0b110111:\n                # Next is a low surrogate! Merge them together\n                res += chr(((ord(c) & 0x3ff) << 10 | (ord(n) & 0x3ff)) + 0x10000)\n                # Skip next char, as we already consumed it\n                next(it)\n            else:\n                # Lonely high surrogate\n                res += \"\\\\u{:04x}\".format(ord(c))\n        elif (ord(c) >> 10) == 0b110111:\n            # Lonely low surrogate\n            res += \"\\\\u{:04x}\".format(ord(c))\n        else:\n            # Looks like a normal char...\n            res += c\n    return res"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef string(s):\n    ret = ['\"']\n    for c in s:\n        if ' ' <= c < '\\x7f':\n            if c == \"'\" or c == '\"' or c == '\\\\':\n                ret.append('\\\\')\n            ret.append(c)\n            continue\n        elif c <= '\\x7f':\n            if c in ('\\r', '\\n', '\\t'):\n                # unicode-escape produces bytes\n                ret.append(c.encode('unicode-escape').decode(\"ascii\"))\n                continue\n        i = ord(c)\n        ret.append('\\\\u')\n        ret.append('%x' % (i >> 12))\n        ret.append('%x' % ((i >> 8) & 0x0f))\n        ret.append('%x' % ((i >> 4) & 0x0f))\n        ret.append('%x' % (i & 0x0f))\n    ret.append('\"')\n    return ''.join(ret)", "response": "Convert a string to an ASCII escaped representation including quotation marks."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhighlights a block of text.", "response": "def highlightBlock(self, string):\n        \"\"\" Highlight a block of text.\n        \"\"\"\n        prev_data = self.currentBlock().previous().userData()\n        if prev_data is not None:\n            self._lexer._saved_state_stack = prev_data.syntax_stack\n        elif hasattr(self._lexer, '_saved_state_stack'):\n            del self._lexer._saved_state_stack\n\n        # Lex the text using Pygments\n        index = 0\n        for token, text in self._lexer.get_tokens(string):\n            length = len(text)\n            self.setFormat(index, length, self._get_format(token))\n            index += length\n\n        if hasattr(self._lexer, '_saved_state_stack'):\n            data = PygmentsBlockUserData(\n                syntax_stack=self._lexer._saved_state_stack)\n            self.currentBlock().setUserData(data)\n            # Clean up for the next go-round.\n            del self._lexer._saved_state_stack"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the style to the specified Pygments style.", "response": "def set_style(self, style):\n        \"\"\" Sets the style to the specified Pygments style.\n        \"\"\"\n        style = SolarizedStyle  # get_style_by_name(style)\n        self._style = style\n        self._clear_caches()"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsets a CSS stylesheet.", "response": "def set_style_sheet(self, stylesheet):\n        \"\"\" Sets a CSS stylesheet. The classes in the stylesheet should\n        correspond to those generated by:\n            pygmentize -S <style> -f html\n        Note that 'set_style' and 'set_style_sheet' completely override each\n        other, i.e. they cannot be used in conjunction.\n        \"\"\"\n        self._document.setDefaultStyleSheet(stylesheet)\n        self._style = None\n        self._clear_caches()"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_format(self, token):\n        if token in self._formats:\n            return self._formats[token]\n\n        result = self._get_format_from_style(token, self._style)\n\n        self._formats[token] = result\n        return result", "response": "Returns a QTextCharFormat for the given token."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning a brush for the color.", "response": "def _get_brush(self, color):\n        \"\"\" Returns a brush for the color.\n        \"\"\"\n        result = self._brushes.get(color)\n        if result is None:\n            qcolor = self._get_color(color)\n            result = QtGui.QBrush(qcolor)\n            self._brushes[color] = result\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a QColor built from a Pygments color string.", "response": "def _get_color(self, color):\n        \"\"\" Returns a QColor built from a Pygments color string.\n        \"\"\"\n        qcolor = QtGui.QColor()\n        qcolor.setRgb(int(color[:2], base=16),\n                      int(color[2:4], base=16),\n                      int(color[4:6], base=16))\n        return qcolor"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reload_java_sources(self):\n\n        log.debug(\"Getting sources for %s\" % self.current_class)\n\n        lines = [(\"COMMENTS\", [(\n            \"COMMENT\", \"// filename:{}\\n// digest:{}\\n\\n\".format(\n                self.current_filename, self.current_digest))])]\n\n        method_info_buff = \"\"\n        for method in self.current_class.get_methods():\n            method_info_buff += \"// \" + str(method) + \"\\n\"\n\n        lines.append((\"COMMENTS\", [(\n            \"COMMENT\", method_info_buff + \"\\n\\n\")]))\n\n        lines.extend(self.current_class.get_source_ext())\n\n        # TODO: delete doc when tab is closed? not deleted by \"self\" :(\n        if hasattr(self, \"doc\"):\n            del self.doc\n        self.doc = SourceDocument(parent=self, lines=lines)\n        self.setDocument(self.doc)\n\n        # No need to save hightlighter. highlighBlock will automatically be called\n        # because we passed the QTextDocument to QSyntaxHighlighter constructor\n        MyHighlighter(self.doc, lexer=JavaLexer())", "response": "Reloads all Java sources for the current class."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nuses to detect when cursor position has changed and to auto select word underneath it", "response": "def cursor_position_changed(self):\n        \"\"\"Used to detect when cursor change position and to auto select word\n           underneath it\"\"\"\n        log.debug(\"cursor_position_changed\")\n\n        cur = self.textCursor()\n        log.debug(cur.position())\n        log.debug(cur.selectedText())\n        if len(cur.selectedText()) == 0:\n            cur.select(QtGui.QTextCursor.WordUnderCursor)\n            self.setTextCursor(cur)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef keyPressEvent(self, event):\n        key = event.key()\n        if key == QtCore.Qt.Key_X:\n            self.actionXref()\n        elif key == QtCore.Qt.Key_G:\n            self.actionGoto()\n        elif key == QtCore.Qt.Key_X:\n            self.actionXref()\n        elif key == QtCore.Qt.Key_I:\n            self.actionInfo()\n        elif key == QtCore.Qt.Key_R:\n            self.reload_java_sources()", "response": "Shortcuts for keyboard events"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nchecks if there is already a method with the given name.", "response": "def method_name_exist(self, meth_name):\n        \"\"\"Check if there is already a meth_name method in the current class\n           It is useful before allowing to rename a method to check name does\n           not already exist.\n        \"\"\"\n\n        methods = self.current_class.get_methods()\n        for m in methods:\n            if m.name == meth_name:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncheck if the field_name field exists in the current class", "response": "def field_name_exist(self, field_name):\n        \"\"\"Check if there is already a field_name field in the current class\n           It is useful before allowing to rename a field to check name does\n           not already exist.\n        \"\"\"\n\n        fields = self.class_item.get_fields()\n        for f in fields:\n            if f.name == field_name:\n                return True\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef renameElement(self, oldname, newname, info):\n\n        log.debug(\"Renaming %s into %s in %s\" %\n                        (oldname, newname, self.current_filename))\n        start, end = info\n        try:\n            t = self.doc.binding[start]\n        except:\n            self.mainwin.showStatus(\"Unexpected error in renameElement\")\n            return\n\n        # Determine type of the to-be-renamed element and Androguard internal objects\n        type_ = None\n        if t[0] == 'NAME_METHOD_PROTOTYPE':  # method definition in a class\n            method_ = t[1]\n            if method_ == self.title:\n                method_ = 'init'\n\n            proto_ = t[2].method.proto\n            log.debug(\"Found: class=%s, method=%s, proto=%s\" %\n                            (self.current_class, method_, proto_))\n            type_ = \"METHOD\"\n        elif t[0] == 'NAME_METHOD_INVOKE':  # method call in a method\n            class_, method_ = t[2].split(' -> ')\n            class_ = classdot2class(class_)\n            if class_ == 'this':\n                class_ = self.path\n            proto_ = proto2methodprotofunc(\"\".join(t[3]) + t[4])\n            log.debug(\"Found: class=%s, method=%s, proto=%s\" %\n                            (class_, method_, proto_))\n            type_ = \"METHOD\"\n        elif t[0] == 'NAME_PROTOTYPE':  # class definition on top of a class\n            class_ = t[2] + '.' + t[1]\n            package_ = t[2]\n            log.debug(\"Found: package={}, class={}\".format(package_, class_))\n            type_ = \"CLASS\"\n        elif t[0] == 'NAME_FIELD':\n            field_item = t[3]\n            type_ = \"FIELD\"\n        else:\n            self.mainwin.showStatus(\n                \"Rename not available. Info found: '%s' but object not supported.\"\n                % t[0])\n            return\n\n        # Do the actual renaming\n        if type_ == \"METHOD\":\n            if self.method_name_exist(newname):\n                self.mainwin.showStatus(\"Method name already exist\")\n                return\n\n            method_class_name = self.current_class.get_name()\n            method_name = method_\n            method_proto = proto_\n            current_analysis = self.session.get_analysis(self.current_class)\n\n            method_item = current_analysis.get_method_by_name(\n                method_class_name, method_name, method_proto)\n            if not method_item:\n                self.mainwin.showStatus(\"Impossible to find the method\")\n                return\n\n            method_item.set_name(str(newname))  # unicode to ascii\n        elif type_ == \"CLASS\":\n            newname_class = classdot2class(package_ + '.' + newname)\n            self.mainwin.showStatus(\"New name: %s\" % newname_class)\n            class_item = self.current_class  # getattr(self.mainwin.d, classdot2func(class_))\n            class_item.set_name(str(newname_class))  # unicode to ascii\n            self.mainwin.updateDockWithTree()\n        elif type_ == 'FIELD':\n            if self.field_name_exist(newname):\n                self.mainwin.showStatus(\"Field name already exist\")\n                return\n            field_item.set_name(str(newname))\n        else:\n            self.mainwin.showStatus(\"Unsupported type: %s\" % str(type_))\n            return\n        self.reload_java_sources()", "response": "Called back after a user chose a new name for an element."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef format_value(_type, _data, lookup_string=lambda ix: \"<string>\"):\n\n    # Function to prepend android prefix for attributes/references from the\n    # android library\n    fmt_package = lambda x: \"android:\" if x >> 24 == 1 else \"\"\n\n    # Function to represent integers\n    fmt_int = lambda x: (0x7FFFFFFF & x) - 0x80000000 if x > 0x7FFFFFFF else x\n\n    if _type == TYPE_STRING:\n        return lookup_string(_data)\n\n    elif _type == TYPE_ATTRIBUTE:\n        return \"?{}{:08X}\".format(fmt_package(_data), _data)\n\n    elif _type == TYPE_REFERENCE:\n        return \"@{}{:08X}\".format(fmt_package(_data), _data)\n\n    elif _type == TYPE_FLOAT:\n        return \"%f\" % unpack(\"=f\", pack(\"=L\", _data))[0]\n\n    elif _type == TYPE_INT_HEX:\n        return \"0x%08X\" % _data\n\n    elif _type == TYPE_INT_BOOLEAN:\n        if _data == 0:\n            return \"false\"\n        return \"true\"\n\n    elif _type == TYPE_DIMENSION:\n        return \"{:f}{}\".format(complexToFloat(_data), DIMENSION_UNITS[_data & COMPLEX_UNIT_MASK])\n\n    elif _type == TYPE_FRACTION:\n        return \"{:f}{}\".format(complexToFloat(_data) * 100, FRACTION_UNITS[_data & COMPLEX_UNIT_MASK])\n\n    elif TYPE_FIRST_COLOR_INT <= _type <= TYPE_LAST_COLOR_INT:\n        return \"#%08X\" % _data\n\n    elif TYPE_FIRST_INT <= _type <= TYPE_LAST_INT:\n        return \"%d\" % fmt_int(_data)\n\n    return \"<0x{:X}, type 0x{:02X}>\".format(_data, _type)", "response": "Formats a value based on the type and data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_arsc_info(arscobj):\n    buff = \"\"\n    for package in arscobj.get_packages_names():\n        buff += package + \":\\n\"\n        for locale in arscobj.get_locales(package):\n            buff += \"\\t\" + repr(locale) + \":\\n\"\n            for ttype in arscobj.get_types(package, locale):\n                buff += \"\\t\\t\" + ttype + \":\\n\"\n                try:\n                    tmp_buff = getattr(arscobj, \"get_\" + ttype + \"_resources\")(\n                        package, locale).decode(\"utf-8\", 'replace').split(\"\\n\")\n                    for i in tmp_buff:\n                        buff += \"\\t\\t\\t\" + i + \"\\n\"\n                except AttributeError:\n                    pass\n    return buff", "response": "Return a string containing all resources packages ordered by packagename locale and type."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning the string at the index in the string table.", "response": "def getString(self, idx):\n        \"\"\"\n        Return the string at the index in the string table\n\n        :param idx: index in the string table\n        :return: str\n        \"\"\"\n        if idx in self._cache:\n            return self._cache[idx]\n\n        if idx < 0 or not self.m_stringOffsets or idx > self.stringCount:\n            return \"\"\n\n        offset = self.m_stringOffsets[idx]\n\n        if self.m_isUTF8:\n            self._cache[idx] = self._decode8(offset)\n        else:\n            self._cache[idx] = self._decode16(offset)\n\n        return self._cache[idx]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _decode_bytes(data, encoding, str_len):\n        string = data.decode(encoding, 'replace')\n        if len(string) != str_len:\n            log.warning(\"invalid decoded string length\")\n        return string", "response": "Generic decoding with length check."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints some information on stdout about the string table", "response": "def show(self):\n        \"\"\"\n        Print some information on stdout about the string table\n        \"\"\"\n        print(\"StringBlock(stringsCount=0x%x, \"\n              \"stringsOffset=0x%x, \"\n              \"stylesCount=0x%x, \"\n              \"stylesOffset=0x%x, \"\n              \"flags=0x%x\"\n              \")\" % (self.stringCount,\n                     self.stringsOffset,\n                     self.styleCount,\n                     self.stylesOffset,\n                     self.flags))\n\n        if self.stringCount > 0:\n            print()\n            print(\"String Table: \")\n            for i, s in enumerate(self):\n                print(\"{:08d} {}\".format(i, repr(s)))\n\n        if self.styleCount > 0:\n            print()\n            print(\"Styles Table: \")\n            for i in range(self.styleCount):\n                print(\"{:08d} {}\".format(i, repr(self.getStyle(i))))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef name(self):\n        if self.m_name == -1 or (self.m_event != START_TAG and self.m_event != END_TAG):\n            return ''\n\n        return self.sb[self.m_name]", "response": "Return the name of the tag"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the current namespace mapping as a dictionary", "response": "def nsmap(self):\n        \"\"\"\n        Returns the current namespace mapping as a dictionary\n\n        there are several problems with the map and we try to guess a few\n        things here:\n\n        1) a URI can be mapped by many prefixes, so it is to decide which one to take\n        2) a prefix might map to an empty string (some packers)\n        3) uri+prefix mappings might be included several times\n        4) prefix might be empty\n        \"\"\"\n\n        NSMAP = dict()\n        # solve 3) by using a set\n        for k, v in set(self.namespaces):\n            s_prefix = self.sb[k]\n            s_uri = self.sb[v]\n            # Solve 2) & 4) by not including\n            if s_uri != \"\" and s_prefix != \"\":\n                # solve 1) by using the last one in the list\n                NSMAP[s_prefix] = s_uri\n\n        return NSMAP"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef text(self):\n        if self.m_name == -1 or self.m_event != TEXT:\n            return ''\n\n        return self.sb[self.m_name]", "response": "Return the current text assosicated with the current text"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_attribute_offset(self, index):\n        if self.m_event != START_TAG:\n            log.warning(\"Current event is not START_TAG.\")\n\n        offset = index * ATTRIBUTE_LENGHT\n        if offset >= len(self.m_attributes):\n            log.warning(\"Invalid attribute index\")\n\n        return offset", "response": "Return the offset of the attribute in the m_attributes array for a given attribute index."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef getAttributeUri(self, index):\n        offset = self._get_attribute_offset(index)\n        uri = self.m_attributes[offset + ATTRIBUTE_IX_NAMESPACE_URI]\n\n        return uri", "response": "Returns the numeric ID for the namespace URI of an attribute"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef getAttributeValue(self, index):\n        offset = self._get_attribute_offset(index)\n        valueType = self.m_attributes[offset + ATTRIBUTE_IX_VALUE_TYPE]\n        if valueType == TYPE_STRING:\n            valueString = self.m_attributes[offset + ATTRIBUTE_IX_VALUE_STRING]\n            return self.sb[valueString]\n        return ''", "response": "This function returns the value of the attribute at the given index."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the XML as an UTF - 8 string", "response": "def get_xml(self, pretty=True):\n        \"\"\"\n        Get the XML as an UTF-8 string\n\n        :returns: bytes encoded as UTF-8\n        \"\"\"\n        return etree.tostring(self.root, encoding=\"utf-8\", pretty_print=pretty)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fix_name(self, prefix, name):\n        if not name[0].isalpha() and name[0] != \"_\":\n            log.warning(\"Invalid start for name '{}'. \"\n                        \"XML name must start with a letter.\".format(name))\n            self.packerwarning = True\n            name = \"_{}\".format(name)\n        if name.startswith(\"android:\") and prefix == '' and 'android' in self.axml.nsmap:\n            # Seems be a common thing...\n            log.info(\"Name '{}' starts with 'android:' prefix but 'android' is a known prefix. Replacing prefix.\".format(name))\n            prefix = self._print_namespace(self.axml.nsmap['android'])\n            name = name[len(\"android:\"):]\n            # It looks like this is some kind of packer... Not sure though.\n            self.packerwarning = True\n        elif \":\" in name and prefix == '':\n            self.packerwarning = True\n            embedded_prefix, new_name = name.split(\":\", 1)\n            if embedded_prefix in self.axml.nsmap:\n                log.info(\"Prefix '{}' is in namespace mapping, assume that it is a prefix.\")\n                prefix = self._print_namespace(self.axml.nsmap[embedded_prefix])\n                name = new_name\n            else:\n                # Print out an extra warning\n                log.warning(\"Confused: name contains a unknown namespace prefix: '{}'. \"\n                            \"This is either a broken AXML file or some attempt to break stuff.\".format(name))\n        if not re.match(r\"^[a-zA-Z0-9._-]*$\", name):\n            log.warning(\"Name '{}' contains invalid characters!\".format(name))\n            self.packerwarning = True\n            name = re.sub(r\"[^a-zA-Z0-9._-]\", \"_\", name)\n\n        return prefix, name", "response": "This function will take a name and a prefix and return a fixed version of the name."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _fix_value(self, value):\n        if not self.__charrange or not self.__replacement:\n            self.__charrange = re.compile('^[\\u0020-\\uD7FF\\u0009\\u000A\\u000D\\uE000-\\uFFFD\\U00010000-\\U0010FFFF]*$')\n            self.__replacement = re.compile('[^\\u0020-\\uD7FF\\u0009\\u000A\\u000D\\uE000-\\uFFFD\\U00010000-\\U0010FFFF]')\n\n        # Reading string until \\x00. This is the same as aapt does.\n        if \"\\x00\" in value:\n            self.packerwarning = True\n            log.warning(\"Null byte found in attribute value at position {}: \"\n                        \"Value(hex): '{}'\".format(\n                value.find(\"\\x00\"),\n                binascii.hexlify(value.encode(\"utf-8\"))))\n            value = value[:value.find(\"\\x00\")]\n\n        if not self.__charrange.match(value):\n            log.warning(\"Invalid character in value found. Replacing with '_'.\")\n            self.packerwarning = True\n            value = self.__replacement.sub('_', value)\n        return value", "response": "Fixes the value of a key - value pair."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_locales(self, package_name):\n        self._analyse()\n        return list(self.values[package_name].keys())", "response": "Retrieve a list of all available locales in a given package name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_string_resources(self, package_name, locale='\\x00\\x00'):\n        self._analyse()\n\n        buff = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n        buff += '<resources>\\n'\n\n        try:\n            for i in self.values[package_name][locale][\"string\"]:\n                if any(map(i[1].__contains__, '<&>')):\n                    value = '<![CDATA[%s]]>' % i[1]\n                else:\n                    value = i[1]\n                buff += '<string name=\"{}\">{}</string>\\n'.format(i[0], value)\n        except KeyError:\n            pass\n\n        buff += '</resources>\\n'\n\n        return buff.encode('utf-8')", "response": "Get the XML of all string resources of type string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nget the XML of all resources of type string.", "response": "def get_strings_resources(self):\n        \"\"\"\n        Get the XML (as string) of all resources of type 'string'.\n        This is a combined variant, which has all locales and all package names\n        stored.\n        \"\"\"\n        self._analyse()\n\n        buff = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n\n        buff += \"<packages>\\n\"\n        for package_name in self.get_packages_names():\n            buff += \"<package name=\\\"%s\\\">\\n\" % package_name\n\n            for locale in self.get_locales(package_name):\n                buff += \"<locale value=%s>\\n\" % repr(locale)\n\n                buff += '<resources>\\n'\n                try:\n                    for i in self.values[package_name][locale][\"string\"]:\n                        buff += '<string name=\"{}\">{}</string>\\n'.format(i[0], escape(i[1]))\n                except KeyError:\n                    pass\n\n                buff += '</resources>\\n'\n                buff += '</locale>\\n'\n\n            buff += \"</package>\\n\"\n\n        buff += \"</packages>\\n\"\n\n        return buff.encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_id_resources(self, package_name, locale='\\x00\\x00'):\n        self._analyse()\n\n        buff = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n        buff += '<resources>\\n'\n\n        try:\n            for i in self.values[package_name][locale][\"id\"]:\n                if len(i) == 1:\n                    buff += '<item type=\"id\" name=\"%s\"/>\\n' % (i[0])\n                else:\n                    buff += '<item type=\"id\" name=\"{}\">{}</item>\\n'.format(i[0],\n                                                                       escape(i[1]))\n        except KeyError:\n            pass\n\n        buff += '</resources>\\n'\n\n        return buff.encode('utf-8')", "response": "Get the XML of all resources of type id."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ngets the XML of all bool resources of type bool.", "response": "def get_bool_resources(self, package_name, locale='\\x00\\x00'):\n        \"\"\"\n        Get the XML (as string) of all resources of type 'bool'.\n\n        Read more about bool resources:\n        https://developer.android.com/guide/topics/resources/more-resources.html#Bool\n\n        :param package_name: the package name to get the resources for\n        :param locale: the locale to get the resources for (default: '\\x00\\x00')\n        \"\"\"\n        self._analyse()\n\n        buff = '<?xml version=\"1.0\" encoding=\"utf-8\"?>\\n'\n        buff += '<resources>\\n'\n\n        try:\n            for i in self.values[package_name][locale][\"bool\"]:\n                buff += '<bool name=\"{}\">{}</bool>\\n'.format(i[0], i[1])\n        except KeyError:\n            pass\n\n        buff += '</resources>\\n'\n\n        return buff.encode('utf-8')"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_resolved_res_configs(self, rid, config=None):\n        resolver = ARSCParser.ResourceResolver(self, config)\n        return resolver.resolve(rid)", "response": "Returns a list of resolved resource IDs with their corresponding configuration."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the resources that have the given ID and select the right one based on the configuration.", "response": "def get_res_configs(self, rid, config=None, fallback=True):\n        \"\"\"\n        Return the resources found with the ID `rid` and select\n        the right one based on the configuration, or return all if no configuration was set.\n\n        But we try to be generous here and at least try to resolve something:\n        This method uses a fallback to return at least one resource (the first one in the list)\n        if more than one items are found and the default config is used and no default entry could be found.\n\n        This is usually a bad sign (i.e. the developer did not follow the android documentation:\n        https://developer.android.com/guide/topics/resources/localization.html#failing2)\n        In practise an app might just be designed to run on a single locale and thus only has those locales set.\n\n        You can disable this fallback behaviour, to just return exactly the given result.\n\n        :param rid: resource id as int\n        :param config: a config to resolve from, or None to get all results\n        :param fallback: Enable the fallback for resolving default configuration (default: True)\n        :return: a list of ARSCResTableConfig: ARSCResTableEntry\n        \"\"\"\n        self._analyse()\n\n        if not rid:\n            raise ValueError(\"'rid' should be set\")\n        if not isinstance(rid, int):\n            raise ValueError(\"'rid' must be an int\")\n\n        if rid not in self.resource_values:\n            log.warning(\"The requested rid '0x{:08x}' could not be found in the list of resources.\".format(rid))\n            return []\n\n        res_options = self.resource_values[rid]\n        if len(res_options) > 1 and config:\n            if config in res_options:\n                return [(config, res_options[config])]\n            elif fallback and config == ARSCResTableConfig.default_config():\n                log.warning(\"No default resource config could be found for the given rid '0x{:08x}', using fallback!\".format(rid))\n                return [list(self.resource_values[rid].items())[0]]\n            else:\n                return []\n        else:\n            return list(res_options.items())"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nresolves an id from a binary XML file in the form \"@[package:]DEADBEEF\" and returns a tuple of package name and resource id.", "response": "def parse_id(name):\n        \"\"\"\n        Resolves an id from a binary XML file in the form \"@[package:]DEADBEEF\"\n        and returns a tuple of package name and resource id.\n        If no package name was given, i.e. the ID has the form \"@DEADBEEF\",\n        the package name is set to None.\n\n        Raises a ValueError if the id is malformed.\n\n        :param name: the string of the resource, as in the binary XML file\n        :return: a tuple of (resource_id, package_name).\n        \"\"\"\n\n        if not name.startswith('@'):\n            raise ValueError(\"Not a valid resource ID, must start with @: '{}'\".format(name))\n\n        # remove @\n        name = name[1:]\n\n        package = None\n        if ':' in name:\n            package, res_id = name.split(':', 1)\n        else:\n            res_id = name\n\n        if len(res_id) != 8:\n            raise ValueError(\"Numerical ID is not 8 characters long: '{}'\".format(res_id))\n\n        try:\n            return int(res_id, 16), package\n        except ValueError:\n            raise ValueError(\"ID is not a hex ID: '{}'\".format(res_id))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_resource_xml_name(self, r_id, package=None):\n        if package:\n            resource, name, i_id = self.get_id(package, r_id)\n            if not i_id:\n                return None\n            return \"@{}/{}\".format(resource, name)\n        else:\n            for p in self.get_packages_names():\n                r, n, i_id = self.get_id(p, r_id)\n                if i_id:\n                    # found the resource in this package\n                    package = p\n                    resource = r\n                    name = n\n                    break\n            if not package:\n                return None\n            else:\n                return \"@{}:{}/{}\".format(package, resource, name)", "response": "Returns the XML name for a resource in the specified package."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_language_and_region(self):\n        if self.locale != 0:\n            _language = self._unpack_language_or_region([self.locale & 0xff, (self.locale & 0xff00) >> 8, ], ord('a'))\n            _region = self._unpack_language_or_region([(self.locale & 0xff0000) >> 16, (self.locale & 0xff000000) >> 24, ], ord('0'))\n            return (_language + \"-r\" + _region) if _region else _language\n        return \"\\x00\\x00\"", "response": "Returns the combined language + region string for the current locale."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn the formatted value according to data_type.", "response": "def format_value(self):\n        \"\"\"\n        Return the formatted (interpreted) data according to `data_type`.\n        \"\"\"\n        return format_value(\n            self.data_type,\n            self.data,\n            self.parent.stringpool_main.getString\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef on_http_error(error):\n    def wrap(f):\n        @functools.wraps(f)\n        def wrapped_f(*args, **kwargs):\n            try:\n                return f(*args, **kwargs)\n            except GitlabHttpError as e:\n                raise error(e.error_message, e.response_code, e.response_body)\n        return wrapped_f\n    return wrap", "response": "Manage GitlabHttpError exceptions.\n\n    This decorator function can be used to catch GitlabHttpError exceptions\n    raise specialized exceptions instead.\n\n    Args:\n        error(Exception): The exception type to raise -- must inherit from\n            GitlabError"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the id of the resource.", "response": "def get_id(self):\n        \"\"\"Returns the id of the resource.\"\"\"\n        if self._id_attr is None or not hasattr(self, self._id_attr):\n            return None\n        return getattr(self, self._id_attr)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nretrieve a list of objects.", "response": "def list(self, **kwargs):\n        \"\"\"Retrieve a list of objects.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            list: The list of objects, or a generator if `as_list` is False\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the server cannot perform the request\n        \"\"\"\n\n        path = '/users/%s/projects' % self._parent.id\n        return ListMixin.list(self, path=path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef block(self, **kwargs):\n        path = '/users/%s/block' % self.id\n        server_data = self.manager.gitlab.http_post(path, **kwargs)\n        if server_data is True:\n            self._attrs['state'] = 'blocked'\n        return server_data", "response": "Block the user s user s internal cache."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, id=None, new_data={}, **kwargs):\n\n        data = new_data.copy()\n        if 'domain_whitelist' in data and data['domain_whitelist'] is None:\n            data.pop('domain_whitelist')\n        super(ApplicationSettingsManager, self).update(id, data, **kwargs)", "response": "Update an object on the server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set(self, name, value, feature_group=None, user=None, **kwargs):\n        path = '%s/%s' % (self.path, name.replace('/', '%2F'))\n        data = {'value': value, 'feature_group': feature_group, 'user': user}\n        server_data = self.gitlab.http_post(path, post_data=data, **kwargs)\n        return self._obj_cls(self, server_data)", "response": "Create or update the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef create(self, data, **kwargs):\n        CreateMixin._check_missing_create_attrs(self, data)\n        path = '%s/%s' % (self.path, data.pop('issue_id'))\n        server_data = self.gitlab.http_post(path, **kwargs)\n        # The epic_issue_id attribute doesn't exist when creating the resource,\n        # but is used everywhere elese. Let's create it to be consistent client\n        # side\n        server_data['epic_issue_id'] = server_data['id']\n        return self._obj_cls(self, server_data)", "response": "Create a new object."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef all(self, **kwargs):\n\n        path = '%s/all' % self.path\n        obj = self.gitlab.http_list(path, **kwargs)\n        return [self._obj_cls(self, item) for item in obj]", "response": "List all the members included inherited ones."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef issues(self, **kwargs):\n\n        path = '%s/%s/issues' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = GroupIssueManager(self.manager.gitlab,\n                                    parent=self.manager._parent)\n        # FIXME(gpocentek): the computed manager path is not correct\n        return RESTObjectList(manager, GroupIssue, data_list)", "response": "List the issues related to this milestone."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist the merge requests related to this milestone.", "response": "def merge_requests(self, **kwargs):\n        \"\"\"List the merge requests related to this milestone.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of merge requests\n        \"\"\"\n        path = '%s/%s/merge_requests' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = GroupIssueManager(self.manager.gitlab,\n                                    parent=self.manager._parent)\n        # FIXME(gpocentek): the computed manager path is not correct\n        return RESTObjectList(manager, GroupMergeRequest, data_list)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ntransfer a project to this group.", "response": "def transfer_project(self, to_project_id, **kwargs):\n        \"\"\"Transfer a project to this group.\n\n        Args:\n            to_project_id (int): ID of the project to transfer\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabTransferProjectError: If the project could not be transfered\n        \"\"\"\n        path = '/groups/%s/projects/%s' % (self.id, to_project_id)\n        self.manager.gitlab.http_post(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds an LDAP group link.", "response": "def add_ldap_group_link(self, cn, group_access, provider, **kwargs):\n        \"\"\"Add an LDAP group link.\n\n        Args:\n            cn (str): CN of the LDAP group\n            group_access (int): Minimum access level for members of the LDAP\n                group\n            provider (str): LDAP provider for the LDAP group\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n        \"\"\"\n        path = '/groups/%s/ldap_group_links' % self.get_id()\n        data = {'cn': cn, 'group_access': group_access, 'provider': provider}\n        self.manager.gitlab.http_post(path, post_data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete_ldap_group_link(self, cn, provider=None, **kwargs):\n        path = '/groups/%s/ldap_group_links' % self.get_id()\n        if provider is not None:\n            path += '/%s' % provider\n        path += '/%s' % cn\n        self.manager.gitlab.http_delete(path)", "response": "Delete an LDAP group link."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef list(self, **kwargs):\n        data = kwargs.copy()\n        if self.gitlab.per_page:\n            data.setdefault('per_page', self.gitlab.per_page)\n\n        if 'provider' in data:\n            path = '/ldap/%s/groups' % data['provider']\n        else:\n            path = self._path\n\n        obj = self.gitlab.http_list(path, **data)\n        if isinstance(obj, list):\n            return [self._obj_cls(self, item) for item in obj]\n        else:\n            return base.RESTObjectList(self, self._obj_cls, obj)", "response": "Retrieve a list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef protect(self, developers_can_push=False, developers_can_merge=False,\n                **kwargs):\n        \"\"\"Protect the branch.\n\n        Args:\n            developers_can_push (bool): Set to True if developers are allowed\n                                        to push to the branch\n            developers_can_merge (bool): Set to True if developers are allowed\n                                         to merge to the branch\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabProtectError: If the branch could not be protected\n        \"\"\"\n        id = self.get_id().replace('/', '%2F')\n        path = '%s/%s/protect' % (self.manager.path, id)\n        post_data = {'developers_can_push': developers_can_push,\n                     'developers_can_merge': developers_can_merge}\n        self.manager.gitlab.http_put(path, post_data=post_data, **kwargs)\n        self._attrs['protected'] = True", "response": "Protect the branch.\n\n        Args:\n            developers_can_push (bool): Set to True if developers are allowed\n                                        to push to the branch\n            developers_can_merge (bool): Set to True if developers are allowed\n                                         to merge to the branch\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabProtectError: If the branch could not be protected"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef unprotect(self, **kwargs):\n        id = self.get_id().replace('/', '%2F')\n        path = '%s/%s/unprotect' % (self.manager.path, id)\n        self.manager.gitlab.http_put(path, **kwargs)\n        self._attrs['protected'] = False", "response": "Unprotect the branch.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabProtectError: If the branch could not be unprotected"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef cancel(self, **kwargs):\n        path = '%s/%s/cancel' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)", "response": "Cancel the job.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobCancelError: If the job could not be canceled"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nretrying the job. Args: **kwargs: Extra options to send to the server (e.g. sudo) Raises: GitlabAuthenticationError: If authentication is not correct GitlabJobRetryError: If the job could not be retried", "response": "def retry(self, **kwargs):\n        \"\"\"Retry the job.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobRetryError: If the job could not be retried\n        \"\"\"\n        path = '%s/%s/retry' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ntriggers a job explicitly.", "response": "def play(self, **kwargs):\n        \"\"\"Trigger a job explicitly.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobPlayError: If the job could not be triggered\n        \"\"\"\n        path = '%s/%s/play' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef erase(self, **kwargs):\n        path = '%s/%s/erase' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)", "response": "Erase the job (remove job artifacts and trace).\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabJobEraseError: If the job could not be erased"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nprevent artifacts from being deleted when expiration is set.", "response": "def keep_artifacts(self, **kwargs):\n        \"\"\"Prevent artifacts from being deleted when expiration is set.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the request could not be performed\n        \"\"\"\n        path = '%s/%s/artifacts/keep' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new object.", "response": "def create(self, data, **kwargs):\n        \"\"\"Create a new object.\n\n        Args:\n            data (dict): Parameters to send to the server to create the\n                         resource\n            **kwargs: Extra options to send to the server (e.g. sudo or\n                      'ref_name', 'stage', 'name', 'all')\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n\n        Returns:\n            RESTObject: A new instance of the manage object class build with\n                        the data sent by the server\n        \"\"\"\n        # project_id and commit_id are in the data dict when using the CLI, but\n        # they are missing when using only the API\n        # See #511\n        base_path = '/projects/%(project_id)s/statuses/%(commit_id)s'\n        if 'project_id' in data and 'commit_id' in data:\n            path = base_path % data\n        else:\n            path = self._compute_path(base_path)\n        return CreateMixin.create(self, data, path=path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef diff(self, **kwargs):\n        path = '%s/%s/diff' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)", "response": "Generate the diff of the current object and the current one."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef cherry_pick(self, branch, **kwargs):\n        path = '%s/%s/cherry_pick' % (self.manager.path, self.get_id())\n        post_data = {'branch': branch}\n        self.manager.gitlab.http_post(path, post_data=post_data, **kwargs)", "response": "Cherry - pick a commit into a branch."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist the references the commit is pushed to.", "response": "def refs(self, type='all', **kwargs):\n        \"\"\"List the references the commit is pushed to.\n\n        Args:\n            type (str): The scope of references ('branch', 'tag' or 'all')\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the references could not be retrieved\n\n        Returns:\n            list: The references the commit is pushed to.\n        \"\"\"\n        path = '%s/%s/refs' % (self.manager.path, self.get_id())\n        data = {'type': type}\n        return self.manager.gitlab.http_get(path, query_data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the merge requests related to the current commit.", "response": "def merge_requests(self, **kwargs):\n        \"\"\"List the merge requests related to the commit.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the references could not be retrieved\n\n        Returns:\n            list: The merge requests related to the commit.\n        \"\"\"\n        path = '%s/%s/merge_requests' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef stop(self, **kwargs):\n        path = '%s/%s/stop' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path, **kwargs)", "response": "Stop the environment.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabStopError: If the operation failed"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nenable a deploy key for a project.", "response": "def enable(self, key_id, **kwargs):\n        \"\"\"Enable a deploy key for a project.\n\n        Args:\n            key_id (int): The ID of the key to enable\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabProjectDeployKeyError: If the key could not be enabled\n        \"\"\"\n        path = '%s/%s/enable' % (self.path, key_id)\n        self.gitlab.http_post(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef list(self, **kwargs):\n\n        path = self._compute_path('/projects/%(project_id)s/forks')\n        return ListMixin.list(self, path=path, **kwargs)", "response": "Retrieve a list of objects."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate a new object.", "response": "def create(self, data, **kwargs):\n        \"\"\"Create a new object.\n\n        Args:\n            data (dict): parameters to send to the server to create the\n                         resource\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            RESTObject, RESTObject: The source and target issues\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n        \"\"\"\n        self._check_missing_create_attrs(data)\n        server_data = self.gitlab.http_post(self.path, post_data=data,\n                                            **kwargs)\n        source_issue = ProjectIssue(self._parent.manager,\n                                    server_data['source_issue'])\n        target_issue = ProjectIssue(self._parent.manager,\n                                    server_data['target_issue'])\n        return source_issue, target_issue"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nmoving the issue to another project.", "response": "def move(self, to_project_id, **kwargs):\n        \"\"\"Move the issue to another project.\n\n        Args:\n            to_project_id(int): ID of the target project\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabUpdateError: If the issue could not be moved\n        \"\"\"\n        path = '%s/%s/move' % (self.manager.path, self.get_id())\n        data = {'to_project_id': to_project_id}\n        server_data = self.manager.gitlab.http_post(path, post_data=data,\n                                                    **kwargs)\n        self._update_attrs(server_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef closed_by(self, **kwargs):\n        path = '%s/%s/closed_by' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)", "response": "List merge requests that will close the issue when merged."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef set_release_description(self, description, **kwargs):\n        id = self.get_id().replace('/', '%2F')\n        path = '%s/%s/release' % (self.manager.path, id)\n        data = {'description': description}\n        if self.release is None:\n            try:\n                server_data = self.manager.gitlab.http_post(path,\n                                                            post_data=data,\n                                                            **kwargs)\n            except exc.GitlabHttpError as e:\n                raise exc.GitlabCreateError(e.response_code, e.error_message)\n        else:\n            try:\n                server_data = self.manager.gitlab.http_put(path,\n                                                           post_data=data,\n                                                           **kwargs)\n            except exc.GitlabHttpError as e:\n                raise exc.GitlabUpdateError(e.response_code, e.error_message)\n        self.release = server_data", "response": "Set the release notes on the tag."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef set_approvers(self, approver_ids=[], approver_group_ids=[], **kwargs):\n        path = '%s/%s/approvers' % (self._parent.manager.path,\n                                    self._parent.get_id())\n        data = {'approver_ids': approver_ids,\n                'approver_group_ids': approver_group_ids}\n        self.gitlab.http_put(path, post_data=data, **kwargs)", "response": "Change MR - level allowed approvers and approver groups."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cancel_merge_when_pipeline_succeeds(self, **kwargs):\n\n        path = ('%s/%s/cancel_merge_when_pipeline_succeeds' %\n                (self.manager.path, self.get_id()))\n        server_data = self.manager.gitlab.http_put(path, **kwargs)\n        self._update_attrs(server_data)", "response": "Cancel merge when the pipeline succeeds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef commits(self, **kwargs):\n\n        path = '%s/%s/commits' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = ProjectCommitManager(self.manager.gitlab,\n                                       parent=self.manager._parent)\n        return RESTObjectList(manager, ProjectCommit, data_list)", "response": "List the merge request commits."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef changes(self, **kwargs):\n        path = '%s/%s/changes' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)", "response": "List the merge request changes."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef pipelines(self, **kwargs):\n\n        path = '%s/%s/pipelines' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)", "response": "List the merge request pipelines."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\napprove the merge request.", "response": "def approve(self, sha=None, **kwargs):\n        \"\"\"Approve the merge request.\n\n        Args:\n            sha (str): Head SHA of MR\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabMRApprovalError: If the approval failed\n        \"\"\"\n        path = '%s/%s/approve' % (self.manager.path, self.get_id())\n        data = {}\n        if sha:\n            data['sha'] = sha\n\n        server_data = self.manager.gitlab.http_post(path, post_data=data,\n                                                    **kwargs)\n        self._update_attrs(server_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\naccepting the merge request.", "response": "def merge(self, merge_commit_message=None,\n              should_remove_source_branch=False,\n              merge_when_pipeline_succeeds=False,\n              **kwargs):\n        \"\"\"Accept the merge request.\n\n        Args:\n            merge_commit_message (bool): Commit message\n            should_remove_source_branch (bool): If True, removes the source\n                                                branch\n            merge_when_pipeline_succeeds (bool): Wait for the build to succeed,\n                                                 then merge\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabMRClosedError: If the merge failed\n        \"\"\"\n        path = '%s/%s/merge' % (self.manager.path, self.get_id())\n        data = {}\n        if merge_commit_message:\n            data['merge_commit_message'] = merge_commit_message\n        if should_remove_source_branch:\n            data['should_remove_source_branch'] = True\n        if merge_when_pipeline_succeeds:\n            data['merge_when_pipeline_succeeds'] = True\n\n        server_data = self.manager.gitlab.http_put(path, post_data=data,\n                                                   **kwargs)\n        self._update_attrs(server_data)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nlist the issues related to this milestone.", "response": "def issues(self, **kwargs):\n        \"\"\"List issues related to this milestone.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of issues\n        \"\"\"\n\n        path = '%s/%s/issues' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = ProjectIssueManager(self.manager.gitlab,\n                                      parent=self.manager._parent)\n        # FIXME(gpocentek): the computed manager path is not correct\n        return RESTObjectList(manager, ProjectIssue, data_list)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nlists the merge requests related to this milestone.", "response": "def merge_requests(self, **kwargs):\n        \"\"\"List the merge requests related to this milestone.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of merge requests\n        \"\"\"\n        path = '%s/%s/merge_requests' % (self.manager.path, self.get_id())\n        data_list = self.manager.gitlab.http_list(path, as_list=False,\n                                                  **kwargs)\n        manager = ProjectMergeRequestManager(self.manager.gitlab,\n                                             parent=self.manager._parent)\n        # FIXME(gpocentek): the computed manager path is not correct\n        return RESTObjectList(manager, ProjectMergeRequest, data_list)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndeleting a Label on the server.", "response": "def delete(self, name, **kwargs):\n        \"\"\"Delete a Label on the server.\n\n        Args:\n            name: The name of the label\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        self.gitlab.http_delete(self.path, query_data={'name': name}, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsaves the changes made to the file to the server.", "response": "def save(self, branch, commit_message, **kwargs):\n        \"\"\"Save the changes made to the file to the server.\n\n        The object is updated to match what the server returns.\n\n        Args:\n            branch (str): Branch in which the file will be updated\n            commit_message (str): Message to send with the commit\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabUpdateError: If the server cannot perform the request\n        \"\"\"\n        self.branch = branch\n        self.commit_message = commit_message\n        self.file_path = self.file_path.replace('/', '%2F')\n        super(ProjectFile, self).save(**kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndeletes the file from the server.", "response": "def delete(self, branch, commit_message, **kwargs):\n        \"\"\"Delete the file from the server.\n\n        Args:\n            branch (str): Branch from which the file will be removed\n            commit_message (str): Commit message for the deletion\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        file_path = self.get_id().replace('/', '%2F')\n        self.manager.delete(file_path, branch, commit_message, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get(self, file_path, ref, **kwargs):\n        file_path = file_path.replace('/', '%2F')\n        return GetMixin.get(self, file_path, ref=ref, **kwargs)", "response": "Retrieve a single file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncreate a new object.", "response": "def create(self, data, **kwargs):\n        \"\"\"Create a new object.\n\n        Args:\n            data (dict): parameters to send to the server to create the\n                         resource\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            RESTObject: a new instance of the managed object class built with\n                the data sent by the server\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n        \"\"\"\n\n        self._check_missing_create_attrs(data)\n        new_data = data.copy()\n        file_path = new_data.pop('file_path').replace('/', '%2F')\n        path = '%s/%s' % (self.path, file_path)\n        server_data = self.gitlab.http_post(path, post_data=new_data, **kwargs)\n        return self._obj_cls(self, server_data)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef update(self, file_path, new_data={}, **kwargs):\n\n        data = new_data.copy()\n        file_path = file_path.replace('/', '%2F')\n        data['file_path'] = file_path\n        path = '%s/%s' % (self.path, file_path)\n        self._check_missing_update_attrs(data)\n        return self.gitlab.http_put(path, post_data=data, **kwargs)", "response": "Update an object on the server."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndeleting a file on the server.", "response": "def delete(self, file_path, branch, commit_message, **kwargs):\n        \"\"\"Delete a file on the server.\n\n        Args:\n            file_path (str): Path of the file to remove\n            branch (str): Branch from which the file will be removed\n            commit_message (str): Commit message for the deletion\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        path = '%s/%s' % (self.path, file_path.replace('/', '%2F'))\n        data = {'branch': branch, 'commit_message': commit_message}\n        self.gitlab.http_delete(path, query_data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the content of a file for a commit.", "response": "def raw(self, file_path, ref, streamed=False, action=None, chunk_size=1024,\n            **kwargs):\n        \"\"\"Return the content of a file for a commit.\n\n        Args:\n            ref (str): ID of the commit\n            filepath (str): Path of the file to return\n            streamed (bool): If True the data will be processed by chunks of\n                `chunk_size` and each chunk is passed to `action` for\n                treatment\n            action (callable): Callable responsible of dealing with chunk of\n                data\n            chunk_size (int): Size of each chunk\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the file could not be retrieved\n\n        Returns:\n            str: The file content\n        \"\"\"\n        file_path = file_path.replace('/', '%2F').replace('.', '%2E')\n        path = '%s/%s/raw' % (self.path, file_path)\n        query_data = {'ref': ref}\n        result = self.gitlab.http_get(path, query_data=query_data,\n                                      streamed=streamed, raw=True, **kwargs)\n        return utils.response_content(result, streamed, action, chunk_size)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncreate a new object.", "response": "def create(self, data, **kwargs):\n        \"\"\"Creates a new object.\n\n        Args:\n            data (dict): Parameters to send to the server to create the\n                         resource\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n\n        Returns:\n            RESTObject: A new instance of the managed object class build with\n                the data sent by the server\n        \"\"\"\n        path = self.path[:-1]  # drop the 's'\n        return CreateMixin.create(self, data, path=path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef take_ownership(self, **kwargs):\n        path = '%s/%s/take_ownership' % (self.manager.path, self.get_id())\n        server_data = self.manager.gitlab.http_post(path, **kwargs)\n        self._update_attrs(server_data)", "response": "Update the owner of a pipeline schedule."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get(self, id, **kwargs):\n        obj = super(ProjectServiceManager, self).get(id, **kwargs)\n        obj.id = id\n        return obj", "response": "Retrieve a single object."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nupdating an object on the server.", "response": "def update(self, id=None, new_data={}, **kwargs):\n        \"\"\"Update an object on the server.\n\n        Args:\n            id: ID of the object to update (can be None if not required)\n            new_data: the update data for the object\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            dict: The new object data (*not* a RESTObject)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabUpdateError: If the server cannot perform the request\n        \"\"\"\n        super(ProjectServiceManager, self).update(id, new_data, **kwargs)\n        self.id = id"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download(self, streamed=False, action=None, chunk_size=1024, **kwargs):\n        path = '/projects/%s/export/download' % self.project_id\n        result = self.manager.gitlab.http_get(path, streamed=streamed,\n                                              raw=True, **kwargs)\n        return utils.response_content(result, streamed, action, chunk_size)", "response": "Download the archive of a project export."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of files in the repository.", "response": "def repository_tree(self, path='', ref='', recursive=False, **kwargs):\n        \"\"\"Return a list of files in the repository.\n\n        Args:\n            path (str): Path of the top folder (/ by default)\n            ref (str): Reference to a commit or branch\n            recursive (bool): Whether to get the tree recursively\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the server failed to perform the request\n\n        Returns:\n            list: The representation of the tree\n        \"\"\"\n        gl_path = '/projects/%s/repository/tree' % self.get_id()\n        query_data = {'recursive': recursive}\n        if path:\n            query_data['path'] = path\n        if ref:\n            query_data['ref'] = ref\n        return self.manager.gitlab.http_list(gl_path, query_data=query_data,\n                                             **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a file by blob SHA.", "response": "def repository_blob(self, sha, **kwargs):\n        \"\"\"Return a file by blob SHA.\n\n        Args:\n            sha(str): ID of the blob\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the server failed to perform the request\n\n        Returns:\n            dict: The blob content and metadata\n        \"\"\"\n\n        path = '/projects/%s/repository/blobs/%s' % (self.get_id(), sha)\n        return self.manager.gitlab.http_get(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef repository_compare(self, from_, to, **kwargs):\n        path = '/projects/%s/repository/compare' % self.get_id()\n        query_data = {'from': from_, 'to': to}\n        return self.manager.gitlab.http_get(path, query_data=query_data,\n                                            **kwargs)", "response": "Return a diff between two branches or commits."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns a list of contributors for the project.", "response": "def repository_contributors(self, **kwargs):\n        \"\"\"Return a list of contributors for the project.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the server failed to perform the request\n\n        Returns:\n            list: The contributors\n        \"\"\"\n        path = '/projects/%s/repository/contributors' % self.get_id()\n        return self.manager.gitlab.http_list(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ncreating a forked from to relation between existing projects.", "response": "def create_fork_relation(self, forked_from_id, **kwargs):\n        \"\"\"Create a forked from/to relation between existing projects.\n\n        Args:\n            forked_from_id (int): The ID of the project that was forked from\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the relation could not be created\n        \"\"\"\n        path = '/projects/%s/fork/%s' % (self.get_id(), forked_from_id)\n        self.manager.gitlab.http_post(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete_fork_relation(self, **kwargs):\n        path = '/projects/%s/fork' % self.get_id()\n        self.manager.gitlab.http_delete(path, **kwargs)", "response": "Delete a forked relation between existing projects."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ndeletes merged branches. Args: **kwargs: Extra options to send to the server (e.g. sudo) Raises: GitlabAuthenticationError: If authentication is not correct GitlabDeleteError: If the server failed to perform the request", "response": "def delete_merged_branches(self, **kwargs):\n        \"\"\"Delete merged branches.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server failed to perform the request\n        \"\"\"\n        path = '/projects/%s/repository/merged_branches' % self.get_id()\n        self.manager.gitlab.http_delete(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the languages used in the project with percentage value.", "response": "def languages(self, **kwargs):\n        \"\"\"Get languages used in the project with percentage value.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the server failed to perform the request\n        \"\"\"\n        path = '/projects/%s/languages' % self.get_id()\n        return self.manager.gitlab.http_get(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nshare the project with a group.", "response": "def share(self, group_id, group_access, expires_at=None, **kwargs):\n        \"\"\"Share the project with a group.\n\n        Args:\n            group_id (int): ID of the group.\n            group_access (int): Access level for the group.\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server failed to perform the request\n        \"\"\"\n        path = '/projects/%s/share' % self.get_id()\n        data = {'group_id': group_id,\n                'group_access': group_access,\n                'expires_at': expires_at}\n        self.manager.gitlab.http_post(path, post_data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ndelete a shared project link within a group.", "response": "def unshare(self, group_id, **kwargs):\n        \"\"\"Delete a shared project link within a group.\n\n        Args:\n            group_id (int): ID of the group.\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server failed to perform the request\n        \"\"\"\n        path = '/projects/%s/share/%s' % (self.get_id(), group_id)\n        self.manager.gitlab.http_delete(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ntriggering a CI build.", "response": "def trigger_pipeline(self, ref, token, variables={}, **kwargs):\n        \"\"\"Trigger a CI build.\n\n        See https://gitlab.com/help/ci/triggers/README.md#trigger-a-build\n\n        Args:\n            ref (str): Commit to build; can be a branch name or a tag\n            token (str): The trigger token\n            variables (dict): Variables passed to the build script\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server failed to perform the request\n        \"\"\"\n        path = '/projects/%s/trigger/pipeline' % self.get_id()\n        post_data = {'ref': ref, 'token': token, 'variables': variables}\n        attrs = self.manager.gitlab.http_post(\n            path, post_data=post_data, **kwargs)\n        return ProjectPipeline(self.pipelines, attrs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef housekeeping(self, **kwargs):\n        path = '/projects/%s/housekeeping' % self.get_id()\n        self.manager.gitlab.http_post(path, **kwargs)", "response": "Start the housekeeping task."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nupload the specified file into the project.", "response": "def upload(self, filename, filedata=None, filepath=None, **kwargs):\n        \"\"\"Upload the specified file into the project.\n\n        .. note::\n\n            Either ``filedata`` or ``filepath`` *MUST* be specified.\n\n        Args:\n            filename (str): The name of the file being uploaded\n            filedata (bytes): The raw data of the file being uploaded\n            filepath (str): The path to a local file to upload (optional)\n\n        Raises:\n            GitlabConnectionError: If the server cannot be reached\n            GitlabUploadError: If the file upload fails\n            GitlabUploadError: If ``filedata`` and ``filepath`` are not\n                specified\n            GitlabUploadError: If both ``filedata`` and ``filepath`` are\n                specified\n\n        Returns:\n            dict: A ``dict`` with the keys:\n                * ``alt`` - The alternate text for the upload\n                * ``url`` - The direct url to the uploaded file\n                * ``markdown`` - Markdown for the uploaded file\n        \"\"\"\n        if filepath is None and filedata is None:\n            raise GitlabUploadError(\"No file contents or path specified\")\n\n        if filedata is not None and filepath is not None:\n            raise GitlabUploadError(\"File contents and file path specified\")\n\n        if filepath is not None:\n            with open(filepath, \"rb\") as f:\n                filedata = f.read()\n\n        url = ('/projects/%(id)s/uploads' % {\n            'id': self.id,\n        })\n        file_info = {\n            'file': (filename, filedata),\n        }\n        data = self.manager.gitlab.http_post(url, files=file_info)\n\n        return {\n            \"alt\": data['alt'],\n            \"url\": data['url'],\n            \"markdown\": data['markdown']\n        }"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns a snapshot of the repository.", "response": "def snapshot(self, wiki=False, streamed=False, action=None,\n                 chunk_size=1024, **kwargs):\n        \"\"\"Return a snapshot of the repository.\n\n        Args:\n            wiki (bool): If True return the wiki repository\n            streamed (bool): If True the data will be processed by chunks of\n                `chunk_size` and each chunk is passed to `action` for\n                treatment.\n            action (callable): Callable responsible of dealing with chunk of\n                data\n            chunk_size (int): Size of each chunk\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the content could not be retrieved\n\n        Returns:\n            str: The uncompressed tar archive of the repository\n        \"\"\"\n        path = '/projects/%s/snapshot' % self.get_id()\n        result = self.manager.gitlab.http_get(path, streamed=streamed,\n                                              raw=True, **kwargs)\n        return utils.response_content(result, streamed, action, chunk_size)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nsearch the project resources matching the provided string.", "response": "def search(self, scope, search, **kwargs):\n        \"\"\"Search the project resources matching the provided string.'\n\n        Args:\n            scope (str): Scope of the search\n            search (str): Search string\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabSearchError: If the server failed to perform the request\n\n        Returns:\n            GitlabList: A list of dicts describing the resources found.\n        \"\"\"\n        data = {'scope': scope, 'search': search}\n        path = '/projects/%s/search' % self.get_id()\n        return self.manager.gitlab.http_list(path, query_data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nstarts the mirror pull process for the project.", "response": "def mirror_pull(self, **kwargs):\n        \"\"\"Start the pull mirroring process for the project.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server failed to perform the request\n        \"\"\"\n        path = '/projects/%s/mirror/pull' % self.get_id()\n        self.manager.gitlab.http_post(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef transfer_project(self, to_namespace, **kwargs):\n        path = '/projects/%s/transfer' % (self.id,)\n        self.manager.gitlab.http_put(path,\n                                     post_data={\"namespace\": to_namespace},\n                                     **kwargs)", "response": "Transfer a project to the given namespace"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nimports a project from an archive file.", "response": "def import_project(self, file, path, namespace=None, overwrite=False,\n                       override_params=None, **kwargs):\n        \"\"\"Import a project from an archive file.\n\n        Args:\n            file: Data or file object containing the project\n            path (str): Name and path for the new project\n            namespace (str): The ID or path of the namespace that the project\n                will be imported to\n            overwrite (bool): If True overwrite an existing project with the\n                same path\n            override_params (dict): Set the specific settings for the project\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the server failed to perform the request\n\n        Returns:\n            dict: A representation of the import status.\n        \"\"\"\n        files = {\n            'file': ('file.tar.gz', file)\n        }\n        data = {\n            'path': path,\n            'overwrite': overwrite\n        }\n        if override_params:\n            for k, v in override_params.items():\n                data['override_params[%s]' % k] = v\n        if namespace:\n            data['namespace'] = namespace\n        return self.gitlab.http_post('/projects/import', post_data=data,\n                                     files=files, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nlisting all the runners.", "response": "def all(self, scope=None, **kwargs):\n        \"\"\"List all the runners.\n\n        Args:\n            scope (str): The scope of runners to show, one of: specific,\n                shared, active, paused, online\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the server failed to perform the request\n\n        Returns:\n            list(Runner): a list of runners matching the scope.\n        \"\"\"\n        path = '/runners/all'\n        query_data = {}\n        if scope is not None:\n            query_data['scope'] = scope\n        return self.gitlab.http_list(path, query_data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef verify(self, token, **kwargs):\n        path = '/runners/verify'\n        post_data = {'token': token}\n        self.gitlab.http_post(path, post_data=post_data, **kwargs)", "response": "Validates authentication credentials for a registered Runner."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef mark_as_done(self, **kwargs):\n        path = '%s/%s/mark_as_done' % (self.manager.path, self.id)\n        server_data = self.manager.gitlab.http_post(path, **kwargs)\n        self._update_attrs(server_data)", "response": "Mark the todo as done."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nmark all the todos as done.", "response": "def mark_all_as_done(self, **kwargs):\n        \"\"\"Mark all the todos as done.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabTodoError: If the server failed to perform the request\n\n        Returns:\n            int: The number of todos maked done\n        \"\"\"\n        result = self.gitlab.http_post('/todos/mark_as_done', **kwargs)\n        try:\n            return int(result)\n        except ValueError:\n            return 0"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets the status of the geo node.", "response": "def status(self, **kwargs):\n        \"\"\"Get the status of the geo node.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the server failed to perform the request\n\n        Returns:\n            dict: The status of the geo node\n        \"\"\"\n        path = '/geo_nodes/%s/status' % self.get_id()\n        return self.manager.gitlab.http_get(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nretrieve a single object from the server.", "response": "def get(self, id, lazy=False, **kwargs):\n        \"\"\"Retrieve a single object.\n\n        Args:\n            id (int or str): ID of the object to retrieve\n            lazy (bool): If True, don't request the server, but create a\n                         shallow object giving access to the managers. This is\n                         useful if you want to avoid useless calls to the API.\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            object: The generated RESTObject.\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabGetError: If the server cannot perform the request\n        \"\"\"\n        if not isinstance(id, int):\n            id = id.replace('/', '%2F')\n        path = '%s/%s' % (self.path, id)\n        if lazy is True:\n            return self._obj_cls(self, {self._obj_cls._id_attr: id})\n        server_data = self.gitlab.http_get(path, **kwargs)\n        return self._obj_cls(self, server_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get(self, id=None, **kwargs):\n        server_data = self.gitlab.http_get(self.path, **kwargs)\n        if server_data is None:\n            return None\n        return self._obj_cls(self, server_data)", "response": "Retrieve a single object."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef refresh(self, **kwargs):\n        if self._id_attr:\n            path = '%s/%s' % (self.manager.path, self.id)\n        else:\n            path = self.manager.path\n        server_data = self.manager.gitlab.http_get(path, **kwargs)\n        self._update_attrs(server_data)", "response": "Refresh a single object from server."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves a list of objects.", "response": "def list(self, **kwargs):\n        \"\"\"Retrieve a list of objects.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            list: The list of objects, or a generator if `as_list` is False\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the server cannot perform the request\n        \"\"\"\n\n        # Duplicate data to avoid messing with what the user sent us\n        data = kwargs.copy()\n        if self.gitlab.per_page:\n            data.setdefault('per_page', self.gitlab.per_page)\n\n        # We get the attributes that need some special transformation\n        types = getattr(self, '_types', {})\n        if types:\n            for attr_name, type_cls in types.items():\n                if attr_name in data.keys():\n                    type_obj = type_cls(data[attr_name])\n                    data[attr_name] = type_obj.get_for_api()\n\n        # Allow to overwrite the path, handy for custom listings\n        path = data.pop('path', self.path)\n\n        obj = self.gitlab.http_list(path, **data)\n        if isinstance(obj, list):\n            return [self._obj_cls(self, item) for item in obj]\n        else:\n            return base.RESTObjectList(self, self._obj_cls, obj)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a new object.", "response": "def create(self, data, **kwargs):\n        \"\"\"Create a new object.\n\n        Args:\n            data (dict): parameters to send to the server to create the\n                         resource\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            RESTObject: a new instance of the managed object class built with\n                the data sent by the server\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabCreateError: If the server cannot perform the request\n        \"\"\"\n        self._check_missing_create_attrs(data)\n        files = {}\n\n        # We get the attributes that need some special transformation\n        types = getattr(self, '_types', {})\n        if types:\n            # Duplicate data to avoid messing with what the user sent us\n            data = data.copy()\n            for attr_name, type_cls in types.items():\n                if attr_name in data.keys():\n                    type_obj = type_cls(data[attr_name])\n\n                    # if the type if FileAttribute we need to pass the data as\n                    # file\n                    if issubclass(type_cls, g_types.FileAttribute):\n                        k = type_obj.get_file_name(attr_name)\n                        files[attr_name] = (k, data.pop(attr_name))\n                    else:\n                        data[attr_name] = type_obj.get_for_api()\n\n        # Handle specific URL for creation\n        path = kwargs.pop('path', self.path)\n        server_data = self.gitlab.http_post(path, post_data=data, files=files,\n                                            **kwargs)\n        return self._obj_cls(self, server_data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn the HTTP method to use.", "response": "def _get_update_method(self):\n        \"\"\"Return the HTTP method to use.\n\n        Returns:\n            object: http_put (default) or http_post\n        \"\"\"\n        if getattr(self, '_update_uses_post', False):\n            http_method = self.gitlab.http_post\n        else:\n            http_method = self.gitlab.http_put\n        return http_method"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates an object on the server.", "response": "def update(self, id=None, new_data={}, **kwargs):\n        \"\"\"Update an object on the server.\n\n        Args:\n            id: ID of the object to update (can be None if not required)\n            new_data: the update data for the object\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            dict: The new object data (*not* a RESTObject)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabUpdateError: If the server cannot perform the request\n        \"\"\"\n\n        if id is None:\n            path = self.path\n        else:\n            path = '%s/%s' % (self.path, id)\n\n        self._check_missing_update_attrs(new_data)\n        files = {}\n\n        # We get the attributes that need some special transformation\n        types = getattr(self, '_types', {})\n        if types:\n            # Duplicate data to avoid messing with what the user sent us\n            new_data = new_data.copy()\n            for attr_name, type_cls in types.items():\n                if attr_name in new_data.keys():\n                    type_obj = type_cls(new_data[attr_name])\n\n                    # if the type if FileAttribute we need to pass the data as\n                    # file\n                    if issubclass(type_cls, g_types.FileAttribute):\n                        k = type_obj.get_file_name(attr_name)\n                        files[attr_name] = (k, new_data.pop(attr_name))\n                    else:\n                        new_data[attr_name] = type_obj.get_for_api()\n\n        http_method = self._get_update_method()\n        return http_method(path, post_data=new_data, files=files, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating or update the object.", "response": "def set(self, key, value, **kwargs):\n        \"\"\"Create or update the object.\n\n        Args:\n            key (str): The key of the object to create/update\n            value (str): The value to set for the object\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabSetError: If an error occured\n\n        Returns:\n            obj: The created/updated attribute\n        \"\"\"\n        path = '%s/%s' % (self.path, key.replace('/', '%2F'))\n        data = {'value': value}\n        server_data = self.gitlab.http_put(path, post_data=data, **kwargs)\n        return self._obj_cls(self, server_data)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ndeletes an object on the server.", "response": "def delete(self, id, **kwargs):\n        \"\"\"Delete an object on the server.\n\n        Args:\n            id: ID of the object to delete\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabDeleteError: If the server cannot perform the request\n        \"\"\"\n        if id is None:\n            path = self.path\n        else:\n            if not isinstance(id, int):\n                id = id.replace('/', '%2F')\n            path = '%s/%s' % (self.path, id)\n        self.gitlab.http_delete(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nsave the changes made to the object to the server.", "response": "def save(self, **kwargs):\n        \"\"\"Save the changes made to the object to the server.\n\n        The object is updated to match what the server returns.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raise:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabUpdateError: If the server cannot perform the request\n        \"\"\"\n        updated_data = self._get_updated_data()\n        # Nothing to update. Server fails if sent an empty dict.\n        if not updated_data:\n            return\n\n        # call the manager\n        obj_id = self.get_id()\n        server_data = self.manager.update(obj_id, updated_data, **kwargs)\n        if server_data is not None:\n            self._update_attrs(server_data)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef user_agent_detail(self, **kwargs):\n        path = '%s/%s/user_agent_detail' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)", "response": "Get the user agent detail."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef approve(self, access_level=gitlab.DEVELOPER_ACCESS, **kwargs):\n\n        path = '%s/%s/approve' % (self.manager.path, self.id)\n        data = {'access_level': access_level}\n        server_data = self.manager.gitlab.http_put(path, post_data=data,\n                                                   **kwargs)\n        self._update_attrs(server_data)", "response": "Approve an access request."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef todo(self, **kwargs):\n        path = '%s/%s/todo' % (self.manager.path, self.get_id())\n        self.manager.gitlab.http_post(path, **kwargs)", "response": "Create a todo associated to the object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget the time stats for the object.", "response": "def time_stats(self, **kwargs):\n        \"\"\"Get time stats for the object.\n\n        Args:\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabTimeTrackingError: If the time tracking update cannot be done\n        \"\"\"\n        # Use the existing time_stats attribute if it exist, otherwise make an\n        # API call\n        if 'time_stats' in self.attributes:\n            return self.attributes['time_stats']\n\n        path = '%s/%s/time_stats' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef time_estimate(self, duration, **kwargs):\n        path = '%s/%s/time_estimate' % (self.manager.path, self.get_id())\n        data = {'duration': duration}\n        return self.manager.gitlab.http_post(path, post_data=data, **kwargs)", "response": "Set an estimated time of work for the object."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_time_estimate(self, **kwargs):\n        path = '%s/%s/reset_time_estimate' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_post(path, **kwargs)", "response": "Resets the estimated time for the object to 0 seconds."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reset_spent_time(self, **kwargs):\n        path = '%s/%s/reset_spent_time' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_post(path, **kwargs)", "response": "Resets the time spent working on the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nlists the participants. Args: all (bool): If True, return all the items, without pagination per_page (int): Number of items to retrieve per request page (int): ID of the page to return (starts with page 1) as_list (bool): If set to False and no pagination option is defined, return a generator instead of a list **kwargs: Extra options to send to the server (e.g. sudo) Raises: GitlabAuthenticationError: If authentication is not correct GitlabListError: If the list could not be retrieved Returns: RESTObjectList: The list of participants", "response": "def participants(self, **kwargs):\n        \"\"\"List the participants.\n\n        Args:\n            all (bool): If True, return all the items, without pagination\n            per_page (int): Number of items to retrieve per request\n            page (int): ID of the page to return (starts with page 1)\n            as_list (bool): If set to False and no pagination option is\n                defined, return a generator instead of a list\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabListError: If the list could not be retrieved\n\n        Returns:\n            RESTObjectList: The list of participants\n        \"\"\"\n\n        path = '%s/%s/participants' % (self.manager.path, self.get_id())\n        return self.manager.gitlab.http_get(path, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef render(self, link_url, image_url, **kwargs):\n        path = '%s/render' % self.path\n        data = {'link_url': link_url, 'image_url': image_url}\n        return self.gitlab.http_get(path, data, **kwargs)", "response": "Preview link_url and image_url after interpolation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef from_config(cls, gitlab_id=None, config_files=None):\n        config = gitlab.config.GitlabConfigParser(gitlab_id=gitlab_id,\n                                                  config_files=config_files)\n        return cls(config.url, private_token=config.private_token,\n                   oauth_token=config.oauth_token,\n                   ssl_verify=config.ssl_verify, timeout=config.timeout,\n                   http_username=config.http_username,\n                   http_password=config.http_password,\n                   api_version=config.api_version,\n                   per_page=config.per_page)", "response": "Create a Gitlab connection from a list of configuration files."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef auth(self):\n        if self.private_token or self.oauth_token:\n            self._token_auth()\n        else:\n            self._credentials_auth()", "response": "Performs an authentication.\n\n        Uses either the private token, or the email/password pair.\n\n        The `user` attribute will hold a `gitlab.objects.CurrentUser` object on\n        success."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn the version and revision of the gitlab server.", "response": "def version(self):\n        \"\"\"Returns the version and revision of the gitlab server.\n\n        Note that self.version and self.revision will be set on the gitlab\n        object.\n\n        Returns:\n            tuple (str, str): The server version and server revision.\n                              ('unknown', 'unknwown') if the server doesn't\n                              perform as expected.\n        \"\"\"\n        if self._server_version is None:\n            try:\n                data = self.http_get('/version')\n                self._server_version = data['version']\n                self._server_revision = data['revision']\n            except Exception:\n                self._server_version = self._server_revision = 'unknown'\n\n        return self._server_version, self._server_revision"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef lint(self, content, **kwargs):\n        post_data = {'content': content}\n        data = self.http_post('/ci/lint', post_data=post_data, **kwargs)\n        return (data['status'] == 'valid', data['errors'])", "response": "Validate a gitlab CI configuration."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nrender an arbitrary Markdown document.", "response": "def markdown(self, text, gfm=False, project=None, **kwargs):\n        \"\"\"Render an arbitrary Markdown document.\n\n        Args:\n            text (str): The markdown text to render\n            gfm (bool): Render text using GitLab Flavored Markdown. Default is\n                False\n            project (str): Full path of a project used a context when `gfm` is\n                True\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabMarkdownError: If the server cannot perform the request\n\n        Returns:\n            str: The HTML rendering of the markdown text.\n        \"\"\"\n        post_data = {'text': text, 'gfm': gfm}\n        if project is not None:\n            post_data['project'] = project\n        data = self.http_post('/markdown', post_data=post_data, **kwargs)\n        return data['html']"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef set_license(self, license, **kwargs):\n        data = {'license': license}\n        return self.http_post('/license', post_data=data, **kwargs)", "response": "Add a new license."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nbuild the full url from a path.", "response": "def _build_url(self, path):\n        \"\"\"Returns the full url from path.\n\n        If path is already a url, return it unchanged. If it's a path, append\n        it to the stored url.\n\n        Returns:\n            str: The full URL\n        \"\"\"\n        if path.startswith('http://') or path.startswith('https://'):\n            return path\n        else:\n            return '%s%s' % (self._url, path)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef http_request(self, verb, path, query_data={}, post_data=None,\n                     streamed=False, files=None, **kwargs):\n        \"\"\"Make an HTTP request to the Gitlab server.\n\n        Args:\n            verb (str): The HTTP method to call ('get', 'post', 'put',\n                        'delete')\n            path (str): Path or full URL to query ('/projects' or\n                        'http://whatever/v4/api/projecs')\n            query_data (dict): Data to send as query parameters\n            post_data (dict): Data to send in the body (will be converted to\n                              json)\n            streamed (bool): Whether the data should be streamed\n            files (dict): The files to send to the server\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            A requests result object.\n\n        Raises:\n            GitlabHttpError: When the return code is not 2xx\n        \"\"\"\n\n        url = self._build_url(path)\n\n        params = {}\n        utils.copy_dict(params, query_data)\n\n        # Deal with kwargs: by default a user uses kwargs to send data to the\n        # gitlab server, but this generates problems (python keyword conflicts\n        # and python-gitlab/gitlab conflicts).\n        # So we provide a `query_parameters` key: if it's there we use its dict\n        # value as arguments for the gitlab server, and ignore the other\n        # arguments, except pagination ones (per_page and page)\n        if 'query_parameters' in kwargs:\n            utils.copy_dict(params, kwargs['query_parameters'])\n            for arg in ('per_page', 'page'):\n                if arg in kwargs:\n                    params[arg] = kwargs[arg]\n        else:\n            utils.copy_dict(params, kwargs)\n\n        opts = self._get_session_opts(content_type='application/json')\n\n        verify = opts.pop('verify')\n        timeout = opts.pop('timeout')\n\n        # We need to deal with json vs. data when uploading files\n        if files:\n            data = post_data\n            json = None\n            del opts[\"headers\"][\"Content-type\"]\n        else:\n            json = post_data\n            data = None\n\n        # Requests assumes that `.` should not be encoded as %2E and will make\n        # changes to urls using this encoding. Using a prepped request we can\n        # get the desired behavior.\n        # The Requests behavior is right but it seems that web servers don't\n        # always agree with this decision (this is the case with a default\n        # gitlab installation)\n        req = requests.Request(verb, url, json=json, data=data, params=params,\n                               files=files, **opts)\n        prepped = self.session.prepare_request(req)\n        prepped.url = utils.sanitized_url(prepped.url)\n        settings = self.session.merge_environment_settings(\n            prepped.url, {}, streamed, verify, None)\n\n        # obey the rate limit by default\n        obey_rate_limit = kwargs.get(\"obey_rate_limit\", True)\n\n        # set max_retries to 10 by default, disable by setting it to -1\n        max_retries = kwargs.get(\"max_retries\", 10)\n        cur_retries = 0\n\n        while True:\n            result = self.session.send(prepped, timeout=timeout, **settings)\n\n            self._check_redirects(result)\n\n            if 200 <= result.status_code < 300:\n                return result\n\n            if 429 == result.status_code and obey_rate_limit:\n                if max_retries == -1 or cur_retries < max_retries:\n                    wait_time = 2 ** cur_retries * 0.1\n                    if \"Retry-After\" in result.headers:\n                        wait_time = int(result.headers[\"Retry-After\"])\n                    cur_retries += 1\n                    time.sleep(wait_time)\n                    continue\n\n            error_message = result.content\n            try:\n                error_json = result.json()\n                for k in ('message', 'error'):\n                    if k in error_json:\n                        error_message = error_json[k]\n            except (KeyError, ValueError, TypeError):\n                pass\n\n            if result.status_code == 401:\n                raise GitlabAuthenticationError(\n                    response_code=result.status_code,\n                    error_message=error_message,\n                    response_body=result.content)\n\n            raise GitlabHttpError(response_code=result.status_code,\n                                  error_message=error_message,\n                                  response_body=result.content)", "response": "Make an HTTP request to the Gitlab server."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef http_get(self, path, query_data={}, streamed=False, raw=False,\n                 **kwargs):\n        \"\"\"Make a GET request to the Gitlab server.\n\n        Args:\n            path (str): Path or full URL to query ('/projects' or\n                        'http://whatever/v4/api/projecs')\n            query_data (dict): Data to send as query parameters\n            streamed (bool): Whether the data should be streamed\n            raw (bool): If True do not try to parse the output as json\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            A requests result object is streamed is True or the content type is\n            not json.\n            The parsed json data otherwise.\n\n        Raises:\n            GitlabHttpError: When the return code is not 2xx\n            GitlabParsingError: If the json data could not be parsed\n        \"\"\"\n        result = self.http_request('get', path, query_data=query_data,\n                                   streamed=streamed, **kwargs)\n\n        if (result.headers['Content-Type'] == 'application/json'\n           and not streamed\n           and not raw):\n            try:\n                return result.json()\n            except Exception:\n                raise GitlabParsingError(\n                    error_message=\"Failed to parse the server message\")\n        else:\n            return result", "response": "Make a GET request to the Gitlab server."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nmakes a GET request to the Gitlab server for list - oriented queries.", "response": "def http_list(self, path, query_data={}, as_list=None, **kwargs):\n        \"\"\"Make a GET request to the Gitlab server for list-oriented queries.\n\n        Args:\n            path (str): Path or full URL to query ('/projects' or\n                        'http://whatever/v4/api/projecs')\n            query_data (dict): Data to send as query parameters\n            **kwargs: Extra options to send to the server (e.g. sudo, page,\n                      per_page)\n\n        Returns:\n            list: A list of the objects returned by the server. If `as_list` is\n            False and no pagination-related arguments (`page`, `per_page`,\n            `all`) are defined then a GitlabList object (generator) is returned\n            instead. This object will make API calls when needed to fetch the\n            next items from the server.\n\n        Raises:\n            GitlabHttpError: When the return code is not 2xx\n            GitlabParsingError: If the json data could not be parsed\n        \"\"\"\n\n        # In case we want to change the default behavior at some point\n        as_list = True if as_list is None else as_list\n\n        get_all = kwargs.pop('all', False)\n        url = self._build_url(path)\n\n        if get_all is True:\n            return list(GitlabList(self, url, query_data, **kwargs))\n\n        if 'page' in kwargs or as_list is True:\n            # pagination requested, we return a list\n            return list(GitlabList(self, url, query_data, get_next=False,\n                                   **kwargs))\n\n        # No pagination, generator requested\n        return GitlabList(self, url, query_data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef http_post(self, path, query_data={}, post_data={}, files=None,\n                  **kwargs):\n        \"\"\"Make a POST request to the Gitlab server.\n\n        Args:\n            path (str): Path or full URL to query ('/projects' or\n                        'http://whatever/v4/api/projecs')\n            query_data (dict): Data to send as query parameters\n            post_data (dict): Data to send in the body (will be converted to\n                              json)\n            files (dict): The files to send to the server\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            The parsed json returned by the server if json is return, else the\n            raw content\n\n        Raises:\n            GitlabHttpError: When the return code is not 2xx\n            GitlabParsingError: If the json data could not be parsed\n        \"\"\"\n        result = self.http_request('post', path, query_data=query_data,\n                                   post_data=post_data, files=files, **kwargs)\n        try:\n            if result.headers.get('Content-Type', None) == 'application/json':\n                return result.json()\n        except Exception:\n            raise GitlabParsingError(\n                error_message=\"Failed to parse the server message\")\n        return result", "response": "Make a POST request to the Gitlab server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef http_put(self, path, query_data={}, post_data={}, files=None,\n                 **kwargs):\n        \"\"\"Make a PUT request to the Gitlab server.\n\n        Args:\n            path (str): Path or full URL to query ('/projects' or\n                        'http://whatever/v4/api/projecs')\n            query_data (dict): Data to send as query parameters\n            post_data (dict): Data to send in the body (will be converted to\n                              json)\n            files (dict): The files to send to the server\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Returns:\n            The parsed json returned by the server.\n\n        Raises:\n            GitlabHttpError: When the return code is not 2xx\n            GitlabParsingError: If the json data could not be parsed\n        \"\"\"\n        result = self.http_request('put', path, query_data=query_data,\n                                   post_data=post_data, files=files, **kwargs)\n        try:\n            return result.json()\n        except Exception:\n            raise GitlabParsingError(\n                error_message=\"Failed to parse the server message\")", "response": "Make a PUT request to the Gitlab server."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nsearches GitLab resources matching the provided string.", "response": "def search(self, scope, search, **kwargs):\n        \"\"\"Search GitLab resources matching the provided string.'\n\n        Args:\n            scope (str): Scope of the search\n            search (str): Search string\n            **kwargs: Extra options to send to the server (e.g. sudo)\n\n        Raises:\n            GitlabAuthenticationError: If authentication is not correct\n            GitlabSearchError: If the server failed to perform the request\n\n        Returns:\n            GitlabList: A list of dicts describing the resources found.\n        \"\"\"\n        data = {'scope': scope, 'search': search}\n        return self.http_list('/search', query_data=data, **kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef run_callback(self):\n\n        if self._callback_func is not None:\n            try:\n                self._callback_func(self._request, self._result)\n            except Exception:  # pylint: disable=broad-except\n                LOGGER.exception('An unhandled error occurred while running '\n                                 'future callback')", "response": "Calls the callback_func passing in the two positional arguments conditionally waiting if the callback function hasn t been set yet."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_send_message(self, connection, send_message):\n        self._send_message[connection] = send_message\n        LOGGER.debug(\"Added send_message function \"\n                     \"for connection %s\", connection)", "response": "Adds a send_message function to the Dispatcher s _send_message dictionary indexed by connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds a send_last_message function to the Dispatcher s _send_last_message dictionary indexed by connection.", "response": "def add_send_last_message(self, connection, send_last_message):\n        \"\"\"Adds a send_last_message function to the Dispatcher's\n        dictionary of functions indexed by connection.\n\n        Args:\n            connection (str): A locally unique identifier\n                provided by the receiver of messages.\n            send_last_message (fn): The method that should be called\n                by the dispatcher to respond to messages which\n                arrive via connection, when the connection should be closed\n                after the message has been sent.\n        \"\"\"\n        self._send_last_message[connection] = send_last_message\n        LOGGER.debug(\"Added send_last_message function \"\n                     \"for connection %s\", connection)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nremoving a send_message function previously registered with the Dispatcher.", "response": "def remove_send_message(self, connection):\n        \"\"\"Removes a send_message function previously registered\n        with the Dispatcher.\n\n        Args:\n            connection (str): A locally unique identifier provided\n                by the receiver of messages.\n        \"\"\"\n        if connection in self._send_message:\n            del self._send_message[connection]\n            LOGGER.debug(\"Removed send_message function \"\n                         \"for connection %s\", connection)\n        else:\n            LOGGER.warning(\"Attempted to remove send_message \"\n                           \"function for connection %s, but no \"\n                           \"send_message function was registered\",\n                           connection)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef remove_send_last_message(self, connection):\n        if connection in self._send_last_message:\n            del self._send_last_message[connection]\n            LOGGER.debug(\"Removed send_last_message function \"\n                         \"for connection %s\", connection)\n        else:\n            LOGGER.warning(\"Attempted to remove send_last_message \"\n                           \"function for connection %s, but no \"\n                           \"send_last_message function was registered\",\n                           connection)", "response": "Removes a send_last_message function previously registered with the Dispatcher."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_preprocessor(self, message_type, preprocessor, executor):\n        '''\n        Sets PREPROCESSOR to run on MESSAGE_TYPE in EXECUTOR.\n\n        PREPROCESSOR: fn(message_content: bytes) -> PreprocessorResult\n        '''\n        self._preprocessors[message_type] = \\\n            _PreprocessorManager(\n                executor=executor,\n                preprocessor=preprocessor)", "response": "Sets the PREPROCESSOR to run on MESSAGE_TYPE in EXECUTOR."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef walk(self, head=None):\n        head = head or self._root_node\n\n        queue = []\n        queue.insert(0, head)\n\n        while queue:\n            node = queue.pop()\n\n            yield node.num, node.previous, node.siblings\n\n            for child in node.siblings:\n                if child in self._graph:\n                    queue.insert(0, self._graph[child])", "response": "Do a breadth - first walk of the graph yielding on each node and its children."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the sawtooth configuration directory based on the SAWTOOTH_HOME environment variable or OS defaults.", "response": "def _get_config_dir():\n    \"\"\"Returns the sawtooth configuration directory based on the\n    SAWTOOTH_HOME environment variable (if set) or OS defaults.\n    \"\"\"\n    if 'SAWTOOTH_HOME' in os.environ:\n        return os.path.join(os.environ['SAWTOOTH_HOME'], 'etc')\n\n    if os.name == 'nt':\n        base_dir = \\\n            os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n        return os.path.join(base_dir, 'conf')\n\n    return '/etc/sawtooth'"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndetermine the directory path based on the configuration settings.", "response": "def _get_dir(toml_config_setting, sawtooth_home_dir, windows_dir, default_dir):\n    \"\"\"Determines the directory path based on configuration.\n\n    Arguments:\n        toml_config_setting (str): The name of the config setting related\n            to the directory which will appear in path.toml.\n        sawtooth_home_dir (str): The directory under the SAWTOOTH_HOME\n            environment variable.  For example, for 'data' if the data\n            directory is $SAWTOOTH_HOME/data.\n        windows_dir (str): The windows path relative to the computed base\n            directory.\n        default_dir (str): The default path on Linux.\n\n    Returns:\n        directory (str): The path.\n    \"\"\"\n    conf_file = os.path.join(_get_config_dir(), 'path.toml')\n    if os.path.exists(conf_file):\n        with open(conf_file) as fd:\n            raw_config = fd.read()\n        toml_config = toml.loads(raw_config)\n        if toml_config_setting in toml_config:\n            return toml_config[toml_config_setting]\n\n    if 'SAWTOOTH_HOME' in os.environ:\n        return os.path.join(os.environ['SAWTOOTH_HOME'], sawtooth_home_dir)\n\n    if os.name == 'nt':\n        base_dir = \\\n            os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n        return os.path.join(base_dir, windows_dir)\n\n    return default_dir"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_role(self, name):\n\n        address = _create_role_address(name)\n        role_list_bytes = None\n\n        try:\n            role_list_bytes = self._state_view.get(address=address)\n        except KeyError:\n            return None\n\n        if role_list_bytes is not None:\n            role_list = _create_from_bytes(role_list_bytes,\n                                           identity_pb2.RoleList)\n            for role in role_list.roles:\n                if role.name == name:\n                    return role\n        return None", "response": "Get a single Role by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef get_roles(self):\n\n        prefix = _IDENTITY_NS + _ROLE_NS\n        rolelist_list = [\n            _create_from_bytes(d, identity_pb2.RoleList)\n            for _, d in self._state_view.leaves(prefix=prefix)\n        ]\n        roles = []\n        for role_list in rolelist_list:\n            for role in role_list.roles:\n                roles.append(role)\n        return sorted(roles, key=lambda r: r.name)", "response": "Return all the Roles under the Identity namespace."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_policy(self, name):\n\n        address = _create_policy_address(name)\n        policy_list_bytes = None\n\n        try:\n            policy_list_bytes = self._state_view.get(address=address)\n        except KeyError:\n            return None\n\n        if policy_list_bytes is not None:\n            policy_list = _create_from_bytes(policy_list_bytes,\n                                             identity_pb2.PolicyList)\n            for policy in policy_list.policies:\n                if policy.name == name:\n                    return policy\n        return None", "response": "Get a single Policy by name."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_policies(self):\n\n        prefix = _IDENTITY_NS + _POLICY_NS\n        policylist_list = [\n            _create_from_bytes(d, identity_pb2.PolicyList)\n            for _, d in self._state_view.leaves(prefix=prefix)\n        ]\n        policies = []\n        for policy_list in policylist_list:\n            for policy in policy_list.policies:\n                policies.append(policy)\n        return sorted(policies, key=lambda p: p.name)", "response": "Returns all the Policies under the Identity namespace."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns an ordered list of batches to inject at the beginning of the block. Can also return None if no batches should be injected at the beginning of the block.", "response": "def block_start(self, previous_block):\n        \"\"\"Returns an ordered list of batches to inject at the beginning of the\n        block. Can also return None if no batches should be injected.\n\n        Args:\n            previous_block (Block): The previous block.\n\n        Returns:\n            A list of batches to inject.\n        \"\"\"\n\n        previous_header_bytes = previous_block.header\n        previous_header = BlockHeader()\n        previous_header.ParseFromString(previous_header_bytes)\n\n        block_info = BlockInfo(\n            block_num=previous_header.block_num,\n            previous_block_id=previous_header.previous_block_id,\n            signer_public_key=previous_header.signer_public_key,\n            header_signature=previous_block.header_signature,\n            timestamp=int(time.time()))\n\n        return [self.create_batch(block_info)]"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nenforces the validation rules for the given list of batches.", "response": "def enforce_validation_rules(settings_view, expected_signer, batches):\n    \"\"\"\n    Retrieve the validation rules stored in state and check that the\n    given batches do not violate any of those rules. These rules include:\n\n        NofX: Only N of transaction type X may be included in a block.\n        XatY: A transaction of type X must be in the list at position Y.\n        local: A transaction must be signed by the given public key\n\n    If any setting stored in state does not match the required format for\n    that rule, the rule will be ignored.\n\n    Args:\n        settings_view (:obj:SettingsView): the settings view to find the\n            current rule values\n        expected_signer (str): the public key used to use for local signing\n        batches (:list:Batch): the list of batches to validate\n\n    \"\"\"\n    rules = settings_view.get_setting(\n        \"sawtooth.validator.block_validation_rules\")\n\n    if rules is None:\n        return True\n\n    transactions = []\n    for batch in batches:\n        transactions += batch.transactions\n\n    rules = rules.split(\";\")\n    valid = True\n    for rule in rules:\n        try:\n            rule_type, arguments = rule.split(\":\")\n        except ValueError:\n            LOGGER.warning(\"Validation Rule Ignored, not in the correct \"\n                           \"format: %s\",\n                           rule)\n            continue\n\n        rule_type = rule_type.strip()\n        # NofX: Only N of transaction type X may be included in a block.\n        if rule_type == \"NofX\":\n            valid = _do_nofx(transactions, arguments)\n\n        # XatY: A transaction of type X must be in the block at position Y.\n        elif rule_type == \"XatY\":\n            valid = _do_xaty(transactions, arguments)\n\n        # local: A transaction must be signed by the same key as the block.\n        elif rule_type == \"local\":\n            valid = _do_local(\n                transactions, expected_signer, arguments)\n\n        if not valid:\n            return False\n\n    return valid"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _do_nofx(transactions, arguments):\n    try:\n        num, family = arguments.split(',')\n        limit = int(num.strip())\n    except ValueError:\n        LOGGER.warning(\"Ignore, NofX requires arguments in the format \"\n                       \"int,family not %s\", arguments)\n        return True\n    count = 0\n    family = family.strip()\n    for txn in transactions:\n        header = TransactionHeader()\n        header.ParseFromString(txn.header)\n        if header.family_name == family:\n            count += 1\n\n        if count > limit:\n            LOGGER.debug(\"Too many transactions of type %s\", family)\n            return False\n\n    return True", "response": "Check if the transactions are of type N of X."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate the transactions for the XatY rule.", "response": "def _do_xaty(transactions, arguments):\n    \"\"\"\n    A transaction of type X must be in the block at position Y. The\n    first argument is interpreted as the name of a transaction family.\n    The second argument must be interpretable as an integer and defines\n    the index of the transaction in the block that must be checked.\n    Negative numbers can be used and count backwards from the last\n    transaction in the block. The first transaction in the block has\n    index 0. The last transaction in the block has index -1. If abs(Y)\n    is larger than the number of transactions per block, then there\n    would not be a transaction of type X at Y and the block would be\n    invalid. For example, the string \"XatY:intkey,0\" means the first\n    transaction in the block must be an intkey transaction.\n    \"\"\"\n    try:\n        family, num = arguments.split(',')\n        position = int(num.strip())\n    except ValueError:\n        LOGGER.warning(\"Ignore, XatY requires arguments in the format \"\n                       \"family,position not %s\", arguments)\n        return True\n\n    family = family.strip()\n    if abs(position) >= len(transactions):\n        LOGGER.debug(\"Block does not have enough transactions to \"\n                     \"validate this rule XatY:%s\", arguments)\n        return False\n    txn = transactions[position]\n\n    header = TransactionHeader()\n    header.ParseFromString(txn.header)\n    if header.family_name != family:\n        LOGGER.debug(\"Transaction at postion %s is not of type %s\",\n                     position, family)\n        return False\n    return True"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _do_local(transactions, expected_signer, arguments):\n    indices = arguments.split(\",\")\n    txns_len = len(transactions)\n    for index in indices:\n        try:\n            index = int(index.strip())\n        except ValueError:\n            LOGGER.warning(\"Ignore, local requries one or more comma \"\n                           \"seperated integers that represent indices, not\"\n                           \" %s\", arguments)\n            return True\n\n        if abs(index) >= txns_len:\n            LOGGER.debug(\"Ignore, Block does not have enough \"\n                         \"transactions to validate this rule local:%s\",\n                         index)\n            continue\n        txn = transactions[index]\n        header = TransactionHeader()\n        header.ParseFromString(txn.header)\n\n        if header.signer_public_key != expected_signer:\n            LOGGER.debug(\"Transaction at postion %s was not signed by the\"\n                         \" same key as the block.\", index)\n            return False\n    return True", "response": "Validate that a transaction is signed by the same key as the block."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef load_toml_validator_config(filename):\n    if not os.path.exists(filename):\n        LOGGER.info(\n            \"Skipping validator config loading from non-existent config file:\"\n            \" %s\", filename)\n        return ValidatorConfig()\n\n    LOGGER.info(\"Loading validator information from config: %s\", filename)\n\n    try:\n        with open(filename) as fd:\n            raw_config = fd.read()\n    except IOError as e:\n        raise LocalConfigurationError(\n            \"Unable to load validator configuration file: {}\".format(str(e)))\n\n    toml_config = toml.loads(raw_config)\n    invalid_keys = set(toml_config.keys()).difference(\n        ['bind', 'endpoint', 'peering', 'seeds', 'peers', 'network_public_key',\n         'network_private_key', 'scheduler', 'permissions', 'roles',\n         'opentsdb_url', 'opentsdb_db', 'opentsdb_username',\n         'opentsdb_password', 'minimum_peer_connectivity',\n         'maximum_peer_connectivity', 'state_pruning_block_depth',\n         'fork_cache_keep_time',\n         'component_thread_pool_workers', 'network_thread_pool_workers',\n         'signature_thread_pool_workers'])\n    if invalid_keys:\n        raise LocalConfigurationError(\n            \"Invalid keys in validator config: \"\n            \"{}\".format(\", \".join(sorted(list(invalid_keys)))))\n    bind_network = None\n    bind_component = None\n    bind_consensus = None\n    for bind in toml_config.get(\"bind\", []):\n        if \"network\" in bind:\n            bind_network = bind[bind.find(\":\") + 1:]\n        if \"component\" in bind:\n            bind_component = bind[bind.find(\":\") + 1:]\n        if \"consensus\" in bind:\n            bind_consensus = bind[bind.find(\":\") + 1:]\n\n    network_public_key = None\n    network_private_key = None\n\n    if toml_config.get(\"network_public_key\") is not None:\n        network_public_key = toml_config.get(\"network_public_key\").encode()\n\n    if toml_config.get(\"network_private_key\") is not None:\n        network_private_key = toml_config.get(\"network_private_key\").encode()\n\n    config = ValidatorConfig(\n        bind_network=bind_network,\n        bind_component=bind_component,\n        bind_consensus=bind_consensus,\n        endpoint=toml_config.get(\"endpoint\", None),\n        peering=toml_config.get(\"peering\", None),\n        seeds=toml_config.get(\"seeds\", None),\n        peers=toml_config.get(\"peers\", None),\n        network_public_key=network_public_key,\n        network_private_key=network_private_key,\n        scheduler=toml_config.get(\"scheduler\", None),\n        permissions=parse_permissions(toml_config.get(\"permissions\", None)),\n        roles=toml_config.get(\"roles\", None),\n        opentsdb_url=toml_config.get(\"opentsdb_url\", None),\n        opentsdb_db=toml_config.get(\"opentsdb_db\", None),\n        opentsdb_username=toml_config.get(\"opentsdb_username\", None),\n        opentsdb_password=toml_config.get(\"opentsdb_password\", None),\n        minimum_peer_connectivity=toml_config.get(\n            \"minimum_peer_connectivity\", None),\n        maximum_peer_connectivity=toml_config.get(\n            \"maximum_peer_connectivity\", None),\n        state_pruning_block_depth=toml_config.get(\n            \"state_pruning_block_depth\", None),\n        fork_cache_keep_time=toml_config.get(\n            \"fork_cache_keep_time\", None),\n        component_thread_pool_workers=toml_config.get(\n            \"component_thread_pool_workers\", None),\n        network_thread_pool_workers=toml_config.get(\n            \"network_thread_pool_workers\", None),\n        signature_thread_pool_workers=toml_config.get(\n            \"signature_thread_pool_workers\", None)\n    )\n\n    return config", "response": "Loads a validator config from the specified TOML file."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving a list of ValidatorConfig objects merges them into a single ValidatorConfig.", "response": "def merge_validator_config(configs):\n    \"\"\"\n    Given a list of ValidatorConfig objects, merges them into a single\n    ValidatorConfig, giving priority in the order of the configs\n    (first has highest priority).\n    \"\"\"\n    bind_network = None\n    bind_component = None\n    bind_consensus = None\n    endpoint = None\n    peering = None\n    seeds = None\n    peers = None\n    network_public_key = None\n    network_private_key = None\n    scheduler = None\n    permissions = None\n    roles = None\n    opentsdb_url = None\n    opentsdb_db = None\n    opentsdb_username = None\n    opentsdb_password = None\n    minimum_peer_connectivity = None\n    maximum_peer_connectivity = None\n    state_pruning_block_depth = None\n    fork_cache_keep_time = None\n    component_thread_pool_workers = None\n    network_thread_pool_workers = None\n    signature_thread_pool_workers = None\n\n    for config in reversed(configs):\n        if config.bind_network is not None:\n            bind_network = config.bind_network\n        if config.bind_component is not None:\n            bind_component = config.bind_component\n        if config.bind_consensus is not None:\n            bind_consensus = config.bind_consensus\n        if config.endpoint is not None:\n            endpoint = config.endpoint\n        if config.peering is not None:\n            peering = config.peering\n        if config.seeds is not None:\n            seeds = config.seeds\n        if config.peers is not None:\n            peers = config.peers\n        if config.network_public_key is not None:\n            network_public_key = config.network_public_key\n        if config.network_private_key is not None:\n            network_private_key = config.network_private_key\n        if config.scheduler is not None:\n            scheduler = config.scheduler\n        if config.permissions is not None or config.permissions == {}:\n            permissions = config.permissions\n        if config.roles is not None:\n            roles = config.roles\n        if config.opentsdb_url is not None:\n            opentsdb_url = config.opentsdb_url\n        if config.opentsdb_db is not None:\n            opentsdb_db = config.opentsdb_db\n        if config.opentsdb_username is not None:\n            opentsdb_username = config.opentsdb_username\n        if config.opentsdb_password is not None:\n            opentsdb_password = config.opentsdb_password\n        if config.minimum_peer_connectivity is not None:\n            minimum_peer_connectivity = config.minimum_peer_connectivity\n        if config.maximum_peer_connectivity is not None:\n            maximum_peer_connectivity = config.maximum_peer_connectivity\n        if config.state_pruning_block_depth is not None:\n            state_pruning_block_depth = config.state_pruning_block_depth\n        if config.fork_cache_keep_time is not None:\n            fork_cache_keep_time = config.fork_cache_keep_time\n        if config.component_thread_pool_workers is not None:\n            component_thread_pool_workers = \\\n                config.component_thread_pool_workers\n        if config.network_thread_pool_workers is not None:\n            network_thread_pool_workers = \\\n                config.network_thread_pool_workers\n        if config.signature_thread_pool_workers is not None:\n            signature_thread_pool_workers = \\\n                config.signature_thread_pool_workers\n\n    return ValidatorConfig(\n        bind_network=bind_network,\n        bind_component=bind_component,\n        bind_consensus=bind_consensus,\n        endpoint=endpoint,\n        peering=peering,\n        seeds=seeds,\n        peers=peers,\n        network_public_key=network_public_key,\n        network_private_key=network_private_key,\n        scheduler=scheduler,\n        permissions=permissions,\n        roles=roles,\n        opentsdb_url=opentsdb_url,\n        opentsdb_db=opentsdb_db,\n        opentsdb_username=opentsdb_username,\n        opentsdb_password=opentsdb_password,\n        minimum_peer_connectivity=minimum_peer_connectivity,\n        maximum_peer_connectivity=maximum_peer_connectivity,\n        state_pruning_block_depth=state_pruning_block_depth,\n        fork_cache_keep_time=fork_cache_keep_time,\n        component_thread_pool_workers=component_thread_pool_workers,\n        network_thread_pool_workers=network_thread_pool_workers,\n        signature_thread_pool_workers=signature_thread_pool_workers\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a cryptographic signer for the key_name from the key_dir.", "response": "def load_identity_signer(key_dir, key_name):\n    \"\"\"Loads a private key from the key directory, based on a validator's\n    identity.\n\n    Args:\n        key_dir (str): The path to the key directory.\n        key_name (str): The name of the key to load.\n\n    Returns:\n        Signer: the cryptographic signer for the key\n    \"\"\"\n    key_path = os.path.join(key_dir, '{}.priv'.format(key_name))\n\n    if not os.path.exists(key_path):\n        raise LocalConfigurationError(\n            \"No such signing key file: {}\".format(key_path))\n    if not os.access(key_path, os.R_OK):\n        raise LocalConfigurationError(\n            \"Key file is not readable: {}\".format(key_path))\n\n    LOGGER.info('Loading signing key: %s', key_path)\n    try:\n        with open(key_path, 'r') as key_file:\n            private_key_str = key_file.read().strip()\n    except IOError as e:\n        raise LocalConfigurationError(\n            \"Could not load key file: {}\".format(str(e)))\n\n    try:\n        private_key = Secp256k1PrivateKey.from_hex(private_key_str)\n    except signing.ParseError as e:\n        raise LocalConfigurationError(\n            \"Invalid key in file {}: {}\".format(key_path, str(e)))\n\n    context = signing.create_context('secp256k1')\n    crypto_factory = CryptoFactory(context)\n    return crypto_factory.new_signer(private_key)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_status_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'status',\n        help='Displays information about validator status',\n        description=\"Provides a subcommand to show a validator\\'s status\")\n\n    grand_parsers = parser.add_subparsers(title='subcommands',\n                                          dest='subcommand')\n    grand_parsers.required = True\n    add_status_show_parser(grand_parsers, parent_parser)", "response": "Adds a subparser for the status command"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_next_of_type(self, processor_type):\n        with self._condition:\n            if processor_type not in self:\n                self.wait_for_registration(processor_type)\n            try:\n                processor = self[processor_type].next_processor()\n            except NoProcessorVacancyError:\n                processor = self.wait_for_vacancy(processor_type)\n            processor.inc_occupancy()\n            return processor", "response": "Get the next available processor of a particular type and increment its occupancy counter."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef remove(self, processor_identity):\n        with self._condition:\n            processor_types = self._identities.get(processor_identity)\n            if processor_types is None:\n                LOGGER.warning(\"transaction processor with identity %s tried \"\n                               \"to unregister but was not registered\",\n                               processor_identity)\n                return\n            for processor_type in processor_types:\n                if processor_type not in self._processors:\n                    LOGGER.warning(\"processor type %s not a known processor \"\n                                   \"type but is associated with identity %s\",\n                                   processor_type,\n                                   processor_identity)\n                    continue\n                self._processors[processor_type].remove_processor(\n                    processor_identity=processor_identity)\n                if not self._processors[processor_type]:\n                    del self._processors[processor_type]", "response": "Removes all of the Processors for the given zeromq identity."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nwaits for a particular processor type to register or until is_cancelled is True.", "response": "def wait_for_registration(self, processor_type):\n        \"\"\"Waits for a particular processor type to register or until\n        is_cancelled is True. is_cancelled cannot be part of this class\n        since we aren't cancelling all waiting for a processor_type,\n        but just this particular wait.\n\n        Args:\n            processor_type (ProcessorType): The family, and version of\n                the transaction processor.\n\n        Returns:\n            None\n        \"\"\"\n        with self._condition:\n            self._condition.wait_for(lambda: (\n                processor_type in self\n                or self._cancelled_event.is_set()))\n            if self._cancelled_event.is_set():\n                raise WaitCancelledException()"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef wait_for_vacancy(self, processor_type):\n\n        with self._condition:\n            self._condition.wait_for(lambda: (\n                self._processor_available(processor_type)\n                or self._cancelled_event.is_set()))\n            if self._cancelled_event.is_set():\n                raise WaitCancelledException()\n            processor = self[processor_type].next_processor()\n            return processor", "response": "Waits for a particular processor type to have the capacity to\n            handle additional transactions or until is_cancelled is True."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget the value of a setting at the given key.", "response": "def _get_setting(self, key, default_value=None, value_type=str):\n        \"\"\"Get the setting stored at the given key.\n\n        Args:\n            key (str): the setting key\n            default_value (str, optional): The default value, if none is\n                found. Defaults to None.\n            value_type (function, optional): The type of a setting value.\n                Defaults to `str`.\n\n        Returns:\n            str: The value of the setting if found, default_value\n            otherwise.\n        \"\"\"\n        try:\n            state_entry = self._state_view.get(\n                SettingsView.setting_address(key))\n        except KeyError:\n            return default_value\n\n        if state_entry is not None:\n            setting = Setting()\n            setting.ParseFromString(state_entry)\n            for setting_entry in setting.entries:\n                if setting_entry.key == key:\n                    return value_type(setting_entry.value)\n\n        return default_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_setting_list(self,\n                         key,\n                         default_value=None,\n                         delimiter=',',\n                         value_type=str):\n        \"\"\"Get the setting stored at the given key and split it to a list.\n\n        Args:\n            key (str): the setting key\n            default_value (list, optional): The default value, if none is\n                found. Defaults to None.\n            delimiter (list of str, optional): The delimiter to break on.\n                Defaults to ','.\n            value_type (function, optional): The type of a setting value in the\n                list. Defaults to `str`.\n\n        Returns:\n            list of str: The values of the setting if found, default_value\n            otherwise.\n\n            If a value is found, it is split using the given delimiter.\n        \"\"\"\n        value = self.get_setting(key)\n        if value is not None:\n            setting_list = [value_type(v) for v in value.split(delimiter)]\n        else:\n            setting_list = default_value\n\n        return setting_list", "response": "Get the setting stored at the given key and split it to a list."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef setting_address(key):\n        # split the key into 4 parts, maximum\n        key_parts = key.split('.', maxsplit=_MAX_KEY_PARTS - 1)\n        # compute the short hash of each part\n        addr_parts = [_short_hash(x.encode()) for x in key_parts]\n        # pad the parts with the empty hash, if needed\n        addr_parts.extend([_EMPTY_PART] * (_MAX_KEY_PARTS - len(addr_parts)))\n\n        return CONFIG_STATE_NAMESPACE + ''.join(addr_parts)", "response": "Computes the radix address for the given setting key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def submit_batches(self, request):\n        timer_ctx = self._post_batches_total_time.time()\n        self._post_batches_count.inc()\n\n        # Parse request\n        if request.headers['Content-Type'] != 'application/octet-stream':\n            LOGGER.debug(\n                'Submission headers had wrong Content-Type: %s',\n                request.headers['Content-Type'])\n            self._post_batches_error.inc()\n            raise errors.SubmissionWrongContentType()\n\n        body = await request.read()\n        if not body:\n            LOGGER.debug('Submission contained an empty body')\n            self._post_batches_error.inc()\n            raise errors.NoBatchesSubmitted()\n\n        try:\n            batch_list = BatchList()\n            batch_list.ParseFromString(body)\n        except DecodeError:\n            LOGGER.debug('Submission body could not be decoded: %s', body)\n            self._post_batches_error.inc()\n            raise errors.BadProtobufSubmitted()\n\n        # Query validator\n        error_traps = [error_handlers.BatchInvalidTrap,\n                       error_handlers.BatchQueueFullTrap]\n        validator_query = client_batch_submit_pb2.ClientBatchSubmitRequest(\n            batches=batch_list.batches)\n\n        with self._post_batches_validator_time.time():\n            await self._query_validator(\n                Message.CLIENT_BATCH_SUBMIT_REQUEST,\n                client_batch_submit_pb2.ClientBatchSubmitResponse,\n                validator_query,\n                error_traps)\n\n        # Build response envelope\n        id_string = ','.join(b.header_signature for b in batch_list.batches)\n\n        status = 202\n        link = self._build_url(request, path='/batch_statuses', id=id_string)\n\n        retval = self._wrap_response(\n            request,\n            metadata={'link': link},\n            status=status)\n\n        timer_ctx.stop()\n        return retval", "response": "Submits a batch list to the validator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfetching the committed status of batches by either a POST or GET.", "response": "async def list_statuses(self, request):\n        \"\"\"Fetches the committed status of batches by either a POST or GET.\n\n        Request:\n            body: A JSON array of one or more id strings (if POST)\n            query:\n                - id: A comma separated list of up to 15 ids (if GET)\n                - wait: Request should not return until all batches committed\n\n        Response:\n            data: A JSON object, with batch ids as keys, and statuses as values\n            link: The /batch_statuses link queried (if GET)\n        \"\"\"\n        error_traps = [error_handlers.StatusResponseMissing]\n\n        # Parse batch ids from POST body, or query paramaters\n        if request.method == 'POST':\n            if request.headers['Content-Type'] != 'application/json':\n                LOGGER.debug(\n                    'Request headers had wrong Content-Type: %s',\n                    request.headers['Content-Type'])\n                raise errors.StatusWrongContentType()\n\n            ids = await request.json()\n\n            if (not ids\n                    or not isinstance(ids, list)\n                    or not all(isinstance(i, str) for i in ids)):\n                LOGGER.debug('Request body was invalid: %s', ids)\n                raise errors.StatusBodyInvalid()\n            for i in ids:\n                self._validate_id(i)\n\n        else:\n            ids = self._get_filter_ids(request)\n            if not ids:\n                LOGGER.debug('Request for statuses missing id query')\n                raise errors.StatusIdQueryInvalid()\n\n        # Query validator\n        validator_query = \\\n            client_batch_submit_pb2.ClientBatchStatusRequest(\n                batch_ids=ids)\n        self._set_wait(request, validator_query)\n\n        response = await self._query_validator(\n            Message.CLIENT_BATCH_STATUS_REQUEST,\n            client_batch_submit_pb2.ClientBatchStatusResponse,\n            validator_query,\n            error_traps)\n\n        # Send response\n        if request.method != 'POST':\n            metadata = self._get_metadata(request, response)\n        else:\n            metadata = None\n\n        data = self._drop_id_prefixes(\n            self._drop_empty_props(response['batch_statuses']))\n\n        return self._wrap_response(request, data=data, metadata=metadata)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nfetching list of data entries optionally filtered by address prefix.", "response": "async def list_state(self, request):\n        \"\"\"Fetches list of data entries, optionally filtered by address prefix.\n\n        Request:\n            query:\n                - head: The id of the block to use as the head of the chain\n                - address: Return entries whose addresses begin with this\n                prefix\n\n        Response:\n            data: An array of leaf objects with address and data keys\n            head: The head used for this query (most recent if unspecified)\n            link: The link to this exact query, including head block\n            paging: Paging info and nav, like total resources and a next link\n        \"\"\"\n        paging_controls = self._get_paging_controls(request)\n\n        head, root = await self._head_to_root(request.url.query.get(\n            'head', None))\n        validator_query = client_state_pb2.ClientStateListRequest(\n            state_root=root,\n            address=request.url.query.get('address', None),\n            sorting=self._get_sorting_message(request, \"default\"),\n            paging=self._make_paging_message(paging_controls))\n\n        response = await self._query_validator(\n            Message.CLIENT_STATE_LIST_REQUEST,\n            client_state_pb2.ClientStateListResponse,\n            validator_query)\n\n        return self._wrap_paginated_response(\n            request=request,\n            response=response,\n            controls=paging_controls,\n            data=response.get('entries', []),\n            head=head)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\nasync def fetch_state(self, request):\n        error_traps = [\n            error_handlers.InvalidAddressTrap,\n            error_handlers.StateNotFoundTrap]\n\n        address = request.match_info.get('address', '')\n        head = request.url.query.get('head', None)\n\n        head, root = await self._head_to_root(head)\n        response = await self._query_validator(\n            Message.CLIENT_STATE_GET_REQUEST,\n            client_state_pb2.ClientStateGetResponse,\n            client_state_pb2.ClientStateGetRequest(\n                state_root=root, address=address),\n            error_traps)\n\n        return self._wrap_response(\n            request,\n            data=response['value'],\n            metadata=self._get_metadata(request, response, head=head))", "response": "Fetches data from a specific address in the validator s state tree."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch list of blocks from validator optionally filtered by id.", "response": "async def list_blocks(self, request):\n        \"\"\"Fetches list of blocks from validator, optionally filtered by id.\n\n        Request:\n            query:\n                - head: The id of the block to use as the head of the chain\n                - id: Comma separated list of block ids to include in results\n\n        Response:\n            data: JSON array of fully expanded Block objects\n            head: The head used for this query (most recent if unspecified)\n            link: The link to this exact query, including head block\n            paging: Paging info and nav, like total resources and a next link\n        \"\"\"\n        paging_controls = self._get_paging_controls(request)\n        validator_query = client_block_pb2.ClientBlockListRequest(\n            head_id=self._get_head_id(request),\n            block_ids=self._get_filter_ids(request),\n            sorting=self._get_sorting_message(request, \"block_num\"),\n            paging=self._make_paging_message(paging_controls))\n\n        response = await self._query_validator(\n            Message.CLIENT_BLOCK_LIST_REQUEST,\n            client_block_pb2.ClientBlockListResponse,\n            validator_query)\n\n        return self._wrap_paginated_response(\n            request=request,\n            response=response,\n            controls=paging_controls,\n            data=[self._expand_block(b) for b in response['blocks']])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nfetch a specific block from the validator specified by id.", "response": "async def fetch_block(self, request):\n        \"\"\"Fetches a specific block from the validator, specified by id.\n        Request:\n            path:\n                - block_id: The 128-character id of the block to be fetched\n\n        Response:\n            data: A JSON object with the data from the fully expanded Block\n            link: The link to this exact query\n        \"\"\"\n        error_traps = [error_handlers.BlockNotFoundTrap]\n\n        block_id = request.match_info.get('block_id', '')\n        self._validate_id(block_id)\n\n        response = await self._query_validator(\n            Message.CLIENT_BLOCK_GET_BY_ID_REQUEST,\n            client_block_pb2.ClientBlockGetResponse,\n            client_block_pb2.ClientBlockGetByIdRequest(block_id=block_id),\n            error_traps)\n\n        return self._wrap_response(\n            request,\n            data=self._expand_block(response['block']),\n            metadata=self._get_metadata(request, response))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def list_batches(self, request):\n        paging_controls = self._get_paging_controls(request)\n        validator_query = client_batch_pb2.ClientBatchListRequest(\n            head_id=self._get_head_id(request),\n            batch_ids=self._get_filter_ids(request),\n            sorting=self._get_sorting_message(request, \"default\"),\n            paging=self._make_paging_message(paging_controls))\n\n        response = await self._query_validator(\n            Message.CLIENT_BATCH_LIST_REQUEST,\n            client_batch_pb2.ClientBatchListResponse,\n            validator_query)\n\n        return self._wrap_paginated_response(\n            request=request,\n            response=response,\n            controls=paging_controls,\n            data=[self._expand_batch(b) for b in response['batches']])", "response": "Fetches list of batches from validator optionally filtered by id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def fetch_batch(self, request):\n        error_traps = [error_handlers.BatchNotFoundTrap]\n\n        batch_id = request.match_info.get('batch_id', '')\n        self._validate_id(batch_id)\n\n        response = await self._query_validator(\n            Message.CLIENT_BATCH_GET_REQUEST,\n            client_batch_pb2.ClientBatchGetResponse,\n            client_batch_pb2.ClientBatchGetRequest(batch_id=batch_id),\n            error_traps)\n\n        return self._wrap_response(\n            request,\n            data=self._expand_batch(response['batch']),\n            metadata=self._get_metadata(request, response))", "response": "Fetches a specific batch from the validator specified by id."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\nasync def list_transactions(self, request):\n        paging_controls = self._get_paging_controls(request)\n        validator_query = client_transaction_pb2.ClientTransactionListRequest(\n            head_id=self._get_head_id(request),\n            transaction_ids=self._get_filter_ids(request),\n            sorting=self._get_sorting_message(request, \"default\"),\n            paging=self._make_paging_message(paging_controls))\n\n        response = await self._query_validator(\n            Message.CLIENT_TRANSACTION_LIST_REQUEST,\n            client_transaction_pb2.ClientTransactionListResponse,\n            validator_query)\n\n        data = [self._expand_transaction(t) for t in response['transactions']]\n\n        return self._wrap_paginated_response(\n            request=request,\n            response=response,\n            controls=paging_controls,\n            data=data)", "response": "Fetches list of txns from validator optionally filtered by id."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def fetch_transaction(self, request):\n        error_traps = [error_handlers.TransactionNotFoundTrap]\n\n        txn_id = request.match_info.get('transaction_id', '')\n        self._validate_id(txn_id)\n\n        response = await self._query_validator(\n            Message.CLIENT_TRANSACTION_GET_REQUEST,\n            client_transaction_pb2.ClientTransactionGetResponse,\n            client_transaction_pb2.ClientTransactionGetRequest(\n                transaction_id=txn_id),\n            error_traps)\n\n        return self._wrap_response(\n            request,\n            data=self._expand_transaction(response['transaction']),\n            metadata=self._get_metadata(request, response))", "response": "Fetches a specific transaction from the validator specified by id."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\nasync def list_receipts(self, request):\n        error_traps = [error_handlers.ReceiptNotFoundTrap]\n\n        # Parse transaction ids from POST body, or query paramaters\n        if request.method == 'POST':\n            if request.headers['Content-Type'] != 'application/json':\n                LOGGER.debug(\n                    'Request headers had wrong Content-Type: %s',\n                    request.headers['Content-Type'])\n                raise errors.ReceiptWrongContentType()\n\n            ids = await request.json()\n\n            if (not ids\n                    or not isinstance(ids, list)\n                    or not all(isinstance(i, str) for i in ids)):\n                LOGGER.debug('Request body was invalid: %s', ids)\n                raise errors.ReceiptBodyInvalid()\n            for i in ids:\n                self._validate_id(i)\n\n        else:\n            ids = self._get_filter_ids(request)\n            if not ids:\n                LOGGER.debug('Request for receipts missing id query')\n                raise errors.ReceiptIdQueryInvalid()\n\n        # Query validator\n        validator_query = \\\n            client_receipt_pb2.ClientReceiptGetRequest(\n                transaction_ids=ids)\n        self._set_wait(request, validator_query)\n\n        response = await self._query_validator(\n            Message.CLIENT_RECEIPT_GET_REQUEST,\n            client_receipt_pb2.ClientReceiptGetResponse,\n            validator_query,\n            error_traps)\n\n        # Send response\n        if request.method != 'POST':\n            metadata = self._get_metadata(request, response)\n        else:\n            metadata = None\n\n        data = self._drop_id_prefixes(\n            self._drop_empty_props(response['receipts']))\n\n        return self._wrap_response(request, data=data, metadata=metadata)", "response": "Fetches the receipts for a single item in the database."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nfetching the peers from the validator.", "response": "async def fetch_peers(self, request):\n        \"\"\"Fetches the peers from the validator.\n        Request:\n\n        Response:\n            data: JSON array of peer endpoints\n            link: The link to this exact query\n        \"\"\"\n\n        response = await self._query_validator(\n            Message.CLIENT_PEERS_GET_REQUEST,\n            client_peers_pb2.ClientPeersGetResponse,\n            client_peers_pb2.ClientPeersGetRequest())\n\n        return self._wrap_response(\n            request,\n            data=response['peers'],\n            metadata=self._get_metadata(request, response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfetch information pertaining to the valiator s status.", "response": "async def fetch_status(self, request):\n        '''Fetches information pertaining to the valiator's status.'''\n\n        response = await self._query_validator(\n            Message.CLIENT_STATUS_GET_REQUEST,\n            client_status_pb2.ClientStatusGetResponse,\n            client_status_pb2.ClientStatusGetRequest())\n\n        return self._wrap_response(\n            request,\n            data={\n                'peers': response['peers'],\n                'endpoint': response['endpoint']\n            },\n            metadata=self._get_metadata(request, response))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a request to the validator and parses the response.", "response": "async def _query_validator(self, request_type, response_proto,\n                               payload, error_traps=None):\n        \"\"\"Sends a request to the validator and parses the response.\n        \"\"\"\n        LOGGER.debug(\n            'Sending %s request to validator',\n            self._get_type_name(request_type))\n\n        payload_bytes = payload.SerializeToString()\n        response = await self._send_request(request_type, payload_bytes)\n        content = self._parse_response(response_proto, response)\n\n        LOGGER.debug(\n            'Received %s response from validator with status %s',\n            self._get_type_name(response.message_type),\n            self._get_status_name(response_proto, content.status))\n\n        self._check_status_errors(response_proto, content, error_traps)\n        return self._message_to_dict(content)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending a request to the ZMQ connection with the handler s Connection", "response": "async def _send_request(self, request_type, payload):\n        \"\"\"Uses an executor to send an asynchronous ZMQ request to the\n        validator with the handler's Connection\n        \"\"\"\n        try:\n            return await self._connection.send(\n                message_type=request_type,\n                message_content=payload,\n                timeout=self._timeout)\n        except DisconnectError:\n            LOGGER.warning('Validator disconnected while waiting for response')\n            raise errors.ValidatorDisconnected()\n        except asyncio.TimeoutError:\n            LOGGER.warning('Timed out while waiting for validator response')\n            raise errors.ValidatorTimedOut()\n        except SendBackoffTimeoutError:\n            LOGGER.warning('Failed sending message - Backoff timed out')\n            raise errors.SendBackoffTimeout()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nparse the content from a validator response Message.", "response": "def _parse_response(proto, response):\n        \"\"\"Parses the content from a validator response Message.\n        \"\"\"\n        try:\n            content = proto()\n            content.ParseFromString(response.content)\n            return content\n        except (DecodeError, AttributeError):\n            LOGGER.error('Validator response was not parsable: %s', response)\n            raise errors.ValidatorResponseInvalid()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _check_status_errors(proto, content, error_traps=None):\n        if content.status == proto.OK:\n            return\n\n        try:\n            if content.status == proto.INTERNAL_ERROR:\n                raise errors.UnknownValidatorError()\n        except AttributeError:\n            # Not every protobuf has every status enum, so pass AttributeErrors\n            pass\n\n        try:\n            if content.status == proto.NOT_READY:\n                raise errors.ValidatorNotReady()\n        except AttributeError:\n            pass\n\n        try:\n            if content.status == proto.NO_ROOT:\n                raise errors.HeadNotFound()\n        except AttributeError:\n            pass\n\n        try:\n            if content.status == proto.INVALID_PAGING:\n                raise errors.PagingInvalid()\n        except AttributeError:\n            pass\n\n        try:\n            if content.status == proto.INVALID_SORT:\n                raise errors.SortInvalid()\n        except AttributeError:\n            pass\n\n        # Check custom error traps from the particular route message\n        if error_traps is not None:\n            for trap in error_traps:\n                trap.check(content.status)", "response": "Raises HTTPErrors based on error statuses sent from validator."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncreate the JSON response envelope to be sent back to the client.", "response": "def _wrap_response(request, data=None, metadata=None, status=200):\n        \"\"\"Creates the JSON response envelope to be sent back to the client.\n        \"\"\"\n        envelope = metadata or {}\n\n        if data is not None:\n            envelope['data'] = data\n\n        return web.Response(\n            status=status,\n            content_type='application/json',\n            text=json.dumps(\n                envelope,\n                indent=2,\n                separators=(',', ': '),\n                sort_keys=True))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _wrap_paginated_response(cls, request, response, controls, data,\n                                 head=None):\n        \"\"\"Builds the metadata for a pagingated response and wraps everying in\n        a JSON encoded web.Response\n        \"\"\"\n        paging_response = response['paging']\n        if head is None:\n            head = response['head_id']\n        link = cls._build_url(\n            request,\n            head=head,\n            start=paging_response['start'],\n            limit=paging_response['limit'])\n\n        paging = {}\n        limit = controls.get('limit')\n        start = controls.get(\"start\")\n        paging[\"limit\"] = limit\n        paging[\"start\"] = start\n        # If there are no resources, there should be nothing else in paging\n        if paging_response.get(\"next\") == \"\":\n            return cls._wrap_response(\n                request,\n                data=data,\n                metadata={\n                    'head': head,\n                    'link': link,\n                    'paging': paging\n                })\n\n        next_id = paging_response['next']\n        paging['next_position'] = next_id\n\n        # Builds paging urls specific to this response\n        def build_pg_url(start=None):\n            return cls._build_url(request, head=head, limit=limit, start=start)\n\n        paging['next'] = build_pg_url(paging_response['next'])\n\n        return cls._wrap_response(\n            request,\n            data=data,\n            metadata={\n                'head': head,\n                'link': link,\n                'paging': paging\n            })", "response": "Builds the metadata for a pagingated response and wraps everying in\n        a JSON encoded web. Response object"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _get_metadata(cls, request, response, head=None):\n        head = response.get('head_id', head)\n        metadata = {'link': cls._build_url(request, head=head)}\n\n        if head is not None:\n            metadata['head'] = head\n        return metadata", "response": "Parses out the head and link properties based on the HTTP Request\n        from the client and the Protobuf response from the validator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _build_url(cls, request, path=None, **changes):\n        changes = {k: v for k, v in changes.items() if v is not None}\n        queries = {**request.url.query, **changes}\n        queries = {k: v for k, v in queries.items() if v is not False}\n        query_strings = []\n\n        def add_query(key):\n            query_strings.append('{}={}'.format(key, queries[key])\n                                 if queries[key] != '' else key)\n\n        def del_query(key):\n            queries.pop(key, None)\n\n        if 'head' in queries:\n            add_query('head')\n            del_query('head')\n\n        if 'start' in changes:\n            add_query('start')\n        elif 'start' in queries:\n            add_query('start')\n\n        del_query('start')\n\n        if 'limit' in queries:\n            add_query('limit')\n            del_query('limit')\n\n        for key in sorted(queries):\n            add_query(key)\n\n        scheme = cls._get_forwarded(request, 'proto') or request.url.scheme\n        host = cls._get_forwarded(request, 'host') or request.host\n        forwarded_path = cls._get_forwarded(request, 'path')\n        path = path if path is not None else request.path\n        query = '?' + '&'.join(query_strings) if query_strings else ''\n\n        url = '{}://{}{}{}{}'.format(scheme, host, forwarded_path, path, query)\n        return url", "response": "Builds a response URL by overriding the original queries with\n        specified change queries."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngetting a forwarded value from the Forwarded header if present or the equivalent X - Forwarded - header if not present.", "response": "def _get_forwarded(request, key):\n        \"\"\"Gets a forwarded value from the `Forwarded` header if present, or\n        the equivalent `X-Forwarded-` header if not. If neither is present,\n        returns an empty string.\n        \"\"\"\n        forwarded = request.headers.get('Forwarded', '')\n        match = re.search(\n            r'(?<={}=).+?(?=[\\s,;]|$)'.format(key),\n            forwarded,\n            re.IGNORECASE)\n\n        if match is not None:\n            header = match.group(0)\n\n            if header[0] == '\"' and header[-1] == '\"':\n                return header[1:-1]\n\n            return header\n\n        return request.headers.get('X-Forwarded-{}'.format(key.title()), '')"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _expand_block(cls, block):\n        cls._parse_header(BlockHeader, block)\n        if 'batches' in block:\n            block['batches'] = [cls._expand_batch(b) for b in block['batches']]\n        return block", "response": "Deserializes a Block s header and the header of its Batches."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _parse_header(cls, header_proto, resource):\n        header = header_proto()\n        try:\n            header_bytes = base64.b64decode(resource['header'])\n            header.ParseFromString(header_bytes)\n        except (KeyError, TypeError, ValueError, DecodeError):\n            header = resource.get('header', None)\n            LOGGER.error(\n                'The validator sent a resource with %s %s',\n                'a missing header' if header is None else 'an invalid header:',\n                header or '')\n            raise errors.ResourceHeaderInvalid()\n\n        resource['header'] = cls._message_to_dict(header)\n        return resource", "response": "Deserializes a resource s base64 encoded Protobuf header."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nparses start and limit queries into a paging controls dict.", "response": "def _get_paging_controls(request):\n        \"\"\"Parses start and/or limit queries into a paging controls dict.\n        \"\"\"\n        start = request.url.query.get('start', None)\n        limit = request.url.query.get('limit', None)\n        controls = {}\n\n        if limit is not None:\n            try:\n                controls['limit'] = int(limit)\n            except ValueError:\n                LOGGER.debug('Request query had an invalid limit: %s', limit)\n                raise errors.CountInvalid()\n\n            if controls['limit'] <= 0:\n                LOGGER.debug('Request query had an invalid limit: %s', limit)\n                raise errors.CountInvalid()\n\n        if start is not None:\n            controls['start'] = start\n\n        return controls"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nturn a raw paging controls dict into Protobuf ClientPagingControls.", "response": "def _make_paging_message(controls):\n        \"\"\"Turns a raw paging controls dict into Protobuf ClientPagingControls.\n        \"\"\"\n\n        return client_list_control_pb2.ClientPagingControls(\n            start=controls.get('start', None),\n            limit=controls.get('limit', None))"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nparses the reverse query into a list of ClientSortControls protobuf messages.", "response": "def _get_sorting_message(request, key):\n        \"\"\"Parses the reverse query into a list of ClientSortControls protobuf\n        messages.\n        \"\"\"\n        control_list = []\n        reverse = request.url.query.get('reverse', None)\n        if reverse is None:\n            return control_list\n\n        if reverse.lower() == \"\":\n            control_list.append(client_list_control_pb2.ClientSortControls(\n                reverse=True,\n                keys=key.split(\",\")\n            ))\n        elif reverse.lower() != 'false':\n            control_list.append(client_list_control_pb2.ClientSortControls(\n                reverse=True,\n                keys=reverse.split(\",\")\n            ))\n\n        return control_list"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _set_wait(self, request, validator_query):\n        wait = request.url.query.get('wait', 'false')\n        if wait.lower() != 'false':\n            validator_query.wait = True\n            try:\n                validator_query.timeout = int(wait)\n            except ValueError:\n                # By default, waits for 95% of REST API's configured timeout\n                validator_query.timeout = int(self._timeout * 0.95)", "response": "Parses the wait query parameter and sets the corresponding\n        wait and timeout properties in the validator query."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _drop_empty_props(self, item):\n        if isinstance(item, list):\n            return [self._drop_empty_props(i) for i in item]\n        if isinstance(item, dict):\n            return {\n                k: self._drop_empty_props(v)\n                for k, v in item.items() if v != ''\n            }\n        return item", "response": "Remove empty strings from nested dicts."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _drop_id_prefixes(self, item):\n        if isinstance(item, list):\n            return [self._drop_id_prefixes(i) for i in item]\n        if isinstance(item, dict):\n            return {\n                'id' if k.endswith('id') else k: self._drop_id_prefixes(v)\n                for k, v in item.items()\n            }\n        return item", "response": "Drop id prefixes from nested dicts."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch the head query and validates if present.", "response": "def _get_head_id(cls, request):\n        \"\"\"Fetches the request's head query, and validates if present.\n        \"\"\"\n        head_id = request.url.query.get('head', None)\n\n        if head_id is not None:\n            cls._validate_id(head_id)\n\n        return head_id"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_filter_ids(cls, request):\n        id_query = request.url.query.get('id', None)\n\n        if id_query is None:\n            return None\n\n        filter_ids = id_query.split(',')\n        for filter_id in filter_ids:\n            cls._validate_id(filter_id)\n\n        return filter_ids", "response": "Parses the id filter paramter from the url query."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef create_view(self, state_root_hash=None):\n        # Create a default Merkle database and if we have a state root hash,\n        # update the Merkle database's root to that\n        if state_root_hash is None:\n            state_root_hash = INIT_ROOT_KEY\n\n        merkle_db = MerkleDatabase(self._database,\n                                   merkle_root=state_root_hash)\n\n        return StateView(merkle_db)", "response": "Creates a StateView for the given state root hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef add_subscriber(self, connection_id, subscriptions,\n                       last_known_block_id):\n        \"\"\"Register the subscriber for the given event subscriptions.\n\n        Raises:\n            InvalidFilterError\n                One of the filters in the subscriptions is invalid.\n        \"\"\"\n        with self._subscribers_cv:\n            self._subscribers[connection_id] = \\\n                EventSubscriber(\n                    connection_id, subscriptions, last_known_block_id)\n\n        LOGGER.debug(\n            'Added Subscriber %s for %s', connection_id, subscriptions)", "response": "Register the given event subscriptions for the given connection."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef catchup_subscriber(self, connection_id):\n        with self._subscribers_cv:\n            subscriber = self._subscribers[connection_id]\n            last_known_block_id = subscriber.get_last_known_block_id()\n            subscriptions = subscriber.subscriptions\n\n        if last_known_block_id is not None:\n            LOGGER.debug(\n                'Catching up Subscriber %s from %s',\n                connection_id, last_known_block_id)\n\n            # Send catchup events one block at a time\n            for block_id in self.get_catchup_block_ids(last_known_block_id):\n                events = self.get_events_for_block_id(block_id, subscriptions)\n                event_list = EventList(events=events)\n                self._send(connection_id, event_list.SerializeToString())", "response": "Catchup a subscriber from a given last known block id."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a list of catchup block identifiers that are not yet in the chain head.", "response": "def get_catchup_block_ids(self, last_known_block_id):\n        '''\n        Raises:\n            PossibleForkDetectedError\n        '''\n        # If latest known block is not the current chain head, catch up\n        catchup_up_blocks = []\n        chain_head = self._block_store.chain_head\n        if chain_head and last_known_block_id != chain_head.identifier:\n            # Start from the chain head and get blocks until we reach the\n            # known block\n            for block in self._block_store.get_predecessor_iter():\n                # All the blocks if NULL_BLOCK_IDENTIFIER\n                if last_known_block_id != NULL_BLOCK_IDENTIFIER:\n                    if block.identifier == last_known_block_id:\n                        break\n                catchup_up_blocks.append(block.identifier)\n\n        return list(reversed(catchup_up_blocks))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of events associated with all the block ids.", "response": "def get_events_for_block_ids(self, block_ids, subscriptions):\n        \"\"\"Get a list of events associated with all the block ids.\n\n        Args:\n            block_ids (list of str): The block ids to search for events that\n                match each subscription.\n            subscriptions (list of EventSubscriptions): EventFilter and\n                event type to filter events.\n\n        Returns (list of Events): The Events associated which each block id.\n\n        Raises:\n            KeyError\n                A block id isn't found within the block store or a transaction\n                is missing from the receipt store.\n        \"\"\"\n\n        blocks = [self._block_store[block_id] for block_id in block_ids]\n        return self.get_events_for_blocks(blocks, subscriptions)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget a list of events associated with all the blocks.", "response": "def get_events_for_blocks(self, blocks, subscriptions):\n        \"\"\"Get a list of events associated with all the blocks.\n\n        Args:\n            blocks (list of BlockWrapper): The blocks to search for events that\n                match each subscription.\n            subscriptions (list of EventSubscriptions): EventFilter and\n                event type to filter events.\n\n        Returns (list of Events): The Events associated which each block id.\n\n        Raises:\n            KeyError A receipt is missing from the receipt store.\n        \"\"\"\n\n        events = []\n        for blkw in blocks:\n            events.extend(self.get_events_for_block(blkw, subscriptions))\n        return events"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning a module by name.", "response": "def get_consensus_module(module_name):\n        \"\"\"Returns a consensus module by name.\n\n        Args:\n            module_name (str): The name of the module to load.\n\n        Returns:\n            module: The consensus module.\n\n        Raises:\n            UnknownConsensusModuleError: Raised if the given module_name does\n                not correspond to a consensus implementation.\n        \"\"\"\n        module_package = module_name\n        if module_name == 'genesis':\n            module_package = (\n                'sawtooth_validator.journal.consensus.genesis.'\n                'genesis_consensus'\n            )\n        elif module_name == 'devmode':\n            module_package = (\n                'sawtooth_validator.journal.consensus.dev_mode.'\n                'dev_mode_consensus'\n            )\n\n        try:\n            return importlib.import_module(module_package)\n        except ImportError:\n            raise UnknownConsensusModuleError(\n                'Consensus module \"{}\" does not exist.'.format(module_name))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning the consensus module name based on the current state view and the block id.", "response": "def get_configured_consensus_module(block_id, state_view):\n        \"\"\"Returns the consensus_module based on the consensus module set by\n        the \"sawtooth_settings\" transaction family.\n\n        Args:\n            block_id (str): the block id associated with the current state_view\n            state_view (:obj:`StateView`): the current state view to use for\n                setting values\n        Raises:\n            UnknownConsensusModuleError: Thrown when an invalid consensus\n                module has been configured.\n        \"\"\"\n        settings_view = SettingsView(state_view)\n\n        default_consensus = \\\n            'genesis' if block_id == NULL_BLOCK_IDENTIFIER else 'devmode'\n        consensus_module_name = settings_view.get_setting(\n            'sawtooth.consensus.algorithm', default_value=default_consensus)\n        return ConsensusFactory.get_consensus_module(\n            consensus_module_name)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef parse_args(args):\n    parser = argparse.ArgumentParser(\n        description='Starts the REST API application and connects to a '\n        'specified validator.')\n\n    parser.add_argument('-B', '--bind',\n                        help='identify host and port for API to run on \\\n                        default: http://localhost:8008)',\n                        action='append')\n    parser.add_argument('-C', '--connect',\n                        help='specify URL to connect to a running validator')\n    parser.add_argument('-t', '--timeout',\n                        help='set time (in seconds) to wait for validator \\\n                        response')\n    parser.add_argument('--client-max-size',\n                        type=int,\n                        help='the max size (in bytes) of a request body')\n    parser.add_argument('-v', '--verbose',\n                        action='count',\n                        default=0,\n                        help='enable more verbose output to stderr')\n    parser.add_argument('--opentsdb-url',\n                        help='specify host and port for Open TSDB database \\\n                        used for metrics')\n    parser.add_argument('--opentsdb-db',\n                        help='specify name of database for storing metrics')\n\n    try:\n        version = pkg_resources.get_distribution(DISTRIBUTION_NAME).version\n    except pkg_resources.DistributionNotFound:\n        version = 'UNKNOWN'\n\n    parser.add_argument(\n        '-V', '--version',\n        action='version',\n        version=(DISTRIBUTION_NAME + ' (Hyperledger Sawtooth) version {}')\n        .format(version),\n        help='display version information')\n\n    return parser.parse_args(args)", "response": "Parse command line flags added to rest_api command."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef start_rest_api(host, port, connection, timeout, registry,\n                   client_max_size=None):\n    \"\"\"Builds the web app, adds route handlers, and finally starts the app.\n    \"\"\"\n    loop = asyncio.get_event_loop()\n    connection.open()\n    app = web.Application(loop=loop, client_max_size=client_max_size)\n    app.on_cleanup.append(lambda app: connection.close())\n\n    # Add routes to the web app\n    LOGGER.info('Creating handlers for validator at %s', connection.url)\n\n    handler = RouteHandler(loop, connection, timeout, registry)\n\n    app.router.add_post('/batches', handler.submit_batches)\n    app.router.add_get('/batch_statuses', handler.list_statuses)\n    app.router.add_post('/batch_statuses', handler.list_statuses)\n\n    app.router.add_get('/state', handler.list_state)\n    app.router.add_get('/state/{address}', handler.fetch_state)\n\n    app.router.add_get('/blocks', handler.list_blocks)\n    app.router.add_get('/blocks/{block_id}', handler.fetch_block)\n\n    app.router.add_get('/batches', handler.list_batches)\n    app.router.add_get('/batches/{batch_id}', handler.fetch_batch)\n\n    app.router.add_get('/transactions', handler.list_transactions)\n    app.router.add_get(\n        '/transactions/{transaction_id}',\n        handler.fetch_transaction)\n\n    app.router.add_get('/receipts', handler.list_receipts)\n    app.router.add_post('/receipts', handler.list_receipts)\n\n    app.router.add_get('/peers', handler.fetch_peers)\n    app.router.add_get('/status', handler.fetch_status)\n\n    subscriber_handler = StateDeltaSubscriberHandler(connection)\n    app.router.add_get('/subscriptions', subscriber_handler.subscriptions)\n    app.on_shutdown.append(lambda app: subscriber_handler.on_shutdown())\n\n    # Start app\n    LOGGER.info('Starting REST API on %s:%s', host, port)\n\n    web.run_app(\n        app,\n        host=host,\n        port=port,\n        access_log=LOGGER,\n        access_log_format='%r: %s status, %b size, in %Tf s')", "response": "Starts the REST API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_genesis_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'genesis',\n        help='Creates the genesis.batch file for initializing the validator',\n        description='Generates the genesis.batch file for '\n        'initializing the validator.',\n        epilog='This command generates a serialized GenesisData protobuf '\n        'message and stores it in the genesis.batch file. One or more input '\n        'files (optional) can contain serialized BatchList protobuf messages '\n        'to add to the GenesisData. The output shows the location of this '\n        'file. By default, the genesis.batch file is stored in '\n        '/var/lib/sawtooth. If $SAWTOOTH_HOME is set, the location is '\n        '$SAWTOOTH_HOME/data/genesis.batch. Use the --output option to change '\n        'the name of the file.',\n        parents=[parent_parser])\n\n    parser.add_argument(\n        '-o', '--output',\n        type=str,\n        help='choose the output file for GenesisData')\n\n    parser.add_argument(\n        'input_file',\n        nargs='*',\n        type=str,\n        help='file or files containing batches to add to the resulting '\n        'GenesisData')", "response": "Adds the arg parsers needed for the genesis command."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ngives the command args, take an series of input files containing GenesisData, combine all the batches into one GenesisData, and output the result into a new file.", "response": "def do_genesis(args, data_dir=None):\n    \"\"\"Given the command args, take an series of input files containing\n    GenesisData, combine all the batches into one GenesisData, and output the\n    result into a new file.\n    \"\"\"\n\n    if data_dir is None:\n        data_dir = get_data_dir()\n\n    if not os.path.exists(data_dir):\n        raise CliException(\n            \"Data directory does not exist: {}\".format(data_dir))\n\n    genesis_batches = []\n    for input_file in args.input_file:\n        print('Processing {}...'.format(input_file))\n        input_data = BatchList()\n        try:\n            with open(input_file, 'rb') as in_file:\n                input_data.ParseFromString(in_file.read())\n        except:\n            raise CliException('Unable to read {}'.format(input_file))\n\n        genesis_batches += input_data.batches\n\n    _validate_depedencies(genesis_batches)\n    _check_required_settings(genesis_batches)\n\n    if args.output:\n        genesis_file = args.output\n    else:\n        genesis_file = os.path.join(data_dir, 'genesis.batch')\n\n    print('Generating {}'.format(genesis_file))\n    output_data = GenesisData(batches=genesis_batches)\n    with open(genesis_file, 'wb') as out_file:\n        out_file.write(output_data.SerializeToString())"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nvalidate the transaction dependencies for the transactions contained within the batches.", "response": "def _validate_depedencies(batches):\n    \"\"\"Validates the transaction dependencies for the transactions contained\n    within the sequence of batches. Given that all the batches are expected to\n    to be executed for the genesis blocks, it is assumed that any dependent\n    transaction will proceed the depending transaction.\n    \"\"\"\n    transaction_ids = set()\n    for batch in batches:\n        for txn in batch.transactions:\n            txn_header = TransactionHeader()\n            txn_header.ParseFromString(txn.header)\n\n            if txn_header.dependencies:\n                unsatisfied_deps = [\n                    id for id in txn_header.dependencies\n                    if id not in transaction_ids\n                ]\n                if unsatisfied_deps:\n                    raise CliException(\n                        'Unsatisfied dependency in given transactions:'\n                        ' {}'.format(unsatisfied_deps))\n\n            transaction_ids.add(txn.header_signature)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _check_required_settings(batches):\n    required_settings = [\n        'sawtooth.consensus.algorithm.name',\n        'sawtooth.consensus.algorithm.version']\n\n    for batch in batches:\n        for txn in batch.transactions:\n            txn_header = TransactionHeader()\n            txn_header.ParseFromString(txn.header)\n            if txn_header.family_name == 'sawtooth_settings':\n                settings_payload = SettingsPayload()\n                settings_payload.ParseFromString(txn.payload)\n                if settings_payload.action == SettingsPayload.PROPOSE:\n                    proposal = SettingProposal()\n                    proposal.ParseFromString(settings_payload.data)\n                    if proposal.setting in required_settings:\n                        required_settings.remove(proposal.setting)\n\n    if required_settings:\n        raise CliException(\n            'The following setting(s) are required at genesis, but were not '\n            'included in the genesis batches: {}'.format(required_settings))", "response": "Ensure that all settings required at genesis are set."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get(self, key, index=None):\n        records = self.get_multi([key], index=index)\n\n        try:\n            return records[0][1]  # return the value from the key/value tuple\n        except IndexError:\n            return None", "response": "Retrieves a value associated with a key from the database"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def on_shutdown(self):\n        await self._unregister_subscriptions()\n\n        self._accepting = False\n\n        for (ws, _) in self._subscribers:\n            await ws.close(code=aiohttp.WSCloseCode.GOING_AWAY,\n                           message='Server shutdown')", "response": "Clean up any outstanding subscriptions and close all websockets."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling incoming requests for new subscription websockets.", "response": "async def subscriptions(self, request):\n        \"\"\"\n        Handles requests for new subscription websockets.\n\n        Args:\n            request (aiohttp.Request): the incoming request\n\n        Returns:\n            aiohttp.web.WebSocketResponse: the websocket response, when the\n                resulting websocket is closed\n        \"\"\"\n\n        if not self._accepting:\n            return web.Response(status=503)\n\n        web_sock = web.WebSocketResponse()\n        await web_sock.prepare(request)\n\n        async for msg in web_sock:\n            if msg.type == aiohttp.WSMsgType.TEXT:\n                await self._handle_message(web_sock, msg.data)\n            elif msg.type == aiohttp.WSMsgType.ERROR:\n                LOGGER.warning(\n                    'Web socket connection closed with exception %s',\n                    web_sock.exception())\n                await web_sock.close()\n\n        await self._handle_unsubscribe(web_sock)\n\n        return web_sock"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _future_done_callback(self, request, result):\n        self._in_process_transactions_count.dec()\n        req = processor_pb2.TpProcessRequest()\n        req.ParseFromString(request)\n        response = processor_pb2.TpProcessResponse()\n        response.ParseFromString(result.content)\n\n        processor_type = ProcessorType(\n            req.header.family_name,\n            req.header.family_version)\n\n        self._processor_manager[processor_type].get_processor(\n            result.connection_id).dec_occupancy()\n        self._processor_manager.notify()\n\n        self._get_tp_process_response_counter(\n            response.Status.Name(response.status)).inc()\n\n        if result.connection_id in self._open_futures and \\\n                req.signature in self._open_futures[result.connection_id]:\n            del self._open_futures[result.connection_id][req.signature]\n\n        if response.status == processor_pb2.TpProcessResponse.OK:\n            state_sets, state_deletes, events, data = \\\n                self._context_manager.get_execution_results(req.context_id)\n\n            state_changes = [\n                transaction_receipt_pb2.StateChange(\n                    address=addr,\n                    value=value,\n                    type=transaction_receipt_pb2.StateChange.SET)\n                for addr, value in state_sets.items()\n            ] + [\n                transaction_receipt_pb2.StateChange(\n                    address=addr,\n                    type=transaction_receipt_pb2.StateChange.DELETE)\n                for addr in state_deletes\n            ]\n\n            self._scheduler.set_transaction_execution_result(\n                txn_signature=req.signature,\n                is_valid=True,\n                context_id=req.context_id,\n                state_changes=state_changes,\n                events=events,\n                data=data)\n\n        elif response.status == processor_pb2.TpProcessResponse.INTERNAL_ERROR:\n            LOGGER.error(\n                \"Transaction processor internal error: %s \"\n                \"(transaction: %s, name: %s, version: %s)\",\n                response.message,\n                req.signature,\n                req.header.family_name,\n                req.header.family_version)\n\n            # Make sure that the transaction wasn't unscheduled in the interim\n            if self._scheduler.is_transaction_in_schedule(req.signature):\n                self._execute(\n                    processor_type=processor_type,\n                    content=request,\n                    signature=req.signature)\n\n        else:\n            self._context_manager.delete_contexts(\n                context_id_list=[req.context_id])\n\n            self._fail_transaction(\n                txn_signature=req.signature,\n                context_id=req.context_id,\n                error_message=response.message,\n                error_data=response.extended_data)", "response": "Callback function that is called when the request is done."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_batch_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'batch',\n        help='Displays information about batches and submit new batches',\n        description='Provides subcommands to display Batch information and '\n        'submit Batches to the validator via the REST API.')\n\n    grand_parsers = parser.add_subparsers(title='subcommands',\n                                          dest='subcommand')\n    grand_parsers.required = True\n    add_batch_list_parser(grand_parsers, parent_parser)\n    add_batch_show_parser(grand_parsers, parent_parser)\n    add_batch_status_parser(grand_parsers, parent_parser)\n    add_batch_submit_parser(grand_parsers, parent_parser)", "response": "Adds arguments parsers for the batch list batch show and batch status commands."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_batch(args):\n    if args.subcommand == 'list':\n        do_batch_list(args)\n\n    if args.subcommand == 'show':\n        do_batch_show(args)\n\n    if args.subcommand == 'status':\n        do_batch_status(args)\n\n    if args.subcommand == 'submit':\n        do_batch_submit(args)", "response": "Runs the batch list show or batch status command printing output\n            to the console"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nrun the batch - status command", "response": "def do_batch_status(args):\n    \"\"\"Runs the batch-status command, printing output to the console\n\n        Args:\n            args: The parsed arguments sent to the command at runtime\n    \"\"\"\n    rest_client = RestClient(args.url, args.user)\n    batch_ids = args.batch_ids.split(',')\n\n    if args.wait and args.wait > 0:\n        statuses = rest_client.get_statuses(batch_ids, args.wait)\n    else:\n        statuses = rest_client.get_statuses(batch_ids)\n\n    if args.format == 'yaml':\n        fmt.print_yaml(statuses)\n    elif args.format == 'json':\n        fmt.print_json(statuses)\n    else:\n        raise AssertionError('Missing handler: {}'.format(args.format))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking the block to see if it is complete and if it can be passed to the block_manager and if it can be passed to the block_manager and if it can be passed to the block_manager and if it can be passed to the block_manager. If it can t complete the block is dropped and return None.", "response": "def _complete_block(self, block):\n        \"\"\" Check the block to see if it is complete and if it can be passed to\n            the journal. If the block's predecessor is not in the block_manager\n            the predecessor is requested and the current block is added to the\n            the incomplete_block cache. If the block.batches and\n            block.header.batch_ids are not the same length, the batch_id list\n            is checked against the batch_cache to see if the batch_list can be\n            built. If any batches are missing from the block and we do not have\n            the batches in the batch_cache, they are requested. The block is\n            then added to the incomplete_block cache. If we can complete the\n            block, a new batch list is created in the correct order and added\n            to the block. The block is now considered complete and is returned.\n            If block.batches and block.header.batch_ids are the same length,\n            the block's batch list needs to be in the same order as the\n            block.header.batch_ids list. If the block has all of its expected\n            batches but are not in the correct order, the batch list is rebuilt\n            and added to the block. Once a block has the correct batch list it\n            is added to the block_manager and is returned.\n\n        \"\"\"\n\n        if block.header_signature in self._block_manager:\n            LOGGER.debug(\"Drop duplicate block: %s\", block)\n            return None\n\n        # NOTE: We cannot assume that if the previous block _is_ in the block\n        # manager, that it will still be in there when this block is complete.\n        if block.previous_block_id not in self._block_manager:\n            return self._request_previous_if_not_already_requested(block)\n\n        # Check for same number of batch_ids and batches\n        # If different starting building batch list, Otherwise there is a batch\n        # that does not belong, block should be dropped.\n        if len(block.batches) > len(block.header.batch_ids):\n            LOGGER.debug(\"Block has extra batches. Dropping %s\", block)\n            return None\n\n        # used to supplement batch_cache, contains batches already in block\n        temp_batches = {}\n        for batch in block.batches:\n            temp_batches[batch.header_signature] = batch\n\n        # The block is missing batches. Check to see if we can complete it.\n        if len(block.batches) != len(block.header.batch_ids):\n            building = True\n            for batch_id in block.header.batch_ids:\n                if batch_id not in self._batch_cache and \\\n                        batch_id not in temp_batches:\n                    # Request all missing batches\n                    if batch_id not in self._incomplete_blocks:\n                        self._incomplete_blocks[batch_id] = [block]\n                    elif block not in self._incomplete_blocks[batch_id]:\n                        self._incomplete_blocks[batch_id] += [block]\n\n                    # We have already requested the batch, do not do so again\n                    if batch_id in self._requested:\n                        return None\n                    self._requested[batch_id] = None\n                    self._gossip.broadcast_batch_by_batch_id_request(batch_id)\n                    building = False\n\n            # The block cannot be completed.\n            if not building:\n                return None\n\n            batches = self._finalize_batch_list(block, temp_batches)\n            del block.batches[:]\n            # reset batches with full list batches\n            block.batches.extend(batches)\n            if block.header_signature in self._requested:\n                del self._requested[block.header_signature]\n\n            return self._put_or_request_if_missing_predecessor(block)\n\n        batch_id_list = [x.header_signature for x in block.batches]\n        # Check to see if batchs are in the correct order.\n        if batch_id_list == list(block.header.batch_ids):\n            if block.header_signature in self._requested:\n                del self._requested[block.header_signature]\n\n            return self._put_or_request_if_missing_predecessor(block)\n\n        # Check to see if the block has all batch_ids and they can be put\n        # in the correct order\n        if sorted(batch_id_list) == sorted(list(block.header.batch_ids)):\n            batches = self._finalize_batch_list(block, temp_batches)\n            # Clear batches from block\n            del block.batches[:]\n            # reset batches with full list batches\n            if batches is not None:\n                block.batches.extend(batches)\n            else:\n                return None\n\n            if block.header_signature in self._requested:\n                del self._requested[block.header_signature]\n\n            return self._put_or_request_if_missing_predecessor(block)\n\n        LOGGER.debug(\"Block.header.batch_ids does not match set of \"\n                     \"batches in block.batches Dropping %s\", block)\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nrunning the batch list or batch show command printing output to the console", "response": "def do_state(args):\n    \"\"\"Runs the batch list or batch show command, printing output to the\n    console\n\n        Args:\n            args: The parsed arguments sent to the command at runtime\n    \"\"\"\n    rest_client = RestClient(args.url, args.user)\n\n    if args.subcommand == 'list':\n        response = rest_client.list_state(args.subtree, args.head)\n        leaves = response['data']\n        head = response['head']\n        keys = ('address', 'size', 'data')\n        headers = tuple(k.upper() for k in keys)\n\n        def parse_leaf_row(leaf, decode=True):\n            decoded = b64decode(leaf['data'])\n            return (\n                leaf['address'],\n                len(decoded),\n                str(decoded) if decode else leaf['data'])\n\n        if args.format == 'default':\n            fmt.print_terminal_table(headers, leaves, parse_leaf_row)\n            print('HEAD BLOCK: \"{}\"'.format(head))\n\n        elif args.format == 'csv':\n            fmt.print_csv(headers, leaves, parse_leaf_row)\n            print('(data for head block: \"{}\")'.format(head))\n\n        elif args.format == 'json' or args.format == 'yaml':\n            state_data = {\n                'head': head,\n                'data': [{k: d for k, d in zip(keys, parse_leaf_row(l, False))}\n                         for l in leaves]}\n\n            if args.format == 'yaml':\n                fmt.print_yaml(state_data)\n            elif args.format == 'json':\n                fmt.print_json(state_data)\n            else:\n                raise AssertionError('Missing handler: {}'.format(args.format))\n\n        else:\n            raise AssertionError('Missing handler: {}'.format(args.format))\n\n    if args.subcommand == 'show':\n        output = rest_client.get_leaf(args.address, args.head)\n        if output is not None:\n            print('DATA: \"{}\"'.format(b64decode(output['data'])))\n            print('HEAD: \"{}\"'.format(output['head']))\n        else:\n            raise CliException('No data available at {}'.format(args.address))"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _compute_merkle_root(self, required_state_root):\n\n        state_hash = None\n        if self._previous_valid_batch_c_id is not None:\n            publishing_or_genesis = self._always_persist or \\\n                required_state_root is None\n            state_hash = self._squash(\n                state_root=self._previous_state_hash,\n                context_ids=[self._previous_valid_batch_c_id],\n                persist=self._always_persist, clean_up=publishing_or_genesis)\n            if self._always_persist is True:\n                return state_hash\n            if state_hash == required_state_root:\n                self._squash(state_root=self._previous_state_hash,\n                             context_ids=[self._previous_valid_batch_c_id],\n                             persist=True, clean_up=True)\n        return state_hash", "response": "Computes the merkle root of the state changes in the context_id\n            which are corresponding with _last_valid_batch_c_id as applied to the context_id\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget the head block of the current chain.", "response": "def chain_head(self):\n        \"\"\"\n        Return the head block of the current chain.\n        \"\"\"\n        (vec_ptr, vec_len, vec_cap) = ffi.prepare_vec_result()\n\n        try:\n            _libexec(\n                'commit_store_get_chain_head',\n                self.pointer,\n                ctypes.byref(vec_ptr),\n                ctypes.byref(vec_len),\n                ctypes.byref(vec_cap))\n        except ValueError:\n            return None\n\n        return self.deserialize_block(\n            ffi.from_rust_vec(vec_ptr, vec_len, vec_cap))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn an iterator that traverses the blocks in the most recent version of the a .", "response": "def get_block_iter(self, start_block=None, start_block_num=None,\n                       reverse=True):\n        \"\"\"Returns an iterator that traverses blocks in block number order.\n\n        Args:\n            start_block (:obj:`BlockWrapper`): the block from which traversal\n                begins\n            start_block_num (str): a starting block number, in hex, from where\n                traversal begins; only used if no starting_block is provided\n\n            reverse (bool): If True, traverse the blocks in from most recent\n                to oldest block. Otherwise, it traverse the blocks in the\n                opposite order.\n\n        Returns:\n            An iterator of block wrappers\n\n        Raises:\n            ValueError: If start_block or start_block_num do not specify a\n                valid block\n        \"\"\"\n        start = None\n        if start_block_num:\n            if len(start_block_num) < 2:\n                raise ValueError(\"Invalid start block num\")\n            if start_block_num[:2] != \"0x\":\n                raise ValueError(\"Invalid start block num\")\n            start = int(start_block_num, 16)\n        elif start_block:\n            start = start_block.block_num\n\n        return _BlockStoreIter(\n            self.pointer,\n            start,\n            reverse)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_blocks(self, block_ids):\n        return list(\n            filter(\n                lambda b: b is not None,\n                map(self._get_block_by_id_or_none, block_ids)))", "response": "Returns all blocks with the given set of block_ids."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngetting the batch that has the transaction with the given id.", "response": "def get_batch_by_transaction(self, transaction_id):\n        \"\"\"\n        Check to see if the requested transaction_id is in the current chain.\n        If so, find the batch that has the transaction referenced by the\n        transaction_id and return the batch. This is done by finding the block\n        and searching for the batch.\n\n        :param transaction_id (string): The id of the transaction that is being\n            requested.\n        :return:\n        The batch that has the transaction.\n        \"\"\"\n        payload = self._get_data_by_id(\n            transaction_id, 'commit_store_get_batch_by_transaction')\n\n        batch = Batch()\n        batch.ParseFromString(payload)\n\n        return batch"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ngetting the batch with the given id and return it.", "response": "def get_batch(self, batch_id):\n        \"\"\"\n        Check to see if the requested batch_id is in the current chain. If so,\n        find the batch with the batch_id and return it. This is done by\n        finding the block and searching for the batch.\n\n        :param batch_id (string): The id of the batch requested.\n        :return:\n        The batch with the batch_id.\n        \"\"\"\n\n        payload = self._get_data_by_id(batch_id, 'commit_store_get_batch')\n\n        batch = Batch()\n        batch.ParseFromString(payload)\n\n        return batch"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_transaction(self, transaction_id):\n        payload = self._get_data_by_id(\n            transaction_id, 'commit_store_get_transaction')\n\n        txn = Transaction()\n        txn.ParseFromString(payload)\n\n        return txn", "response": "Returns a Transaction object from the block store by its id."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef format_terminal_row(headers, example_row):\n\n    def format_column(col):\n        if isinstance(col, str):\n            return '{{:{w}.{w}}}'\n        return '{{:<{w}}}'\n\n    widths = [max(len(h), len(str(d))) for h, d in zip(headers, example_row)]\n\n    # Truncate last column to fit terminal width\n    original_last_width = widths[-1]\n    if sys.stdout.isatty():\n        widths[-1] = max(\n            len(headers[-1]),\n            # console width - width of other columns and gutters - 3 for '...'\n            tty.width() - sum(w + 2 for w in widths[0:-1]) - 3)\n\n    # Build format string\n    cols = [format_column(c).format(w=w) for c, w in zip(example_row, widths)]\n    format_string = '  '.join(cols)\n    if original_last_width > widths[-1]:\n        format_string += '...'\n\n    return format_string", "response": "Uses headers and a row of example data to generate a format string for printing a single row of data."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting the data in a terminal table.", "response": "def print_terminal_table(headers, data_list, parse_row_fn):\n    \"\"\"Uses a set of headers, raw data, and a row parsing function, to print\n    data to the terminal in a table of rows and columns.\n\n    Args:\n        headers (tuple of strings): The headers for each column of data\n        data_list (list of dicts): Raw response data from the validator\n        parse_row_fn (function): Parses a dict of data into a tuple of columns\n            Expected args:\n                data (dict): A single response object from the validator\n            Expected return:\n                cols (tuple): The properties to display in each column\n    \"\"\"\n    data_iter = iter(data_list)\n    try:\n        example = next(data_iter)\n        example_row = parse_row_fn(example)\n        data_iter = itertools.chain([example], data_iter)\n    except StopIteration:\n        example_row = [''] * len(headers)\n\n    format_string = format_terminal_row(headers, example_row)\n\n    top_row = format_string.format(*headers)\n    print(top_row[0:-3] if top_row.endswith('...') else top_row)\n    for data in data_iter:\n        print(format_string.format(*parse_row_fn(data)))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_csv(headers, data_list, parse_row_fn):\n    try:\n        writer = csv.writer(sys.stdout)\n        writer.writerow(headers)\n        for data in data_list:\n            writer.writerow(parse_row_fn(data))\n    except csv.Error as e:\n        raise CliException('Error writing CSV: {}'.format(e))", "response": "Takes headers data and a row parsing function and prints data\n            to the console in a csv format."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef append(self, key, item):\n        with self._lock:\n            if key in self._dict:\n                self._dict[key].append(item)\n            else:\n                self._dict[key] = [item]", "response": "Append item to the list at key. Creates the list at key if it doesn t exist."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef set(self, key, items):\n        if not isinstance(items, list):\n            raise ValueError(\"items must be a list\")\n        with self._lock:\n            self._dict[key] = items.copy()", "response": "Set key to a copy of items"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the key to a copy of items and return the list that was previously stored if the key was set.", "response": "def swap(self, key, items):\n        \"\"\"Set key to a copy of items and return the list that was previously\n        stored if the key was set. If not key was set, returns an empty list.\n        \"\"\"\n        if not isinstance(items, list):\n            raise ValueError(\"items must be a list\")\n        return_value = []\n        with self._lock:\n            if key in self._dict:\n                return_value = self._dict[key]\n            # Make a copy since we don't want users keeping a reference that is\n            # outside the lock\n            self._dict[key] = items.copy()\n        return return_value"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nremove and returns the list stored at key. If the key is not set return default.", "response": "def pop(self, key, default):\n        \"\"\"If the key is set, remove and return the list stored at key.\n        Otherwise return default.\"\"\"\n        with self._lock:\n            return self._dict.pop(key, default)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get(self, key, default):\n        with self._lock:\n            try:\n                return self._dict[key].copy()\n            except KeyError:\n                return default", "response": "Returns a copy of the list stored at key. If the key is not set return default."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nremoving batches from the pending cache if found in the block store and notifies any observers.", "response": "def chain_update(self, block, receipts):\n        \"\"\"Removes batches from the pending cache if found in the block store,\n        and notifies any observers.\n        \"\"\"\n        with self._lock:\n            for batch_id in self._pending.copy():\n                if self._batch_committed(batch_id):\n                    self._pending.remove(batch_id)\n                    self._update_observers(batch_id,\n                                           ClientBatchStatus.COMMITTED)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef notify_txn_invalid(self, txn_id, message=None, extended_data=None):\n        invalid_txn_info = {'id': txn_id}\n        if message is not None:\n            invalid_txn_info['message'] = message\n        if extended_data is not None:\n            invalid_txn_info['extended_data'] = extended_data\n\n        with self._lock:\n            for batch_id, txn_ids in self._batch_info.items():\n                if txn_id in txn_ids:\n                    if batch_id not in self._invalid:\n                        self._invalid[batch_id] = [invalid_txn_info]\n                    else:\n                        self._invalid[batch_id].append(invalid_txn_info)\n                    self._pending.discard(batch_id)\n                    self._update_observers(batch_id, ClientBatchStatus.INVALID)\n                    return", "response": "Notify all observers that a transaction is rejected or extended data or has been sent to the invalid cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nnotifying the observers of a pending batch.", "response": "def notify_batch_pending(self, batch):\n        \"\"\"Adds a Batch id to the pending cache, with its transaction ids.\n\n        Args:\n            batch (str): The id of the pending batch\n        \"\"\"\n        txn_ids = {t.header_signature for t in batch.transactions}\n        with self._lock:\n            self._pending.add(batch.header_signature)\n            self._batch_info[batch.header_signature] = txn_ids\n            self._update_observers(batch.header_signature,\n                                   ClientBatchStatus.PENDING)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the status enum for a batch.", "response": "def get_status(self, batch_id):\n        \"\"\"Returns the status enum for a batch.\n\n        Args:\n            batch_id (str): The id of the batch to get the status for\n\n        Returns:\n            int: The status enum\n        \"\"\"\n        with self._lock:\n            if self._batch_committed(batch_id):\n                return ClientBatchStatus.COMMITTED\n            if batch_id in self._invalid:\n                return ClientBatchStatus.INVALID\n            if batch_id in self._pending:\n                return ClientBatchStatus.PENDING\n            return ClientBatchStatus.UNKNOWN"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning a dict with the statuses for the requested batches.", "response": "def get_statuses(self, batch_ids):\n        \"\"\"Returns a statuses dict for the requested batches.\n\n        Args:\n            batch_ids (list of str): The ids of the batches to get statuses for\n\n        Returns:\n            dict: A dict with keys of batch ids, and values of status enums\n        \"\"\"\n        with self._lock:\n            return {b: self.get_status(b) for b in batch_ids}"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_invalid_txn_info(self, batch_id):\n        with self._lock:\n            return [info.copy() for info in self._invalid.get(batch_id, [])]", "response": "Fetches the id of the Invalid Transaction that failed within a particular\n        Batch and as well as any error message or other data about the failure."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef watch_statuses(self, observer, batch_ids):\n        with self._lock:\n            statuses = self.get_statuses(batch_ids)\n            if self._has_no_pendings(statuses):\n                observer.notify_batches_finished(statuses)\n            else:\n                self._observers[observer] = statuses", "response": "Allows a component to be notified when a set of of\n        batches is no longer PENDING."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nupdates each observer tracking a particular batch with its new status.", "response": "def _update_observers(self, batch_id, status):\n        \"\"\"Updates each observer tracking a particular batch with its new\n        status. If all statuses are no longer pending, notifies the observer\n        and removes it from the list.\n        \"\"\"\n        for observer, statuses in self._observers.copy().items():\n            if batch_id in statuses:\n                statuses[batch_id] = status\n                if self._has_no_pendings(statuses):\n                    observer.notify_batches_finished(statuses)\n                    self._observers.pop(observer)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn True if a statuses dict has no PENDING statuses.", "response": "def _has_no_pendings(self, statuses):\n        \"\"\"Returns True if a statuses dict has no PENDING statuses.\n        \"\"\"\n        return all(s != ClientBatchStatus.PENDING for s in statuses.values())"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the value in this context or None for each address in the list of addresses. Useful for getting values for each address in the context manager.", "response": "def get(self, addresses):\n        \"\"\"Returns the value in this context, or None, for each address in\n        addresses. Useful for gets on the context manager.\n\n        Args:\n            addresses (list of str): The addresses to return values for, if\n                within this context.\n\n        Returns:\n            results (list of bytes): The values in state for these addresses.\n        \"\"\"\n\n        with self._lock:\n            results = []\n            for add in addresses:\n                self.validate_read(add)\n                results.append(self._get(add))\n            return results"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_if_set(self, addresses):\n\n        with self._lock:\n            results = []\n            for add in addresses:\n                results.append(self._get_if_set(add))\n            return results", "response": "Returns the value set at each address in the context or None if there is no value set at the address in this context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn a list of addresses that have been deleted or None if it hasn t been deleted.", "response": "def get_if_deleted(self, addresses):\n        \"\"\"Returns a list of addresses that have been deleted, or None if it\n        hasn't been deleted.\n\n        Args:\n            addresses (list of str): The addresses to check if deleted.\n\n        Returns:\n            (list of str): The addresses, if deleted, or None.\n        \"\"\"\n\n        with self._lock:\n            results = []\n            for add in addresses:\n                results.append(self._get_if_deleted(add))\n            return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns the value at an address if it was an input to the txn but never set.", "response": "def get_if_not_set(self, addresses):\n        \"\"\"Returns the value at an address if it was an input to the txn but\n        never set. It returns None if that address was never set in the\n        merkle database, or if the address is not within the context.\n\n        Args:\n            addresses (list of str): The full 70 character addresses.\n\n        Returns:\n            (list): bytes at that address but not set within the context\n        \"\"\"\n\n        with self._lock:\n            results = []\n            for add in addresses:\n                results.append(self._get_if_not_set(add))\n            return results"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns all the addresses and opaque values set in the context.", "response": "def get_all_if_set(self):\n        \"\"\"Return all the addresses and opaque values set in the context.\n        Useful in the squash method.\n\n        Returns:\n            (dict of str to bytes): The addresses and bytes that have\n                been set in the context.\n        \"\"\"\n\n        with self._lock:\n            results = {}\n            for add, fut in self._state.items():\n                if self._contains_and_set(add):\n                    results[add] = fut.result()\n            return results"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_all_if_deleted(self):\n\n        with self._lock:\n            results = {}\n            for add, fut in self._state.items():\n                if self._contains_and_deleted(add):\n                    results[add] = fut.result()\n            return results", "response": "Return all the addresses deleted in the context."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncreates futures needed before reading the the address s value from the merkle tree.", "response": "def create_prefetch(self, addresses):\n        \"\"\"Create futures needed before starting the process of reading the\n        address's value from the merkle tree.\n\n        Args:\n            addresses (list of str): addresses in the txn's inputs that\n                aren't in any base context (or any in the chain).\n        \"\"\"\n\n        with self._lock:\n            for add in addresses:\n                self._state[add] = _ContextFuture(address=add,\n                                                  wait_for_tree=True)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncreating futures from inputs with the current value for that address.", "response": "def create_initial(self, address_values):\n        \"\"\"Create futures from inputs with the current value for that address\n        at the start of that context.\n\n        Args:\n            address_values (list of tuple): The tuple is string, bytes of the\n                address and value.\n        \"\"\"\n\n        with self._lock:\n            for add, val in address_values:\n                self._state[add] = _ContextFuture(address=add, result=val)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef set_from_tree(self, address_value_dict):\n\n        for address, value in address_value_dict.items():\n            if address in self._state:\n                self._state[address].set_result(result=value,\n                                                from_tree=True)", "response": "Set the result for each future at the given addresses with the value\n        stored in the merkle database."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalls by the context manager s delete method to mark an entry for deletion or create a new future and immediately set it to the future immediately.", "response": "def delete_direct(self, addresses):\n        \"\"\"Called in the context manager's delete method to either\n        mark an entry for deletion , or create a new future and immediately\n        set it for deletion in the future.\n\n        Args:\n            address_list (list of str): The unique full addresses.\n\n        Raises:\n            AuthorizationException\n        \"\"\"\n\n        with self._lock:\n            for address in addresses:\n                self._validate_write(address)\n                if address in self._state:\n                    self._state[address].set_deleted()\n                else:\n                    fut = _ContextFuture(address=address)\n                    self._state[address] = fut\n                    fut.set_deleted()"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncalling by the context manager s set method to set the value of the a value for an address.", "response": "def set_direct(self, address_value_dict):\n        \"\"\"Called in the context manager's set method to either overwrite the\n        value for an address, or create a new future and immediately set a\n        value in the future.\n\n        Args:\n            address_value_dict (dict of str:bytes): The unique full addresses\n                with bytes to set at that address.\n\n        Raises:\n            AuthorizationException\n        \"\"\"\n\n        with self._lock:\n            for address, value in address_value_dict.items():\n                self._validate_write(address)\n                if address in self._state:\n                    self._state[address].set_result(result=value)\n                else:\n                    fut = _ContextFuture(address=address)\n                    self._state[address] = fut\n                    fut.set_result(result=value)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nraises an exception if the address is not allowed to be set in this context based on txn outputs.", "response": "def _validate_write(self, address):\n        \"\"\"Raises an exception if the address is not allowed to be set\n        in this context, based on txn outputs.\n\n        Notes:\n            Checks that the address is either listed fully as one of the\n            outputs, or some portion of the address is listed as a namespace\n            in the outputs of the txn.\n\n        Args:\n            address (str): The address to be validated. The context manager\n                validates the address correctness (70 hex characters).\n        Returns:\n            None\n\n        Raises:\n            AuthorizationException\n        \"\"\"\n\n        if not any(address.startswith(ns) for ns in self._write_list):\n            raise AuthorizationException(address=address)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef validate_read(self, address):\n\n        if not any(address.startswith(ns) for ns in self._read_list):\n            raise AuthorizationException(address=address)", "response": "Raises an exception if the address is not allowed to be read in\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef result(self):\n\n        if self._read_only:\n            return self._result\n        with self._condition:\n            if self._wait_for_tree and not self._result_set_in_context:\n                self._condition.wait_for(\n                    lambda: self._tree_has_set or self._result_set_in_context)\n            return self._result", "response": "Return the value at an address optionally waiting until it is set in the context manager or set based on the pre - fetch mechanism."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsetting the value at an address.", "response": "def set_result(self, result, from_tree=False):\n        \"\"\"Set the addresses's value unless the future has been declared\n        read only.\n\n        Args:\n            result (bytes): The value at an address.\n            from_tree (bool): Whether the value is being set by a read from\n                the merkle tree.\n\n        Returns:\n            None\n        \"\"\"\n\n        if self._read_only:\n            if not from_tree:\n                LOGGER.warning(\"Tried to set address %s on a\"\n                               \" read-only context.\",\n                               self.address)\n            return\n\n        with self._condition:\n            if self._read_only:\n                if not from_tree:\n                    LOGGER.warning(\"Tried to set address %s on a\"\n                                   \" read-only context.\",\n                                   self.address)\n                return\n            if from_tree:\n                # If the result has not been set in the context, overwrite the\n                # value with the value from the merkle tree. Otherwise, do\n                # nothing.\n                if not self._result_set_in_context:\n                    self._result = result\n                    self._tree_has_set = True\n            else:\n                self._result = result\n                self._result_set_in_context = True\n                self._deleted = False\n\n            self._condition.notify_all()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nverifies the state of all blocks in the state store.", "response": "def verify_state(global_state_db, blockstore, bind_component, scheduler_type):\n    \"\"\"\n    Verify the state root hash of all blocks is in state and if not,\n    reconstruct the missing state. Assumes that there are no \"holes\" in\n    state, ie starting from genesis, state is present for all blocks up to some\n    point and then not at all. If persist is False, this recomputes state in\n    memory for all blocks in the blockstore and verifies the state root\n    hashes.\n\n    Raises:\n        InvalidChainError: The chain in the blockstore is not valid.\n        ExecutionError: An unrecoverable error was encountered during batch\n            execution.\n    \"\"\"\n    state_view_factory = StateViewFactory(global_state_db)\n\n    # Check if we should do state verification\n    start_block, prev_state_root = search_for_present_state_root(\n        blockstore, state_view_factory)\n\n    if start_block is None:\n        LOGGER.info(\n            \"Skipping state verification: chain head's state root is present\")\n        return\n\n    LOGGER.info(\n        \"Recomputing missing state from block %s with %s scheduler\",\n        start_block, scheduler_type)\n\n    component_thread_pool = InstrumentedThreadPoolExecutor(\n        max_workers=10,\n        name='Component')\n\n    component_dispatcher = Dispatcher()\n    component_service = Interconnect(\n        bind_component,\n        component_dispatcher,\n        secured=False,\n        heartbeat=False,\n        max_incoming_connections=20,\n        monitor=True,\n        max_future_callback_workers=10)\n\n    context_manager = ContextManager(global_state_db)\n\n    transaction_executor = TransactionExecutor(\n        service=component_service,\n        context_manager=context_manager,\n        settings_view_factory=SettingsViewFactory(state_view_factory),\n        scheduler_type=scheduler_type,\n        invalid_observers=[])\n\n    component_service.set_check_connections(\n        transaction_executor.check_connections)\n\n    component_dispatcher.add_handler(\n        validator_pb2.Message.TP_RECEIPT_ADD_DATA_REQUEST,\n        tp_state_handlers.TpReceiptAddDataHandler(context_manager),\n        component_thread_pool)\n\n    component_dispatcher.add_handler(\n        validator_pb2.Message.TP_EVENT_ADD_REQUEST,\n        tp_state_handlers.TpEventAddHandler(context_manager),\n        component_thread_pool)\n\n    component_dispatcher.add_handler(\n        validator_pb2.Message.TP_STATE_DELETE_REQUEST,\n        tp_state_handlers.TpStateDeleteHandler(context_manager),\n        component_thread_pool)\n\n    component_dispatcher.add_handler(\n        validator_pb2.Message.TP_STATE_GET_REQUEST,\n        tp_state_handlers.TpStateGetHandler(context_manager),\n        component_thread_pool)\n\n    component_dispatcher.add_handler(\n        validator_pb2.Message.TP_STATE_SET_REQUEST,\n        tp_state_handlers.TpStateSetHandler(context_manager),\n        component_thread_pool)\n\n    component_dispatcher.add_handler(\n        validator_pb2.Message.TP_REGISTER_REQUEST,\n        processor_handlers.ProcessorRegisterHandler(\n            transaction_executor.processor_manager),\n        component_thread_pool)\n\n    component_dispatcher.add_handler(\n        validator_pb2.Message.TP_UNREGISTER_REQUEST,\n        processor_handlers.ProcessorUnRegisterHandler(\n            transaction_executor.processor_manager),\n        component_thread_pool)\n\n    component_dispatcher.start()\n    component_service.start()\n\n    process_blocks(\n        initial_state_root=prev_state_root,\n        blocks=blockstore.get_block_iter(\n            start_block=start_block, reverse=False),\n        transaction_executor=transaction_executor,\n        context_manager=context_manager,\n        state_view_factory=state_view_factory)\n\n    component_dispatcher.stop()\n    component_service.stop()\n    component_thread_pool.shutdown(wait=True)\n    transaction_executor.stop()\n    context_manager.stop()"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef search_for_present_state_root(blockstore, state_view_factory):\n    # If there is no chain to process, then we are done.\n    block = blockstore.chain_head\n    if block is None:\n        return None, None\n\n    # Check the head first\n    if state_db_has_root(state_view_factory, block.state_root_hash):\n        return None, None\n\n    prev_state_root = INIT_ROOT_KEY\n    for block in blockstore.get_block_iter(reverse=False):\n        if not state_db_has_root(state_view_factory, block.state_root_hash):\n            return block, prev_state_root\n        prev_state_root = block.state_root_hash\n\n    # This should never happen, since we already checked that the chain head\n    # didn't have a state root\n    raise ExecutionError(\n        \"Chain head state both missing but all blocks had state root present\")", "response": "Search through the blockstore and return the first block with a missing state root"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef is_valid_endpoint_host(interfaces, endpoint):\n        result = urlparse(endpoint)\n        hostname = result.hostname\n        if hostname is None:\n            return False\n\n        for interface in interfaces:\n            if interface == hostname:\n                return False\n\n        return True", "response": "An endpoint host name is valid if it is a URL and the hostname of the network interface is not the name of the network interface."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling a ConnectionRequest and ConnectionResponse message.", "response": "def handle(self, connection_id, message_content):\n        \"\"\"\n        A connection must use one of the supported authorization types\n        to prove their identity. If a requester deviates\n        from the procedure in any way, the requester will be rejected and the\n        connection will be closed. The same is true if the requester sends\n        multiple ConnectionRequests or multiple of any authorization-type\n        message. The validator receiving a new connection will receive a\n        ConnectionRequest. The validator will respond with a ConnectionResponse\n        message. The ConnectionResponse message will contain a list of\n        RoleEntry messages and an AuthorizationType. Role entries are\n        the accepted type of connections that are supported on the endpoint\n        that the ConnectionRequest was sent to. AuthorizationType describes the\n        procedure required to gain access to that role. If the validator is not\n        accepting connections or does not support the listed authorization\n        type, return an ConnectionResponse.ERROR and close the connection.\n        \"\"\"\n        message = ConnectionRequest()\n        message.ParseFromString(message_content)\n        LOGGER.debug(\"got connect message from %s. sending ack\", connection_id)\n\n        # Need to use join here to get the string \"0.0.0.0\". Otherwise,\n        # bandit thinks we are binding to all interfaces and returns a\n        # Medium security risk.\n        interfaces = [\"*\", \".\".join([\"0\", \"0\", \"0\", \"0\"])]\n        interfaces += netifaces.interfaces()\n        if self.is_valid_endpoint_host(interfaces, message.endpoint) is False:\n            LOGGER.warning(\"Connecting peer provided an invalid endpoint: %s; \"\n                           \"Ignoring connection request.\",\n                           message.endpoint)\n            connection_response = ConnectionResponse(\n                status=ConnectionResponse.ERROR)\n            return HandlerResult(\n                HandlerStatus.RETURN_AND_CLOSE,\n                message_out=connection_response,\n                message_type=validator_pb2.Message.\n                AUTHORIZATION_CONNECTION_RESPONSE)\n\n        LOGGER.debug(\"Endpoint of connecting node is %s\", message.endpoint)\n        self._network.update_connection_endpoint(connection_id,\n                                                 message.endpoint)\n\n        # Get what AuthorizationType the network role requires\n        roles = self._network.roles\n        auth_type = roles.get(\"network\")\n        if auth_type == AuthorizationType.TRUST:\n            role_type = ConnectionResponse.RoleEntry(\n                role=RoleType.Value(\"NETWORK\"),\n                auth_type=ConnectionResponse.TRUST)\n            connection_response = ConnectionResponse(roles=[role_type])\n        elif auth_type == AuthorizationType.CHALLENGE:\n            role_type = ConnectionResponse.RoleEntry(\n                role=RoleType.Value(\"NETWORK\"),\n                auth_type=ConnectionResponse.CHALLENGE)\n            connection_response = ConnectionResponse(roles=[role_type])\n        else:\n            LOGGER.warning(\"Network role is set to an unsupported\"\n                           \"Authorization Type: %s\", auth_type)\n            connection_response = ConnectionResponse(\n                status=ConnectionResponse.ERROR)\n            return HandlerResult(\n                HandlerStatus.RETURN_AND_CLOSE,\n                message_out=connection_response,\n                message_type=validator_pb2.Message.\n                AUTHORIZATION_CONNECTION_RESPONSE)\n\n        try:\n            is_outbound_connection = self._network.is_outbound_connection(\n                connection_id)\n        except KeyError:\n            # Connection has gone away, drop message\n            return HandlerResult(HandlerStatus.DROP)\n\n        if not is_outbound_connection:\n            if self._network.allow_inbound_connection():\n                LOGGER.debug(\"Allowing incoming connection: %s\", connection_id)\n                connection_response.status = connection_response.OK\n            else:\n                connection_response.status = connection_response.ERROR\n                return HandlerResult(\n                    HandlerStatus.RETURN_AND_CLOSE,\n                    message_out=connection_response,\n                    message_type=validator_pb2.Message.\n                    AUTHORIZATION_CONNECTION_RESPONSE)\n\n        if self._network.get_connection_status(connection_id) is not None:\n            LOGGER.debug(\"Connection has already sent ConnectionRequest:\"\n                         \" %s, Remove connection.\", connection_id)\n            connection_response.status = connection_response.ERROR\n            return HandlerResult(\n                HandlerStatus.RETURN_AND_CLOSE,\n                message_out=connection_response,\n                message_type=validator_pb2.Message.\n                AUTHORIZATION_CONNECTION_RESPONSE)\n\n        self._network.update_connection_status(\n            connection_id,\n            ConnectionStatus.CONNECTION_REQUEST)\n\n        return HandlerResult(\n            HandlerStatus.RETURN,\n            message_out=connection_response,\n            message_type=validator_pb2.Message.\n            AUTHORIZATION_CONNECTION_RESPONSE)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nhandling a connection request and response.", "response": "def handle(self, connection_id, message_content):\n        \"\"\"\n        If the connection wants to take on a role that requires a challenge to\n        be signed, it will request the challenge by sending an\n        AuthorizationChallengeRequest to the validator it wishes to connect to.\n        The validator will send back a random payload that must be signed.\n        If the connection has not sent a ConnectionRequest or the connection\n        has already recieved an AuthorizationChallengeResponse, an\n        AuthorizationViolation will be returned and the connection will be\n        closed.\n        \"\"\"\n        if self._network.get_connection_status(connection_id) != \\\n                ConnectionStatus.CONNECTION_REQUEST:\n            LOGGER.debug(\"Connection's previous message was not a\"\n                         \" ConnectionRequest, Remove connection to %s\",\n                         connection_id)\n            violation = AuthorizationViolation(\n                violation=RoleType.Value(\"NETWORK\"))\n            return HandlerResult(\n                HandlerStatus.RETURN_AND_CLOSE,\n                message_out=violation,\n                message_type=validator_pb2.Message\n                .AUTHORIZATION_VIOLATION)\n\n        random_payload = os.urandom(PAYLOAD_LENGTH)\n        self._challenge_payload_cache[connection_id] = random_payload\n        auth_challenge_response = AuthorizationChallengeResponse(\n            payload=random_payload)\n\n        self._network.update_connection_status(\n            connection_id,\n            ConnectionStatus.AUTH_CHALLENGE_REQUEST)\n\n        return HandlerResult(\n            HandlerStatus.RETURN,\n            message_out=auth_challenge_response,\n            message_type=validator_pb2.Message.\n            AUTHORIZATION_CHALLENGE_RESPONSE)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle(self, connection_id, message_content):\n        if self._network.get_connection_status(connection_id) != \\\n                ConnectionStatus.AUTH_CHALLENGE_REQUEST:\n            LOGGER.debug(\"Connection's previous message was not a\"\n                         \" AuthorizationChallengeRequest, Remove connection to\"\n                         \"%s\",\n                         connection_id)\n            return AuthorizationChallengeSubmitHandler \\\n                ._network_violation_result()\n\n        auth_challenge_submit = AuthorizationChallengeSubmit()\n        auth_challenge_submit.ParseFromString(message_content)\n\n        try:\n            payload = self._challenge_payload_cache[connection_id]\n        except KeyError:\n            LOGGER.warning(\"Connection's challenge payload expired before a\"\n                           \"response was received. %s\", connection_id)\n            return AuthorizationChallengeSubmitHandler \\\n                ._network_violation_result()\n\n        context = create_context('secp256k1')\n        try:\n            public_key = Secp256k1PublicKey.from_hex(\n                auth_challenge_submit.public_key)\n        except ParseError:\n            LOGGER.warning('Authorization Challenge Request cannot be '\n                           'verified. Invalid public key %s',\n                           auth_challenge_submit.public_key)\n            return AuthorizationChallengeSubmitHandler \\\n                ._network_violation_result()\n\n        if not context.verify(auth_challenge_submit.signature,\n                              payload,\n                              public_key):\n            LOGGER.warning(\"Signature was not able to be verified. Remove \"\n                           \"connection to %s\", connection_id)\n            return AuthorizationChallengeSubmitHandler \\\n                ._network_violation_result()\n\n        roles = self._network.roles\n        for role in auth_challenge_submit.roles:\n            if role == RoleType.Value(\"NETWORK\") or role == \\\n                    RoleType.Value(\"ALL\"):\n                permitted = False\n                if \"network\" in roles:\n                    permitted = self._permission_verifier.check_network_role(\n                        auth_challenge_submit.public_key)\n                if not permitted:\n                    return AuthorizationChallengeSubmitHandler \\\n                        ._network_violation_result()\n\n        self._network.update_connection_public_key(\n            connection_id,\n            auth_challenge_submit.public_key)\n\n        if RoleType.Value(\"NETWORK\") in auth_challenge_submit.roles:\n            # Need to send ConnectionRequest to authorize ourself with the\n            # connection if they initialized the connection\n            try:\n                is_outbound_connection = self._network.is_outbound_connection(\n                    connection_id)\n            except KeyError:\n                # Connection has gone away, drop message\n                return HandlerResult(HandlerStatus.DROP)\n\n            if not is_outbound_connection:\n                self._network.send_connect_request(connection_id)\n            else:\n                # If this is an outbound connection, authorization is complete\n                # for both connections and peering/topology build out can\n                # begin.\n                self._gossip.connect_success(connection_id)\n\n        auth_challenge_result = AuthorizationChallengeResult(\n            roles=[RoleType.Value(\"NETWORK\")])\n\n        LOGGER.debug(\"Connection: %s is approved\", connection_id)\n        self._network.update_connection_status(\n            connection_id,\n            ConnectionStatus.CONNECTED)\n        return HandlerResult(\n            HandlerStatus.RETURN,\n            message_out=auth_challenge_result,\n            message_type=validator_pb2.Message.AUTHORIZATION_CHALLENGE_RESULT)", "response": "Handle a new message received from the validator."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef handle(self, connection_id, message_content):\n        LOGGER.warning(\"Received AuthorizationViolation from %s\",\n                       connection_id)\n        # Close the connection\n        endpoint = self._network.connection_id_to_endpoint(connection_id)\n        self._network.remove_connection(connection_id)\n        self._gossip.remove_temp_endpoint(endpoint)\n        return HandlerResult(HandlerStatus.DROP)", "response": "Handle an AuthorizationViolation message."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef do_transaction(args):\n    rest_client = RestClient(args.url, args.user)\n\n    if args.subcommand == 'list':\n        transactions = rest_client.list_transactions()\n        keys = ('transaction_id', 'family', 'version', 'size', 'payload')\n        headers = tuple(k.upper() if k != 'version' else 'VERS' for k in keys)\n\n        def parse_txn_row(transaction, decode=True):\n            decoded = b64decode(transaction['payload'])\n            return (\n                transaction['header_signature'],\n                transaction['header']['family_name'],\n                transaction['header']['family_version'],\n                len(decoded),\n                str(decoded) if decode else transaction['payload'])\n\n        if args.format == 'default':\n            fmt.print_terminal_table(headers, transactions, parse_txn_row)\n\n        elif args.format == 'csv':\n            fmt.print_csv(headers, transactions, parse_txn_row)\n\n        elif args.format == 'json' or args.format == 'yaml':\n            data = [{k: d for k, d in zip(keys, parse_txn_row(b, False))}\n                    for b in transactions]\n\n            if args.format == 'yaml':\n                fmt.print_yaml(data)\n            elif args.format == 'json':\n                fmt.print_json(data)\n            else:\n                raise AssertionError('Missing handler: {}'.format(args.format))\n\n        else:\n            raise AssertionError('Missing handler: {}'.format(args.format))\n\n    if args.subcommand == 'show':\n        output = rest_client.get_transaction(args.transaction_id)\n\n        if args.key:\n            if args.key == 'payload':\n                output = b64decode(output['payload'])\n            elif args.key in output:\n                output = output[args.key]\n            elif args.key in output['header']:\n                output = output['header'][args.key]\n            else:\n                raise CliException(\n                    'Key \"{}\" not found in transaction or header'.format(\n                        args.key))\n\n        if args.format == 'yaml':\n            fmt.print_yaml(output)\n        elif args.format == 'json':\n            fmt.print_json(output)\n        else:\n            raise AssertionError('Missing handler: {}'.format(args.format))", "response": "Runs the list or show command"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nloading the CLI config file into the ARGS.", "response": "def load_cli_config(args):\n    \"\"\"Modifies ARGS in-place to have the attributes defined in the CLI\n    config file if it doesn't already have them. Certain default\n    values are given if they are not in ARGS or the config file.\n    \"\"\"\n    default_cli_config = _load_default_cli_config()\n    toml_config = _load_toml_cli_config()\n\n    for config in (toml_config, default_cli_config):\n        for key, val in config.items():\n            if key in args and getattr(args, key) is not None:\n                pass\n            else:\n                setattr(args, key, val)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get(self, txn_id):\n        if txn_id not in self._receipt_db:\n            raise KeyError('Unknown transaction id {}'.format(txn_id))\n\n        txn_receipt_bytes = self._receipt_db[txn_id]\n        txn_receipt = TransactionReceipt()\n        txn_receipt.ParseFromString(txn_receipt_bytes)\n        return txn_receipt", "response": "Returns the TransactionReceipt for the given transaction id."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_batches(self, batch_list):\n        if isinstance(batch_list, BaseMessage):\n            batch_list = batch_list.SerializeToString()\n\n        return self._post('/batches', batch_list)", "response": "Sends a list of batches to the validator."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _submit_request(self, url, params=None, data=None, headers=None,\n                        method=\"GET\"):\n        \"\"\"Submits the given request, and handles the errors appropriately.\n\n        Args:\n            url (str): the request to send.\n            params (dict): params to be passed along to get/post\n            data (bytes): the data to include in the request.\n            headers (dict): the headers to include in the request.\n            method (str): the method to use for the request, \"POST\" or \"GET\".\n\n        Returns:\n            tuple of (int, str): The response status code and the json parsed\n                body, or the error message.\n\n        Raises:\n            `CliException`: If any issues occur with the URL.\n        \"\"\"\n        if headers is None:\n            headers = {}\n\n        if self._auth_header is not None:\n            headers['Authorization'] = self._auth_header\n\n        try:\n            if method == 'POST':\n                result = requests.post(\n                    url, params=params, data=data, headers=headers)\n            elif method == 'GET':\n                result = requests.get(\n                    url, params=params, data=data, headers=headers)\n            result.raise_for_status()\n            return (result.status_code, result.json())\n        except requests.exceptions.HTTPError as e:\n            return (e.response.status_code, e.response.reason)\n        except RemoteDisconnected as e:\n            raise CliException(e)\n        except (requests.exceptions.MissingSchema,\n                requests.exceptions.InvalidURL) as e:\n            raise CliException(e)\n        except requests.exceptions.ConnectionError as e:\n            raise CliException(\n                ('Unable to connect to \"{}\": '\n                 'make sure URL is correct').format(self._base_url))", "response": "Submits the given request and handles errors appropriately."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef add_list_blocks_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'list-blocks',\n        help='List blocks from different nodes.',\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog='',\n        parents=[parent_parser, base_multinode_parser()])\n\n    parser.add_argument(\n        '-n',\n        '--count',\n        default=10,\n        type=int,\n        help='the number of blocks to list')", "response": "Adds the arg parsers needed for the compare command."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef chain_update(self, block, receipts):\n\n        block_events = BlockEventExtractor(block).extract([\n            EventSubscription(event_type=\"sawtooth/block-commit\")])\n        receipt_events = ReceiptEventExtractor(receipts).extract([\n            EventSubscription(event_type=\"identity/update\")])\n\n        for event in block_events:\n            forked = self._handle_block_commit(event)\n            if forked:\n                return\n\n        for event in receipt_events:\n            if event.event_type == \"identity/update\":\n                self._handle_txn_commit(event)", "response": "Handles both block - commit and identity - update events and txn - commit events."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef load_toml_rest_api_config(filename):\n    if not os.path.exists(filename):\n        LOGGER.info(\n            \"Skipping rest api loading from non-existent config file: %s\",\n            filename)\n        return RestApiConfig()\n\n    LOGGER.info(\"Loading rest api information from config: %s\", filename)\n\n    try:\n        with open(filename) as fd:\n            raw_config = fd.read()\n    except IOError as e:\n        raise RestApiConfigurationError(\n            \"Unable to load rest api configuration file: {}\".format(str(e)))\n\n    toml_config = toml.loads(raw_config)\n\n    invalid_keys = set(toml_config.keys()).difference(\n        ['bind', 'connect', 'timeout', 'opentsdb_db', 'opentsdb_url',\n         'opentsdb_username', 'opentsdb_password', 'client_max_size'])\n    if invalid_keys:\n        raise RestApiConfigurationError(\n            \"Invalid keys in rest api config: {}\".format(\n                \", \".join(sorted(list(invalid_keys)))))\n    config = RestApiConfig(\n        bind=toml_config.get(\"bind\", None),\n        connect=toml_config.get('connect', None),\n        timeout=toml_config.get('timeout', None),\n        opentsdb_url=toml_config.get('opentsdb_url', None),\n        opentsdb_db=toml_config.get('opentsdb_db', None),\n        opentsdb_username=toml_config.get('opentsdb_username', None),\n        opentsdb_password=toml_config.get('opentsdb_password', None),\n        client_max_size=toml_config.get('client_max_size', None)\n    )\n\n    return config", "response": "Loads a rest api configuration file from the file system."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ngive a list of PathConfig objects merges them into a single RestApiConfig object.", "response": "def merge_rest_api_config(configs):\n    \"\"\"\n    Given a list of PathConfig objects, merges them into a single PathConfig,\n    giving priority in the order of the configs (first has highest priority).\n    \"\"\"\n    bind = None\n    connect = None\n    timeout = None\n    opentsdb_url = None\n    opentsdb_db = None\n    opentsdb_username = None\n    opentsdb_password = None\n    client_max_size = None\n\n    for config in reversed(configs):\n        if config.bind is not None:\n            bind = config.bind\n        if config.connect is not None:\n            connect = config.connect\n        if config.timeout is not None:\n            timeout = config.timeout\n        if config.opentsdb_url is not None:\n            opentsdb_url = config.opentsdb_url\n        if config.opentsdb_db is not None:\n            opentsdb_db = config.opentsdb_db\n        if config.opentsdb_username is not None:\n            opentsdb_username = config.opentsdb_username\n        if config.opentsdb_password is not None:\n            opentsdb_password = config.opentsdb_password\n        if config.client_max_size is not None:\n            client_max_size = config.client_max_size\n\n    return RestApiConfig(\n        bind=bind,\n        connect=connect,\n        timeout=timeout,\n        opentsdb_url=opentsdb_url,\n        opentsdb_db=opentsdb_db,\n        opentsdb_username=opentsdb_username,\n        opentsdb_password=opentsdb_password,\n        client_max_size=client_max_size)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_block_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'block',\n        description='Provides subcommands to display information about the '\n        'blocks in the current blockchain.',\n        help='Displays information on blocks in the current blockchain')\n\n    grand_parsers = parser.add_subparsers(\n        title='subcommands',\n        dest='subcommand')\n\n    grand_parsers.required = True\n\n    description = (\n        'Displays information for all blocks on the current '\n        'blockchain, including the block id and number, public keys all '\n        'of allsigners, and number of transactions and batches.')\n\n    list_parser = grand_parsers.add_parser(\n        'list',\n        help='Displays information for all blocks on the current blockchain',\n        description=description,\n        parents=[base_http_parser(), base_list_parser()],\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n\n    list_parser.add_argument(\n        '-n',\n        '--count',\n        default=100,\n        type=int,\n        help='the number of blocks to list',\n    )\n\n    description = (\n        'Displays information about the specified block on '\n        'the current blockchain')\n\n    show_parser = grand_parsers.add_parser(\n        'show',\n        help=description,\n        description=description + '.',\n        parents=[base_http_parser(), base_show_parser()],\n        formatter_class=argparse.RawDescriptionHelpFormatter)\n    show_parser.add_argument(\n        'block_id',\n        type=str,\n        help='id (header_signature) of the block')", "response": "Adds the argument parsers for the block list and block show commands."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef do_block(args):\n    rest_client = RestClient(args.url, args.user)\n\n    if args.subcommand == 'list':\n        block_generator = rest_client.list_blocks()\n        blocks = []\n        left = args.count\n        for block in block_generator:\n            blocks.append(block)\n            left -= 1\n            if left <= 0:\n                break\n\n        keys = ('num', 'block_id', 'batches', 'txns', 'signer')\n        headers = tuple(k.upper() if k != 'batches' else 'BATS' for k in keys)\n\n        def parse_block_row(block):\n            batches = block.get('batches', [])\n            txns = [t for b in batches for t in b['transactions']]\n            return (\n                block['header'].get('block_num', 0),\n                block['header_signature'],\n                len(batches),\n                len(txns),\n                block['header']['signer_public_key'])\n\n        if args.format == 'default':\n            fmt.print_terminal_table(headers, blocks, parse_block_row)\n\n        elif args.format == 'csv':\n            fmt.print_csv(headers, blocks, parse_block_row)\n\n        elif args.format == 'json' or args.format == 'yaml':\n            data = [{k: d for k, d in zip(keys, parse_block_row(b))}\n                    for b in blocks]\n\n            if args.format == 'yaml':\n                fmt.print_yaml(data)\n            elif args.format == 'json':\n                fmt.print_json(data)\n            else:\n                raise AssertionError('Missing handler: {}'.format(args.format))\n\n        else:\n            raise AssertionError('Missing handler: {}'.format(args.format))\n\n    if args.subcommand == 'show':\n        output = rest_client.get_block(args.block_id)\n\n        if args.key:\n            if args.key in output:\n                output = output[args.key]\n            elif args.key in output['header']:\n                output = output['header'][args.key]\n            else:\n                raise CliException(\n                    'key \"{}\" not found in block or header'.format(args.key))\n\n        if args.format == 'yaml':\n            fmt.print_yaml(output)\n        elif args.format == 'json':\n            fmt.print_json(output)\n        else:\n            raise AssertionError('Missing handler: {}'.format(args.format))", "response": "Runs the block list or block show command printing output to the console"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\nasync def incoming(self):\n        msg = await self._queue.get()\n        self._queue.task_done()\n        return msg", "response": "Returns the next incoming message."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits for a reply to a given correlation id.", "response": "async def await_reply(self, correlation_id, timeout=None):\n        \"\"\"Wait for a reply to a given correlation id.  If a timeout is\n        provided, it will raise a asyncio.TimeoutError.\n        \"\"\"\n        try:\n            result = await asyncio.wait_for(\n                self._futures[correlation_id], timeout=timeout)\n\n            return result\n        finally:\n            del self._futures[correlation_id]"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nfail all the expected replies with a given error.", "response": "def fail_all(self, err):\n        \"\"\"Fail all the expected replies with a given error.\n        \"\"\"\n        for c_id in self._futures:\n            self._fail_reply(c_id, err)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngiving a message route it to the incoming queue or to the outgoing queue.", "response": "async def route_msg(self, msg):\n        \"\"\"Given a message, route it either to the incoming queue, or to the\n        future associated with its correlation_id.\n        \"\"\"\n        if msg.correlation_id in self._futures:\n            self._set_reply(msg.correlation_id, msg)\n        else:\n            await self._push_incoming(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\nasync def start(self):\n        self._is_running = True\n\n        while self._is_running:\n            try:\n                zmq_msg = await self._socket.recv_multipart()\n\n                message = Message()\n                message.ParseFromString(zmq_msg[-1])\n\n                await self._msg_router.route_msg(message)\n            except DecodeError as e:\n                LOGGER.warning('Unable to decode: %s', e)\n            except zmq.ZMQError as e:\n                LOGGER.warning('Unable to receive: %s', e)\n                return\n            except asyncio.CancelledError:\n                self._is_running = False", "response": "Starts receiving messages on the underlying socket and passes them to the message router."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef open(self):\n        LOGGER.info('Connecting to %s', self._url)\n        asyncio.ensure_future(self._do_start())", "response": "Opens the connection to the remote end."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nregistering a callback to be triggered when a specific connection state change is received.", "response": "def on_connection_state_change(self, event_type, callback):\n        \"\"\"Register a callback for a specific connection state change.\n\n        Register a callback to be triggered when the connection changes to\n        the specified state, signified by a ConnectionEvent.\n\n        The callback must be a coroutine.\n\n        Args:\n            event_type (ConnectionEvent): the connection event to listen for\n            callback (coroutine): a coroutine to call on the event occurrence\n        \"\"\"\n        listeners = self._connection_state_listeners.get(event_type, [])\n        listeners.append(callback)\n        self._connection_state_listeners[event_type] = listeners"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\nasync def send(self, message_type, message_content, timeout=None):\n        return await self._sender.send(\n            message_type, message_content, timeout=timeout)", "response": "Sends a message and returns a future for the response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nclose the connection. All outstanding futures for replies will be sent a DisconnectError.", "response": "def close(self):\n        \"\"\"Closes the connection.\n\n        All outstanding futures for replies will be sent a DisconnectError.\n        \"\"\"\n        if self._recv_task:\n            self._recv_task.cancel()\n\n        self._disable_monitoring()\n\n        if self._monitor_task and not self._monitor_task.done():\n            self._monitor_task.cancel()\n\n        self._receiver.cancel()\n        self._socket.close(linger=0)\n\n        self._msg_router.fail_all(DisconnectError())"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef delete(self, key):\n        with self._lmdb.begin(write=True, buffers=True) as txn:\n            txn.delete(key.encode())", "response": "Removes a key value from the database."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nreturn a list of keys in the database.", "response": "def keys(self, index=None):\n        \"\"\"Returns a list of keys in the database\n        \"\"\"\n        with self._lmdb.begin() as txn:\n            return [key.decode() for key, _ in txn.cursor()]"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef build_block(self):\n        header_bytes = self.block_header.SerializeToString()\n        block = Block(header=header_bytes,\n                      header_signature=self._header_signature)\n        block.batches.extend(self.batches)\n        return block", "response": "Builds a candidate block into its finalized form for broadcast."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nsets the status of an executed transaction.", "response": "def set_transaction_execution_result(\n            self, txn_signature, is_valid, context_id, state_changes, events,\n            data, error_message, error_data):\n        \"\"\"Set the status of an executed transaction.\n\n        Called by the executor after a transaction has been processed.\n\n        The scheduler must know when transactions have finished being\n        applied so that it can determine which transactions will become\n        eligible for processing.\n\n        Args:\n            txn_signature (str): The signature of the transaction, which\n                must match the header_signature field of the Transaction\n                object which was part of the added Batch.\n            is_valid (bool): True if transaction applied successfully or False\n                if the transaction failed and was not applied.\n            context_id (str): If status is True, contains the context_id\n                associated with the state changes made by the transaction.\n                If status is False, this should be set to None.\n\n        Raises:\n            ValueError: Thrown if transaction_signature does not match a\n            transaction.\n        \"\"\"\n        raise NotImplementedError()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nadding a list of blocks in a chain to the cache.", "response": "def add_chain(self, chain):\n        \"\"\"\n        Add block in a chain in the correct order. Also add all of the blocks\n        to the cache before doing a purge.\n        \"\"\"\n        with self._lock:\n            chain.sort(key=lambda x: x.block_num)\n            for block in chain:\n                block_id = block.header_signature\n                if block_id not in self._cache:\n                    self._cache[block_id] = self.CachedValue(block)\n                    if block.previous_block_id in self._cache:\n                        self._cache[block.previous_block_id].inc_count()\n\n            if time.time() > self._next_purge_time:\n                self._purge_expired()\n                self._next_purge_time = time.time() + self._purge_frequency"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nremove all expired entries from the cache that do not have a reference count.", "response": "def _purge_expired(self):\n        \"\"\"\n        Remove all expired entries from the cache that do not have a reference\n        count.\n        \"\"\"\n        time_horizon = time.time() - self._keep_time\n        new_cache = {}\n        dec_count_for = []\n        for (k, v) in self._cache.items():\n            if v.count > 0:\n                if k not in self._block_store:\n                    new_cache[k] = v\n                else:\n                    if v.timestamp > time_horizon:\n                        new_cache[k] = v\n                    else:\n                        block = v.value\n                        if block is not None:\n                            dec_count_for.append(block.previous_block_id)\n\n            elif v.timestamp > time_horizon:\n                new_cache[k] = v\n\n            else:\n                block = v.value\n                # Handle NULL_BLOCK_IDENTIFIER\n                if block is not None:\n                    dec_count_for.append(block.previous_block_id)\n\n        self._cache = new_cache\n        for block_id in dec_count_for:\n            if block_id in self._cache:\n                self._cache[block_id].dec_count()"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check(cls, status):\n        assert cls.trigger is not None, 'Invalid ErrorTrap, trigger not set'\n        assert cls.error is not None, 'Invalid ErrorTrap, error not set'\n\n        if status == cls.trigger:\n            # pylint: disable=not-callable\n            # cls.error will be callable at runtime\n            raise cls.error()", "response": "Checks if a status enum was set and raises an appropriate error."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_batches(self, block, keep_batches):\n        clone = Block()\n        clone.header = block.header\n        clone.header_signature = block.header_signature\n        clone.batches.extend([\n            batch for batch in block.batches\n            if batch.header_signature in keep_batches\n        ])\n        return clone", "response": "Returns a copy of the block with the non - injected\n        batches removed."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef initialize_block(self, block_header):\n        # Using the current chain head, we need to create a state view so we\n        # can get our config values.\n        state_view = \\\n            BlockWrapper.state_view_for_block(\n                self._block_cache.block_store.chain_head,\n                self._state_view_factory)\n\n        settings_view = SettingsView(state_view)\n        self._min_wait_time = settings_view.get_setting(\n            \"sawtooth.consensus.min_wait_time\", self._min_wait_time, int)\n        self._max_wait_time = settings_view.get_setting(\n            \"sawtooth.consensus.max_wait_time\", self._max_wait_time, int)\n        self._valid_block_publishers = settings_view.get_setting(\n            \"sawtooth.consensus.valid_block_publishers\",\n            self._valid_block_publishers,\n            list)\n\n        block_header.consensus = b\"Devmode\"\n        self._start_time = time.time()\n        self._wait_time = random.uniform(\n            self._min_wait_time, self._max_wait_time)\n        return True", "response": "Initialize the block with the given block header."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck if a candidate block is ready to be claimed.", "response": "def check_publish_block(self, block_header):\n        \"\"\"Check if a candidate block is ready to be claimed.\n\n        block_header (BlockHeader): the block_header to be checked if it\n            should be claimed\n        Returns:\n            Boolean: True if the candidate block_header should be claimed.\n        \"\"\"\n        if any(publisher_key != block_header.signer_public_key\n               for publisher_key in self._valid_block_publishers):\n            return False\n\n        if self._min_wait_time == 0:\n            return True\n\n        if self._min_wait_time < 0:\n            return False\n\n        assert self._min_wait_time > 0\n\n        if self._max_wait_time <= 0:\n            return self._start_time + self._min_wait_time <= time.time()\n\n        assert self._max_wait_time > 0\n\n        if self._max_wait_time <= self._min_wait_time:\n            return False\n\n        assert 0 < self._min_wait_time < self._max_wait_time\n\n        return self._start_time + self._wait_time <= time.time()"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef is_batch_signer_authorized(self, batch, state_root=None,\n                                   from_state=False):\n        \"\"\" Check the batch signing key against the allowed transactor\n            permissions. The roles being checked are the following, from first\n            to last:\n                \"transactor.batch_signer\"\n                \"transactor\"\n                \"default\"\n\n            The first role that is set will be the one used to enforce if the\n            batch signer is allowed.\n\n            Args:\n                batch (Batch): The batch that is being verified.\n                state_root(string): The state root of the previous block. If\n                    this is None, the current state root hash will be\n                    retrieved.\n                from_state (bool): Whether the identity value should be read\n                    directly from state, instead of using the cached values.\n                    This should be used when the state_root passed is not from\n                    the current chain head.\n\n        \"\"\"\n        if state_root is None:\n            state_root = self._current_root_func()\n            if state_root == INIT_ROOT_KEY:\n                LOGGER.debug(\"Chain head is not set yet. Permit all.\")\n                return True\n\n        self._cache.update_view(state_root)\n\n        header = BatchHeader()\n        header.ParseFromString(batch.header)\n\n        role = self._cache.get_role(\n            \"transactor.batch_signer\",\n            state_root,\n            from_state)\n\n        if role is None:\n            role = self._cache.get_role(\"transactor\", state_root, from_state)\n\n        if role is None:\n            policy_name = \"default\"\n        else:\n            policy_name = role.policy_name\n\n        policy = self._cache.get_policy(policy_name, state_root, from_state)\n        if policy is None:\n            allowed = True\n        else:\n            allowed = self._allowed(header.signer_public_key, policy)\n\n        if allowed:\n            return self.is_transaction_signer_authorized(\n                batch.transactions,\n                state_root,\n                from_state)\n        LOGGER.debug(\"Batch Signer: %s is not permitted.\",\n                     header.signer_public_key)\n        return False", "response": "Checks if the batch signer is authorized."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nchecking if the transaction signing key against the allowed transactor permissions.", "response": "def is_transaction_signer_authorized(self, transactions, state_root,\n                                         from_state):\n        \"\"\" Check the transaction signing key against the allowed transactor\n            permissions. The roles being checked are the following, from first\n            to last:\n                \"transactor.transaction_signer.<TP_Name>\"\n                \"transactor.transaction_signer\"\n                \"transactor\"\n                \"default\"\n\n            The first role that is set will be the one used to enforce if the\n            transaction signer is allowed.\n\n            Args:\n                transactions (List of Transactions): The transactions that are\n                    being verified.\n                state_root(string): The state root of the previous block. If\n                    this is None, the current state root hash will be\n                    retrieved.\n                from_state (bool): Whether the identity value should be read\n                    directly from state, instead of using the cached values.\n                    This should be used when the state_root passed is not from\n                    the current chain head.\n        \"\"\"\n        role = None\n        if role is None:\n            role = self._cache.get_role(\"transactor.transaction_signer\",\n                                        state_root, from_state)\n\n        if role is None:\n            role = self._cache.get_role(\"transactor\", state_root, from_state)\n\n        if role is None:\n            policy_name = \"default\"\n        else:\n            policy_name = role.policy_name\n\n        policy = self._cache.get_policy(policy_name, state_root, from_state)\n\n        family_roles = {}\n        for transaction in transactions:\n            header = TransactionHeader()\n            header.ParseFromString(transaction.header)\n            family_policy = None\n            if header.family_name not in family_roles:\n                role = self._cache.get_role(\n                    \"transactor.transaction_signer.\" + header.family_name,\n                    state_root,\n                    from_state)\n\n                if role is not None:\n                    family_policy = self._cache.get_policy(role.policy_name,\n                                                           state_root,\n                                                           from_state)\n                family_roles[header.family_name] = family_policy\n            else:\n                family_policy = family_roles[header.family_name]\n\n            if family_policy is not None:\n                if not self._allowed(header.signer_public_key, family_policy):\n                    LOGGER.debug(\"Transaction Signer: %s is not permitted.\",\n                                 header.signer_public_key)\n                    return False\n            else:\n                if policy is not None:\n                    if not self._allowed(header.signer_public_key, policy):\n                        LOGGER.debug(\n                            \"Transaction Signer: %s is not permitted.\",\n                            header.signer_public_key)\n                        return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncheck the batch signing key against the allowed off - chain batch signer permissions.", "response": "def check_off_chain_batch_roles(self, batch):\n        \"\"\" Check the batch signing key against the allowed off-chain\n            transactor permissions. The roles being checked are the following,\n            from first to last:\n                \"transactor.batch_signer\"\n                \"transactor\"\n\n            The first role that is set will be the one used to enforce if the\n            batch signer is allowed.\n\n            Args:\n                batch (Batch): The batch that is being verified.\n                state_root(string): The state root of the previous block. If\n                    this is None, the current state root hash will be\n                    retrieved.\n\n        \"\"\"\n        if self._permissions is None:\n            return True\n        header = BatchHeader()\n        header.ParseFromString(batch.header)\n        policy = None\n        if \"transactor.batch_signer\" in self._permissions:\n            policy = self._permissions[\"transactor.batch_signer\"]\n\n        elif \"transactor\" in self._permissions:\n            policy = self._permissions[\"transactor\"]\n\n        allowed = True\n        if policy is not None:\n            allowed = self._allowed(header.signer_public_key, policy)\n\n        if allowed:\n            return self.check_off_chain_transaction_roles(batch.transactions)\n\n        LOGGER.debug(\"Batch Signer: %s is not permitted by local\"\n                     \" configuration.\", header.signer_public_key)\n        return False"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef check_off_chain_transaction_roles(self, transactions):\n        policy = None\n        if \"transactor.transaction_signer\" in self._permissions:\n            policy = self._permissions[\"transactor.transaction_signer\"]\n\n        elif \"transactor\" in self._permissions:\n            policy = self._permissions[\"transactor\"]\n\n        for transaction in transactions:\n            header = TransactionHeader()\n            header.ParseFromString(transaction.header)\n            family_role = \"transactor.transaction_signer.\" + \\\n                header.family_name\n            family_policy = None\n            if family_role in self._permissions:\n                family_policy = self._permissions[family_role]\n\n            if family_policy is not None:\n                if not self._allowed(header.signer_public_key, family_policy):\n                    LOGGER.debug(\"Transaction Signer: %s is not permitted\"\n                                 \"by local configuration.\",\n                                 header.signer_public_key)\n                    return False\n\n            elif policy is not None:\n                if not self._allowed(header.signer_public_key, policy):\n                    LOGGER.debug(\"Transaction Signer: %s is not permitted\"\n                                 \"by local configuration.\",\n                                 header.signer_public_key)\n                    return False\n\n        return True", "response": "Checks the transaction signing key against the allowed off - chain transaction roles."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nchecking if the public key is permitted to participate in the network.", "response": "def check_network_role(self, public_key):\n        \"\"\" Check the public key of a node on the network to see if they are\n            permitted to participate. The roles being checked are the\n            following, from first to last:\n                \"network\"\n                \"default\"\n\n            The first role that is set will be the one used to enforce if the\n            node is allowed.\n\n            Args:\n                public_key (string): The public key belonging to a node on the\n                    network\n        \"\"\"\n        state_root = self._current_root_func()\n        if state_root == INIT_ROOT_KEY:\n            LOGGER.debug(\"Chain head is not set yet. Permit all.\")\n            return True\n\n        self._cache.update_view(state_root)\n        role = self._cache.get_role(\"network\", state_root)\n\n        if role is None:\n            policy_name = \"default\"\n        else:\n            policy_name = role.policy_name\n        policy = self._cache.get_policy(policy_name, state_root)\n        if policy is not None:\n            if not self._allowed(public_key, policy):\n                LOGGER.debug(\"Node is not permitted: %s.\", public_key)\n                return False\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nuse to retrieve an identity role.", "response": "def get_role(self, item, state_root, from_state=False):\n        \"\"\"\n        Used to retrieve an identity role.\n        Args:\n            item (string): the name of the role to be fetched\n            state_root(string): The state root of the previous block.\n            from_state (bool): Whether the identity value should be read\n                directly from state, instead of using the cached values.\n                This should be used when the state_root passed is not from\n                the current chain head.\n        \"\"\"\n        if from_state:\n            # if from state use identity_view and do not add to cache\n            if self._identity_view is None:\n                self.update_view(state_root)\n            value = self._identity_view.get_role(item)\n            return value\n\n        value = self._cache.get(item)\n        if value is None:\n            if self._identity_view is None:\n                self.update_view(state_root)\n            value = self._identity_view.get_role(item)\n            self._cache[item] = value\n        return value"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ntaking a statuses dict and formats it for transmission with Protobuf and ZMQ.", "response": "def _format_batch_statuses(statuses, batch_ids, tracker):\n    \"\"\"Takes a statuses dict and formats it for transmission with Protobuf and\n    ZMQ.\n\n    Args:\n        statuses (dict of int): Dict with batch ids as the key, status as value\n        batch_ids (list of str): The batch ids in their original order\n        tracker (BatchTracker): A batch tracker with access to invalid info\n    \"\"\"\n    proto_statuses = []\n\n    for batch_id in batch_ids:\n        if statuses[batch_id] == \\\n           client_batch_submit_pb2.ClientBatchStatus.INVALID:\n            invalid_txns = tracker.get_invalid_txn_info(batch_id)\n            for txn_info in invalid_txns:\n                try:\n                    txn_info['transaction_id'] = txn_info.pop('id')\n                except KeyError as e:\n                    LOGGER.debug(e)\n        else:\n            invalid_txns = None\n\n        proto_statuses.append(\n            client_batch_submit_pb2.ClientBatchStatus(\n                batch_id=batch_id,\n                status=statuses[batch_id],\n                invalid_transactions=invalid_txns))\n\n    return proto_statuses"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nhandling parsing incoming requests and wrapping the final response.", "response": "def handle(self, connection_id, message_content):\n        \"\"\"Handles parsing incoming requests, and wrapping the final response.\n\n        Args:\n            connection_id (str): ZMQ identity sent over ZMQ socket\n            message_content (bytes): Byte encoded request protobuf to be parsed\n\n        Returns:\n            HandlerResult: result to be sent in response back to client\n        \"\"\"\n        try:\n            request = self._request_proto()\n            request.ParseFromString(message_content)\n        except DecodeError:\n            LOGGER.info('Protobuf %s failed to deserialize', request)\n            return self._wrap_result(self._status.INTERNAL_ERROR)\n\n        try:\n            response = self._respond(request)\n        except _ResponseFailed as e:\n            response = e.status\n\n        return self._wrap_result(response)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _wrap_result(self, response):\n        if isinstance(response, int):\n            response = self._wrap_response(response)\n\n        return HandlerResult(\n            status=HandlerStatus.RETURN,\n            message_out=self._response_proto(**response),\n            message_type=self._response_type)", "response": "Wraps child s response in a HandlerResult to be sent back to client."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_head_block(self, request):\n        if request.head_id:\n            if self._id_regex.fullmatch(request.head_id) is None:\n                LOGGER.debug('Invalid head id requested: %s', request.head_id)\n                raise _ResponseFailed(self._status.NO_ROOT)\n            try:\n                return self._block_store[request.head_id]\n            except KeyError as e:\n                LOGGER.debug('Unable to find block \"%s\" in store', e)\n                raise _ResponseFailed(self._status.NO_ROOT)\n\n        else:\n            return self._get_chain_head()", "response": "Fetches the request specified head block or the chain head."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nset the root of the merkle tree returning any head id used.", "response": "def _set_root(self, request):\n        \"\"\"Sets the root of the merkle tree, returning any head id used.\n\n        Note:\n            This method will fail if `_tree` has not been set\n\n        Args:\n            request (object): The parsed protobuf request object\n\n        Returns:\n            str: the state root of the head block used to specify the root\n\n        Raises:\n            ResponseFailed: Failed to set the root if the merkle tree\n        \"\"\"\n        if request.state_root:\n            root = request.state_root\n        else:\n            head = self._get_chain_head()\n            root = head.state_root_hash\n\n        try:\n            self._tree.set_merkle_root(root)\n        except KeyError as e:\n            LOGGER.debug('Unable to find root \"%s\" in database', e)\n            raise _ResponseFailed(self._status.NO_ROOT)\n\n        return root"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nbuilding a list of blocks or resources derived from blocks and appends them to the end of the list", "response": "def _list_store_resources(self, request, head_id, filter_ids,\n                              resource_fetcher, block_xform):\n        \"\"\"Builds a list of blocks or resources derived from blocks,\n        handling multiple possible filter requests:\n            - filtered by a set of ids\n            - filtered by head block\n            - filtered by both id and head block\n            - not filtered (all current resources)\n\n        Note:\n            This method will fail if `_block_store` has not been set\n\n        Args:\n            request (object): The parsed protobuf request object\n            head_id (str): Either request.head_id, or the current chain head\n            filter_ids (list of str): the resource ids (if any) to filter by\n            resource_fetcher (function): Fetches a resource by its id\n                Expected args:\n                    resource_id: The id of the resource to be fetched\n                Expected return:\n                    object: The resource to be appended to the results\n            block_xform (function): Transforms a block into a list of resources\n                Expected args:\n                    block: A block object from the block store\n                Expected return:\n                    list: To be concatenated to the end of the results\n\n        Returns:\n            list: List of blocks or data from blocks. If filtered by ids,\n                they will be listed in the same order as the id filters,\n                otherwise they will be ordered from newest to oldest\n        \"\"\"\n        resources = []\n\n        # Simply fetch by id if filtered by id but not by head block\n        if filter_ids and not request.head_id:\n            for resource_id in filter_ids:\n                try:\n                    resources.append(resource_fetcher(resource_id))\n                except (KeyError, ValueError, TypeError):\n                    # Invalid ids should be omitted, not raise an exception\n                    pass\n\n        # Traverse block chain to build results for most scenarios\n        else:\n            current_id = head_id\n            while current_id in self._block_store:\n                block = self._block_store[current_id].block\n                resources += block_xform(block)\n                header = BlockHeader()\n                header.ParseFromString(block.header)\n                current_id = header.previous_block_id\n\n        # If filtering by head AND ids, the traverse results must be winnowed\n        if request.head_id and filter_ids:\n            matches = {\n                r.header_signature: r\n                for r in resources if r.header_signature in filter_ids\n            }\n            resources = [matches[i] for i in filter_ids if i in matches]\n\n        return resources"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _validate_ids(self, resource_ids):\n        for resource_id in resource_ids:\n            if self._id_regex.fullmatch(resource_id) is None:\n                LOGGER.debug('Invalid resource id requested: %s', resource_id)\n                raise _ResponseFailed(self._status.INVALID_ID)", "response": "Validates a list of resource ids raising a ResponseFailed error if invalid."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate a state root raises a ResponseFailed error if invalid.", "response": "def _validate_state_root(self, state_root):\n        \"\"\"Validates a state root, raising a ResponseFailed error if invalid.\n\n        Args:\n            state_root (str): The state_root to validate\n\n        Raises:\n            ResponseFailed: The state_root was invalid, and a status of\n                INVALID_ROOT will be sent with the response.\n        \"\"\"\n        if self._state_root_regex.fullmatch(state_root) is None:\n            LOGGER.debug('Invalid state root: %s', state_root)\n            raise _ResponseFailed(self._status.INVALID_ROOT)"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nvalidate a namespace raising a ResponseFailed error if invalid.", "response": "def _validate_namespace(self, namespace):\n        \"\"\"Validates a namespace, raising a ResponseFailed error if invalid.\n\n        Args:\n            state_root (str): The state_root to validate\n\n        Raises:\n            ResponseFailed: The state_root was invalid, and a status of\n                INVALID_ROOT will be sent with the response.\n        \"\"\"\n        if self._namespace_regex.fullmatch(namespace) is None:\n            LOGGER.debug('Invalid namespace: %s', namespace)\n            raise _ResponseFailed(self._status.INVALID_ADDRESS)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef paginate_resources(cls, request, resources, on_fail_status):\n        if not resources:\n            return (resources, client_list_control_pb2.ClientPagingResponse())\n\n        paging = request.paging\n        limit = min(paging.limit, MAX_PAGE_SIZE) or DEFAULT_PAGE_SIZE\n        # Find the start index from the location marker sent\n        try:\n            if paging.start:\n                start_index = cls.index_by_id(paging.start, resources)\n            else:\n                start_index = 0\n\n            if start_index < 0 or start_index >= len(resources):\n                raise AssertionError\n        except AssertionError:\n            raise _ResponseFailed(on_fail_status)\n\n        paged_resources = resources[start_index: start_index + limit]\n        if start_index + limit < len(resources):\n            paging_response = client_list_control_pb2.ClientPagingResponse(\n                next=cls.id_by_index(start_index + limit, resources),\n                start=cls.id_by_index(start_index, resources),\n                limit=limit)\n        else:\n            paging_response = client_list_control_pb2.ClientPagingResponse(\n                start=cls.id_by_index(start_index, resources),\n                limit=limit)\n\n        return paged_resources, paging_response", "response": "Truncates a list of resources based on ClientPagingControls\n           ."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef index_by_id(cls, target_id, resources):\n        for index in range(len(resources)):\n            if cls.id_by_index(index, resources) == target_id:\n                return index\n\n        raise AssertionError", "response": "Helper method to fetch the index of a resource by its id or header signature"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef id_by_index(index, resources):\n        if index < 0 or index >= len(resources):\n            return ''\n\n        try:\n            return resources[index].header_signature\n        except AttributeError:\n            return resources[index].address", "response": "Helper method to fetch the id or address of a resource by its index"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef sort_resources(cls, request, resources, fail_enum, header_proto=None):\n        if not request.sorting:\n            return resources\n\n        value_handlers = cls._get_handler_set(request, fail_enum, header_proto)\n\n        def sorter(resource_a, resource_b):\n            for handler in value_handlers:\n                val_a, val_b = handler.get_sort_values(resource_a, resource_b)\n\n                if val_a < val_b:\n                    return handler.xform_result(-1)\n                if val_a > val_b:\n                    return handler.xform_result(1)\n\n            return 0\n\n        return sorted(resources, key=cmp_to_key(sorter))", "response": "Sorts a list of resources based on a list of sort controls."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_handler_set(cls, request, fail_enum, header_proto=None):\n        added = set()\n        handlers = []\n\n        for controls in request.sorting:\n            control_bytes = controls.SerializeToString()\n            if control_bytes not in added:\n                added.add(control_bytes)\n                handlers.append(\n                    cls._ValueHandler(controls, fail_enum, header_proto))\n\n        return handlers", "response": "Goes through the list of ClientSortControls and returns a list of _ValueHandlers. Maintains order for spamming."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall by the BatchTracker when batches have finished.", "response": "def notify_batches_finished(self, statuses):\n        \"\"\"Called by the BatchTracker the _BatchWaiter is observing. Should not\n        be called by handlers.\n\n        Args:\n            statuses (dict of int): A dict with keys of batch ids, and values\n                of status enums\n        \"\"\"\n        with self._wait_condition:\n            self._statuses = statuses\n            self._wait_condition.notify()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nwaits for a list of batches to be committed to the block chain and returns the statuses of those batches.", "response": "def wait_for_batches(self, batch_ids, timeout=None):\n        \"\"\"Locks until a list of batch ids is committed to the block chain\n        or a timeout is exceeded. Returns the statuses of those batches.\n\n        Args:\n            batch_ids (list of str): The ids of the batches to wait for\n            timeout(int): Maximum time in seconds to wait for\n\n        Returns:\n            list of BatchStatus: BatchStatuses to send back to client\n        \"\"\"\n        self._batch_tracker.watch_statuses(self, batch_ids)\n        timeout = timeout or DEFAULT_TIMEOUT\n        start_time = time()\n\n        with self._wait_condition:\n            while True:\n                if self._statuses is not None:\n                    return _format_batch_statuses(\n                        self._statuses, batch_ids, self._batch_tracker)\n\n                if time() - start_time > timeout:\n                    statuses = self._batch_tracker.get_statuses(batch_ids)\n                    return _format_batch_statuses(\n                        statuses, batch_ids, self._batch_tracker)\n\n                self._wait_condition.wait(timeout - (time() - start_time))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef check_directory(path, human_readable_name):\n    if not os.path.exists(path):\n        LOGGER.error(\"%s directory does not exist: %s\",\n                     human_readable_name,\n                     path)\n        return False\n\n    if not os.path.isdir(path):\n        LOGGER.error(\"%s directory is not a directory: %s\",\n                     human_readable_name,\n                     path)\n        return False\n\n    errors = True\n    if not os.access(path, os.R_OK):\n        LOGGER.error(\"%s directory is not readable: %s\",\n                     human_readable_name,\n                     path)\n        errors = False\n    if not os.access(path, os.W_OK):\n        LOGGER.error(\"%s directory is not writable: %s\",\n                     human_readable_name,\n                     path)\n        errors = False\n    return errors", "response": "Verify that the directory exists and is readable and writable."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nadding the arg parsers needed for the identity command and its subcommands.", "response": "def add_identity_parser(subparsers, parent_parser):\n    \"\"\"Creates the arg parsers needed for the identity command and\n    its subcommands.\n    \"\"\"\n    # identity\n    parser = subparsers.add_parser(\n        'identity',\n        help='Works with optional roles, policies, and permissions',\n        description='Provides subcommands to work with roles and policies.')\n\n    identity_parsers = parser.add_subparsers(\n        title=\"subcommands\",\n        dest=\"subcommand\")\n\n    identity_parsers.required = True\n\n    # policy\n    policy_parser = identity_parsers.add_parser(\n        'policy',\n        help='Provides subcommands to display existing policies and create '\n        'new policies',\n        description='Provides subcommands to list the current policies '\n        'stored in state and to create new policies.')\n\n    policy_parsers = policy_parser.add_subparsers(\n        title='policy',\n        dest='policy_cmd')\n\n    policy_parsers.required = True\n\n    # policy create\n    create_parser = policy_parsers.add_parser(\n        'create',\n        help='Creates batches of sawtooth-identity transactions for setting a '\n        'policy',\n        description='Creates a policy that can be set to a role or changes a '\n        'policy without resetting the role.')\n\n    create_parser.add_argument(\n        '-k', '--key',\n        type=str,\n        help='specify the signing key for the resulting batches')\n\n    create_target_group = create_parser.add_mutually_exclusive_group()\n\n    create_target_group.add_argument(\n        '-o', '--output',\n        type=str,\n        help='specify the output filename for the resulting batches')\n\n    create_target_group.add_argument(\n        '--url',\n        type=str,\n        help=\"identify the URL of a validator's REST API\",\n        default='http://localhost:8008')\n\n    create_parser.add_argument(\n        '--wait',\n        type=int,\n        default=15,\n        help=\"set time, in seconds, to wait for the policy to commit when \"\n             \"submitting to the REST API.\")\n\n    create_parser.add_argument(\n        'name',\n        type=str,\n        help='name of the new policy')\n\n    create_parser.add_argument(\n        'rule',\n        type=str,\n        nargs=\"+\",\n        help='rule with the format \"PERMIT_KEY <key>\" or \"DENY_KEY <key> '\n        '(multiple \"rule\" arguments can be specified)')\n\n    # policy list\n    list_parser = policy_parsers.add_parser(\n        'list',\n        help='Lists the current policies',\n        description='Lists the policies that are currently set in state.')\n\n    list_parser.add_argument(\n        '--url',\n        type=str,\n        help=\"identify the URL of a validator's REST API\",\n        default='http://localhost:8008')\n\n    list_parser.add_argument(\n        '--format',\n        default='default',\n        choices=['default', 'csv', 'json', 'yaml'],\n        help='choose the output format')\n\n    # role\n    role_parser = identity_parsers.add_parser(\n        'role',\n        help='Provides subcommands to display existing roles and create '\n        'new roles',\n        description='Provides subcommands to list the current roles '\n        'stored in state and to create new roles.')\n\n    role_parsers = role_parser.add_subparsers(\n        title='role',\n        dest='role_cmd')\n\n    role_parsers.required = True\n\n    # role create\n    create_parser = role_parsers.add_parser(\n        'create',\n        help='Creates a new role that can be used to enforce permissions',\n        description='Creates a new role that can be used to enforce '\n        'permissions.')\n\n    create_parser.add_argument(\n        '-k', '--key',\n        type=str,\n        help='specify the signing key for the resulting batches')\n\n    create_parser.add_argument(\n        '--wait',\n        type=int,\n        default=15,\n        help='set time, in seconds, to wait for a role to commit '\n        'when submitting to  the REST API.')\n\n    create_target_group = create_parser.add_mutually_exclusive_group()\n\n    create_target_group.add_argument(\n        '-o', '--output',\n        type=str,\n        help='specify the output filename for the resulting batches')\n\n    create_target_group.add_argument(\n        '--url',\n        type=str,\n        help=\"the URL of a validator's REST API\",\n        default='http://localhost:8008')\n\n    create_parser.add_argument(\n        'name',\n        type=str,\n        help='name of the role')\n\n    create_parser.add_argument(\n        'policy',\n        type=str,\n        help='identify policy that role will be restricted to')\n\n    # role list\n    list_parser = role_parsers.add_parser(\n        'list',\n        help='Lists the current keys and values of roles',\n        description='Displays the roles that are currently set in state.')\n\n    list_parser.add_argument(\n        '--url',\n        type=str,\n        help=\"identify the URL of a validator's REST API\",\n        default='http://localhost:8008')\n\n    list_parser.add_argument(\n        '--format',\n        default='default',\n        choices=['default', 'csv', 'json', 'yaml'],\n        help='choose the output format')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nexecute the config commands subcommands.", "response": "def do_identity(args):\n    \"\"\"Executes the config commands subcommands.\n    \"\"\"\n    if args.subcommand == 'policy' and args.policy_cmd == 'create':\n        _do_identity_policy_create(args)\n    elif args.subcommand == 'policy' and args.policy_cmd == 'list':\n        _do_identity_policy_list(args)\n    elif args.subcommand == 'role' and args.role_cmd == 'create':\n        _do_identity_role_create(args)\n    elif args.subcommand == 'role' and args.role_cmd == 'list':\n        _do_identity_role_list(args)\n    else:\n        raise AssertionError(\n            '\"{}\" is not a valid subcommand of \"identity\"'.format(\n                args.subcommand))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _do_identity_policy_create(args):\n    signer = _read_signer(args.key)\n\n    txns = [_create_policy_txn(signer, args.name, args.rule)]\n\n    batch = _create_batch(signer, txns)\n\n    batch_list = BatchList(batches=[batch])\n\n    if args.output is not None:\n        try:\n            with open(args.output, 'wb') as batch_file:\n                batch_file.write(batch_list.SerializeToString())\n        except IOError as e:\n            raise CliException(\n                'Unable to write to batch file: {}'.format(str(e)))\n    elif args.url is not None:\n        rest_client = RestClient(args.url)\n        rest_client.send_batches(batch_list)\n        if args.wait and args.wait > 0:\n            batch_id = batch.header_signature\n            wait_time = 0\n            start_time = time.time()\n\n            while wait_time < args.wait:\n                statuses = rest_client.get_statuses(\n                    [batch_id],\n                    args.wait - int(wait_time))\n                wait_time = time.time() - start_time\n                if statuses[0]['status'] == 'COMMITTED':\n                    print(\n                        'Policy committed in {:.6} sec'.format(wait_time))\n                    return\n\n                # Wait a moment so as not to hammer the Rest Api\n                time.sleep(0.2)\n\n            print('Wait timed out! Policy was not committed...')\n            print('{:128.128}  {}'.format(\n                batch_id,\n                statuses[0]['status']))\n            exit(1)\n    else:\n        raise AssertionError('No target for create set.')", "response": "Executes the policy create subcommand."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist the current on - chain configuration values.", "response": "def _do_identity_role_list(args):\n    \"\"\"Lists the current on-chain configuration values.\n    \"\"\"\n    rest_client = RestClient(args.url)\n    state = rest_client.list_state(subtree=IDENTITY_NAMESPACE + _ROLE_PREFIX)\n\n    head = state['head']\n    state_values = state['data']\n    printable_roles = []\n    for state_value in state_values:\n        role_list = RoleList()\n        decoded = b64decode(state_value['data'])\n        role_list.ParseFromString(decoded)\n\n        for role in role_list.roles:\n            printable_roles.append(role)\n\n    printable_roles.sort(key=lambda r: r.name)\n\n    if args.format == 'default':\n        tty_width = tty.width()\n        for role in printable_roles:\n            # Set value width to the available terminal space, or the min width\n            width = tty_width - len(role.name) - 3\n            width = width if width > _MIN_PRINT_WIDTH else _MIN_PRINT_WIDTH\n            value = (role.policy_name[:width] + '...'\n                     if len(role.policy_name) > width\n                     else role.policy_name)\n            print('{}: {}'.format(role.name, value))\n    elif args.format == 'csv':\n        try:\n            writer = csv.writer(sys.stdout, quoting=csv.QUOTE_ALL)\n            writer.writerow(['KEY', 'VALUE'])\n            for role in printable_roles:\n                writer.writerow([role.name, role.policy_name])\n        except csv.Error:\n            raise CliException('Error writing CSV')\n    elif args.format == 'json' or args.format == 'yaml':\n        roles_snapshot = {\n            'head': head,\n            'roles': {role.name: role.policy_name\n                      for role in printable_roles}\n        }\n        if args.format == 'json':\n            print(json.dumps(roles_snapshot, indent=2, sort_keys=True))\n        else:\n            print(yaml.dump(roles_snapshot, default_flow_style=False)[0:-1])\n    else:\n        raise AssertionError('Unknown format {}'.format(args.format))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nread the given file as a hex key.", "response": "def _read_signer(key_filename):\n    \"\"\"Reads the given file as a hex key.\n\n    Args:\n        key_filename: The filename where the key is stored. If None,\n            defaults to the default key for the current user.\n\n    Returns:\n        Signer: the signer\n\n    Raises:\n        CliException: If unable to read the file.\n    \"\"\"\n    filename = key_filename\n    if filename is None:\n        filename = os.path.join(os.path.expanduser('~'),\n                                '.sawtooth',\n                                'keys',\n                                getpass.getuser() + '.priv')\n\n    try:\n        with open(filename, 'r') as key_file:\n            signing_key = key_file.read().strip()\n    except IOError as e:\n        raise CliException('Unable to read key file: {}'.format(str(e)))\n\n    try:\n        private_key = Secp256k1PrivateKey.from_hex(signing_key)\n    except ParseError as e:\n        raise CliException('Unable to read key in file: {}'.format(str(e)))\n\n    context = create_context('secp256k1')\n    crypto_factory = CryptoFactory(context)\n    return crypto_factory.new_signer(private_key)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _create_batch(signer, transactions):\n    txn_ids = [txn.header_signature for txn in transactions]\n    batch_header = BatchHeader(\n        signer_public_key=signer.get_public_key().as_hex(),\n        transaction_ids=txn_ids).SerializeToString()\n\n    return Batch(\n        header=batch_header,\n        header_signature=signer.sign(batch_header),\n        transactions=transactions)", "response": "Creates a batch from a list of transactions and a public key and signs\n   "}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a ExecutionContext object that will run a transaction against the given state hash.", "response": "def create_context(self, state_hash, base_contexts, inputs, outputs):\n        \"\"\"Create a ExecutionContext to run a transaction against.\n\n        Args:\n            state_hash: (str): Merkle root to base state on.\n            base_contexts (list of str): Context ids of contexts that will\n                have their state applied to make this context.\n            inputs (list of str): Addresses that can be read from.\n            outputs (list of str): Addresses that can be written to.\n        Returns:\n            context_id (str): the unique context_id of the session\n        \"\"\"\n\n        for address in inputs:\n            if not self.namespace_is_valid(address):\n                raise CreateContextException(\n                    \"Address or namespace {} listed in inputs is not \"\n                    \"valid\".format(address))\n        for address in outputs:\n            if not self.namespace_is_valid(address):\n                raise CreateContextException(\n                    \"Address or namespace {} listed in outputs is not \"\n                    \"valid\".format(address))\n\n        addresses_to_find = [add for add in inputs if len(add) == 70]\n\n        address_values, reads = self._find_address_values_in_chain(\n            base_contexts=base_contexts,\n            addresses_to_find=addresses_to_find)\n\n        context = ExecutionContext(\n            state_hash=state_hash,\n            read_list=inputs,\n            write_list=outputs,\n            base_context_ids=base_contexts)\n\n        contexts_asked_not_found = [cid for cid in base_contexts\n                                    if cid not in self._contexts]\n        if contexts_asked_not_found:\n            raise KeyError(\n                \"Basing a new context off of context ids {} \"\n                \"that are not in context manager\".format(\n                    contexts_asked_not_found))\n\n        context.create_initial(address_values)\n\n        self._contexts[context.session_id] = context\n\n        if reads:\n            context.create_prefetch(reads)\n            self._address_queue.put_nowait(\n                (context.session_id, state_hash, reads))\n        return context.session_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndelete the contexts from the ContextManager.", "response": "def delete_contexts(self, context_id_list):\n        \"\"\"Delete contexts from the ContextManager.\n\n        Args:\n            context_id_list (list): a list of context ids\n\n        Returns:\n            None\n\n        \"\"\"\n        for c_id in context_id_list:\n            if c_id in self._contexts:\n                del self._contexts[c_id]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef delete(self, context_id, address_list):\n\n        if context_id not in self._contexts:\n            return False\n\n        context = self._contexts[context_id]\n\n        for add in address_list:\n            if not self.address_is_valid(address=add):\n                raise AuthorizationException(address=add)\n\n        context.delete_direct(address_list)\n\n        return True", "response": "Delete the values associated with a list of addresses for a specific context."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ngetting the values associated with a list of addresses for a specific context.", "response": "def get(self, context_id, address_list):\n        \"\"\"Get the values associated with list of addresses, for a specific\n        context referenced by context_id.\n\n        Args:\n            context_id (str): the return value of create_context, referencing\n                a particular context.\n            address_list (list): a list of address strs\n\n        Returns:\n            values_list (list): a list of (address, value) tuples\n\n        Raises:\n            AuthorizationException: Raised when an address in address_list is\n                not authorized either by not being in the inputs for the\n                txn associated with this context, or it is under a namespace\n                but the characters that are under the namespace are not valid\n                address characters.\n        \"\"\"\n\n        if context_id not in self._contexts:\n            return []\n        for add in address_list:\n            if not self.address_is_valid(address=add):\n                raise AuthorizationException(address=add)\n\n        context = self._contexts[context_id]\n\n        addresses_in_ctx = [add for add in address_list if add in context]\n        addresses_not_in_ctx = list(set(address_list) - set(addresses_in_ctx))\n\n        values = context.get(addresses_in_ctx)\n        values_list = list(zip(addresses_in_ctx, values))\n        if addresses_not_in_ctx:\n            # Validate the addresses that won't be validated by a direct get on\n            # the context.\n            for address in addresses_not_in_ctx:\n                context.validate_read(address)\n            try:\n                address_values, reads = self._find_address_values_in_chain(\n                    base_contexts=[context_id],\n                    addresses_to_find=addresses_not_in_ctx)\n            except KeyError:\n                # This is in the exceptional case when a txn is in flight\n                # and so the context may not exist but the tp is asking\n                # about it.\n                return []\n\n            values_list.extend(address_values)\n\n            if reads:\n                tree = MerkleDatabase(self._database, context.merkle_root)\n                add_values = []\n                for add in reads:\n                    value = None\n                    try:\n                        value = tree.get(add)\n                    except KeyError:\n                        # The address is not in the radix tree/merkle tree\n                        pass\n                    add_values.append((add, value))\n                values_list.extend(add_values)\n\n            values_list.sort(key=lambda x: address_list.index(x[0]))\n\n        return values_list"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef add_execution_data(self, context_id, data):\n        if context_id not in self._contexts:\n            LOGGER.warning(\"Context_id not in contexts, %s\", context_id)\n            return False\n\n        context = self._contexts.get(context_id)\n        context.add_execution_data(data)\n        return True", "response": "Add data to the execution result."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nadd an execution event to the execution result.", "response": "def add_execution_event(self, context_id, event):\n        \"\"\"Within a context, append data to the execution result.\n\n        Args:\n            context_id (str): the context id returned by create_context\n            data_type (str): type of data to append\n            data (bytes): data to append\n\n        Returns:\n            (bool): True if the operation is successful, False if\n                the context_id doesn't reference a known context.\n        \"\"\"\n        if context_id not in self._contexts:\n            LOGGER.warning(\"Context_id not in contexts, %s\", context_id)\n            return False\n\n        context = self._contexts.get(context_id)\n        context.add_execution_event(event)\n        return True"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\napplies the given puts and deletes atomically.", "response": "def update(self, puts, deletes):\n        \"\"\"Applies the given puts and deletes atomically.\n\n        Args:\n            puts (:iterable:`tuple`): an iterable of key/value pairs to insert\n            deletes (:iterable:str:) an iterable of keys to delete\n        \"\"\"\n        with self._lmdb.begin(write=True, buffers=True) as txn:\n            cursor = txn.cursor(self._main_db)\n            # Process deletes first, to handle the case of new items replacing\n            # old index locations\n            for key in deletes:\n                if not cursor.set_key(key.encode()):\n                    # value doesn't exist\n                    continue\n\n                value = self._deserializer(bytes(cursor.value()))\n                cursor.delete()\n\n                for (index_db, index_key_fn) in self._indexes.values():\n                    index_keys = index_key_fn(value)\n                    index_cursor = txn.cursor(index_db)\n                    for idx_key in index_keys:\n                        if index_cursor.set_key(idx_key):\n                            index_cursor.delete()\n\n            # process all the inserts\n            for key, value in puts:\n                packed = self._serializer(value)\n\n                cursor.put(key.encode(), packed, overwrite=True)\n\n                for (index_db, index_key_fn) in self._indexes.values():\n                    index_keys = index_key_fn(value)\n                    index_cursor = txn.cursor(index_db)\n                    for idx_key in index_keys:\n                        index_cursor.put(idx_key, key.encode())\n\n        self.sync()"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns a list of keys in the database.", "response": "def keys(self, index=None):\n        \"\"\"Returns a list of keys in the database\n        \"\"\"\n        if index is not None and index not in self._indexes:\n            raise ValueError('Index {} does not exist'.format(index))\n\n        db = self._indexes[index][0] if index else self._main_db\n        with self._lmdb.begin(db=db) as txn:\n            return [\n                key.decode()\n                for key in txn.cursor().iternext(keys=True, values=False)\n            ]"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\npackaging up transactions into a batch and send them to the network.", "response": "def send(self, transactions):\n        \"\"\" Package up transactions into a batch and send them to the\n        network via the provided batch_sender.\n        :param transactions: list of transactions to package and broadcast.\n        :return: None\n        \"\"\"\n        txn_signatures = [txn.header_signature for txn in transactions]\n        header = BatchHeader(\n            signer_public_key=self._identity_signer.get_public_key().as_hex(),\n            transaction_ids=txn_signatures\n        ).SerializeToString()\n\n        signature = self._identity_signer.sign(header)\n        batch = Batch(\n            header=header,\n            transactions=transactions,\n            header_signature=signature)\n\n        self._batch_sender.send(batch)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef base_multinode_parser():\n    base_parser = ArgumentParser(add_help=False)\n\n    base_parser.add_argument(\n        'urls',\n        type=str,\n        nargs='+',\n        help=\"The URLs of the validator's REST APIs of interest, separated by\"\n        \" commas or spaces. (no default)\")\n    base_parser.add_argument(\n        '--users',\n        type=str,\n        action='append',\n        metavar='USERNAME[:PASSWORD]',\n        help='Specify the users to authorize requests, in the same order as '\n        'the URLs, separate by commas. Passing empty strings between commas '\n        'is supported.')\n\n    return base_parser", "response": "Creates a parser with arguments specific to sending HTTP requests to multiple REST APIs."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncreate a parser with arguments specific to sending an HTTP request to the REST API.", "response": "def base_http_parser():\n    \"\"\"Creates a parser with arguments specific to sending an HTTP request\n    to the REST API.\n\n    Returns:\n        {ArgumentParser}: Base parser with default HTTP args\n    \"\"\"\n    base_parser = ArgumentParser(add_help=False)\n\n    base_parser.add_argument(\n        '--url',\n        type=str,\n        help=\"identify the URL of the validator's REST API \"\n        \"(default: http://localhost:8008)\")\n    base_parser.add_argument(\n        '-u', '--user',\n        type=str,\n        metavar='USERNAME[:PASSWORD]',\n        help='specify the user to authorize request')\n\n    return base_parser"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef base_list_parser():\n    base_parser = ArgumentParser(add_help=False)\n\n    base_parser.add_argument(\n        '-F', '--format',\n        action='store',\n        default='default',\n        choices=['csv', 'json', 'yaml', 'default'],\n        help='choose the output format')\n\n    return base_parser", "response": "Creates a parser with arguments specific to formatting lists\n    of resources."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a parser with arguments specific to formatting a single resource.", "response": "def base_show_parser():\n    \"\"\"Creates a parser with arguments specific to formatting a\n    single resource.\n\n    Returns:\n        {ArgumentParser}: Base parser with default show args\n    \"\"\"\n    base_parser = ArgumentParser(add_help=False)\n\n    base_parser.add_argument(\n        '-k', '--key',\n        type=str,\n        help='show a single property from the block or header')\n    base_parser.add_argument(\n        '-F', '--format',\n        action='store',\n        default='yaml',\n        choices=['yaml', 'json'],\n        help='choose the output format (default: yaml)')\n\n    return base_parser"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns a list of tuples of block id and BlockStatus pairs.", "response": "def get_block_statuses(self, block_ids):\n        \"\"\"Returns a list of tuples of (block id, BlockStatus) pairs.\n        \"\"\"\n        try:\n            return [\n                (block_id.hex(),\n                 self._chain_controller.block_validation_result(\n                     block_id.hex()))\n                for block_id in block_ids\n            ]\n        except KeyError as key_error:\n            raise UnknownBlock(key_error.args[0])"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn a list of key value pairs.", "response": "def settings_get(self, block_id, settings):\n        '''Returns a list of key/value pairs (str, str).'''\n\n        block = self._get_blocks([block_id.hex()])[0]\n\n        block_header = BlockHeader()\n        block_header.ParseFromString(block.header)\n\n        try:\n            settings_view = self._settings_view_factory.create_settings_view(\n                block_header.state_root_hash)\n        except KeyError:\n            LOGGER.error(\n                'Settings from block %s requested, but root hash %s was '\n                'missing. Returning no setting values.',\n                block_id.hex(),\n                block_header.state_root_hash)\n            # The state root does not exist, which may indicate a pruned root\n            # from a dropped fork or an invalid state.\n            return []\n\n        result = []\n        for setting in settings:\n            try:\n                value = settings_view.get_setting(setting)\n            except KeyError:\n                # if the key is missing, leave it out of the response\n                continue\n\n            result.append((setting, value))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef state_get(self, block_id, addresses):\n        '''Returns a list of address/data pairs (str, bytes)'''\n        block = self._get_blocks([block_id.hex()])[0]\n        block_header = BlockHeader()\n        block_header.ParseFromString(block.header)\n\n        try:\n            state_view = self._state_view_factory.create_view(\n                block_header.state_root_hash)\n        except KeyError:\n            LOGGER.error(\n                'State from block %s requested, but root hash %s was missing. '\n                'Returning empty state.',\n                block_id.hex(),\n                block_header.state_root_hash)\n            # The state root does not exist, which may indicate a pruned root\n            # from a dropped fork or an invalid state.\n            return []\n\n        result = []\n\n        for address in addresses:\n            # a fully specified address\n            if len(address) == 70:\n                try:\n                    value = state_view.get(address)\n                except KeyError:\n                    # if the key is missing, leave it out of the response\n                    continue\n\n                result.append((address, value))\n                continue\n\n            # an address prefix\n            leaves = state_view.leaves(address)\n\n            for leaf in leaves:\n                result.append(leaf)\n\n        return result", "response": "Returns a list of address and data pairs ( str bytes )"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef add_settings_parser(subparsers, parent_parser):\n    # The following parser is for the settings subsection of commands.  These\n    # commands display information about the currently applied on-chain\n    # settings.\n\n    settings_parser = subparsers.add_parser(\n        'settings',\n        help='Displays on-chain settings',\n        description='Displays the values of currently active on-chain '\n                    'settings.')\n\n    settings_parsers = settings_parser.add_subparsers(\n        title='settings',\n        dest='settings_cmd')\n    settings_parsers.required = True\n\n    list_parser = settings_parsers.add_parser(\n        'list',\n        help='Lists the current keys and values of on-chain settings',\n        description='List the current keys and values of on-chain '\n                    'settings. The content can be exported to various '\n                    'formats for external consumption.'\n    )\n\n    list_parser.add_argument(\n        '--url',\n        type=str,\n        help=\"identify the URL of a validator's REST API\",\n        default='http://localhost:8008')\n\n    list_parser.add_argument(\n        '--filter',\n        type=str,\n        default='',\n        help='filters keys that begin with this value')\n\n    list_parser.add_argument(\n        '--format',\n        default='default',\n        choices=['default', 'csv', 'json', 'yaml'],\n        help='choose the output format')", "response": "Adds the args parser needed for the settings command and its\n    subcommands."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nlist the current on - chain configuration values.", "response": "def _do_settings_list(args):\n    \"\"\"Lists the current on-chain configuration values.\n    \"\"\"\n    rest_client = RestClient(args.url)\n    state = rest_client.list_state(subtree=SETTINGS_NAMESPACE)\n\n    prefix = args.filter\n\n    head = state['head']\n    state_values = state['data']\n    printable_settings = []\n    proposals_address = _key_to_address('sawtooth.settings.vote.proposals')\n    for state_value in state_values:\n        if state_value['address'] == proposals_address:\n            # This is completely internal setting and we won't list it here\n            continue\n\n        decoded = b64decode(state_value['data'])\n        setting = Setting()\n        setting.ParseFromString(decoded)\n\n        for entry in setting.entries:\n            if entry.key.startswith(prefix):\n                printable_settings.append(entry)\n\n    printable_settings.sort(key=lambda s: s.key)\n\n    if args.format == 'default':\n        tty_width = tty.width()\n        for setting in printable_settings:\n            # Set value width to the available terminal space, or the min width\n            width = tty_width - len(setting.key) - 3\n            width = width if width > _MIN_PRINT_WIDTH else _MIN_PRINT_WIDTH\n            value = (setting.value[:width] + '...'\n                     if len(setting.value) > width\n                     else setting.value)\n            print('{}: {}'.format(setting.key, value))\n    elif args.format == 'csv':\n        try:\n            writer = csv.writer(sys.stdout, quoting=csv.QUOTE_ALL)\n            writer.writerow(['KEY', 'VALUE'])\n            for setting in printable_settings:\n                writer.writerow([setting.key, setting.value])\n        except csv.Error:\n            raise CliException('Error writing CSV')\n    elif args.format == 'json' or args.format == 'yaml':\n        settings_snapshot = {\n            'head': head,\n            'settings': {setting.key: setting.value\n                         for setting in printable_settings}\n        }\n        if args.format == 'json':\n            print(json.dumps(settings_snapshot, indent=2, sort_keys=True))\n        else:\n            print(yaml.dump(settings_snapshot, default_flow_style=False)[0:-1])\n    else:\n        raise AssertionError('Unknown format {}'.format(args.format))"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating the state address for a given setting key.", "response": "def _key_to_address(key):\n    \"\"\"Creates the state address for a given setting key.\n    \"\"\"\n    key_parts = key.split('.', maxsplit=_MAX_KEY_PARTS - 1)\n    key_parts.extend([''] * (_MAX_KEY_PARTS - len(key_parts)))\n\n    return SETTINGS_NAMESPACE + ''.join(_short_hash(x) for x in key_parts)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _create_injector(self, injector):\n        if injector == \"block_info\":\n            block_info_injector = importlib.import_module(\n                \"sawtooth_validator.journal.block_info_injector\")\n\n            return block_info_injector.BlockInfoInjector(\n                self._state_view_factory, self._signer)\n\n        raise UnknownBatchInjectorError(injector)", "response": "Returns a new batch injector"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef on_block_received(self, block_id):\n        _libexec('chain_controller_on_block_received', self.pointer,\n                 ctypes.c_char_p(block_id.encode('utf-8')))", "response": "Called by the controller when a block is received."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd the arg parsers needed for the compare chains command.", "response": "def add_compare_chains_parser(subparsers, parent_parser):\n    \"\"\"Creates the arg parsers needed for the compare command.\n    \"\"\"\n    parser = subparsers.add_parser(\n        'compare-chains',\n        help='Compare chains from different nodes.',\n        description=(\n            'Compute and display information about how the chains at '\n            'different nodes differ.'\n        ),\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        epilog='''\nBy default, prints a table of summary data and a table of per-node data with\nthe following fields. Pass --tree for a fork graph.\n\nCOMMON ANCESTOR\n    The most recent block that all chains have in common.\n\nCOMMON HEIGHT\n    Let min_height := the minimum height of any chain across all nodes passed\n    in. COMMON HEIGHT = min_height.\n\nHEAD\n    The block id of the most recent block on a given chain.\n\nHEIGHT\n    The block number of the most recent block on a given chain.\n\nLAG\n    Let max_height := the maximum height of any chain across all nodes passed\n    in. LAG = max_height - HEIGHT for a given chain.\n\nDIVERG\n    Let common_ancestor_height := the height of the COMMON ANCESTOR.\n    DIVERG = HEIGHT - common_ancestor_height\n\n''',\n        parents=[parent_parser, base_multinode_parser()])\n\n    parser.add_argument(\n        '-l',\n        '--limit',\n        default=25,\n        type=int,\n        help='the number of blocks to request at a time',\n    )\n\n    parser.add_argument(\n        '--table',\n        action='store_true',\n        help='Print out a fork table for all nodes since the common ancestor.')\n\n    parser.add_argument(\n        '--tree',\n        action='store_true',\n        help='Print out a fork tree for all nodes since the common ancestor.')"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ncalculates and outputs comparison between all nodes on the network.", "response": "def do_compare_chains(args):\n    \"\"\"Calculates and outputs comparison between all nodes on the network.\"\"\"\n    urls = split_comma_append_args(args.urls)\n    users = split_comma_append_args(args.users)\n    clients = make_rest_apis(urls, users)\n\n    broken = []\n\n    chains, errors = get_chain_generators(clients, args.limit)\n    broken.extend(errors)\n    for node in errors:\n        print(\"Error connecting to node %d: %s\" % (node, urls[node]))\n    if not chains:\n        print(\"No nodes reporting\")\n        return\n\n    tails, errors = get_tails(chains)\n    broken.extend(errors)\n    for node in errors:\n        del chains[node]\n    for node in errors:\n        print(\"Failed to reach common height with node %d: %s\" % (\n            node, urls[node]))\n    if not chains:\n        print(\"Failed to get common height\")\n        return\n\n    graph, errors = build_fork_graph(chains, tails)\n    broken.extend(errors)\n    for node in errors:\n        print(\"Failed to reach common ancestor with node %d: %s\" % (\n            node, urls[node]))\n    if not graph:\n        print(\"Failed to build fork graph\")\n        return\n\n    # Transform tails and errors into the format expected by the print\n    # functions. Because errors can occur while building the graph, we need to\n    # remove the tails for those clients.\n    broken.sort()\n    node_id_map = get_node_id_map(broken, len(clients))\n    tails = list(map(\n        lambda item: item[1],\n        filter(\n            lambda item: item[0] not in broken,\n            sorted(tails.items()))))\n\n    if args.table:\n        print_table(graph, tails, node_id_map)\n\n    elif args.tree:\n        print_tree(graph, tails, node_id_map)\n\n    else:\n        print_summary(graph, tails, node_id_map)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nprinting out summary and per - node comparison data.", "response": "def print_summary(graph, tails, node_id_map):\n    \"\"\"Print out summary and per-node comparison data.\"\"\"\n    # Get comparison data\n    heads = get_heads(tails)\n    heights = get_heights(tails)\n    max_height = max(heights)\n    common_height, block_ids_at_common_height = get_common_height(tails)\n    lags = get_lags(heights, max_height)\n    common_ancestor = graph.root\n    divergences = get_divergences(heights, graph.root)\n\n    # Print summary info\n    col_1 = 8\n    col_n = 8\n    format_str = '{:<' + str(col_1) + '} ' + ('{:<' + str(col_n) + '} ') * 2\n    header = format_str.format(\"COMMON\", \"HEIGHT\", \"BLOCKS\")\n    print(header)\n    print(\"-\" * len(header))\n    print(format_str.format(\n        \"ANCESTOR\", common_ancestor.num, common_ancestor.ident[:col_n]))\n    print(format_str.format(\n        \"HEIGHT\", common_height, str(block_ids_at_common_height)))\n    print()\n\n    # Print per-node data\n    node_col_width = get_col_width_for_num(len(tails), len(\"NODE\"))\n    num_col_width = get_col_width_for_num(max_height, len(\"HEIGHT\"))\n    lag_col_width = get_col_width_for_num(max(lags), len(\"LAG\"))\n    diverg_col_width = get_col_width_for_num(max(divergences), len(\"DIVERG\"))\n\n    format_str = (\n        '{:<' + str(node_col_width) + '} '\n        '{:<8} '\n        '{:<' + str(num_col_width) + '} '\n        '{:<' + str(lag_col_width) + '} '\n        '{:<' + str(diverg_col_width) + '}'\n    )\n\n    header = format_str.format(\"NODE\", \"HEAD\", \"HEIGHT\", \"LAG\", \"DIVERG\")\n    print(header)\n    print('-' * len(header))\n\n    for i, _ in enumerate(tails):\n        print(format_str.format(\n            node_id_map[i],\n            heads[i],\n            heights[i],\n            lags[i],\n            divergences[i],\n        ))\n    print()"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef print_table(graph, tails, node_id_map):\n    node_count = len(tails)\n\n    # Get the width of the table columns\n    num_col_width = max(\n        floor(log(max(get_heights(tails)), 10)) + 1,\n        len(\"NUM\"))\n    node_col_width = max(\n        floor(log(node_count, 10)) + 1,\n        8)\n\n    # Construct the output format string\n    format_str = ''\n    format_str += '{:<' + str(num_col_width) + '} '\n    for _ in range(node_count):\n        format_str += '{:<' + str(node_col_width) + '} '\n\n    nodes_header = [\"NODE \" + str(node_id_map[i]) for i in range(node_count)]\n    header = format_str.format(\"NUM\", *nodes_header)\n    print(header)\n    print('-' * len(header))\n\n    prev_block_num = -1\n    node_list = [''] * node_count\n    for block_num, _, siblings in graph.walk():\n        if block_num != prev_block_num:\n            # Need to skip the first one\n            if prev_block_num != -1:\n                print(format_str.format(prev_block_num, *node_list))\n\n            node_list.clear()\n            node_list.extend([''] * node_count)\n            prev_block_num = block_num\n\n        for block_id, node_ids in siblings.items():\n            for node_id in node_ids:\n                node_list[node_id] = block_id[:8]\n\n    # Print the last one\n    print(format_str.format(prev_block_num, *node_list))", "response": "Print out a table of nodes and blocks they have at each common ancestor."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nprints out a tree of blocks starting from the common ancestor.", "response": "def print_tree(graph, tails, node_id_map):\n    \"\"\"Print out a tree of blocks starting from the common ancestor.\"\"\"\n    # Example:\n    # |\n    # | 5\n    # *  a {0, 1, 2, 3, 4}\n    # |\n    # | 6\n    # |\\\n    # * |  b {0, 1, 2, 3}\n    # | *  n {4}\n    # | |\n    # | | 7\n    # * |  c {0, 1, 2, 3}\n    # | *  o {4}\n    # | |\n    # | | 8\n    # |\\ \\\n    # * | |  i {2, 3}\n    # | * |  d {0, 1}\n    # | | *  p {4}\n    # | | |\n    # | | | 9\n    # * | |  j {2, 3}\n    # | * |  e {0, 1}\n    # | | *  q {4}\n    # | | |\n    # | | | 10\n    # * | |  k {2, 3}\n    # | * |  f {0, 1}\n    # | | *  r {4}\n    # | | |\n    # | | | 11\n    # |\\ \\ \\\n    # | | |\\ \\\n    # * | | | |    g {0}\n    # | * | | |    h {1}\n    # |   * | |    l {2}\n    # |   | * |    m {3}\n    # |   |   *    s {4}\n    # |  /   /\n    # | |  /\n    # | | | 12\n    # * | |   t {0}\n    # | * |   u {2}\n    # | | *   v {4}\n    # | |\n    # | | 13\n    # * |   w {0}\n    # | *   x {2}\n    # |\n    # | 14\n    # *   y {0}\n    # | 15\n    # *   z {0}\n\n    walker = graph.walk()\n    next_block_num, next_parent, next_siblings = next(walker)\n    prev_cliques = []\n\n    done = False\n    while not done:\n        cliques = {}\n        block_num = next_block_num\n\n        # Read all the cliques for this block number\n        try:\n            while block_num == next_block_num:\n                cliques[next_parent] = next_siblings\n                next_block_num, next_parent, next_siblings = next(walker)\n        except StopIteration:\n            # Do one last iteration after we've consumed the entire graph\n            done = True\n\n        print_cliques(prev_cliques, cliques, node_id_map)\n\n        print_block_num_row(block_num, prev_cliques, cliques)\n\n        print_splits(prev_cliques, cliques)\n\n        print_folds(prev_cliques, cliques)\n\n        prev_cliques = build_ordered_cliques(prev_cliques, cliques)\n\n    print_cliques(prev_cliques, [], node_id_map)"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\norders the new cliques based on the order of their ancestors in the previous iteration.", "response": "def build_ordered_cliques(cliques, next_cliques):\n    \"\"\"Order the new cliques based on the order of their ancestors in the\n    previous iteration.\"\"\"\n    def sort_key(clique):\n        return -len(clique[1])\n\n    if not cliques:\n        return list(sorted(\n            list(next_cliques.values())[0].items(),\n            key=sort_key))\n\n    ordered_cliques = []\n    for _, clique in enumerate(cliques):\n        parent, _ = clique\n\n        # If this fork continues\n        if parent in next_cliques:\n            # Sort the cliques in descending order of the size of the\n            # clique, so that the main chain tends to the left\n            ordered_cliques.extend(\n                sorted(next_cliques[parent].items(), key=sort_key))\n\n        # Else drop it\n\n    return ordered_cliques"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nprinting a row that removes the given column and shifts all the following columns.", "response": "def print_fold(column_to_fold, total_columns, skips):\n    \"\"\"Print a row that removes the given column and shifts all the following\n    columns.\"\"\"\n    format_str = '{:<2}' * (total_columns - 1)\n    cols = []\n    for i in range(column_to_fold):\n        # print(i)\n        if i in skips:\n            cols.append(\"  \")\n        else:\n            cols.append(\"| \")\n    for i in range(column_to_fold + 1, total_columns):\n        # print(i)\n        if i in skips:\n            cols.append(\"  \")\n        else:\n            cols.append(\" /\")\n    print(format_str.format(*cols))"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef print_block_num_row(block_num, cliques, next_cliques):\n    n_cliques = len(cliques)\n    if n_cliques == 0:\n        print('|  {}'.format(block_num))\n        return\n\n    def mapper(clique):\n        block_id, _ = clique\n        if block_id not in next_cliques:\n            return ' '\n        return '|'\n\n    format_str = '{:<' + str(n_cliques * 2) + '} {}'\n    branches = list(map(mapper, cliques))\n    for end in ('', block_num):\n        print(format_str.format(' '.join(branches), end))", "response": "Print out a row of padding and a row with the block number. Includes\n    the branches prior to this block number."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef print_cliques(cliques, next_cliques, node_id_map):\n    n_cliques = len(cliques)\n    format_str = '{:<' + str(n_cliques * 2) + '}  {} {}'\n    branches = ['|'] * len(cliques)\n    for i, clique in enumerate(cliques):\n        block_id, nodes = clique\n        print(format_str.format(\n            ' '.join(branches[:i] + ['*'] + branches[i + 1:]),\n            block_id[:8], format_siblings(nodes, node_id_map)))\n        if block_id not in next_cliques:\n            branches[i] = ' '", "response": "Print a '*' on each branch with its block id and the ids of the nodes\n    that have the block."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting shifts for new forks.", "response": "def print_splits(cliques, next_cliques):\n    \"\"\"Print shifts for new forks.\"\"\"\n    splits = 0\n    for i, clique in enumerate(cliques):\n        parent, _ = clique\n\n        # If this fork continues\n        if parent in next_cliques:\n            # If there is a new fork, print a split\n            if len(next_cliques[parent]) > 1:\n                print_split(i + splits, len(cliques) + splits)\n                splits += 1"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nprint a row that splits the given column into two columns while shifting all the following columns.", "response": "def print_split(column_to_split, total_columns):\n    \"\"\"Print a row that splits the given column into two columns while\n    shifting all the following columns.\"\"\"\n    out = \"\"\n    for _ in range(column_to_split):\n        out += \"| \"\n    out += \"|\\\\\"\n    for _ in range(column_to_split + 1, total_columns):\n        out += \" \\\\\"\n    print(out)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_tails(chains):\n\n    def get_num_of_oldest(blocks):\n        return blocks[0].num\n\n    # Get the first block from every chain\n    tails = {}\n    bad_chains = []\n    for i, chain in chains.items():\n        try:\n            tails[i] = [next(chain)]\n        except StopIteration:\n            bad_chains.append(i)\n\n    # Find the minimum block number between all chains\n    min_block_num = min(map(get_num_of_oldest, tails.values()))\n\n    # Walk all chains back to the minimum block number, adding blocks to the\n    # chain lists as we go\n    for i, chain in chains.items():\n        if i not in bad_chains:\n            tail = tails[i]\n            while get_num_of_oldest(tail) > min_block_num:\n                try:\n                    block = next(chain)\n                except StopIteration:\n                    bad_chains.append(i)\n                    break\n                tail.insert(0, block)\n\n    return tails, bad_chains", "response": "Returns a dictionary of lists of lists of lists of blocks from all chains that have communication problems."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns whether all the collections return equal values when called with key.", "response": "def _compare_across(collections, key):\n    \"\"\"Return whether all the collections return equal values when called with\n    `key`.\"\"\"\n    if len(collections) < 2:\n        return True\n    c0 = key(collections[0])\n    return all(c0 == key(c) for c in collections[1:])"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef build_fork_graph(chains, tails):\n    graph = ForkGraph()\n    bad_chains = []\n\n    # Add tails to the graph first\n    for i, tail in tails.items():\n        for block in reversed(tail):\n            graph.add_block(i, block)\n\n    # If we are already at the common ancestor, stop\n    if _compare_across(\n        [tail[0] for tail in tails.values()], key=lambda block: block.ident\n    ):\n        return graph, bad_chains\n\n    # Chains should now all be at the same height, so we can walk back\n    # to common ancestor\n    while True:\n        heads = []\n        for i, chain in chains.items():\n            if i not in bad_chains:\n                try:\n                    head = next(chain)\n                except StopIteration:\n                    bad_chains.append(i)\n                heads.append((i, head))\n\n        for i, block in heads:\n            graph.add_block(i, block)\n        if _compare_across(heads, key=lambda head: head[1].ident):\n            break\n\n    prune_unreporting_peers(graph, bad_chains)\n\n    return graph, bad_chains", "response": "Builds a fork graph from a set of chains and tails."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\njoins the identifier tuple with periods.", "response": "def _join(self, identifier, instance=None, tags=None):\n        \"\"\"\n        Join the identifier tuple with periods \".\", combine the arbitrary tags\n        with the base tags and the identifier tag, convert tags to \"tag=value\"\n        format, and then join everything with \",\".\n        \"\"\"\n        tag_list = []\n        if tags is not None:\n            tag_list.extend(tags.items())\n        tag_list.extend(self._base_tags)\n        return \".\".join(identifier) + \",\" + \",\".join(\n            \"{}={}\".format(k, v)\n            for k, v in tag_list\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_config_parser(subparsers, parent_parser):\n    parser = subparsers.add_parser(\n        'config',\n        help='Changes genesis block settings and create, view, and '\n        'vote on settings proposals',\n        description='Provides subcommands to change genesis block settings '\n                    'and to view, create, and vote on existing proposals.'\n    )\n\n    config_parsers = parser.add_subparsers(title=\"subcommands\",\n                                           dest=\"subcommand\")\n    config_parsers.required = True", "response": "Adds the arg parsers needed for the config command and its subcommands."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _do_config_proposal_create(args):\n    settings = [s.split('=', 1) for s in args.setting]\n\n    signer = _read_signer(args.key)\n\n    txns = [_create_propose_txn(signer, setting)\n            for setting in settings]\n\n    batch = _create_batch(signer, txns)\n\n    batch_list = BatchList(batches=[batch])\n\n    if args.output is not None:\n        try:\n            with open(args.output, 'wb') as batch_file:\n                batch_file.write(batch_list.SerializeToString())\n        except IOError as e:\n            raise CliException(\n                'Unable to write to batch file: {}'.format(str(e)))\n    elif args.sabre_output is not None:\n        for i, txn in enumerate(txns):\n            with open(\"{}-{}\".format(args.sabre_output, i), 'wb') as outfile:\n                outfile.write(txn.payload)\n    elif args.url is not None:\n        rest_client = RestClient(args.url)\n        rest_client.send_batches(batch_list)\n    else:\n        raise AssertionError('No target for create set.')", "response": "Executes the proposal create subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _do_config_proposal_list(args):\n\n    def _accept(candidate, public_key, prefix):\n        # Check to see if the first public key matches the given public key\n        # (if it is not None).  This public key belongs to the user that\n        # created it.\n        has_pub_key = (not public_key\n                       or candidate.votes[0].public_key == public_key)\n        has_prefix = candidate.proposal.setting.startswith(prefix)\n        return has_prefix and has_pub_key\n\n    candidates_payload = _get_proposals(RestClient(args.url))\n    candidates = [\n        c for c in candidates_payload.candidates\n        if _accept(c, args.public_key, args.filter)\n    ]\n\n    if args.format == 'default':\n        for candidate in candidates:\n            print('{}: {} => {}'.format(\n                candidate.proposal_id,\n                candidate.proposal.setting,\n                candidate.proposal.value))\n    elif args.format == 'csv':\n        writer = csv.writer(sys.stdout, quoting=csv.QUOTE_ALL)\n        writer.writerow(['PROPOSAL_ID', 'KEY', 'VALUE'])\n        for candidate in candidates:\n            writer.writerow([\n                candidate.proposal_id,\n                candidate.proposal.setting,\n                candidate.proposal.value])\n    elif args.format == 'json' or args.format == 'yaml':\n        candidates_snapshot = \\\n            {c.proposal_id: {c.proposal.setting: c.proposal.value}\n             for c in candidates}\n\n        if args.format == 'json':\n            print(json.dumps(candidates_snapshot, indent=2, sort_keys=True))\n        else:\n            print(yaml.dump(candidates_snapshot,\n                            default_flow_style=False)[0:-1])\n    else:\n        raise AssertionError('Unknown format {}'.format(args.format))", "response": "Executes the proposal list subcommand."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nexecuting the proposal vote subcommand.", "response": "def _do_config_proposal_vote(args):\n    \"\"\"Executes the 'proposal vote' subcommand.  Given a key file, a proposal\n    id and a vote value, it generates a batch of sawtooth_settings transactions\n    in a BatchList instance.  The BatchList is file or submitted to a\n    validator.\n    \"\"\"\n    signer = _read_signer(args.key)\n    rest_client = RestClient(args.url)\n\n    proposals = _get_proposals(rest_client)\n\n    proposal = None\n    for candidate in proposals.candidates:\n        if candidate.proposal_id == args.proposal_id:\n            proposal = candidate\n            break\n\n    if proposal is None:\n        raise CliException('No proposal exists with the given id')\n\n    for vote_record in proposal.votes:\n        if vote_record.public_key == signer.get_public_key().as_hex():\n            raise CliException(\n                'A vote has already been recorded with this signing key')\n\n    txn = _create_vote_txn(\n        signer,\n        args.proposal_id,\n        proposal.proposal.setting,\n        args.vote_value)\n    batch = _create_batch(signer, [txn])\n\n    batch_list = BatchList(batches=[batch])\n\n    rest_client.send_batches(batch_list)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates an individual sawtooth_settings transaction for the given setting key and value.", "response": "def _create_propose_txn(signer, setting_key_value):\n    \"\"\"Creates an individual sawtooth_settings transaction for the given\n    key and value.\n    \"\"\"\n    setting_key, setting_value = setting_key_value\n    nonce = hex(random.randint(0, 2**64))\n    proposal = SettingProposal(\n        setting=setting_key,\n        value=setting_value,\n        nonce=nonce)\n    payload = SettingsPayload(data=proposal.SerializeToString(),\n                              action=SettingsPayload.PROPOSE)\n\n    return _make_txn(signer, setting_key, payload)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef _create_vote_txn(signer, proposal_id, setting_key, vote_value):\n    if vote_value == 'accept':\n        vote_id = SettingVote.ACCEPT\n    else:\n        vote_id = SettingVote.REJECT\n\n    vote = SettingVote(proposal_id=proposal_id, vote=vote_id)\n    payload = SettingsPayload(data=vote.SerializeToString(),\n                              action=SettingsPayload.VOTE)\n\n    return _make_txn(signer, setting_key, payload)", "response": "Creates an individual sawtooth_settings transaction for voting on a particular setting key."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _make_txn(signer, setting_key, payload):\n    serialized_payload = payload.SerializeToString()\n    header = TransactionHeader(\n        signer_public_key=signer.get_public_key().as_hex(),\n        family_name='sawtooth_settings',\n        family_version='1.0',\n        inputs=_config_inputs(setting_key),\n        outputs=_config_outputs(setting_key),\n        dependencies=[],\n        payload_sha512=hashlib.sha512(serialized_payload).hexdigest(),\n        batcher_public_key=signer.get_public_key().as_hex()\n    ).SerializeToString()\n\n    return Transaction(\n        header=header,\n        header_signature=signer.sign(header),\n        payload=serialized_payload)", "response": "Creates and signs a sawtooth_settings transaction with a payload."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nnotifies that a new message was received from a peer.", "response": "def notify_peer_message(self, message, sender_id):\n        \"\"\"A new message was received from a peer\"\"\"\n        payload = message.SerializeToString()\n        self._notify(\n            \"consensus_notifier_notify_peer_message\",\n            payload,\n            len(payload),\n            sender_id,\n            len(sender_id))"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef notify_block_new(self, block):\n        payload = block.SerializeToString()\n        self._notify(\n            \"consensus_notifier_notify_block_new\", payload, len(payload))", "response": "Notify that a new block was received and passed initial consensus validation"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef notify_engine_activated(self, chain_head):\n        chain_head_bytes = chain_head.SerializeToString()\n        self._notify(\n            \"consensus_notifier_notify_engine_activated\",\n            chain_head_bytes,\n            len(chain_head_bytes))", "response": "Notify that the consensus engine has been activated."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef update(self, address, updater, prune=False):\n        '''\n        Walk to ADDRESS, creating nodes if necessary, and set the data\n        there to UPDATER(data).\n\n        Arguments:\n            address (str): the address to be updated\n        '''\n\n        node = self._get_or_create(address)\n\n        node.data = updater(node.data)\n\n        if prune:\n            node.children.clear()", "response": "Walk to ADDRESS and update the data of the node with the given updater."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nremoves all children and descendants below ADDRESS. Returns True if all children are removed False otherwise.", "response": "def prune(self, address):\n        '''\n        Remove all children (and descendants) below ADDRESS.\n\n        Arguments:\n            address (str): the address to be pruned\n        '''\n\n        try:\n            for step in self._walk_to_address(address):\n                node = step\n        except AddressNotInTree:\n            return\n\n        node.children.clear()"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nreturning an iterator over the node addresses and data for ADDRESS.", "response": "def walk(self, address):\n        '''\n        Returns a stream of pairs of node addresses and data, raising\n        AddressNotInTree if ADDRESS is not in the tree.\n\n        First the ancestors of ADDRESS (including itself) are yielded,\n        earliest to latest, and then the descendants of ADDRESS are\n        yielded in an unspecified order.\n\n        Arguments:\n            address (str): the address to be walked\n        '''\n\n        for step in self._walk_to_address(address):\n            node = step\n            yield node.address, node.data\n\n        to_process = deque()\n\n        to_process.extendleft(\n            node.children)\n\n        while to_process:\n            node = to_process.pop()\n\n            yield node.address, node.data\n\n            if node.children:\n                to_process.extendleft(\n                    node.children)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nreturn all predecessor transaction ids for a write of the provided address.", "response": "def find_write_predecessors(self, address):\n        \"\"\"Returns all predecessor transaction ids for a write of the provided\n        address.\n\n        Arguments:\n            address (str): the radix address\n\n        Returns: a set of transaction ids\n        \"\"\"\n        # A write operation must be preceded by:\n        #   - The \"enclosing writer\", which is the writer at the address or\n        #     the nearest writer higher (closer to the root) in the tree.\n        #   - The \"enclosing readers\", which are the readers at the address\n        #     or higher in the tree.\n        #   - The \"children writers\", which include all writers which are\n        #     lower in the tree than the address.\n        #   - The \"children readers\", which include all readers which are\n        #     lower in the tree than the address.\n        #\n        # The enclosing writer must be added as it may have modified a node\n        # which must not happen after the current write.\n        #\n        # Writers which are higher in the tree than the enclosing writer may\n        # have modified a node at or under the given address.  However, we do\n        # not need to include them here as they will have been considered a\n        # predecessor to the enclosing writer.\n        #\n        # Enclosing readers must be included.  Technically, we only need to add\n        # enclosing readers which occurred after the enclosing writer, since\n        # the readers preceding the writer will have been considered a\n        # predecessor of the enclosing writer.  However, with the current\n        # data structure we can not determine the difference between readers\n        # so we specify them all; this is mostly harmless as it will not change\n        # the eventual sort order generated by the scheduler.\n        #\n        # Children readers must be added, since their reads must happen prior\n        # to the write.\n\n        predecessors = set()\n\n        enclosing_writer = None\n\n        node_stream = self._tree.walk(address)\n\n        address_len = len(address)\n\n        # First, walk down from the root to the address, collecting all readers\n        # and updating the enclosing_writer if needed.\n\n        try:\n            for node_address, node in node_stream:\n                if node is not None:\n                    predecessors.update(node.readers)\n\n                    if node.writer is not None:\n                        enclosing_writer = node.writer\n\n                    if len(node_address) >= address_len:\n                        break\n\n        # If the address isn't on the tree, then there aren't any\n        # predecessors below the node to worry about (because there\n        # isn't anything at all), so return the predecessors that have\n        # already been collected.\n        except AddressNotInTree as err:\n            if err.match is not None:\n                return self.find_write_predecessors(err.match)\n\n            return predecessors\n\n        finally:\n            if enclosing_writer is not None:\n                predecessors.add(enclosing_writer)\n\n        # Next, descend down the tree starting at the address node and\n        # find all descendant readers and writers.\n\n        for _, node in node_stream:\n            if node is not None:\n                if node.writer is not None:\n                    predecessors.add(node.writer)\n\n                predecessors.update(node.readers)\n\n        return predecessors"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nadd a predecessor - successor relationship between one txn id and a set of predecessors.", "response": "def add_relationship(self, txn_id, predecessors):\n        \"\"\"Add a predecessor-successor relationship between one txn id and\n        a set of predecessors.\n\n        Args:\n            txn_id (str): The transaction id of the transaction.\n            predecessors (set): The transaction ids of the\n                transaction's predecessors\n\n        Returns:\n            None\n        \"\"\"\n\n        all_pred = set(predecessors)\n        for pred in predecessors:\n            all_pred.update(self._predecessors_by_id[pred])\n\n        self._predecessors_by_id[txn_id] = all_pred"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning whether the predecessor is a predecessor or a predecessor or a predecessor...of any of the others.", "response": "def is_predecessor_of_other(self, predecessor, others):\n        \"\"\"Returns whether the predecessor is a predecessor or a predecessor\n        of a predecessor...of any of the others.\n\n        Args:\n            predecessor (str): The txn id of the predecessor.\n            others (list(str)): The txn id of the successor.\n\n        Returns:\n            (bool)\n\n        \"\"\"\n\n        return any(predecessor in self._predecessors_by_id[o] for o in others)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nuse the predecessor tree to find dependencies based on inputs.", "response": "def _find_input_dependencies(self, inputs):\n        \"\"\"Use the predecessor tree to find dependencies based on inputs.\n\n        Returns: A list of transaction ids.\n        \"\"\"\n        dependencies = []\n        for address in inputs:\n            dependencies.extend(\n                self._predecessor_tree.find_read_predecessors(address))\n        return dependencies"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _find_output_dependencies(self, outputs):\n        dependencies = []\n        for address in outputs:\n            dependencies.extend(\n                self._predecessor_tree.find_write_predecessors(address))\n        return dependencies", "response": "Find the dependencies of the outputs."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _get_contexts_for_squash(self, batch_signature):\n\n        batch = self._batches_by_id[batch_signature].batch\n        index = self._batches.index(batch)\n        contexts = []\n        txns_added_predecessors = []\n        for b in self._batches[index::-1]:\n            batch_is_valid = True\n            contexts_from_batch = []\n            for txn in b.transactions[::-1]:\n                result = self._txn_results[txn.header_signature]\n                if not result.is_valid:\n                    batch_is_valid = False\n                    break\n                else:\n                    txn_id = txn.header_signature\n                    if txn_id not in txns_added_predecessors:\n                        txns_added_predecessors.append(\n                            self._txn_predecessors[txn_id])\n                        contexts_from_batch.append(result.context_id)\n            if batch_is_valid:\n                contexts.extend(contexts_from_batch)\n\n        return contexts", "response": "Given a batch signature return a list of all base contexts that have not been removed from the scheduler."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ndecide if the possible_successor should be replayed.", "response": "def _is_txn_to_replay(self, txn_id, possible_successor, already_seen):\n        \"\"\"Decide if possible_successor should be replayed.\n\n        Args:\n            txn_id (str): Id of txn in failed batch.\n            possible_successor (str): Id of txn to possibly replay.\n            already_seen (list): A list of possible_successors that have\n                been replayed.\n\n        Returns:\n            (bool): If the possible_successor should be replayed.\n        \"\"\"\n\n        is_successor = self._is_predecessor_of_possible_successor(\n            txn_id,\n            possible_successor)\n        in_different_batch = not self._is_in_same_batch(txn_id,\n                                                        possible_successor)\n        has_not_been_seen = possible_successor not in already_seen\n\n        return is_successor and in_different_batch and has_not_been_seen"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _remove_subsequent_result_because_of_batch_failure(self, sig):\n\n        batch = self._batches_by_txn_id[sig]\n        seen = []\n        for txn in batch.transactions:\n            txn_id = txn.header_signature\n            for poss_successor in self._scheduled.copy():\n                if not self.is_transaction_in_schedule(poss_successor):\n                    continue\n\n                if self._is_txn_to_replay(txn_id, poss_successor, seen):\n                    if self._txn_has_result(poss_successor):\n                        del self._txn_results[poss_successor]\n                        self._scheduled.remove(poss_successor)\n                        self._txns_available[poss_successor] = \\\n                            self._transactions[poss_successor]\n                    else:\n                        self._outstanding.add(poss_successor)\n                    seen.append(poss_successor)", "response": "Remove transactions from scheduled and txn_results for successors of txns in a failed batch."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nset the first batch id that doesn t have all results.", "response": "def _set_least_batch_id(self, txn_signature):\n        \"\"\"Set the first batch id that doesn't have all results.\n\n        Args:\n            txn_signature (str): The txn identifier of the transaction with\n                results being set.\n\n        \"\"\"\n\n        batch = self._batches_by_txn_id[txn_signature]\n\n        least_index = self._index_of_batch(\n            self._batches_by_id[self._least_batch_id_wo_results].batch)\n\n        current_index = self._index_of_batch(batch)\n        all_prior = False\n\n        if current_index <= least_index:\n            return\n            # Test to see if all batches from the least_batch to\n            # the prior batch to the current batch have results.\n        if all(\n                all(t.header_signature in self._txn_results\n                    for t in b.transactions)\n                for b in self._batches[least_index:current_index]):\n            all_prior = True\n        if not all_prior:\n            return\n        possible_least = self._batches[current_index].header_signature\n        # Find the first batch from the current batch on, that doesn't have\n        # all results.\n        for b in self._batches[current_index:]:\n            if not all(t.header_signature in self._txn_results\n                       for t in b.transactions):\n                possible_least = b.header_signature\n                break\n        self._least_batch_id_wo_results = possible_least"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nreturn whether the transaction is in a valid batch.", "response": "def _txn_is_in_valid_batch(self, txn_id):\n        \"\"\"Returns whether the transaction is in a valid batch.\n\n        Args:\n            txn_id (str): The transaction header signature.\n\n        Returns:\n            (bool): True if the txn's batch is valid, False otherwise.\n        \"\"\"\n\n        batch = self._batches_by_txn_id[txn_id]\n\n        # Return whether every transaction in the batch with a\n        # transaction result is valid\n        return all(\n            self._txn_results[sig].is_valid\n            for sig in set(self._txn_results).intersection(\n                (txn.header_signature for txn in batch.transactions)))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef do_keygen(args):\n    if args.key_name is not None:\n        key_name = args.key_name\n    else:\n        key_name = 'validator'\n\n    key_dir = get_key_dir()\n\n    if not os.path.exists(key_dir):\n        raise CliException(\"Key directory does not exist: {}\".format(key_dir))\n\n    priv_filename = os.path.join(key_dir, key_name + '.priv')\n    pub_filename = os.path.join(key_dir, key_name + '.pub')\n\n    if not args.force:\n        file_exists = False\n        for filename in [priv_filename, pub_filename]:\n            if os.path.exists(filename):\n                file_exists = True\n                print('file exists: {}'.format(filename), file=sys.stderr)\n        if file_exists:\n            raise CliException(\n                'files exist, rerun with --force to overwrite existing files')\n\n    context = create_context('secp256k1')\n\n    private_key = context.new_random_private_key()\n    public_key = context.get_public_key(private_key)\n\n    try:\n        priv_exists = os.path.exists(priv_filename)\n        with open(priv_filename, 'w') as priv_fd:\n            if not args.quiet:\n                if priv_exists:\n                    print('overwriting file: {}'.format(priv_filename))\n                else:\n                    print('writing file: {}'.format(priv_filename))\n            priv_fd.write(private_key.as_hex())\n            priv_fd.write('\\n')\n            # Get the uid and gid of the key directory\n            keydir_info = os.stat(key_dir)\n            keydir_gid = keydir_info.st_gid\n            keydir_uid = keydir_info.st_uid\n            # Set user and group on keys to the user/group of the key directory\n            os.chown(priv_filename, keydir_uid, keydir_gid)\n            # Set the private key u+rw g+r\n            os.chmod(priv_filename, 0o640)\n\n        pub_exists = os.path.exists(pub_filename)\n        with open(pub_filename, 'w') as pub_fd:\n            if not args.quiet:\n                if pub_exists:\n                    print('overwriting file: {}'.format(pub_filename))\n                else:\n                    print('writing file: {}'.format(pub_filename))\n            pub_fd.write(public_key.as_hex())\n            pub_fd.write('\\n')\n            # Set user and group on keys to the user/group of the key directory\n            os.chown(pub_filename, keydir_uid, keydir_gid)\n            # Set the public key u+rw g+r o+r\n            os.chmod(pub_filename, 0o644)\n\n    except IOError as ioe:\n        raise CliException('IOError: {}'.format(str(ioe)))", "response": "Executes the key generation operation."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef size():\n    try:\n        assert os != 'nt' and sys.stdout.isatty()\n        rows, columns = os.popen('stty size', 'r').read().split()\n    except (AssertionError, AttributeError, ValueError):\n        # in case of failure, use dimensions of a full screen 13\" laptop\n        rows, columns = DEFAULT_HEIGHT, DEFAULT_WIDTH\n\n    return int(rows), int(columns)", "response": "Determines the height and width of the console window\n           "}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_peers(self, connection_id):\n        with self._lock:\n            # Needs to actually be the list of advertised endpoints of\n            # our peers\n            peer_endpoints = list(self._peers.values())\n            if self._endpoint:\n                peer_endpoints.append(self._endpoint)\n            peers_response = GetPeersResponse(peer_endpoints=peer_endpoints)\n            try:\n                # Send a one_way message because the connection will be closed\n                # if this is a temp connection.\n                self._network.send(\n                    validator_pb2.Message.GOSSIP_GET_PEERS_RESPONSE,\n                    peers_response.SerializeToString(),\n                    connection_id,\n                    one_way=True)\n            except ValueError:\n                LOGGER.debug(\"Connection disconnected: %s\", connection_id)", "response": "Sends a message containing our peers to the specified connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds candidate endpoints to the list of endpoints to be used to attempt to peer with.", "response": "def add_candidate_peer_endpoints(self, peer_endpoints):\n        \"\"\"Adds candidate endpoints to the list of endpoints to\n        attempt to peer with.\n\n        Args:\n            peer_endpoints ([str]): A list of public uri's which the\n                validator can attempt to peer with.\n        \"\"\"\n        if self._topology:\n            self._topology.add_candidate_peer_endpoints(peer_endpoints)\n        else:\n            LOGGER.debug(\"Could not add peer endpoints to topology. \"\n                         \"ConnectionManager does not exist.\")"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_peers_public_keys(self):\n        with self._lock:\n            # Use a generator inside the list comprehension to filter out None\n            # values in a single pass\n            return [key for key\n                    in (self._network.connection_id_to_public_key(peer)\n                        for peer\n                        in copy.copy(self._peers))\n                    if key is not None]", "response": "Returns the list of public keys for all peers."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nregister a new peer with the specified connection_id.", "response": "def register_peer(self, connection_id, endpoint):\n        \"\"\"Registers a connected connection_id.\n\n        Args:\n            connection_id (str): A unique identifier which identifies an\n                connection on the network server socket.\n            endpoint (str): The publically reachable endpoint of the new\n                peer\n        \"\"\"\n        with self._lock:\n            if len(self._peers) < self._maximum_peer_connectivity:\n                self._peers[connection_id] = endpoint\n                self._topology.set_connection_status(connection_id,\n                                                     PeerStatus.PEER)\n                LOGGER.debug(\"Added connection_id %s with endpoint %s, \"\n                             \"connected identities are now %s\",\n                             connection_id, endpoint, self._peers)\n            else:\n                raise PeeringException(\n                    \"At maximum configured number of peers: {} \"\n                    \"Rejecting peering request from {}.\".format(\n                        self._maximum_peer_connectivity,\n                        endpoint))\n\n        public_key = self.peer_to_public_key(connection_id)\n        if public_key:\n            self._consensus_notifier.notify_peer_connected(public_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unregister_peer(self, connection_id):\n        public_key = self.peer_to_public_key(connection_id)\n        if public_key:\n            self._consensus_notifier.notify_peer_disconnected(public_key)\n\n        with self._lock:\n            if connection_id in self._peers:\n                del self._peers[connection_id]\n                LOGGER.debug(\"Removed connection_id %s, \"\n                             \"connected identities are now %s\",\n                             connection_id, self._peers)\n                self._topology.set_connection_status(connection_id,\n                                                     PeerStatus.TEMP)\n            else:\n                LOGGER.warning(\"Connection unregister failed as connection \"\n                               \"was not registered: %s\",\n                               connection_id)", "response": "Removes a connection from the registry."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef send(self, message_type, message, connection_id, one_way=False):\n        try:\n            self._network.send(message_type, message, connection_id,\n                               one_way=one_way)\n        except ValueError:\n            LOGGER.debug(\"Connection %s is no longer valid. \"\n                         \"Removing from list of peers.\",\n                         connection_id)\n            if connection_id in self._peers:\n                del self._peers[connection_id]", "response": "Sends a message via the network."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nbroadcasts a gossip message to all peers unless they are in the excluded list.", "response": "def broadcast(self, gossip_message, message_type, exclude=None):\n        \"\"\"Broadcast gossip messages.\n\n        Broadcast the message to all peers unless they are in the excluded\n        list.\n\n        Args:\n            gossip_message: The message to be broadcast.\n            message_type: Type of the message.\n            exclude: A list of connection_ids that should be excluded from this\n                broadcast.\n        \"\"\"\n        with self._lock:\n            if exclude is None:\n                exclude = []\n            for connection_id in self._peers.copy():\n                if connection_id not in exclude and \\\n                        self._network.is_connection_handshake_complete(\n                            connection_id):\n                    self.send(\n                        message_type,\n                        gossip_message.SerializeToString(),\n                        connection_id,\n                        one_way=True)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef add_candidate_peer_endpoints(self, peer_endpoints):\n        with self._lock:\n            for endpoint in peer_endpoints:\n                if endpoint not in self._candidate_peer_endpoints:\n                    self._candidate_peer_endpoints.append(endpoint)", "response": "Adds candidate endpoints to the list of endpoints to\nCOOKIE if they are not already in the list."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect_success(self, connection_id):\n        endpoint = self._network.connection_id_to_endpoint(connection_id)\n        endpoint_info = self._temp_endpoints.get(endpoint)\n\n        LOGGER.debug(\"Endpoint has completed authorization: %s (id: %s)\",\n                     endpoint,\n                     connection_id)\n        if endpoint_info is None:\n            LOGGER.debug(\"Received unknown endpoint: %s\", endpoint)\n\n        elif endpoint_info.status == EndpointStatus.PEERING:\n            self._connect_success_peering(connection_id, endpoint)\n\n        elif endpoint_info.status == EndpointStatus.TOPOLOGY:\n            self._connect_success_topology(connection_id)\n\n        else:\n            LOGGER.debug(\"Endpoint has unknown status: %s\", endpoint)\n\n        with self._lock:\n            if endpoint in self._temp_endpoints:\n                del self._temp_endpoints[endpoint]", "response": "Called when a connection is successfully established."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef prepare_vec_result(pointer_type=ctypes.c_uint8):\n    return (\n        ctypes.POINTER(pointer_type)(),\n        ctypes.c_size_t(0),\n        ctypes.c_size_t(0),\n    )", "response": "Returns pair of byte pointer and size value for use as return parameters\n    in a LIBRARY call"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef drop(self):\n        if self._ptr:\n            LIBRARY.call(self._drop_ffi_fn, self._ptr)\n            self._ptr = None", "response": "Explicitly drop this pointer."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the default PathConfig as calculated based on the environment variables SAWTOOTH_HOME SAWTOOTH_LOGS SAWTOOTH_DATA and operating system.", "response": "def get_default_path_config():\n    \"\"\"Returns the default PathConfig as calculated based on SAWTOOTH_HOME\n    (if set) and operating system.\n    \"\"\"\n    if 'SAWTOOTH_HOME' in os.environ:\n        home_dir = os.environ['SAWTOOTH_HOME']\n        return PathConfig(\n            config_dir=os.path.join(home_dir, 'etc'),\n            log_dir=os.path.join(home_dir, 'logs'),\n            data_dir=os.path.join(home_dir, 'data'),\n            key_dir=os.path.join(home_dir, 'keys'),\n            policy_dir=os.path.join(home_dir, 'policy'))\n\n    if os.name == 'nt':\n        # Paths appropriate for Windows.\n        base_dir = \\\n            os.path.dirname(os.path.dirname(os.path.abspath(sys.argv[0])))\n        return PathConfig(\n            config_dir=os.path.join(base_dir, 'conf'),\n            log_dir=os.path.join(base_dir, 'logs'),\n            data_dir=os.path.join(base_dir, 'data'),\n            key_dir=os.path.join(base_dir, 'conf', 'keys'),\n            policy_dir=os.path.join(base_dir, 'policy'))\n\n    # Paths appropriate for modern Linux distributions.\n    return PathConfig(\n        config_dir='/etc/sawtooth',\n        log_dir='/var/log/sawtooth',\n        data_dir='/var/lib/sawtooth',\n        key_dir='/etc/sawtooth/keys',\n        policy_dir='/etc/sawtooth/policy')"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a TOML file from the filesystem.", "response": "def load_toml_path_config(filename):\n    \"\"\"Returns a PathConfig created by loading a TOML file from the\n    filesystem.\n    \"\"\"\n    if not os.path.exists(filename):\n        LOGGER.info(\n            \"Skipping path loading from non-existent config file: %s\",\n            filename)\n        return PathConfig()\n\n    LOGGER.info(\"Loading path information from config: %s\", filename)\n\n    try:\n        with open(filename) as fd:\n            raw_config = fd.read()\n    except IOError as e:\n        raise LocalConfigurationError(\n            \"Unable to load path configuration file: {}\".format(str(e)))\n\n    toml_config = toml.loads(raw_config)\n\n    invalid_keys = set(toml_config.keys()).difference(\n        ['data_dir', 'key_dir', 'log_dir', 'policy_dir'])\n    if invalid_keys:\n        raise LocalConfigurationError(\"Invalid keys in path config: {}\".format(\n            \", \".join(sorted(list(invalid_keys)))))\n\n    config = PathConfig(\n        config_dir=None,\n        data_dir=toml_config.get('data_dir', None),\n        key_dir=toml_config.get('key_dir', None),\n        log_dir=toml_config.get('log_dir', None),\n        policy_dir=toml_config.get('policy_dir', None)\n    )\n\n    return config"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ngiving a list of PathConfig objects and a config_dir_override returns a new PathConfig object that is merged with the existing ones.", "response": "def merge_path_config(configs, config_dir_override):\n    \"\"\"\n    Given a list of PathConfig objects, merges them into a single PathConfig,\n    giving priority in the order of the configs (first has highest priority).\n    \"\"\"\n    config_dir = None\n    log_dir = None\n    data_dir = None\n    key_dir = None\n    policy_dir = None\n\n    for config in reversed(configs):\n        if config.config_dir is not None:\n            config_dir = config.config_dir\n        if config.log_dir is not None:\n            log_dir = config.log_dir\n        if config.data_dir is not None:\n            data_dir = config.data_dir\n        if config.key_dir is not None:\n            key_dir = config.key_dir\n        if config.policy_dir is not None:\n            policy_dir = config.policy_dir\n\n    if config_dir_override is not None:\n        config_dir = config_dir_override\n\n    return PathConfig(\n        config_dir=config_dir,\n        log_dir=log_dir,\n        data_dir=data_dir,\n        key_dir=key_dir,\n        policy_dir=policy_dir)"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ndetermining if the system needs a genesis block.", "response": "def requires_genesis(self):\n        \"\"\"\n        Determines if the system should be put in genesis mode\n\n        Returns:\n            bool: return whether or not a genesis block is required to be\n                generated.\n\n        Raises:\n            InvalidGenesisStateError: raises this error if there is invalid\n                combination of the following: genesis.batch, existing chain\n                head, and block chain id.\n        \"\"\"\n\n        genesis_file = os.path.join(self._data_dir, 'genesis.batch')\n        has_genesis_batches = Path(genesis_file).is_file()\n        LOGGER.debug('genesis_batch_file: %s',\n                     genesis_file if has_genesis_batches else 'not found')\n\n        chain_head = self._block_store.chain_head\n        has_chain_head = chain_head is not None\n        if has_chain_head:\n            LOGGER.debug('chain_head: %s', chain_head)\n\n        block_chain_id = self._chain_id_manager.get_block_chain_id()\n        is_genesis_node = block_chain_id is None\n        LOGGER.debug(\n            'block_chain_id: %s',\n            block_chain_id if not is_genesis_node else 'not yet specified')\n\n        if has_genesis_batches and has_chain_head:\n            raise InvalidGenesisStateError(\n                'Cannot have a genesis_batch_file and an existing chain')\n\n        if has_genesis_batches and not is_genesis_node:\n            raise InvalidGenesisStateError(\n                'Cannot have a genesis_batch_file and join an existing network'\n            )\n\n        if not has_genesis_batches and not has_chain_head:\n            LOGGER.info('No chain head and not the genesis node: '\n                        'starting in peering mode')\n\n        return has_genesis_batches and not has_chain_head and is_genesis_node"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef start(self, on_done):\n        genesis_file = os.path.join(self._data_dir, 'genesis.batch')\n        try:\n            with open(genesis_file, 'rb') as batch_file:\n                genesis_data = genesis_pb2.GenesisData()\n                genesis_data.ParseFromString(batch_file.read())\n            LOGGER.info('Producing genesis block from %s', genesis_file)\n        except IOError:\n            raise InvalidGenesisStateError(\n                \"Genesis File {} specified, but unreadable\".format(\n                    genesis_file))\n\n        initial_state_root = self._context_manager.get_first_root()\n\n        genesis_batches = [batch for batch in genesis_data.batches]\n        if genesis_batches:\n            scheduler = SerialScheduler(\n                self._context_manager.get_squash_handler(),\n                initial_state_root,\n                always_persist=True)\n\n            LOGGER.debug('Adding %s batches', len(genesis_data.batches))\n            for batch in genesis_data.batches:\n                scheduler.add_batch(batch)\n\n            self._transaction_executor.execute(scheduler)\n\n            scheduler.finalize()\n            scheduler.complete(block=True)\n\n        txn_receipts = []\n        state_hash = initial_state_root\n        for batch in genesis_batches:\n            result = scheduler.get_batch_execution_result(\n                batch.header_signature)\n            if result is None or not result.is_valid:\n                raise InvalidGenesisStateError(\n                    'Unable to create genesis block, due to batch {}'\n                    .format(batch.header_signature))\n            if result.state_hash is not None:\n                state_hash = result.state_hash\n\n            txn_results = scheduler.get_transaction_execution_results(\n                batch.header_signature)\n            txn_receipts += self._make_receipts(txn_results)\n\n        settings_view = SettingsView(\n            self._state_view_factory.create_view(state_hash))\n        name = settings_view.get_setting('sawtooth.consensus.algorithm.name')\n        version = settings_view.get_setting(\n            'sawtooth.consensus.algorithm.version')\n        if name is None or version is None:\n            raise LocalConfigurationError(\n                'Unable to start validator; sawtooth.consensus.algorithm.name '\n                'and sawtooth.consensus.algorithm.version must be set in the '\n                'genesis block.')\n\n        LOGGER.debug('Produced state hash %s for genesis block.', state_hash)\n\n        block_builder = self._generate_genesis_block()\n        block_builder.add_batches(genesis_batches)\n        block_builder.set_state_hash(state_hash)\n\n        block_publisher = self._get_block_publisher(initial_state_root)\n        if not block_publisher.initialize_block(block_builder.block_header):\n            LOGGER.error('Consensus refused to initialize consensus block.')\n            raise InvalidGenesisConsensusError(\n                'Consensus refused to initialize genesis block.')\n\n        if not block_publisher.finalize_block(block_builder.block_header):\n            LOGGER.error('Consensus refused to finalize genesis block.')\n            raise InvalidGenesisConsensusError(\n                'Consensus refused to finalize genesis block.')\n\n        self._sign_block(block_builder)\n\n        block = block_builder.build_block()\n\n        blkw = BlockWrapper(block=block)\n\n        LOGGER.info('Genesis block created: %s', blkw)\n\n        self._block_manager.put([blkw.block])\n        self._block_manager.persist(blkw.identifier, \"commit_store\")\n\n        self._txn_receipt_store.chain_update(block, txn_receipts)\n        self._chain_id_manager.save_block_chain_id(block.header_signature)\n\n        LOGGER.debug('Deleting genesis data.')\n        os.remove(genesis_file)\n\n        if on_done is not None:\n            on_done()", "response": "Starts the genesis block creation process."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _get_block_publisher(self, state_hash):\n        state_view = self._state_view_factory.create_view(state_hash)\n        try:\n            class BatchPublisher:\n                def send(self, transactions):\n                    # Consensus implementations are expected to have handling\n                    # in place for genesis operation. This should includes\n                    # adding any authorization and registrations required\n                    # for the genesis node to the Genesis Batch list and\n                    # detecting validation of the Genesis Block and handle it\n                    # correctly. Batch publication is not allowed during\n                    # genesis operation since there is no network to validate\n                    # the batch yet.\n                    raise InvalidGenesisConsensusError(\n                        'Consensus cannot send transactions during genesis.')\n\n            consensus = ConsensusFactory.get_configured_consensus_module(\n                NULL_BLOCK_IDENTIFIER,\n                state_view)\n            return consensus.BlockPublisher(\n                BlockCache(self._block_store),\n                state_view_factory=self._state_view_factory,\n                batch_publisher=BatchPublisher(),\n                data_dir=self._data_dir,\n                config_dir=self._config_dir,\n                validator_id=self._identity_signer.get_public_key().as_hex())\n        except UnknownConsensusModuleError as e:\n            raise InvalidGenesisStateError(e)", "response": "Returns the block publisher based on the consensus module set by the state_hash."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _generate_genesis_block(self):\n        genesis_header = block_pb2.BlockHeader(\n            block_num=0,\n            previous_block_id=NULL_BLOCK_IDENTIFIER,\n            signer_public_key=self._identity_signer.get_public_key().as_hex())\n\n        return BlockBuilder(genesis_header)", "response": "Returns a blocker wrapper with the basics of the block header in place\n       "}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _sign_block(self, block):\n        block_header = block.block_header\n        header_bytes = block_header.SerializeToString()\n        signature = self._identity_signer.sign(header_bytes)\n        block.set_signature(signature)\n        return block", "response": "Signs the block with the identity signer"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _purge_expired(self):\n        time_horizon = time.time() - self._keep_time\n        new_cache = {}\n        for (k, v) in self._cache.items():\n            if v.timestamp > time_horizon:\n                new_cache[k] = v\n        self._cache = new_cache", "response": "Remove all expired entries from the cache."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ndetermine if there is a log config in the config directory and returns it.", "response": "def _get_config():\n    \"\"\"Determines if there is a log config in the config directory\n       and returns it. If it does not exist, return None.\n\n    Returns:\n        log_config (dict): The dictionary to pass to logging.config.dictConfig\n    \"\"\"\n    conf_file = os.path.join(_get_config_dir(), 'log_config.toml')\n    if os.path.exists(conf_file):\n        with open(conf_file) as fd:\n            raw_config = fd.read()\n        log_config = toml.loads(raw_config)\n        return log_config\n\n    conf_file = os.path.join(_get_config_dir(), 'log_config.yaml')\n    if os.path.exists(conf_file):\n        with open(conf_file) as fd:\n            raw_config = fd.read()\n        log_config = yaml.safe_load(raw_config)\n        return log_config\n\n    return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nadds a parser for the peer command", "response": "def add_peer_parser(subparsers, parent_parser):\n    \"\"\"Adds argument parser for the peer command\n\n        Args:\n            subparsers: Add parsers to this subparser object\n            parent_parser: The parent argparse.ArgumentParser object\n    \"\"\"\n    parser = subparsers.add_parser(\n        'peer',\n        help='Displays information about validator peers',\n        description=\"Provides a subcommand to list a validator's peers\")\n\n    grand_parsers = parser.add_subparsers(title='subcommands',\n                                          dest='subcommand')\n    grand_parsers.required = True\n    add_peer_list_parser(grand_parsers, parent_parser)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef pending_batch_info(self):\n        c_length = ctypes.c_int(0)\n        c_limit = ctypes.c_int(0)\n        self._call(\n            'pending_batch_info',\n            ctypes.byref(c_length),\n            ctypes.byref(c_limit))\n\n        return (c_length.value, c_limit.value)", "response": "Returns a tuple of the current size of the pending batch queue\n            and the current queue limit."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\ncalls by the block manager when the current head block has been updated.", "response": "def on_chain_updated(self, chain_head,\n                         committed_batches=None,\n                         uncommitted_batches=None):\n        \"\"\"\n        The existing chain has been updated, the current head block has\n        changed.\n\n        :param chain_head: the new head of block_chain, can be None if\n        no block publishing is desired.\n        :param committed_batches: the set of batches that were committed\n         as part of the new chain.\n        :param uncommitted_batches: the list of transactions if any that are\n        now de-committed when the new chain was selected.\n        :return: None\n        \"\"\"\n        try:\n            self._py_call(\n                'on_chain_updated',\n                ctypes.py_object(chain_head),\n                ctypes.py_object(committed_batches),\n                ctypes.py_object(uncommitted_batches))\n\n        # pylint: disable=broad-except\n        except Exception:\n            LOGGER.exception(\n                \"Unhandled exception in BlockPublisher.on_chain_updated\")"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_message(self, msg, connection_id=None):\n        zmq_identity = None\n        if connection_id is not None and self._connections is not None:\n            if connection_id in self._connections:\n                connection_info = self._connections.get(connection_id)\n                if connection_info.connection_type == \\\n                        ConnectionType.ZMQ_IDENTITY:\n                    zmq_identity = connection_info.connection\n            else:\n                LOGGER.debug(\"Can't send to %s, not in self._connections\",\n                             connection_id)\n\n        self._ready.wait()\n\n        if zmq_identity is None:\n            message_bundle = [msg.SerializeToString()]\n        else:\n            message_bundle = [bytes(zmq_identity),\n                              msg.SerializeToString()]\n\n        try:\n            asyncio.run_coroutine_threadsafe(\n                self._send_message_frame(message_bundle),\n                self._event_loop)\n        except RuntimeError:\n            # run_coroutine_threadsafe will throw a RuntimeError if\n            # the eventloop is closed. This occurs on shutdown.\n            pass", "response": "Sends a message to the specified connection."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a message to the last available message in the queue.", "response": "def send_last_message(self, msg, connection_id=None):\n        \"\"\"\n        Should be used instead of send_message, when you want to close the\n        connection once the message is sent.\n\n        :param msg: protobuf validator_pb2.Message\n        \"\"\"\n        zmq_identity = None\n        if connection_id is not None and self._connections is not None:\n            if connection_id in self._connections:\n                connection_info = self._connections.get(connection_id)\n                if connection_info.connection_type == \\\n                        ConnectionType.ZMQ_IDENTITY:\n                    zmq_identity = connection_info.connection\n                del self._connections[connection_id]\n\n            else:\n                LOGGER.debug(\"Can't send to %s, not in self._connections\",\n                             connection_id)\n                return\n\n        self._ready.wait()\n\n        try:\n            asyncio.run_coroutine_threadsafe(\n                self._send_last_message(zmq_identity, msg),\n                self._event_loop)\n        except RuntimeError:\n            # run_coroutine_threadsafe will throw a RuntimeError if\n            # the eventloop is closed. This occurs on shutdown.\n            pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ngets the public key for a connection.", "response": "def connection_id_to_public_key(self, connection_id):\n        \"\"\"\n        Get stored public key for a connection.\n        \"\"\"\n        with self._connections_lock:\n            try:\n                connection_info = self._connections[connection_id]\n                return connection_info.public_key\n            except KeyError:\n                return None"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef public_key_to_connection_id(self, public_key):\n        with self._connections_lock:\n            for connection_id, connection_info in self._connections.items():\n                if connection_info.public_key == public_key:\n                    return connection_id\n\n            return None", "response": "Get the connection id for a public key."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connection_id_to_endpoint(self, connection_id):\n        with self._connections_lock:\n            try:\n                connection_info = self._connections[connection_id]\n                return connection_info.uri\n            except KeyError:\n                return None", "response": "Get the endpoint for a connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_connection_status(self, connection_id):\n        with self._connections_lock:\n            try:\n                connection_info = self._connections[connection_id]\n                return connection_info.status\n            except KeyError:\n                return None", "response": "Get the status of a connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef allow_inbound_connection(self):\n        LOGGER.debug(\"Determining whether inbound connection should \"\n                     \"be allowed. num connections: %s max %s\",\n                     len(self._connections),\n                     self._max_incoming_connections)\n        return self._max_incoming_connections >= len(self._connections)", "response": "Determines if an additional incoming network connection\n        should be permitted."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef add_outbound_connection(self, uri):\n        LOGGER.debug(\"Adding connection to %s\", uri)\n        conn = OutboundConnection(\n            connections=self._connections,\n            endpoint=uri,\n            dispatcher=self._dispatcher,\n            zmq_identity=self._zmq_identity,\n            secured=self._secured,\n            server_public_key=self._server_public_key,\n            server_private_key=self._server_private_key,\n            future_callback_threadpool=self._future_callback_threadpool,\n            heartbeat=True,\n            connection_timeout=self._connection_timeout)\n\n        self.outbound_connections[uri] = conn\n        conn.start()\n\n        self._add_connection(conn, uri)\n\n        connect_message = ConnectionRequest(endpoint=self._public_endpoint)\n        conn.send(\n            validator_pb2.Message.NETWORK_CONNECT,\n            connect_message.SerializeToString(),\n            callback=partial(\n                self._connect_callback,\n                connection=conn,\n            ))\n\n        return conn", "response": "Adds an outbound connection to the network."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef send_connect_request(self, connection_id):\n        connect_message = ConnectionRequest(endpoint=self._public_endpoint)\n        self._safe_send(\n            validator_pb2.Message.NETWORK_CONNECT,\n            connect_message.SerializeToString(),\n            connection_id,\n            callback=partial(\n                self._inbound_connection_request_callback,\n                connection_id=connection_id))", "response": "Sends a ConnectionRequest to an inbound connection."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef send(self, message_type, data, connection_id, callback=None,\n             one_way=False):\n        \"\"\"\n        Send a message of message_type\n        :param connection_id: the identity for the connection to send to\n        :param message_type: validator_pb2.Message.* enum value\n        :param data: bytes serialized protobuf\n        :return: future.Future\n        \"\"\"\n        if connection_id not in self._connections:\n            raise ValueError(\"Unknown connection id: {}\".format(connection_id))\n        connection_info = self._connections.get(connection_id)\n        if connection_info.connection_type == \\\n                ConnectionType.ZMQ_IDENTITY:\n            message = validator_pb2.Message(\n                correlation_id=_generate_id(),\n                content=data,\n                message_type=message_type)\n\n            timer_tag = get_enum_name(message.message_type)\n            timer_ctx = self._get_send_response_timer(timer_tag).time()\n            fut = future.Future(\n                message.correlation_id,\n                message.content,\n                callback,\n                timeout=self._connection_timeout,\n                timer_ctx=timer_ctx)\n            if not one_way:\n                self._futures.put(fut)\n\n            self._send_receive_thread.send_message(msg=message,\n                                                   connection_id=connection_id)\n            return fut\n\n        return connection_info.connection.send(\n            message_type,\n            data,\n            callback=callback,\n            one_way=one_way)", "response": "Send a message to a specific connection."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the connection id associated with a zmq - style uri which identifies a publically reachable endpoint or raises KeyError if the endpoint is not available.", "response": "def get_connection_id_by_endpoint(self, endpoint):\n        \"\"\"Returns the connection id associated with a publically\n        reachable endpoint or raises KeyError if the endpoint is not\n        found.\n\n        Args:\n            endpoint (str): A zmq-style uri which identifies a publically\n                reachable endpoint.\n        \"\"\"\n        with self._connections_lock:\n            for connection_id in self._connections:\n                connection_info = self._connections[connection_id]\n                if connection_info.uri == endpoint:\n                    return connection_id\n            raise KeyError()"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the endpoint of a connection.", "response": "def update_connection_endpoint(self, connection_id, endpoint):\n        \"\"\"Adds the endpoint to the connection definition. When the\n        connection is created by the send/receive thread, we do not\n        yet have the endpoint of the remote node. That is not known\n        until we process the incoming ConnectRequest.\n\n        Args:\n            connection_id (str): The identifier for the connection.\n            endpoint (str): A zmq-style uri which identifies a publically\n                reachable endpoint.\n        \"\"\"\n        if connection_id in self._connections:\n            connection_info = self._connections[connection_id]\n            self._connections[connection_id] = \\\n                ConnectionInfo(connection_info.connection_type,\n                               connection_info.connection,\n                               endpoint,\n                               connection_info.status,\n                               connection_info.public_key)\n\n        else:\n            LOGGER.debug(\"Could not update the endpoint %s for \"\n                         \"connection_id %s. The connection does not \"\n                         \"exist.\",\n                         endpoint,\n                         connection_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nupdate the public key of a connection.", "response": "def update_connection_public_key(self, connection_id, public_key):\n        \"\"\"Adds the public_key to the connection definition.\n\n        Args:\n            connection_id (str): The identifier for the connection.\n            public_key (str): The public key used to enforce permissions on\n                connections.\n\n        \"\"\"\n        if connection_id in self._connections:\n            connection_info = self._connections[connection_id]\n            self._connections[connection_id] = \\\n                ConnectionInfo(connection_info.connection_type,\n                               connection_info.connection,\n                               connection_info.uri,\n                               connection_info.status,\n                               public_key)\n        else:\n            LOGGER.debug(\"Could not update the public key %s for \"\n                         \"connection_id %s. The connection does not \"\n                         \"exist.\",\n                         public_key,\n                         connection_id)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nsending a message of message_type and data to the specified connection.", "response": "def send_last_message(self, message_type, data,\n                          connection_id, callback=None, one_way=False):\n        \"\"\"\n        Send a message of message_type and close the connection.\n        :param connection_id: the identity for the connection to send to\n        :param message_type: validator_pb2.Message.* enum value\n        :param data: bytes serialized protobuf\n        :return: future.Future\n        \"\"\"\n        if connection_id not in self._connections:\n            raise ValueError(\"Unknown connection id: {}\".format(connection_id))\n        connection_info = self._connections.get(connection_id)\n        if connection_info.connection_type == \\\n                ConnectionType.ZMQ_IDENTITY:\n            message = validator_pb2.Message(\n                correlation_id=_generate_id(),\n                content=data,\n                message_type=message_type)\n\n            fut = future.Future(message.correlation_id, message.content,\n                                callback, timeout=self._connection_timeout)\n\n            if not one_way:\n                self._futures.put(fut)\n\n            self._send_receive_thread.send_last_message(\n                msg=message,\n                connection_id=connection_id)\n            return fut\n\n        del self._connections[connection_id]\n        return connection_info.connection.send_last_message(\n            message_type,\n            data,\n            callback=callback)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef send(self, message_type, data, callback=None, one_way=False):\n        message = validator_pb2.Message(\n            correlation_id=_generate_id(),\n            content=data,\n            message_type=message_type)\n\n        fut = future.Future(message.correlation_id, message.content,\n                            callback, timeout=self._connection_timeout)\n        if not one_way:\n            self._futures.put(fut)\n\n        self._send_receive_thread.send_message(message)\n        return fut", "response": "Sends a message of message_type to the remote peer."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef header(self):\n        if self._block_header is None:\n            self._block_header = BlockHeader()\n            self._block_header.ParseFromString(self.block.header)\n        return self._block_header", "response": "Returns the header of the block"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef state_view_for_block(block_wrapper, state_view_factory):\n        state_root_hash = \\\n            block_wrapper.state_root_hash \\\n            if block_wrapper is not None else None\n\n        return state_view_factory.create_view(state_root_hash)", "response": "Returns the state view object associated with the given block."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef settings_view_for_block(block_wrapper, settings_view_factory):\n        state_root_hash = \\\n            block_wrapper.state_root_hash \\\n            if block_wrapper is not None else None\n\n        return settings_view_factory.create_settings_view(state_root_hash)", "response": "Returns the settings view for an arbitrary block."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef thread(data, default=u\"Untitled.\", id=None):\n\n    html = html5lib.parse(data, treebuilder=\"dom\")\n\n    assert html.lastChild.nodeName == \"html\"\n    html = html.lastChild\n\n    # aka getElementById, but limited to div and section tags\n    el = list(filter(lambda i: i.attributes[\"id\"].value == \"isso-thread\",\n                     filter(lambda i: \"id\" in i.attributes,\n                            chain(*map(html.getElementsByTagName, (\"div\", \"section\"))))))\n\n    if not el:\n        return id, default\n\n    el = el[0]\n    visited = []\n\n    def recurse(node):\n        for child in node.childNodes:\n            if child.nodeType != child.ELEMENT_NODE:\n                continue\n            if child.nodeName.upper() == \"H1\":\n                return child\n            if child not in visited:\n                return recurse(child)\n\n    def gettext(rv):\n        for child in rv.childNodes:\n            if child.nodeType == child.TEXT_NODE:\n                yield child.nodeValue\n            if child.nodeType == child.ELEMENT_NODE:\n                for item in gettext(child):\n                    yield item\n\n    try:\n        id = unquote(el.attributes[\"data-isso-id\"].value)\n    except (KeyError, AttributeError):\n        pass\n\n    try:\n        return id, unquote(el.attributes[\"data-title\"].value)\n    except (KeyError, AttributeError):\n        pass\n\n    while el is not None:  # el.parentNode is None in the very end\n\n        visited.append(el)\n        rv = recurse(el)\n\n        if rv:\n            return id, ''.join(gettext(rv)).strip()\n\n        el = el.parentNode\n\n    return id, default", "response": "Extract thread id from web page."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef insert(self, thread):\n        thread_id = thread['id']\n        title = thread['title']\n        self.db.threads.new(thread_id, title)\n\n        comments = list(map(self._build_comment, thread['comments']))\n        comments.sort(key=lambda comment: comment['id'])\n        self.count += len(comments)\n        for comment in comments:\n            self.db.comments.add(thread_id, comment)", "response": "Process a thread and insert its comments in the DB."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef migrate(self):\n        with io.open(self.json_file, 'rt', encoding='utf8') as fh:\n            threads = json.load(fh)\n        progress = Progress(len(threads))\n\n        for i, thread in enumerate(threads):\n            progress.update(i, str(i))\n            self.insert(thread)\n\n        progress.finish(\"{0} threads, {1} comments\".format(len(threads), self.count))", "response": "Process the input file and fill the DB."}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nadds new comment to the database and return a dictionary of fields and database values.", "response": "def add(self, uri, c):\n        \"\"\"\n        Add new comment to DB and return a mapping of :attribute:`fields` and\n        database values.\n        \"\"\"\n\n        if c.get(\"parent\") is not None:\n            ref = self.get(c[\"parent\"])\n            if ref.get(\"parent\") is not None:\n                c[\"parent\"] = ref[\"parent\"]\n\n        self.db.execute([\n            'INSERT INTO comments (',\n            '    tid, parent,'\n            '    created, modified, mode, remote_addr,',\n            '    text, author, email, website, voters, notification)',\n            'SELECT',\n            '    threads.id, ?,',\n            '    ?, ?, ?, ?,',\n            '    ?, ?, ?, ?, ?, ?',\n            'FROM threads WHERE threads.uri = ?;'], (\n            c.get('parent'),\n            c.get('created') or time.time(), None, c[\"mode\"], c['remote_addr'],\n            c['text'], c.get('author'), c.get('email'), c.get('website'), buffer(\n                Bloomfilter(iterable=[c['remote_addr']]).array), c.get('notification'),\n            uri)\n        )\n\n        return dict(zip(Comments.fields, self.db.execute(\n            'SELECT *, MAX(c.id) FROM comments AS c INNER JOIN threads ON threads.uri = ?',\n            (uri, )).fetchone()))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nturning off email notifications for replies to this comment.", "response": "def unsubscribe(self, email, id):\n        \"\"\"\n        Turn off email notifications for replies to this comment.\n        \"\"\"\n        self.db.execute([\n            'UPDATE comments SET',\n            '    notification=0',\n            'WHERE email=? AND (id=? OR parent=?);'], (email, id, id))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nupdates comment with values from data.", "response": "def update(self, id, data):\n        \"\"\"\n        Update comment :param:`id` with values from :param:`data` and return\n        updated comment.\n        \"\"\"\n        self.db.execute([\n            'UPDATE comments SET',\n            ','.join(key + '=' + '?' for key in data),\n            'WHERE id=?;'],\n            list(data.values()) + [id])\n\n        return self.get(id)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nget a specific comment by ID.", "response": "def get(self, id):\n        \"\"\"\n        Search for comment :param:`id` and return a mapping of :attr:`fields`\n        and values.\n        \"\"\"\n        rv = self.db.execute(\n            'SELECT * FROM comments WHERE id=?', (id, )).fetchone()\n        if rv:\n            return dict(zip(Comments.fields, rv))\n\n        return None"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef fetchall(self, mode=5, after=0, parent='any', order_by='id',\n                 limit=100, page=0, asc=1):\n        \"\"\"\n        Return comments for admin with :param:`mode`.\n        \"\"\"\n        fields_comments = ['tid', 'id', 'parent', 'created', 'modified',\n                           'mode', 'remote_addr', 'text', 'author',\n                           'email', 'website', 'likes', 'dislikes']\n        fields_threads = ['uri', 'title']\n        sql_comments_fields = ', '.join(['comments.' + f\n                                         for f in fields_comments])\n        sql_threads_fields = ', '.join(['threads.' + f\n                                        for f in fields_threads])\n        sql = ['SELECT ' + sql_comments_fields + ', ' + sql_threads_fields + ' '\n               'FROM comments INNER JOIN threads '\n               'ON comments.tid=threads.id '\n               'WHERE comments.mode = ? ']\n        sql_args = [mode]\n\n        if parent != 'any':\n            if parent is None:\n                sql.append('AND comments.parent IS NULL')\n            else:\n                sql.append('AND comments.parent=?')\n                sql_args.append(parent)\n\n        # custom sanitization\n        if order_by not in ['id', 'created', 'modified', 'likes', 'dislikes', 'tid']:\n            sql.append('ORDER BY ')\n            sql.append(\"comments.created\")\n            if not asc:\n                sql.append(' DESC')\n        else:\n            sql.append('ORDER BY ')\n            sql.append('comments.' + order_by)\n            if not asc:\n                sql.append(' DESC')\n            sql.append(\", comments.created\")\n\n        if limit:\n            sql.append('LIMIT ?,?')\n            sql_args.append(page * limit)\n            sql_args.append(limit)\n\n        rv = self.db.execute(sql, sql_args).fetchall()\n        for item in rv:\n            yield dict(zip(fields_comments + fields_threads, item))", "response": "Returns all the comments for admin with : param : mode."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\nfetch comments for a given uri with optional mode.", "response": "def fetch(self, uri, mode=5, after=0, parent='any',\n              order_by='id', asc=1, limit=None):\n        \"\"\"\n        Return comments for :param:`uri` with :param:`mode`.\n        \"\"\"\n        sql = ['SELECT comments.* FROM comments INNER JOIN threads ON',\n               '    threads.uri=? AND comments.tid=threads.id AND (? | comments.mode) = ?',\n               '    AND comments.created>?']\n\n        sql_args = [uri, mode, mode, after]\n\n        if parent != 'any':\n            if parent is None:\n                sql.append('AND comments.parent IS NULL')\n            else:\n                sql.append('AND comments.parent=?')\n                sql_args.append(parent)\n\n        # custom sanitization\n        if order_by not in ['id', 'created', 'modified', 'likes', 'dislikes']:\n            order_by = 'id'\n        sql.append('ORDER BY ')\n        sql.append(order_by)\n        if not asc:\n            sql.append(' DESC')\n\n        if limit:\n            sql.append('LIMIT ?')\n            sql_args.append(limit)\n\n        rv = self.db.execute(sql, sql_args).fetchall()\n        for item in rv:\n            yield dict(zip(Comments.fields, item))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef delete(self, id):\n\n        refs = self.db.execute(\n            'SELECT * FROM comments WHERE parent=?', (id, )).fetchone()\n\n        if refs is None:\n            self.db.execute('DELETE FROM comments WHERE id=?', (id, ))\n            self._remove_stale()\n            return None\n\n        self.db.execute('UPDATE comments SET text=? WHERE id=?', ('', id))\n        self.db.execute('UPDATE comments SET mode=? WHERE id=?', (4, id))\n        for field in ('author', 'website'):\n            self.db.execute('UPDATE comments SET %s=? WHERE id=?' %\n                            field, (None, id))\n\n        self._remove_stale()\n        return self.get(id)", "response": "Delete a comment. There are two distinctions: a comment is referenced\n        by another valid comment's parent attribute or stand-a-lone. In this\n        case the comment can't be removed without losing depending comments.\n        Hence, delete removes all visible data such as text, author, email,\n        website sets the mode field to 4.\n\n        In the second case this comment can be safely removed without any side\n        effects."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef reply_count(self, url, mode=5, after=0):\n\n        sql = ['SELECT comments.parent,count(*)',\n               'FROM comments INNER JOIN threads ON',\n               '   threads.uri=? AND comments.tid=threads.id AND',\n               '   (? | comments.mode = ?) AND',\n               '   comments.created > ?',\n               'GROUP BY comments.parent']\n\n        return dict(self.db.execute(sql, [url, mode, mode, after]).fetchall())", "response": "Return comment count for one url and all reply threads for one url."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning comment count for one ore more urls..", "response": "def count(self, *urls):\n        \"\"\"\n        Return comment count for one ore more urls..\n        \"\"\"\n\n        threads = dict(self.db.execute([\n            'SELECT threads.uri, COUNT(comments.id) FROM comments',\n            'LEFT OUTER JOIN threads ON threads.id = tid AND comments.mode = 1',\n            'GROUP BY threads.uri'\n        ]).fetchall())\n\n        return [threads.get(url, 0) for url in urls]"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef purge(self, delta):\n        self.db.execute([\n            'DELETE FROM comments WHERE mode = 2 AND ? - created > ?;'\n        ], (time.time(), delta))\n        self._remove_stale()", "response": "Remove comments older than delta."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef timedelta(string):\n\n    keys = [\"weeks\", \"days\", \"hours\", \"minutes\", \"seconds\"]\n    regex = \"\".join([\"((?P<%s>\\\\d+)%s ?)?\" % (k, k[0]) for k in keys])\n    kwargs = {}\n    for k, v in re.match(regex, string).groupdict(default=\"0\").items():\n        kwargs[k] = int(v)\n\n    rv = datetime.timedelta(**kwargs)\n    if rv == datetime.timedelta():\n        raise ValueError(\"invalid human-readable timedelta\")\n\n    return datetime.timedelta(**kwargs)", "response": "Parse a string into a datetime. timedelta object."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncalculate hash from value ( must be bytes ).", "response": "def hash(self, val):\n        \"\"\"Calculate hash from value (must be bytes).\"\"\"\n\n        if not isinstance(val, bytes):\n            raise _TypeError(\"val\", \"bytes\", val)\n\n        rv = self.compute(val)\n\n        if not isinstance(val, bytes):\n            raise _TypeError(\"val\", \"bytes\", rv)\n\n        return rv"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncalculate hash from unicode value and return hex value as unicode", "response": "def uhash(self, val):\n        \"\"\"Calculate hash from unicode value and return hex value as unicode\"\"\"\n\n        if not isinstance(val, string_types):\n            raise _TypeError(\"val\", \"str\", val)\n\n        return codecs.encode(self.hash(val.encode(\"utf-8\")), \"hex_codec\").decode(\"utf-8\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef xhr(func):\n\n    \"\"\"\n    @apiDefine csrf\n    @apiHeader {string=\"application/json\"} Content-Type\n        The content type must be set to `application/json` to prevent CSRF attacks.\n    \"\"\"\n\n    def dec(self, env, req, *args, **kwargs):\n\n        if req.content_type and not req.content_type.startswith(\"application/json\"):\n            raise Forbidden(\"CSRF\")\n        return func(self, env, req, *args, **kwargs)\n\n    return dec", "response": "A decorator to check for CSRF on POST PUT DELETE using a form element and JS to execute automatically."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef moderate(self, environ, request, id, action, key):\n        try:\n            id = self.isso.unsign(key, max_age=2**32)\n        except (BadSignature, SignatureExpired):\n            raise Forbidden\n\n        item = self.comments.get(id)\n        thread = self.threads.get(item['tid'])\n        link = local(\"origin\") + thread[\"uri\"] + \"#isso-%i\" % item[\"id\"]\n\n        if item is None:\n            raise NotFound\n\n        if request.method == \"GET\":\n            modal = (\n                \"<!DOCTYPE html>\"\n                \"<html>\"\n                \"<head>\"\n                \"<script>\"\n                \"  if (confirm('%s: Are you sure?')) {\"\n                \"      xhr = new XMLHttpRequest;\"\n                \"      xhr.open('POST', window.location.href);\"\n                \"      xhr.send(null);\"\n                \"      xhr.onload = function() {\"\n                \"          window.location.href = %s;\"\n                \"      };\"\n                \"  }\"\n                \"</script>\" % (action.capitalize(), json.dumps(link)))\n\n            return Response(modal, 200, content_type=\"text/html\")\n\n        if action == \"activate\":\n            if item['mode'] == 1:\n                return Response(\"Already activated\", 200)\n            with self.isso.lock:\n                self.comments.activate(id)\n            self.signal(\"comments.activate\", thread, item)\n            return Response(\"Yo\", 200)\n        elif action == \"edit\":\n            data = request.get_json()\n            with self.isso.lock:\n                rv = self.comments.update(id, data)\n            for key in set(rv.keys()) - API.FIELDS:\n                rv.pop(key)\n            self.signal(\"comments.edit\", rv)\n            return JSON(rv, 200)\n        else:\n            with self.isso.lock:\n                self.comments.delete(id)\n            self.cache.delete(\n                'hash', (item['email'] or item['remote_addr']).encode('utf-8'))\n            self.signal(\"comments.delete\", id)\n            return Response(\"Yo\", 200)\n\n        \"\"\"\n        @api {get} / get comments\n        @apiGroup Thread\n        @apiDescription Queries the comments of a thread.\n\n        @apiParam {string} uri\n            The URI of thread to get the comments from.\n        @apiParam {number} [parent]\n            Return only comments that are children of the comment with the provided ID.\n        @apiUse plainParam\n        @apiParam {number} [limit]\n            The maximum number of returned top-level comments. Omit for unlimited results.\n        @apiParam {number} [nested_limit]\n            The maximum number of returned nested comments per commint. Omit for unlimited results.\n        @apiParam {number} [after]\n            Includes only comments were added after the provided UNIX timestamp.\n\n        @apiSuccess {number} total_replies\n            The number of replies if the `limit` parameter was not set. If `after` is set to `X`, this is the number of comments that were created after `X`. So setting `after` may change this value!\n        @apiSuccess {Object[]} replies\n            The list of comments. Each comment also has the `total_replies`, `replies`, `id` and `hidden_replies` properties to represent nested comments.\n        @apiSuccess {number} id\n            Id of the comment `replies` is the list of replies of. `null` for the list of toplevel comments.\n        @apiSuccess {number} hidden_replies\n            The number of comments that were ommited from the results because of the `limit` request parameter. Usually, this will be `total_replies` - `limit`.\n\n        @apiExample {curl} Get 2 comments with 5 responses:\n            curl 'https://comments.example.com/?uri=/thread/&limit=2&nested_limit=5'\n        @apiSuccessExample Example reponse:\n            {\n              \"total_replies\": 14,\n              \"replies\": [\n                {\n                  \"website\": null,\n                  \"author\": null,\n                  \"parent\": null,\n                  \"created\": 1464818460.732863,\n                  \"text\": \"&lt;p&gt;Hello, World!&lt;/p&gt;\",\n                  \"total_replies\": 1,\n                  \"hidden_replies\": 0,\n                  \"dislikes\": 2,\n                  \"modified\": null,\n                  \"mode\": 1,\n                  \"replies\": [\n                    {\n                      \"website\": null,\n                      \"author\": null,\n                      \"parent\": 1,\n                      \"created\": 1464818460.769638,\n                      \"text\": \"&lt;p&gt;Hi, now some Markdown: &lt;em&gt;Italic&lt;/em&gt;, &lt;strong&gt;bold&lt;/strong&gt;, &lt;code&gt;monospace&lt;/code&gt;.&lt;/p&gt;\",\n                      \"dislikes\": 0,\n                      \"modified\": null,\n                      \"mode\": 1,\n                      \"hash\": \"2af4e1a6c96a\",\n                      \"id\": 2,\n                      \"likes\": 2\n                    }\n                  ],\n                  \"hash\": \"1cb6cc0309a2\",\n                  \"id\": 1,\n                  \"likes\": 2\n                },\n                {\n                  \"website\": null,\n                  \"author\": null,\n                  \"parent\": null,\n                  \"created\": 1464818460.80574,\n                  \"text\": \"&lt;p&gt;Lorem ipsum dolor sit amet, consectetur adipisicing elit. Accusantium at commodi cum deserunt dolore, error fugiat harum incidunt, ipsa ipsum mollitia nam provident rerum sapiente suscipit tempora vitae? Est, qui?&lt;/p&gt;\",\n                  \"total_replies\": 0,\n                  \"hidden_replies\": 0,\n                  \"dislikes\": 0,\n                  \"modified\": null,\n                  \"mode\": 1,\n                  \"replies\": [],\n                  \"hash\": \"1cb6cc0309a2\",\n                  \"id\": 3,\n                  \"likes\": 0\n                },\n                \"id\": null,\n                \"hidden_replies\": 12\n            }\n        \"\"\"", "response": "Handles the moderate action for the given item."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef threaded(func):\n\n    def dec(self, *args, **kwargs):\n        thread.start_new_thread(func, (self, ) + args, kwargs)\n\n    return dec", "response": "Decorator to execute each function in a separate thread."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef host(environ):  # pragma: no cover\n\n    url = environ['wsgi.url_scheme'] + '://'\n\n    if environ.get('HTTP_HOST'):\n        url += environ['HTTP_HOST']\n    else:\n        url += environ['SERVER_NAME']\n\n        if environ['wsgi.url_scheme'] == 'https':\n            if environ['SERVER_PORT'] != '443':\n                url += ':' + environ['SERVER_PORT']\n        else:\n            if environ['SERVER_PORT'] != '80':\n                url += ':' + environ['SERVER_PORT']\n\n    return url + quote(environ.get('SCRIPT_NAME', ''))", "response": "Reconstructs the host from the environment. A modified version\n    of http://www. python. org / dev / peps / pep - 3379/#url - reconstruction - order - of - construction"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nparsing url into netloc port ssl", "response": "def urlsplit(name):\n    \"\"\"\n    Parse :param:`name` into (netloc, port, ssl)\n    \"\"\"\n\n    if not (isinstance(name, string_types)):\n        name = str(name)\n\n    if not name.startswith(('http://', 'https://')):\n        name = 'http://' + name\n\n    rv = urlparse(name)\n    if rv.scheme == 'https' and rv.port is None:\n        return rv.netloc, 443, True\n    return rv.netloc.rsplit(':')[0], rv.port or 80, rv.scheme == 'https'"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef urljoin(netloc, port, ssl):\n\n    rv = (\"https\" if ssl else \"http\") + \"://\" + netloc\n    if ssl and port != 443 or not ssl and port != 80:\n        rv += \":%i\" % port\n    return rv", "response": "Join the netloc and port with the base URL."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that returns a valid HTTP Origin or localhost if none found.", "response": "def origin(hosts):\n    \"\"\"\n    Return a function that returns a valid HTTP Origin or localhost\n    if none found.\n    \"\"\"\n\n    hosts = [urlsplit(h) for h in hosts]\n\n    def func(environ):\n        if 'ISSO_CORS_ORIGIN' in environ:\n            return environ['ISSO_CORS_ORIGIN']\n\n        if not hosts:\n            return \"http://invalid.local\"\n\n        loc = environ.get(\"HTTP_ORIGIN\", environ.get(\"HTTP_REFERER\", None))\n\n        if loc is None:\n            return urljoin(*hosts[0])\n\n        for split in hosts:\n            if urlsplit(loc) == split:\n                return urljoin(*split)\n        else:\n            return urljoin(*hosts[0])\n\n    return func"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef urlretrieve(self, url, filename, data=None):\n        logger.info('saving: \\'%s\\' to \\'%s\\'', url, filename)\n\n        if _is_py3:\n            return _urlretrieve_with_opener(self.opener, url, filename, data=data)\n\n        return self.opener2.retrieve(url, filename, data=data)", "response": "Similar to urllib. urlretrieve but that only that filename is required."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef parse_url(url):\n    if \":\" not in url:\n        raise ValueError(\"url is invalid\")\n\n    scheme, url = url.split(\":\", 1)\n\n    parsed = urlparse(url, scheme=\"ws\")\n    if parsed.hostname:\n        hostname = parsed.hostname\n    else:\n        raise ValueError(\"hostname is invalid\")\n    port = 0\n    if parsed.port:\n        port = parsed.port\n\n    is_secure = False\n    if scheme == \"ws\":\n        if not port:\n            port = 80\n    elif scheme == \"wss\":\n        is_secure = True\n        if not port:\n            port = 443\n    else:\n        raise ValueError(\"scheme %s is invalid\" % scheme)\n\n    if parsed.path:\n        resource = parsed.path\n    else:\n        resource = \"/\"\n\n    if parsed.query:\n        resource += \"?\" + parsed.query\n\n    return hostname, port, resource, is_secure", "response": "parse url and return hostname port resource path and flag of secure mode"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\ngets proxy information from environment variables and return them as a tuple.", "response": "def get_proxy_info(\n        hostname, is_secure, proxy_host=None, proxy_port=0, proxy_auth=None,\n        no_proxy=None, proxy_type='http'):\n    \"\"\"\n    try to retrieve proxy host and port from environment\n    if not provided in options.\n    result is (proxy_host, proxy_port, proxy_auth).\n    proxy_auth is tuple of username and password\n     of proxy authentication information.\n\n    hostname: websocket server name.\n\n    is_secure:  is the connection secure? (wss)\n                looks for \"https_proxy\" in env\n                before falling back to \"http_proxy\"\n\n    options:    \"http_proxy_host\" - http proxy host name.\n                \"http_proxy_port\" - http proxy port.\n                \"http_no_proxy\"   - host names, which doesn't use proxy.\n                \"http_proxy_auth\" - http proxy auth information.\n                                    tuple of username and password.\n                                    default is None\n                \"proxy_type\"      - if set to \"socks5\" PySocks wrapper\n                                    will be used in place of a http proxy.\n                                    default is \"http\"\n    \"\"\"\n    if _is_no_proxy_host(hostname, no_proxy):\n        return None, 0, None\n\n    if proxy_host:\n        port = proxy_port\n        auth = proxy_auth\n        return proxy_host, port, auth\n\n    env_keys = [\"http_proxy\"]\n    if is_secure:\n        env_keys.insert(0, \"https_proxy\")\n\n    for key in env_keys:\n        value = os.environ.get(key, None)\n        if value:\n            proxy = urlparse(value)\n            auth = (proxy.username, proxy.password) if proxy.username else None\n            return proxy.hostname, proxy.port, auth\n\n    return None, 0, None"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ncreating a new connection to a url and return a WebSocket object.", "response": "def create_connection(url, timeout=None, class_=WebSocket, **options):\n    \"\"\"\n    connect to url and return websocket object.\n\n    Connect to url and return the WebSocket object.\n    Passing optional timeout parameter will set the timeout on the socket.\n    If no timeout is supplied,\n    the global default timeout setting returned by getdefauttimeout() is used.\n    You can customize using 'options'.\n    If you set \"header\" list object, you can set your own custom header.\n\n    >>> conn = create_connection(\"ws://echo.websocket.org/\",\n         ...     header=[\"User-Agent: MyProgram\",\n         ...             \"x-custom: header\"])\n\n\n    timeout: socket timeout time. This value is integer.\n             if you set None for this value,\n             it means \"use default_timeout value\"\n\n    class_: class to instantiate when creating the connection. It has to implement\n            settimeout and connect. It's __init__ should be compatible with\n            WebSocket.__init__, i.e. accept all of it's kwargs.\n    options: \"header\" -> custom http header list or dict.\n             \"cookie\" -> cookie value.\n             \"origin\" -> custom origin url.\n             \"suppress_origin\" -> suppress outputting origin header.\n             \"host\"   -> custom host header string.\n             \"http_proxy_host\" - http proxy host name.\n             \"http_proxy_port\" - http proxy port. If not set, set to 80.\n             \"http_no_proxy\"   - host names, which doesn't use proxy.\n             \"http_proxy_auth\" - http proxy auth information.\n                                    tuple of username and password.\n                                    default is None\n             \"enable_multithread\" -> enable lock for multithread.\n             \"redirect_limit\" -> number of redirects to follow.\n             \"sockopt\" -> socket options\n             \"sslopt\" -> ssl option\n             \"subprotocols\" - array of available sub protocols.\n                              default is None.\n             \"skip_utf8_validation\" - skip utf8 validation.\n             \"socket\" - pre-initialized stream socket.\n    \"\"\"\n    sockopt = options.pop(\"sockopt\", [])\n    sslopt = options.pop(\"sslopt\", {})\n    fire_cont_frame = options.pop(\"fire_cont_frame\", False)\n    enable_multithread = options.pop(\"enable_multithread\", False)\n    skip_utf8_validation = options.pop(\"skip_utf8_validation\", False)\n    websock = class_(sockopt=sockopt, sslopt=sslopt,\n                     fire_cont_frame=fire_cont_frame,\n                     enable_multithread=enable_multithread,\n                     skip_utf8_validation=skip_utf8_validation, **options)\n    websock.settimeout(timeout if timeout is not None else getdefaulttimeout())\n    websock.connect(url, **options)\n    return websock"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef settimeout(self, timeout):\n        self.sock_opt.timeout = timeout\n        if self.sock:\n            self.sock.settimeout(timeout)", "response": "Set the timeout to the websocket."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef connect(self, url, **options):\n        # FIXME: \"subprotocols\" are getting lost, not passed down\n        # FIXME: \"header\", \"cookie\", \"origin\" and \"host\" too\n        self.sock_opt.timeout = options.get('timeout', self.sock_opt.timeout)\n        self.sock, addrs = connect(url, self.sock_opt, proxy_info(**options),\n                                   options.pop('socket', None))\n\n        try:\n            self.handshake_response = handshake(self.sock, *addrs, **options)\n            for attempt in range(options.pop('redirect_limit', 3)):\n                if self.handshake_response.status in SUPPORTED_REDIRECT_STATUSES:\n                    url = self.handshake_response.headers['location']\n                    self.sock.close()\n                    self.sock, addrs =  connect(url, self.sock_opt, proxy_info(**options),\n                                                options.pop('socket', None))\n                    self.handshake_response = handshake(self.sock, *addrs, **options)\n            self.connected = True\n        except:\n            if self.sock:\n                self.sock.close()\n                self.sock = None\n            raise", "response": "Connect to url and return a new instance of the class."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, payload, opcode=ABNF.OPCODE_TEXT):\n\n        frame = ABNF.create_frame(payload, opcode)\n        return self.send_frame(frame)", "response": "Send the data as string."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nsending the data frame.", "response": "def send_frame(self, frame):\n        \"\"\"\n        Send the data frame.\n\n        frame: frame data created  by ABNF.create_frame\n\n        >>> ws = create_connection(\"ws://echo.websocket.org/\")\n        >>> frame = ABNF.create_frame(\"Hello\", ABNF.OPCODE_TEXT)\n        >>> ws.send_frame(frame)\n        >>> cont_frame = ABNF.create_frame(\"My name is \", ABNF.OPCODE_CONT, 0)\n        >>> ws.send_frame(frame)\n        >>> cont_frame = ABNF.create_frame(\"Foo Bar\", ABNF.OPCODE_CONT, 1)\n        >>> ws.send_frame(frame)\n\n        \"\"\"\n        if self.get_mask_key:\n            frame.get_mask_key = self.get_mask_key\n        data = frame.format()\n        length = len(data)\n        trace(\"send: \" + repr(data))\n\n        with self.lock:\n            while data:\n                l = self._send(data)\n                data = data[l:]\n\n        return length"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsending ping data. payload: data payload to send server.", "response": "def ping(self, payload=\"\"):\n        \"\"\"\n        send ping data.\n\n        payload: data payload to send server.\n        \"\"\"\n        if isinstance(payload, six.text_type):\n            payload = payload.encode(\"utf-8\")\n        self.send(payload, ABNF.OPCODE_PING)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef pong(self, payload):\n        if isinstance(payload, six.text_type):\n            payload = payload.encode(\"utf-8\")\n        self.send(payload, ABNF.OPCODE_PONG)", "response": "send pong data.\n\n        payload: data payload to send server."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recv(self):\n        with self.readlock:\n            opcode, data = self.recv_data()\n        if six.PY3 and opcode == ABNF.OPCODE_TEXT:\n            return data.decode(\"utf-8\")\n        elif opcode == ABNF.OPCODE_TEXT or opcode == ABNF.OPCODE_BINARY:\n            return data\n        else:\n            return ''", "response": "Receive string data from the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef recv_data(self, control_frame=False):\n        opcode, frame = self.recv_data_frame(control_frame)\n        return opcode, frame.data", "response": "Receive data with operation code."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef recv_data_frame(self, control_frame=False):\n        while True:\n            frame = self.recv_frame()\n            if not frame:\n                # handle error:\n                # 'NoneType' object has no attribute 'opcode'\n                raise WebSocketProtocolException(\n                    \"Not a valid frame %s\" % frame)\n            elif frame.opcode in (ABNF.OPCODE_TEXT, ABNF.OPCODE_BINARY, ABNF.OPCODE_CONT):\n                self.cont_frame.validate(frame)\n                self.cont_frame.add(frame)\n\n                if self.cont_frame.is_fire(frame):\n                    return self.cont_frame.extract(frame)\n\n            elif frame.opcode == ABNF.OPCODE_CLOSE:\n                self.send_close()\n                return frame.opcode, frame\n            elif frame.opcode == ABNF.OPCODE_PING:\n                if len(frame.data) < 126:\n                    self.pong(frame.data)\n                else:\n                    raise WebSocketProtocolException(\n                        \"Ping message is too long\")\n                if control_frame:\n                    return frame.opcode, frame\n            elif frame.opcode == ABNF.OPCODE_PONG:\n                if control_frame:\n                    return frame.opcode, frame", "response": "Receive data from the broker."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef send_close(self, status=STATUS_NORMAL, reason=six.b(\"\")):\n        if status < 0 or status >= ABNF.LENGTH_16:\n            raise ValueError(\"code is invalid range\")\n        self.connected = False\n        self.send(struct.pack('!H', status) + reason, ABNF.OPCODE_CLOSE)", "response": "send close data to the server."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef close(self, status=STATUS_NORMAL, reason=six.b(\"\"), timeout=3):\n        if self.connected:\n            if status < 0 or status >= ABNF.LENGTH_16:\n                raise ValueError(\"code is invalid range\")\n\n            try:\n                self.connected = False\n                self.send(struct.pack('!H', status) +\n                          reason, ABNF.OPCODE_CLOSE)\n                sock_timeout = self.sock.gettimeout()\n                self.sock.settimeout(timeout)\n                start_time = time.time()\n                while timeout is None or time.time() - start_time < timeout:\n                    try:\n                        frame = self.recv_frame()\n                        if frame.opcode != ABNF.OPCODE_CLOSE:\n                            continue\n                        if isEnabledForError():\n                            recv_status = struct.unpack(\"!H\", frame.data[0:2])[0]\n                            if recv_status != STATUS_NORMAL:\n                                error(\"close status: \" + repr(recv_status))\n                        break\n                    except:\n                        break\n                self.sock.settimeout(sock_timeout)\n                self.sock.shutdown(socket.SHUT_RDWR)\n            except:\n                pass\n\n            self.shutdown()", "response": "Close Websocket object\n\n        status: status code to send. see STATUS_XXX.\n\n        reason: the reason to close. This must be string.\n\n        timeout: timeout until receive a close frame.\n            If None, it will wait forever until receive a close frame."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nturn on or off the traceability.", "response": "def enableTrace(traceable, handler = logging.StreamHandler()):\n    \"\"\"\n    turn on/off the traceability.\n\n    traceable: boolean value. if set True, traceability is enabled.\n    \"\"\"\n    global _traceEnabled\n    _traceEnabled = traceable\n    if traceable:\n        _logger.addHandler(handler)\n        _logger.setLevel(logging.DEBUG)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nvalidates the ABNF frame.", "response": "def validate(self, skip_utf8_validation=False):\n        \"\"\"\n        validate the ABNF frame.\n        skip_utf8_validation: skip utf8 validation.\n        \"\"\"\n        if self.rsv1 or self.rsv2 or self.rsv3:\n            raise WebSocketProtocolException(\"rsv is not implemented, yet\")\n\n        if self.opcode not in ABNF.OPCODES:\n            raise WebSocketProtocolException(\"Invalid opcode %r\", self.opcode)\n\n        if self.opcode == ABNF.OPCODE_PING and not self.fin:\n            raise WebSocketProtocolException(\"Invalid ping frame.\")\n\n        if self.opcode == ABNF.OPCODE_CLOSE:\n            l = len(self.data)\n            if not l:\n                return\n            if l == 1 or l >= 126:\n                raise WebSocketProtocolException(\"Invalid close frame.\")\n            if l > 2 and not skip_utf8_validation and not validate_utf8(self.data[2:]):\n                raise WebSocketProtocolException(\"Invalid close frame.\")\n\n            code = 256 * \\\n                six.byte2int(self.data[0:1]) + six.byte2int(self.data[1:2])\n            if not self._is_valid_close_status(code):\n                raise WebSocketProtocolException(\"Invalid close opcode.\")"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncreate a new frame to send text binary and other data.", "response": "def create_frame(data, opcode, fin=1):\n        \"\"\"\n        create frame to send text, binary and other data.\n\n        data: data to send. This is string value(byte array).\n            if opcode is OPCODE_TEXT and this value is unicode,\n            data value is converted into unicode string, automatically.\n\n        opcode: operation code. please see OPCODE_XXX.\n\n        fin: fin flag. if set to 0, create continue fragmentation.\n        \"\"\"\n        if opcode == ABNF.OPCODE_TEXT and isinstance(data, six.text_type):\n            data = data.encode(\"utf-8\")\n        # mask must be set if send data from client\n        return ABNF(fin, 0, 0, 0, opcode, 1, data)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nformats this object to string to send to server.", "response": "def format(self):\n        \"\"\"\n        format this object to string(byte array) to send data to server.\n        \"\"\"\n        if any(x not in (0, 1) for x in [self.fin, self.rsv1, self.rsv2, self.rsv3]):\n            raise ValueError(\"not 0 or 1\")\n        if self.opcode not in ABNF.OPCODES:\n            raise ValueError(\"Invalid OPCODE\")\n        length = len(self.data)\n        if length >= ABNF.LENGTH_63:\n            raise ValueError(\"data is too long\")\n\n        frame_header = chr(self.fin << 7\n                           | self.rsv1 << 6 | self.rsv2 << 5 | self.rsv3 << 4\n                           | self.opcode)\n        if length < ABNF.LENGTH_7:\n            frame_header += chr(self.mask << 7 | length)\n            frame_header = six.b(frame_header)\n        elif length < ABNF.LENGTH_16:\n            frame_header += chr(self.mask << 7 | 0x7e)\n            frame_header = six.b(frame_header)\n            frame_header += struct.pack(\"!H\", length)\n        else:\n            frame_header += chr(self.mask << 7 | 0x7f)\n            frame_header = six.b(frame_header)\n            frame_header += struct.pack(\"!Q\", length)\n\n        if not self.mask:\n            return frame_header + self.data\n        else:\n            mask_key = self.get_mask_key(4)\n            return frame_header + self._get_masked(mask_key)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nmasks or unmask data. Just do xor for each byte in data. Just do xor for each byte in data.", "response": "def mask(mask_key, data):\n        \"\"\"\n        mask or unmask data. Just do xor for each byte\n\n        mask_key: 4 byte string(byte).\n\n        data: data to mask/unmask.\n        \"\"\"\n        if data is None:\n            data = \"\"\n\n        if isinstance(mask_key, six.text_type):\n            mask_key = six.b(mask_key)\n\n        if isinstance(data, six.text_type):\n            data = six.b(data)\n\n        if numpy:\n            origlen = len(data)\n            _mask_key = mask_key[3] << 24 | mask_key[2] << 16 | mask_key[1] << 8 | mask_key[0]\n\n            # We need data to be a multiple of four...\n            data += bytes(\" \" * (4 - (len(data) % 4)), \"us-ascii\")\n            a = numpy.frombuffer(data, dtype=\"uint32\")\n            masked = numpy.bitwise_xor(a, [_mask_key]).astype(\"uint32\")\n            if len(data) > origlen:\n              return masked.tobytes()[:origlen]\n            return masked.tobytes()\n        else:\n            _m = array.array(\"B\", mask_key)\n            _d = array.array(\"B\", data)\n            return _mask(_m, _d)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef send(self, data, opcode=ABNF.OPCODE_TEXT):\n\n        if not self.sock or self.sock.send(data, opcode) == 0:\n            raise WebSocketConnectionClosedException(\n                \"Connection is already closed.\")", "response": "send data to the local socket."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nrunning event loop for WebSocket framework. This loop is infinite loop and is alive during websocket is available. sockopt: values for socket.setsockopt. sockopt must be tuple and each element is argument of sock.setsockopt. sslopt: ssl socket optional dict. ping_interval: automatically send \"ping\" command every specified period(second) if set to 0, not send automatically. ping_timeout: timeout(second) if the pong message is not received. http_proxy_host: http proxy host name. http_proxy_port: http proxy port. If not set, set to 80. http_no_proxy: host names, which doesn't use proxy. skip_utf8_validation: skip utf8 validation. host: update host header. origin: update origin header. dispatcher: customize reading data from socket. suppress_origin: suppress outputting origin header. Returns ------- False if caught KeyboardInterrupt True if other exception was raised during a loop", "response": "def run_forever(self, sockopt=None, sslopt=None,\n                    ping_interval=0, ping_timeout=None,\n                    http_proxy_host=None, http_proxy_port=None,\n                    http_no_proxy=None, http_proxy_auth=None,\n                    skip_utf8_validation=False,\n                    host=None, origin=None, dispatcher=None,\n                    suppress_origin = False, proxy_type=None):\n        \"\"\"\n        run event loop for WebSocket framework.\n        This loop is infinite loop and is alive during websocket is available.\n        sockopt: values for socket.setsockopt.\n            sockopt must be tuple\n            and each element is argument of sock.setsockopt.\n        sslopt: ssl socket optional dict.\n        ping_interval: automatically send \"ping\" command\n            every specified period(second)\n            if set to 0, not send automatically.\n        ping_timeout: timeout(second) if the pong message is not received.\n        http_proxy_host: http proxy host name.\n        http_proxy_port: http proxy port. If not set, set to 80.\n        http_no_proxy: host names, which doesn't use proxy.\n        skip_utf8_validation: skip utf8 validation.\n        host: update host header.\n        origin: update origin header.\n        dispatcher: customize reading data from socket.\n        suppress_origin: suppress outputting origin header.\n\n        Returns\n        -------\n        False if caught KeyboardInterrupt\n        True if other exception was raised during a loop\n        \"\"\"\n\n        if ping_timeout is not None and ping_timeout <= 0:\n            ping_timeout = None\n        if ping_timeout and ping_interval and ping_interval <= ping_timeout:\n            raise WebSocketException(\"Ensure ping_interval > ping_timeout\")\n        if not sockopt:\n            sockopt = []\n        if not sslopt:\n            sslopt = {}\n        if self.sock:\n            raise WebSocketException(\"socket is already opened\")\n        thread = None\n        self.keep_running = True\n        self.last_ping_tm = 0\n        self.last_pong_tm = 0\n\n        def teardown(close_frame=None):\n            \"\"\"\n            Tears down the connection.\n            If close_frame is set, we will invoke the on_close handler with the\n            statusCode and reason from there.\n            \"\"\"\n            if thread and thread.isAlive():\n                event.set()\n                thread.join()\n            self.keep_running = False\n            if self.sock:\n                self.sock.close()\n            close_args = self._get_close_args(\n                close_frame.data if close_frame else None)\n            self._callback(self.on_close, *close_args)\n            self.sock = None\n\n        try:\n            self.sock = WebSocket(\n                self.get_mask_key, sockopt=sockopt, sslopt=sslopt,\n                fire_cont_frame=self.on_cont_message is not None,\n                skip_utf8_validation=skip_utf8_validation,\n                enable_multithread=True if ping_interval else False)\n            self.sock.settimeout(getdefaulttimeout())\n            self.sock.connect(\n                self.url, header=self.header, cookie=self.cookie,\n                http_proxy_host=http_proxy_host,\n                http_proxy_port=http_proxy_port, http_no_proxy=http_no_proxy,\n                http_proxy_auth=http_proxy_auth, subprotocols=self.subprotocols,\n                host=host, origin=origin, suppress_origin=suppress_origin,\n                proxy_type=proxy_type)\n            if not dispatcher:\n                dispatcher = self.create_dispatcher(ping_timeout)\n\n            self._callback(self.on_open)\n\n            if ping_interval:\n                event = threading.Event()\n                thread = threading.Thread(\n                    target=self._send_ping, args=(ping_interval, event))\n                thread.setDaemon(True)\n                thread.start()\n\n            def read():\n                if not self.keep_running:\n                    return teardown()\n\n                op_code, frame = self.sock.recv_data_frame(True)\n                if op_code == ABNF.OPCODE_CLOSE:\n                    return teardown(frame)\n                elif op_code == ABNF.OPCODE_PING:\n                    self._callback(self.on_ping, frame.data)\n                elif op_code == ABNF.OPCODE_PONG:\n                    self.last_pong_tm = time.time()\n                    self._callback(self.on_pong, frame.data)\n                elif op_code == ABNF.OPCODE_CONT and self.on_cont_message:\n                    self._callback(self.on_data, frame.data,\n                                   frame.opcode, frame.fin)\n                    self._callback(self.on_cont_message,\n                                   frame.data, frame.fin)\n                else:\n                    data = frame.data\n                    if six.PY3 and op_code == ABNF.OPCODE_TEXT:\n                        data = data.decode(\"utf-8\")\n                    self._callback(self.on_data, data, frame.opcode, True)\n                    self._callback(self.on_message, data)\n\n                return True\n\n            def check():\n                if (ping_timeout):\n                    has_timeout_expired = time.time() - self.last_ping_tm > ping_timeout\n                    has_pong_not_arrived_after_last_ping = self.last_pong_tm - self.last_ping_tm < 0\n                    has_pong_arrived_too_late = self.last_pong_tm - self.last_ping_tm > ping_timeout\n\n                    if (self.last_ping_tm\n                            and has_timeout_expired\n                            and (has_pong_not_arrived_after_last_ping or has_pong_arrived_too_late)):\n                        raise WebSocketTimeoutException(\"ping/pong timed out\")\n                return True\n\n            dispatcher.read(self.sock.sock, read, check)\n        except (Exception, KeyboardInterrupt, SystemExit) as e:\n            self._callback(self.on_error, e)\n            if isinstance(e, SystemExit):\n                # propagate SystemExit further\n                raise\n            teardown()\n            return not isinstance(e, KeyboardInterrupt)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef _get_close_args(self, data):\n        # if the on_close callback is \"old\", just return empty list\n        if sys.version_info < (3, 0):\n            if not self.on_close or len(inspect.getargspec(self.on_close).args) != 3:\n                return []\n        else:\n            if not self.on_close or len(inspect.getfullargspec(self.on_close).args) != 3:\n                return []\n\n        if data and len(data) >= 2:\n            code = 256 * six.byte2int(data[0:1]) + six.byte2int(data[1:2])\n            reason = data[2:].decode('utf-8')\n            return [code, reason]\n\n        return [None, None]", "response": "this function extracts the code reason from the close body"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get(self, url, headers=None, params=None, stream=False, timeout=None):\n        if timeout is None:\n            timeout = self.timeout\n\n        response = requests.get(\n            url, headers=headers, params=params, stream=stream, timeout=timeout\n        )\n\n        return RequestsHttpResponse(response)", "response": "Send a GET request to the server and return the response."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\npost request. :param str url: Request url :param dict headers: (optional) Request headers :param data: (optional) Dictionary, bytes, or file-like object to send in the body :param timeout: (optional), How long to wait for the server to send data before giving up, as a float, or a (connect timeout, read timeout) float tuple. Default is :py:attr:`self.timeout` :type timeout: float | tuple(float, float) :rtype: :py:class:`RequestsHttpResponse` :return: RequestsHttpResponse instance", "response": "def post(self, url, headers=None, data=None, timeout=None):\n        \"\"\"POST request.\n\n        :param str url: Request url\n        :param dict headers: (optional) Request headers\n        :param data: (optional) Dictionary, bytes, or file-like object to send in the body\n        :param timeout: (optional), How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is :py:attr:`self.timeout`\n        :type timeout: float | tuple(float, float)\n        :rtype: :py:class:`RequestsHttpResponse`\n        :return: RequestsHttpResponse instance\n        \"\"\"\n        if timeout is None:\n            timeout = self.timeout\n\n        response = requests.post(\n            url, headers=headers, data=data, timeout=timeout\n        )\n\n        return RequestsHttpResponse(response)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef iter_content(self, chunk_size=1024, decode_unicode=False):\n        return self.response.iter_content(chunk_size=chunk_size, decode_unicode=decode_unicode)", "response": "Get request body as iterator content ( stream."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef to_snake_case(text):\n    s1 = re.sub('(.)([A-Z][a-z]+)', r'\\1_\\2', text)\n    return re.sub('([a-z0-9])([A-Z])', r'\\1_\\2', s1).lower()", "response": "Convert text to snake case."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef to_camel_case(text):\n    split = text.split('_')\n    return split[0] + \"\".join(x.title() for x in split[1:])", "response": "Convert to camel case."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef safe_compare_digest(val1, val2):\n    if len(val1) != len(val2):\n        return False\n\n    result = 0\n    if PY3 and isinstance(val1, bytes) and isinstance(val2, bytes):\n        for i, j in zip(val1, val2):\n            result |= i ^ j\n    else:\n        for i, j in zip(val1, val2):\n            result |= (ord(i) ^ ord(j))\n\n    return result == 0", "response": "safe_compare_digest method.\n\n    :param val1: string or bytes for compare\n    :type val1: str | bytes\n    :param val2: string or bytes for compare\n    :type val2: str | bytes"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns dictionary from this object.", "response": "def as_json_dict(self):\n        \"\"\"Return dictionary from this object.\n\n        :return: dict\n        \"\"\"\n        data = {}\n        for key, value in self.__dict__.items():\n            camel_key = utils.to_camel_case(key)\n            if isinstance(value, (list, tuple, set)):\n                data[camel_key] = list()\n                for item in value:\n                    if hasattr(item, 'as_json_dict'):\n                        data[camel_key].append(item.as_json_dict())\n                    else:\n                        data[camel_key].append(item)\n\n            elif hasattr(value, 'as_json_dict'):\n                data[camel_key] = value.as_json_dict()\n            elif value is not None:\n                data[camel_key] = value\n\n        return data"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncreates a new instance from a dict.", "response": "def new_from_json_dict(cls, data):\n        \"\"\"Create a new instance from a dict.\n\n        :param data: JSON dict\n        :rtype:\n        :return:\n        \"\"\"\n        new_data = {utils.to_snake_case(key): value\n                    for key, value in data.items()}\n\n        return cls(**new_data)"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_or_new_from_json_dict(data, cls):\n        if isinstance(data, cls):\n            return data\n        elif isinstance(data, dict):\n            return cls.new_from_json_dict(data)\n\n        return None", "response": "Get or create a new object from json dict if needed."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_or_new_from_json_dict_with_types(\n            data, cls_map, type_key='type'\n    ):\n        \"\"\"Get `cls` object w/ deserialization from json by using type key hint if needed.\n\n        If data is instance of one of cls, return data.\n        Else if data is instance of dict, create instance from dict.\n        Else, return None.\n\n        :param data:\n        :param cls_map:\n        :param type_key:\n        :rtype: object\n        :return:\n        \"\"\"\n        if isinstance(data, tuple(cls_map.values())):\n            return data\n        elif isinstance(data, dict):\n            type_val = data[type_key]\n            if type_val in cls_map:\n                return cls_map[type_val].new_from_json_dict(data)\n\n        return None", "response": "Get or create a new object from json dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef reply_message(self, reply_token, messages, timeout=None):\n        if not isinstance(messages, (list, tuple)):\n            messages = [messages]\n\n        data = {\n            'replyToken': reply_token,\n            'messages': [message.as_json_dict() for message in messages]\n        }\n\n        self._post(\n            '/v2/bot/message/reply', data=json.dumps(data), timeout=timeout\n        )", "response": "Call reply message API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsending messages to users groups and rooms at any time.", "response": "def push_message(self, to, messages, timeout=None):\n        \"\"\"Call push message API.\n\n        https://devdocs.line.me/en/#push-message\n\n        Send messages to users, groups, and rooms at any time.\n\n        :param str to: ID of the receiver\n        :param messages: Messages.\n            Max: 5\n        :type messages: T <= :py:class:`linebot.models.send_messages.SendMessage` |\n            list[T <= :py:class:`linebot.models.send_messages.SendMessage`]\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        \"\"\"\n        if not isinstance(messages, (list, tuple)):\n            messages = [messages]\n\n        data = {\n            'to': to,\n            'messages': [message.as_json_dict() for message in messages]\n        }\n\n        self._post(\n            '/v2/bot/message/push', data=json.dumps(data), timeout=timeout\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncall get profile API.", "response": "def get_profile(self, user_id, timeout=None):\n        \"\"\"Call get profile API.\n\n        https://devdocs.line.me/en/#bot-api-get-profile\n\n        Get user profile information.\n\n        :param str user_id: User ID\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        :rtype: :py:class:`linebot.models.responses.Profile`\n        :return: Profile instance\n        \"\"\"\n        response = self._get(\n            '/v2/bot/profile/{user_id}'.format(user_id=user_id),\n            timeout=timeout\n        )\n\n        return Profile.new_from_json_dict(response.json)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls get group member profile API.", "response": "def get_group_member_profile(self, group_id, user_id, timeout=None):\n        \"\"\"Call get group member profile API.\n\n        https://devdocs.line.me/en/#get-group-room-member-profile\n\n        Gets the user profile of a member of a group that\n        the bot is in. This can be the user ID of a user who has\n        not added the bot as a friend or has blocked the bot.\n\n        :param str group_id: Group ID\n        :param str user_id: User ID\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        :rtype: :py:class:`linebot.models.responses.Profile`\n        :return: Profile instance\n        \"\"\"\n        response = self._get(\n            '/v2/bot/group/{group_id}/member/{user_id}'.format(group_id=group_id, user_id=user_id),\n            timeout=timeout\n        )\n\n        return Profile.new_from_json_dict(response.json)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ncalls get room member profile API.", "response": "def get_room_member_profile(self, room_id, user_id, timeout=None):\n        \"\"\"Call get room member profile API.\n\n        https://devdocs.line.me/en/#get-group-room-member-profile\n\n        Gets the user profile of a member of a room that\n        the bot is in. This can be the user ID of a user who has\n        not added the bot as a friend or has blocked the bot.\n\n        :param str room_id: Room ID\n        :param str user_id: User ID\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        :rtype: :py:class:`linebot.models.responses.Profile`\n        :return: Profile instance\n        \"\"\"\n        response = self._get(\n            '/v2/bot/room/{room_id}/member/{user_id}'.format(room_id=room_id, user_id=user_id),\n            timeout=timeout\n        )\n\n        return Profile.new_from_json_dict(response.json)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_group_member_ids(self, group_id, start=None, timeout=None):\n        params = None if start is None else {'start': start}\n\n        response = self._get(\n            '/v2/bot/group/{group_id}/members/ids'.format(group_id=group_id),\n            params=params,\n            timeout=timeout\n        )\n\n        return MemberIds.new_from_json_dict(response.json)", "response": "Call get group member IDs API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall get room member IDs API.", "response": "def get_room_member_ids(self, room_id, start=None, timeout=None):\n        \"\"\"Call get room member IDs API.\n\n        https://devdocs.line.me/en/#get-group-room-member-ids\n\n        Gets the user IDs of the members of a group that the bot is in.\n        This includes the user IDs of users who have not added the bot as a friend\n        or has blocked the bot.\n\n        :param str room_id: Room ID\n        :param str start: continuationToken\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        :rtype: :py:class:`linebot.models.responses.MemberIds`\n        :return: MemberIds instance\n        \"\"\"\n        params = None if start is None else {'start': start}\n\n        response = self._get(\n            '/v2/bot/room/{room_id}/members/ids'.format(room_id=room_id),\n            params=params,\n            timeout=timeout\n        )\n\n        return MemberIds.new_from_json_dict(response.json)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_message_content(self, message_id, timeout=None):\n        response = self._get(\n            '/v2/bot/message/{message_id}/content'.format(message_id=message_id),\n            stream=True, timeout=timeout\n        )\n\n        return Content(response)", "response": "Call get content API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncall leave group API.", "response": "def leave_group(self, group_id, timeout=None):\n        \"\"\"Call leave group API.\n\n        https://devdocs.line.me/en/#leave\n\n        Leave a group.\n\n        :param str group_id: Group ID\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        \"\"\"\n        self._post(\n            '/v2/bot/group/{group_id}/leave'.format(group_id=group_id),\n            timeout=timeout\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef leave_room(self, room_id, timeout=None):\n        self._post(\n            '/v2/bot/room/{room_id}/leave'.format(room_id=room_id),\n            timeout=timeout\n        )", "response": "Call leave room API."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_rich_menu(self, rich_menu_id, timeout=None):\n        response = self._get(\n            '/v2/bot/richmenu/{rich_menu_id}'.format(rich_menu_id=rich_menu_id),\n            timeout=timeout\n        )\n\n        return RichMenuResponse.new_from_json_dict(response.json)", "response": "Call get rich menu API."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalling create rich menu API.", "response": "def create_rich_menu(self, rich_menu, timeout=None):\n        \"\"\"Call create rich menu API.\n\n        https://developers.line.me/en/docs/messaging-api/reference/#create-rich-menu\n\n        :param rich_menu: Inquired to create a rich menu object.\n        :type rich_menu: T <= :py:class:`linebot.models.rich_menu.RichMenu`\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        :rtype: str\n        :return: rich menu id\n        \"\"\"\n        response = self._post(\n            '/v2/bot/richmenu', data=rich_menu.as_json_string(), timeout=timeout\n        )\n\n        return response.json.get('richMenuId')"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndelete a rich menu from the server.", "response": "def delete_rich_menu(self, rich_menu_id, timeout=None):\n        \"\"\"Call delete rich menu API.\n\n        https://developers.line.me/en/docs/messaging-api/reference/#delete-rich-menu\n\n        :param str rich_menu_id: ID of an uploaded rich menu\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        \"\"\"\n        self._delete(\n            '/v2/bot/richmenu/{rich_menu_id}'.format(rich_menu_id=rich_menu_id),\n            timeout=timeout\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_rich_menu_id_of_user(self, user_id, timeout=None):\n        response = self._get(\n            '/v2/bot/user/{user_id}/richmenu'.format(user_id=user_id),\n            timeout=timeout\n        )\n\n        return response.json.get('richMenuId')", "response": "Call get rich menu ID of user API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlinking rich menu to user API.", "response": "def link_rich_menu_to_user(self, user_id, rich_menu_id, timeout=None):\n        \"\"\"Call link rich menu to user API.\n\n        https://developers.line.me/en/docs/messaging-api/reference/#link-rich-menu-to-user\n\n        :param str user_id: ID of an uploaded rich menu\n        :param str rich_menu_id: ID of the user\n        :type timeout: float | tuple(float, float)\n        \"\"\"\n        self._post(\n            '/v2/bot/user/{user_id}/richmenu/{rich_menu_id}'.format(\n                user_id=user_id,\n                rich_menu_id=rich_menu_id\n            ),\n            timeout=timeout\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef unlink_rich_menu_from_user(self, user_id, timeout=None):\n        self._delete(\n            '/v2/bot/user/{user_id}/richmenu'.format(user_id=user_id),\n            timeout=timeout\n        )", "response": "Call unlink rich menu from user API."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall download rich menu image API.", "response": "def get_rich_menu_image(self, rich_menu_id, timeout=None):\n        \"\"\"Call download rich menu image API.\n\n        https://developers.line.me/en/docs/messaging-api/reference/#download-rich-menu-image\n\n        :param str rich_menu_id: ID of the rich menu with the image to be downloaded\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        :rtype: :py:class:`linebot.models.responses.Content`\n        :return: Content instance\n        \"\"\"\n        response = self._get(\n            '/v2/bot/richmenu/{rich_menu_id}/content'.format(rich_menu_id=rich_menu_id),\n            timeout=timeout\n        )\n\n        return Content(response)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nupload an image to a rich menu.", "response": "def set_rich_menu_image(self, rich_menu_id, content_type, content, timeout=None):\n        \"\"\"Call upload rich menu image API.\n\n        https://developers.line.me/en/docs/messaging-api/reference/#upload-rich-menu-image\n\n        Uploads and attaches an image to a rich menu.\n\n        :param str rich_menu_id: IDs of the richmenu\n        :param str content_type: image/jpeg or image/png\n        :param content: image content as bytes, or file-like object\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        \"\"\"\n        self._post(\n            '/v2/bot/richmenu/{rich_menu_id}/content'.format(rich_menu_id=rich_menu_id),\n            data=content,\n            headers={'Content-Type': content_type},\n            timeout=timeout\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function to\ncall get rich menu list API.", "response": "def get_rich_menu_list(self, timeout=None):\n        \"\"\"Call get rich menu list API.\n\n        https://developers.line.me/en/docs/messaging-api/reference/#get-rich-menu-list\n\n        :param timeout: (optional) How long to wait for the server\n            to send data before giving up, as a float,\n            or a (connect timeout, read timeout) float tuple.\n            Default is self.http_client.timeout\n        :type timeout: float | tuple(float, float)\n        :rtype: list(T <= :py:class:`linebot.models.reponse.RichMenuResponse`)\n        :return: list[RichMenuResponse] instance\n        \"\"\"\n        response = self._get(\n            '/v2/bot/richmenu/list',\n            timeout=timeout\n        )\n\n        result = []\n        for richmenu in response.json['richmenus']:\n            result.append(RichMenuResponse.new_from_json_dict(richmenu))\n\n        return result"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\ncheck signature. https://devdocs.line.me/en/#webhook-authentication :param str body: Request body (as text) :param str signature: X-Line-Signature value (as text) :rtype: bool :return: result", "response": "def validate(self, body, signature):\n        \"\"\"Check signature.\n\n        https://devdocs.line.me/en/#webhook-authentication\n\n        :param str body: Request body (as text)\n        :param str signature: X-Line-Signature value (as text)\n        :rtype: bool\n        :return: result\n        \"\"\"\n        gen_signature = hmac.new(\n            self.channel_secret,\n            body.encode('utf-8'),\n            hashlib.sha256\n        ).digest()\n\n        return compare_digest(\n                signature.encode('utf-8'), base64.b64encode(gen_signature)\n        )"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef parse(self, body, signature):\n        if not self.signature_validator.validate(body, signature):\n            raise InvalidSignatureError(\n                'Invalid signature. signature=' + signature)\n\n        body_json = json.loads(body)\n        events = []\n        for event in body_json['events']:\n            event_type = event['type']\n            if event_type == 'message':\n                events.append(MessageEvent.new_from_json_dict(event))\n            elif event_type == 'follow':\n                events.append(FollowEvent.new_from_json_dict(event))\n            elif event_type == 'unfollow':\n                events.append(UnfollowEvent.new_from_json_dict(event))\n            elif event_type == 'join':\n                events.append(JoinEvent.new_from_json_dict(event))\n            elif event_type == 'leave':\n                events.append(LeaveEvent.new_from_json_dict(event))\n            elif event_type == 'postback':\n                events.append(PostbackEvent.new_from_json_dict(event))\n            elif event_type == 'beacon':\n                events.append(BeaconEvent.new_from_json_dict(event))\n            elif event_type == 'accountLink':\n                events.append(AccountLinkEvent.new_from_json_dict(event))\n            else:\n                LOGGER.warn('Unknown event type. type=' + event_type)\n\n        return events", "response": "Parse webhook request body as text."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef add(self, event, message=None):\n        def decorator(func):\n            if isinstance(message, (list, tuple)):\n                for it in message:\n                    self.__add_handler(func, event, message=it)\n            else:\n                self.__add_handler(func, event, message=message)\n\n            return func\n\n        return decorator", "response": "Decorator to add a handler method to the internal list of related items."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nhandling webhook. :param str body: Webhook request body (as text) :param str signature: X-Line-Signature value (as text)", "response": "def handle(self, body, signature):\n        \"\"\"Handle webhook.\n\n        :param str body: Webhook request body (as text)\n        :param str signature: X-Line-Signature value (as text)\n        \"\"\"\n        events = self.parser.parse(body, signature)\n\n        for event in events:\n            func = None\n            key = None\n\n            if isinstance(event, MessageEvent):\n                key = self.__get_handler_key(\n                    event.__class__, event.message.__class__)\n                func = self._handlers.get(key, None)\n\n            if func is None:\n                key = self.__get_handler_key(event.__class__)\n                func = self._handlers.get(key, None)\n\n            if func is None:\n                func = self._default\n\n            if func is None:\n                LOGGER.info('No handler of ' + key + ' and no default handler')\n            else:\n                args_count = self.__get_args_count(func)\n                if args_count == 0:\n                    func()\n                else:\n                    func(event)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef get_action(action):\n    action_obj = Base.get_or_new_from_json_dict_with_types(\n        action, {\n            'postback': PostbackAction,\n            'message': MessageAction,\n            'uri': URIAction,\n            'datetimepicker': DatetimePickerAction,\n            'camera': CameraAction,\n            'cameraRoll': CameraRollAction,\n            'location': LocationAction,\n        }\n    )\n    return action_obj", "response": "Get action object from json dict."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef save_structure_to_file(structure: JsonExportable, filename: str) -> None:\n    json_structure = {'node': structure._asdict(),\n                      'instaloader': {'version': __version__, 'node_type': structure.__class__.__name__}}\n    compress = filename.endswith('.xz')\n    if compress:\n        with lzma.open(filename, 'wt', check=lzma.CHECK_NONE) as fp:\n            json.dump(json_structure, fp=fp, separators=(',', ':'))\n    else:\n        with open(filename, 'wt') as fp:\n            json.dump(json_structure, fp=fp, indent=4, sort_keys=True)", "response": "Saves a structure to a. json file such that it can be loaded by load_structure_from_file."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nloads a Post Profile or StoryItem from a. json. xz file that ends in. json or. json. xz.", "response": "def load_structure_from_file(context: InstaloaderContext, filename: str) -> JsonExportable:\n    \"\"\"Loads a :class:`Post`, :class:`Profile` or :class:`StoryItem` from a '.json' or '.json.xz' file that\n    has been saved by :func:`save_structure_to_file`.\n\n    :param context: :attr:`Instaloader.context` linked to the new object, used for additional queries if neccessary.\n    :param filename: Filename, ends in '.json' or '.json.xz'\n    \"\"\"\n    compressed = filename.endswith('.xz')\n    if compressed:\n        fp = lzma.open(filename, 'rt')\n    else:\n        fp = open(filename, 'rt')\n    json_structure = json.load(fp)\n    fp.close()\n    if 'node' in json_structure and 'instaloader' in json_structure and \\\n            'node_type' in json_structure['instaloader']:\n        node_type = json_structure['instaloader']['node_type']\n        if node_type == \"Post\":\n            return Post(context, json_structure['node'])\n        elif node_type == \"Profile\":\n            return Profile(context, json_structure['node'])\n        elif node_type == \"StoryItem\":\n            return StoryItem(context, json_structure['node'])\n        else:\n            raise InvalidArgumentException(\"{}: Not an Instaloader JSON.\".format(filename))\n    elif 'shortcode' in json_structure:\n        # Post JSON created with Instaloader v3\n        return Post.from_shortcode(context, json_structure['shortcode'])\n    else:\n        raise InvalidArgumentException(\"{}: Not an Instaloader JSON.\".format(filename))"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ncreating a post object from a given shortcode", "response": "def from_shortcode(cls, context: InstaloaderContext, shortcode: str):\n        \"\"\"Create a post object from a given shortcode\"\"\"\n        # pylint:disable=protected-access\n        post = cls(context, {'shortcode': shortcode})\n        post._node = post._full_metadata\n        return post"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef from_mediaid(cls, context: InstaloaderContext, mediaid: int):\n        return cls.from_shortcode(context, Post.mediaid_to_shortcode(mediaid))", "response": "Create a post object from a given mediaid"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the Profile instance of the Post s owner.", "response": "def owner_profile(self) -> 'Profile':\n        \"\"\":class:`Profile` instance of the Post's owner.\"\"\"\n        if not self._owner_profile:\n            if 'username' in self._node['owner']:\n                owner_struct = self._node['owner']\n            else:\n                # Sometimes, the 'owner' structure does not contain the username, only the user's ID.  In that case,\n                # this call triggers downloading of the complete Post metadata struct, where the owner username\n                # is contained.\n                # Note that we cannot use Profile.from_id() here since that would lead us into a recursion.\n                owner_struct = self._full_metadata['owner']\n            self._owner_profile = Profile(self._context, owner_struct)\n        return self._owner_profile"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\niterating over all sidecar nodes of a Post.", "response": "def get_sidecar_nodes(self) -> Iterator[PostSidecarNode]:\n        \"\"\"Sidecar nodes of a Post with typename==GraphSidecar.\"\"\"\n        if self.typename == 'GraphSidecar':\n            for edge in self._field('edge_sidecar_to_children', 'edges'):\n                node = edge['node']\n                is_video = node['is_video']\n                yield PostSidecarNode(is_video=is_video, display_url=node['display_url'],\n                                      video_url=node['video_url'] if is_video else None)"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nreturns the caption of the media.", "response": "def caption(self) -> Optional[str]:\n        \"\"\"Caption.\"\"\"\n        if \"edge_media_to_caption\" in self._node and self._node[\"edge_media_to_caption\"][\"edges\"]:\n            return self._node[\"edge_media_to_caption\"][\"edges\"][0][\"node\"][\"text\"]\n        elif \"caption\" in self._node:\n            return self._node[\"caption\"]"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlist of all lowercased hashtags that occur in the Post s caption.", "response": "def caption_hashtags(self) -> List[str]:\n        \"\"\"List of all lowercased hashtags (without preceeding #) that occur in the Post's caption.\"\"\"\n        if not self.caption:\n            return []\n        # This regular expression is from jStassen, adjusted to use Python's \\w to support Unicode\n        # http://blog.jstassen.com/2016/03/code-regex-for-instagram-username-and-hashtags/\n        hashtag_regex = re.compile(r\"(?:#)(\\w(?:(?:\\w|(?:\\.(?!\\.))){0,28}(?:\\w))?)\")\n        return re.findall(hashtag_regex, self.caption.lower())"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef caption_mentions(self) -> List[str]:\n        if not self.caption:\n            return []\n        # This regular expression is from jStassen, adjusted to use Python's \\w to support Unicode\n        # http://blog.jstassen.com/2016/03/code-regex-for-instagram-username-and-hashtags/\n        mention_regex = re.compile(r\"(?:@)(\\w(?:(?:\\w|(?:\\.(?!\\.))){0,28}(?:\\w))?)\")\n        return re.findall(mention_regex, self.caption.lower())", "response": "List of all lowercased profiles that are mentioned in the Post s caption without preceeding @."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nlists of all lowercased users that are tagged in the Post.", "response": "def tagged_users(self) -> List[str]:\n        \"\"\"List of all lowercased users that are tagged in the Post.\"\"\"\n        try:\n            return [edge['node']['user']['username'].lower() for edge in self._field('edge_media_to_tagged_user',\n                                                                                     'edges')]\n        except KeyError:\n            return []"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef viewer_has_liked(self) -> Optional[bool]:\n        if not self._context.is_logged_in:\n            return None\n        if 'likes' in self._node and 'viewer_has_liked' in self._node['likes']:\n            return self._node['likes']['viewer_has_liked']\n        return self._field('viewer_has_liked')", "response": "Whether the viewer has liked the post or None if not logged in."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_comments(self) -> Iterator[PostComment]:\n        def _postcommentanswer(node):\n            return PostCommentAnswer(id=int(node['id']),\n                                     created_at_utc=datetime.utcfromtimestamp(node['created_at']),\n                                     text=node['text'],\n                                     owner=Profile(self._context, node['owner']))\n\n        def _postcommentanswers(node):\n            if 'edge_threaded_comments' not in node:\n                return\n            answer_count = node['edge_threaded_comments']['count']\n            if answer_count == 0:\n                # Avoid doing additional requests if there are no comment answers\n                return\n            answer_edges = node['edge_threaded_comments']['edges']\n            if answer_count == len(answer_edges):\n                # If the answer's metadata already contains all comments, don't do GraphQL requests to obtain them\n                yield from (_postcommentanswer(comment['node']) for comment in answer_edges)\n                return\n            yield from (_postcommentanswer(answer_node) for answer_node in\n                        self._context.graphql_node_list(\"51fdd02b67508306ad4484ff574a0b62\",\n                                                        {'comment_id': node['id']},\n                                                        'https://www.instagram.com/p/' + self.shortcode + '/',\n                                                        lambda d: d['data']['comment']['edge_threaded_comments']))\n\n        def _postcomment(node):\n            return PostComment(*_postcommentanswer(node),\n                               answers=_postcommentanswers(node))\n        if self.comments == 0:\n            # Avoid doing additional requests if there are no comments\n            return\n        try:\n            comment_edges = self._field('edge_media_to_parent_comment', 'edges')\n            answers_count = sum([edge['node']['edge_threaded_comments']['count'] for edge in comment_edges])\n            threaded_comments_available = True\n        except KeyError:\n            comment_edges = self._field('edge_media_to_comment', 'edges')\n            answers_count = 0\n            threaded_comments_available = False\n\n        if self.comments == len(comment_edges) + answers_count:\n            # If the Post's metadata already contains all parent comments, don't do GraphQL requests to obtain them\n            yield from (_postcomment(comment['node']) for comment in comment_edges)\n            return\n        yield from (_postcomment(node) for node in\n                    self._context.graphql_node_list(\n                        \"97b41c52301f77ce508f55e66d17620e\" if threaded_comments_available\n                        else \"f0986789a5c5d17c2400faebf16efd0d\",\n                        {'shortcode': self.shortcode},\n                        'https://www.instagram.com/p/' + self.shortcode + '/',\n                        lambda d:\n                        d['data']['shortcode_media'][\n                            'edge_media_to_parent_comment' if threaded_comments_available else 'edge_media_to_comment'],\n                        self._rhx_gis))", "response": "Iterate over all comments of the post."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef get_likes(self) -> Iterator['Profile']:\n        if self.likes == 0:\n            # Avoid doing additional requests if there are no comments\n            return\n        likes_edges = self._field('edge_media_preview_like', 'edges')\n        if self.likes == len(likes_edges):\n            # If the Post's metadata already contains all likes, don't do GraphQL requests to obtain them\n            yield from (Profile(self._context, like['node']) for like in likes_edges)\n            return\n        yield from (Profile(self._context, node) for node in\n                    self._context.graphql_node_list(\"1cb6ec562846122743b61e492c85999f\", {'shortcode': self.shortcode},\n                                                    'https://www.instagram.com/p/' + self.shortcode + '/',\n                                                    lambda d: d['data']['shortcode_media']['edge_liked_by'],\n                                                    self._rhx_gis))", "response": "Iterate over all likes of the post. A : class:`Profile` instance is yielded."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef location(self) -> Optional[PostLocation]:\n        loc = self._field(\"location\")\n        if self._location or not loc:\n            return self._location\n        location_id = int(loc['id'])\n        if any(k not in loc for k in ('name', 'slug', 'has_public_page', 'lat', 'lng')):\n            loc = self._context.get_json(\"explore/locations/{0}/\".format(location_id),\n                                         params={'__a': 1})['graphql']['location']\n        self._location = PostLocation(location_id, loc['name'], loc['slug'], loc['has_public_page'],\n                                      loc['lat'], loc['lng'])\n        return self._location", "response": "Returns the PostLocation namedtuple with fields id lat and lng and name."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef from_username(cls, context: InstaloaderContext, username: str):\n        # pylint:disable=protected-access\n        profile = cls(context, {'username': username.lower()})\n        profile._obtain_metadata()  # to raise ProfileNotExistException now in case username is invalid\n        return profile", "response": "Create a Profile instance from a given username. Raise exception if it does not exist."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef from_id(cls, context: InstaloaderContext, profile_id: int):\n        if profile_id in context.profile_id_cache:\n            return context.profile_id_cache[profile_id]\n        data = context.graphql_query('7c16654f22c819fb63d1183034a5162f',\n                                     {'user_id': str(profile_id),\n                                      'include_chaining': False,\n                                      'include_reel': True,\n                                      'include_suggested_users': False,\n                                      'include_logged_out_extras': False,\n                                      'include_highlight_reels': False},\n                                     rhx_gis=context.root_rhx_gis)['data']['user']\n        if data:\n            profile = cls(context, data['reel']['owner'])\n        else:\n            raise ProfileNotExistsException(\"No profile found, the user may have blocked you (ID: \" +\n                                            str(profile_id) + \").\")\n        context.profile_id_cache[profile_id] = profile\n        return profile", "response": "Create a Profile instance from a given userid."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef profile_pic_url(self) -> str:\n        if self._context.is_logged_in:\n            try:\n                return self._iphone_struct['hd_profile_pic_url_info']['url']\n            except (InstaloaderException, KeyError) as err:\n                self._context.error('{} Unable to fetch high quality profile pic.'.format(err))\n                return self._metadata(\"profile_pic_url_hd\")\n        else:\n            return self._metadata(\"profile_pic_url_hd\")", "response": "Return URL of profile picture. If logged in the HD version is returned otherwise a lower - quality version."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nretrieving all posts from a profile.", "response": "def get_posts(self) -> Iterator[Post]:\n        \"\"\"Retrieve all posts from a profile.\"\"\"\n        self._obtain_metadata()\n        yield from (Post(self._context, node, self) for node in\n                    self._context.graphql_node_list(\"472f257a40c653c64c666ce877d59d2b\",\n                                                    {'id': self.userid},\n                                                    'https://www.instagram.com/{0}/'.format(self.username),\n                                                    lambda d: d['data']['user']['edge_owner_to_timeline_media'],\n                                                    self._rhx_gis,\n                                                    self._metadata('edge_owner_to_timeline_media')))"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nretrieves all posts where a profile is tagged.", "response": "def get_tagged_posts(self) -> Iterator[Post]:\n        \"\"\"Retrieve all posts where a profile is tagged.\n\n        .. versionadded:: 4.0.7\"\"\"\n        self._obtain_metadata()\n        yield from (Post(self._context, node, self if int(node['owner']['id']) == self.userid else None) for node in\n                    self._context.graphql_node_list(\"e31a871f7301132ceaab56507a66bbb7\",\n                                                    {'id': self.userid},\n                                                    'https://www.instagram.com/{0}/'.format(self.username),\n                                                    lambda d: d['data']['user']['edge_user_to_photos_of_you'],\n                                                    self._rhx_gis))"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nretrieves list of followers of given profile.", "response": "def get_followers(self) -> Iterator['Profile']:\n        \"\"\"\n        Retrieve list of followers of given profile.\n        To use this, one needs to be logged in and private profiles has to be followed.\n        \"\"\"\n        if not self._context.is_logged_in:\n            raise LoginRequiredException(\"--login required to get a profile's followers.\")\n        self._obtain_metadata()\n        yield from (Profile(self._context, node) for node in\n                    self._context.graphql_node_list(\"37479f2b8209594dde7facb0d904896a\",\n                                                    {'id': str(self.userid)},\n                                                    'https://www.instagram.com/' + self.username + '/',\n                                                    lambda d: d['data']['user']['edge_followed_by'],\n                                                    self._rhx_gis))"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef owner_profile(self) -> Profile:\n        if not self._owner_profile:\n            self._owner_profile = Profile.from_id(self._context, self._node['owner']['id'])\n        return self._owner_profile", "response": "Returns the Profile instance of the story item s owner."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nreturn the unique ID of the story.", "response": "def unique_id(self) -> str:\n        \"\"\"\n        This ID only equals amongst :class:`Story` instances which have the same owner and the same set of\n        :class:`StoryItem`. For all other :class:`Story` instances this ID is different.\n        \"\"\"\n        if not self._unique_id:\n            id_list = [item.mediaid for item in self.get_items()]\n            id_list.sort()\n            self._unique_id = str().join([str(self.owner_id)] + list(map(str, id_list)))\n        return self._unique_id"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nretrieve all items from a story.", "response": "def get_items(self) -> Iterator[StoryItem]:\n        \"\"\"Retrieve all items from a story.\"\"\"\n        yield from (StoryItem(self._context, item, self.owner_profile) for item in reversed(self._node['items']))"}
{"SOURCE": "codesearchnet", "instruction": "Can you create a Python 3 function that\nretrieves all associated highlight items.", "response": "def get_items(self) -> Iterator[StoryItem]:\n        \"\"\"Retrieve all associated highlight items.\"\"\"\n        self._fetch_items()\n        yield from (StoryItem(self._context, item, self.owner_profile) for item in self._items)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef copy_session(session: requests.Session) -> requests.Session:\n    new = requests.Session()\n    new.cookies = requests.utils.cookiejar_from_dict(requests.utils.dict_from_cookiejar(session.cookies))\n    new.headers = session.headers.copy()\n    return new", "response": "Duplicates a requests. Session."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef log(self, *msg, sep='', end='\\n', flush=False):\n        if not self.quiet:\n            print(*msg, sep=sep, end=end, flush=flush)", "response": "Log a message to stdout that can be suppressed with - q."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nlogging a non - fatal error message to stderr.", "response": "def error(self, msg, repeat_at_end=True):\n        \"\"\"Log a non-fatal error message to stderr, which is repeated at program termination.\n\n        :param msg: Message to be printed.\n        :param repeat_at_end: Set to false if the message should be printed, but not repeated at program termination.\"\"\"\n        print(msg, file=sys.stderr)\n        if repeat_at_end:\n            self.error_log.append(msg)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef close(self):\n        if self.error_log and not self.quiet:\n            print(\"\\nErrors occured:\", file=sys.stderr)\n            for err in self.error_log:\n                print(err, file=sys.stderr)\n        self._session.close()", "response": "Print error log and close session"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nreturns default HTTP header we use for requests.", "response": "def _default_http_header(self, empty_session_only: bool = False) -> Dict[str, str]:\n        \"\"\"Returns default HTTP header we use for requests.\"\"\"\n        header = {'Accept-Encoding': 'gzip, deflate',\n                  'Accept-Language': 'en-US,en;q=0.8',\n                  'Connection': 'keep-alive',\n                  'Content-Length': '0',\n                  'Host': 'www.instagram.com',\n                  'Origin': 'https://www.instagram.com',\n                  'Referer': 'https://www.instagram.com/',\n                  'User-Agent': self.user_agent,\n                  'X-Instagram-AJAX': '1',\n                  'X-Requested-With': 'XMLHttpRequest'}\n        if empty_session_only:\n            del header['Host']\n            del header['Origin']\n            del header['Referer']\n            del header['X-Instagram-AJAX']\n            del header['X-Requested-With']\n        return header"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nreturns our default anonymous requests. Session object.", "response": "def get_anonymous_session(self) -> requests.Session:\n        \"\"\"Returns our default anonymous requests.Session object.\"\"\"\n        session = requests.Session()\n        session.cookies.update({'sessionid': '', 'mid': '', 'ig_pr': '1',\n                                'ig_vw': '1920', 'csrftoken': '',\n                                's_network': '', 'ds_user_id': ''})\n        session.headers.update(self._default_http_header(empty_session_only=True))\n        return session"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nsave the session to a file.", "response": "def save_session_to_file(self, sessionfile):\n        \"\"\"Not meant to be used directly, use :meth:`Instaloader.save_session_to_file`.\"\"\"\n        pickle.dump(requests.utils.dict_from_cookiejar(self._session.cookies), sessionfile)"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef load_session_from_file(self, username, sessionfile):\n        session = requests.Session()\n        session.cookies = requests.utils.cookiejar_from_dict(pickle.load(sessionfile))\n        session.headers.update(self._default_http_header())\n        session.headers.update({'X-CSRFToken': session.cookies.get_dict()['csrftoken']})\n        self._session = session\n        self.username = username", "response": "Loads a session from a file."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef two_factor_login(self, two_factor_code):\n        if not self.two_factor_auth_pending:\n            raise InvalidArgumentException(\"No two-factor authentication pending.\")\n        (session, user, two_factor_id) = self.two_factor_auth_pending\n\n        login = session.post('https://www.instagram.com/accounts/login/ajax/two_factor/',\n                             data={'username': user, 'verificationCode': two_factor_code, 'identifier': two_factor_id},\n                             allow_redirects=True)\n        resp_json = login.json()\n        if resp_json['status'] != 'ok':\n            if 'message' in resp_json:\n                raise BadCredentialsException(\"Login error: {}\".format(resp_json['message']))\n            else:\n                raise BadCredentialsException(\"Login error: \\\"{}\\\" status.\".format(resp_json['status']))\n        session.headers.update({'X-CSRFToken': login.cookies['csrftoken']})\n        self._session = session\n        self.username = user\n        self.two_factor_auth_pending = None", "response": "Second step of login if 2FA is enabled."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noutputting the number of GraphQL queries grouped by their query_hash within the last time.", "response": "def _dump_query_timestamps(self, current_time: float):\n        \"\"\"Output the number of GraphQL queries grouped by their query_hash within the last time.\"\"\"\n        windows = [10, 11, 15, 20, 30, 60]\n        print(\"GraphQL requests:\", file=sys.stderr)\n        for query_hash, times in self._graphql_query_timestamps.items():\n            print(\"  {}\".format(query_hash), file=sys.stderr)\n            for window in windows:\n                reqs_in_sliding_window = sum(t > current_time - window * 60 for t in times)\n                print(\"    last {} minutes: {} requests\".format(window, reqs_in_sliding_window), file=sys.stderr)"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _graphql_request_count_per_sliding_window(self, query_hash: str) -> int:\n        if self.is_logged_in:\n            max_reqs = {'1cb6ec562846122743b61e492c85999f': 20, '33ba35852cb50da46f5b5e889df7d159': 20}\n        else:\n            max_reqs = {'1cb6ec562846122743b61e492c85999f': 200, '33ba35852cb50da46f5b5e889df7d159': 200}\n        return max_reqs.get(query_hash) or min(max_reqs.values())", "response": "Return how many GraphQL requests can be done within the sliding window."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _graphql_query_waittime(self, query_hash: str, current_time: float, untracked_queries: bool = False) -> int:\n        sliding_window = 660\n        if query_hash not in self._graphql_query_timestamps:\n            self._graphql_query_timestamps[query_hash] = []\n        self._graphql_query_timestamps[query_hash] = list(filter(lambda t: t > current_time - 60 * 60,\n                                                                 self._graphql_query_timestamps[query_hash]))\n        reqs_in_sliding_window = list(filter(lambda t: t > current_time - sliding_window,\n                                             self._graphql_query_timestamps[query_hash]))\n        count_per_sliding_window = self._graphql_request_count_per_sliding_window(query_hash)\n        if len(reqs_in_sliding_window) < count_per_sliding_window and not untracked_queries:\n            return max(0, self._graphql_earliest_next_request_time - current_time)\n        next_request_time = min(reqs_in_sliding_window) + sliding_window + 6\n        if untracked_queries:\n            self._graphql_earliest_next_request_time = next_request_time\n        return round(max(next_request_time, self._graphql_earliest_next_request_time) - current_time)", "response": "Calculate time needed to wait before GraphQL query can be executed."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\ncall before a GraphQL query is made in order to stay within Instagram's rate limits. :param query_hash: The query_hash parameter of the query. :param untracked_queries: True, if 429 has been returned to apply 429 logic.", "response": "def _ratecontrol_graphql_query(self, query_hash: str, untracked_queries: bool = False):\n        \"\"\"Called before a GraphQL query is made in order to stay within Instagram's rate limits.\n\n        :param query_hash: The query_hash parameter of the query.\n        :param untracked_queries: True, if 429 has been returned to apply 429 logic.\n        \"\"\"\n        if not untracked_queries:\n            waittime = self._graphql_query_waittime(query_hash, time.monotonic(), untracked_queries)\n            assert waittime >= 0\n            if waittime > 10:\n                self.log('\\nToo many queries in the last time. Need to wait {} seconds, until {:%H:%M}.'\n                         .format(waittime, datetime.now() + timedelta(seconds=waittime)))\n            time.sleep(waittime)\n            if query_hash not in self._graphql_query_timestamps:\n                self._graphql_query_timestamps[query_hash] = [time.monotonic()]\n            else:\n                self._graphql_query_timestamps[query_hash].append(time.monotonic())\n        else:\n            text_for_429 = (\"HTTP error code 429 was returned because too many queries occured in the last time. \"\n                            \"Please do not use Instagram in your browser or run multiple instances of Instaloader \"\n                            \"in parallel.\")\n            print(textwrap.fill(text_for_429), file=sys.stderr)\n            current_time = time.monotonic()\n            waittime = self._graphql_query_waittime(query_hash, current_time, untracked_queries)\n            assert waittime >= 0\n            if waittime > 10:\n                self.log('The request will be retried in {} seconds, at {:%H:%M}.'\n                         .format(waittime, datetime.now() + timedelta(seconds=waittime)))\n            self._dump_query_timestamps(current_time)\n            time.sleep(waittime)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_json(self, path: str, params: Dict[str, Any], host: str = 'www.instagram.com',\n                 session: Optional[requests.Session] = None, _attempt=1) -> Dict[str, Any]:\n        \"\"\"JSON request to Instagram.\n\n        :param path: URL, relative to the given domain which defaults to www.instagram.com/\n        :param params: GET parameters\n        :param host: Domain part of the URL from where to download the requested JSON; defaults to www.instagram.com\n        :param session: Session to use, or None to use self.session\n        :return: Decoded response dictionary\n        :raises QueryReturnedBadRequestException: When the server responds with a 400.\n        :raises QueryReturnedNotFoundException: When the server responds with a 404.\n        :raises ConnectionException: When query repeatedly failed.\n        \"\"\"\n        is_graphql_query = 'query_hash' in params and 'graphql/query' in path\n        sess = session if session else self._session\n        try:\n            self.do_sleep()\n            if is_graphql_query:\n                self._ratecontrol_graphql_query(params['query_hash'])\n            resp = sess.get('https://{0}/{1}'.format(host, path), params=params, allow_redirects=False)\n            while resp.is_redirect:\n                redirect_url = resp.headers['location']\n                self.log('\\nHTTP redirect from https://{0}/{1} to {2}'.format(host, path, redirect_url))\n                if redirect_url.startswith('https://{}/'.format(host)):\n                    resp = sess.get(redirect_url if redirect_url.endswith('/') else redirect_url + '/',\n                                    params=params, allow_redirects=False)\n                else:\n                    break\n            if resp.status_code == 400:\n                raise QueryReturnedBadRequestException(\"400 Bad Request\")\n            if resp.status_code == 404:\n                raise QueryReturnedNotFoundException(\"404 Not Found\")\n            if resp.status_code == 429:\n                raise TooManyRequestsException(\"429 Too Many Requests\")\n            if resp.status_code != 200:\n                raise ConnectionException(\"HTTP error code {}.\".format(resp.status_code))\n            is_html_query = not is_graphql_query and not \"__a\" in params and host == \"www.instagram.com\"\n            if is_html_query:\n                match = re.search(r'window\\._sharedData = (.*);</script>', resp.text)\n                if match is None:\n                    raise ConnectionException(\"Could not find \\\"window._sharedData\\\" in html response.\")\n                return json.loads(match.group(1))\n            else:\n                resp_json = resp.json()\n            if 'status' in resp_json and resp_json['status'] != \"ok\":\n                if 'message' in resp_json:\n                    raise ConnectionException(\"Returned \\\"{}\\\" status, message \\\"{}\\\".\".format(resp_json['status'],\n                                                                                               resp_json['message']))\n                else:\n                    raise ConnectionException(\"Returned \\\"{}\\\" status.\".format(resp_json['status']))\n            return resp_json\n        except (ConnectionException, json.decoder.JSONDecodeError, requests.exceptions.RequestException) as err:\n            error_string = \"JSON Query to {}: {}\".format(path, err)\n            if _attempt == self.max_connection_attempts:\n                raise ConnectionException(error_string) from err\n            self.error(error_string + \" [retrying; skip with ^C]\", repeat_at_end=False)\n            try:\n                if isinstance(err, TooManyRequestsException):\n                    self._ratecontrol_graphql_query(params['query_hash'], untracked_queries=True)\n                return self.get_json(path=path, params=params, host=host, session=sess, _attempt=_attempt + 1)\n            except KeyboardInterrupt:\n                self.error(\"[skipped by user]\", repeat_at_end=False)\n                raise ConnectionException(error_string) from err", "response": "This method handles the GET request to Instagram and returns the JSON response."}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\ndoes a GraphQL Query.", "response": "def graphql_query(self, query_hash: str, variables: Dict[str, Any],\n                      referer: Optional[str] = None, rhx_gis: Optional[str] = None) -> Dict[str, Any]:\n        \"\"\"\n        Do a GraphQL Query.\n\n        :param query_hash: Query identifying hash.\n        :param variables: Variables for the Query.\n        :param referer: HTTP Referer, or None.\n        :param rhx_gis: 'rhx_gis' variable as somewhere returned by Instagram, needed to 'sign' request\n        :return: The server's response dictionary.\n        \"\"\"\n        with copy_session(self._session) as tmpsession:\n            tmpsession.headers.update(self._default_http_header(empty_session_only=True))\n            del tmpsession.headers['Connection']\n            del tmpsession.headers['Content-Length']\n            tmpsession.headers['authority'] = 'www.instagram.com'\n            tmpsession.headers['scheme'] = 'https'\n            tmpsession.headers['accept'] = '*/*'\n            if referer is not None:\n                tmpsession.headers['referer'] = urllib.parse.quote(referer)\n\n            variables_json = json.dumps(variables, separators=(',', ':'))\n\n            if rhx_gis:\n                #self.log(\"rhx_gis {} query_hash {}\".format(rhx_gis, query_hash))\n                values = \"{}:{}\".format(rhx_gis, variables_json)\n                x_instagram_gis = hashlib.md5(values.encode()).hexdigest()\n                tmpsession.headers['x-instagram-gis'] = x_instagram_gis\n\n            resp_json = self.get_json('graphql/query',\n                                      params={'query_hash': query_hash,\n                                              'variables': variables_json},\n                                      session=tmpsession)\n        if 'status' not in resp_json:\n            self.error(\"GraphQL response did not contain a \\\"status\\\" field.\")\n        return resp_json"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef graphql_node_list(self, query_hash: str, query_variables: Dict[str, Any],\n                          query_referer: Optional[str],\n                          edge_extractor: Callable[[Dict[str, Any]], Dict[str, Any]],\n                          rhx_gis: Optional[str] = None,\n                          first_data: Optional[Dict[str, Any]] = None) -> Iterator[Dict[str, Any]]:\n        \"\"\"Retrieve a list of GraphQL nodes.\"\"\"\n\n        def _query():\n            query_variables['first'] = self._graphql_page_length\n            try:\n                return edge_extractor(self.graphql_query(query_hash, query_variables, query_referer, rhx_gis))\n            except QueryReturnedBadRequestException:\n                new_page_length = int(self._graphql_page_length / 2)\n                if new_page_length >= 12:\n                    self._graphql_page_length = new_page_length\n                    self.error(\"HTTP Error 400 (Bad Request) on GraphQL Query. Retrying with shorter page length.\",\n                               repeat_at_end=False)\n                    return _query()\n                else:\n                    raise\n\n        if first_data:\n            data = first_data\n        else:\n            data = _query()\n        yield from (edge['node'] for edge in data['edges'])\n        while data['page_info']['has_next_page']:\n            query_variables['after'] = data['page_info']['end_cursor']\n            data = _query()\n            yield from (edge['node'] for edge in data['edges'])", "response": "Retrieve a list of GraphQL nodes."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_iphone_json(self, path: str, params: Dict[str, Any]) -> Dict[str, Any]:\n        with copy_session(self._session) as tempsession:\n            tempsession.headers['User-Agent'] = 'Instagram 10.3.2 (iPhone7,2; iPhone OS 9_3_3; en_US; en-US; ' \\\n                                                'scale=2.00; 750x1334) AppleWebKit/420+'\n            for header in ['Host', 'Origin', 'X-Instagram-AJAX', 'X-Requested-With']:\n                tempsession.headers.pop(header, None)\n            return self.get_json(path, params, 'i.instagram.com', tempsession)", "response": "Send a GET request to i. iPhone."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nwrites raw response data into a file.", "response": "def write_raw(self, resp: Union[bytes, requests.Response], filename: str) -> None:\n        \"\"\"Write raw response data into a file.\n\n        .. versionadded:: 4.2.1\"\"\"\n        self.log(filename, end=' ', flush=True)\n        with open(filename, 'wb') as file:\n            if isinstance(resp, requests.Response):\n                shutil.copyfileobj(resp.raw, file)\n            else:\n                file.write(resp)"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\ndownloads a file anonymously.", "response": "def get_raw(self, url: str, _attempt=1) -> requests.Response:\n        \"\"\"Downloads a file anonymously.\n\n        :raises QueryReturnedNotFoundException: When the server responds with a 404.\n        :raises QueryReturnedForbiddenException: When the server responds with a 403.\n        :raises ConnectionException: When download failed.\n\n        .. versionadded:: 4.2.1\"\"\"\n        with self.get_anonymous_session() as anonymous_session:\n            resp = anonymous_session.get(url, stream=True)\n        if resp.status_code == 200:\n            resp.raw.decode_content = True\n            return resp\n        else:\n            if resp.status_code == 403:\n                # suspected invalid URL signature\n                raise QueryReturnedForbiddenException(\"403 when accessing {}.\".format(url))\n            if resp.status_code == 404:\n                # 404 not worth retrying.\n                raise QueryReturnedNotFoundException(\"404 when accessing {}.\".format(url))\n            raise ConnectionException(\"HTTP error code {}.\".format(resp.status_code))"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef get_and_write_raw(self, url: str, filename: str) -> None:\n        self.write_raw(self.get_raw(url), filename)", "response": "Downloads and writes anonymously - requested raw data into a file."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef get_default_session_filename(username: str) -> str:\n    dirname = tempfile.gettempdir() + \"/\" + \".instaloader-\" + getpass.getuser()\n    filename = dirname + \"/\" + \"session-\" + username\n    return filename.lower()", "response": "Returns default session filename for given username."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\noverriding to substitute {ATTRIBUTE } by attributes of our _item.", "response": "def get_value(self, key, args, kwargs):\n        \"\"\"Override to substitute {ATTRIBUTE} by attributes of our _item.\"\"\"\n        if hasattr(self._item, key):\n            return getattr(self._item, key)\n        return super().get_value(key, args, kwargs)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\noverriding string. Formatter. format_field to have our photos default format_spec for datetime objects and to photos let None yield an empty string rather than None.", "response": "def format_field(self, value, format_spec):\n        \"\"\"Override :meth:`string.Formatter.format_field` to have our\n         default format_spec for :class:`datetime.Datetime` objects, and to\n         let None yield an empty string rather than ``None``.\"\"\"\n        if isinstance(value, datetime) and not format_spec:\n            return super().format_field(value, '%Y-%m-%d_%H-%M-%S')\n        if value is None:\n            return ''\n        return super().format_field(value, format_spec)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef vformat(self, format_string, args, kwargs):\n        ret = super().vformat(format_string, args, kwargs)\n        if platform.system() == 'Windows':\n            ret = ret.replace(':', '\\ua789').replace('<', '\\ufe64').replace('>', '\\ufe65').replace('\\\"', '\\uff02')\n            ret = ret.replace('\\\\', '\\uff3c').replace('|', '\\uff5c').replace('?', '\\ufe16').replace('*', '\\uff0a')\n        return ret", "response": "Override string. Formatter. vformat for character substitution in paths for Windows see issue #84."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nyields an anonymous instance of an Instaloader instance ; Then copy its error log.", "response": "def anonymous_copy(self):\n        \"\"\"Yield an anonymous, otherwise equally-configured copy of an Instaloader instance; Then copy its error log.\"\"\"\n        new_loader = Instaloader(self.context.sleep, self.context.quiet, self.context.user_agent, self.dirname_pattern,\n                                 self.filename_pattern, download_pictures=self.download_pictures,\n                                 download_videos=self.download_videos,\n                                 download_video_thumbnails=self.download_video_thumbnails,\n                                 download_geotags=self.download_geotags, download_comments=self.download_comments,\n                                 save_metadata=self.save_metadata, compress_json=self.compress_json,\n                                 post_metadata_txt_pattern=self.post_metadata_txt_pattern,\n                                 storyitem_metadata_txt_pattern=self.storyitem_metadata_txt_pattern,\n                                 max_connection_attempts=self.context.max_connection_attempts)\n        yield new_loader\n        self.context.error_log.extend(new_loader.context.error_log)\n        new_loader.context.error_log = []  # avoid double-printing of errors\n        new_loader.close()"}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef download_pic(self, filename: str, url: str, mtime: datetime,\n                     filename_suffix: Optional[str] = None, _attempt: int = 1) -> bool:\n        \"\"\"Downloads and saves picture with given url under given directory with given timestamp.\n        Returns true, if file was actually downloaded, i.e. updated.\"\"\"\n        urlmatch = re.search('\\\\.[a-z0-9]*\\\\?', url)\n        file_extension = url[-3:] if urlmatch is None else urlmatch.group(0)[1:-1]\n        if filename_suffix is not None:\n            filename += '_' + filename_suffix\n        filename += '.' + file_extension\n        # A post is considered \"commited\" if the json file exists and is not malformed.\n        if self.commit_mode:\n            if self._committed and os.path.isfile(filename):\n                self.context.log(filename + ' exists', end=' ', flush=True)\n                return False\n        else:\n            if os.path.isfile(filename):\n                self.context.log(filename + ' exists', end=' ', flush=True)\n                return False\n        self.context.get_and_write_raw(url, filename)\n        os.utime(filename, (datetime.now().timestamp(), mtime.timestamp()))\n        return True", "response": "Downloads and saves picture with given url under given directory with given timestamp. Returns True if file was actually downloaded i. e. updated."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef save_metadata_json(self, filename: str, structure: JsonExportable) -> None:\n        if self.compress_json:\n            filename += '.json.xz'\n        else:\n            filename += '.json'\n        save_structure_to_file(structure, filename)\n        if isinstance(structure, (Post, StoryItem)):\n            # log 'json ' message when saving Post or StoryItem\n            self.context.log('json', end=' ', flush=True)", "response": "Saves metadata JSON file of a structure."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nsaving the picture caption to the file.", "response": "def save_caption(self, filename: str, mtime: datetime, caption: str) -> None:\n        \"\"\"Updates picture caption / Post metadata info\"\"\"\n        def _elliptify(caption):\n            pcaption = caption.replace('\\n', ' ').strip()\n            return '[' + ((pcaption[:29] + u\"\\u2026\") if len(pcaption) > 31 else pcaption) + ']'\n        filename += '.txt'\n        caption += '\\n'\n        pcaption = _elliptify(caption)\n        caption = caption.encode(\"UTF-8\")\n        with suppress(FileNotFoundError):\n            with open(filename, 'rb') as file:\n                file_caption = file.read()\n            if file_caption.replace(b'\\r\\n', b'\\n') == caption.replace(b'\\r\\n', b'\\n'):\n                try:\n                    self.context.log(pcaption + ' unchanged', end=' ', flush=True)\n                except UnicodeEncodeError:\n                    self.context.log('txt unchanged', end=' ', flush=True)\n                return None\n            else:\n                def get_filename(index):\n                    return filename if index == 0 else '{0}_old_{2:02}{1}'.format(*os.path.splitext(filename), index)\n\n                i = 0\n                while os.path.isfile(get_filename(i)):\n                    i = i + 1\n                for index in range(i, 0, -1):\n                    os.rename(get_filename(index - 1), get_filename(index))\n                try:\n                    self.context.log(_elliptify(file_caption.decode(\"UTF-8\")) + ' updated', end=' ', flush=True)\n                except UnicodeEncodeError:\n                    self.context.log('txt updated', end=' ', flush=True)\n        try:\n            self.context.log(pcaption, end=' ', flush=True)\n        except UnicodeEncodeError:\n            self.context.log('txt', end=' ', flush=True)\n        with open(filename, 'wb') as text_file:\n            shutil.copyfileobj(BytesIO(caption), text_file)\n        os.utime(filename, (datetime.now().timestamp(), mtime.timestamp()))"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef save_location(self, filename: str, location: PostLocation, mtime: datetime) -> None:\n        filename += '_location.txt'\n        location_string = (location.name + \"\\n\" +\n                           \"https://maps.google.com/maps?q={0},{1}&ll={0},{1}\\n\".format(location.lat,\n                                                                                        location.lng))\n        with open(filename, 'wb') as text_file:\n            shutil.copyfileobj(BytesIO(location_string.encode()), text_file)\n        os.utime(filename, (datetime.now().timestamp(), mtime.timestamp()))\n        self.context.log('geo', end=' ', flush=True)", "response": "Save post location name and Google Maps link."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef download_profilepic(self, profile: Profile, _attempt: int = 1) -> None:\n\n        def _epoch_to_string(epoch: datetime) -> str:\n            return epoch.strftime('%Y-%m-%d_%H-%M-%S_UTC')\n\n        profile_pic_response = self.context.get_raw(profile.profile_pic_url)\n        if 'Last-Modified' in profile_pic_response.headers:\n            date_object = datetime.strptime(profile_pic_response.headers[\"Last-Modified\"], '%a, %d %b %Y %H:%M:%S GMT')\n            profile_pic_bytes = None\n            profile_pic_identifier = _epoch_to_string(date_object)\n        else:\n            date_object = None\n            profile_pic_bytes = profile_pic_response.content\n            profile_pic_identifier = md5(profile_pic_bytes).hexdigest()[:16]\n        profile_pic_extension = 'jpg'\n        if ((format_string_contains_key(self.dirname_pattern, 'profile') or\n             format_string_contains_key(self.dirname_pattern, 'target'))):\n            filename = '{0}/{1}_profile_pic.{2}'.format(self.dirname_pattern.format(profile=profile.username.lower(),\n                                                                                    target=profile.username.lower()),\n                                                        profile_pic_identifier, profile_pic_extension)\n        else:\n            filename = '{0}/{1}_{2}_profile_pic.{3}'.format(self.dirname_pattern.format(), profile.username.lower(),\n                                                            profile_pic_identifier, profile_pic_extension)\n        content_length = profile_pic_response.headers.get('Content-Length', None)\n        if os.path.isfile(filename) and (not self.context.is_logged_in or\n                                         content_length is not None and os.path.getsize(filename) >= int(content_length)):\n            self.context.log(filename + ' already exists')\n            return None\n        self.context.write_raw(profile_pic_bytes if profile_pic_bytes else profile_pic_response, filename)\n        if date_object:\n            os.utime(filename, (datetime.now().timestamp(), date_object.timestamp()))\n        self.context.log('')", "response": "Downloads and saves profile pic."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nsaving internally stored requests. Session object to file.", "response": "def save_session_to_file(self, filename: Optional[str] = None) -> None:\n        \"\"\"Saves internally stored :class:`requests.Session` object.\n\n        :param filename: Filename, or None to use default filename.\n        \"\"\"\n        if filename is None:\n            filename = get_default_session_filename(self.context.username)\n        dirname = os.path.dirname(filename)\n        if dirname != '' and not os.path.exists(dirname):\n            os.makedirs(dirname)\n            os.chmod(dirname, 0o700)\n        with open(filename, 'wb') as sessionfile:\n            os.chmod(filename, 0o600)\n            self.context.save_session_to_file(sessionfile)\n            self.context.log(\"Saved session to %s.\" % filename)"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\nload a session object from a file.", "response": "def load_session_from_file(self, username: str, filename: Optional[str] = None) -> None:\n        \"\"\"Internally stores :class:`requests.Session` object loaded from file.\n\n        If filename is None, the file with the default session path is loaded.\n\n        :raises FileNotFoundError: If the file does not exist.\n        \"\"\"\n        if filename is None:\n            filename = get_default_session_filename(username)\n        with open(filename, 'rb') as sessionfile:\n            self.context.load_session_from_file(username, sessionfile)\n            self.context.log(\"Loaded session from %s.\" % filename)"}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nlog in to instagram with given username and password and internally store session object.", "response": "def login(self, user: str, passwd: str) -> None:\n        \"\"\"Log in to instagram with given username and password and internally store session object.\n\n        :raises InvalidArgumentException: If the provided username does not exist.\n        :raises BadCredentialsException: If the provided password is wrong.\n        :raises ConnectionException: If connection to Instagram failed.\n        :raises TwoFactorAuthRequiredException: First step of 2FA login done, now call :meth:`Instaloader.two_factor_login`.\"\"\"\n        self.context.login(user, passwd)"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef format_filename(self, item: Union[Post, StoryItem], target: Optional[str] = None):\n        return _PostPathFormatter(item).format(self.filename_pattern, target=target)", "response": "Format a filename of a post or story item according to filename - pattern parameter."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading all the images and videos associated with a given instagram post.", "response": "def download_post(self, post: Post, target: str) -> bool:\n        \"\"\"\n        Download everything associated with one instagram post node, i.e. picture, caption and video.\n\n        :param post: Post to download.\n        :param target: Target name, i.e. profile name, #hashtag, :feed; for filename.\n        :return: True if something was downloaded, False otherwise, i.e. file was already there\n        \"\"\"\n\n        dirname = _PostPathFormatter(post).format(self.dirname_pattern, target=target)\n        filename = dirname + '/' + self.format_filename(post, target=target)\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n\n        # Download the image(s) / video thumbnail and videos within sidecars if desired\n        downloaded = True\n        self._committed = self.check_if_committed(filename)\n        if self.download_pictures:\n            if post.typename == 'GraphSidecar':\n                edge_number = 1\n                for sidecar_node in post.get_sidecar_nodes():\n                    # Download picture or video thumbnail\n                    if not sidecar_node.is_video or self.download_video_thumbnails is True:\n                        downloaded &= self.download_pic(filename=filename, url=sidecar_node.display_url,\n                                                        mtime=post.date_local, filename_suffix=str(edge_number))\n                    # Additionally download video if available and desired\n                    if sidecar_node.is_video and self.download_videos is True:\n                        downloaded &= self.download_pic(filename=filename, url=sidecar_node.video_url,\n                                                        mtime=post.date_local, filename_suffix=str(edge_number))\n                    edge_number += 1\n            elif post.typename == 'GraphImage':\n                downloaded = self.download_pic(filename=filename, url=post.url, mtime=post.date_local)\n            elif post.typename == 'GraphVideo':\n                if self.download_video_thumbnails is True:\n                    downloaded = self.download_pic(filename=filename, url=post.url, mtime=post.date_local)\n            else:\n                self.context.error(\"Warning: {0} has unknown typename: {1}\".format(post, post.typename))\n\n        # Save caption if desired\n        metadata_string = _ArbitraryItemFormatter(post).format(self.post_metadata_txt_pattern).strip()\n        if metadata_string:\n            self.save_caption(filename=filename, mtime=post.date_local, caption=metadata_string)\n\n        # Download video if desired\n        if post.is_video and self.download_videos is True:\n            downloaded &= self.download_pic(filename=filename, url=post.video_url, mtime=post.date_local)\n\n        # Download geotags if desired\n        if self.download_geotags and post.location:\n            self.save_location(filename, post.location, post.date_local)\n\n        # Update comments if desired\n        if self.download_comments is True:\n            self.update_comments(filename=filename, post=post)\n\n        # Save metadata as JSON if desired.\n        if self.save_metadata is not False:\n            self.save_metadata_json(filename, post)\n\n        self.context.log()\n        return downloaded"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function that can\nget available stories for the given user IDs.", "response": "def get_stories(self, userids: Optional[List[int]] = None) -> Iterator[Story]:\n        \"\"\"Get available stories from followees or all stories of users whose ID are given.\n        Does not mark stories as seen.\n        To use this, one needs to be logged in\n\n        :param userids: List of user IDs to be processed in terms of downloading their stories, or None.\n        \"\"\"\n\n        if not userids:\n            data = self.context.graphql_query(\"d15efd8c0c5b23f0ef71f18bf363c704\",\n                                              {\"only_stories\": True})[\"data\"][\"user\"]\n            if data is None:\n                raise BadResponseException('Bad stories reel JSON.')\n            userids = list(edge[\"node\"][\"id\"] for edge in data[\"feed_reels_tray\"][\"edge_reels_tray_to_reel\"][\"edges\"])\n\n        def _userid_chunks():\n            userids_per_query = 100\n            for i in range(0, len(userids), userids_per_query):\n                yield userids[i:i + userids_per_query]\n\n        for userid_chunk in _userid_chunks():\n            stories = self.context.graphql_query(\"bf41e22b1c4ba4c9f31b844ebb7d9056\",\n                                                 {\"reel_ids\": userid_chunk, \"precomposed_overlay\": False})[\"data\"]\n            yield from (Story(self.context, media) for media in stories['reels_media'])"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef download_stories(self,\n                         userids: Optional[List[Union[int, Profile]]] = None,\n                         fast_update: bool = False,\n                         filename_target: Optional[str] = ':stories',\n                         storyitem_filter: Optional[Callable[[StoryItem], bool]] = None) -> None:\n        \"\"\"\n        Download available stories from user followees or all stories of users whose ID are given.\n        Does not mark stories as seen.\n        To use this, one needs to be logged in\n\n        :param userids: List of user IDs or Profiles to be processed in terms of downloading their stories\n        :param fast_update: If true, abort when first already-downloaded picture is encountered\n        :param filename_target: Replacement for {target} in dirname_pattern and filename_pattern\n               or None if profile name should be used instead\n        :param storyitem_filter: function(storyitem), which returns True if given StoryItem should be downloaded\n        \"\"\"\n\n        if not userids:\n            self.context.log(\"Retrieving all visible stories...\")\n        else:\n            userids = [p if isinstance(p, int) else p.userid for p in userids]\n\n        for user_story in self.get_stories(userids):\n            name = user_story.owner_username\n            self.context.log(\"Retrieving stories from profile {}.\".format(name))\n            totalcount = user_story.itemcount\n            count = 1\n            for item in user_story.get_items():\n                if storyitem_filter is not None and not storyitem_filter(item):\n                    self.context.log(\"<{} skipped>\".format(item), flush=True)\n                    continue\n                self.context.log(\"[%3i/%3i] \" % (count, totalcount), end=\"\", flush=True)\n                count += 1\n                with self.context.error_catcher('Download story from user {}'.format(name)):\n                    downloaded = self.download_storyitem(item, filename_target if filename_target else name)\n                    if fast_update and not downloaded:\n                        break", "response": "Download stories from user followees or all stories of users."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ndownload one user story.", "response": "def download_storyitem(self, item: StoryItem, target: str) -> bool:\n        \"\"\"Download one user story.\n\n        :param item: Story item, as in story['items'] for story in :meth:`get_stories`\n        :param target: Replacement for {target} in dirname_pattern and filename_pattern\n        :return: True if something was downloaded, False otherwise, i.e. file was already there\n        \"\"\"\n\n        date_local = item.date_local\n        dirname = _PostPathFormatter(item).format(self.dirname_pattern, target=target)\n        filename = dirname + '/' + self.format_filename(item, target=target)\n        os.makedirs(os.path.dirname(filename), exist_ok=True)\n        downloaded = False\n        if not item.is_video or self.download_video_thumbnails is True:\n            url = item.url\n            downloaded = self.download_pic(filename=filename, url=url, mtime=date_local)\n        if item.is_video and self.download_videos is True:\n            downloaded |= self.download_pic(filename=filename, url=item.video_url, mtime=date_local)\n        # Save caption if desired\n        metadata_string = _ArbitraryItemFormatter(item).format(self.storyitem_metadata_txt_pattern).strip()\n        if metadata_string:\n            self.save_caption(filename=filename, mtime=item.date_local, caption=metadata_string)\n        # Save metadata as JSON if desired.\n        if self.save_metadata is not False:\n            self.save_metadata_json(filename, item)\n        self.context.log()\n        return downloaded"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nget all highlights from a user.", "response": "def get_highlights(self, user: Union[int, Profile]) -> Iterator[Highlight]:\n        \"\"\"Get all highlights from a user.\n        To use this, one needs to be logged in.\n\n        .. versionadded:: 4.1\n\n        :param user: ID or Profile of the user whose highlights should get fetched.\n        \"\"\"\n\n        userid = user if isinstance(user, int) else user.userid\n        data = self.context.graphql_query(\"7c16654f22c819fb63d1183034a5162f\",\n                                          {\"user_id\": userid, \"include_chaining\": False, \"include_reel\": False,\n                                           \"include_suggested_users\": False, \"include_logged_out_extras\": False,\n                                           \"include_highlight_reels\": True})[\"data\"][\"user\"]['edge_highlight_reels']\n        if data is None:\n            raise BadResponseException('Bad highlights reel JSON.')\n        yield from (Highlight(self.context, edge['node'], user if isinstance(user, Profile) else None)\n                    for edge in data['edges'])"}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\ndownloading available highlights from a user.", "response": "def download_highlights(self,\n                            user: Union[int, Profile],\n                            fast_update: bool = False,\n                            filename_target: Optional[str] = None,\n                            storyitem_filter: Optional[Callable[[StoryItem], bool]] = None) -> None:\n        \"\"\"\n        Download available highlights from a user whose ID is given.\n        To use this, one needs to be logged in.\n\n        .. versionadded:: 4.1\n\n        :param user: ID or Profile of the user whose highlights should get downloaded.\n        :param fast_update: If true, abort when first already-downloaded picture is encountered\n        :param filename_target: Replacement for {target} in dirname_pattern and filename_pattern\n               or None if profile name and the highlights' titles should be used instead\n        :param storyitem_filter: function(storyitem), which returns True if given StoryItem should be downloaded\n        \"\"\"\n        for user_highlight in self.get_highlights(user):\n            name = user_highlight.owner_username\n            self.context.log(\"Retrieving highlights \\\"{}\\\" from profile {}\".format(user_highlight.title, name))\n            totalcount = user_highlight.itemcount\n            count = 1\n            for item in user_highlight.get_items():\n                if storyitem_filter is not None and not storyitem_filter(item):\n                    self.context.log(\"<{} skipped>\".format(item), flush=True)\n                    continue\n                self.context.log(\"[%3i/%3i] \" % (count, totalcount), end=\"\", flush=True)\n                count += 1\n                with self.context.error_catcher('Download highlights \\\"{}\\\" from user {}'.format(user_highlight.title, name)):\n                    downloaded = self.download_storyitem(item, filename_target\n                                                         if filename_target\n                                                         else '{}/{}'.format(name, user_highlight.title))\n                    if fast_update and not downloaded:\n                        break"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_feed_posts(self) -> Iterator[Post]:\n\n        data = self.context.graphql_query(\"d6f4427fbe92d846298cf93df0b937d3\", {})[\"data\"]\n\n        while True:\n            feed = data[\"user\"][\"edge_web_feed_timeline\"]\n            yield from (Post(self.context, edge[\"node\"]) for edge in feed[\"edges\"]\n                        if not edge[\"node\"][\"__typename\"] == \"GraphSuggestedUserFeedUnit\")\n            if not feed[\"page_info\"][\"has_next_page\"]:\n                break\n            data = self.context.graphql_query(\"d6f4427fbe92d846298cf93df0b937d3\",\n                                              {'fetch_media_item_count': 12,\n                                               'fetch_media_item_cursor': feed[\"page_info\"][\"end_cursor\"],\n                                               'fetch_comment_count': 4,\n                                               'fetch_like': 10,\n                                               'has_stories': False})[\"data\"]", "response": "Get Posts of the user s feed."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ndownloading pictures from the user s feed.", "response": "def download_feed_posts(self, max_count: int = None, fast_update: bool = False,\n                            post_filter: Optional[Callable[[Post], bool]] = None) -> None:\n        \"\"\"\n        Download pictures from the user's feed.\n\n        Example to download up to the 20 pics the user last liked::\n\n            loader = Instaloader()\n            loader.load_session_from_file('USER')\n            loader.download_feed_posts(max_count=20, fast_update=True,\n                                       post_filter=lambda post: post.viewer_has_liked)\n\n        :param max_count: Maximum count of pictures to download\n        :param fast_update: If true, abort when first already-downloaded picture is encountered\n        :param post_filter: function(post), which returns True if given picture should be downloaded\n        \"\"\"\n        self.context.log(\"Retrieving pictures from your feed...\")\n        count = 1\n        for post in self.get_feed_posts():\n            if max_count is not None and count > max_count:\n                break\n            name = post.owner_username\n            if post_filter is not None and not post_filter(post):\n                self.context.log(\"<pic by %s skipped>\" % name, flush=True)\n                continue\n            self.context.log(\"[%3i] %s \" % (count, name), end=\"\", flush=True)\n            count += 1\n            with self.context.error_catcher('Download feed'):\n                downloaded = self.download_post(post, target=':feed')\n                if fast_update and not downloaded:\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef download_saved_posts(self, max_count: int = None, fast_update: bool = False,\n                             post_filter: Optional[Callable[[Post], bool]] = None) -> None:\n        \"\"\"Download user's saved pictures.\n\n        :param max_count: Maximum count of pictures to download\n        :param fast_update: If true, abort when first already-downloaded picture is encountered\n        :param post_filter: function(post), which returns True if given picture should be downloaded\n        \"\"\"\n        self.context.log(\"Retrieving saved posts...\")\n        count = 1\n        for post in Profile.from_username(self.context, self.context.username).get_saved_posts():\n            if max_count is not None and count > max_count:\n                break\n            if post_filter is not None and not post_filter(post):\n                self.context.log(\"<{} skipped>\".format(post), flush=True)\n                continue\n            self.context.log(\"[{:>3}] \".format(count), end=str(), flush=True)\n            count += 1\n            with self.context.error_catcher('Download saved posts'):\n                downloaded = self.download_post(post, target=':saved')\n                if fast_update and not downloaded:\n                    break", "response": "Download user s saved posts."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef get_location_posts(self, location: str) -> Iterator[Post]:\n        has_next_page = True\n        end_cursor = None\n        while has_next_page:\n            if end_cursor:\n                params = {'__a': 1, 'max_id': end_cursor}\n            else:\n                params = {'__a': 1}\n            location_data = self.context.get_json('explore/locations/{0}/'.format(location),\n                                                  params)['graphql']['location']['edge_location_to_media']\n            yield from (Post(self.context, edge['node']) for edge in location_data['edges'])\n            has_next_page = location_data['page_info']['has_next_page']\n            end_cursor = location_data['page_info']['end_cursor']", "response": "Get Posts which are listed by Instagram for a given Location."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_location(self, location: str,\n                          max_count: Optional[int] = None,\n                          post_filter: Optional[Callable[[Post], bool]] = None,\n                          fast_update: bool = False) -> None:\n        \"\"\"Download pictures of one location.\n\n        To download the last 30 pictures with location 362629379, do::\n\n            loader = Instaloader()\n            loader.download_location(362629379, max_count=30)\n\n        :param location: Location to download, as Instagram numerical ID\n        :param max_count: Maximum count of pictures to download\n        :param post_filter: function(post), which returns True if given picture should be downloaded\n        :param fast_update: If true, abort when first already-downloaded picture is encountered\n\n        .. versionadded:: 4.2\n        \"\"\"\n        self.context.log(\"Retrieving pictures for location {}...\".format(location))\n        count = 1\n        for post in self.get_location_posts(location):\n            if max_count is not None and count > max_count:\n                break\n            self.context.log('[{0:3d}] %{1} '.format(count, location), end='', flush=True)\n            if post_filter is not None and not post_filter(post):\n                self.context.log('<skipped>')\n                continue\n            count += 1\n            with self.context.error_catcher('Download location {}'.format(location)):\n                downloaded = self.download_post(post, target='%' + location)\n                if fast_update and not downloaded:\n                    break", "response": "Download pictures of one location."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef get_explore_posts(self) -> Iterator[Post]:\n        data = self.context.get_json('explore/', {})\n        yield from (Post(self.context, node)\n                    for node in self.context.graphql_node_list(\"df0dcc250c2b18d9fd27c5581ef33c7c\",\n                                                               {}, 'https://www.instagram.com/explore/',\n                                                               lambda d: d['data']['user']['edge_web_discover_media'],\n                                                               data['rhx_gis']))", "response": "Get Posts which are worthy of exploring suggested by Instagram."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef get_hashtag_posts(self, hashtag: str) -> Iterator[Post]:\n        has_next_page = True\n        end_cursor = None\n        while has_next_page:\n            if end_cursor:\n                params = {'__a': 1, 'max_id': end_cursor}\n            else:\n                params = {'__a': 1}\n            hashtag_data = self.context.get_json('explore/tags/{0}/'.format(hashtag),\n                                                 params)['graphql']['hashtag']['edge_hashtag_to_media']\n            yield from (Post(self.context, edge['node']) for edge in hashtag_data['edges'])\n            has_next_page = hashtag_data['page_info']['has_next_page']\n            end_cursor = hashtag_data['page_info']['end_cursor']", "response": "Get all posts associated with a hashtag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_hashtag(self, hashtag: str,\n                         max_count: Optional[int] = None,\n                         post_filter: Optional[Callable[[Post], bool]] = None,\n                         fast_update: bool = False) -> None:\n        \"\"\"Download pictures of one hashtag.\n\n        To download the last 30 pictures with hashtag #cat, do::\n\n            loader = Instaloader()\n            loader.download_hashtag('cat', max_count=30)\n\n        :param hashtag: Hashtag to download, without leading '#'\n        :param max_count: Maximum count of pictures to download\n        :param post_filter: function(post), which returns True if given picture should be downloaded\n        :param fast_update: If true, abort when first already-downloaded picture is encountered\n        \"\"\"\n        hashtag = hashtag.lower()\n        self.context.log(\"Retrieving pictures with hashtag {}...\".format(hashtag))\n        count = 1\n        for post in self.get_hashtag_posts(hashtag):\n            if max_count is not None and count > max_count:\n                break\n            self.context.log('[{0:3d}] #{1} '.format(count, hashtag), end='', flush=True)\n            if post_filter is not None and not post_filter(post):\n                self.context.log('<skipped>')\n                continue\n            count += 1\n            with self.context.error_catcher('Download hashtag #{}'.format(hashtag)):\n                downloaded = self.download_post(post, target='#' + hashtag)\n                if fast_update and not downloaded:\n                    break", "response": "Download pictures of a hashtag."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef download_tagged(self, profile: Profile, fast_update: bool = False,\n                        target: Optional[str] = None,\n                        post_filter: Optional[Callable[[Post], bool]] = None) -> None:\n        \"\"\"Download all posts where a profile is tagged.\n\n        .. versionadded:: 4.1\"\"\"\n        if target is None:\n            target = profile.username + '/:tagged'\n        self.context.log(\"Retrieving tagged posts for profile {}.\".format(profile.username))\n        count = 1\n        for post in profile.get_tagged_posts():\n            self.context.log(\"[%3i/???] \" % (count), end=\"\", flush=True)\n            count += 1\n            if post_filter is not None and not post_filter(post):\n                self.context.log('<{} skipped>'.format(post))\n            with self.context.error_catcher('Download tagged {}'.format(profile.username)):\n                downloaded = self.download_post(post, target)\n                if fast_update and not downloaded:\n                    break", "response": "Download all posts tagged by a profile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef save_profile_id(self, profile: Profile):\n        os.makedirs(self.dirname_pattern.format(profile=profile.username,\n                                                target=profile.username), exist_ok=True)\n        with open(self._get_id_filename(profile.username), 'w') as text_file:\n            text_file.write(str(profile.userid) + \"\\n\")\n            self.context.log(\"Stored ID {0} for profile {1}.\".format(profile.userid, profile.username))", "response": "Save ID of profile locally."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef check_profile_id(self, profile_name: str) -> Profile:\n        profile = None\n        with suppress(ProfileNotExistsException):\n            profile = Profile.from_username(self.context, profile_name)\n        profile_exists = profile is not None\n        id_filename = self._get_id_filename(profile_name)\n        try:\n            with open(id_filename, 'rb') as id_file:\n                profile_id = int(id_file.read())\n            if (not profile_exists) or \\\n                    (profile_id != profile.userid):\n                if profile_exists:\n                    self.context.log(\"Profile {0} does not match the stored unique ID {1}.\".format(profile_name,\n                                                                                                   profile_id))\n                else:\n                    self.context.log(\"Trying to find profile {0} using its unique ID {1}.\".format(profile_name,\n                                                                                                  profile_id))\n                profile_from_id = Profile.from_id(self.context, profile_id)\n                newname = profile_from_id.username\n                self.context.log(\"Profile {0} has changed its name to {1}.\".format(profile_name, newname))\n                if ((format_string_contains_key(self.dirname_pattern, 'profile') or\n                     format_string_contains_key(self.dirname_pattern, 'target'))):\n                    os.rename(self.dirname_pattern.format(profile=profile_name.lower(),\n                                                          target=profile_name.lower()),\n                              self.dirname_pattern.format(profile=newname.lower(),\n                                                          target=newname.lower()))\n                else:\n                    os.rename('{0}/{1}_id'.format(self.dirname_pattern.format(), profile_name.lower()),\n                              '{0}/{1}_id'.format(self.dirname_pattern.format(), newname.lower()))\n                return profile_from_id\n            return profile\n        except (FileNotFoundError, ValueError):\n            pass\n        if profile_exists:\n            self.save_profile_id(profile)\n            return profile\n        raise ProfileNotExistsException(\"Profile {0} does not exist.\".format(profile_name))", "response": "Check whether the ID of a profile with given name matches and return the current name of the profile and store ID of the profile."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef download_profiles(self, profiles: Set[Profile],\n                          profile_pic: bool = True, posts: bool = True,\n                          tagged: bool = False, highlights: bool = False, stories: bool = False,\n                          fast_update: bool = False,\n                          post_filter: Optional[Callable[[Post], bool]] = None,\n                          storyitem_filter: Optional[Callable[[Post], bool]] = None,\n                          raise_errors: bool = False):\n        \"\"\"High-level method to download set of profiles.\n\n        :param profiles: Set of profiles to download.\n        :param profile_pic: not :option:`--no-profile-pic`.\n        :param posts: not :option:`--no-posts`.\n        :param tagged: :option:`--tagged`.\n        :param highlights: :option:`--highlights`.\n        :param stories: :option:`--stories`.\n        :param fast_update: :option:`--fast-update`.\n        :param post_filter: :option:`--post-filter`.\n        :param storyitem_filter: :option:`--post-filter`.\n        :param raise_errors:\n           Whether :exc:`LoginRequiredException` and :exc:`PrivateProfileNotFollowedException` should be raised or\n           catched and printed with :meth:`InstaloaderContext.error_catcher`.\n\n        .. versionadded:: 4.1\"\"\"\n\n        def _error_raiser(_str):\n            yield\n\n        error_handler = _error_raiser if raise_errors else self.context.error_catcher\n\n        for profile in profiles:\n            with error_handler(profile.username):\n                profile_name = profile.username\n\n                # Download profile picture\n                if profile_pic:\n                    with self.context.error_catcher('Download profile picture of {}'.format(profile_name)):\n                        self.download_profilepic(profile)\n\n                # Save metadata as JSON if desired.\n                if self.save_metadata:\n                    json_filename = '{0}/{1}_{2}'.format(self.dirname_pattern.format(profile=profile_name,\n                                                                                     target=profile_name),\n                                                         profile_name, profile.userid)\n                    self.save_metadata_json(json_filename, profile)\n\n                # Catch some errors\n                if profile.is_private and (tagged or highlights or posts):\n                    if not self.context.is_logged_in:\n                        raise LoginRequiredException(\"--login=USERNAME required.\")\n                    if not profile.followed_by_viewer and self.context.username != profile.username:\n                        raise PrivateProfileNotFollowedException(\"Private but not followed.\")\n\n                # Download tagged, if requested\n                if tagged:\n                    with self.context.error_catcher('Download tagged of {}'.format(profile_name)):\n                        self.download_tagged(profile, fast_update=fast_update, post_filter=post_filter)\n\n                # Download highlights, if requested\n                if highlights:\n                    with self.context.error_catcher('Download highlights of {}'.format(profile_name)):\n                        self.download_highlights(profile, fast_update=fast_update, storyitem_filter=storyitem_filter)\n\n                # Iterate over pictures and download them\n                if posts:\n                    self.context.log(\"Retrieving posts from profile {}.\".format(profile_name))\n                    totalcount = profile.mediacount\n                    count = 1\n                    for post in profile.get_posts():\n                        self.context.log(\"[%3i/%3i] \" % (count, totalcount), end=\"\", flush=True)\n                        count += 1\n                        if post_filter is not None and not post_filter(post):\n                            self.context.log('<skipped>')\n                            continue\n                        with self.context.error_catcher(\"Download {} of {}\".format(post, profile_name)):\n                            # The PostChangedException gets raised if the Post's id/shortcode changed while obtaining\n                            # additional metadata. This is most likely the case if a HTTP redirect takes place while\n                            # resolving the shortcode URL.\n                            # The `post_changed` variable keeps the fast-update functionality alive: A Post which is\n                            # obained after a redirect has probably already been downloaded as a previous Post of the\n                            # same Profile.\n                            # Observed in issue #225: https://github.com/instaloader/instaloader/issues/225\n                            post_changed = False\n                            while True:\n                                try:\n                                    downloaded = self.download_post(post, target=profile_name)\n                                    break\n                                except PostChangedException:\n                                    post_changed = True\n                                    continue\n                            if fast_update and not downloaded and not post_changed:\n                                break\n\n        if stories and profiles:\n            with self.context.error_catcher(\"Download stories\"):\n                self.context.log(\"Downloading stories\")\n                self.download_stories(userids=list(profiles), fast_update=fast_update, filename_target=None,\n                                      storyitem_filter=storyitem_filter)", "response": "High - level method to download set of profiles."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading one profile .. deprecated:: 4.1 Use :meth:`Instaloader.download_profiles`.", "response": "def download_profile(self, profile_name: Union[str, Profile],\n                         profile_pic: bool = True, profile_pic_only: bool = False,\n                         fast_update: bool = False,\n                         download_stories: bool = False, download_stories_only: bool = False,\n                         download_tagged: bool = False, download_tagged_only: bool = False,\n                         post_filter: Optional[Callable[[Post], bool]] = None,\n                         storyitem_filter: Optional[Callable[[StoryItem], bool]] = None) -> None:\n        \"\"\"Download one profile\n\n        .. deprecated:: 4.1\n           Use :meth:`Instaloader.download_profiles`.\n        \"\"\"\n\n        # Get profile main page json\n        # check if profile does exist or name has changed since last download\n        # and update name and json data if necessary\n        if isinstance(profile_name, str):\n            profile = self.check_profile_id(profile_name.lower())\n        else:\n            profile = profile_name\n\n        profile_name = profile.username\n\n        # Save metadata as JSON if desired.\n        if self.save_metadata is not False:\n            json_filename = '{0}/{1}_{2}'.format(self.dirname_pattern.format(profile=profile_name, target=profile_name),\n                                                 profile_name, profile.userid)\n            self.save_metadata_json(json_filename, profile)\n\n        if self.context.is_logged_in and profile.has_blocked_viewer and not profile.is_private:\n            # raising ProfileNotExistsException invokes \"trying again anonymously\" logic\n            raise ProfileNotExistsException(\"Profile {} has blocked you\".format(profile_name))\n\n        # Download profile picture\n        if profile_pic or profile_pic_only:\n            with self.context.error_catcher('Download profile picture of {}'.format(profile_name)):\n                self.download_profilepic(profile)\n        if profile_pic_only:\n            return\n\n        # Catch some errors\n        if profile.is_private:\n            if not self.context.is_logged_in:\n                raise LoginRequiredException(\"profile %s requires login\" % profile_name)\n            if not profile.followed_by_viewer and \\\n                    self.context.username != profile.username:\n                raise PrivateProfileNotFollowedException(\"Profile %s: private but not followed.\" % profile_name)\n        else:\n            if self.context.is_logged_in and not (download_stories or download_stories_only):\n                self.context.log(\"profile %s could also be downloaded anonymously.\" % profile_name)\n\n        # Download stories, if requested\n        if download_stories or download_stories_only:\n            if profile.has_viewable_story:\n                with self.context.error_catcher(\"Download stories of {}\".format(profile_name)):\n                    self.download_stories(userids=[profile.userid], filename_target=profile_name,\n                                          fast_update=fast_update, storyitem_filter=storyitem_filter)\n            else:\n                self.context.log(\"{} does not have any stories.\".format(profile_name))\n        if download_stories_only:\n            return\n\n        # Download tagged, if requested\n        if download_tagged or download_tagged_only:\n            with self.context.error_catcher('Download tagged of {}'.format(profile_name)):\n                self.download_tagged(profile, fast_update=fast_update, post_filter=post_filter)\n        if download_tagged_only:\n            return\n\n        # Iterate over pictures and download them\n        self.context.log(\"Retrieving posts from profile {}.\".format(profile_name))\n        totalcount = profile.mediacount\n        count = 1\n        for post in profile.get_posts():\n            self.context.log(\"[%3i/%3i] \" % (count, totalcount), end=\"\", flush=True)\n            count += 1\n            if post_filter is not None and not post_filter(post):\n                self.context.log('<skipped>')\n                continue\n            with self.context.error_catcher('Download profile {}'.format(profile_name)):\n                downloaded = self.download_post(post, target=profile_name)\n                if fast_update and not downloaded:\n                    break"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nchecks to see if the current post has been committed.", "response": "def check_if_committed(self, filename: str) -> bool:\n        \"\"\"Checks to see if the current post has been committed.\n\n        A post is considered committed if its json metadata file exists and is not malformed.\n\n        .. versionadded:: 4.2\n        \"\"\"\n        if os.path.isfile(filename + '.json.xz'):\n            filename += '.json.xz'\n        elif os.path.isfile(filename + '.json'):\n            filename += '.json'\n        else:\n            return False\n        try:\n            load_structure_from_file(self.context, filename)\n            return True\n        except (FileNotFoundError, lzma.LZMAError, json.decoder.JSONDecodeError):\n            return False"}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nlogging in and internally stores session and asks user for password interactively.", "response": "def interactive_login(self, username: str) -> None:\n        \"\"\"Logs in and internally stores session, asking user for password interactively.\n\n        :raises LoginRequiredException: when in quiet mode.\n        :raises InvalidArgumentException: If the provided username does not exist.\n        :raises ConnectionException: If connection to Instagram failed.\"\"\"\n        if self.context.quiet:\n            raise LoginRequiredException(\"Quiet mode requires given password or valid session file.\")\n        try:\n            password = None\n            while password is None:\n                password = getpass.getpass(prompt=\"Enter Instagram password for %s: \" % username)\n                try:\n                    self.login(username, password)\n                except BadCredentialsException as err:\n                    print(err, file=sys.stderr)\n                    password = None\n        except TwoFactorAuthRequiredException:\n            while True:\n                try:\n                    code = input(\"Enter 2FA verification code: \")\n                    self.two_factor_login(code)\n                    break\n                except BadCredentialsException:\n                    pass"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef filterstr_to_filterfunc(filter_str: str, item_type: type):\n\n    # The filter_str is parsed, then all names occurring in its AST are replaced by loads to post.<name>. A\n    # function Post->bool is returned which evaluates the filter with the post as 'post' in its namespace.\n\n    class TransformFilterAst(ast.NodeTransformer):\n        def visit_Name(self, node: ast.Name):\n            # pylint:disable=no-self-use\n            if not isinstance(node.ctx, ast.Load):\n                raise InvalidArgumentException(\"Invalid filter: Modifying variables ({}) not allowed.\".format(node.id))\n            if node.id == \"datetime\":\n                return node\n            if not hasattr(item_type, node.id):\n                raise InvalidArgumentException(\"Invalid filter: {} not a {} attribute.\".format(node.id,\n                                                                                               item_type.__name__))\n            new_node = ast.Attribute(ast.copy_location(ast.Name('item', ast.Load()), node), node.id,\n                                     ast.copy_location(ast.Load(), node))\n            return ast.copy_location(new_node, node)\n\n    input_filename = '<command line filter parameter>'\n    compiled_filter = compile(TransformFilterAst().visit(ast.parse(filter_str, filename=input_filename, mode='eval')),\n                              filename=input_filename, mode='eval')\n\n    def filterfunc(item) -> bool:\n        # pylint:disable=eval-used\n        return bool(eval(compiled_filter, {'item': item, 'datetime': datetime.datetime}))\n\n    return filterfunc", "response": "Takes an input filter string and makes a filter_func Callable out of it."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\ndownloading set of profiles, hashtags etc. and handle logging in and session files if desired.", "response": "def _main(instaloader: Instaloader, targetlist: List[str],\n          username: Optional[str] = None, password: Optional[str] = None,\n          sessionfile: Optional[str] = None,\n          download_profile_pic: bool = True, download_posts=True,\n          download_stories: bool = False, download_highlights: bool = False, download_tagged: bool = False,\n          fast_update: bool = False,\n          max_count: Optional[int] = None, post_filter_str: Optional[str] = None,\n          storyitem_filter_str: Optional[str] = None) -> None:\n    \"\"\"Download set of profiles, hashtags etc. and handle logging in and session files if desired.\"\"\"\n    # Parse and generate filter function\n    post_filter = None\n    if post_filter_str is not None:\n        post_filter = filterstr_to_filterfunc(post_filter_str, Post)\n        instaloader.context.log('Only download posts with property \"{}\".'.format(post_filter_str))\n    storyitem_filter = None\n    if storyitem_filter_str is not None:\n        storyitem_filter = filterstr_to_filterfunc(storyitem_filter_str, StoryItem)\n        instaloader.context.log('Only download storyitems with property \"{}\".'.format(storyitem_filter_str))\n    # Login, if desired\n    if username is not None:\n        try:\n            instaloader.load_session_from_file(username, sessionfile)\n        except FileNotFoundError as err:\n            if sessionfile is not None:\n                print(err, file=sys.stderr)\n            instaloader.context.log(\"Session file does not exist yet - Logging in.\")\n        if not instaloader.context.is_logged_in or username != instaloader.test_login():\n            if password is not None:\n                try:\n                    instaloader.login(username, password)\n                except TwoFactorAuthRequiredException:\n                    while True:\n                        try:\n                            code = input(\"Enter 2FA verification code: \")\n                            instaloader.two_factor_login(code)\n                            break\n                        except BadCredentialsException:\n                            pass\n            else:\n                instaloader.interactive_login(username)\n        instaloader.context.log(\"Logged in as %s.\" % username)\n    # Try block for KeyboardInterrupt (save session on ^C)\n    profiles = set()\n    anonymous_retry_profiles = set()\n    try:\n        # Generate set of profiles, already downloading non-profile targets\n        for target in targetlist:\n            if (target.endswith('.json') or target.endswith('.json.xz')) and os.path.isfile(target):\n                with instaloader.context.error_catcher(target):\n                    structure = load_structure_from_file(instaloader.context, target)\n                    if isinstance(structure, Post):\n                        if post_filter is not None and not post_filter(structure):\n                            instaloader.context.log(\"<{} ({}) skipped>\".format(structure, target), flush=True)\n                            continue\n                        instaloader.context.log(\"Downloading {} ({})\".format(structure, target))\n                        instaloader.download_post(structure, os.path.dirname(target))\n                    elif isinstance(structure, StoryItem):\n                        if storyitem_filter is not None and not storyitem_filter(structure):\n                            instaloader.context.log(\"<{} ({}) skipped>\".format(structure, target), flush=True)\n                            continue\n                        instaloader.context.log(\"Attempting to download {} ({})\".format(structure, target))\n                        instaloader.download_storyitem(structure, os.path.dirname(target))\n                    elif isinstance(structure, Profile):\n                        raise InvalidArgumentException(\"Profile JSON are ignored. Pass \\\"{}\\\" to download that profile\"\n                                                       .format(structure.username))\n                    else:\n                        raise InvalidArgumentException(\"{} JSON file not supported as target\"\n                                                       .format(structure.__class__.__name__))\n                continue\n            # strip '/' characters to be more shell-autocompletion-friendly\n            target = target.rstrip('/')\n            with instaloader.context.error_catcher(target):\n                if target[0] == '@':\n                    instaloader.context.log(\"Retrieving followees of %s...\" % target[1:])\n                    profile = Profile.from_username(instaloader.context, target[1:])\n                    for followee in profile.get_followees():\n                        instaloader.save_profile_id(followee)\n                        profiles.add(followee)\n                elif target[0] == '#':\n                    instaloader.download_hashtag(hashtag=target[1:], max_count=max_count, fast_update=fast_update,\n                                                 post_filter=post_filter)\n                elif target[0] == '-':\n                    instaloader.download_post(Post.from_shortcode(instaloader.context, target[1:]), target)\n                elif target[0] == \"%\":\n                    instaloader.download_location(location=target[1:], max_count=max_count, fast_update=fast_update,\n                                                  post_filter=post_filter)\n                elif target == \":feed\":\n                    instaloader.download_feed_posts(fast_update=fast_update, max_count=max_count,\n                                                    post_filter=post_filter)\n                elif target == \":stories\":\n                    instaloader.download_stories(fast_update=fast_update, storyitem_filter=storyitem_filter)\n                elif target == \":saved\":\n                    instaloader.download_saved_posts(fast_update=fast_update, max_count=max_count,\n                                                     post_filter=post_filter)\n                else:\n                    try:\n                        profile = instaloader.check_profile_id(target)\n                        if instaloader.context.is_logged_in and profile.has_blocked_viewer:\n                            if download_profile_pic or ((download_posts or download_tagged) and not profile.is_private):\n                                raise ProfileNotExistsException(\"{} blocked you; But we download her anonymously.\"\n                                                                .format(target))\n                            else:\n                                instaloader.context.error(\"{} blocked you.\".format(target))\n                        else:\n                            profiles.add(profile)\n                    except ProfileNotExistsException as err:\n                        # Not only our profile.has_blocked_viewer condition raises ProfileNotExistsException,\n                        # check_profile_id() also does, since access to blocked profile may be responded with 404.\n                        if instaloader.context.is_logged_in and (download_profile_pic or download_posts or\n                                                                 download_tagged):\n                            instaloader.context.log(err)\n                            instaloader.context.log(\"Trying again anonymously, helps in case you are just blocked.\")\n                            with instaloader.anonymous_copy() as anonymous_loader:\n                                with instaloader.context.error_catcher():\n                                    anonymous_retry_profiles.add(anonymous_loader.check_profile_id(target))\n                                    instaloader.context.error(\"Warning: {} will be downloaded anonymously (\\\"{}\\\").\"\n                                                              .format(target, err))\n                        else:\n                            raise\n        if len(profiles) > 1:\n            instaloader.context.log(\"Downloading {} profiles: {}\".format(len(profiles),\n                                                                         ' '.join([p.username for p in profiles])))\n        if profiles and download_profile_pic and not instaloader.context.is_logged_in:\n            instaloader.context.error(\"Warning: Use --login to download HD version of profile pictures.\")\n        instaloader.download_profiles(profiles,\n                                      download_profile_pic, download_posts, download_tagged, download_highlights,\n                                      download_stories, fast_update, post_filter, storyitem_filter)\n        if anonymous_retry_profiles:\n            instaloader.context.log(\"Downloading anonymously: {}\"\n                                    .format(' '.join([p.username for p in anonymous_retry_profiles])))\n            with instaloader.anonymous_copy() as anonymous_loader:\n                anonymous_loader.download_profiles(anonymous_retry_profiles,\n                                                   download_profile_pic, download_posts, download_tagged,\n                                                   fast_update=fast_update, post_filter=post_filter)\n    except KeyboardInterrupt:\n        print(\"\\nInterrupted by user.\", file=sys.stderr)\n    # Save session if it is useful\n    if instaloader.context.is_logged_in:\n        instaloader.save_session_to_file(sessionfile)\n    # User might be confused if Instaloader does nothing\n    if not targetlist:\n        if instaloader.context.is_logged_in:\n            # Instaloader did at least save a session file\n            instaloader.context.log(\"No targets were specified, thus nothing has been downloaded.\")\n        else:\n            # Instloader did not do anything\n            instaloader.context.log(\"usage:\" + usage_string())"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef right_censor_lifetimes(lifetimes, max_, min_=0):\n    n = lifetimes.shape[0]\n    u = min_ + (max_ - min_) * random.rand(n)\n    observations = np.minimum(u, lifetimes)\n    return observations, lifetimes == observations", "response": "Right censor the deaths uniformly with lifetimes"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_covariates(n, d, n_binary=0, p=0.5):\n    # pylint: disable=chained-comparison\n    assert n_binary >= 0 and n_binary <= d, \"binary must be between 0 and d\"\n    covariates = np.zeros((n, d + 1))\n    covariates[:, : d - n_binary] = random.exponential(1, size=(n, d - n_binary))\n    covariates[:, d - n_binary : -1] = random.binomial(1, p, size=(n, n_binary))\n    covariates[:, -1] = np.ones(n)\n    return covariates", "response": "Generate a set of random non - linear covariates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef constant_coefficients(d, timelines, constant=True, independent=0):\n    return time_varying_coefficients(d, timelines, constant, independent=independent, randgen=random.normal)", "response": "Returns the constant coefficients of the given dataset."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef time_varying_coefficients(d, timelines, constant=False, independent=0, randgen=random.exponential):\n    t = timelines.shape[0]\n    try:\n        a = np.arange(d)\n        random.shuffle(a)\n        independent = a[:independent]\n    except IndexError:\n        pass\n\n    n_funcs = len(FUNCS)\n    coefficients = np.zeros((t, d))\n    data_generators = []\n    for i in range(d):\n        f = FUNCS[random.randint(0, n_funcs)] if not constant else constant_\n        if i in independent:\n            beta = 0\n        else:\n            beta = randgen((1 - constant) * 0.5 / d)\n        coefficients[:, i] = f(timelines, alpha=randgen(2000.0 / t), beta=beta)\n        data_generators.append(f.__doc__)\n\n    df_coefficients = pd.DataFrame(coefficients, columns=data_generators, index=timelines)\n    return df_coefficients", "response": "Time vary coefficients for a single object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef generate_hazard_rates(n, d, timelines, constant=False, independent=0, n_binary=0, model=\"aalen\"):\n    covariates = generate_covariates(n, d, n_binary=n_binary)\n    if model == \"aalen\":\n        coefficients = time_varying_coefficients(d + 1, timelines, independent=independent, constant=constant)\n        hazard_rates = np.dot(covariates, coefficients.T)\n        return (pd.DataFrame(hazard_rates.T, index=timelines), coefficients, pd.DataFrame(covariates))\n    if model == \"cox\":\n        covariates = covariates[:, :-1]\n        coefficients = constant_coefficients(d, timelines, independent)\n        baseline = time_varying_coefficients(1, timelines)\n        hazard_rates = np.exp(np.dot(covariates, coefficients.T)) * baseline[baseline.columns[0]].values\n        coefficients[\"baseline: \" + baseline.columns[0]] = baseline.values\n        return (pd.DataFrame(hazard_rates.T, index=timelines), coefficients, pd.DataFrame(covariates))\n    raise Exception", "response": "Generate the hazard rates for a single site."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef generate_random_lifetimes(hazard_rates, timelines, size=1, censor=None):\n    n = hazard_rates.shape[1]\n    survival_times = np.empty((n, size))\n    cumulative_hazards = cumulative_integral(hazard_rates.values, timelines).T\n\n    for i in range(size):\n        u = random.rand(n, 1)\n        e = -np.log(u)\n        v = (e - cumulative_hazards) < 0\n        cross = v.argmax(1)\n        survival_times[:, i] = timelines[cross]\n        survival_times[cross == 0, i] = np.inf\n\n    if censor is not None:\n        if isinstance(censor, bool):\n            T = timelines.max()\n            rv = T * random.uniform(size=survival_times.shape)\n        else:\n            rv = censor\n\n        observed = np.less_equal(survival_times, rv)\n        survival_times = np.minimum(rv, survival_times)\n        return survival_times.T, observed.T\n    else:\n        return survival_times", "response": "Generate random variates from the survival function."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\ngiving hazard rates reconstruct the survival curves", "response": "def construct_survival_curves(hazard_rates, timelines):\n    \"\"\"\n    Given hazard rates, reconstruct the survival curves\n\n    Parameters\n    ----------\n    hazard_rates: (n,t) array\n    timelines: (t,) the observational times\n\n    Returns\n    -------\n    t: survial curves, (n,t) array\n    \"\"\"\n    cumulative_hazards = cumulative_integral(hazard_rates.values, timelines)\n    return pd.DataFrame(np.exp(-cumulative_hazards), index=timelines)"}
{"SOURCE": "codesearchnet", "instruction": "How would you implement a function in Python 3 that\nfits the Aalen Additive model to a Pandas DataFrame.", "response": "def fit(self, df, duration_col, event_col=None, weights_col=None, show_progress=False):\n        \"\"\"\n        Parameters\n        ----------\n        Fit the Aalen Additive model to a dataset.\n\n        Parameters\n        ----------\n        df: DataFrame\n            a Pandas DataFrame with necessary columns `duration_col` and\n            `event_col` (see below), covariates columns, and special columns (weights).\n            `duration_col` refers to\n            the lifetimes of the subjects. `event_col` refers to whether\n            the 'death' events was observed: 1 if observed, 0 else (censored).\n\n        duration_col: string\n            the name of the column in DataFrame that contains the subjects'\n            lifetimes.\n\n        event_col: string, optional\n            the  name of the column in DataFrame that contains the subjects' death\n            observation. If left as None, assume all individuals are uncensored.\n\n        weights_col: string, optional\n            an optional column in the DataFrame, df, that denotes the weight per subject.\n            This column is expelled and not used as a covariate, but as a weight in the\n            final regression. Default weight is 1.\n            This can be used for case-weights. For example, a weight of 2 means there were two subjects with\n            identical observations.\n            This can be used for sampling weights.\n\n        show_progress: boolean, optional (default=False)\n            Since the fitter is iterative, show iteration number.\n\n\n        Returns\n        -------\n        self: AalenAdditiveFitter\n            self with additional new properties: ``cumulative_hazards_``, etc.\n\n        Examples\n        --------\n        >>> from lifelines import AalenAdditiveFitter\n        >>>\n        >>> df = pd.DataFrame({\n        >>>     'T': [5, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>>     'E': [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n        >>>     'var': [0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2],\n        >>>     'age': [4, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>> })\n        >>>\n        >>> aaf = AalenAdditiveFitter()\n        >>> aaf.fit(df, 'T', 'E')\n        >>> aaf.predict_median(df)\n        >>> aaf.print_summary()\n\n        \"\"\"\n        self._time_fit_was_called = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\") + \" UTC\"\n        self._censoring_type = CensoringType.RIGHT\n\n        df = df.copy()\n\n        self.duration_col = duration_col\n        self.event_col = event_col\n        self.weights_col = weights_col\n\n        self._n_examples = df.shape[0]\n\n        X, T, E, weights = self._preprocess_dataframe(df)\n\n        self.durations = T.copy()\n        self.event_observed = E.copy()\n        self.weights = weights.copy()\n\n        self._norm_std = X.std(0)\n\n        # if we included an intercept, we need to fix not divide by zero.\n        if self.fit_intercept:\n            self._norm_std[\"_intercept\"] = 1.0\n        else:\n            # a _intercept was provided\n            self._norm_std[self._norm_std < 1e-8] = 1.0\n\n        self.hazards_, self.cumulative_hazards_, self.cumulative_variance_ = self._fit_model(\n            normalize(X, 0, self._norm_std), T, E, weights, show_progress\n        )\n        self.hazards_ /= self._norm_std\n        self.cumulative_hazards_ /= self._norm_std\n        self.cumulative_variance_ /= self._norm_std\n        self.confidence_intervals_ = self._compute_confidence_intervals()\n\n        self._index = self.hazards_.index\n\n        self._predicted_hazards_ = self.predict_cumulative_hazard(X).iloc[-1].values.ravel()\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\npredicts the cumulative hazard rates for the individual with the given names.", "response": "def predict_cumulative_hazard(self, X):\n        \"\"\"\n        Returns the hazard rates for the individuals\n\n        Parameters\n        ----------\n        X: a (n,d) covariate numpy array or DataFrame. If a DataFrame, columns\n            can be in any order. If a numpy array, columns must be in the\n            same order as the training data.\n\n        \"\"\"\n        n, _ = X.shape\n\n        cols = _get_index(X)\n        if isinstance(X, pd.DataFrame):\n            order = self.cumulative_hazards_.columns\n            order = order.drop(\"_intercept\") if self.fit_intercept else order\n            X_ = X[order].values\n        else:\n            X_ = X\n\n        X_ = X_ if not self.fit_intercept else np.c_[X_, np.ones((n, 1))]\n\n        timeline = self._index\n        individual_cumulative_hazards_ = pd.DataFrame(\n            np.dot(self.cumulative_hazards_, X_.T), index=timeline, columns=cols\n        )\n\n        return individual_cumulative_hazards_"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef predict_expectation(self, X):\n        index = _get_index(X)\n        t = self._index\n        return pd.DataFrame(trapz(self.predict_survival_function(X)[index].values.T, t), index=index)", "response": "Compute the expected lifetime E [ T ] using covariates X."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef smoothed_hazards_(self, bandwidth=1):\n        timeline = self._index.values\n        return pd.DataFrame(\n            np.dot(epanechnikov_kernel(timeline[:, None], timeline, bandwidth), self.hazards_.values),\n            columns=self.hazards_.columns,\n            index=timeline,\n        )", "response": "Returns a DataFrame with the smoothed hazards."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef summary(self):\n        df = pd.DataFrame(index=self.cumulative_hazards_.columns)\n\n        betas, se = self._compute_slopes()\n        df[\"slope(coef)\"] = betas\n        df[\"se(slope(coef))\"] = se\n        return df", "response": "Summary statistics describing the fit."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nprinting summary statistics describing the fit coefficients and error bounds of the fit.", "response": "def print_summary(self, decimals=2, **kwargs):\n        \"\"\"\n        Print summary statistics describing the fit, the coefficients, and the error bounds.\n\n        Parameters\n        -----------\n        decimals: int, optional (default=2)\n            specify the number of decimal places to show\n        kwargs:\n            print additional meta data in the output (useful to provide model names, dataset names, etc.) when comparing\n            multiple outputs.\n\n        \"\"\"\n\n        # Print information about data first\n        justify = string_justify(18)\n        print(self)\n        print(\"{} = '{}'\".format(justify(\"duration col\"), self.duration_col))\n        print(\"{} = '{}'\".format(justify(\"event col\"), self.event_col))\n        if self.weights_col:\n            print(\"{} = '{}'\".format(justify(\"weights col\"), self.weights_col))\n\n        if self.coef_penalizer > 0:\n            print(\"{} = '{}'\".format(justify(\"coef penalizer\"), self.coef_penalizer))\n\n        if self.smoothing_penalizer > 0:\n            print(\"{} = '{}'\".format(justify(\"smoothing penalizer\"), self.smoothing_penalizer))\n\n        print(\"{} = {}\".format(justify(\"number of subjects\"), self._n_examples))\n        print(\"{} = {}\".format(justify(\"number of events\"), self.event_observed.sum()))\n        print(\"{} = {}\".format(justify(\"time fit was run\"), self._time_fit_was_called))\n\n        for k, v in kwargs.items():\n            print(\"{} = {}\\n\".format(justify(k), v))\n\n        print(end=\"\\n\")\n        print(\"---\")\n\n        df = self.summary\n        print(\n            df.to_string(\n                float_format=format_floats(decimals),\n                formatters={\"p\": format_p_value(decimals), \"exp(coef)\": format_exp_floats(decimals)},\n            )\n        )\n\n        # Significance code explanation\n        print(\"---\")\n        print(\"Concordance = {:.{prec}f}\".format(self.score_, prec=decimals))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict_percentile(self, X, ancillary_X=None, p=0.5):\n        lambda_, rho_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n\n        return pd.DataFrame(lambda_ * np.power(-np.log(p), 1 / rho_), index=_get_index(X))", "response": "Predict the median lifetimes for the individuals."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\npredicting the expectation of lifetimes for the individuals.", "response": "def predict_expectation(self, X, ancillary_X=None):\n        \"\"\"\n        Predict the expectation of lifetimes, :math:`E[T | x]`.\n\n        Parameters\n        ----------\n        X: numpy array or DataFrame\n            a (n,d) covariate numpy array or DataFrame. If a DataFrame, columns\n            can be in any order. If a numpy array, columns must be in the\n            same order as the training data.\n        ancillary_X: numpy array or DataFrame, optional\n            a (n,d) covariate numpy array or DataFrame. If a DataFrame, columns\n            can be in any order. If a numpy array, columns must be in the\n            same order as the training data.\n\n        Returns\n        -------\n        percentiles: DataFrame\n            the median lifetimes for the individuals. If the survival curve of an\n            individual does not cross 0.5, then the result is infinity.\n\n\n        See Also\n        --------\n        predict_median\n        \"\"\"\n        lambda_, rho_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n        return pd.DataFrame((lambda_ * gamma(1 + 1 / rho_)), index=_get_index(X))"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef predict_cumulative_hazard(self, X, times=None, ancillary_X=None):\n        times = coalesce(times, self.timeline, np.unique(self.durations))\n        lambda_, rho_ = self._prep_inputs_for_prediction_and_return_scores(X, ancillary_X)\n\n        return pd.DataFrame(np.outer(times, 1 / lambda_) ** rho_, columns=_get_index(X), index=times)", "response": "Predict the cumulative hazard rate of subjects in X at time points times."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef fit(\n        self,\n        durations,\n        event_observed,\n        event_of_interest,\n        timeline=None,\n        entry=None,\n        label=\"AJ_estimate\",\n        alpha=None,\n        ci_labels=None,\n        weights=None,\n    ):  # pylint: disable=too-many-arguments,too-many-locals\n        \"\"\"\n        Parameters\n        ----------\n          durations: an array or pd.Series of length n -- duration of subject was observed for\n          event_observed: an array, or pd.Series, of length n. Integer indicator of distinct events. Must be\n             only positive integers, where 0 indicates censoring.\n          event_of_interest: integer -- indicator for event of interest. All other integers are considered competing events\n             Ex) event_observed contains 0, 1, 2 where 0:censored, 1:lung cancer, and 2:death. If event_of_interest=1, then death (2)\n             is considered a competing event. The returned cumulative incidence function corresponds to risk of lung cancer\n          timeline: return the best estimate at the values in timelines (positively increasing)\n          entry: an array, or pd.Series, of length n -- relative time when a subject entered the study. This is\n             useful for left-truncated (not left-censored) observations. If None, all members of the population\n             were born at time 0.\n          label: a string to name the column of the estimate.\n          alpha: the alpha value in the confidence intervals. Overrides the initializing\n             alpha for this call to fit only.\n          ci_labels: add custom column names to the generated confidence intervals\n                as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<1-alpha/2>\n          weights: n array, or pd.Series, of length n, if providing a weighted dataset. For example, instead\n              of providing every subject as a single element of `durations` and `event_observed`, one could\n              weigh subject differently.\n\n        Returns\n        -------\n        self : AalenJohansenFitter\n          self, with new properties like ``cumulative_incidence_``.\n        \"\"\"\n\n        self._censoring_type = CensoringType.RIGHT\n\n        # Checking for tied event times\n        ties = self._check_for_duplicates(durations=durations, events=event_observed)\n\n        if ties:\n            warnings.warn(\n                dedent(\n                    \"\"\"Tied event times were detected. The Aalen-Johansen estimator cannot handle tied event times.\n                To resolve ties, data is randomly jittered.\"\"\"\n                ),\n                Warning,\n            )\n            durations = self._jitter(\n                durations=pd.Series(durations),\n                event=pd.Series(event_observed),\n                jitter_level=self._jitter_level,\n                seed=self._seed,\n            )\n\n        alpha = alpha if alpha else self.alpha\n\n        # Creating label for event of interest & indicator for that event\n        event_of_interest = int(event_of_interest)\n        cmprisk_label = \"CIF_\" + str(event_of_interest)\n        self.label_cmprisk = \"observed_\" + str(event_of_interest)\n\n        # Fitting Kaplan-Meier for either event of interest OR competing risk\n        km = KaplanMeierFitter().fit(\n            durations, event_observed=event_observed, timeline=timeline, entry=entry, weights=weights\n        )\n        aj = km.event_table\n        aj[\"overall_survival\"] = km.survival_function_\n        aj[\"lagged_overall_survival\"] = aj[\"overall_survival\"].shift()\n\n        # Setting up table for calculations and to return to user\n        event_spec = pd.Series(event_observed) == event_of_interest\n        self.durations, self.event_observed, *_, event_table = _preprocess_inputs(\n            durations=durations, event_observed=event_spec, timeline=timeline, entry=entry, weights=weights\n        )\n        event_spec_times = event_table[\"observed\"]\n        event_spec_times = event_spec_times.rename(self.label_cmprisk)\n        aj = pd.concat([aj, event_spec_times], axis=1).reset_index()\n\n        # Estimator of Cumulative Incidence (Density) Function\n        aj[cmprisk_label] = (aj[self.label_cmprisk] / aj[\"at_risk\"] * aj[\"lagged_overall_survival\"]).cumsum()\n        aj.loc[0, cmprisk_label] = 0  # Setting initial CIF to be zero\n        aj = aj.set_index(\"event_at\")\n\n        # Setting attributes\n        self._estimation_method = \"cumulative_density_\"\n        self._estimate_name = \"cumulative_density_\"\n        self._predict_label = label\n        self._update_docstrings()\n\n        self._label = label\n        self.cumulative_density_ = pd.DataFrame(aj[cmprisk_label])\n\n        # Technically, cumulative incidence, but consistent with KaplanMeierFitter\n        self.event_table = aj[\n            [\"removed\", \"observed\", self.label_cmprisk, \"censored\", \"entrance\", \"at_risk\"]\n        ]  # Event table\n\n        if self._calc_var:\n            self.variance_, self.confidence_interval_ = self._bounds(\n                aj[\"lagged_overall_survival\"], alpha=alpha, ci_labels=ci_labels\n            )\n        else:\n            self.variance_, self.confidence_interval_ = None, None\n\n        return self", "response": "Fits the Aalen Johansen Fitter to obtain the best estimate of the given duration for the given entry."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 function for\nreturning the extent to jitter tied event times.", "response": "def _jitter(self, durations, event, jitter_level, seed=None):\n        \"\"\"Determine extent to jitter tied event times. Automatically called by fit if tied event times are detected\n        \"\"\"\n        np.random.seed(seed)\n\n        if jitter_level <= 0:\n            raise ValueError(\"The jitter level is less than zero, please select a jitter value greater than 0\")\n\n        event_times = durations[event != 0].copy()\n        n = event_times.shape[0]\n\n        # Determining extent to jitter event times up or down\n        shift = np.random.uniform(low=-1, high=1, size=n) * jitter_level\n        event_times += shift\n        durations_jitter = event_times.align(durations)[0].fillna(durations)\n\n        # Recursive call if event times are still tied after jitter\n        if self._check_for_duplicates(durations=durations_jitter, events=event):\n            return self._jitter(durations=durations_jitter, event=event, jitter_level=jitter_level, seed=seed)\n        return durations_jitter"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef _bounds(self, lagged_survival, alpha, ci_labels):\n        # Preparing environment\n        ci = 1 - alpha\n        df = self.event_table.copy()\n        df[\"Ft\"] = self.cumulative_density_\n        df[\"lagS\"] = lagged_survival.fillna(1)\n        if ci_labels is None:\n            ci_labels = [\"%s_upper_%g\" % (self._predict_label, ci), \"%s_lower_%g\" % (self._predict_label, ci)]\n        assert len(ci_labels) == 2, \"ci_labels should be a length 2 array.\"\n\n        # Have to loop through each time independently. Don't think there is a faster way\n        all_vars = []\n        for _, r in df.iterrows():\n            sf = df.loc[df.index <= r.name].copy()\n            F_t = float(r[\"Ft\"])\n            first_term = np.sum(\n                (F_t - sf[\"Ft\"]) ** 2 * sf[\"observed\"] / sf[\"at_risk\"] / (sf[\"at_risk\"] - sf[\"observed\"])\n            )\n            second_term = np.sum(\n                sf[\"lagS\"] ** 2\n                / sf[\"at_risk\"]\n                * sf[self.label_cmprisk]\n                / sf[\"at_risk\"]\n                * (sf[\"at_risk\"] - sf[self.label_cmprisk])\n                / sf[\"at_risk\"]\n            )\n            third_term = np.sum((F_t - sf[\"Ft\"]) / sf[\"at_risk\"] * sf[\"lagS\"] * sf[self.label_cmprisk] / sf[\"at_risk\"])\n            variance = first_term + second_term - 2 * third_term\n            all_vars.append(variance)\n        df[\"variance\"] = all_vars\n\n        # Calculating Confidence Intervals\n        df[\"F_transformed\"] = np.log(-np.log(df[\"Ft\"]))\n        df[\"se_transformed\"] = np.sqrt(df[\"variance\"]) / (df[\"Ft\"] * np.absolute(np.log(df[\"Ft\"])))\n        zalpha = inv_normal_cdf(1 - alpha / 2)\n        df[ci_labels[0]] = np.exp(-np.exp(df[\"F_transformed\"] + zalpha * df[\"se_transformed\"]))\n        df[ci_labels[1]] = np.exp(-np.exp(df[\"F_transformed\"] - zalpha * df[\"se_transformed\"]))\n        return df[\"variance\"], df[ci_labels]", "response": "This method calculates the bounds of the cluster for the current state of the state of the cluster."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script to\ncheck for duplicated event times in the data set. This is narrowed to detecting duplicated event times where the events are of different types", "response": "def _check_for_duplicates(durations, events):\n        \"\"\"Checks for duplicated event times in the data set. This is narrowed to detecting duplicated event times\n        where the events are of different types\n        \"\"\"\n        # Setting up DataFrame to detect duplicates\n        df = pd.DataFrame({\"t\": durations, \"e\": events})\n\n        # Finding duplicated event times\n        dup_times = df.loc[df[\"e\"] != 0, \"t\"].duplicated(keep=False)\n\n        # Finding duplicated events and event times\n        dup_events = df.loc[df[\"e\"] != 0, [\"t\", \"e\"]].duplicated(keep=False)\n\n        # Detect duplicated times with different event types\n        return (dup_times & (~dup_events)).any()"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the times when one or more survival functions reach the qth percentile.", "response": "def qth_survival_times(q, survival_functions, cdf=False):\n    \"\"\"\n    Find the times when one or more survival functions reach the qth percentile.\n\n    Parameters\n    ----------\n    q: float or array\n      a float between 0 and 1 that represents the time when the survival function hits the qth percentile.\n    survival_functions: a (n,d) DataFrame or numpy array.\n      If DataFrame, will return index values (actual times)\n      If numpy array, will return indices.\n    cdf: boolean, optional\n      When doing left-censored data, cdf=True is used.\n\n    Returns\n    -------\n    float, or DataFrame\n         if d==1, returns a float, np.inf if infinity.\n         if d > 1, an DataFrame containing the first times the value was crossed.\n\n    See Also\n    --------\n    qth_survival_time, median_survival_times\n    \"\"\"\n    # pylint: disable=cell-var-from-loop,misplaced-comparison-constant,no-else-return\n\n    q = pd.Series(q)\n\n    if not ((q <= 1).all() and (0 <= q).all()):\n        raise ValueError(\"q must be between 0 and 1\")\n\n    survival_functions = pd.DataFrame(survival_functions)\n\n    if survival_functions.shape[1] == 1 and q.shape == (1,):\n        q = q[0]\n        # If you add print statements to `qth_survival_time`, you'll see it's called\n        # once too many times. This is expected Pandas behavior\n        # https://stackoverflow.com/questions/21635915/why-does-pandas-apply-calculate-twice\n        return survival_functions.apply(lambda s: qth_survival_time(q, s, cdf=cdf)).iloc[0]\n    else:\n        d = {_q: survival_functions.apply(lambda s: qth_survival_time(_q, s, cdf=cdf)) for _q in q}\n        survival_times = pd.DataFrame(d).T\n\n        #  Typically, one would expect that the output should equal the \"height\" of q.\n        #  An issue can arise if the Series q contains duplicate values. We solve\n        #  this by duplicating the entire row.\n        if q.duplicated().any():\n            survival_times = survival_times.loc[q]\n\n        return survival_times"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef qth_survival_time(q, survival_function, cdf=False):\n    if type(survival_function) is pd.DataFrame:  # pylint: disable=unidiomatic-typecheck\n        if survival_function.shape[1] > 1:\n            raise ValueError(\n                \"Expecting a dataframe (or series) with a single column. Provide that or use utils.qth_survival_times.\"\n            )\n\n        survival_function = survival_function.T.squeeze()\n    if cdf:\n        if survival_function.iloc[0] > q:\n            return -np.inf\n        v = survival_function.index[survival_function.searchsorted([q])[0]]\n    else:\n        if survival_function.iloc[-1] > q:\n            return np.inf\n        v = survival_function.index[(-survival_function).searchsorted([-q])[0]]\n    return v", "response": "Returns the time when a single survival function reaches the qth percentile."}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\njoining multiple event series together into DataFrames. A generalization of `survival_table_from_events` to data with groups. Previously called `group_event_series` pre 0.2.3. Parameters ---------- groups: a (n,) array individuals' group ids. durations: a (n,) array durations of each individual event_observed: a (n,) array event observations, 1 if observed, 0 else. birth_times: a (n,) array when the subject was first observed. A subject's death event is then at [birth times + duration observed]. Normally set to all zeros, but can be positive or negative. limit: Returns ------- unique_groups: np.array array of all the unique groups present removed: DataFrame DataFrame of removal count data at event_times for each group, column names are 'removed:<group name>' observed: DataFrame DataFrame of observed count data at event_times for each group, column names are 'observed:<group name>' censored: DataFrame DataFrame of censored count data at event_times for each group, column names are 'censored:<group name>' Example ------- >>> #input >>> group_survival_table_from_events(waltonG, waltonT, np.ones_like(waltonT)) #data available in test_suite.py >>> #output >>> [ >>> array(['control', 'miR-137'], dtype=object), >>> removed:control removed:miR-137 >>> event_at >>> 6 0 1 >>> 7 2 0 >>> 9 0 3 >>> 13 0 3 >>> 15 0 2 >>> , >>> observed:control observed:miR-137 >>> event_at >>> 6 0 1 >>> 7 2 0 >>> 9 0 3 >>> 13 0 3 >>> 15 0 2 >>> , >>> censored:control censored:miR-137 >>> event_at >>> 6 0 0 >>> 7 0 0 >>> 9 0 0 >>> , >>> ] See Also -------- survival_table_from_events", "response": "def group_survival_table_from_events(\n    groups, durations, event_observed, birth_times=None, limit=-1\n):  # pylint: disable=too-many-locals\n    \"\"\"\n    Joins multiple event series together into DataFrames. A generalization of\n    `survival_table_from_events` to data with groups. Previously called `group_event_series` pre 0.2.3.\n\n    Parameters\n    ----------\n    groups: a (n,) array\n      individuals' group ids.\n    durations: a (n,)  array\n      durations of each individual\n    event_observed: a (n,) array\n      event observations, 1 if observed, 0 else.\n    birth_times: a (n,) array\n      when the subject was first observed. A subject's death event is then at [birth times + duration observed].\n      Normally set to all zeros, but can be positive or negative.\n    limit:\n\n    Returns\n    -------\n    unique_groups: np.array\n      array of all the unique groups present\n    removed: DataFrame\n      DataFrame of removal count data at event_times for each group, column names are 'removed:<group name>'\n    observed: DataFrame\n      DataFrame of observed count data at event_times for each group, column names are 'observed:<group name>'\n    censored: DataFrame\n      DataFrame of censored count data at event_times for each group, column names are 'censored:<group name>'\n\n    Example\n    -------\n    >>> #input\n    >>> group_survival_table_from_events(waltonG, waltonT, np.ones_like(waltonT)) #data available in test_suite.py\n    >>> #output\n    >>> [\n    >>>     array(['control', 'miR-137'], dtype=object),\n    >>>               removed:control  removed:miR-137\n    >>>     event_at\n    >>>     6                       0                1\n    >>>     7                       2                0\n    >>>     9                       0                3\n    >>>     13                      0                3\n    >>>     15                      0                2\n    >>>     ,\n    >>>               observed:control  observed:miR-137\n    >>>     event_at\n    >>>     6                        0                 1\n    >>>     7                        2                 0\n    >>>     9                        0                 3\n    >>>     13                       0                 3\n    >>>     15                       0                 2\n    >>>     ,\n    >>>               censored:control  censored:miR-137\n    >>>     event_at\n    >>>     6                        0                 0\n    >>>     7                        0                 0\n    >>>     9                        0                 0\n    >>>     ,\n    >>> ]\n\n    See Also\n    --------\n    survival_table_from_events\n\n    \"\"\"\n\n    n = np.max(groups.shape)\n    assert n == np.max(durations.shape) == np.max(event_observed.shape), \"inputs must be of the same length.\"\n\n    if birth_times is None:\n        # Create some birth times\n        birth_times = np.zeros(np.max(durations.shape))\n        birth_times[:] = np.min(durations)\n\n    assert n == np.max(birth_times.shape), \"inputs must be of the same length.\"\n\n    groups, durations, event_observed, birth_times = [\n        pd.Series(np.asarray(vector).reshape(n)) for vector in [groups, durations, event_observed, birth_times]\n    ]\n    unique_groups = groups.unique()\n\n    for i, group in enumerate(unique_groups):\n        ix = groups == group\n        T = durations[ix]\n        C = event_observed[ix]\n        B = birth_times[ix]\n        group_name = str(group)\n        columns = [\n            event_name + \":\" + group_name for event_name in [\"removed\", \"observed\", \"censored\", \"entrance\", \"at_risk\"]\n        ]\n        if i == 0:\n            survival_table = survival_table_from_events(T, C, B, columns=columns)\n        else:\n            survival_table = survival_table.join(survival_table_from_events(T, C, B, columns=columns), how=\"outer\")\n\n    survival_table = survival_table.fillna(0)\n    # hmmm pandas its too bad I can't do data.loc[:limit] and leave out the if.\n    if int(limit) != -1:\n        survival_table = survival_table.loc[:limit]\n\n    return (\n        unique_groups,\n        survival_table.filter(like=\"removed:\"),\n        survival_table.filter(like=\"observed:\"),\n        survival_table.filter(like=\"censored:\"),\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\nreturn a DataFrame that contains the survival table for the given events.", "response": "def survival_table_from_events(\n    death_times,\n    event_observed,\n    birth_times=None,\n    columns=[\"removed\", \"observed\", \"censored\", \"entrance\", \"at_risk\"],\n    weights=None,\n    collapse=False,\n    intervals=None,\n):  # pylint: disable=dangerous-default-value,too-many-locals\n    \"\"\"\n    Parameters\n    ----------\n    death_times: (n,) array\n      represent the event times\n    event_observed: (n,) array\n      1 if observed event, 0 is censored event.\n    birth_times: a (n,) array, optional\n      representing when the subject was first observed. A subject's death event is then at [birth times + duration observed].\n      If None (default), birth_times are set to be the first observation or 0, which ever is smaller.\n    columns: iterable, optional\n      a 3-length array to call the, in order, removed individuals, observed deaths\n      and censorships.\n    weights: (n,1) array, optional\n      Optional argument to use weights for individuals. Assumes weights of 1 if not provided.\n    collapse: boolean, optional (default=False)\n      If True, collapses survival table into lifetable to show events in interval bins\n    intervals: iterable, optional\n      Default None, otherwise a list/(n,1) array of interval edge measures. If left as None\n      while collapse=True, then Freedman-Diaconis rule for histogram bins will be used to determine intervals.\n\n    Returns\n    -------\n    DataFrame\n      Pandas DataFrame with index as the unique times or intervals in event_times. The columns named\n      'removed' refers to the number of individuals who were removed from the population\n      by the end of the period. The column 'observed' refers to the number of removed\n      individuals who were observed to have died (i.e. not censored.) The column\n      'censored' is defined as 'removed' - 'observed' (the number of individuals who\n      left the population due to event_observed)\n\n    Example\n    -------\n\n    >>> #Uncollapsed output\n    >>>           removed  observed  censored  entrance   at_risk\n    >>> event_at\n    >>> 0               0         0         0        11        11\n    >>> 6               1         1         0         0        11\n    >>> 7               2         2         0         0        10\n    >>> 9               3         3         0         0         8\n    >>> 13              3         3         0         0         5\n    >>> 15              2         2         0         0         2\n    >>> #Collapsed output\n    >>>          removed observed censored at_risk\n    >>>              sum      sum      sum     max\n    >>> event_at\n    >>> (0, 2]        34       33        1     312\n    >>> (2, 4]        84       42       42     278\n    >>> (4, 6]        64       17       47     194\n    >>> (6, 8]        63       16       47     130\n    >>> (8, 10]       35       12       23      67\n    >>> (10, 12]      24        5       19      32\n\n    See Also\n    --------\n    group_survival_table_from_events\n    \"\"\"\n    removed, observed, censored, entrance, at_risk = columns\n    death_times = np.asarray(death_times)\n    if birth_times is None:\n        birth_times = min(0, death_times.min()) * np.ones(death_times.shape[0])\n    else:\n        birth_times = np.asarray(birth_times)\n        if np.any(birth_times > death_times):\n            raise ValueError(\"birth time must be less than time of death.\")\n\n    if weights is None:\n        weights = 1\n\n    # deal with deaths and censorships\n    df = pd.DataFrame(death_times, columns=[\"event_at\"])\n    df[removed] = np.asarray(weights)\n    df[observed] = np.asarray(weights) * (np.asarray(event_observed).astype(bool))\n    death_table = df.groupby(\"event_at\").sum()\n    death_table[censored] = (death_table[removed] - death_table[observed]).astype(int)\n\n    # deal with late births\n    births = pd.DataFrame(birth_times, columns=[\"event_at\"])\n    births[entrance] = np.asarray(weights)\n    births_table = births.groupby(\"event_at\").sum()\n    event_table = death_table.join(births_table, how=\"outer\", sort=True).fillna(0)  # http://wesmckinney.com/blog/?p=414\n    event_table[at_risk] = event_table[entrance].cumsum() - event_table[removed].cumsum().shift(1).fillna(0)\n\n    # group by intervals\n    if (collapse) or (intervals is not None):\n        event_table = _group_event_table_by_intervals(event_table, intervals)\n\n    if (np.asarray(weights).astype(int) != weights).any():\n        return event_table.astype(float)\n    return event_table.astype(int)"}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef datetimes_to_durations(\n    start_times, end_times, fill_date=datetime.today(), freq=\"D\", dayfirst=False, na_values=None\n):\n    \"\"\"\n    This is a very flexible function for transforming arrays of start_times and end_times\n    to the proper format for lifelines: duration and event observation arrays.\n\n    Parameters\n    ----------\n    start_times: an array, Series or DataFrame\n        iterable representing start times. These can be strings, or datetime objects.\n    end_times: an array, Series or DataFrame\n        iterable representing end times. These can be strings, or datetimes. These values can be None, or an empty string, which corresponds to censorship.\n    fill_date: datetime, optional (default=datetime.Today())\n        the date to use if end_times is a None or empty string. This corresponds to last date\n        of observation. Anything after this date is also censored.\n    freq: string, optional (default='D')\n        the units of time to use.  See Pandas 'freq'. Default 'D' for days.\n    dayfirst: boolean, optional (default=False)\n         convert assuming European-style dates, i.e. day/month/year.\n    na_values : list, optional\n        list of values to recognize as NA/NaN. Ex: ['', 'NaT']\n\n    Returns\n    -------\n    T: numpy array\n        array of floats representing the durations with time units given by freq.\n    C: numpy array\n        boolean array of event observations: 1 if death observed, 0 else.\n\n    Examples\n    --------\n    >>> from lifelines.utils import datetimes_to_durations\n    >>>\n    >>> start_dates = ['2015-01-01', '2015-04-01', '2014-04-05']\n    >>> end_dates = ['2016-02-02', None, '2014-05-06']\n    >>>\n    >>> T, E = datetimes_to_durations(start_dates, end_dates, freq=\"D\")\n    >>> T # array([ 397., 1414.,   31.])\n    >>> E # array([ True, False,  True])\n\n    \"\"\"\n    fill_date = pd.to_datetime(fill_date)\n    freq_string = \"timedelta64[%s]\" % freq\n    start_times = pd.Series(start_times).copy()\n    end_times = pd.Series(end_times).copy()\n\n    C = ~(pd.isnull(end_times).values | end_times.isin(na_values or [\"\"]))\n    end_times[~C] = fill_date\n    start_times_ = pd.to_datetime(start_times, dayfirst=dayfirst)\n    end_times_ = pd.to_datetime(end_times, dayfirst=dayfirst, errors=\"coerce\")\n\n    deaths_after_cutoff = end_times_ > fill_date\n    C[deaths_after_cutoff] = False\n\n    T = (end_times_ - start_times_).values.astype(freq_string).astype(float)\n    if (T < 0).sum():\n        warnings.warn(\"Warning: some values of start_times are after end_times\")\n    return T, C.values", "response": "This function converts a list of datetimes into a list of durations."}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nperforms cross validation on a dataset.", "response": "def k_fold_cross_validation(\n    fitters,\n    df,\n    duration_col,\n    event_col=None,\n    k=5,\n    evaluation_measure=concordance_index,\n    predictor=\"predict_expectation\",\n    predictor_kwargs={},\n    fitter_kwargs={},\n):  # pylint: disable=dangerous-default-value,too-many-arguments,too-many-locals\n    \"\"\"\n    Perform cross validation on a dataset. If multiple models are provided,\n    all models will train on each of the k subsets.\n\n    Parameters\n    ----------\n    fitters: model\n      one or several objects which possess a method: ``fit(self, data, duration_col, event_col)``\n      Note that the last two arguments will be given as keyword arguments,\n      and that event_col is optional. The objects must also have\n      the \"predictor\" method defined below.\n    df: DataFrame\n      a Pandas DataFrame with necessary columns `duration_col` and (optional) `event_col`, plus\n      other covariates. `duration_col` refers to the lifetimes of the subjects. `event_col`\n      refers to whether the 'death' events was observed: 1 if observed, 0 else (censored).\n    duration_col: (n,) array\n      the column in DataFrame that contains the subjects lifetimes.\n    event_col: (n,) array\n      the column in DataFrame that contains the subject's death observation. If left\n      as None, assumes all individuals are non-censored.\n    k: int\n      the number of folds to perform. n/k data will be withheld for testing on.\n    evaluation_measure: function\n      a function that accepts either (event_times, predicted_event_times),\n      or (event_times, predicted_event_times, event_observed)\n      and returns something (could be anything).\n      Default: statistics.concordance_index: (C-index)\n      between two series of event times\n    predictor: string\n      a string that matches a prediction method on the fitter instances.\n      For example, ``predict_expectation`` or ``predict_percentile``.\n      Default is \"predict_expectation\"\n      The interface for the method is: ``predict(self, data, **optional_kwargs)``\n    fitter_kwargs:\n      keyword args to pass into fitter.fit method\n    predictor_kwargs:\n      keyword args to pass into predictor-method.\n\n    Returns\n    -------\n    results: list\n      (k,1) list of scores for each fold. The scores can be anything.\n    \"\"\"\n    # Make sure fitters is a list\n    try:\n        fitters = list(fitters)\n    except TypeError:\n        fitters = [fitters]\n    # Each fitter has its own scores\n    fitterscores = [[] for _ in fitters]\n\n    n, _ = df.shape\n    df = df.copy()\n\n    if event_col is None:\n        event_col = \"E\"\n        df[event_col] = 1.0\n\n    df = df.reindex(np.random.permutation(df.index)).sort_values(event_col)\n\n    assignments = np.array((n // k + 1) * list(range(1, k + 1)))\n    assignments = assignments[:n]\n\n    testing_columns = df.columns.drop([duration_col, event_col])\n\n    for i in range(1, k + 1):\n\n        ix = assignments == i\n        training_data = df.loc[~ix]\n        testing_data = df.loc[ix]\n\n        T_actual = testing_data[duration_col].values\n        E_actual = testing_data[event_col].values\n        X_testing = testing_data[testing_columns]\n\n        for fitter, scores in zip(fitters, fitterscores):\n            # fit the fitter to the training data\n            fitter.fit(training_data, duration_col=duration_col, event_col=event_col, **fitter_kwargs)\n            T_pred = getattr(fitter, predictor)(X_testing, **predictor_kwargs).values\n\n            try:\n                scores.append(evaluation_measure(T_actual, T_pred, E_actual))\n            except TypeError:\n                scores.append(evaluation_measure(T_actual, T_pred))\n\n    # If a single fitter was given as argument, return a single result\n    if len(fitters) == 1:\n        return fitterscores[0]\n    return fitterscores"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef normalize(X, mean=None, std=None):\n    if mean is None or std is None:\n        mean = X.mean(0)\n        std = X.std(0)\n    return (X - mean) / std", "response": "Normalize X to have mean and std 0 and 1."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _additive_estimate(events, timeline, _additive_f, _additive_var, reverse):\n    if reverse:\n        events = events.sort_index(ascending=False)\n        at_risk = events[\"entrance\"].sum() - events[\"removed\"].cumsum().shift(1).fillna(0)\n\n        deaths = events[\"observed\"]\n\n        estimate_ = np.cumsum(_additive_f(at_risk, deaths)).sort_index().shift(-1).fillna(0)\n        var_ = np.cumsum(_additive_var(at_risk, deaths)).sort_index().shift(-1).fillna(0)\n    else:\n        deaths = events[\"observed\"]\n\n        # Why subtract entrants like this? see https://github.com/CamDavidsonPilon/lifelines/issues/497\n        # specifically, we kill people, compute the ratio, and then \"add\" the entrants. This means that\n        # the population should not have the late entrants. The only exception to this rule\n        # is the first period, where entrants happen _prior_ to deaths.\n        entrances = events[\"entrance\"].copy()\n        entrances.iloc[0] = 0\n        population = events[\"at_risk\"] - entrances\n\n        estimate_ = np.cumsum(_additive_f(population, deaths))\n        var_ = np.cumsum(_additive_var(population, deaths))\n\n    timeline = sorted(timeline)\n    estimate_ = estimate_.reindex(timeline, method=\"pad\").fillna(0)\n    var_ = var_.reindex(timeline, method=\"pad\")\n    var_.index.name = \"timeline\"\n    estimate_.index.name = \"timeline\"\n\n    return estimate_, var_", "response": "Compute the Kaplan Meier and Nelson - Aalen estimates."}
{"SOURCE": "codesearchnet", "instruction": "Given the following Python 3 function, write the documentation\ndef _preprocess_inputs(durations, event_observed, timeline, entry, weights):\n\n    n = len(durations)\n    durations = np.asarray(pass_for_numeric_dtypes_or_raise_array(durations)).reshape((n,))\n\n    # set to all observed if event_observed is none\n    if event_observed is None:\n        event_observed = np.ones(n, dtype=int)\n    else:\n        event_observed = np.asarray(event_observed).reshape((n,)).copy().astype(int)\n\n    if entry is not None:\n        entry = np.asarray(entry).reshape((n,))\n\n    event_table = survival_table_from_events(durations, event_observed, entry, weights=weights)\n    if timeline is None:\n        timeline = event_table.index.values\n    else:\n        timeline = np.asarray(timeline)\n\n    return (durations, event_observed, timeline.astype(float), entry, event_table)", "response": "Preprocess the inputs for lifelines."}
{"SOURCE": "codesearchnet", "instruction": "How would you code a function in Python 3 to\nuse the utility `to_numeric` to check that x is convertible to numeric values, and then convert. Any errors are reported back to the user. Parameters ---------- x: list, array, Series, DataFrame Notes ------ This actually allows objects like timedeltas (converted to microseconds), and strings as numbers.", "response": "def pass_for_numeric_dtypes_or_raise_array(x):\n    \"\"\"\n    Use the utility `to_numeric` to check that x is convertible to numeric values, and then convert. Any errors\n    are reported back to the user.\n\n    Parameters\n    ----------\n    x: list, array, Series, DataFrame\n\n    Notes\n    ------\n    This actually allows objects like timedeltas (converted to microseconds), and strings as numbers.\n\n    \"\"\"\n    try:\n        if isinstance(x, (pd.Series, pd.DataFrame)):\n            v = pd.to_numeric(x.squeeze())\n        else:\n            v = pd.to_numeric(np.asarray(x).squeeze())\n\n        if v.size == 0:\n            raise ValueError(\"Empty array/Series passed in.\")\n        return v\n\n    except:\n        raise ValueError(\"Values must be numeric: no strings, datetimes, objects, etc.\")"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef add_covariate_to_timeline(\n    long_form_df,\n    cv,\n    id_col,\n    duration_col,\n    event_col,\n    start_col=\"start\",\n    stop_col=\"stop\",\n    add_enum=False,\n    overwrite=True,\n    cumulative_sum=False,\n    cumulative_sum_prefix=\"cumsum_\",\n    delay=0,\n):  # pylint: disable=too-many-arguments\n    \"\"\"\n    This is a util function to help create a long form table tracking subjects' covariate changes over time. It is meant\n    to be used iteratively as one adds more and more covariates to track over time. Before using this function, it is recommended\n    to view the documentation at https://lifelines.readthedocs.io/en/latest/Survival%20Regression.html#dataset-creation-for-time-varying-regression.\n\n    Parameters\n    ----------\n    long_form_df: DataFrame\n        a DataFrame that has the initial or intermediate \"long\" form of time-varying observations. Must contain\n        columns id_col, 'start', 'stop', and event_col. See function `to_long_format` to transform data into long form.\n    cv: DataFrame\n        a DataFrame that contains (possibly more than) one covariate to track over time. Must contain columns\n        id_col and duration_col. duration_col represents time since the start of the subject's life.\n    id_col: string\n        the column in long_form_df and cv representing a unique identifier for subjects.\n    duration_col: string\n        the column in cv that represents the time-since-birth the observation occurred at.\n    event_col: string\n        the column in df that represents if the event-of-interest occurred\n    add_enum: boolean, optional\n         a Boolean flag to denote whether to add a column enumerating rows per subject. Useful to specify a specific\n        observation, ex: df[df['enum'] == 1] will grab the first observations per subject.\n    overwrite: boolean, optional\n        if True, covariate values in long_form_df will be overwritten by covariate values in cv if the column exists in both\n        cv and long_form_df and the timestamps are identical. If False, the default behavior will be to sum\n        the values together.\n    cumulative_sum: boolean, optional\n        sum over time the new covariates. Makes sense if the covariates are new additions, and not state changes (ex:\n        administering more drugs vs taking a temperature.)\n    cumulative_sum_prefix: string, optional\n        a prefix to add to calculated cumulative sum columns\n    delay: int, optional\n        add a delay to covariates (useful for checking for reverse causality in analysis)\n\n    Returns\n    -------\n    long_form_df: DataFrame\n        A DataFrame with updated rows to reflect the novel times slices (if any) being added from cv, and novel (or updated) columns\n        of new covariates from cv\n\n    See Also\n    --------\n    to_episodic_format\n    to_long_format\n    covariates_from_event_matrix\n    \"\"\"\n\n    def remove_redundant_rows(cv):\n        \"\"\"\n        Removes rows where no change occurs. Ex:\n\n        cv = pd.DataFrame.from_records([\n            {'id': 1, 't': 0, 'var3': 0, 'var4': 1},\n            {'id': 1, 't': 1, 'var3': 0, 'var4': 1},  # redundant, as nothing changed during the interval\n            {'id': 1, 't': 6, 'var3': 1, 'var4': 1},\n        ])\n\n        If cumulative_sum, then redundant rows are not redundant.\n        \"\"\"\n        if cumulative_sum:\n            return cv\n        cols = cv.columns.difference([duration_col])\n        cv = cv.loc[(cv[cols].shift() != cv[cols]).any(axis=1)]\n        return cv\n\n    def transform_cv_to_long_format(cv):\n        return cv.rename(columns={duration_col: start_col})\n\n    def expand(df, cvs):\n        id_ = df.name\n        try:\n            cv = cvs.get_group(id_)\n        except KeyError:\n            return df\n\n        final_state = bool(df[event_col].iloc[-1])\n        final_stop_time = df[stop_col].iloc[-1]\n        df = df.drop([id_col, event_col, stop_col], axis=1).set_index(start_col)\n        cv = cv.drop([id_col], axis=1).set_index(start_col).loc[:final_stop_time]\n\n        if cumulative_sum:\n            cv = cv.cumsum()\n            cv = cv.add_prefix(cumulative_sum_prefix)\n\n        # How do I want to merge existing columns at the same time - could be\n        # new observations (update) or new treatment applied (sum).\n        # There may be more options in the future.\n        if not overwrite:\n            expanded_df = cv.combine(df, lambda s1, s2: s1 + s2, fill_value=0, overwrite=False)\n        elif overwrite:\n            expanded_df = cv.combine_first(df)\n\n        n = expanded_df.shape[0]\n        expanded_df = expanded_df.reset_index()\n        expanded_df[stop_col] = expanded_df[start_col].shift(-1)\n        expanded_df[id_col] = id_\n        expanded_df[event_col] = False\n        expanded_df.at[n - 1, event_col] = final_state\n        expanded_df.at[n - 1, stop_col] = final_stop_time\n\n        if add_enum:\n            expanded_df[\"enum\"] = np.arange(1, n + 1)\n\n        if cumulative_sum:\n            expanded_df[cv.columns] = expanded_df[cv.columns].ffill().fillna(0)\n\n        return expanded_df.ffill()\n\n    if delay < 0:\n        raise ValueError(\"delay parameter must be equal to or greater than 0\")\n\n    if any(col not in long_form_df for col in (id_col, event_col, start_col, stop_col)):\n        raise IndexError(\"Missing column in long_form_df\")\n\n    cv[duration_col] += delay\n    cv = cv.dropna()\n    cv = cv.sort_values([id_col, duration_col])\n    cvs = cv.pipe(remove_redundant_rows).pipe(transform_cv_to_long_format).groupby(id_col)\n\n    long_form_df = long_form_df.groupby(id_col, group_keys=False).apply(expand, cvs=cvs)\n    return long_form_df.reset_index(drop=True)", "response": "This function adds a new covariates to the timeline."}
{"SOURCE": "codesearchnet", "instruction": "Write a Python 3 script for\nreturning a function that formats the exp. column numbers to a number of decimals.", "response": "def format_exp_floats(decimals):\n    \"\"\"\n    sometimes the exp. column can be too large\n    \"\"\"\n    threshold = 10 ** 5\n    return (\n        lambda n: \"{:.{prec}e}\".format(n, prec=decimals) if n > threshold else \"{:4.{prec}f}\".format(n, prec=decimals)\n    )"}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nfits the nelson aalen fitter to obtain the best estimate at the given duration.", "response": "def fit(\n        self,\n        durations,\n        event_observed=None,\n        timeline=None,\n        entry=None,\n        label=\"BFH_estimate\",\n        alpha=None,\n        ci_labels=None,\n    ):  # pylint: disable=too-many-arguments\n        \"\"\"\n        Parameters\n        ----------\n        durations: an array, or pd.Series, of length n\n            duration subject was observed for\n        timeline:\n            return the best estimate at the values in timelines (positively increasing)\n        event_observed: an array, or pd.Series, of length n\n            True if the the death was observed, False if the event was lost (right-censored). Defaults all True if event_observed==None\n        entry: an array, or pd.Series, of length n\n           relative time when a subject entered the study. This is\n           useful for left-truncated observations, i.e the birth event was not observed.\n           If None, defaults to all 0 (all birth events observed.)\n        label: string\n            a string to name the column of the estimate.\n        alpha: float, optional (default=0.05)\n            the alpha value in the confidence intervals. Overrides the initializing\n           alpha for this call to fit only.\n        ci_labels: iterable\n            add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<alpha>\n\n\n        Returns\n        -------\n          self, with new properties like ``survival_function_``.\n\n        \"\"\"\n        self._censoring_type = CensoringType.RIGHT\n        self._label = label\n        alpha = coalesce(alpha, self.alpha)\n\n        naf = NelsonAalenFitter(alpha=alpha)\n        naf.fit(\n            durations, event_observed=event_observed, timeline=timeline, label=label, entry=entry, ci_labels=ci_labels\n        )\n        self.durations, self.event_observed, self.timeline, self.entry, self.event_table = (\n            naf.durations,\n            naf.event_observed,\n            naf.timeline,\n            naf.entry,\n            naf.event_table,\n        )\n\n        # estimation\n        self.survival_function_ = np.exp(-naf.cumulative_hazard_)\n        self.confidence_interval_ = np.exp(-naf.confidence_interval_)\n\n        # estimation methods\n        self._estimation_method = \"survival_function_\"\n        self._estimate_name = \"survival_function_\"\n        self._predict_label = label\n        self._update_docstrings()\n\n        # plotting functions\n        self.plot_survival_function = self.plot\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef fit(\n        self,\n        durations,\n        event_observed=None,\n        timeline=None,\n        entry=None,\n        label=\"KM_estimate\",\n        left_censorship=False,\n        alpha=None,\n        ci_labels=None,\n        weights=None,\n    ):  # pylint: disable=too-many-arguments,too-many-locals\n        \"\"\"\n        Fit the model to a right-censored dataset\n\n        Parameters\n        ----------\n          durations: an array, list, pd.DataFrame or pd.Series\n            length n -- duration subject was observed for\n          event_observed: an array, list, pd.DataFrame, or pd.Series, optional\n             True if the the death was observed, False if the event was lost (right-censored). Defaults all True if event_observed==None\n          timeline: an array, list, pd.DataFrame, or pd.Series, optional\n            return the best estimate at the values in timelines (postively increasing)\n          entry: an array, list, pd.DataFrame, or pd.Series, optional\n             relative time when a subject entered the study. This is useful for left-truncated (not left-censored) observations. If None, all members of the population\n             entered study when they were \"born\".\n          label: string, optional\n            a string to name the column of the estimate.\n          alpha: float, optional\n            the alpha value in the confidence intervals. Overrides the initializing alpha for this call to fit only.\n          left_censorship: bool, optional (default=False)\n            Deprecated, use ``fit_left_censoring``\n          ci_labels: tuple, optional\n                add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<1-alpha/2>\n          weights: an array, list, pd.DataFrame, or pd.Series, optional\n              if providing a weighted dataset. For example, instead\n              of providing every subject as a single element of `durations` and `event_observed`, one could\n              weigh subject differently.\n\n        Returns\n        -------\n        self: KaplanMeierFitter\n          self with new properties like ``survival_function_``, ``plot()``, ``median``\n\n        \"\"\"\n        if left_censorship:\n            warnings.warn(\n                \"kwarg left_censorship is deprecated and will be removed in a future release. Please use ``.fit_left_censoring`` instead.\",\n                DeprecationWarning,\n            )\n\n        self._censoring_type = CensoringType.RIGHT\n        return self._fit(durations, event_observed, timeline, entry, label, alpha, ci_labels, weights)", "response": "Fit the model to a right - censored dataset."}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit_left_censoring(\n        self,\n        durations,\n        event_observed=None,\n        timeline=None,\n        entry=None,\n        label=\"KM_estimate\",\n        alpha=None,\n        ci_labels=None,\n        weights=None,\n    ):\n        \"\"\"\n        Fit the model to a left-censored dataset\n\n        Parameters\n        ----------\n          durations: an array, list, pd.DataFrame or pd.Series\n            length n -- duration subject was observed for\n          event_observed: an array, list, pd.DataFrame, or pd.Series, optional\n             True if the the death was observed, False if the event was lost (right-censored). Defaults all True if event_observed==None\n          timeline: an array, list, pd.DataFrame, or pd.Series, optional\n            return the best estimate at the values in timelines (postively increasing)\n          entry: an array, list, pd.DataFrame, or pd.Series, optional\n             relative time when a subject entered the study. This is useful for left-truncated (not left-censored) observations. If None, all members of the population\n             entered study when they were \"born\".\n          label: string, optional\n            a string to name the column of the estimate.\n          alpha: float, optional\n            the alpha value in the confidence intervals. Overrides the initializing alpha for this call to fit only.\n          left_censorship: bool, optional (default=False)\n            Deprecated, use ``fit_left_censoring``\n          ci_labels: tuple, optional\n                add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<1-alpha/2>\n          weights: an array, list, pd.DataFrame, or pd.Series, optional\n              if providing a weighted dataset. For example, instead\n              of providing every subject as a single element of `durations` and `event_observed`, one could\n              weigh subject differently.\n\n        Returns\n        -------\n        self: KaplanMeierFitter\n          self with new properties like ``survival_function_``, ``plot()``, ``median``\n\n        \"\"\"\n        self._censoring_type = CensoringType.LEFT\n        return self._fit(durations, event_observed, timeline, entry, label, alpha, ci_labels, weights)", "response": "Fit the model to a left - censored dataset."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\nfitting the KaplanMeierFitter with the given properties.", "response": "def _fit(\n        self,\n        durations,\n        event_observed=None,\n        timeline=None,\n        entry=None,\n        label=\"KM_estimate\",\n        alpha=None,\n        ci_labels=None,\n        weights=None,\n    ):  # pylint: disable=too-many-arguments,too-many-locals\n        \"\"\"\n        Parameters\n        ----------\n          durations: an array, list, pd.DataFrame or pd.Series\n            length n -- duration subject was observed for\n          event_observed: an array, list, pd.DataFrame, or pd.Series, optional\n             True if the the death was observed, False if the event was lost (right-censored). Defaults all True if event_observed==None\n          timeline: an array, list, pd.DataFrame, or pd.Series, optional\n            return the best estimate at the values in timelines (postively increasing)\n          entry: an array, list, pd.DataFrame, or pd.Series, optional\n             relative time when a subject entered the study. This is useful for left-truncated (not left-censored) observations. If None, all members of the population\n             entered study when they were \"born\".\n          label: string, optional\n            a string to name the column of the estimate.\n          alpha: float, optional\n            the alpha value in the confidence intervals. Overrides the initializing alpha for this call to fit only.\n          left_censorship: bool, optional (default=False)\n            True if durations and event_observed refer to left censorship events. Default False\n          ci_labels: tuple, optional\n                add custom column names to the generated confidence intervals as a length-2 list: [<lower-bound name>, <upper-bound name>]. Default: <label>_lower_<1-alpha/2>\n          weights: an array, list, pd.DataFrame, or pd.Series, optional\n              if providing a weighted dataset. For example, instead\n              of providing every subject as a single element of `durations` and `event_observed`, one could\n              weigh subject differently.\n\n        Returns\n        -------\n        self: KaplanMeierFitter\n          self with new properties like ``survival_function_``, ``plot()``, ``median``\n\n        \"\"\"\n        self._check_values(durations)\n        if event_observed is not None:\n            self._check_values(event_observed)\n\n        self._label = label\n\n        if weights is not None:\n            weights = np.asarray(weights)\n            if (weights.astype(int) != weights).any():\n                warnings.warn(\n                    \"\"\"It looks like your weights are not integers, possibly propensity scores then?\n  It's important to know that the naive variance estimates of the coefficients are biased. Instead use Monte Carlo to\n  estimate the variances. See paper \"Variance estimation when using inverse probability of treatment weighting (IPTW) with survival analysis\"\n  or \"Adjusted Kaplan-Meier estimator and log-rank test with inverse probability of treatment weighting for survival data.\"\n                  \"\"\",\n                    StatisticalWarning,\n                )\n\n        # if the user is interested in left-censorship, we return the cumulative_density_, no survival_function_,\n        is_left_censoring = self._censoring_type == CensoringType.LEFT\n        primary_estimate_name = \"survival_function_\" if not is_left_censoring else \"cumulative_density_\"\n        secondary_estimate_name = \"cumulative_density_\" if not is_left_censoring else \"survival_function_\"\n\n        self.durations, self.event_observed, self.timeline, self.entry, self.event_table = _preprocess_inputs(\n            durations, event_observed, timeline, entry, weights\n        )\n\n        alpha = alpha if alpha else self.alpha\n        log_estimate, cumulative_sq_ = _additive_estimate(\n            self.event_table, self.timeline, self._additive_f, self._additive_var, is_left_censoring\n        )\n\n        if entry is not None:\n            # a serious problem with KM is that when the sample size is small and there are too few early\n            # truncation times, it may happen that is the number of patients at risk and the number of deaths is the same.\n            # we adjust for this using the Breslow-Fleming-Harrington estimator\n            n = self.event_table.shape[0]\n            net_population = (self.event_table[\"entrance\"] - self.event_table[\"removed\"]).cumsum()\n            if net_population.iloc[: int(n / 2)].min() == 0:\n                ix = net_population.iloc[: int(n / 2)].idxmin()\n                raise StatError(\n                    \"\"\"There are too few early truncation times and too many events. S(t)==0 for all t>%g. Recommend BreslowFlemingHarringtonFitter.\"\"\"\n                    % ix\n                )\n\n        # estimation\n        setattr(self, primary_estimate_name, pd.DataFrame(np.exp(log_estimate), columns=[self._label]))\n        setattr(self, secondary_estimate_name, pd.DataFrame(1 - np.exp(log_estimate), columns=[self._label]))\n\n        self.__estimate = getattr(self, primary_estimate_name)\n        self.confidence_interval_ = self._bounds(cumulative_sq_[:, None], alpha, ci_labels)\n        self.median_ = median_survival_times(self.__estimate, left_censorship=is_left_censoring)\n        self._cumulative_sq_ = cumulative_sq_\n\n        setattr(self, \"confidence_interval_\" + primary_estimate_name, self.confidence_interval_)\n        setattr(self, \"confidence_interval_\" + secondary_estimate_name, 1 - self.confidence_interval_)\n\n        # estimation methods\n        self._estimation_method = primary_estimate_name\n        self._estimate_name = primary_estimate_name\n        self._predict_label = label\n        self._update_docstrings()\n\n        return self"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef cumulative_density_at_times(self, times, label=None):\n        label = coalesce(label, self._label)\n        return pd.Series(1 - self.predict(times), index=_to_array(times), name=label)", "response": "Returns a Pandas series of the predicted cumulative density at specific times"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef plot_cumulative_density(self, **kwargs):\n        return _plot_estimate(\n            self,\n            estimate=self.cumulative_density_,\n            confidence_intervals=self.confidence_interval_cumulative_density_,\n            **kwargs\n        )", "response": "Plots the cumulative density of the current state of the entry in the current state of the object."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef fit(\n        self,\n        df,\n        id_col,\n        event_col,\n        start_col=\"start\",\n        stop_col=\"stop\",\n        weights_col=None,\n        show_progress=False,\n        step_size=None,\n        robust=False,\n        strata=None,\n        initial_point=None,\n    ):  # pylint: disable=too-many-arguments\n        \"\"\"\n        Fit the Cox Proportional Hazard model to a time varying dataset. Tied survival times\n        are handled using Efron's tie-method.\n\n        Parameters\n        -----------\n        df: DataFrame\n            a Pandas DataFrame with necessary columns `duration_col` and\n           `event_col`, plus other covariates. `duration_col` refers to\n           the lifetimes of the subjects. `event_col` refers to whether\n           the 'death' events was observed: 1 if observed, 0 else (censored).\n        id_col: string\n            A subject could have multiple rows in the DataFrame. This column contains\n           the unique identifier per subject.\n        event_col: string\n           the column in DataFrame that contains the subjects' death\n           observation. If left as None, assume all individuals are non-censored.\n        start_col: string\n            the column that contains the start of a subject's time period.\n        stop_col: string\n            the column that contains the end of a subject's time period.\n        weights_col: string, optional\n            the column that contains (possibly time-varying) weight of each subject-period row.\n        show_progress: since the fitter is iterative, show convergence\n           diagnostics.\n        robust: boolean, optional (default: True)\n            Compute the robust errors using the Huber sandwich estimator, aka Wei-Lin estimate. This does not handle\n          ties, so if there are high number of ties, results may significantly differ. See\n          \"The Robust Inference for the Cox Proportional Hazards Model\", Journal of the American Statistical Association, Vol. 84, No. 408 (Dec., 1989), pp. 1074- 1078\n        step_size: float, optional\n            set an initial step size for the fitting algorithm.\n        strata: list or string, optional\n            specify a column or list of columns n to use in stratification. This is useful if a\n            categorical covariate does not obey the proportional hazard assumption. This\n            is used similar to the `strata` expression in R.\n            See http://courses.washington.edu/b515/l17.pdf.\n        initial_point: (d,) numpy array, optional\n            initialize the starting point of the iterative\n            algorithm. Default is the zero vector.\n\n        Returns\n        --------\n        self: CoxTimeVaryingFitter\n            self, with additional properties like ``hazards_`` and ``print_summary``\n\n        \"\"\"\n        self.strata = coalesce(strata, self.strata)\n        self.robust = robust\n        if self.robust:\n            raise NotImplementedError(\"Not available yet.\")\n\n        self.event_col = event_col\n        self.id_col = id_col\n        self.stop_col = stop_col\n        self.start_col = start_col\n        self._time_fit_was_called = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\")\n\n        df = df.copy()\n\n        if not (id_col in df and event_col in df and start_col in df and stop_col in df):\n            raise KeyError(\"A column specified in the call to `fit` does not exist in the DataFrame provided.\")\n\n        if weights_col is None:\n            self.weights_col = None\n            assert (\n                \"__weights\" not in df.columns\n            ), \"__weights is an internal lifelines column, please rename your column first.\"\n            df[\"__weights\"] = 1.0\n        else:\n            self.weights_col = weights_col\n            if (df[weights_col] <= 0).any():\n                raise ValueError(\"values in weights_col must be positive.\")\n\n        df = df.rename(\n            columns={id_col: \"id\", event_col: \"event\", start_col: \"start\", stop_col: \"stop\", weights_col: \"__weights\"}\n        )\n\n        if self.strata is None:\n            df = df.set_index(\"id\")\n        else:\n            df = df.set_index(_to_list(self.strata) + [\"id\"])  # TODO: needs to be a list\n            df = df.sort_index()\n\n        events, start, stop = (\n            pass_for_numeric_dtypes_or_raise_array(df.pop(\"event\")).astype(bool),\n            df.pop(\"start\"),\n            df.pop(\"stop\"),\n        )\n        weights = df.pop(\"__weights\").astype(float)\n\n        df = df.astype(float)\n        self._check_values(df, events, start, stop)\n\n        self._norm_mean = df.mean(0)\n        self._norm_std = df.std(0)\n\n        hazards_ = self._newton_rhaphson(\n            normalize(df, self._norm_mean, self._norm_std),\n            events,\n            start,\n            stop,\n            weights,\n            initial_point=initial_point,\n            show_progress=show_progress,\n            step_size=step_size,\n        )\n\n        self.hazards_ = pd.Series(hazards_, index=df.columns, name=\"coef\") / self._norm_std\n        self.variance_matrix_ = -inv(self._hessian_) / np.outer(self._norm_std, self._norm_std)\n        self.standard_errors_ = self._compute_standard_errors(\n            normalize(df, self._norm_mean, self._norm_std), events, start, stop, weights\n        )\n        self.confidence_intervals_ = self._compute_confidence_intervals()\n        self.baseline_cumulative_hazard_ = self._compute_cumulative_baseline_hazard(df, events, start, stop, weights)\n        self.baseline_survival_ = self._compute_baseline_survival()\n        self.event_observed = events\n        self.start_stop_and_events = pd.DataFrame({\"event\": events, \"start\": start, \"stop\": stop})\n        self.weights = weights\n\n        self._n_examples = df.shape[0]\n        self._n_unique = df.index.unique().shape[0]\n        return self", "response": "Fit the Cox Proportional Hazards model to a time - varying dataset."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _newton_rhaphson(\n        self,\n        df,\n        events,\n        start,\n        stop,\n        weights,\n        show_progress=False,\n        step_size=None,\n        precision=10e-6,\n        max_steps=50,\n        initial_point=None,\n    ):  # pylint: disable=too-many-arguments,too-many-locals,too-many-branches,too-many-statements\n        \"\"\"\n        Newton Rhaphson algorithm for fitting CPH model.\n\n        Parameters\n        ----------\n        df: DataFrame\n        stop_times_events: DataFrame\n             meta information about the subjects history\n        show_progress: boolean, optional (default: True)\n            to show verbose output of convergence\n        step_size: float\n            > 0 to determine a starting step size in NR algorithm.\n        precision: float\n            the convergence halts if the norm of delta between\n                     successive positions is less than epsilon.\n\n        Returns\n        --------\n        beta: (1,d) numpy array.\n        \"\"\"\n        assert precision <= 1.0, \"precision must be less than or equal to 1.\"\n\n        _, d = df.shape\n\n        # make sure betas are correct size.\n        if initial_point is not None:\n            beta = initial_point\n        else:\n            beta = np.zeros((d,))\n\n        i = 0\n        converging = True\n        ll, previous_ll = 0, 0\n        start_time = time.time()\n\n        step_sizer = StepSizer(step_size)\n        step_size = step_sizer.next()\n\n        while converging:\n            i += 1\n\n            if self.strata is None:\n                h, g, ll = self._get_gradients(\n                    df.values, events.values, start.values, stop.values, weights.values, beta\n                )\n            else:\n                g = np.zeros_like(beta)\n                h = np.zeros((d, d))\n                ll = 0\n                for _h, _g, _ll in self._partition_by_strata_and_apply(\n                    df, events, start, stop, weights, self._get_gradients, beta\n                ):\n                    g += _g\n                    h += _h\n                    ll += _ll\n\n            if i == 1 and np.all(beta == 0):\n                # this is a neat optimization, the null partial likelihood\n                # is the same as the full partial but evaluated at zero.\n                # if the user supplied a non-trivial initial point, we need to delay this.\n                self._log_likelihood_null = ll\n\n            if self.penalizer > 0:\n                # add the gradient and hessian of the l2 term\n                g -= self.penalizer * beta\n                h.flat[:: d + 1] -= self.penalizer\n\n            try:\n                # reusing a piece to make g * inv(h) * g.T faster later\n                inv_h_dot_g_T = spsolve(-h, g, sym_pos=True)\n            except ValueError as e:\n                if \"infs or NaNs\" in str(e):\n                    raise ConvergenceError(\n                        \"\"\"hessian or gradient contains nan or inf value(s). Convergence halted. Please see the following tips in the lifelines documentation:\nhttps://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n\"\"\",\n                        e,\n                    )\n                else:\n                    # something else?\n                    raise e\n            except LinAlgError as e:\n                raise ConvergenceError(\n                    \"\"\"Convergence halted due to matrix inversion problems. Suspicion is high colinearity. Please see the following tips in the lifelines documentation:\nhttps://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n\"\"\",\n                    e,\n                )\n\n            delta = step_size * inv_h_dot_g_T\n\n            if np.any(np.isnan(delta)):\n                raise ConvergenceError(\n                    \"\"\"delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation:\nhttps://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n\"\"\"\n                )\n            # Save these as pending result\n            hessian, gradient = h, g\n            norm_delta = norm(delta)\n            newton_decrement = g.dot(inv_h_dot_g_T) / 2\n\n            if show_progress:\n                print(\n                    \"Iteration %d: norm_delta = %.5f, step_size = %.5f, ll = %.5f, newton_decrement = %.5f, seconds_since_start = %.1f\"\n                    % (i, norm_delta, step_size, ll, newton_decrement, time.time() - start_time)\n                )\n\n            # convergence criteria\n            if norm_delta < precision:\n                converging, completed = False, True\n            elif previous_ll > 0 and abs(ll - previous_ll) / (-previous_ll) < 1e-09:\n                # this is what R uses by default\n                converging, completed = False, True\n            elif newton_decrement < 10e-8:\n                converging, completed = False, True\n            elif i >= max_steps:\n                # 50 iterations steps with N-R is a lot.\n                # Expected convergence is less than 10 steps\n                converging, completed = False, False\n            elif step_size <= 0.0001:\n                converging, completed = False, False\n            elif abs(ll) < 0.0001 and norm_delta > 1.0:\n                warnings.warn(\n                    \"The log-likelihood is getting suspiciously close to 0 and the delta is still large. There may be complete separation in the dataset. This may result in incorrect inference of coefficients. \\\nSee https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\",\n                    ConvergenceWarning,\n                )\n                converging, completed = False, False\n\n            step_size = step_sizer.update(norm_delta).next()\n\n            beta += delta\n\n        self._hessian_ = hessian\n        self._score_ = gradient\n        self._log_likelihood = ll\n\n        if show_progress and completed:\n            print(\"Convergence completed after %d iterations.\" % (i))\n        elif show_progress and not completed:\n            print(\"Convergence failed. See any warning messages.\")\n\n        # report to the user problems that we detect.\n        if completed and norm_delta > 0.1:\n            warnings.warn(\n                \"Newton-Rhapson convergence completed but norm(delta) is still high, %.3f. This may imply non-unique solutions to the maximum likelihood. Perhaps there is colinearity or complete separation in the dataset?\"\n                % norm_delta,\n                ConvergenceWarning,\n            )\n        elif not completed:\n            warnings.warn(\"Newton-Rhapson failed to converge sufficiently in %d steps.\" % max_steps, ConvergenceWarning)\n\n        return beta", "response": "This function is used to fit the Newton - Rhaphson model for the given subject history."}
{"SOURCE": "codesearchnet", "instruction": "Implement a Python 3 function for\ncalculating the first and second order vector differentials with respect to beta.", "response": "def _get_gradients(self, X, events, start, stop, weights, beta):  # pylint: disable=too-many-locals\n        \"\"\"\n        Calculates the first and second order vector differentials, with respect to beta.\n\n        Returns\n        -------\n        hessian: (d, d) numpy array,\n        gradient: (d,) numpy array\n        log_likelihood: float\n        \"\"\"\n\n        _, d = X.shape\n        hessian = np.zeros((d, d))\n        gradient = np.zeros(d)\n        log_lik = 0\n        # weights = weights[:, None]\n        unique_death_times = np.unique(stop[events])\n\n        for t in unique_death_times:\n\n            # I feel like this can be made into some tree-like structure\n            ix = (start < t) & (t <= stop)\n\n            X_at_t = X[ix]\n            weights_at_t = weights[ix]\n            stops_events_at_t = stop[ix]\n            events_at_t = events[ix]\n\n            phi_i = weights_at_t * np.exp(np.dot(X_at_t, beta))\n            phi_x_i = phi_i[:, None] * X_at_t\n            phi_x_x_i = np.dot(X_at_t.T, phi_x_i)\n\n            # Calculate sums of Risk set\n            risk_phi = array_sum_to_scalar(phi_i)\n            risk_phi_x = matrix_axis_0_sum_to_array(phi_x_i)\n            risk_phi_x_x = phi_x_x_i\n\n            # Calculate the sums of Tie set\n            deaths = events_at_t & (stops_events_at_t == t)\n\n            tied_death_counts = array_sum_to_scalar(deaths.astype(int))  # should always at least 1\n\n            xi_deaths = X_at_t[deaths]\n\n            x_death_sum = matrix_axis_0_sum_to_array(weights_at_t[deaths, None] * xi_deaths)\n\n            weight_count = array_sum_to_scalar(weights_at_t[deaths])\n            weighted_average = weight_count / tied_death_counts\n\n            #\n            # This code is near identical to the _batch algorithm in CoxPHFitter. In fact, see _batch for comments.\n            #\n\n            if tied_death_counts > 1:\n\n                # A good explaination for how Efron handles ties. Consider three of five subjects who fail at the time.\n                # As it is not known a priori that who is the first to fail, so one-third of\n                # (\u03c61 + \u03c62 + \u03c63) is adjusted from sum_j^{5} \u03c6j after one fails. Similarly two-third\n                # of (\u03c61 + \u03c62 + \u03c63) is adjusted after first two individuals fail, etc.\n\n                # a lot of this is now in einstien notation for performance, but see original \"expanded\" code here\n                # https://github.com/CamDavidsonPilon/lifelines/blob/e7056e7817272eb5dff5983556954f56c33301b1/lifelines/fitters/cox_time_varying_fitter.py#L458-L490\n\n                tie_phi = array_sum_to_scalar(phi_i[deaths])\n                tie_phi_x = matrix_axis_0_sum_to_array(phi_x_i[deaths])\n                tie_phi_x_x = np.dot(xi_deaths.T, phi_i[deaths, None] * xi_deaths)\n\n                increasing_proportion = np.arange(tied_death_counts) / tied_death_counts\n                denom = 1.0 / (risk_phi - increasing_proportion * tie_phi)\n                numer = risk_phi_x - np.outer(increasing_proportion, tie_phi_x)\n\n                a1 = np.einsum(\"ab, i->ab\", risk_phi_x_x, denom) - np.einsum(\n                    \"ab, i->ab\", tie_phi_x_x, increasing_proportion * denom\n                )\n            else:\n                # no tensors here, but do some casting to make it easier in the converging step next.\n                denom = 1.0 / np.array([risk_phi])\n                numer = risk_phi_x\n                a1 = risk_phi_x_x * denom\n\n            summand = numer * denom[:, None]\n            a2 = summand.T.dot(summand)\n\n            gradient = gradient + x_death_sum - weighted_average * summand.sum(0)\n            log_lik = log_lik + np.dot(x_death_sum, beta) + weighted_average * np.log(denom).sum()\n            hessian = hessian + weighted_average * (a2 - a1)\n\n        return hessian, gradient, log_lik"}
{"SOURCE": "codesearchnet", "instruction": "Can you implement a function in Python 3 that\nprints summary statistics describing the fit coefficients and error bounds of the fit.", "response": "def print_summary(self, decimals=2, **kwargs):\n        \"\"\"\n        Print summary statistics describing the fit, the coefficients, and the error bounds.\n\n        Parameters\n        -----------\n        decimals: int, optional (default=2)\n            specify the number of decimal places to show\n        kwargs:\n            print additional meta data in the output (useful to provide model names, dataset names, etc.) when comparing\n            multiple outputs.\n\n        \"\"\"\n\n        # Print information about data first\n        justify = string_justify(18)\n\n        print(self)\n        print(\"{} = '{}'\".format(justify(\"event col\"), self.event_col))\n\n        if self.weights_col:\n            print(\"{} = '{}'\".format(justify(\"weights col\"), self.weights_col))\n\n        if self.strata:\n            print(\"{} = {}\".format(justify(\"strata\"), self.strata))\n\n        if self.penalizer > 0:\n            print(\"{} = {}\".format(justify(\"penalizer\"), self.penalizer))\n\n        print(\"{} = {}\".format(justify(\"number of subjects\"), self._n_unique))\n        print(\"{} = {}\".format(justify(\"number of periods\"), self._n_examples))\n        print(\"{} = {}\".format(justify(\"number of events\"), self.event_observed.sum()))\n        print(\"{} = {:.{prec}f}\".format(justify(\"log-likelihood\"), self._log_likelihood, prec=decimals))\n        print(\"{} = {} UTC\".format(justify(\"time fit was run\"), self._time_fit_was_called))\n\n        for k, v in kwargs.items():\n            print(\"{} = {}\\n\".format(justify(k), v))\n\n        print(end=\"\\n\")\n        print(\"---\")\n\n        df = self.summary\n        # Significance codes last\n        print(\n            df.to_string(\n                float_format=format_floats(decimals),\n                formatters={\"p\": format_p_value(decimals), \"exp(coef)\": format_exp_floats(decimals)},\n            )\n        )\n\n        # Significance code explanation\n        print(\"---\")\n        print(\n            \"Log-likelihood ratio test = {:.{prec}f} on {} df, -log2(p)={:.{prec}f}\".format(\n                *self._compute_likelihood_ratio_test(), prec=decimals\n            )\n        )"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef plot(self, columns=None, **errorbar_kwargs):\n        from matplotlib import pyplot as plt\n\n        ax = errorbar_kwargs.pop(\"ax\", None) or plt.figure().add_subplot(111)\n\n        errorbar_kwargs.setdefault(\"c\", \"k\")\n        errorbar_kwargs.setdefault(\"fmt\", \"s\")\n        errorbar_kwargs.setdefault(\"markerfacecolor\", \"white\")\n        errorbar_kwargs.setdefault(\"markeredgewidth\", 1.25)\n        errorbar_kwargs.setdefault(\"elinewidth\", 1.25)\n        errorbar_kwargs.setdefault(\"capsize\", 3)\n\n        z = inv_normal_cdf(1 - self.alpha / 2)\n\n        if columns is None:\n            columns = self.hazards_.index\n\n        yaxis_locations = list(range(len(columns)))\n        symmetric_errors = z * self.standard_errors_[columns].to_frame().squeeze(axis=1).values.copy()\n        hazards = self.hazards_[columns].values.copy()\n\n        order = np.argsort(hazards)\n\n        ax.errorbar(hazards[order], yaxis_locations, xerr=symmetric_errors[order], **errorbar_kwargs)\n        best_ylim = ax.get_ylim()\n        ax.vlines(0, -2, len(columns) + 1, linestyles=\"dashed\", linewidths=1, alpha=0.65)\n        ax.set_ylim(best_ylim)\n\n        tick_labels = [columns[i] for i in order]\n\n        ax.set_yticks(yaxis_locations)\n        ax.set_yticklabels(tick_labels)\n        ax.set_xlabel(\"log(HR) (%g%% CI)\" % ((1 - self.alpha) * 100))\n\n        return ax", "response": "Plots the coefficients of the current object."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _compute_delta_beta(self, df, events, start, stop, weights):\n\n        score_residuals = self._compute_residuals(df, events, start, stop, weights) * weights[:, None]\n\n        naive_var = inv(self._hessian_)\n        delta_betas = -score_residuals.dot(naive_var) / self._norm_std.values\n\n        return delta_betas", "response": "compute the delta betas as a result of excluding ith row"}
{"SOURCE": "codesearchnet", "instruction": "Here you have a function in Python 3, explain what it does\ndef fit(\n        self,\n        df,\n        duration_col=None,\n        event_col=None,\n        show_progress=False,\n        initial_point=None,\n        strata=None,\n        step_size=None,\n        weights_col=None,\n        cluster_col=None,\n        robust=False,\n        batch_mode=None,\n    ):\n        \"\"\"\n        Fit the Cox proportional hazard model to a dataset.\n\n        Parameters\n        ----------\n        df: DataFrame\n            a Pandas DataFrame with necessary columns `duration_col` and\n            `event_col` (see below), covariates columns, and special columns (weights, strata).\n            `duration_col` refers to\n            the lifetimes of the subjects. `event_col` refers to whether\n            the 'death' events was observed: 1 if observed, 0 else (censored).\n\n        duration_col: string\n            the name of the column in DataFrame that contains the subjects'\n            lifetimes.\n\n        event_col: string, optional\n            the  name of thecolumn in DataFrame that contains the subjects' death\n            observation. If left as None, assume all individuals are uncensored.\n\n        weights_col: string, optional\n            an optional column in the DataFrame, df, that denotes the weight per subject.\n            This column is expelled and not used as a covariate, but as a weight in the\n            final regression. Default weight is 1.\n            This can be used for case-weights. For example, a weight of 2 means there were two subjects with\n            identical observations.\n            This can be used for sampling weights. In that case, use `robust=True` to get more accurate standard errors.\n\n        show_progress: boolean, optional (default=False)\n            since the fitter is iterative, show convergence\n            diagnostics. Useful if convergence is failing.\n\n        initial_point: (d,) numpy array, optional\n            initialize the starting point of the iterative\n            algorithm. Default is the zero vector.\n\n        strata: list or string, optional\n            specify a column or list of columns n to use in stratification. This is useful if a\n            categorical covariate does not obey the proportional hazard assumption. This\n            is used similar to the `strata` expression in R.\n            See http://courses.washington.edu/b515/l17.pdf.\n\n        step_size: float, optional\n            set an initial step size for the fitting algorithm. Setting to 1.0 may improve performance, but could also hurt convergence.\n\n        robust: boolean, optional (default=False)\n            Compute the robust errors using the Huber sandwich estimator, aka Wei-Lin estimate. This does not handle\n            ties, so if there are high number of ties, results may significantly differ. See\n            \"The Robust Inference for the Cox Proportional Hazards Model\", Journal of the American Statistical Association, Vol. 84, No. 408 (Dec., 1989), pp. 1074- 1078\n\n        cluster_col: string, optional\n            specifies what column has unique identifiers for clustering covariances. Using this forces the sandwich estimator (robust variance estimator) to\n            be used.\n\n        batch_mode: bool, optional\n            enabling batch_mode can be faster for datasets with a large number of ties. If left as None, lifelines will choose the best option.\n\n        Returns\n        -------\n        self: CoxPHFitter\n            self with additional new properties: ``print_summary``, ``hazards_``, ``confidence_intervals_``, ``baseline_survival_``, etc.\n\n\n        Note\n        ----\n        Tied survival times are handled using Efron's tie-method.\n\n\n        Examples\n        --------\n        >>> from lifelines import CoxPHFitter\n        >>>\n        >>> df = pd.DataFrame({\n        >>>     'T': [5, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>>     'E': [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n        >>>     'var': [0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2],\n        >>>     'age': [4, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>> })\n        >>>\n        >>> cph = CoxPHFitter()\n        >>> cph.fit(df, 'T', 'E')\n        >>> cph.print_summary()\n        >>> cph.predict_median(df)\n\n\n        >>> from lifelines import CoxPHFitter\n        >>>\n        >>> df = pd.DataFrame({\n        >>>     'T': [5, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>>     'E': [1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0],\n        >>>     'var': [0, 0, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2],\n        >>>     'weights': [1.1, 0.5, 2.0, 1.6, 1.2, 4.3, 1.4, 4.5, 3.0, 3.2, 0.4, 6.2],\n        >>>     'month': [10, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>>     'age': [4, 3, 9, 8, 7, 4, 4, 3, 2, 5, 6, 7],\n        >>> })\n        >>>\n        >>> cph = CoxPHFitter()\n        >>> cph.fit(df, 'T', 'E', strata=['month', 'age'], robust=True, weights_col='weights')\n        >>> cph.print_summary()\n        >>> cph.predict_median(df)\n\n        \"\"\"\n        if duration_col is None:\n            raise TypeError(\"duration_col cannot be None.\")\n\n        self._censoring_type = CensoringType.RIGHT\n        self._time_fit_was_called = datetime.utcnow().strftime(\"%Y-%m-%d %H:%M:%S\") + \" UTC\"\n        self.duration_col = duration_col\n        self.event_col = event_col\n        self.robust = robust\n        self.cluster_col = cluster_col\n        self.weights_col = weights_col\n        self._n_examples = df.shape[0]\n        self._batch_mode = batch_mode\n        self.strata = coalesce(strata, self.strata)\n\n        X, T, E, weights, original_index, self._clusters = self._preprocess_dataframe(df)\n\n        self.durations = T.copy()\n        self.event_observed = E.copy()\n        self.weights = weights.copy()\n\n        if self.strata is not None:\n            self.durations.index = original_index\n            self.event_observed.index = original_index\n            self.weights.index = original_index\n\n        self._norm_mean = X.mean(0)\n        self._norm_std = X.std(0)\n        X_norm = normalize(X, self._norm_mean, self._norm_std)\n\n        hazards_ = self._fit_model(\n            X_norm, T, E, weights=weights, initial_point=initial_point, show_progress=show_progress, step_size=step_size\n        )\n\n        self.hazards_ = pd.Series(hazards_, index=X.columns, name=\"coef\") / self._norm_std\n\n        self.variance_matrix_ = -inv(self._hessian_) / np.outer(self._norm_std, self._norm_std)\n        self.standard_errors_ = self._compute_standard_errors(X_norm, T, E, weights)\n        self.confidence_intervals_ = self._compute_confidence_intervals()\n\n        self._predicted_partial_hazards_ = (\n            self.predict_partial_hazard(X)\n            .rename(columns={0: \"P\"})\n            .assign(T=self.durations.values, E=self.event_observed.values, W=self.weights.values)\n            .set_index(X.index)\n        )\n        self.baseline_hazard_ = self._compute_baseline_hazards()\n        self.baseline_cumulative_hazard_ = self._compute_baseline_cumulative_hazard()\n        self.baseline_survival_ = self._compute_baseline_survival()\n\n        if hasattr(self, \"_concordance_score_\"):\n            # we have already fit the model.\n            del self._concordance_score_\n\n        return self", "response": "Fit the Cox proportional hazard model to a dataset."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _fit_model(\n        self,\n        X,\n        T,\n        E,\n        weights=None,\n        initial_point=None,\n        step_size=None,\n        precision=1e-07,\n        show_progress=True,\n        max_steps=50,\n    ):  # pylint: disable=too-many-statements,too-many-branches\n        \"\"\"\n        Newton Rhaphson algorithm for fitting CPH model.\n\n        Note\n        ----\n        The data is assumed to be sorted on T!\n\n        Parameters\n        ----------\n        X: (n,d) Pandas DataFrame of observations.\n        T: (n) Pandas Series representing observed durations.\n        E: (n) Pandas Series representing death events.\n        weights: (n) an iterable representing weights per observation.\n        initial_point: (d,) numpy array of initial starting point for\n                      NR algorithm. Default 0.\n        step_size: float, optional\n            > 0.001 to determine a starting step size in NR algorithm.\n        precision: float, optional\n            the convergence halts if the norm of delta between\n            successive positions is less than epsilon.\n        show_progress: boolean, optional\n            since the fitter is iterative, show convergence\n                 diagnostics.\n        max_steps: int, optional\n            the maximum number of iterations of the Newton-Rhaphson algorithm.\n\n        Returns\n        -------\n        beta: (1,d) numpy array.\n        \"\"\"\n        self.path = []\n        assert precision <= 1.0, \"precision must be less than or equal to 1.\"\n        _, d = X.shape\n\n        # make sure betas are correct size.\n        if initial_point is not None:\n            assert initial_point.shape == (d,)\n            beta = initial_point\n        else:\n            beta = np.zeros((d,))\n\n        step_sizer = StepSizer(step_size)\n        step_size = step_sizer.next()\n\n        # Method of choice is just efron right now\n        if self.tie_method == \"Efron\":\n            decision = BatchVsSingle.decide(self._batch_mode, T)\n            get_gradients = getattr(self, \"_get_efron_values_%s\" % decision)\n            self._batch_mode = decision == \"batch\"\n        else:\n            raise NotImplementedError(\"Only Efron is available.\")\n\n        i = 0\n        converging = True\n        ll, previous_ll = 0, 0\n        start = time.time()\n\n        while converging:\n            self.path.append(beta.copy())\n\n            i += 1\n\n            if self.strata is None:\n\n                h, g, ll = get_gradients(X.values, T.values, E.values, weights.values, beta)\n\n            else:\n                g = np.zeros_like(beta)\n                h = np.zeros((beta.shape[0], beta.shape[0]))\n                ll = 0\n                for _h, _g, _ll in self._partition_by_strata_and_apply(X, T, E, weights, get_gradients, beta):\n                    g += _g\n                    h += _h\n                    ll += _ll\n\n            if i == 1 and np.all(beta == 0):\n                # this is a neat optimization, the null partial likelihood\n                # is the same as the full partial but evaluated at zero.\n                # if the user supplied a non-trivial initial point, we need to delay this.\n                self._log_likelihood_null = ll\n\n            if self.penalizer > 0:\n                # add the gradient and hessian of the l2 term\n                g -= self.penalizer * beta\n                h.flat[:: d + 1] -= self.penalizer\n\n            # reusing a piece to make g * inv(h) * g.T faster later\n            try:\n                inv_h_dot_g_T = spsolve(-h, g, assume_a=\"pos\", check_finite=False)\n            except ValueError as e:\n                if \"infs or NaNs\" in str(e):\n                    raise ConvergenceError(\n                        \"\"\"Hessian or gradient contains nan or inf value(s). Convergence halted. Please see the following tips in the lifelines documentation:\nhttps://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n\"\"\",\n                        e,\n                    )\n                else:\n                    # something else?\n                    raise e\n            except LinAlgError as e:\n                raise ConvergenceError(\n                    \"\"\"Convergence halted due to matrix inversion problems. Suspicion is high collinearity. Please see the following tips in the lifelines documentation:\nhttps://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n\"\"\",\n                    e,\n                )\n\n            delta = inv_h_dot_g_T\n\n            if np.any(np.isnan(delta)):\n                raise ConvergenceError(\n                    \"\"\"delta contains nan value(s). Convergence halted. Please see the following tips in the lifelines documentation:\nhttps://lifelines.readthedocs.io/en/latest/Examples.html#problems-with-convergence-in-the-cox-proportional-hazard-model\n\"\"\"\n                )\n\n            # Save these as pending result\n            hessian, gradient = h, g\n            norm_delta = norm(delta)\n\n            # reusing an above piece to make g * inv(h) * g.T faster.\n            newton_decrement = g.dot(inv_h_dot_g_T) / 2\n\n            if show_progress:\n                print(\n                    \"Iteration %d: norm_delta = %.5f, step_size = %.4f, ll = %.5f, newton_decrement = %.5f, seconds_since_start = %.1f\"\n                    % (i, norm_delta, step_size, ll, newton_decrement, time.time() - start)\n                )\n\n            # convergence criteria\n            if norm_delta < precision:\n                converging, completed = False, True\n            elif previous_ll != 0 and abs(ll - previous_ll) / (-previous_ll) < 1e-09:\n                # this is what R uses by default\n                converging, completed = False, True\n            elif newton_decrement < precision:\n                converging, completed = False, True\n            elif i >= max_steps:\n                # 50 iterations steps with N-R is a lot.\n                # Expected convergence is ~10 steps\n                converging, completed = False, False\n            elif step_size <= 0.00001:\n                converging, completed = False, False\n            elif abs(ll) < 0.0001 and norm_delta > 1.0:\n                warnings.warn(\n                    \"The log-likelihood is getting suspiciously close to 0 and the delta is still large. There may be complete separation in the dataset. This may result in incorrect inference of coefficients. \\\nSee https://stats.stackexchange.com/questions/11109/how-to-deal-with-perfect-separation-in-logistic-regression\",\n                    ConvergenceWarning,\n                )\n                converging, completed = False, False\n\n            beta += step_size * delta\n\n            previous_ll = ll\n            step_size = step_sizer.update(norm_delta).next()\n\n        self._hessian_ = hessian\n        self._score_ = gradient\n        self._log_likelihood = ll\n\n        if show_progress and completed:\n            print(\"Convergence completed after %d iterations.\" % (i))\n        elif show_progress and not completed:\n            print(\"Convergence failed. See any warning messages.\")\n\n        # report to the user problems that we detect.\n        if completed and norm_delta > 0.1:\n            warnings.warn(\n                \"Newton-Rhapson convergence completed but norm(delta) is still high, %.3f. This may imply non-unique solutions to the maximum likelihood. Perhaps there is collinearity or complete separation in the dataset?\"\n                % norm_delta,\n                ConvergenceWarning,\n            )\n        elif not completed:\n            warnings.warn(\"Newton-Rhapson failed to converge sufficiently in %d steps.\" % max_steps, ConvergenceWarning)\n\n        return beta", "response": "Fits the model for the given newton - rhaphson state."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncalculate the first and second order vector differentials with respect to beta and returns the values for the efron approximation of the data.", "response": "def _get_efron_values_single(self, X, T, E, weights, beta):\n        \"\"\"\n        Calculates the first and second order vector differentials, with respect to beta.\n        Note that X, T, E are assumed to be sorted on T!\n\n        A good explanation for Efron. Consider three of five subjects who fail at the time.\n        As it is not known a priori that who is the first to fail, so one-third of\n        (\u03c61 + \u03c62 + \u03c63) is adjusted from sum_j^{5} \u03c6j after one fails. Similarly two-third\n        of (\u03c61 + \u03c62 + \u03c63) is adjusted after first two individuals fail, etc.\n\n        From https://cran.r-project.org/web/packages/survival/survival.pdf:\n\n        \"Setting all weights to 2 for instance will give the same coefficient estimate but halve the variance. When\n        the Efron approximation for ties (default) is employed replication of the data will not give exactly the same coefficients as the\n        weights option, and in this case the weighted fit is arguably the correct one.\"\n\n        Parameters\n        ----------\n        X: array\n            (n,d) numpy array of observations.\n        T: array\n            (n) numpy array representing observed durations.\n        E: array\n            (n) numpy array representing death events.\n        weights: array\n            (n) an array representing weights per observation.\n        beta: array\n            (1, d) numpy array of coefficients.\n\n        Returns\n        -------\n        hessian:\n            (d, d) numpy array,\n        gradient:\n            (1, d) numpy array\n        log_likelihood: float\n        \"\"\"\n\n        n, d = X.shape\n        hessian = np.zeros((d, d))\n        gradient = np.zeros((d,))\n        log_lik = 0\n\n        # Init risk and tie sums to zero\n        x_death_sum = np.zeros((d,))\n        risk_phi, tie_phi = 0, 0\n        risk_phi_x, tie_phi_x = np.zeros((d,)), np.zeros((d,))\n        risk_phi_x_x, tie_phi_x_x = np.zeros((d, d)), np.zeros((d, d))\n\n        # Init number of ties and weights\n        weight_count = 0.0\n        tied_death_counts = 0\n        scores = weights * np.exp(np.dot(X, beta))\n\n        # Iterate backwards to utilize recursive relationship\n        for i in range(n - 1, -1, -1):\n            # Doing it like this to preserve shape\n            ti = T[i]\n            ei = E[i]\n            xi = X[i]\n            score = scores[i]\n            w = weights[i]\n\n            # Calculate phi values\n            phi_i = score\n            phi_x_i = phi_i * xi\n            phi_x_x_i = np.outer(xi, phi_x_i)\n\n            # Calculate sums of Risk set\n            risk_phi = risk_phi + phi_i\n            risk_phi_x = risk_phi_x + phi_x_i\n            risk_phi_x_x = risk_phi_x_x + phi_x_x_i\n\n            # Calculate sums of Ties, if this is an event\n            if ei:\n                x_death_sum = x_death_sum + w * xi\n                tie_phi = tie_phi + phi_i\n                tie_phi_x = tie_phi_x + phi_x_i\n                tie_phi_x_x = tie_phi_x_x + phi_x_x_i\n\n                # Keep track of count\n                tied_death_counts += 1\n                weight_count += w\n\n            if i > 0 and T[i - 1] == ti:\n                # There are more ties/members of the risk set\n                continue\n            elif tied_death_counts == 0:\n                # Only censored with current time, move on\n                continue\n\n            # There was atleast one event and no more ties remain. Time to sum.\n            #\n            # This code is near identical to the _batch algorithm below. In fact, see _batch for comments.\n            #\n            weighted_average = weight_count / tied_death_counts\n\n            if tied_death_counts > 1:\n                increasing_proportion = np.arange(tied_death_counts) / tied_death_counts\n                denom = 1.0 / (risk_phi - increasing_proportion * tie_phi)\n                numer = risk_phi_x - np.outer(increasing_proportion, tie_phi_x)\n                a1 = np.einsum(\"ab,i->ab\", risk_phi_x_x, denom) - np.einsum(\n                    \"ab,i->ab\", tie_phi_x_x, increasing_proportion * denom\n                )\n            else:\n                denom = 1.0 / np.array([risk_phi])\n                numer = risk_phi_x\n                a1 = risk_phi_x_x * denom\n\n            summand = numer * denom[:, None]\n            a2 = summand.T.dot(summand)\n\n            gradient = gradient + x_death_sum - weighted_average * summand.sum(0)\n\n            log_lik = log_lik + np.dot(x_death_sum, beta) + weighted_average * np.log(denom).sum()\n            hessian = hessian + weighted_average * (a2 - a1)\n\n            # reset tie values\n            tied_death_counts = 0\n            weight_count = 0.0\n            x_death_sum = np.zeros((d,))\n            tie_phi = 0\n            tie_phi_x = np.zeros((d,))\n            tie_phi_x_x = np.zeros((d, d))\n\n        return hessian, gradient, log_lik"}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef _get_efron_values_batch(self, X, T, E, weights, beta):  # pylint: disable=too-many-locals\n\n        n, d = X.shape\n        hessian = np.zeros((d, d))\n        gradient = np.zeros((d,))\n        log_lik = 0\n        # weights = weights[:, None]\n\n        # Init risk and tie sums to zero\n        risk_phi, tie_phi = 0, 0\n        risk_phi_x, tie_phi_x = np.zeros((d,)), np.zeros((d,))\n        risk_phi_x_x, tie_phi_x_x = np.zeros((d, d)), np.zeros((d, d))\n\n        # counts are sorted by -T\n        _, counts = np.unique(-T, return_counts=True)\n        scores = weights * np.exp(np.dot(X, beta))\n        pos = n\n\n        for count_of_removals in counts:\n\n            slice_ = slice(pos - count_of_removals, pos)\n\n            X_at_t = X[slice_]\n            weights_at_t = weights[slice_]\n\n            phi_i = scores[slice_, None]\n            phi_x_i = phi_i * X_at_t\n            phi_x_x_i = np.dot(X_at_t.T, phi_x_i)\n\n            # Calculate sums of Risk set\n            risk_phi = risk_phi + array_sum_to_scalar(phi_i)\n            risk_phi_x = risk_phi_x + matrix_axis_0_sum_to_array(phi_x_i)\n            risk_phi_x_x = risk_phi_x_x + phi_x_x_i\n\n            # Calculate the sums of Tie set\n            deaths = E[slice_]\n\n            tied_death_counts = array_sum_to_scalar(deaths.astype(int))\n            if tied_death_counts == 0:\n                # no deaths, can continue\n                pos -= count_of_removals\n                continue\n\n            xi_deaths = X_at_t[deaths]\n            weights_deaths = weights_at_t[deaths]\n\n            x_death_sum = matrix_axis_0_sum_to_array(weights_deaths[:, None] * xi_deaths)\n\n            weight_count = array_sum_to_scalar(weights_deaths)\n            weighted_average = weight_count / tied_death_counts\n\n            if tied_death_counts > 1:\n\n                # a lot of this is now in Einstein notation for performance, but see original \"expanded\" code here\n                # https://github.com/CamDavidsonPilon/lifelines/blob/e7056e7817272eb5dff5983556954f56c33301b1/lifelines/fitters/coxph_fitter.py#L755-L789\n\n                # it's faster if we can skip computing these when we don't need to.\n                tie_phi = array_sum_to_scalar(phi_i[deaths])\n                tie_phi_x = matrix_axis_0_sum_to_array(phi_x_i[deaths])\n                tie_phi_x_x = np.dot(xi_deaths.T, phi_i[deaths] * xi_deaths)\n\n                increasing_proportion = np.arange(tied_death_counts) / tied_death_counts\n                denom = 1.0 / (risk_phi - increasing_proportion * tie_phi)\n                numer = risk_phi_x - np.outer(increasing_proportion, tie_phi_x)\n\n                # computes outer products and sums them together.\n                # Naive approach is to\n                # 1) broadcast tie_phi_x_x and increasing_proportion into a (tied_death_counts, d, d) matrix\n                # 2) broadcast risk_phi_x_x and denom into a (tied_death_counts, d, d) matrix\n                # 3) subtract them, and then sum to (d, d)\n                # Alternatively, we can sum earlier without having to explicitly create (_, d, d) matrices. This is used here.\n                #\n                a1 = np.einsum(\"ab,i->ab\", risk_phi_x_x, denom) - np.einsum(\n                    \"ab,i->ab\", tie_phi_x_x, increasing_proportion * denom\n                )\n            else:\n                # no tensors here, but do some casting to make it easier in the converging step next.\n                denom = 1.0 / np.array([risk_phi])\n                numer = risk_phi_x\n                a1 = risk_phi_x_x * denom\n\n            summand = numer * denom[:, None]\n            # This is a batch outer product.\n            # given a matrix t, for each row, m, compute it's outer product: m.dot(m.T), and stack these new matrices together.\n            # which would be: np.einsum(\"Bi, Bj->Bij\", t, t)\n            a2 = summand.T.dot(summand)\n\n            gradient = gradient + x_death_sum - weighted_average * summand.sum(0)\n            log_lik = log_lik + np.dot(x_death_sum, beta) + weighted_average * np.log(denom).sum()\n            hessian = hessian + weighted_average * (a2 - a1)\n            pos -= count_of_removals\n\n        return hessian, gradient, log_lik", "response": "This function computes the values of the efron ties for each entry in the set of economy s terms."}
{"SOURCE": "codesearchnet", "instruction": "Implement a function in Python 3 to\ncompute the schoenfeld of the strata that is in strata X T E and weights.", "response": "def _compute_schoenfeld_within_strata(self, X, T, E, weights):\n        \"\"\"\n        A positive value of the residual shows an X value that is higher than expected at that death time.\n        \"\"\"\n        # TODO: the diff_against is gross\n        # This uses Efron ties.\n\n        n, d = X.shape\n\n        if not np.any(E):\n            # sometimes strata have no deaths. This means nothing is returned\n            # in the below code.\n            return np.zeros((n, d))\n\n        # Init risk and tie sums to zero\n        risk_phi, tie_phi = 0, 0\n        risk_phi_x, tie_phi_x = np.zeros((1, d)), np.zeros((1, d))\n\n        # Init number of ties and weights\n        weight_count = 0.0\n        tie_count = 0\n\n        scores = weights * np.exp(np.dot(X, self.hazards_))\n\n        diff_against = []\n\n        schoenfeld_residuals = np.empty((0, d))\n\n        # Iterate backwards to utilize recursive relationship\n        for i in range(n - 1, -1, -1):\n            # Doing it like this to preserve shape\n            ti = T[i]\n            ei = E[i]\n            xi = X[i : i + 1]\n            score = scores[i : i + 1]\n            w = weights[i]\n\n            # Calculate phi values\n            phi_i = score\n            phi_x_i = phi_i * xi\n\n            # Calculate sums of Risk set\n            risk_phi = risk_phi + phi_i\n            risk_phi_x = risk_phi_x + phi_x_i\n\n            # Calculate sums of Ties, if this is an event\n            diff_against.append((xi, ei))\n            if ei:\n\n                tie_phi = tie_phi + phi_i\n                tie_phi_x = tie_phi_x + phi_x_i\n\n                # Keep track of count\n                tie_count += 1  # aka death counts\n                weight_count += w\n\n            if i > 0 and T[i - 1] == ti:\n                # There are more ties/members of the risk set\n                continue\n            elif tie_count == 0:\n                for _ in diff_against:\n                    schoenfeld_residuals = np.append(schoenfeld_residuals, np.zeros((1, d)), axis=0)\n                diff_against = []\n                continue\n\n            # There was atleast one event and no more ties remain. Time to sum.\n            weighted_mean = np.zeros((1, d))\n\n            for l in range(tie_count):\n\n                numer = risk_phi_x - l * tie_phi_x / tie_count\n                denom = risk_phi - l * tie_phi / tie_count\n\n                weighted_mean += numer / (denom * tie_count)\n\n            for xi, ei in diff_against:\n                schoenfeld_residuals = np.append(schoenfeld_residuals, ei * (xi - weighted_mean), axis=0)\n\n            # reset tie values\n            tie_count = 0\n            weight_count = 0.0\n            tie_phi = 0\n            tie_phi_x = np.zeros((1, d))\n            diff_against = []\n\n        return schoenfeld_residuals[::-1]"}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef summary(self):\n        ci = 1 - self.alpha\n        with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n            df = pd.DataFrame(index=self.hazards_.index)\n            df[\"coef\"] = self.hazards_\n            df[\"exp(coef)\"] = np.exp(self.hazards_)\n            df[\"se(coef)\"] = self.standard_errors_\n            df[\"z\"] = self._compute_z_values()\n            df[\"p\"] = self._compute_p_values()\n            df[\"-log2(p)\"] = -np.log2(df[\"p\"])\n            df[\"lower %g\" % ci] = self.confidence_intervals_[\"lower-bound\"]\n            df[\"upper %g\" % ci] = self.confidence_intervals_[\"upper-bound\"]\n            return df", "response": "Summary statistics describing the fit."}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef predict_log_partial_hazard(self, X):\n        hazard_names = self.hazards_.index\n\n        if isinstance(X, pd.Series) and ((X.shape[0] == len(hazard_names) + 2) or (X.shape[0] == len(hazard_names))):\n            X = X.to_frame().T\n            return self.predict_log_partial_hazard(X)\n        elif isinstance(X, pd.Series):\n            assert len(hazard_names) == 1, \"Series not the correct argument\"\n            X = X.to_frame().T\n            return self.predict_log_partial_hazard(X)\n\n        index = _get_index(X)\n\n        if isinstance(X, pd.DataFrame):\n            order = hazard_names\n            X = X.reindex(order, axis=\"columns\")\n            check_for_numeric_dtypes_or_raise(X)\n            X = X.values\n\n        X = X.astype(float)\n\n        X = normalize(X, self._norm_mean.values, 1)\n        return pd.DataFrame(np.dot(X, self.hazards_), index=index)", "response": "r This is equivalent to R s linear. predictors."}
{"SOURCE": "codesearchnet", "instruction": "Explain what the following Python 3 code does\ndef predict_cumulative_hazard(self, X, times=None):\n\n        if self.strata:\n            cumulative_hazard_ = pd.DataFrame()\n            for stratum, stratified_X in X.groupby(self.strata):\n                try:\n                    c_0 = self.baseline_cumulative_hazard_[[stratum]]\n                except KeyError:\n                    raise StatError(\n                        \"\"\"The stratum %s was not found in the original training data. For example, try\nthe following on the original dataset, df: `df.groupby(%s).size()`. Expected is that %s is not present in the output.\n\"\"\"\n                        % (stratum, self.strata, stratum)\n                    )\n                col = _get_index(stratified_X)\n                v = self.predict_partial_hazard(stratified_X)\n                cumulative_hazard_ = cumulative_hazard_.merge(\n                    pd.DataFrame(np.dot(c_0, v.T), index=c_0.index, columns=col),\n                    how=\"outer\",\n                    right_index=True,\n                    left_index=True,\n                )\n        else:\n\n            c_0 = self.baseline_cumulative_hazard_\n            v = self.predict_partial_hazard(X)\n            col = _get_index(v)\n            cumulative_hazard_ = pd.DataFrame(np.dot(c_0, v.T), columns=col, index=c_0.index)\n\n        if times is not None:\n            # non-linear interpolations can push the survival curves above 1 and below 0.\n            return dataframe_interpolate_at_times(cumulative_hazard_, times)\n        return cumulative_hazard_", "response": "Predicts the cumulative hazard of individuals over the original training data."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate a brief explanation for the following Python 3 code\ndef predict_survival_function(self, X, times=None):\n        return np.exp(-self.predict_cumulative_hazard(X, times=times))", "response": "Predict the survival function for individuals given their covariates."}
{"SOURCE": "codesearchnet", "instruction": "Can you generate the documentation for the following Python 3 function\ndef predict_percentile(self, X, p=0.5):\n        subjects = _get_index(X)\n        return qth_survival_times(p, self.predict_survival_function(X)[subjects]).T", "response": "Predict the median lifetimes for the individuals X."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef _compute_baseline_survival(self):\n        survival_df = np.exp(-self.baseline_cumulative_hazard_)\n        if self.strata is None:\n            survival_df.columns = [\"baseline survival\"]\n        return survival_df", "response": "Compute the baseline survival for a given set of calendars."}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef plot_covariate_groups(self, covariates, values, plot_baseline=True, **kwargs):\n        from matplotlib import pyplot as plt\n\n        covariates = _to_list(covariates)\n        n_covariates = len(covariates)\n        values = _to_array(values)\n        if len(values.shape) == 1:\n            values = values[None, :].T\n\n        if n_covariates != values.shape[1]:\n            raise ValueError(\"The number of covariates must equal to second dimension of the values array.\")\n\n        for covariate in covariates:\n            if covariate not in self.hazards_.index:\n                raise KeyError(\"covariate `%s` is not present in the original dataset\" % covariate)\n\n        set_kwargs_drawstyle(kwargs, \"steps-post\")\n\n        if self.strata is None:\n            axes = kwargs.pop(\"ax\", None) or plt.figure().add_subplot(111)\n            x_bar = self._norm_mean.to_frame().T\n            X = pd.concat([x_bar] * values.shape[0])\n\n            if np.array_equal(np.eye(n_covariates), values):\n                X.index = [\"%s=1\" % c for c in covariates]\n            else:\n                X.index = [\", \".join(\"%s=%g\" % (c, v) for (c, v) in zip(covariates, row)) for row in values]\n            for covariate, value in zip(covariates, values.T):\n                X[covariate] = value\n\n            self.predict_survival_function(X).plot(ax=axes, **kwargs)\n            if plot_baseline:\n                self.baseline_survival_.plot(ax=axes, ls=\":\", color=\"k\", drawstyle=\"steps-post\")\n\n        else:\n            axes = []\n            for stratum, baseline_survival_ in self.baseline_survival_.iteritems():\n                ax = plt.figure().add_subplot(1, 1, 1)\n                x_bar = self._norm_mean.to_frame().T\n\n                for name, value in zip(_to_list(self.strata), _to_tuple(stratum)):\n                    x_bar[name] = value\n\n                X = pd.concat([x_bar] * values.shape[0])\n                if np.array_equal(np.eye(len(covariates)), values):\n                    X.index = [\"%s=1\" % c for c in covariates]\n                else:\n                    X.index = [\", \".join(\"%s=%g\" % (c, v) for (c, v) in zip(covariates, row)) for row in values]\n                for covariate, value in zip(covariates, values.T):\n                    X[covariate] = value\n\n                self.predict_survival_function(X).plot(ax=ax, **kwargs)\n                if plot_baseline:\n                    baseline_survival_.plot(\n                        ax=ax, ls=\":\", label=\"stratum %s baseline survival\" % str(stratum), drawstyle=\"steps-post\"\n                    )\n                plt.legend()\n                axes.append(ax)\n        return axes", "response": "Plots the categorical variables at once and then the predicted survival curve at all of the values in the original dataset."}
{"SOURCE": "codesearchnet", "instruction": "Make a summary of the following Python 3 code\ndef check_assumptions(\n        self, training_df, advice=True, show_plots=False, p_value_threshold=0.01, plot_n_bootstraps=10\n    ):\n        \"\"\"\n        Use this function to test the proportional hazards assumption. See usage example at\n        https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n\n\n        Parameters\n        -----------\n\n        training_df: DataFrame\n            the original DataFrame used in the call to ``fit(...)`` or a sub-sampled version.\n        advice: boolean, optional\n            display advice as output to the user's screen\n        show_plots: boolean, optional\n            display plots of the scaled schoenfeld residuals and loess curves. This is an eyeball test for violations.\n            This will slow down the function significantly.\n        p_value_threshold: float, optional\n            the threshold to use to alert the user of violations. See note below.\n        plot_n_bootstraps:\n            in the plots displayed, also display plot_n_bootstraps bootstrapped loess curves. This will slow down\n            the function significantly.\n\n\n        Examples\n        ----------\n\n        >>> from lifelines.datasets import load_rossi\n        >>> from lifelines import CoxPHFitter\n        >>>\n        >>> rossi = load_rossi()\n        >>> cph = CoxPHFitter().fit(rossi, 'week', 'arrest')\n        >>>\n        >>> cph.check_assumptions(rossi)\n\n\n        Notes\n        -------\n        The ``p_value_threshold`` is arbitrarily set at 0.01. Under the null, some covariates\n        will be below the threshold (i.e. by chance). This is compounded when there are many covariates.\n\n        Similarly, when there are lots of observations, even minor deviances from the proportional hazard\n        assumption will be flagged.\n\n        With that in mind, it's best to use a combination of statistical tests and eyeball tests to\n        determine the most serious violations.\n\n\n        References\n        -----------\n        section 5 in https://socialsciences.mcmaster.ca/jfox/Books/Companion/appendices/Appendix-Cox-Regression.pdf,\n        http://www.mwsug.org/proceedings/2006/stats/MWSUG-2006-SD08.pdf,\n        http://eprints.lse.ac.uk/84988/1/06_ParkHendry2015-ReassessingSchoenfeldTests_Final.pdf\n        \"\"\"\n\n        if not training_df.index.is_unique:\n            raise IndexError(\n                \"`training_df` index should be unique for this exercise. Please make it unique or use `.reset_index(drop=True)` to force a unique index\"\n            )\n\n        residuals = self.compute_residuals(training_df, kind=\"scaled_schoenfeld\")\n        test_results = proportional_hazard_test(\n            self, training_df, time_transform=[\"rank\", \"km\"], precomputed_residuals=residuals\n        )\n\n        residuals_and_duration = residuals.join(training_df[self.duration_col])\n\n        counter = 0\n        n = residuals_and_duration.shape[0]\n\n        for variable in self.hazards_.index:\n            minumum_observed_p_value = test_results.summary.loc[variable, \"p\"].min()\n            if np.round(minumum_observed_p_value, 2) > p_value_threshold:\n                continue\n\n            counter += 1\n\n            if counter == 1:\n                if advice:\n                    print(\n                        fill(\n                            \"\"\"The ``p_value_threshold`` is set at %g. Even under the null hypothesis of no violations, some covariates will be below the threshold by chance. This is compounded when there are many covariates. Similarly, when there are lots of observations, even minor deviances from the proportional hazard assumption will be flagged.\"\"\"\n                            % p_value_threshold,\n                            width=100,\n                        )\n                    )\n                    print()\n                    print(\n                        fill(\n                            \"\"\"With that in mind, it's best to use a combination of statistical tests and visual tests to determine the most serious violations. Produce visual plots using ``check_assumptions(..., show_plots=True)`` and looking for non-constant lines. See link [A] below for a full example.\"\"\",\n                            width=100,\n                        )\n                    )\n                    print()\n                test_results.print_summary()\n                print()\n\n            print()\n            print(\n                \"%d. Variable '%s' failed the non-proportional test: p-value is %s.\"\n                % (counter, variable, format_p_value(4)(minumum_observed_p_value)),\n                end=\"\\n\\n\",\n            )\n\n            if advice:\n                values = training_df[variable]\n                value_counts = values.value_counts()\n                n_uniques = value_counts.shape[0]\n\n                # Arbitrary chosen 10 and 4 to check for ability to use strata col.\n                # This should capture dichotomous / low cardinality values.\n                if n_uniques <= 10 and value_counts.min() >= 5:\n                    print(\n                        fill(\n                            \"   Advice: with so few unique values (only {0}), you can include `strata=['{1}', ...]` in the call in `.fit`. See documentation in link [E] below.\".format(\n                                n_uniques, variable\n                            ),\n                            width=100,\n                        )\n                    )\n                else:\n                    print(\n                        fill(\n                            \"\"\"   Advice 1: the functional form of the variable '{var}' might be incorrect. That is, there may be non-linear terms missing. The proportional hazard test used is very sensitive to incorrect functional forms. See documentation in link [D] below on how to specify a functional form.\"\"\".format(\n                                var=variable\n                            ),\n                            width=100,\n                        ),\n                        end=\"\\n\\n\",\n                    )\n                    print(\n                        fill(\n                            \"\"\"   Advice 2: try binning the variable '{var}' using pd.cut, and then specify it in `strata=['{var}', ...]` in the call in `.fit`. See documentation in link [B] below.\"\"\".format(\n                                var=variable\n                            ),\n                            width=100,\n                        ),\n                        end=\"\\n\\n\",\n                    )\n                    print(\n                        fill(\n                            \"\"\"   Advice 3: try adding an interaction term with your time variable. See documentation in link [C] below.\"\"\".format(\n                                var=variable\n                            ),\n                            width=100,\n                        ),\n                        end=\"\\n\\n\",\n                    )\n\n            if show_plots:\n\n                from matplotlib import pyplot as plt\n\n                fig = plt.figure()\n\n                # plot variable against all time transformations.\n                for i, (transform_name, transformer) in enumerate(TimeTransformers().iter([\"rank\", \"km\"]), start=1):\n                    p_value = test_results.summary.loc[(variable, transform_name), \"p\"]\n\n                    ax = fig.add_subplot(1, 2, i)\n\n                    y = residuals_and_duration[variable]\n                    tt = transformer(self.durations, self.event_observed, self.weights)[self.event_observed.values]\n\n                    ax.scatter(tt, y, alpha=0.75)\n\n                    y_lowess = lowess(tt.values, y.values)\n                    ax.plot(tt, y_lowess, color=\"k\", alpha=1.0, linewidth=2)\n\n                    # bootstrap some possible other lowess lines. This is an approximation of the 100% confidence intervals\n                    for _ in range(plot_n_bootstraps):\n                        ix = sorted(np.random.choice(n, n))\n                        tt_ = tt.values[ix]\n                        y_lowess = lowess(tt_, y.values[ix])\n                        ax.plot(tt_, y_lowess, color=\"k\", alpha=0.30)\n\n                    best_xlim = ax.get_xlim()\n                    ax.hlines(0, 0, tt.max(), linestyles=\"dashed\", linewidths=1)\n                    ax.set_xlim(best_xlim)\n\n                    ax.set_xlabel(\"%s-transformed time\\n(p=%.4f)\" % (transform_name, p_value), fontsize=10)\n\n                fig.suptitle(\"Scaled Schoenfeld residuals of '%s'\" % variable, fontsize=14)\n                plt.tight_layout()\n                plt.subplots_adjust(top=0.90)\n\n        if advice and counter > 0:\n            print(\n                dedent(\n                    r\"\"\"\n                ---\n                [A]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html\n                [B]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Bin-variable-and-stratify-on-it\n                [C]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Introduce-time-varying-covariates\n                [D]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Modify-the-functional-form\n                [E]  https://lifelines.readthedocs.io/en/latest/jupyter_notebooks/Proportional%20hazard%20assumption.html#Stratification\n            \"\"\"\n                )\n            )\n\n        if counter == 0:\n            print(\"Proportional hazard assumption looks okay.\")", "response": "This function checks the proportional hazards and returns a dictionary of information about the most serious violations."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nreturns the concordance score of the current model AUC A", "response": "def score_(self):\n        \"\"\"\n        The concordance score (also known as the c-index) of the fit.  The c-index is a generalization of the ROC AUC\n        to survival data, including censorships.\n\n        For this purpose, the ``score_`` is a measure of the predictive accuracy of the fitted model\n        onto the training dataset.\n\n        References\n        ----------\n        https://stats.stackexchange.com/questions/133817/stratified-concordance-index-survivalsurvconcordance\n\n        \"\"\"\n        # pylint: disable=access-member-before-definition\n        if not hasattr(self, \"_concordance_score_\"):\n            if self.strata:\n                # https://stats.stackexchange.com/questions/133817/stratified-concordance-index-survivalsurvconcordance\n                num_correct, num_tied, num_pairs = 0, 0, 0\n                for _, _df in self._predicted_partial_hazards_.groupby(self.strata):\n                    if _df.shape[0] == 1:\n                        continue\n                    _num_correct, _num_tied, _num_pairs = _concordance_summary_statistics(\n                        _df[\"T\"].values, -_df[\"P\"].values, _df[\"E\"].values\n                    )\n                    num_correct += _num_correct\n                    num_tied += _num_tied\n                    num_pairs += _num_pairs\n            else:\n                df = self._predicted_partial_hazards_\n                num_correct, num_tied, num_pairs = _concordance_summary_statistics(\n                    df[\"T\"].values, -df[\"P\"].values, df[\"E\"].values\n                )\n\n            self._concordance_score_ = _concordance_ratio(num_correct, num_tied, num_pairs)\n            return self._concordance_score_\n        return self._concordance_score_"}
{"SOURCE": "codesearchnet", "instruction": "Create a Python 3 function for\nreturning the concordance index between two series of event times and predicted scores.", "response": "def concordance_index(event_times, predicted_scores, event_observed=None):\n    \"\"\"\n    Calculates the concordance index (C-index) between two series\n    of event times. The first is the real survival times from\n    the experimental data, and the other is the predicted survival\n    times from a model of some kind.\n\n    The c-index is the average of how often a model says X is greater than Y when, in the observed\n    data, X is indeed greater than Y. The c-index also handles how to handle censored values\n    (obviously, if Y is censored, it's hard to know if X is truly greater than Y).\n\n\n    The concordance index is a value between 0 and 1 where:\n\n    - 0.5 is the expected result from random predictions,\n    - 1.0 is perfect concordance and,\n    - 0.0 is perfect anti-concordance (multiply predictions with -1 to get 1.0)\n\n    Parameters\n    ----------\n    event_times: iterable\n         a length-n iterable of observed survival times.\n    predicted_scores: iterable\n        a length-n iterable of predicted scores - these could be survival times, or hazards, etc. See https://stats.stackexchange.com/questions/352183/use-median-survival-time-to-calculate-cph-c-statistic/352435#352435\n    event_observed: iterable, optional\n        a length-n iterable censorship flags, 1 if observed, 0 if not. Default None assumes all observed.\n\n    Returns\n    -------\n    c-index: float\n      a value between 0 and 1.\n\n    References\n    -----------\n    Harrell FE, Lee KL, Mark DB. Multivariable prognostic models: issues in\n    developing models, evaluating assumptions and adequacy, and measuring and\n    reducing errors. Statistics in Medicine 1996;15(4):361-87.\n\n    Examples\n    --------\n\n    >>> from lifelines.utils import concordance_index\n    >>> cph = CoxPHFitter().fit(df, 'T', 'E')\n    >>> concordance_index(df['T'], -cph.predict_partial_hazard(df), df['E'])\n\n    \"\"\"\n    event_times = np.asarray(event_times, dtype=float)\n    predicted_scores = np.asarray(predicted_scores, dtype=float)\n\n    # Allow for (n, 1) or (1, n) arrays\n    if event_times.ndim == 2 and (event_times.shape[0] == 1 or event_times.shape[1] == 1):\n        # Flatten array\n        event_times = event_times.ravel()\n    # Allow for (n, 1) or (1, n) arrays\n    if predicted_scores.ndim == 2 and (predicted_scores.shape[0] == 1 or predicted_scores.shape[1] == 1):\n        # Flatten array\n        predicted_scores = predicted_scores.ravel()\n\n    if event_times.shape != predicted_scores.shape:\n        raise ValueError(\"Event times and predictions must have the same shape\")\n    if event_times.ndim != 1:\n        raise ValueError(\"Event times can only be 1-dimensional: (n,)\")\n\n    if event_observed is None:\n        event_observed = np.ones(event_times.shape[0], dtype=float)\n    else:\n        event_observed = np.asarray(event_observed, dtype=float).ravel()\n        if event_observed.shape != event_times.shape:\n            raise ValueError(\"Observed events must be 1-dimensional of same length as event times\")\n\n    num_correct, num_tied, num_pairs = _concordance_summary_statistics(event_times, predicted_scores, event_observed)\n\n    return _concordance_ratio(num_correct, num_tied, num_pairs)"}
{"SOURCE": "codesearchnet", "instruction": "How would you explain what the following Python 3 function does\ndef _concordance_summary_statistics(\n    event_times, predicted_event_times, event_observed\n):  # pylint: disable=too-many-locals\n    \"\"\"Find the concordance index in n * log(n) time.\n\n    Assumes the data has been verified by lifelines.utils.concordance_index first.\n    \"\"\"\n    # Here's how this works.\n    #\n    # It would be pretty easy to do if we had no censored data and no ties. There, the basic idea\n    # would be to iterate over the cases in order of their true event time (from least to greatest),\n    # while keeping track of a pool of *predicted* event times for all cases previously seen (= all\n    # cases that we know should be ranked lower than the case we're looking at currently).\n    #\n    # If the pool has O(log n) insert and O(log n) RANK (i.e., \"how many things in the pool have\n    # value less than x\"), then the following algorithm is n log n:\n    #\n    # Sort the times and predictions by time, increasing\n    # n_pairs, n_correct := 0\n    # pool := {}\n    # for each prediction p:\n    #     n_pairs += len(pool)\n    #     n_correct += rank(pool, p)\n    #     add p to pool\n    #\n    # There are three complications: tied ground truth values, tied predictions, and censored\n    # observations.\n    #\n    # - To handle tied true event times, we modify the inner loop to work in *batches* of observations\n    # p_1, ..., p_n whose true event times are tied, and then add them all to the pool\n    # simultaneously at the end.\n    #\n    # - To handle tied predictions, which should each count for 0.5, we switch to\n    #     n_correct += min_rank(pool, p)\n    #     n_tied += count(pool, p)\n    #\n    # - To handle censored observations, we handle each batch of tied, censored observations just\n    # after the batch of observations that died at the same time (since those censored observations\n    # are comparable all the observations that died at the same time or previously). However, we do\n    # NOT add them to the pool at the end, because they are NOT comparable with any observations\n    # that leave the study afterward--whether or not those observations get censored.\n    if np.logical_not(event_observed).all():\n        return (0, 0, 0)\n\n    died_mask = event_observed.astype(bool)\n    # TODO: is event_times already sorted? That would be nice...\n    died_truth = event_times[died_mask]\n    ix = np.argsort(died_truth)\n    died_truth = died_truth[ix]\n    died_pred = predicted_event_times[died_mask][ix]\n\n    censored_truth = event_times[~died_mask]\n    ix = np.argsort(censored_truth)\n    censored_truth = censored_truth[ix]\n    censored_pred = predicted_event_times[~died_mask][ix]\n\n    censored_ix = 0\n    died_ix = 0\n    times_to_compare = _BTree(np.unique(died_pred))\n    num_pairs = np.int64(0)\n    num_correct = np.int64(0)\n    num_tied = np.int64(0)\n\n    # we iterate through cases sorted by exit time:\n    # - First, all cases that died at time t0. We add these to the sortedlist of died times.\n    # - Then, all cases that were censored at time t0. We DON'T add these since they are NOT\n    #   comparable to subsequent elements.\n    while True:\n        has_more_censored = censored_ix < len(censored_truth)\n        has_more_died = died_ix < len(died_truth)\n        # Should we look at some censored indices next, or died indices?\n        if has_more_censored and (not has_more_died or died_truth[died_ix] > censored_truth[censored_ix]):\n            pairs, correct, tied, next_ix = _handle_pairs(censored_truth, censored_pred, censored_ix, times_to_compare)\n            censored_ix = next_ix\n        elif has_more_died and (not has_more_censored or died_truth[died_ix] <= censored_truth[censored_ix]):\n            pairs, correct, tied, next_ix = _handle_pairs(died_truth, died_pred, died_ix, times_to_compare)\n            for pred in died_pred[died_ix:next_ix]:\n                times_to_compare.insert(pred)\n            died_ix = next_ix\n        else:\n            assert not (has_more_died or has_more_censored)\n            break\n\n        num_pairs += pairs\n        num_correct += correct\n        num_tied += tied\n\n    return (num_correct, num_tied, num_pairs)", "response": "This function is used to compute the concordance index for the given event times and predicted event times."}
{"SOURCE": "codesearchnet", "instruction": "Can you write a function in Python 3 where it\nhandles all pairs that exited at the same time as pred.", "response": "def _handle_pairs(truth, pred, first_ix, times_to_compare):\n    \"\"\"\n    Handle all pairs that exited at the same time as truth[first_ix].\n\n    Returns\n    -------\n      (pairs, correct, tied, next_ix)\n      new_pairs: The number of new comparisons performed\n      new_correct: The number of comparisons correctly predicted\n      next_ix: The next index that needs to be handled\n    \"\"\"\n    next_ix = first_ix\n    while next_ix < len(truth) and truth[next_ix] == truth[first_ix]:\n        next_ix += 1\n    pairs = len(times_to_compare) * (next_ix - first_ix)\n    correct = np.int64(0)\n    tied = np.int64(0)\n    for i in range(first_ix, next_ix):\n        rank, count = times_to_compare.rank(pred[i])\n        correct += rank\n        tied += count\n\n    return (pairs, correct, tied, next_ix)"}
{"SOURCE": "codesearchnet", "instruction": "Can you tell what is the following Python 3 function doing\ndef _naive_concordance_summary_statistics(event_times, predicted_event_times, event_observed):\n    num_pairs = 0.0\n    num_correct = 0.0\n    num_tied = 0.0\n\n    for a, time_a in enumerate(event_times):\n        pred_a = predicted_event_times[a]\n        event_a = event_observed[a]\n        # Don't want to double count\n        for b in range(a + 1, len(event_times)):\n            time_b = event_times[b]\n            pred_b = predicted_event_times[b]\n            event_b = event_observed[b]\n\n            if _valid_comparison(time_a, time_b, event_a, event_b):\n                num_pairs += 1.0\n                crct, ties = _concordance_value(time_a, time_b, pred_a, pred_b, event_a, event_b)\n                num_correct += crct\n                num_tied += ties\n\n    return (num_correct, num_tied, num_pairs)", "response": "Compute the concordance summary statistics for the n -ive concordance."}
